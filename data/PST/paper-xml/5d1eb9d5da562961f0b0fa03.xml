<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pre-Training with Whole Word Masking for Chinese BERT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-10-29">29 Oct 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yiming</forename><surname>Cui</surname></persName>
							<email>ymcui@ir.hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Research Center for Social Computing and Information Retrieval (SCIR)</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">iFLYTEK Research</orgName>
								<orgName type="laboratory">State Key Laboratory of Cognitive Intelligence</orgName>
								<orgName type="institution">China § iFLYTEK AI Research (Hebei)</orgName>
								<address>
									<settlement>Langfang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Research Center for Social Computing and Information Retrieval (SCIR)</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
							<email>tliu@ir.hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Research Center for Social Computing and Information Retrieval (SCIR)</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bing</forename><surname>Qin</surname></persName>
							<email>qinb@ir.hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Research Center for Social Computing and Information Retrieval (SCIR)</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ziqing</forename><surname>Yang</surname></persName>
							<email>zqyang5@iflytek.com</email>
							<affiliation key="aff1">
								<orgName type="department">iFLYTEK Research</orgName>
								<orgName type="laboratory">State Key Laboratory of Cognitive Intelligence</orgName>
								<orgName type="institution">China § iFLYTEK AI Research (Hebei)</orgName>
								<address>
									<settlement>Langfang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shijin</forename><surname>Wang</surname></persName>
							<email>sjwang3@iflytek.com</email>
							<affiliation key="aff1">
								<orgName type="department">iFLYTEK Research</orgName>
								<orgName type="laboratory">State Key Laboratory of Cognitive Intelligence</orgName>
								<orgName type="institution">China § iFLYTEK AI Research (Hebei)</orgName>
								<address>
									<settlement>Langfang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guoping</forename><surname>Hu</surname></persName>
							<email>gphu@iflytek.com</email>
							<affiliation key="aff1">
								<orgName type="department">iFLYTEK Research</orgName>
								<orgName type="laboratory">State Key Laboratory of Cognitive Intelligence</orgName>
								<orgName type="institution">China § iFLYTEK AI Research (Hebei)</orgName>
								<address>
									<settlement>Langfang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Pre-Training with Whole Word Masking for Chinese BERT</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-10-29">29 Oct 2019</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1906.08101v2[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Bidirectional Encoder Representations from Transformers (BERT) has shown marvelous improvements across various NLP tasks. Recently, an upgraded version of BERT has been released with Whole Word Masking (WWM), which mitigate the drawbacks of masking partial WordPiece tokens in pre-training BERT. In this technical report, we adapt whole word masking in Chinese text, that masking the whole word instead of masking Chinese characters, which could bring another challenge in Masked Language Model (MLM) pre-training task. The proposed models are verified on various NLP tasks, across sentence-level to document-level, including machine reading comprehension (CMRC 2018, DRCD, CJRC), natural language inference (XNLI), sentiment classification (ChnSentiCorp), sentence pair matching (LCQMC, BQ Corpus), and document classification (THUCNews). Experimental results on these datasets show that the whole word masking could bring another significant gain. Moreover, we also examine the effectiveness of the Chinese pre-trained models: BERT, ERNIE, BERTwwm, BERT-wwm-ext, RoBERTa-wwm-ext, and RoBERTa-wwm-ext-large. 1 2 Tsinghua university has also released a model called ERNIE <ref type="bibr" target="#b23">(Zhang et al., 2019b)</ref> but was not trained on Chinese. In this paper, ERNIE refers to the model by <ref type="bibr" target="#b19">Sun et al. (2019)</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Bidirectional Encoder Representations from Transformers (BERT) <ref type="bibr" target="#b8">(Devlin et al., 2019)</ref> has become enormously popular and proven to be effective in recent NLP studies which utilizes large-scale unlabeled training data and generates enriched contextual representations, showing its powerful performance on various natural language processing tasks. As we traverse several popular machine reading comprehension benchmarks, such as SQuAD <ref type="bibr" target="#b14">(Rajpurkar et al., 2018)</ref>, CoQA <ref type="bibr" target="#b17">(Reddy et al., 2019)</ref>, QuAC <ref type="bibr" target="#b3">(Choi et al., 2018)</ref>, NaturalQuestions <ref type="bibr" target="#b10">(Kwiatkowski et al., 2019)</ref>, RACE <ref type="bibr" target="#b11">(Lai et al., 2017)</ref>, we can see that most of the top performing models are based on BERT and its variants <ref type="bibr" target="#b5">(Cui et al., 2017;</ref><ref type="bibr" target="#b7">Dai et al., 2019;</ref><ref type="bibr" target="#b22">Zhang et al., 2019a;</ref><ref type="bibr" target="#b16">Ran et al., 2019)</ref>.</p><p>Recently, the authors of BERT have released an updated version of BERT, which is called Whole Word Masking. The whole word masking mainly mitigates the drawbacks in original BERT that, if the masked WordPiece token <ref type="bibr" target="#b20">(Wu et al., 2016)</ref> belongs to a whole word, then all the WordPiece tokens (which forms a complete word) will be masked altogether. This will explicitly force the model to recover the whole word in Masked Language Model (MLM) pre-training task, instead of just recovering WordPiece tokens, which is much more challenging. Along with the strategy, they also provide pre-trained English models (BERTlarge-wwm) for the community, which is beneficial for the researcher to design more powerful models based on them.</p><p>Before <ref type="bibr" target="#b8">Devlin et al. (2019)</ref> releasing BERT with whole word masking, <ref type="bibr" target="#b19">Sun et al. (2019)</ref> had proposed Enhanced Representation through kNowledge IntEgration (ERNIE) with a similar spirit and trained on not only Wikipedia data but also community QA, Baike (similar to Wikipedia), etc. 2 It was tested on various NLP tasks and showed consistent improvements over BERT.</p><p>In this technical report, we adapt the whole word masking strategy in Chinese BERT to verify its effectiveness. The model was pre-trained on the latest Wikipedia dump in Chinese (both Simplified and Traditional Chinese proportions are kept). Note that, we did not exploit additional data in our model, and aim to provide a more general base for developing NLP systems in Simplified and Traditional Chinese. Extensive experiments are conducted on various Chinese NLP datasets, ranging from sentence-level to document-level, which include machine reading comprehension, sentiment classification, sentence pair matching, natural language inference, document classification, etc. The results show that the proposed model brings another gain over BERT and ERNIE in most of the tasks, and we provide several useful tips for using these pre-trained models, which may be helpful in the future research.</p><p>The contributions of this technical report are listed as follows.</p><p>• We adapt the whole word masking in Chinese BERT and release the pre-trained models for the community.</p><p>• Extensive experiments are carried out to better demonstrate the effectiveness of BERT, ERNIE, and BERT-wwm.</p><p>• Several useful tips are provided on using these pre-trained models on Chinese text.</p><p>2 Chinese BERT with Whole Word Masking</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Methodology</head><p>We strictly follow the original whole word masking codes and did not change other components, such as the percentage of word masking, etc. An example of the whole word masking is depicted in Figure <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Data Processing</head><p>We downloaded the latest Wikipedia dump<ref type="foot" target="#foot_1">3</ref> , and pre-processed with WikiExtractor.py as suggested by <ref type="bibr" target="#b8">Devlin et al. (2019)</ref>  <ref type="bibr" target="#b8">Devlin et al. (2019)</ref>, for computation efficiency and learning long-range dependencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Pre-Training Details</head><p>We assume the whole word masking is a remedy for the BERT to know the word boundary and should be a 'patch' rather than a brand new model. Under this assumption, we did NOT train our model from scratch but from the official BERTbase (Chinese). We train 100K steps on the samples with a maximum length of 128, batch size of 2,560, an initial learning rate of 1e-4 (with warmup ratio 10%). Then, we train another 100K steps on a maximum length of 512 with a batch size of 384 to learn the long-range dependencies and position embeddings. Note that, the input data of the two phases should be changed according to the maximum length. Instead of using original AdamWeightDecayOptimizer in BERT, we use LAMB optimizer <ref type="bibr" target="#b21">(You et al., 2019)</ref> for better scalability in large batch. <ref type="foot" target="#foot_3">5</ref> The pre-training was done on Google Cloud TPU v3 with 128G HBM.<ref type="foot" target="#foot_4">6</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Fine-Tuning on Downstream Tasks</head><p>It is straightforward to use this model, as only one step is needed: replace original Chinese BERT<ref type="foot" target="#foot_5">7</ref> with our model, without changing config and vocabulary file.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We carried out extensive experiments on various natural language processing tasks, covering a wide spectrum of text length, i.e. from sentence-level to document-level. Specifically, we choose the following popular Chinese datasets, including the ones that were also used in BERT and ERNIE. We adopt additional datasets for testing their performance in a wider range.</p><p>• Machine Reading Comprehension (MRC): CMRC 2018 <ref type="bibr" target="#b6">(Cui et al., 2019)</ref>, DRCD <ref type="bibr" target="#b18">(Shao et al., 2018</ref><ref type="bibr">), CJRC (Duan et al., 2019</ref>) • Natural Language Inference (NLI): XNLI <ref type="bibr" target="#b4">(Conneau et al., 2018)</ref> • Sentiment Classification (SC): ChnSenti-Corp<ref type="foot" target="#foot_6">8</ref> </p><formula xml:id="formula_0">[Original Sentence] 使用语言模型来预测下一个词的probability。 [Original Sentence with CWS] 使用 语言 模 模 模型 型 型 来 预 预 预测 测 测 下 一个 词 的 probability 。 [Original BERT Input] 使 用 语 言 [MASK] 型 型 型 来 [MASK] 测 测 测 下 一 个 词 的 pro [MASK] ##lity 。 [Whold Word Masking Input] 使 用 语 言 [MASK] [MASK] 来 [MASK] [MASK] 下 一 个 词 的 [MASK] [MASK] [MASK] 。</formula><p>• Sentence Pair Matching (SPM): LCQMC <ref type="bibr" target="#b13">(Liu et al., 2018)</ref>, BQ Corpus <ref type="bibr" target="#b2">(Chen et al., 2018)</ref> • Document Classification (DC): THUC-News <ref type="bibr" target="#b12">(Li and Sun, 2007)</ref> In order to make a fair comparison, for each dataset, we keep the same hyper-parameters (such maximum length, warm-up steps, etc) and only tune the initial learning rate from 1e-4 to 1e-5 for each model. We run the same experiment for ten times to ensure the reliability of results. The best initial learning rate is determined by selecting the best average development set performance. We report the maximum, and average scores to both evaluate the peak and average performance of these models. For detailed hyper-parameter settings, please see Table <ref type="table" target="#tab_1">1</ref>.</p><p>In this technical report, we focus on comparing existing Chinese pre-trained models: BERT, ERNIE, and our models including BERT-wwm, BERT-wwm-ext, RoBERTa-wwm-ext, RoBERTawwm-ext-large. The model comparisons are depicted in Table <ref type="table" target="#tab_2">2</ref>.</p><p>We carried out all experiments under Tensor-Flow framework <ref type="bibr" target="#b0">(Abadi et al., 2016)</ref>. Note that, ERNIE only provides PaddlePaddle version<ref type="foot" target="#foot_7">9</ref> , so we have to convert the weights into TensorFlow version, where we obtain similar results on XNLI dataset which verifies that the conversion is successful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Machine Reading Comprehension</head><p>Machine Reading Comprehension (MRC) is a representative document-level modeling task which requires to answer the questions based on the given passages. We mainly test these models on three datasets: CMRC 2018, DRCD, and CJRC.</p><p>• CMRC 2018: A span-extraction machine reading comprehension dataset, which is similar to SQuAD <ref type="bibr" target="#b15">(Rajpurkar et al., 2016</ref>) that extract a passage span for the given question.</p><p>• DRCD: This is also a span-extraction MRC dataset, but in Traditional Chinese.</p><p>• CJRC: Similar to CoQA <ref type="bibr" target="#b17">(Reddy et al., 2019)</ref>, which has yes/no questions, noanswer questions and span-extraction questions. The data is collected from Chinese law judgment documents. Note that, we only use small-train-data.json for training. The development and test set are collected inhouse (does not publicly available due to the license issue and is not the same as the official competition).</p><p>The results are depicted in Table <ref type="table" target="#tab_3">3</ref>, 4, 5. As we can see, BERT-wwm yields significant improvements on CMRC 2018 and DRCD, which demonstrate its effectiveness on modeling long sequences. Also, we find that ERNIE does not show a competitive performance on DRCD, which indicate that it is not suitable for processing Traditional Chinese text. After examining the vocabulary of ERNIE, we discovered that the Traditional Chinese characters are removed<ref type="foot" target="#foot_8">10</ref> , and thus, resulting in an inferior performance. When it comes to CJRC, where the text is written in professional ways regarding Chinese laws, BERT-wwm shows moderate improvement over BERT and ERNIE,    but not that salient, indicating that further domain adaptation is needed for non-general domains. Also, in professional domains, the performance of Chinese word segmentor may also decrease and will, in turn, affect the performance of ERNIE/BERT-wwm, which rely on Chinese segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Natural Language Inference</head><p>Following BERT and ERNIE, we use Chinese proportion of XNLI to test these models. The results show that ERNIE outperforms BERT/BERT-wwm significantly overall and BERT-wwm shows competitive performance on the test set.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Sentiment Classification</head><p>We use ChnSentiCorp, where the text should be classified into positive or negative label, for evaluating sentiment classification performance. We can see that ERNIE achieves the best performance on ChnSentiCorp, followed by BERT-wwm and BERT. When it comes to Sina Weibo, BERT-wwm shows better performance in terms of maximum and average scores on the test set. As ERNIE was trained on additional web text, it is beneficial to model non-formal text and capture the sentiment in social communication text, such as Weibo.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Sentence Pair Matching</head><p>We adopt Large-scale Chinese Question Matching Corpus (LCQMC) and BQ Corpus for testing sentence pair matching task. As we can see that ERNIE outperforms BERT/BERT-wwm on LCQMC data. Though the peak performance of BERT-wwm is similar to BERT, the average score is relatively higher, indicating its potential in achieving higher scores (subject to the randomness). However, on BQ Corpus, we find BERTwwm generally outperforms ERNIE and BERT, especially averaged scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Document Classification</head><p>THUCNews is a dataset that contains Sina news in different genres, which is a part of THUCTC. 11 In this paper, specifically, we use a version that contains 50K news in 10 domains (evenly distributed), including sports, finance, technology, etc. 12 As we can see that, BERT-wwm and BERT outperform ERNIE again on long sequence modeling task, demonstrating their effectiveness.   we could not do exhaustive experiments on these datasets. However, we still have some (possibly) useful tips for the readers, where the tips are solely based on the materials above or our experience in using these models.</p><p>• Initial learning rate is the most important hyper-parameters (regardless of BERT or other neural networks), and should ALWAYS be tuned for better performance.</p><p>• As shown in the experimental results, BERT and BERT-wwm share almost the same best initial learning rate, so it is straightforward to apply your initial learning rate in BERT to BERT-wwm. However, we find that ERNIE does not share the same characteristics, so it is STRONGLY recommended to tune the learning rate.</p><p>• As BERT and BERT-wwm were trained on Wikipedia data, they show relatively better performance on the formal text. While, ERNIE was trained on larger data, including web text, which will be useful on casual text, such as Weibo (microblogs).</p><p>• In long-sequence tasks, such as machine reading comprehension and document classification, we suggest using BERT or BERTwwm.</p><p>• As these pre-trained models are trained in general domains, if the task data is extremely different from the pre-training data (Wikipedia for BERT/BERT-wwm), we suggest taking another pre-training steps on the task data, which was also suggested by <ref type="bibr" target="#b8">(Devlin et al., 2019)</ref>.</p><p>• As there are so many possibilities in pretraining stage (such as initial learning rate, global training steps, warm-up steps, etc.), our implementation may not be optimal using the same pre-training data. Readers are advised to train their own model if seeking for another boost in performance. However, if it is unable to do pre-training, choose one of these pre-trained models which was trained on a similar domain to the down-stream task.</p><p>• When dealing with Traditional Chinese text, use BERT or BERT-wwm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Disclaimer</head><p>The experiments only represent the empirical results in certain conditions and should not be regarded as the nature of the respective models. The results may vary using different random seeds, computing devices, etc. Note that, as we have not been testing ERNIE on PaddlePaddle, the results in this technical report may not reflect its true performance (Though we have reproduced several results on the datasets that they had tested.).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this technical report, we utilize the whole word masking strategy for Chinese BERT and release the pre-trained model for the research community. The experimental results indicate that the proposed pre-trained model yields substantial improvements on various NLP tasks, compared to BERT and ERNIE. We hope the release of the pretrained models could further accelerate the natural language processing in the Chinese research community.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Examples of the whole word masking in BERT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Hyper-parameter settings and data statistics in different task. † represents the dataset was also evaluated by BERT<ref type="bibr" target="#b8">(Devlin et al., 2019)</ref>. ‡ represents the dataset was also evaluated by ERNIE<ref type="bibr" target="#b19">(Sun et al., 2019)</ref>. The dataset without any marks represent new benchmarks on these models.</figDesc><table><row><cell>Dataset</cell><cell cols="7">Task MaxLen Batch Epoch Train # Dev # Test #</cell><cell>Domain</cell></row><row><cell>CMRC 2018</cell><cell>MRC</cell><cell>512</cell><cell>64</cell><cell>2</cell><cell>10K</cell><cell>3.2K</cell><cell>4.9K</cell><cell>Wikipedia</cell></row><row><cell>DRCD</cell><cell>MRC</cell><cell>512</cell><cell>64</cell><cell>2</cell><cell>27K</cell><cell>3.5K</cell><cell>3.5K</cell><cell>Wikipedia</cell></row><row><cell>CJRC</cell><cell>MRC</cell><cell>512</cell><cell>64</cell><cell>2</cell><cell>10K</cell><cell>3.2K</cell><cell>3.2K</cell><cell>law</cell></row><row><cell>XNLI  † ‡</cell><cell>NLI</cell><cell>128</cell><cell>64</cell><cell>2</cell><cell>392K</cell><cell>2.5K</cell><cell>5K</cell><cell>various</cell></row><row><cell>ChnSentiCorp  ‡</cell><cell>SC</cell><cell>256</cell><cell>64</cell><cell>3</cell><cell>9.6K</cell><cell>1.2K</cell><cell>1.2K</cell><cell>various</cell></row><row><cell>LCQMC  ‡</cell><cell>SPM</cell><cell>128</cell><cell>64</cell><cell>3</cell><cell>240K</cell><cell>8.8K</cell><cell>12.5K</cell><cell>Zhidao</cell></row><row><cell>BQ Corpus</cell><cell>SPM</cell><cell>128</cell><cell>64</cell><cell>3</cell><cell>100K</cell><cell>10K</cell><cell>10K</cell><cell>QA</cell></row><row><cell>THUCNews</cell><cell>DC</cell><cell>512</cell><cell>64</cell><cell>3</cell><cell>50K</cell><cell>5K</cell><cell>10K</cell><cell>news</cell></row><row><cell></cell><cell></cell><cell></cell><cell>BERT</cell><cell>BERT-wwm</cell><cell></cell><cell cols="2">ERNIE</cell><cell></cell></row><row><cell cols="2">Pre-Train Data</cell><cell cols="2">Wikipedia</cell><cell>Wikipedia</cell><cell cols="4">Wikipedia +Baike+Tieba, etc.</cell></row><row><cell>Sentence #</cell><cell></cell><cell></cell><cell></cell><cell>24M</cell><cell></cell><cell cols="2">173M</cell><cell></cell></row><row><cell cols="2">Vocabulary #</cell><cell></cell><cell cols="2">21,128</cell><cell></cell><cell cols="2">18,000 (17,964)</cell><cell></cell></row><row><cell cols="2">Hidden Activation</cell><cell></cell><cell cols="2">GeLU</cell><cell></cell><cell cols="2">ReLU</cell><cell></cell></row><row><cell cols="2">Hidden Size/Layers</cell><cell></cell><cell></cell><cell cols="2">768 &amp; 12</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Attention Head #</cell><cell></cell><cell></cell><cell></cell><cell>12</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparisons of Chinese pre-trained models.</figDesc><table><row><cell>CMRC 2018</cell><cell>EM</cell><cell>Dev</cell><cell>F1</cell><cell>EM</cell><cell>Test</cell><cell>F1</cell><cell>Challenge EM F1</cell></row><row><cell>BERT</cell><cell cols="7">65.5 (64.4) 84.5 (84.0) 70.0 (68.7) 87.0 (86.3) 18.6 (17.0) 43.3 (41.3)</cell></row><row><cell>ERNIE</cell><cell cols="7">65.4 (64.3) 84.7 (84.2) 69.4 (68.2) 86.6 (86.1) 19.6 (17.0) 44.3 (42.8)</cell></row><row><cell>BERT-wwm</cell><cell cols="7">66.3 (65.0) 85.6 (84.7) 70.5 (69.1) 87.4 (86.7) 21.0 (19.3) 47.0 (43.9)</cell></row><row><cell>BERT-wwm-ext</cell><cell cols="7">67.1 (65.6) 85.7 (85.0) 71.4 (70.0) 87.7 (87.0) 24.0 (20.0) 47.3 (44.6)</cell></row><row><cell>RoBERTa-wwm-ext</cell><cell cols="7">67.4 (66.5) 87.2 (86.5) 72.6 (71.4) 89.4 (88.8) 26.2 (24.6) 51.0 (49.1)</cell></row><row><cell cols="8">RoBERTa-wwm-ext-large 68.5 (67.6) 88.4 (87.9) 74.2 (72.4) 90.6 (90.0) 31.5 (30.1) 60.1 (57.5)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Results on CMRC 2018 (Simplified Chinese). The average score of 10 independent runs is depicted in brackets. Overall best performance is depicted in boldface.</figDesc><table><row><cell>DRCD</cell><cell>EM</cell><cell>Dev</cell><cell>F1</cell><cell>EM</cell><cell>Test</cell><cell>F1</cell></row><row><cell>BERT</cell><cell cols="6">83.1 (82.7) 89.9 (89.6) 82.2 (81.6) 89.2 (88.8)</cell></row><row><cell>ERNIE</cell><cell cols="6">73.2 (73.0) 83.9 (83.8) 71.9 (71.4) 82.5 (82.3)</cell></row><row><cell>BERT-wwm</cell><cell cols="6">84.3 (83.4) 90.5 (90.2) 82.8 (81.8) 89.7 (89.0)</cell></row><row><cell>BERT-wwm-ext</cell><cell cols="6">85.0 (84.5) 91.2 (90.9) 83.6 (83.0) 90.4 (89.9)</cell></row><row><cell>RoBERTa-wwm-ext</cell><cell cols="6">86.6 (85.9) 92.5 (92.2) 85.6 (85.2) 92.0 (91.7)</cell></row></table><note>RoBERTa-wwm-ext-large 89.6 (89.1)94.8 (94.4)  89.6 (88.9)  94.5 (94.1)    </note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Results on DRCD (Traditional Chinese).</figDesc><table><row><cell>CJRC</cell><cell>EM</cell><cell>Dev</cell><cell>F1</cell><cell>EM</cell><cell>Test</cell><cell>F1</cell></row><row><cell>BERT</cell><cell cols="6">54.6 (54.0) 75.4 (74.5) 55.1 (54.1) 75.2 (74.3)</cell></row><row><cell>ERNIE</cell><cell cols="6">54.3 (53.9) 75.3 (74.6) 55.0 (53.9) 75.0 (73.9)</cell></row><row><cell>BERT-wwm</cell><cell cols="6">54.7 (54.0) 75.2 (74.8) 55.1 (54.1) 75.4 (74.4)</cell></row><row><cell>BERT-wwm-ext</cell><cell cols="6">55.6 (54.8) 76.0 (75.3) 55.6 (54.9) 75.8 (75.0)</cell></row><row><cell>RoBERTa-wwm-ext</cell><cell cols="6">58.7 (57.6) 79.1 (78.3) 59.0 (57.8) 79.0 (78.0)</cell></row><row><cell cols="7">RoBERTa-wwm-ext-large 62.1 (61.1) 82.4 (81.6) 62.4 (61.4) 82.2 (81.0)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Results on CJRC.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Results on XNLI.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Results on ChnSentiCorp.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Results on LCQMC and BQ Corpus.</figDesc><table><row><cell>THUCNews</cell><cell>Dev</cell><cell>Test</cell></row><row><cell>BERT</cell><cell cols="2">97.7 (97.4) 97.8 (97.6)</cell></row><row><cell>ERNIE</cell><cell cols="2">97.6 (97.3) 97.5 (97.3)</cell></row><row><cell>BERT-wwm</cell><cell cols="2">98.0 (97.6) 97.8 (97.6)</cell></row><row><cell>BERT-wwm-ext</cell><cell cols="2">97.7 (97.5) 97.7 (97.5)</cell></row><row><cell>RoBERTa-wwm-ext</cell><cell cols="2">98.3 (97.9) 97.8 (97.5)</cell></row><row><cell cols="3">RoBERTa-wwm-ext-large 98.3 (97.7) 97.8 (97.6)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 :</head><label>9</label><figDesc>Results on THUCNews.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">We release all the pre-trained models: https:// github.com/ymcui/Chinese-BERT-wwm</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">https://dumps.wikimedia.org/zhwiki/ latest/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2">http://ltp.ai</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3">For further tests and TensorFlow codes on LAMB optimizer, please refer to: https://github.com/ymcui/ LAMB_Optimizer_TF</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4">6 https://cloud.google.com/tpu</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5">/ 7 https://storage.googleapis.com/bert_ models/2018_11_03/chinese_L-12_H-768_ A-12.zip</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6">https://github.com/pengming617/bert_ classification</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_7">https://github.com/PaddlePaddle/LARK/ tree/develop/ERNIE</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_8">Not checked thoroughly, but we could not find some of the common Traditional Chinese characters.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_9">Useful TipsAs we can see, these pre-trained models behave differently in different natural language processing tasks. Due to the limited computing resources, 11 http://thuctc.thunlp.org 12 https://github.com/gaussic/ text-classification-cnn-rnn</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Yiming Cui would like to thank TensorFlow Research Cloud (TFRC) program for supporting this research.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Supplemental Material</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ltp: A chinese language technology platform</title>
		<author>
			<persName><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenghua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics: Demonstrations</title>
				<meeting>the 23rd International Conference on Computational Linguistics: Demonstrations</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="13" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The BQ corpus: A large-scale domain-specific Chinese corpus for sentence semantic equivalence identification</title>
		<author>
			<persName><forename type="first">Jing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingcai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haijun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daohe</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Buzhou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4946" to="4951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">QuAC: Question answering in context</title>
		<author>
			<persName><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">He</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wentau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2174" to="2184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Xnli: Evaluating crosslingual sentence representations</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruty</forename><surname>Rinott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Attention-overattention neural networks for reading comprehension</title>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhipeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoping</forename><surname>Hu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1055</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="593" to="602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A span-extraction dataset for chinese machine reading comprehension</title>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhipeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoping</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Transformer-xl: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Quoc V Le</surname></persName>
		</author>
		<author>
			<persName><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.02860</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Cjrc: A reliable human-annotated benchmark dataset for chinese judicial reading comprehension</title>
		<author>
			<persName><forename type="first">Baoxin</forename><surname>Xingyi Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dayong</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianxiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">China National Conference on Chinese Computational Linguistics</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="439" to="451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Natural questions: a benchmark for question answering research</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennimaria</forename><surname>Palomaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivia</forename><surname>Redfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danielle</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="453" to="466" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Race: Large-scale reading comprehension dataset from examinations</title>
		<author>
			<persName><forename type="first">Guokun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="796" to="805" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Scalable term selection for text categorization</title>
		<author>
			<persName><forename type="first">Jingyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
				<meeting>the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>EMNLP-CoNLL</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Lcqmc: A large-scale chinese question matching corpus</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingcai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huajun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongfang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Buzhou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
				<meeting>the 27th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1952" to="1962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Know what you don&apos;t know: Unanswerable questions for SQuAD</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="784" to="789" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1264</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Option comparison network for multiplechoice reading comprehension</title>
		<author>
			<persName><forename type="first">Qiu</forename><surname>Ran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiwei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.03033</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Coqa: A conversational question answering challenge</title>
		<author>
			<persName><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="249" to="266" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Drcd: a chinese machine reading comprehension dataset</title>
		<author>
			<persName><forename type="first">Chieh</forename><surname>Chih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trois</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiying</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName><surname>Tsai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.00920</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shikun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danxiang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Hao Tian</surname></persName>
		</author>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09223</idno>
		<title level="m">Ernie: Enhanced representation through knowledge integration</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Quoc V Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<title level="m">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Hseu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Demmel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.00962</idno>
		<title level="m">Reducing bert pre-training time from 3 days to 76 minutes</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Dual comatching network for multi-choice reading comprehension</title>
		<author>
			<persName><forename type="first">Shuailiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuosheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.09381</idno>
		<imprint>
			<date type="published" when="2019">2019a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">ERNIE: Enhanced language representation with informative entities</title>
		<author>
			<persName><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2019</title>
				<meeting>ACL 2019</meeting>
		<imprint>
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
