<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Read, Retrospect, Select: An MRC Framework to Short Text Entity Linking</title>
				<funder ref="#_3GBceFd">
					<orgName type="full">Key Development Program in Shaanxi Province of China</orgName>
				</funder>
				<funder ref="#_bTrR6Ut">
					<orgName type="full">National Key Research and Development Program of China</orgName>
				</funder>
				<funder ref="#_kzrBtGb">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yingjie</forename><surname>Gu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Electronics and Information Engineering</orgName>
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
								<address>
									<postCode>710049</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaoye</forename><surname>Qu</surname></persName>
							<email>quxiaoye@huawei.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Huawei Cloud &amp; AI</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhefeng</forename><surname>Wang</surname></persName>
							<email>wangzhefeng@huawei.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Huawei Cloud &amp; AI</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Baoxing</forename><surname>Huai</surname></persName>
							<email>huaibaoxing@huawei.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Huawei Cloud &amp; AI</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nicholas</forename><forename type="middle">Jing</forename><surname>Yuan</surname></persName>
							<email>nicholas.yuan@huawei.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Huawei Cloud &amp; AI</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaolin</forename><surname>Gui</surname></persName>
							<email>xlgui@mail.xjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Electronics and Information Engineering</orgName>
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
								<address>
									<postCode>710049</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Singer</roleName><forename type="first">Na</forename><surname>Li</surname></persName>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Mention History Cues Flow Australian Open (Golf) Cibulkova</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="laboratory">Li Na (Tennis) Dominika Cibulkova Dominika Cibulkova Australian Open (Tennis) Candidates Dominika Cibulkova Australian Open</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Read, Retrospect, Select: An MRC Framework to Short Text Entity Linking</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note type="submission">Li Na beat Cibulkova in Australian Open Query: Who beat Cibulkova in Australian Open ?</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Option: A</term>
					<term>Li Na (Singer) B</term>
					<term>Li Na (Tennis Player) C</term>
					<term>NIL</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Entity linking (EL) for the rapidly growing short text (e.g. search queries and news titles) is critical to industrial applications. Most existing approaches relying on adequate context for long text EL are not effective for the concise and sparse short text. In this paper, we propose a novel framework called Multi-turn Multiple-choice Machine reading comprehension (M3) to solve the short text EL from a new perspective: a query is generated for each ambiguous mention exploiting its surrounding context, and an option selection module is employed to identify the golden entity from candidates using the query. In this way, M3 framework sufficiently interacts limited context with candidate entities during the encoding process, as well as implicitly considers the dissimilarities inside the candidate bunch in the selection stage. In addition, we design a two-stage verifier incorporated into M3 to address the commonly existed unlinkable problem in short text. To further consider the topical coherence and interdependence among referred entities, M3 leverages a multi-turn fashion to deal with mentions in a sequence manner by retrospecting historical cues. Evaluation shows that our M3 framework achieves the state-of-the-art performance on five Chinese and English datasets for the real-world short text EL.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>The task of entity linking (EL), also known as entity disambiguation, aims to link a given mention that appears in a piece of text to the correct entity in a specific knowledge base (KB) <ref type="bibr" target="#b24">(Shen, Wang, and Han 2014)</ref>. Recently, with the explosive growth of short text in the web, entity linking for short text plays an important role in a wide range of industrial applications, such as parsing queries in search engines, understanding news titles and comments in social media (e.g. Twitter and Wechat).</p><p>In this paper, we focus on short texts entity linking problem. Under this circumstance, the input text is composed of few tens of terms and the surrounding context of mentions is naturally scarce and concise <ref type="bibr" target="#b7">(Ferragina and Scaiella 2010)</ref>. Traditional entity linking methods <ref type="bibr" target="#b9">(Gupta, Singh, and Roth 2017;</ref><ref type="bibr" target="#b18">Newman-Griffis, Lai, and Fosler-Lussier 2018)</ref> Figure <ref type="figure">1</ref>: An illustration of our M3 framework for short text entity linking. The global disambiguation order depends on the ambiguity degrees. mainly employ encoded context and pre-trained candidate entity embeddings to assess topic level context compatibility for entity disambiguation. In this way, the disambiguation process degrades into a semantic matching and entity ranking problem. Specifically, recent state-of-the-art entity linking models <ref type="bibr" target="#b8">(Gillick et al. 2019;</ref><ref type="bibr" target="#b17">Logeswaran et al. 2019)</ref> score each pair of mention context and candidate entity based on their abstract representation. This means that the model lacks fine-grained interaction between mention context and their candidate entities. Semantic matching operations between them are performed only at the encoder output layer and are relatively superficial. Therefore, it is difficult for these models to capture all the lexical, semantic, and syntactic relations for precise entity disambiguation. Although these approaches that benefited from sufficient context achieve significant progress in long text, they fail to process sparse short text due to the inferiority of a restricted context model. Thus, it would be favorable if an entity linking framework can make full use of limited context.</p><p>To alleviate above issue, we propose a Multi-turn Multichoice Machine reading comprehension (M3) framework to tackle the short text EL problem. As shown in Fig. <ref type="figure">1</ref>, to identify the ambiguous mention "Li Na" in the short text "Li Na beat Cibulkova in Australian Open", a query is generated using its surrounding context as below:</p><p>Who beat Cibulkova in Australian Open ?</p><p>Then, an option selection module is further employed to select the correct entity within the candidate bunch based on the given query. This formulation comes with the following key advantages: (1) Considering that the mention's immediate context is a proxy of its type <ref type="bibr" target="#b2">(Chen et al. 2020)</ref>, this form of query construction injects latent mention type information into the query embeddings. From the context "beat Cibulkova", it is feasible for the model to infer the potential entity type of mention is person. (2) In the option selection stage, query and candidates obtain token-by-token interaction, achieving lexical and syntax similarities compared to concatenating mention and candidate embedding during MRC encoding. (3) With the multi-choice setting, candidates are considered simultaneously, thus the comparison among candidates is comprehensive and the difference of scores among candidates is absolute, while the scores of different candidates are relative in both binary classification and ranking strategy. In Figure <ref type="figure">1</ref>, the dissimilarities among candidates ("Singer" vs "Tennis Player") implicitly attracts more attention and the shared part ("Li Na") between options is less highlighted compared to a binary classification strategy <ref type="bibr" target="#b3">(Cheng et al. 2019)</ref>.</p><p>In addition, short texts in web corpora are naturally timesensitive and it is possible that some mention does not have its corresponding entity in the given KB. In order to solve these unlinkable mentions which are labeled as NIL (a special token <ref type="bibr" target="#b24">(Shen, Wang, and Han 2014</ref>)), we further design an NIL verifier incorporated into the M3 framework. Specifically, we devise a two-stage verification strategy: 1) The first stage yields a preliminary decision by sketchily reading the query. 2) The second stage verifies the concrete option and returns the final prediction via intensive reading.</p><p>To further capture the relations among mentions for global disambiguation, we propose a multi-turn fashion to flexibly retrospect historical disambiguation cues in our M3 framework. As shown in Fig. <ref type="figure">1</ref>, when processing the mention "Li Na", the candidate entity "Li Na (Tennis Player)" shows strong relationship with last step referent entity "Australian Open (Tennis)". This sample conforms to our motivation of M3 that knowledge from previously linked entities can be accumulated as dynamic context to facilitate later decisions. To alleviate the error propagation along the knowledge pass, we devise a controllable gate mechanism to prefer the most relevant entities.</p><p>We conduct extensive experiments on both Chinese (Wechat and two out-domain test sets, CNDL Ex and Tencent News) and English (Webscope and an out-domain test KORE50) short text EL datasets. Our M3 framework achieves remarkable improvements of 5.62 percentage points on Wechat test set and an average of 5.03 percentage points on Webscope test set over five different runs com-pared with the state-of-the-art short text EL model <ref type="bibr" target="#b3">(Cheng et al. 2019)</ref>. In addition, we conduct detailed experimental analysis on Wechat dataset to show the effectiveness of our M3 framework.</p><p>Our contributions can be summarized as following: ? To the best of our knowledge, it is the first attempt that a novel multi-turn multiple-choice machine reading comprehension (M3) framework is proposed for short text EL. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work Short Text Entity Linking</head><p>The task of entity linking (EL) is commonly formalized as an entity ranking task according to local and global similarity scores <ref type="bibr" target="#b9">(Gupta, Singh, and Roth 2017;</ref><ref type="bibr" target="#b6">Eshel et al. 2017;</ref><ref type="bibr" target="#b1">Chen et al. 2018;</ref><ref type="bibr" target="#b8">Gillick et al. 2019)</ref>. Local score of candidates is computed by the representations interacted by context of mention and candidate entity. Global EL deals with simultaneous disambiguation for all mentions in the whole text and produces global scores. Finally, candidate entities are ranked via integrated local and global scores and the candidate with highest score will be selected as the entity.</p><p>There are a few previous works focused on short text entity linking. Ferragina and Scaiella (2010) designed a system for short text to solve ambiguity and polysemy in the anchorpage mappings of Wikipedia. <ref type="bibr" target="#b0">Blanco, Ottaviano, and Meij (2015)</ref> gave a solution to represent the entity as the centroid of word vectors of its relevant words. However, it makes the model hard to predict the true entity since relevant words of entities are noisy and representations by word vector are implicit. <ref type="bibr" target="#b27">Yang and Chang (2015)</ref> introduced a learning framework for short text EL, which combines non-linearity and efficiency of tree-based models with structured prediction. Afterwards, <ref type="bibr" target="#b1">Chen et al. (2018)</ref> proposed a method that regards concepts of entities as explicitly fine-grained topics to solve the sparsity and noisy problem of short text entity linking. More recently, <ref type="bibr" target="#b22">Sakor et al. (2019)</ref> proposed a tool to jointly solve the challenges of both entity and relation linking, while it is specifically designed for English dataset and not suitable for Chinese. <ref type="bibr" target="#b3">Cheng et al. (2019)</ref> treated short text entity linking problem as a binary classification task with BERT encoder <ref type="bibr" target="#b5">(Devlin et al. 2018)</ref>, which shows significant results and takes the first place in CCKS 2019 challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Machine Reading Comprehension (MRC)</head><p>Machine reading comprehenison (MRC) can be roughly categorized by cloze <ref type="bibr" target="#b11">(Hill et al. 2015)</ref>, multiple-choice <ref type="bibr" target="#b14">(Lai et al. 2017;</ref><ref type="bibr" target="#b4">Clark et al. 2018)</ref>, span-extraction <ref type="bibr" target="#b21">(Rajpurkar, Jia, and Liang 2018)</ref> and generation <ref type="bibr" target="#b19">(Nguyen et al. 2016</ref>) </p><formula xml:id="formula_0">G BERT + G BERT + 1 i h ? i h 1 i v ? i v Turn i-1 Turn i 1 ?i Q ? ?i Q ?i e 1 ?i e ?</formula><formula xml:id="formula_1">m 1 1 ?i v ? ?i v m 2 m 3 1 Q 1 D 1 Q 1 O First Stage Second Stage A B C</formula><p>Who beat Cibulkova in Australian Open? (i according to the answer types. Over the last few years, many tasks in natural language processing have been framed as reading comprehension while abstracting away the taskspecific modeling constraints. For example, <ref type="bibr" target="#b10">He, Lewis, and Zettlemoyer (2015)</ref> showed that semantic role labeling tasks could be solicited by using question-answer pairs to represent the predicate-argument structure. In recent work, <ref type="bibr" target="#b16">Li et al. (2019)</ref> reformalized the NER task as an MRC question answering task, which is efficiently capable of addressing overlapping entity recognition problems through constructing the query contained prior knowledge. <ref type="bibr" target="#b25">Wu et al. (2019)</ref> treated the co-reference resolution problem as an MRC task.</p><formula xml:id="formula_2">) (ii) (iv) (iii) 1 ?i O ? ?i O</formula><p>The passage is split into sentences where marked with the proposal mentions and the objective is to find the answer set (i.e. co-reference clusters) from the whole passage according to the current question (i.e. sentence).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Our M3 framework for short text entity disambiguation mainly consists of two modules: Local Model with Multiplechoice MRC and NIL Verifier, and Global Model with Multi-turn MRC. The structure of M3 is shown in Fig. <ref type="figure" target="#fig_2">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task Formalization and Candidates Generation</head><p>Formally, given a short text S containing a set of identified mentions M = {m 1 , m 2 , ..., m n }. The goal of an entity linking system is to find a mapping that links each mention m i to a target entity e i which is an unambiguous page in a referent Knowledge Base (e.g. Baidu Baike for Chinese and Wikidata for English) or predict that there is no corresponding entity to current mention in the KB (i.e. e i =NIL). Before entity disambiguation, for each mention m i , potential candidate entities O i ? {e 1 i , ..., e K i } are first chosen by candidate generation from a specific KB, where K is a pre-defined parameter to prune the candidate set. It is worth noting that each candidate O j i ? O i possesses one corresponding description D j i (as shown in Figure <ref type="figure" target="#fig_2">2</ref>) in KB which serves as supporting descriptions. Following previous works <ref type="bibr" target="#b6">(Fang et al. 2019;</ref><ref type="bibr" target="#b15">Le and Titov 2019)</ref>, we adopt the surface matching method to generate candidate entities for each mention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Local Model</head><p>We formalize the local entity disambiguation as a multiplechoice MRC paradigm which consists of query construction and option selection module. Moreover, a NIL verifier is designed to facilitate NIL decision. In detail, to construct the query, we leverage pre-trained model BERT as our backbone. It is worth noting that the mention m i is replaced with a single [MASK] token instead of an explicit question word to construct the query Q i in the actual scene. For example, when identifying the mention "Cibulkova" in "Li Na beat Cibulkova in Australian Open". The query is described as below:</p><p>Li Na beat [MASK] in Australian Open.</p><p>As the explicit type information is not always available in the KB (for example Baidu-Baike in our paper), latent en-tity type (i.e. who) can be perceived by reading the context around the [MASK] token <ref type="bibr" target="#b2">(Chen et al. 2020</ref>). Here we do not further incorporate the mention name into the query as it makes negligible performance difference. Then, in the option selection process, for each option O j i of mention m i , we further concatenate D j i , Q i , and O j i with [CLS] and [SEP] tokens as the input sequence:</p><formula xml:id="formula_3">S j i = {[CLS] D j i [SEP] Q i [SEP] O j i [SEP]}<label>(1)</label></formula><p>The target of option selection module is to select the correct answer (i.e. ground-truth entity) from the available options by making full use of their supporting knowledge. For a query with K answer options, we first obtain K input queries: S 1 i , S 2 i , ..., S K i . Afterwards, we feed each query into BERT encoder and the final prediction for each option is obtained by a feed-forward layer with softmax function over the uppermost layer representation of BERT.</p><formula xml:id="formula_4">H j i = BERT(S j i ), j ? {1, 2, ..., K}<label>(2)</label></formula><formula xml:id="formula_5">?i = Softmax(W T 1 H i + b 1 ) (3) where H i ? R d?K , W 1 ? R d and b 1 ? R K are</formula><p>the learnable parameter and bias respectively. The training loss function of answer entity prediction is defined as cross entropy.</p><formula xml:id="formula_6">L ans = - 1 N N i=1 K j=1 y j i log(? j i )<label>(4)</label></formula><p>where ?j i denotes the prediction of each answer and y j i is the gold entity of m i . N is the number of examples. NIL Verifier Considering the frequently appeared unlinkable problem in short text, following a natural practice of how humans solve linkable mention: the first step is to read through the query and obtain an initial judgement; then, people re-read the query and verify the answer if not so sure, we propose a two-stage verification mechanism. In the first stage, the preliminary judgment is determined by sketchy reading the query Q i . In specific,</p><formula xml:id="formula_7">?i = ?(MLP(BERT(Q i ))) (5)</formula><p>here ? is the sigmoid function, MLP denotes a multi-layer perception. Then we adopt binary cross entropy loss to train this classifier.</p><formula xml:id="formula_8">L nil = - 1 N N i=1 [y i log(? i ) + (1 -y i )log(1 -?i )] (6)</formula><p>where ?i denotes the prediction and y i is the target indicating whether mention m i is linkable or not. N is the number of training examples. Then, in the second stage, an additional option marked as "NIL" (the corresponding description is "This is a NIL option") is appended to candidate entity set in the above local multi-choice model, which is capable to participate in the comparison between options. We argue the first stage is crucial because if only the second stage exists, the model will incline to the linkable options which share more components with the query. Finally, the joint loss for our local model incorporated with NIL verifier is the weighted sum of the answer loss and NIL loss.</p><formula xml:id="formula_9">L local = ? 1 L ans + ? 2 L nil (7)</formula><p>where ? 1 and ? 2 are two hyper-parameters that balance the weight of two losses.</p><p>After local disambiguation, we obtain all candidate entity representations for each mention in the short text, which will be used for following global disambiguation. Meanwhile, the local scores calculated by Eq. 3 will be utilized for sequence ranking in the next stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Global Model</head><p>In this section, we aim to capture topical coherence and interdependency among all mentions for global disambiguation. To achieve this goal, we treat the global disambiguation as a multi-turn fashion extending from above multiplechoice paradigm. The intuitive idea is to utilize previously linked entities to enhance the later decisions through a dynamic multi-turn way. Before global disambiguation, according to the study <ref type="bibr" target="#b26">Yamada et al. (2020)</ref>, starting with mentions that are easier to disambiguate will be effective to reduce the interference of noise data. In our full M3 framework, we rank mentions via their ambiguity degrees produced by the local model with below rules:</p><formula xml:id="formula_10">{ m1 , m2 , ..., mn } = Rank{ max j,k?K ?j i -?k i L1 }<label>(8)</label></formula><p>After sorting mentions, the representation of entity with highest score with respect to mention m1 is set to h 1 , forming the initial history cue. Then, to obtain the global entity scores for following mention mi , we first utilize the linked entities to update current query Qi , which is capable of accumulating previous knowledge in context level. For example, the query for identifying "Australian Open" will be updated with the previously linked entities as following ("Cibulkova" has been linked to "Dominika Cibulkova"):</p><p>Li Na beat Dominika Cibulkova in <ref type="bibr">[MASK]</ref>. Then the updated query and each candidate entity of current mention are concatenated with special tokens [CLS] and [SEP] as the input sequence. Similar to the local model, we leverage the BERT encoder to obtain the representations of each candidate for current mention:</p><formula xml:id="formula_11">v j i = BERT{[CLS] Dj i [SEP] Qi [SEP] ?j i [SEP]}<label>(</label></formula><p>9) Subsequently, we propose to introduce historical cues for current mention disambiguation. However, some previously linked entities may be irrelevant to the current mention and several falsely linked entities may even lead to noise. For this purpose, a gate mechanism is desired to control which part of history cues should be inherited. In specific, a gated network is designed on the current and historical representations h i-1 as follows: </p><formula xml:id="formula_12">u j i = ?(W u [v j i ; h i-1 ])<label>(10)</label></formula><formula xml:id="formula_13">f j i = tanh(W f [u j i h i-1 ; v j i ])<label>(11)</label></formula><formula xml:id="formula_14">g j i = ?(W i v j i + W h h i-1 )<label>(12</label></formula><formula xml:id="formula_15">vj i = g j i f j i + (1 -g j i ) h i-1<label>(13)</label></formula><p>where W u , W f ? R d?2d , W i , W h ? R d?d are learnable parameters. f j i denotes the fusion of history information and current candidate input, and g j i is the control gate to determine how much history information will be remained for each candidate entity. Finally, vj i will be output to calculate the global scores of candidates for the current mention.</p><formula xml:id="formula_16">i = Softmax(W T 2 vj i + b 2 ) (14)</formula><p>where W 2 ? R d and b 2 ? R are the learnable parameter and bias respectively. After prediction, the representation of selected entity ?i with highest global score ?j i for current mention mi is flowed to next step of global disambiguation as updated historical representation. The training loss function for global model is defined as cross entropy.</p><formula xml:id="formula_17">L global = - 1 N N i=1 K j=1 y j i log(? j i )<label>(15)</label></formula><p>where ?j i denotes the global prediction of answer and y j i is the ground-truth entity of m i . N is the number of examples. Rear Fusion Rear fusion is the combination of predicted scores of local model and global model, determining the final disambiguated entity list for each mention in short text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Score(e</head><formula xml:id="formula_18">j i ) = ? ?j i + (1 -?)? j i (<label>16</label></formula><formula xml:id="formula_19">)</formula><p>where ? is the weight to balance local and global score.</p><p>The details of our model are presented in Algorithm 1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments Datasets</head><p>In order to verify the effectiveness of our framework, we conduct experiments on five Chinese and English datasets considering both in-domain and out-domain settings. Since most of the existing datasets on EL are based on long text, which are not suitable for the task of short text EL. We find two public English datasets: Webscope <ref type="bibr" target="#b0">(Blanco, Ottaviano, and Meij 2015)</ref> and KORE50 <ref type="bibr" target="#b12">(Hoffart et al. 2012</ref>) that are suitable for short text EL. However, there are few existing datasets for Chinese short text EL with high quality annotation. Therefore, we construct two Chinese datasets (Wechat and Tencent News) for short text EL and expand one public dataset CNDL <ref type="bibr" target="#b1">(Chen et al. 2018)</ref> to CNDL Ex. Due to the limited space, the detailed construction process and short text samples will be available in the supplementary material. The statistics of these datasets are shown in Table <ref type="table" target="#tab_2">1</ref>. To make a fair comparison, we train all models on Webscope for English and test on Webscope and KORE50. For Chinese datasets, we train on Wechat dataset and test them on the Wechat, CNDL Ex, and Tencent News dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment Setup</head><p>In this paper, our goal is to demonstrate the superiority of M3 framework for short text entity linking. As a full EL system in real-world includes mention detection, candidate generation, and entity disambiguation. Here we focus on disambiguation method and fairly comparing with previous methods under same preliminary condition, i.e. same candidate generation strategy. In detail, We adopt alias dictionary look-up to search the candidates from the Baidu-Baike (Baidu Baike on Feb. 2019) on Chinese datasets and rank them by their page-view in the knowledge base. For English dataset, we adopt surface matching methods to match Wikipedia page (Wikipedia Dump on May. 2020) and calculate the prior probability between mention and candidate description for ranking. Considering the NIL problem, we retain top K candidates, and then K candidates and NIL token serve as candidate sets for each mention.</p><p>In our experiment, we leverage the pre-trained uncased BERT-Base model with 768 dimensions hidden representation as our backbone. For local model, we adopt Adam as optimizer with warmup rate 0.1, initial learning rate 5e-6, and maximum sequence length 256. The hyper-parameter ? 1 , ? 2 are set to 0.75, 0.25. For the global encoder, we use Adam optimizer with a learning rate of 1e-5 and maximum of 512 tokens after tokenization. For each mention, the candidates' number K is set to 5. In rear fusion, ? is 0.5. All ex- periments are performed on one Tesla P100 with 16G GPU memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison Methods</head><p>In our experiments, we compare our proposed model with the following baseline methods: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>We present the entity linking evaluation results in Table <ref type="table" target="#tab_3">2</ref>.</p><p>From Table <ref type="table" target="#tab_3">2</ref> we can observe that compared to the strong </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study</head><p>To better evaluate the contribution of various components to the overall performance, we conduct abundant ablation studies of M3 framework on Wechat dataset in  the results, we can observe that: (1) When removing the NIL verifier the performance drops by 1.16, which indicates that our NIL verifier can effectively tackle NIL problem. (2) When linking mentions by the natural order of appearance in the text instead of re-ranking mention with Eq. 8, the result becomes worse and it means re-ranking helps improve linking ability. (3) If we do not update the query at each turn with previously linked entities, the performance will drop by 0.49, revealing that prior linked knowledge plays a significant role in global model. (4) Replacing our devised gate mechanism with a simple concatenate operation or vanilla GRU structure both degrades the performance, which denotes that our gate mechanism is more efficient to filter noisy entity information. (5) When only utilizing last step entity information as the history cue, the accuracy drops by 0.44, which shows that our history flow among all mentions contributes more to topical coherence capturing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis</head><p>We demonstrate the effectiveness of our proposed model from the following aspects:  <ref type="table" target="#tab_7">5</ref>.</p><p>According to this table, the multi-turn strategy effectively corrects several coherence errors with the help of historical information of linked entities. For example, when linking "Protoss" and "Terrans", the collective inference with linked history cue "Zerg (StarCraft)" promotes our global model to select an entity with highest topical coherence. structure obtains about 1.68% average accuracy improvement by using the history cues. Different from in-domain dataset, the best improvement 2.6% has been achieved by our multi-turn model on the text with 2 mentions. From both in-domain and out-domain settings, our global model with multi-turn strategy shows significant generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generalization of Multi-turn Strategy</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this article, we presents a novel Multi-turn Multiplechoice MRC (M3) framework for short text EL. Firstly, we build query construction and option selection module for local disambiguation with an auxiliary NIL verifier for handling unlinkable entity problem. Then we leverage a multiturn way with historical cues flow to tackle global topical coherence problem among mentions. The experiment results have proved that our M3 framework achieves the state-ofthe-art performance on five Chinese and English short text datasets for real-world applications. In fact, our M3 framework can integrate more types of information if it is available in the KB, such as relations between entities or explicit entity type information. Due to the restriction of the KB, we leave it as a future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Na, is a Chinese... B. Li Na, born July... C. It is a NIL option.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of our proposed M3 framework for short text entity linking. (i) The input short text first forms multi-choice MRC with options from candidate entities and corresponding descriptions from KB for each mention. (ii) Then a NIL verifier to initially determine whether the mention is linkable jointly works with multi-choice MRC to conduct local prediction. (iii) Subsequently, all mentions are re-ranked for global disambiguation inputting sequence based on the local prediction confidence, i.e. the global decision order is m1 (m 2 ) ? m2 (m 3 ) ? m3 (m 1 ). (iv) Finally, a multi-turn module is devised to performed global disambiguation with updated query and entity information flow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(1)<ref type="bibr" target="#b1">Chen et al. (2018)</ref> provided a mention-entity prior statistical estimation from the entity description in knowledge base. (2)<ref type="bibr" target="#b6">Eshel et al. (2017)</ref> proposed to use a modified GRU to encode left and right context of a mention, which is capable of handling the EL problem with noisy local context. (3)<ref type="bibr" target="#b13">Kolitsas, Ganea, and Hofmann (2018)</ref> used bidirectional LSTM networks on the top of learnable char embeddings and represent entity mention as a combination of LSTM hidden states included in the mention spans. (4)<ref type="bibr" target="#b23">Shahbazi et al. (2019)</ref> first proposed a method to learn an entity-aware extension of pre-trained ELMo<ref type="bibr" target="#b20">(Peters et al. 2018</ref>) and obtains significant improvements in many long text EL tasks. (5)<ref type="bibr" target="#b2">Chen et al. (2020)</ref> proposed to improve EL performance via capturing latent entity type information with BERT. This model is able to correct most of the type errors and obtains the state-of-the-art performance on lots of long text EL datasets. (6)<ref type="bibr" target="#b3">Cheng et al. (2019)</ref> proposed a strong method to handle Chinese short text EL and achieve first place in CCKS 2019 short text EL track. They treat the short text EL as a binary classification task and leverage BERT as a backbone to obtain deeply interactive representations between the mention context and candidate entities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Results of global model for different number of mentions. Left: Wechat dataset. Right: CNDL EX dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>)</head><label></label><figDesc>Algorithm 1: M3 framework for Short Text EL Input: Short text S with M = {m1, ..., mn}, candidates Om and descriptions Dm for each mention m Output: Linked entities E = {e1, ..., en} for all mentions for m in M do Build Multi-choice MRC paradigm [Dm, Qm, Om] ; Compute Lans for all options with Eq.(1 ? 4); Qi ; Build Multi-turn MRC paradigm [D mi , Qi, O mi ]; Obtain vi for all options with Eq.(9); Input vi, hi-1 to Gated Network, and select the target entity ?i for mi by Eq.(10 ? 14); Update the history vector v ?i ? hi; Update parameters with loss L global by Eq.(15); end end</figDesc><table><row><cell cols="2">Compute L nil by NIL Verifier with Eq.(5 ? 6);</cell></row><row><cell>L local = ?1Lans + ?2L nil ;</cell><cell></cell></row><row><cell cols="2">Update parameters with joint loss L local ;</cell></row><row><cell>end</cell><cell></cell></row><row><cell cols="2">Re-rank mention M = { m1, ..., mn} with Eq.(8);</cell></row><row><cell>for i in 1 to n do</cell><cell></cell></row><row><cell>if i == 1 then</cell><cell></cell></row><row><cell>Select ?1 in m1 by local score;</cell><cell></cell></row><row><cell cols="2">Obtain initial history vector v ?1 as h1;</cell></row><row><cell>else</cell><cell></cell></row><row><cell>Update Query by linked entity Qi-1</cell><cell>?i-1 -?</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Statistics of short text EL datasets. % NIL is the percentage of NIL mentions in all mentions.</figDesc><table><row><cell>Dataset</cell><cell>Text</cell><cell cols="3">Language Avg./men. % NIL</cell></row><row><cell>Wechat</cell><cell>11,439</cell><cell>Cn</cell><cell>2.00</cell><cell>14.34%</cell></row><row><cell>CNDL Ex</cell><cell>877</cell><cell>Cn</cell><cell>2.03</cell><cell>16.50%</cell></row><row><cell>Tencent News</cell><cell>1,000</cell><cell>Cn</cell><cell>1.73</cell><cell>9.71%</cell></row><row><cell>Webscope</cell><cell>2,635</cell><cell>En</cell><cell>2.26</cell><cell>-</cell></row><row><cell>KORE50</cell><cell>50</cell><cell>En</cell><cell>2.88</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Evaluation on both Chinese and English datasets. ? indicates methods specifically designed for short texts. * denotes datasets only for transfer evaluation. M3 (base) is a variant of M3 (local) model without NIL Verifier. M3 (full) consists of both local and global model. On the English datasets, M3 (base) is equal to M3 (local) as there are no NIL mentions within them.</figDesc><table><row><cell>Model</cell><cell cols="3">Wechat</cell><cell cols="3">CNDL Ex* Tencent News* Avg Cn</cell><cell>Webscope</cell><cell>KORE50*</cell><cell>Avg En.</cell></row><row><cell cols="2">p(e|m) (Chen et al. 2018)</cell><cell>60.68</cell><cell></cell><cell>61.24</cell><cell>60.92</cell><cell>60.95</cell><cell>51.71</cell><cell>50.00</cell><cell>50.86</cell></row><row><cell>Eshel et al. (2017)</cell><cell></cell><cell>77.68</cell><cell></cell><cell>75.59</cell><cell>79.94</cell><cell>77.74</cell><cell>84.58</cell><cell>46.03</cell><cell>65.31</cell></row><row><cell cols="2">Kolitsas, Ganea, and Hofmann (2018)</cell><cell>78.18</cell><cell></cell><cell>74.97</cell><cell>79.48</cell><cell>77.54</cell><cell>87.08</cell><cell>53.17</cell><cell>70.13</cell></row><row><cell cols="2">Shahbazi et al. (2019)</cell><cell>78.26</cell><cell></cell><cell>75.37</cell><cell>79.19</cell><cell>77.61</cell><cell>82.22</cell><cell>57.94</cell><cell>70.08</cell></row><row><cell>Chen et al. (2020)</cell><cell></cell><cell>81.17</cell><cell></cell><cell>78.09</cell><cell>82.77</cell><cell>80.68</cell><cell>84.17</cell><cell>63.49</cell><cell>73.83</cell></row><row><cell cols="2">Cheng et al. (2019) ?</cell><cell>89.20</cell><cell></cell><cell>89.15</cell><cell>88.32</cell><cell>88.89</cell><cell>87.45</cell><cell>71.58</cell><cell>79.52</cell></row><row><cell>M3 (base) ?</cell><cell cols="4">93.07 ? 0.4 91.75 ? 0.5</cell><cell>89.27 ? 0.5</cell><cell>91.36</cell><cell cols="2">89.24 ? 0.5 71.42 ? 0.9</cell><cell>80.33</cell></row><row><cell>M3 (local) ?</cell><cell cols="4">94.06 ? 0.2 93.37 ? 0.4</cell><cell>89.86 ? 0.5</cell><cell>92.43</cell><cell cols="2">89.24 ? 0.5 71.42 ? 0.9</cell><cell>80.33</cell></row><row><cell>M3 (full) ?</cell><cell cols="4">94.82 ? 0.1 94.33 ? 0.2</cell><cell>90.57 ? 0.4</cell><cell>93.24</cell><cell cols="2">92.48 ? 0.5 74.28 ? 0.5</cell><cell>83.38</cell></row><row><cell>Components</cell><cell>Module</cell><cell></cell><cell cols="2">Accuracy Total ?</cell><cell></cell><cell></cell><cell></cell></row><row><cell>All</cell><cell>M3 (full)</cell><cell></cell><cell>94.92</cell><cell>-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>NIL Verifier</cell><cell>without NIL Verifier</cell><cell></cell><cell cols="2">93.76 1.16</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Decision Order</cell><cell cols="4">without mention re-ranking 94.68 0.24</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Query update</cell><cell>without query update</cell><cell></cell><cell cols="2">94.43 0.49</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Gate Mechanism</cell><cell cols="2">Replace with Concatenate Replace with GRU</cell><cell cols="2">94.56 0.36 94.64 0.28</cell><cell></cell><cell></cell><cell></cell></row><row><cell>History Flow</cell><cell>Replace with last history</cell><cell></cell><cell cols="2">94.48 0.44</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Ablation study of M3 framework on Wechat dataset.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Examples that wrongly predicted by binary model<ref type="bibr" target="#b3">Cheng et al. (2019)</ref> but correctly predicted by M3 (base).</figDesc><table><row><cell>Text 1</cell><cell cols="2">Remake from the 2005 Japanese TV series</cell></row><row><cell></cell><cell cols="2">"Queen's classroom".</cell></row><row><cell>Candidates</cell><cell cols="2">(1) Queen's classroom (Japanese TV) (2) Queen's classroom (Korea TV)</cell></row><row><cell>Binary</cell><cell cols="2">(1) 0.725 (2) 0.728</cell></row><row><cell>M3 (base)</cell><cell>(1) 0.82</cell><cell>(2) 0.18</cell></row><row><cell>Text 2</cell><cell cols="2">Three episodes of documentary "Masters In</cell></row><row><cell></cell><cell cols="2">Forbidden City" was popular all over country.</cell></row><row><cell>Candidates</cell><cell cols="2">(1) Masters In Forbidden City (Documentary) (2) Masters In Forbidden City (Movie)</cell></row><row><cell>Binary</cell><cell cols="2">(1) 0.951 (2) 0.953</cell></row><row><cell>M3 (base)</cell><cell>(1) 0.69</cell><cell>(2) 0.30</cell></row><row><cell cols="3">baseline approaches based on the same BERT-based en-</cell></row><row><cell cols="3">coder, our M3 framework exhibits the state-of-the-art per-</cell></row><row><cell cols="3">formance on both Chinese and English short text datasets.</cell></row><row><cell cols="3">On the Wechat datasets, our M3 (local) achieves 4.86 per-</cell></row><row><cell cols="3">centage absolute improvement in terms of accuracy over</cell></row><row><cell cols="3">the strong method (Cheng et al. 2019) specifically designed</cell></row><row><cell cols="3">for short text. Equipped with global module, the average</cell></row><row><cell cols="3">performance of our model M3 (full) further increases to</cell></row><row><cell cols="3">94.82, indicating that our design successfully tackles men-</cell></row><row><cell cols="3">tion coherence problems. On the Webscope dataset, our M3</cell></row><row><cell cols="3">(full) model also obtains incredible improvement compared</cell></row><row><cell cols="3">to baseline EL approaches (Cheng et al. 2019; Chen et al.</cell></row><row><cell cols="3">2020) by 5.03, 8.31 respectively. In addition, Table 2 shows</cell></row><row><cell cols="3">the performance on two Chinese out-domain and one En-</cell></row><row><cell cols="3">glish out-domain datasets. On CNDL Ex and Tencent News</cell></row><row><cell cols="3">datasets, our model presents consistent performance im-</cell></row><row><cell cols="3">provement by 5.18 and 2.25 percentage in terms of accu-</cell></row><row><cell cols="3">racy compared to SOTA method (Cheng et al. 2019). On</cell></row><row><cell cols="3">KORE50 dataset, the baseline (Cheng et al. 2019) performs</cell></row><row><cell cols="3">slightly better than our M3 (local) model by 0.16 but far</cell></row><row><cell cols="3">worse than our M3 (full) model by 2.7 percentage. The per-</cell></row><row><cell cols="3">formance on all these out-domain datasets reveals the ro-</cell></row><row><cell cols="2">bustness of our model.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 .</head><label>3</label><figDesc>FromShort text (Wechat): According to the different ethnic characteristics of Terrans, Zerg and Protoss, mining, replenishing soldier, and making all-round strategic decisions are carried out; after all, they have no divine shield skill of Paladins in World of Warcraft.</figDesc><table><row><cell cols="2">Turn Mention</cell><cell>Candidates</cell><cell cols="4">Local Global Final Golden Entity</cell></row><row><cell>1</cell><cell cols="2">World of Warcraft World of Warcraft</cell><cell>0.99</cell><cell>-</cell><cell>-</cell><cell>World of Warcraft</cell></row><row><cell>2</cell><cell>Zerg</cell><cell>(1) Zerg (StarshipTroopers) (2) Zerg (StarCraft)</cell><cell>0.08 0.91</cell><cell>0.08 0.89</cell><cell cols="2">0.08 Zerg (StarCraft) 0.90</cell></row><row><cell>3</cell><cell>Protoss</cell><cell>(1) Protoss (StarCraft) (2) Protoss (Slayers)</cell><cell>0.28 0.63</cell><cell>0.59 0.08</cell><cell cols="2">0.44 Protoss (StarCraft) 0.36</cell></row><row><cell>4</cell><cell>Paladin</cell><cell>(1) Paladin (Dungeon &amp; Fighter) (2) Paladin (World of Warcraft)</cell><cell>0.07 0.54</cell><cell>0.01 0.97</cell><cell cols="2">0.04 Paladin (World of Warcraft) 0.76</cell></row><row><cell></cell><cell></cell><cell>(1) Terrans (Warcraft)</cell><cell>0.30</cell><cell>0.17</cell><cell>0.24</cell><cell></cell></row><row><cell>5</cell><cell>Terrans</cell><cell>(2) Terrans (StarCraft)</cell><cell>0.21</cell><cell>0.47</cell><cell>0.34</cell><cell>Terrans (StarCraft)</cell></row><row><cell></cell><cell></cell><cell>(3) Terrans (Biological category)</cell><cell>0.33</cell><cell>0.20</cell><cell>0.27</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>An example of our global model with multi-turn fashion to handle topical coherence in a short text with five mentions. The bold item is mistakenly predicted by local model but corrected by global model.</figDesc><table><row><cell>Text 1</cell><cell>The neighbor called the police for Bonnie,</cell></row><row><cell></cell><cell>but the man pulled out a fruit knife that</cell></row><row><cell></cell><cell>had been prepared for a long time.</cell></row><row><cell cols="2">M3 (base) Yu Hanmi (Actress)</cell></row><row><cell cols="2">M3 (local) NIL</cell></row><row><cell>Text 2</cell><cell>Jiang Shan, who knocked in, stood in</cell></row><row><cell></cell><cell>front of the principal's desk, who was sit-</cell></row><row><cell></cell><cell>ting there waiting for him.</cell></row><row><cell cols="2">M3 (base) Jiang Shan City (Zhejiang Province)</cell></row><row><cell cols="2">M3 (local) NIL</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Examples of NIL Verifier.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Effectiveness of Multiple-choice ParadigmIn Table2, we present the results of our M3 (base) and<ref type="bibr" target="#b3">(Cheng et al. 2019</ref>) which treat the EL as a binary classification. From the result, it is obvious that our multi-choice strategy performs significantly better than binary classification in this task. In addition, we show some typical cases in Table4where the two candidate entities with similar descriptions are highly ambiguous. In this scenario,<ref type="bibr" target="#b3">(Cheng et al. 2019</ref>) which independently assigns scores to each candidate can barely distinguish the entities. Nevertheless, our M3 (base) successfully captures the micro-difference between candidates in the lexical-level and recognizes the true entity, which denotes that the dissimilarities among candidates attract more attention with our multiple-choice setting.Effectiveness of NIL Verifier As shown in Table 2 (M3 (base) vs M3 (local)) and Table3(NIL Verifier), NIL Verifier presents significant improvements in both scenarios. Moreover, we provide qualitative analyses to highlight the importance of NIL Verifier in Table6. As shown in this table, when removing NIL verifier, the model tends to link the mention to intrinsic entities in the KB.Topical Coherence Correction As shown inTable 2, M3 (full) achieves better results than M3 (local) with global model, especially on English datasets. To have an intuitive observation of the concrete process of global model, we also provide a prediction example on Wechat dataset in Table</figDesc><table><row><cell>? Does the multiple-choice paradigm in M3 framework bet-</cell></row><row><cell>ter interact limited context with entities than binary clas-</cell></row><row><cell>sification (Cheng et al. 2019)?</cell></row><row><cell>? Does NIL Verifier facilitate discriminating NIL entities?</cell></row><row><cell>? Can global model correct errors occurred in the local</cell></row><row><cell>model with resort to topical coherence among mentions?</cell></row><row><cell>? Can multi-turn strategy boost short texts entity linking</cell></row><row><cell>with different number of mentions?</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This paper was partially supported by <rs type="funder">National Key Research and Development Program of China</rs> (<rs type="grantNumber">2018YFB1800304</rs>); <rs type="funder">Key Development Program in Shaanxi Province of China</rs> (<rs type="grantNumber">2019GY-005</rs>, <rs type="grantNumber">2017ZDXM-GY-011</rs>)</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_bTrR6Ut">
					<idno type="grant-number">2018YFB1800304</idno>
				</org>
				<org type="funding" xml:id="_3GBceFd">
					<idno type="grant-number">2019GY-005</idno>
				</org>
				<org type="funding" xml:id="_kzrBtGb">
					<idno type="grant-number">2017ZDXM-GY-011</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fast and space-efficient entity linking for queries</title>
		<author>
			<persName><forename type="first">R</forename><surname>Blanco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ottaviano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Meij</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Eighth ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="179" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Short text entity linking with fine-grained topics</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 27th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="457" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.01447</idno>
		<title level="m">Improving entity linking by modeling latent entity type information</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
			<affiliation>
				<orgName type="collaboration">Entity Linking for Chinese Short Texts Based on BERT and Entity Name Embeddings. China Conference on Knowledge Graph and Semantic Computing</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pan</surname></persName>
			<affiliation>
				<orgName type="collaboration">Entity Linking for Chinese Short Texts Based on BERT and Entity Name Embeddings. China Conference on Knowledge Graph and Semantic Computing</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dang</surname></persName>
			<affiliation>
				<orgName type="collaboration">Entity Linking for Chinese Short Texts Based on BERT and Entity Name Embeddings. China Conference on Knowledge Graph and Semantic Computing</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
			<affiliation>
				<orgName type="collaboration">Entity Linking for Chinese Short Texts Based on BERT and Entity Name Embeddings. China Conference on Knowledge Graph and Semantic Computing</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
			<affiliation>
				<orgName type="collaboration">Entity Linking for Chinese Short Texts Based on BERT and Entity Name Embeddings. China Conference on Knowledge Graph and Semantic Computing</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
			<affiliation>
				<orgName type="collaboration">Entity Linking for Chinese Short Texts Based on BERT and Entity Name Embeddings. China Conference on Knowledge Graph and Semantic Computing</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
			<affiliation>
				<orgName type="collaboration">Entity Linking for Chinese Short Texts Based on BERT and Entity Name Embeddings. China Conference on Knowledge Graph and Semantic Computing</orgName>
			</affiliation>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Cowhey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schoenick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Tafjord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.05457</idno>
		<title level="m">Think you have solved question answering? try arc, the ai2 reasoning challenge</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<meeting><address><addrLine>Bert</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Joint entity linking with deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Eshel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Radinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Markovitch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Conference on Computational Natural Language Learning</title>
		<meeting>the 21st Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017. CoNLL 2017. 2019</date>
			<biblScope unit="page" from="438" to="447" />
		</imprint>
	</monogr>
	<note>The World Wide Web Conference</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Tagme: on-the-fly annotation of short text fragments (by wikipedia entities)</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ferragina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Scaiella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM international conference on Information and knowledge management</title>
		<meeting>the 19th ACM international conference on Information and knowledge management</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1625" to="1628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning Dense Representations for Entity Retrieval</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lansing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Presta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Baldridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Garcia-Olano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)</title>
		<meeting>the 23rd Conference on Computational Natural Language Learning (CoNLL)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="528" to="537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Entity Linking via Joint Encoding of Types, Descriptions, and Context</title>
		<author>
			<persName><forename type="first">N</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2681" to="2690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Questionanswer driven semantic role labeling: Using natural language to annotate natural language</title>
		<author>
			<persName><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 conference on empirical methods in natural language processing</title>
		<meeting>the 2015 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="643" to="653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.02301</idno>
		<title level="m">The goldilocks principle: Reading children&apos;s books with explicit memory representations</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">KORE: keyphrase overlap relatedness for entity disambiguation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Seufert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Theobald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st ACM international conference on Information and knowledge management</title>
		<meeting>the 21st ACM international conference on Information and knowledge management</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="545" to="554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Endto-End Neural Entity Linking</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kolitsas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O.-E</forename><surname>Ganea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd Conference on Computational Natural Language Learning</title>
		<meeting>the 22nd Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="519" to="529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">RACE: Large-scale ReAding Comprehension Dataset From Examinations</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Distant Learning for Entity Linking with Automatic Noise Detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4081" to="4090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A unified mrc framework for named entity recognition</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.11476</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Zero-Shot Entity Linking by Reading Entity Descriptions</title>
		<author>
			<persName><forename type="first">L</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3449" to="3460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Jointly Embedding Entities and Text with Distant Supervision</title>
		<author>
			<persName><forename type="first">D</forename><surname>Newman-Griffis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fosler-Lussier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Third Workshop on Representation Learning for NLP</title>
		<meeting>The Third Workshop on Representation Learning for NLP</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="195" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Ms marco: A humangenerated machine reading comprehension dataset</title>
		<author>
			<persName><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Know What You Don&apos;t Know: Unanswerable Questions for SQuAD</title>
		<author>
			<persName><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="784" to="789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Old is gold: linguistic driven approach for entity and relation linking of short text</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sakor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">O</forename><surname>Mulang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shekarpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Auer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2336" to="2346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Shahbazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">Z</forename><surname>Fern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ghaeini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Obeidat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tadepalli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.05762</idno>
		<title level="m">Entity-aware ELMo: Learning Contextual Entity Representation for Entity Disambiguation</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Entity linking with a knowledge base: Issues, techniques, and solutions</title>
		<author>
			<persName><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="page" from="443" to="460" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.01746</idno>
		<title level="m">Coreference resolution as query-based span prediction</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Washio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Matsumoto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.00426</idno>
		<title level="m">Global Entity Disambiguation with Pretrained Contextualized Embeddings of Words and Entities</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">S-MART: Novel Treebased Structured Learning Algorithms Applied to Tweet Entity Linking</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="504" to="513" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
