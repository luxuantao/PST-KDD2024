<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Building Extraction in Very High Resolution Remote Sensing Imagery Using Deep Learning and Guided Filters</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-01-19">19 January 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yongyang</forename><surname>Xu</surname></persName>
							<email>yongyangxu@cug.edu.cn</email>
							<idno type="ORCID">0000-0001-7421-4915</idno>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">China University of Geosciences</orgName>
								<address>
									<postCode>430074</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Liang</forename><surname>Wu</surname></persName>
							<email>wuliang@cug.edu.cn</email>
							<idno type="ORCID">0000-0001-7421-4915</idno>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">China University of Geosciences</orgName>
								<address>
									<postCode>430074</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">National Engineering Research Center of Geographic Information System</orgName>
								<address>
									<postCode>430074</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhong</forename><surname>Xie</surname></persName>
							<email>xiezhong@cug.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">China University of Geosciences</orgName>
								<address>
									<postCode>430074</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">National Engineering Research Center of Geographic Information System</orgName>
								<address>
									<postCode>430074</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhanlong</forename><surname>Chen</surname></persName>
							<email>chenzhanlong2005@126.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">China University of Geosciences</orgName>
								<address>
									<postCode>430074</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Building Extraction in Very High Resolution Remote Sensing Imagery Using Deep Learning and Guided Filters</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-01-19">19 January 2018</date>
						</imprint>
					</monogr>
					<idno type="MD5">F5A54A1FE63B41758CBEA1664D14B588</idno>
					<idno type="DOI">10.3390/rs10010144</idno>
					<note type="submission">Received: 19 December 2017; Accepted: 16 January 2018;</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>building extraction</term>
					<term>deep learning</term>
					<term>guided filter</term>
					<term>very high resolution</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Very high resolution (VHR) remote sensing imagery has been used for land cover classification, and it tends to a transition from land-use classification to pixel-level semantic segmentation. Inspired by the recent success of deep learning and the filter method in computer vision, this work provides a segmentation model, which designs an image segmentation neural network based on the deep residual networks and uses a guided filter to extract buildings in remote sensing imagery. Our method includes the following steps: first, the VHR remote sensing imagery is preprocessed and some hand-crafted features are calculated. Second, a designed deep network architecture is trained with the urban district remote sensing image to extract buildings at the pixel level. Third, a guided filter is employed to optimize the classification map produced by deep learning; at the same time, some salt-and-pepper noise is removed. Experimental results based on the Vaihingen and Potsdam datasets demonstrate that our method, which benefits from neural networks and guided filtering, achieves a higher overall accuracy when compared with other machine learning and deep learning methods. The method proposed shows outstanding performance in terms of the building extraction from diversified objects in the urban district.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Remote sensing images with very high resolution (VHR) are widely used in many applications including land cover mapping and monitoring <ref type="bibr" target="#b0">[1]</ref>, multi-angle urban classification analysis <ref type="bibr" target="#b1">[2]</ref>, automatic road detection <ref type="bibr" target="#b2">[3]</ref>, as well as the identification of tree species in forest management <ref type="bibr" target="#b3">[4]</ref>. Several of the practical applications are based on VHR remote sensing imagery classification at the pixel level <ref type="bibr" target="#b4">[5]</ref><ref type="bibr">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref>, also defined as semantic segmentation. Semantic segmentation of remote sensing imagery aims to classify every pixel into a given category, and it is an important task for understanding and inferring objects <ref type="bibr" target="#b8">[9,</ref><ref type="bibr">10]</ref> and the relationships between spatial objects in a scene <ref type="bibr" target="#b10">[11]</ref>.</p><p>Automatic semantic annotation of urban areas plays an important role in many photogrammetry and remote sensing applications, such as building and updating a geographical database, land cover change, and extracting thematic information. In recent years, the development of computing hardware and sensor technologies has made high resolution sampling available with a ground sampling distance (GSD) of 5-30 cm <ref type="bibr" target="#b11">[12]</ref> so that objects such as roof tiles, cars, buildings, and individual branches of trees, are distinguishable, which has increased the interest to perform semantic segmentation in urban areas.</p><p>In the past several years, spatial and spectral features have been used to improve the performance of VHR semantic segmentation based on pixel-wise analysis. Spatial contextual information like the grey level co-occurrence matrix (GLCM) has been employed to obtain a more accurate classification map <ref type="bibr" target="#b12">[13]</ref>. A novel mean shift (MS)-based multiscale method was used in urban mapping <ref type="bibr" target="#b13">[14]</ref>. Morphological profiles (MP) were utilized into the spatial-spectral classification <ref type="bibr" target="#b14">[15]</ref>. Conditional random fields and machine learning, such as SVM and random forest, were also introduced to solve the classification of remote sensing images <ref type="bibr">[6,</ref><ref type="bibr" target="#b15">16]</ref>. In addition, encouraged by deep neural network features that have been shown to have an outstanding capacity in visual recognition <ref type="bibr" target="#b16">[17]</ref>, object detection <ref type="bibr" target="#b17">[18]</ref> and semantic segmentation <ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref>, deep learning was introduced to resolve the old problems in remote sensing <ref type="bibr" target="#b21">[22]</ref>. Deep neural networks have been successfully used to class and densely label high resolution remote imagery <ref type="bibr" target="#b22">[23]</ref>. It can be used in various remote sensing tasks: Detection, classification, or data fusion <ref type="bibr" target="#b23">[24]</ref>. A deep learning framework was proposed to detect buildings in high-resolution multispectral imageries (RGB and near-infrared) <ref type="bibr" target="#b24">[25]</ref>. Multi-scale convolutional neural networks (CNNs) combined with the conditional random fields (CRFs) were used for dense classification in street scenes <ref type="bibr" target="#b25">[26]</ref>. An end-to-end trainable deep convolutional neural network (DCNN) was built to improve semantic image segmentation with boundary detection <ref type="bibr" target="#b11">[12]</ref>.</p><p>Studies have shown that remote sensing image classification results cannot be conclusive <ref type="bibr" target="#b26">[27]</ref>. The reason for this is that although the resolution of remote sensing images have improved, which has been helpful to detect and distinguish various objects on the ground, these improvements have made it more difficult to separate some objects, especially spectrally similar classes, due to the increase of the intra-class variance of objects, such as building, streets, shades and cars, and with decrease of the inter-class variance <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref>. In other words, different objects may present the same spectral values within the remote sensing imagery, which make it more difficult to extract reasonable spatial features to resolve the classification of pixels in extracting the buildings.</p><p>In the last years, fully convolutional networks (FCNs) have shown a good performance of semantic segmentation <ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref>. Indeed, FCNs can not only learn how to classify pixels and determine what it is, but they can also predict the structures of the spatial objects <ref type="bibr" target="#b32">[33]</ref>. The model is able to detect different classes of objects on the ground and predict their shapes, such as buildings, the curves of the roads, trees, and so on. However, it is a little short of being capable of detecting small objects or objects with many boundaries, because the boundaries of the objects are blurred and the results are visually degraded during classing when using FCNs <ref type="bibr" target="#b11">[12]</ref>.</p><p>There has been some research that tries to improve the performance of semantic segmentation and develop a deep neural network structure either by adding skip connections so as to reintroduce the high-frequency detailed information of an imagery after upsampling <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref> or by using dilated convolution combined with CRFs <ref type="bibr">[36]</ref>. The improved FCN model, which is designed as a multi-scale network architecture by adding a skip-layer structure, was trained to perform state-of-the-art natural image semantic segmentation <ref type="bibr" target="#b30">[31]</ref>. A deep FCN with no downsampling was introduced to boost the effective training sample size and improve the classification accuracy <ref type="bibr" target="#b36">[37]</ref>.</p><p>The application of research into urban district classification using VHR remote sensing imagery ranges from urban management to flow monitoring. Recent research makes an effort to improve the accuracy in areas such as encoding of images, extraction of features from raw images <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39]</ref>, and the use of deep neural networks such as CNNs, FCNs, and so on, to label pixels, especially for the VHR remote sensing imagery <ref type="bibr">[40,</ref><ref type="bibr" target="#b40">41]</ref>. However, pixel labelling of the VHR imagery in urban districts offers challenges relating to the varied semantic classes and geometry shapes. Because buildings and the other imperviousness objects in urban areas are very complicated with respect to both their spectral and spatial characteristics, it is inefficient and difficult to extract them. The VHR imagery is usually limited to three or four broad bands, and these spectral features alone may lack the ability to distinguish the objects because different objects have similar spectral values, for example, roads and roofs. Additionally, the same objects may have different spectral values, for example a roof that is divided into two parts by exposure to the sun and the shade. Therefore, discriminative appearance-based features are needed to improve the performance. Fortunately, most of the VHR remote sensing imageries usually have the corresponding overlapping image (or combined camera + LiDAR systems) <ref type="bibr" target="#b11">[12]</ref>, and the digital surface model (DSM) is available, which can be regarded as an additional depth channel.</p><p>Previous researchers have provided useful insights into the various methods that can be used in pixel labelling. However, these methods cannot clearly detect the boundary of the objects, and lack the ability to remove the salt-and-pepper class noise; some pixels with similar spectral values are usually misclassify. To resolve these problems, this work attempts to take semantic labelling methods from computer vision and apply them to building extraction from VHR remote sensing imageries.</p><p>In this paper, we try to improve the classification accuracy by a new model based on deep residual networks (ResNet) <ref type="bibr" target="#b41">[42]</ref>. At the same time, we introduce an object-oriented guided filter to improve the performance of classification. This method, on paper, involves three steps. First, imagery pre-processing is needed to prepare the dataset for deep learning. Second, a deep network is trained to segment VHR remote sensing imagery into two classes: buildings and clutter/unknown. Third, a guided filter is employed to optimize the extraction buildings and an ultimate spectral-spatial classification map of the urban district is achieved by fusing the object-oriented optimized results. All the challenges have resulted in improving the classification accuracy of complex urban area remote sensing imagery. The major contribution of this work is proposing a new model based on ResNet that we defined as Res-U-Net, and exploring a novel framework to perform classification of VHR remote sensing imagery. The experimental results show that the novel framework is more effective at extracting buildings.</p><p>The remainder of this paper is organized as follows: Section 2 presents the building extraction using VHR imagery in urban areas based on deep learning and guided filters; Section 3 describes the experimental results and how to set the parameters; Section 4 is a discussion of our method and Section 5 presents our concluding remarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Methods for Classification in Very High Resolution Remote Sensing Imagery</head><p>In this work a pixel classification method to extract buildings from urban districts within VHR remote sensing imageries based on deep learning and guided filters is proposed. First, the imageries are pre-processed and edge enhancing is used to emphasize the pixels which exist at the edges of the buildings. Some hand-crafted features including the normalized differential vegetation index (NDVI), the normalized digital surface model (NDSM), and the first component of the principal component analysis (PCA1) are extracted based on the color infrared (CIR) imagery, red green blue (RGB) satellite imagery as well as the corresponding digital surface model (DSM). Then, the proposed deep neural network Res-U-Net is introduced for pixel classification, where the hand-crafted features, the original bands, and the ground truth (labeled artificially) are treated as inputs to train the network. The output of the deep neural network is the segmentation map that represents the pixel labeling results. Finally, we briefly introduce the concept of a guided filter to fine-tune the pixel labeling results because the convolutional network tends to blur object boundaries and visually degrade the result when it is applied to remote sensing data <ref type="bibr" target="#b11">[12]</ref>. An overview of the proposed pixel classification framework is illustrated in Figure <ref type="figure" target="#fig_0">1</ref>.</p><p>Remote Sens. 2018, 10, 144 3 of 18 imageries usually have the corresponding overlapping image (or combined camera + LiDAR systems) <ref type="bibr" target="#b11">[12]</ref>, and the digital surface model (DSM) is available, which can be regarded as an additional depth channel. Previous researchers have provided useful insights into the various methods that can be used in pixel labelling. However, these methods cannot clearly detect the boundary of the objects, and lack the ability to remove the salt-and-pepper class noise; some pixels with similar spectral values are usually misclassify. To resolve these problems, this work attempts to take semantic labelling methods from computer vision and apply them to building extraction from VHR remote sensing imageries.</p><p>In this paper, we try to improve the classification accuracy by a new model based on deep residual networks (ResNet) <ref type="bibr" target="#b41">[42]</ref>. At the same time, we introduce an object-oriented guided filter to improve the performance of classification. This method, on paper, involves three steps. First, imagery pre-processing is needed to prepare the dataset for deep learning. Second, a deep network is trained to segment VHR remote sensing imagery into two classes: buildings and clutter/unknown. Third, a guided filter is employed to optimize the extraction buildings and an ultimate spectral-spatial classification map of the urban district is achieved by fusing the object-oriented optimized results. All the challenges have resulted in improving the classification accuracy of complex urban area remote sensing imagery. The major contribution of this work is proposing a new model based on ResNet that we defined as Res-U-Net, and exploring a novel framework to perform classification of VHR remote sensing imagery. The experimental results show that the novel framework is more effective at extracting buildings.</p><p>The remainder of this paper is organized as follows: section two presents the building extraction using VHR imagery in urban areas based on deep learning and guided filters; section three describes the experimental results and how to set the parameters; section four is a discussion of our method and section five presents our concluding remarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Methods for Classification in Very High Resolution Remote Sensing Imagery</head><p>In this work a pixel classification method to extract buildings from urban districts within VHR remote sensing imageries based on deep learning and guided filters is proposed. First, the imageries are pre-processed and edge enhancing is used to emphasize the pixels which exist at the edges of the buildings. Some hand-crafted features including the normalized differential vegetation index (NDVI), the normalized digital surface model (NDSM), and the first component of the principal component analysis (PCA1) are extracted based on the color infrared (CIR) imagery, red green blue (RGB) satellite imagery as well as the corresponding digital surface model (DSM). Then, the proposed deep neural network Res-U-Net is introduced for pixel classification, where the hand-crafted features, the original bands, and the ground truth (labeled artificially) are treated as inputs to train the network. The output of the deep neural network is the segmentation map that represents the pixel labeling results. Finally, we briefly introduce the concept of a guided filter to fine-tune the pixel labeling results because the convolutional network tends to blur object boundaries and visually degrade the result when it is applied to remote sensing data <ref type="bibr" target="#b11">[12]</ref>. An overview of the proposed pixel classification framework is illustrated in Figure <ref type="figure" target="#fig_0">1</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Deep Learning for Remote Sensing Imagery Classification</head><p>Convolutional networks have been widely utilized in applications ranging from whole-image classification <ref type="bibr" target="#b42">[43]</ref><ref type="bibr" target="#b43">[44]</ref><ref type="bibr" target="#b44">[45]</ref> to pixel classification as semantic segmentation in computer vision. Pixel classification includes automatically building maps of geo-localized semantic classes (for example: buildings, impervious surfer, vegetation, and so on) from the earth-observation data <ref type="bibr" target="#b45">[46]</ref>. In recent years, deep learning has become a state-of-the-art tool for pixel classification in remote sensing, as well as other fields. Fully convolutional networks are adapted as effective tools for the semantic labelling of high-resolution remote sensing data. This paper uses the modified and extended architecture ResNet, named Res-U-Net (Figure <ref type="figure" target="#fig_2">2</ref>).</p><p>Remote Sens. 2018, 10, 144 4 of 18</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Deep Learning for Remote Sensing Imagery Classification</head><p>Convolutional networks have been widely utilized in applications ranging from whole-image classification <ref type="bibr" target="#b42">[43]</ref><ref type="bibr" target="#b43">[44]</ref><ref type="bibr" target="#b44">[45]</ref> to pixel classification as semantic segmentation in computer vision. Pixel classification includes automatically building maps of geo-localized semantic classes (for example: buildings, impervious surfer, vegetation, and so on) from the earth-observation data <ref type="bibr" target="#b45">[46]</ref>. In recent years, deep learning has become a state-of-the-art tool for pixel classification in remote sensing, as well as other fields. Fully convolutional networks are adapted as effective tools for the semantic labelling of high-resolution remote sensing data. This paper uses the modified and extended architecture ResNet, named Res-U-Net (Figure <ref type="figure" target="#fig_2">2</ref>). In this paper, we trained the Res-U-Net by adopting the approach of reference <ref type="bibr" target="#b46">[47]</ref>, which is famous for having the ability to work with very little training data but still obtain precise segmentation. The Res-U-Net network consists of two paths: contracting (left) and expansive (right). The left part is the ResNet, which is used to extract the features of input data, and we modified the input layer to adapt the seven elements of the input data. The input layer is followed by a normalization layer and a max pooling layer. The activation layer in the network contains a rectified linear unit (ReLU) and a 2 × 2 max-pooling operation for the subsampling, both of them improve the robustness of the network against distortions and small translations <ref type="bibr" target="#b43">[44]</ref>. During the features extraction, there are four stages and every stage includes several residual blocks. The feature maps in the same block have the same size, and the feature maps in the following blocks are half that of the previous ones. The feature maps in different blocks have different scale features. The expansive part aims to extract the buildings using the feature maps. The number of stages in contracting and expansive is the same. Inspired by the feature pyramid networks <ref type="bibr" target="#b47">[48]</ref>, to obtain the features in multiple scales, a concatenation with the corresponding stage from the contracting part is designed in the deep neural network. Every stage in the expansive part includes the upsampling of the feature In this paper, we trained the Res-U-Net by adopting the approach of reference <ref type="bibr" target="#b46">[47]</ref>, which is famous for having the ability to work with very little training data but still obtain precise segmentation. The Res-U-Net network consists of two paths: contracting (left) and expansive (right). The left part is the ResNet, which is used to extract the features of input data, and we modified the input layer to adapt the seven elements of the input data. The input layer is followed by a normalization layer and a max pooling layer. The activation layer in the network contains a rectified linear unit (ReLU) and a 2 × 2 max-pooling operation for the subsampling, both of them improve the robustness of the network against distortions and small translations <ref type="bibr" target="#b43">[44]</ref>. During the features extraction, there are four stages and every stage includes several residual blocks. The feature maps in the same block have the same size, and the feature maps in the following blocks are half that of the previous ones. The feature maps in different blocks have different scale features. The expansive part aims to extract the buildings using the feature maps. The number of stages in contracting and expansive is the same.</p><p>Inspired by the feature pyramid networks <ref type="bibr" target="#b47">[48]</ref>, to obtain the features in multiple scales, a concatenation with the corresponding stage from the contracting part is designed in the deep neural network. Every stage in the expansive part includes the upsampling of the feature map, a concatenation block and a convolution block, which consists of a 3 × 3 convolution layer, a normalization layer and a rectified linear unit. At the end of the network, a 1 × 1 convolutional layer is added to map the feature vectors to the two classes of buildings and clutter, the outputs of this layer indicate the class scores for the pixel. A softmax layer, used to calculate the classification results, is added at the end of the network. In this work, the deep convolutional network uses the ResNet as a feature extractor, which solves the degradation problem during the layer increases, and it is useful to extract the features in contracting. The concatenation in the expansive part is able to learn multiple scales and different level features, which increases the robustness of the network and improves the accuracy of the building extraction. The output of the softmax layer is a probability map with two channels. It presents the result of the classification between buildings and clutter in every pixel.</p><p>Within the remote sensing imagery and their corresponding normalized digital surface model, hand-crafted features such as NDVI, PCA1 as well as the classified segmentation maps are regarded as the inputs to train the network. The Res-U-Net builds higher level features by the grouping of mapping features of lower level features, and therefore, the results are located more accurately. It transmits the error from a high level to a low level and speeds up the training <ref type="bibr" target="#b46">[47]</ref>. The size of the output of the network is the same as the input and it usesnd-to-end processing. At the beginning of the network, max-polling and convolution layers produce more abstract feature maps, which are beneficial for the up-convolution in order to calculate an accurate pixel classification result.</p><p>The building extraction problem can be regarded as a binary classification problem. During the training of the parameters, it can be solved by a logistic regression using the optimization of the energy function. As with other training methods <ref type="bibr" target="#b46">[47]</ref>, we train the network using the gradient descent to minimize the energy function. The energy function is calculated by the softmax as well as the cross entropy loss function. The softmax is used to calculate the probability map, defined as:</p><formula xml:id="formula_0">p k x i = exp (w k ) T x i K ∑ j=1 exp (w k ) T x i<label>(1)</label></formula><p>where k ∈ {1, 2} which corresponds to the buildings and the clutter, and K represents the number of classes as two. p k x i is the probability that sample x i belongs to class k. The energy function is defined as follows:</p><formula xml:id="formula_1">E = m ∑ i=1 K ∑ k=1 w x i log p k x i<label>(2)</label></formula><p>where x i , y i m i=1 is assumed to be the training data, x i represents the vectored features, and y i is the labeled data, m represents the number of samples, and w is a weight map in the network to be optimized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Guided Filtering</head><p>To fine-tune the buildings extracted by deep learning, the guided filter, which was firstly proposed by He <ref type="bibr" target="#b48">[49]</ref>, is introduced in this work. Like the bilateral filter, it is an edge-preserving smoothing technique. Thanks to the guiding of the input image (guidance image), the filtering result is more structured and less smoothed. The guided filter is better than the bilateral filter in terms of detail and it is more effective <ref type="bibr" target="#b48">[49]</ref>, which makes it widely applicable in computer vision and graphics <ref type="bibr" target="#b49">[50]</ref>. The guided filter assumes that the local linear model exists between the guidance image and the filtering result, so that it will benefit to optimize object classification like buildings.</p><p>The guided filter involves two input images including a guidance image I c and a filtering image I in . The filtering output O is assumed to be a linear transform of I c in a window w k :</p><formula xml:id="formula_2">O(i) = a k I c (i) + b k (3)</formula><p>where a k and b k are the coefficients of the linear transform between the guidance image I c and the filtering image O within window w k (the size of window is w × w). They can be calculated as follows:</p><formula xml:id="formula_3">a k = 1 w 2 ∑ i∈w k I c (i)I in (i) -u k p k σ 2 k + ε (4) b k = p k -a k u k<label>(5)</label></formula><p>where, u k and σ k are the mean and variance of the guidance image I c within the window w k , and p k is the mean of the filtering image I in within the window w k , and ε controls the blur degree of the guided filter. Because pixel i has a relationship with all the windows that cover it, the output of filtering O (i) is calculated as:</p><formula xml:id="formula_4">O(i) = a i I c (i) + b i (6)</formula><p>where a i and b i are the mean of coefficients of all the windows that cover the pixel i. For simplicity, the equation can be rewritten as:</p><formula xml:id="formula_5">O = G(I in , I c , w, ε)<label>(7)</label></formula><p>The original imageries are treated as guiders to optimize the boundaries in order to remove the salt-and-pepper class noise. The result, directly fine-tuned by the guided filter, will result in the over-smoothness of the extracted buildings in the output. However, the building maps should be binary and the pixels in the boundaries change gradient in reality. Therefore, we set a threshold during filtering. If the value is larger than the threshold it will be set to 255, which represents buildings, otherwise, it is equal to 0, which represents the clutter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Datasets</head><p>The ISPRS 2D semantic labelling VHR remote sensing imageries of urban districts are used in the experiments, including the Vaihingen (Germany) and Potsdam (Germany) datasets, as these are open asset datasets provided online. Both of them consist of the near infra-red, red, and green ortho-rectified imagery (or color infra-red, CIR). The corresponding digital surface models (DSMs) generated by dense image matching and ground truth labels are annotated manually. Additionally, the Potsdam dataset has a blue channel, containing 38 ortho-rectified aerial IRRGB images of ≈ 6000 × 6000 (in total, over 1,368,000,000 pixels) at 5 cm spatial resolution, where 24 tiles are labelled with pixel-level ground truth. The Vaihingen dataset comprised of 33 large image patches of ≈ 2500 × 2500, extracted from a larger orthophoto imagery captured over Vaihingen. Overall, there are about 168,287,871 pixels, and the imageries have a ground sample distance (GSD) of 9 cm, where 16 tiles are labelled with pixel-level ground truth. Each of the ground truth labels are made up of building and unknown (clutter). The DSM is a value array which has the same size as the input image and the labelled ground truth. At the same time, the normalized DSMs <ref type="bibr" target="#b50">[51]</ref> are available for us, where the height is computed using the off-ground pixels. The imageries with ground truth are divided into two parts, where 80% are used to train the Res-U-Net and 20% are used to validate the trained model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Preprocessing the Data for Deep Learning</head><p>Although the urban remote sensing imagery used in this work is in high resolution, some object edges are still fuzzy, which result in the object being unrecognizable from the background. Therefore, this work introduces the edge enhancement effect to the remote sensing imagery processing. The edge enhancement is an image-filter that reduces the effect of noise. It can also decrease the complexity of the image computation. Edge enhancement is widely used in fields such as pattern recognition, image semantic segmentation, and so on. This work enhances the edge of the imageries using the python imaging library (PIL). It is a kind of convolutional filter, where a n × n matrix is defined to operate with the digital imagery. Every pixel of the edge enhancement result is a sum-weighted value of the convolution region. The size of the convolution kernel used in the experiment is 5 × 5.</p><p>The size of the total from dataset is approximately 6000 × 6000. If the whole dataset is used as an input for the deep network, millions of paragraphs must be learned, which would lead to a lack of memory. Therefore, we processed the imageries using a 256 × 256 sliding window with a stride of 64 px to produce the samples. Every eight samples were regarded as a batch to train the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Experimental Setup and Results</head><p>To improve the accuracy of the vegetation in this experiment, we computed the NDVI from the near-infrared and the red channels, and it was used as an indicator for the vegetation (NDVI = (NIR -R)/(NIR + R)). A PCA transformation was introduced to extract the first component comprising of brightness, which will be beneficial to classify some special building roofs. The bands of R, G, B (there is no blue band in the Vaihingen data), and CIR, as well as the hand-crafted features including NDVI, NDSM, the first component of PCA, and the corresponding ground truth (Figure <ref type="figure" target="#fig_0">1</ref>) are used as inputs to train the Res-U-Net. The architecture, as well as the parameters used in this work, is shown in Figure <ref type="figure" target="#fig_4">3</ref>.</p><p>Remote Sens. 2018, 10, 144 7 of 18</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Preprocessing the Data for Deep Learning</head><p>Although the urban remote sensing imagery used in this work is in high resolution, some object edges are still fuzzy, which result in the object being unrecognizable from the background. Therefore, this work introduces the edge enhancement effect to the remote sensing imagery processing. The edge enhancement is an image-filter that reduces the effect of noise. It can also decrease the complexity of the image computation. Edge enhancement is widely used in fields such as pattern recognition, image semantic segmentation, and so on. This work enhances the edge of the imageries using the python imaging library (PIL). It is a kind of convolutional filter, where a n n × matrix is defined to operate with the digital imagery. Every pixel of the edge enhancement result is a sumweighted value of the convolution region. The size of the convolution kernel used in the experiment is 5 5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>×</head><p>. The size of the total from dataset is approximately 6 0 0 0 6 0 0 0 × . If the whole dataset is used as an input for the deep network, millions of paragraphs must be learned, which would lead to a lack of memory. Therefore, we processed the imageries using a 2 5 6 2 5 6 × sliding window with a stride of 64 px to produce the samples. Every eight samples were regarded as a batch to train the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Experimental Setup and Results</head><p>To improve the accuracy of the vegetation in this experiment, we computed the NDVI from the near-infrared and the red channels, and it was used as an indicator for the vegetation (NDVI = (NIR -R)/(NIR + R)). A PCA transformation was introduced to extract the first component comprising of brightness, which will be beneficial to classify some special building roofs. The bands of R, G, B (there is no blue band in the Vaihingen data), and CIR, as well as the hand-crafted features including NDVI, NDSM, the first component of PCA, and the corresponding ground truth (Figure <ref type="figure" target="#fig_0">1</ref>) are used as inputs to train the Res-U-Net. The architecture, as well as the parameters used in this work, is shown in Figure <ref type="figure" target="#fig_4">3</ref>. For an individual network, we trained the network with a learning rate of 0.001. To ensure an outstanding learning result, we divided the learning rate by ten every ten epochs. There are 100 epochs during the training and each epoch has 2048 samples. We use the Adam as the optimizer to optimize the network when adjusting parameters like weights, biases, and so on. In case most of the evaluation data have targets, we set the size of evaluation data as 2000 × 2000. For an individual network, we trained the network with a learning rate of 0.001. To ensure an outstanding learning result, we divided the learning rate by ten every ten epochs. There are 100 epochs during the training and each epoch has 2048 samples. We use the Adam as the optimizer to optimize the network when adjusting parameters like weights, biases, and so on. In case most of the evaluation data have targets, we set the size of evaluation data as 2000 × 2000.</p><p>The provided metrics of F 1 score and the global pixel-wise accuracy of each class are used to assess the quantitative performance. F 1 score is a representation of the harmonic mean of precision and recall, and it can be calculated as follows:</p><formula xml:id="formula_6">F 1 i = 2 × precision i × recall i precision i + recall i<label>(8)</label></formula><p>where</p><formula xml:id="formula_7">precision i = TP i TP i + FP i , recall i = TP i TP i + FN i (9)</formula><p>Here, TP i is the number of true positives for class i, FP i and FN i represent false positive and false negative, respectively. These metrics are computed using the pixel-based confusion matrices per tile or by an accumulated confusion matrix. At the same time, the overall accuracy (OA) can be obtained by normalizing the trace from the confusion matrix <ref type="bibr" target="#b51">[52]</ref>.</p><p>The proposed deep learning of the Res-U-Net is implemented using Tensorflow and Keras in the Linux platform with a TITAN GPU (12 GB RAM). After 204,800 iterations, our best model achieves state-of-the-art results on the datasets (Table <ref type="table" target="#tab_0">1</ref>). The changing accuracies and losses of the Potsdam and Vaihingen datasets with the increasing epochs are shown in Figure <ref type="figure" target="#fig_6">4</ref>.</p><p>Remote Sens. 2018, 10, 144 8 of 18</p><p>The provided metrics of F1 score and the global pixel-wise accuracy of each class are used to assess the quantitative performance. F1 score is a representation of the harmonic mean of precision and recall, and it can be calculated as follows:</p><formula xml:id="formula_8">1 2 i i i i i precision recall F precision recall × = × +<label>(8)</label></formula><p>where</p><formula xml:id="formula_9">i i i i TP precision TP FP = + , i i i i TP recall TP FN = +<label>(9)</label></formula><p>Here, TPi is the number of true positives for class i, FPi and FNi represent false positive and false negative, respectively. These metrics are computed using the pixel-based confusion matrices per tile or by an accumulated confusion matrix. At the same time, the overall accuracy (OA) can be obtained by normalizing the trace from the confusion matrix <ref type="bibr" target="#b51">[52]</ref>.</p><p>The proposed deep learning of the Res-U-Net is implemented using Tensorflow and Keras in the Linux platform with a TITAN GPU (12 GB RAM). After 204,800 iterations, our best model achieves state-of-the-art results on the datasets (Table <ref type="table" target="#tab_0">1</ref>). The changing accuracies and losses of the Potsdam and Vaihingen datasets with the increasing epochs are shown in Figure <ref type="figure" target="#fig_6">4</ref>.  The architecture reaches 96.91% overall accuracy over the Potsdam and 97.71% overall accuracy over Vaihingen, respectively. The deep learning frame performs particularly well on impervious ground and the buildings (Figure <ref type="figure" target="#fig_9">5</ref>). The architecture reaches 96.91% overall accuracy over the Potsdam and 97.71% overall accuracy over Vaihingen, respectively. The deep learning frame performs particularly well on impervious ground and the buildings (Figure <ref type="figure" target="#fig_9">5</ref>). Although the accuracy of the pixel labelling improved by using edge enhancement and deep neural networks, the boundaries of the buildings were still blurry and some pixels belonging to the buildings were misclassified (Figure <ref type="figure" target="#fig_15">6b</ref>,e). To improve the performance, a guided filter was introduced. During the optimization by the guided filter, we set values larger than the threshold (t = 90) to 255, which is mentioned in Section 2.2. Otherwise, the values are set to 0. The original imageries as well as the prediction results produced by deep learning are used as the input for the guided filter. From the results (Figure <ref type="figure" target="#fig_10">6</ref>), it is clear that the performance in both of the datasets improved.  Although the accuracy of the pixel labelling improved by using edge enhancement and deep neural networks, the boundaries of the buildings were still blurry and some pixels belonging to the buildings were misclassified (Figure <ref type="figure" target="#fig_15">6b</ref>,e). To improve the performance, a guided filter was introduced. During the optimization by the guided filter, we set values larger than the threshold (t = 90) to 255, which is mentioned in Section 2.2. Otherwise, the values are set to 0. The original imageries as well as the prediction results produced by deep learning are used as the input for the guided filter. From the results (Figure <ref type="figure" target="#fig_10">6</ref>), it is clear that the performance in both of the datasets improved. The architecture reaches 96.91% overall accuracy over the Potsdam and 97.71% overall accuracy over Vaihingen, respectively. The deep learning frame performs particularly well on impervious ground and the buildings (Figure <ref type="figure" target="#fig_9">5</ref>). Although the accuracy of the pixel labelling improved by using edge enhancement and deep neural networks, the boundaries of the buildings were still blurry and some pixels belonging to the buildings were misclassified (Figure <ref type="figure" target="#fig_15">6b</ref>,e). To improve the performance, a guided filter was introduced. During the optimization by the guided filter, we set values larger than the threshold (t = 90) to 255, which is mentioned in Section 2.2. Otherwise, the values are set to 0. The original imageries as well as the prediction results produced by deep learning are used as the input for the guided filter. From the results (Figure <ref type="figure" target="#fig_10">6</ref>), it is clear that the performance in both of the datasets improved.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Some Effects to the Result of Deep Learning</head><p>Although VHR remote sensing imagery is easily applied to distinguish objects on the ground, some edges are not obvious between objects with similar spectral values, so it is difficult to classify the pixels, especially in the urban districts. This work introduces edge enhancing to increase the differences among objects which leads to better performance during classification. We compared the overall accuracy for buildings and clutter classification, as well as precision, recall and F 1 (mentioned above) by both using and not using the preprocessing (Figure <ref type="figure" target="#fig_16">7</ref>), respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Some Effects to the Result of Deep Learning</head><p>Although VHR remote sensing imagery is easily applied to distinguish objects on the ground, some edges are not obvious between objects with similar spectral values, so it is difficult to classify the pixels, especially in the urban districts. This work introduces edge enhancing to increase the differences among objects which leads to better performance during classification. We compared the overall accuracy for buildings and clutter classification, as well as precision, recall and F1 (mentioned above) by both using and not using the preprocessing (Figure <ref type="figure" target="#fig_16">7</ref>), respectively.  As we can see, the overall accuracy of Potsdam has improved by 0.43% and the overall accuracy of Vaihingen has improved by 2.94%. At the same time, the precision and recall for buildings has improved compared to the results computed using the inputs without edge enhancing. Edge enhancement is able to emphasize the indistinct pixels at the edges of the buildings so that they can be classified more precisely, as shown in Figure <ref type="figure" target="#fig_18">8</ref>. It can be easily observed that the performance is poor in some parts like A, B without the edge enhancing preprocessing.</p><p>Remote Sens. 2018, 10, 144 11 of 18</p><p>As we can see, the overall accuracy of Potsdam has improved by 0.43% and the overall accuracy of Vaihingen has improved by 2.94%. At the same time, the precision and recall for buildings has improved compared to the results computed using the inputs without edge enhancing. Edge enhancement is able to emphasize the indistinct pixels at the edges of the buildings so that they can be classified more precisely, as shown in Figure <ref type="figure" target="#fig_18">8</ref>. It can be easily observed that the performance is poor in some parts like A, B without the edge enhancing preprocessing. Precision, recall and the F1 scores have significantly improved thanks to the discriminative power of the digital surface model (DSM) and NDVI. To illustrate some differences between the results achieved by the DSM and NDVI, the controlling variable method was adopted to analysis of the effects of the elements. We compared the performance of deep learning whilst exclude either the DSM or the NDVI and the performance of deep learning only treat the RBG images as input. Table <ref type="table" target="#tab_2">2</ref> compares the results on the Vaihingen and Potsdam datasets. It can be clearly observed that the results support the idea that it is beneficial to use the DSM and the NDVI, and that they improve the overall accuracy by 1.64% and 0.39% for the Potsdam dataset and 1.45% and 2.19% for the Vaihingen dataset. They also improve the F1 by 3.92% and 0.83% for Potsdam and 2.42% and 3.89% for Vaihingen. Compared with the results, it is clear that the limitation of the input only with RBG images, the overall accuracy of deep learning decreased by 2.66% and 2.7%, respectively; and F1 for building decreased by 5.71% and 4.13%, respectively, whilst exclude both the DSM and the NDVI. By analysis, it is clear that the performance using the DSM as a channel of input has improved when compared to the case without the DSM. The recall for buildings in the two datasets decreased by 6.96% and 2.05%, respectively. That is to say, the nature of some pixels that are buildings are Precision, recall and the F 1 scores have significantly improved thanks to the discriminative power of the digital surface model (DSM) and NDVI. To illustrate some differences between the results achieved by the DSM and NDVI, the controlling variable method was adopted to analysis of the effects of the elements. We compared the performance of deep learning whilst exclude either the DSM or the NDVI and the performance of deep learning only treat the RBG images as input. Table <ref type="table" target="#tab_2">2</ref> compares the results on the Vaihingen and Potsdam datasets. It can be clearly observed that the results support the idea that it is beneficial to use the DSM and the NDVI, and that they improve the overall accuracy by 1.64% and 0.39% for the Potsdam dataset and 1.45% and 2.19% for the Vaihingen dataset. They also improve the F 1 by 3.92% and 0.83% for Potsdam and 2.42% and 3.89% for Vaihingen. Compared with the results, it is clear that the limitation of the input only with RBG images, the overall accuracy of deep learning decreased by 2.66% and 2.7%, respectively; and F 1 for building decreased by 5.71% and 4.13%, respectively, whilst exclude both the DSM and the NDVI. By analysis, it is clear that the performance using the DSM as a channel of input has improved when compared to the case without the DSM. The recall for buildings in the two datasets decreased by 6.96% and 2.05%, respectively. That is to say, the nature of some pixels that are buildings are misclassified as clutter. Although the pixels that belong to a roof exposed to the sun and a roof out of the sun are different, they have the same DSM value, so it will perform well when extracting all kinds of building roofs. Some road pixels are very similar to the roof of the building in terms of spectral characteristics, but they have a large difference in DSM. As a result, DSM improves the capability of the model to extract buildings and the classification precision of OA, buildings and clutter. The results can be observed in Figure <ref type="figure" target="#fig_20">9</ref>.</p><p>Remote Sens. 2018, 10, 144 12 of 18 misclassified as clutter. Although the pixels that belong to a roof exposed to the sun and a roof out of the sun are different, they have the same DSM value, so it will perform well when extracting all kinds of building roofs. Some road pixels are very similar to the roof of the building in terms of spectral characteristics, but they have a large difference in DSM. As a result, DSM improves the capability of the model to extract buildings and the classification precision of OA, buildings and clutter. The results can be observed in Figure <ref type="figure" target="#fig_20">9</ref>. The NDVI can show the impact of the underlying background of buildings and the vegetation canopy structure to some degree. In urban areas, some low buildings are always covered with tress, which make it difficult to classify, like part A and B in Figure <ref type="figure" target="#fig_22">10</ref>. When training the network without the NDVI, the overall accuracy and F1 for both buildings and clutter in both Potsdam and Vaihingen datasets decreased. The recall for buildings in the two datasets decreased by 1.65% and 3.86%, respectively. The results (Figure <ref type="figure" target="#fig_22">10</ref>) show that the NDVI as a channel of input to train the model is beneficial to solve the problem.</p><p>Compared with other methods using the same datasets (that is, the training and validation datasets), the results are reported in Table <ref type="table" target="#tab_4">3</ref>. The Res-U-Net proposed in this work shows improvements on building extraction in both datasets. The network extracts features using the ResNet, which works well in contracting, and it benefits a lot from solving the degradation problem during the increase of layers. The expansive concatenated with multiple scales in different blocks and is helpful in classifying the buildings of different sizes. The NDVI can show the impact of the underlying background of buildings and the vegetation canopy structure to some degree. In urban areas, some low buildings are always covered with tress, which make it difficult to classify, like part A and B in Figure <ref type="figure" target="#fig_22">10</ref>. When training the network without the NDVI, the overall accuracy and F 1 for both buildings and clutter in both Potsdam and Vaihingen datasets decreased. The recall for buildings in the two datasets decreased by 1.65% and 3.86%, respectively. The results (Figure <ref type="figure" target="#fig_22">10</ref>) show that the NDVI as a channel of input to train the model is beneficial to solve the problem.</p><p>Compared with other methods using the same datasets (that is, the training and validation datasets), the results are reported in Table <ref type="table" target="#tab_4">3</ref>. The Res-U-Net proposed in this work shows improvements on building extraction in both datasets. The network extracts features using the ResNet, which works well in contracting, and it benefits a lot from solving the degradation problem during the increase of layers. The expansive concatenated with multiple scales in different blocks and is helpful in classifying the buildings of different sizes.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Influence of the Guided Filter</head><p>The threshold used in the optimization by the guided filter is important. Since some pixels near the building edge and the spectrum are similar to the buildings if the threshold is smaller, more pixels will be extracted as buildings and lead to the extracted building area being larger than the real building area. On the other hand, if the threshold is larger, some unclear edges will be excluded and the extracted building area will be smaller than the real area of the buildings (Figure <ref type="figure" target="#fig_23">11</ref>).   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Influence of the Guided Filter</head><p>The threshold used in the optimization by the guided filter is important. Since some pixels near the building edge and the spectrum are similar to the buildings if the threshold is smaller, more pixels will be extracted as buildings and lead to the extracted building area being larger than the real building area. On the other hand, if the threshold is larger, some unclear edges will be excluded and the extracted building area will be smaller than the real area of the buildings (Figure <ref type="figure" target="#fig_23">11</ref>).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Influence of the Guided Filter</head><p>The threshold used in the optimization by the guided filter is important. Since some pixels near the building edge and the spectrum are similar to the buildings if the threshold is smaller, more pixels will be extracted as buildings and lead to the extracted building area being larger than the real building area. On the other hand, if the threshold is larger, some unclear edges will be excluded and the extracted building area will be smaller than the real area of the buildings (Figure <ref type="figure" target="#fig_23">11</ref>).  To get the optimal threshold, we compared the overall accuracy and F 1 of the results using different thresholds. The guided filter with different thresholds was then used by the same predicted results of the Res-U-Net. The thresholds range was between 40 and 175 and the threshold value increased by every five steps. From the result (Figure <ref type="figure" target="#fig_28">12</ref>) we can see that the accuracy increases as the threshold grows until it reaches a threshold of t = 90. After that, the overall accuracy and F 1 decreases with the growing threshold. In this way, the threshold in this work was set to 90 while optimizing using the guided filter. To get the optimal threshold, we compared the overall accuracy and F1 of the results using different thresholds. The guided filter with different thresholds was then used by the same predicted results of the Res-U-Net. The thresholds range was between 40 and 175 and the threshold value increased by every five steps. From the result (Figure <ref type="figure" target="#fig_28">12</ref>) we can see that the accuracy increases as the threshold grows until it reaches a threshold of t = 90. After that, the overall accuracy and F1 decreases with the growing threshold. In this way, the threshold in this work was set to 90 while optimizing using the guided filter. The size of the window in the guided filter also affects the accuracy during optimization. If the size of window is too small, there will be less information in view to be used to guide the optimization and the filtered result will not be able to obtain enough surrounding information during optimization. On the contrary, if the size of window is too large, the information in the window will be mixed, which will mislead the filter optimization. To get the optimal window size in the guided filter, we compared the overall accuracy and F1 of the results using different window sizes from two to 15. From the results (Figure <ref type="figure" target="#fig_30">13</ref>) we can see that the overall accuracy and F1 increased as the window size increased until it reached size = 5. After that, the overall accuracy and F1 decreased with the growing window size. Therefore, the size of the window in the guided filter was set as five while optimizing the Res-U-Net results in the experiments. The size of the window in the guided filter also affects the accuracy during optimization. If the size of window is too small, there will be less information in view to be used to guide the optimization and the filtered result will not be able to obtain enough surrounding information during optimization. On the contrary, if the size of window is too large, the information in the window will be mixed, which will mislead the filter optimization. To get the optimal window size in the guided filter, we compared the overall accuracy and F 1 of the results using different window sizes from two to 15. From the results (Figure <ref type="figure" target="#fig_30">13</ref>) we can see that the overall accuracy and F 1 increased as the window size increased until it reached size = 5. After that, the overall accuracy and F 1 decreased with the growing window size. Therefore, the size of the window in the guided filter was set as five while optimizing the Res-U-Net results in the experiments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, a novel framework to perform building extraction in urban districts with very high resolution (VHR) remote sensing imagery is presented. The major contribution of this work is to explore an alternative technique for labeling objects in urban districts, which combined deep learning and guided filtering. This project aimed to design a network which improved the accuracy of building extraction and introduced a guided filter into the post-processing of the results. In our work, during the preprocessing of the date, we used edge enhancing and it is helpful in improving the performance of the segmentation process. As the deep neural network, Res-U-Net did well in labeling different scales buildings; guided filtering was introduced after the Res-U-Net neural network stage, which optimized the classification results and removed the salt-and-pepper class noise. At the same time, it preserved the boundaries of the objects within the imagery effectively. Experiments were carried out on two VHR remote sensing imagery datasets. Every desirable object was extracted successfully using the method mentioned in this work and the results showed the effectiveness and feasibility of the proposed framework in improving the performance of the urban district remote sensing imagery classification. The method was compared with some classical VHR remote sensing classification such as the fully convolutional network (FCN) as well as the method that combined the convolutional neural network (CNN) and random forest (RF). Experimental results demonstrated that our methods were better than the other methods. The proposed method in this work can obtain improvements in terms of overall accuracy, precision and F1 over the classical classification systems.</p><p>With the development of remote sensing technology, more and more VHR images can be accessed conveniently, and the classification of the urban district plays an important role in practical applications such as urban infrastructure, management, and so on. This work has provided an effective method to improve VHR image classification performance. However, the shape of some </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, a novel framework to perform building extraction in urban districts with very high resolution (VHR) remote sensing imagery is presented. The major contribution of this work is to explore an alternative technique for labeling objects in urban districts, which combined deep learning and guided filtering. This project aimed to design a network which improved the accuracy of building extraction and introduced a guided filter into the post-processing of the results. In our work, during the preprocessing of the date, we used edge enhancing and it is helpful in improving the performance of the segmentation process. As the deep neural network, Res-U-Net did well in labeling different scales buildings; guided filtering was introduced after the Res-U-Net neural network stage, which optimized the classification results and removed the salt-and-pepper class noise. At the same time, it preserved the boundaries of the objects within the imagery effectively. Experiments were carried out on two VHR remote sensing imagery datasets. Every desirable object was extracted successfully using the method mentioned in this work and the results showed the effectiveness and feasibility of the proposed framework in improving the performance of the urban district remote sensing imagery classification. The method was compared with some classical VHR remote sensing classification such as the fully convolutional network (FCN) as well as the method that combined the convolutional neural network (CNN) and random forest (RF). Experimental results demonstrated that our methods were better than the other methods. The proposed method in this work can obtain improvements in terms of overall accuracy, precision and F 1 over the classical classification systems.</p><p>With the development of remote sensing technology, more and more VHR images can be accessed conveniently, and the classification of the urban district plays an important role in practical applications such as urban infrastructure, management, and so on. This work has provided an effective method to improve VHR image classification performance. However, the shape of some buildings that are covered by trees cannot be detected precisely, and some blurry and irregular boundaries are hardly classified. In the future, a more optimized deep neural network is required to improve efficiency and accuracy. At the same time, further improvement may be achieved by combining the deep neural network and the guided filter in an end-to-end model, which would combine the advantage of a guided filter that preserves boundaries and decreases the salt-and-pepper class noise whilst also being convenient to train like the FCN. Instead of treating non-building as a background class, we will take the scene semantic into account and extract the roads and trees as well as the cars and so on in future studies.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. The framework of the pixel classification using deep learning and a guided filter.</figDesc><graphic coords="3,114.89,630.49,365.80,85.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. The framework of the pixel classification using deep learning and a guided filter.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. The architecture of the Res-U-Net used in this work. Each box represents the feature map and the x-y-size of the map is provided at the lower right edge of the box. The arrows denote the different operations which are explained at the lower right of the figure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. The architecture of the Res-U-Net used in this work. Each box represents the feature map and the x-y-size of the map is provided at the lower right edge of the box. The arrows denote the different operations which are explained at the lower right of the figure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Samples of the urban remote sensing imageries used in the experiments.</figDesc><graphic coords="7,158.61,388.21,278.16,214.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Samples of the urban remote sensing imageries used in the experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Plots showing the accuracy and loss of the Res-U-Net network for training the datasets. The training accuracy (a) and the loss (b) change with the epochs increasing in Potsdam. The training accuracy (c) and the loss (d) change with the epochs in Vaihingen.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Plots showing the accuracy and loss of the Res-U-Net network for training the datasets. The training accuracy (a) and the loss (b) change with the epochs increasing in Potsdam. The training accuracy (c) and the loss (d) change with the epochs in Vaihingen.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Results of the buildings extraction of the Vaihingen dataset using deep learning, the imagery (a) as well as the corresponding ground truth (b) and the prediction (c).</figDesc><graphic coords="9,169.22,178.02,257.20,131.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Results of the guided filter. (a) represent the original imageries in the Potsdam and Vaihingen datasets, respectively. (b) represent the corresponding prediction from deep learning. (c) represent the results of guided filter. (d-f) are the original imageries, prediction from deep learning and the results of guided filter in Vaihingen, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Results of the buildings extraction of the Vaihingen dataset using deep learning, the imagery (a) as well as the corresponding ground truth (b) and the prediction (c).</figDesc><graphic coords="9,154.95,140.34,285.78,145.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Results of the buildings extraction of the Vaihingen dataset using deep learning, the imagery (a) as well as the corresponding ground truth (b) and the prediction (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Results of the guided filter. (a) represent the original imageries in the Potsdam and Vaihingen datasets, respectively. (b) represent the corresponding prediction from deep learning. (c) represent the results of guided filter. (d-f) are the original imageries, prediction from deep learning and the results of guided filter in Vaihingen, respectively.</figDesc><graphic coords="9,102.45,436.55,391.02,249.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Results of the guided filter. (a) represent the original imageries in the Potsdam and Vaihingen datasets, respectively. (b) represent the corresponding prediction from deep learning. (c) represent the results of guided filter. (d-f) are the original imageries, prediction from deep learning and the results of guided filter in Vaihingen, respectively.</figDesc><graphic coords="9,121.97,444.61,351.92,224.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. The results of the building extraction from the urban districts of the Potsdam (a) and Vaihingen (b) datasets with and without edge enhancing in the preprocessing stage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. The results of the building extraction from the urban districts of the Potsdam (a) and Vaihingen (b) datasets with and without edge enhancing in the preprocessing stage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Results of the building extraction in the urban district of the Potsdam dataset with and without edge enhancing preprocessing. (a) The original imagery of this urban district. (b) The ground truth of this region. (c) The prediction results using the Res-U-Net without enhanced preprocessing. (d) The prediction results with enhanced preprocessing.</figDesc><graphic coords="11,100.64,183.39,394.02,74.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Results of the building extraction in the urban district of the Potsdam dataset with and without edge enhancing preprocessing. (a) The original imagery of this urban district. (b) The ground truth of this region. (c) The prediction results using the Res-U-Net without enhanced preprocessing. (d) The prediction results with enhanced preprocessing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. The results of the building extraction in the urban district of the Potsdam dataset with and without the DSM as a channel of the input. (a) The original imagery of the urban district. (b) The DSM of this district. (c) The ground of this region. (d) The prediction results using the Res-U-Net without the DSM as a channel of input. (e) The prediction results corresponding to the input with the DSM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. The results of the building extraction in the urban district of the Potsdam dataset with and without the DSM as a channel of the input. (a) The original imagery of the urban district. (b) The DSM of this district. (c) The ground of this region. (d) The prediction results using the Res-U-Net without the DSM as a channel of input. (e) The prediction results corresponding to the input with the DSM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. The results of the building extraction in the urban district of the Potsdam dataset with and without the NDVI as a channel of the input. (a) The original imagery of the urban district. (b) The NDVI of this district. (c) The ground of this region. (d) The prediction results using the Res-U-Net without the NDVI as a channel of input. (e) The prediction results corresponding to the input with the NDVI.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. The results of the guided filter. (a) The imagery of the urban district. (b) The ground truth corresponding with the imagery. (c) The optimization result with a guided filter with a threshold t = 40. (d) The optimization result with a guided filter with a threshold of t = 90. (e) The optimization result with a guided filter with a threshold of t = 165.</figDesc><graphic coords="13,98.31,569.39,397.66,103.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. The results of the building extraction in the urban district of the Potsdam dataset with and without the NDVI as a channel of the input. (a) The original imagery of the urban district. (b) The NDVI of this district. (c) The ground of this region. (d) The prediction results using the Res-U-Net without the NDVI as a channel of input. (e) The prediction results corresponding to the input with the NDVI.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>18 Figure 10 .</head><label>1810</label><figDesc>Figure 10. The results of the building extraction in the urban district of the Potsdam dataset with and without the NDVI as a channel of the input. (a) The original imagery of the urban district. (b) The NDVI of this district. (c) The ground of this region. (d) The prediction results using the Res-U-Net without the NDVI as a channel of input. (e) The prediction results corresponding to the input with the NDVI.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. The results of the guided filter. (a) The imagery of the urban district. (b) The ground truth corresponding with the imagery. (c) The optimization result with a guided filter with a threshold t = 40. (d) The optimization result with a guided filter with a threshold of t = 90. (e) The optimization result with a guided filter with a threshold of t = 165.</figDesc><graphic coords="13,87.44,565.22,419.75,108.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. The results of the guided filter. (a) The imagery of the urban district. (b) The ground truth corresponding with the imagery. (c) The optimization result with a guided filter with a threshold t = 40. (d) The optimization result with a guided filter with a threshold of t = 90. (e) The optimization result with a guided filter with a threshold of t = 165.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head>Figure 12 .</head><label>12</label><figDesc>Figure 12. The overall accuracy (a) and F1 (b) changing as the threshold increases while optimization by the guided filter.</figDesc><graphic coords="14,121.87,379.24,349.86,168.30" type="vector_box" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_29"><head>Figure 12 .</head><label>12</label><figDesc>Figure 12. The overall accuracy (a) and F 1 (b) changing as the threshold increases while optimization by the guided filter.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_30"><head>Figure 13 .</head><label>13</label><figDesc>Figure 13. The overall accuracy (a) and F1 (b) changing as the size of window increases while optimization by the guided filter.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_31"><head>Figure 13 .</head><label>13</label><figDesc>Figure 13. The overall accuracy (a) and 1 (b) changing as the size of window increases while optimization by the guided filter.</figDesc><graphic coords="15,127.70,264.06,339.06,158.94" type="vector_box" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Measures the average accuracy for the classification, precision, recall as well as F 1 score for buildings and clutter in Potsdam and Vaihingen, respectively.</figDesc><table><row><cell>Dataset</cell><cell>OA</cell><cell>Precision (B)</cell><cell>F 1 (B)</cell><cell>Recall (B)</cell><cell>Precision (C)</cell><cell>F 1 (C)</cell><cell>Recall (C)</cell></row><row><cell>Postdam</cell><cell>0.9691</cell><cell>0.9634</cell><cell>0.9390</cell><cell>0.9158</cell><cell>0.9709</cell><cell>0.9793</cell><cell>0.9878</cell></row><row><cell cols="2">Vaihingen 0.9771</cell><cell>0.9621</cell><cell>0.9515</cell><cell>0.9412</cell><cell>0.9816</cell><cell>0.9850</cell><cell>0.9883</cell></row><row><cell></cell><cell cols="6">Where B stand for buildings, and C represent clutter, and OA represents overall accuracy.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Measures the average accuracy for the classification, precision, recall as well as F1 score for buildings and clutter in Potsdam and Vaihingen, respectively.</figDesc><table><row><cell>Remote Sens. 2018, 10, 144</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>(a) Compared with the results whilst exclude either the DSM or the NDVI for Potsdam dataset. (b) Compared with the results whilst exclude either the DSM or the NDVI for Vaihingen dataset.</figDesc><table><row><cell>Elements</cell><cell>OA</cell><cell cols="6">Precision (B) F1 (B) Recall (B) Precision (C) F1 (C) Recall (C)</cell></row><row><cell>all</cell><cell>0.9691</cell><cell>0.9634</cell><cell>0.9390</cell><cell>0.9158</cell><cell>0.9709</cell><cell>0.9793</cell><cell>0.9878</cell></row><row><cell cols="2">Without DSM 0.9527</cell><cell>0.9606</cell><cell>0.8998</cell><cell>0.8462</cell><cell>0.9483</cell><cell>0.9688</cell><cell>0.9901</cell></row><row><cell cols="2">Without NDVI 0.9652</cell><cell>0.9644</cell><cell>0.9307</cell><cell>0.8993</cell><cell>0.9655</cell><cell>0.9768</cell><cell>0.9883</cell></row><row><cell>Only RGB</cell><cell>0.9425</cell><cell>0.9471</cell><cell>0.8819</cell><cell>0.8251</cell><cell>0.9412</cell><cell>0.9621</cell><cell>0.9838</cell></row><row><cell>Elements</cell><cell>OA</cell><cell cols="6">Precision (B) F1 (B) Recall (B) Precision (C) F1 (C) Recall (C)</cell></row><row><cell>all</cell><cell>0.9771</cell><cell>0.9621</cell><cell>0.9515</cell><cell>0.9412</cell><cell>0.9816</cell><cell>0.9850</cell><cell>0.9883</cell></row><row><cell cols="2">Without DSM 0.9626</cell><cell>0.9341</cell><cell>0,9273</cell><cell>0.9207</cell><cell>0.9725</cell><cell>0.9749</cell><cell>0.9773</cell></row><row><cell cols="2">Without NDVI 0.9552</cell><cell>0.9228</cell><cell>0.9126</cell><cell>0.9026</cell><cell>0.9663</cell><cell>0.9699</cell><cell>0.9737</cell></row><row><cell>Only IRRG</cell><cell>0.9501</cell><cell>0.9181</cell><cell>0.9102</cell><cell>0.9025</cell><cell>0.9618</cell><cell>0.9677</cell><cell>0.9736</cell></row></table><note><p>B stand for buildings and C represent clutter, and OA means overall accuracy.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>(a) Compared with the results whilst exclude either the DSM or the NDVI for Potsdam dataset. (b) Compared with the results whilst exclude either the DSM or the NDVI for Vaihingen dataset.</figDesc><table><row><cell>Elements</cell><cell>OA</cell><cell>Precision (B)</cell><cell>F 1 (B)</cell><cell>Recall (B)</cell><cell>Precision (C)</cell><cell>F 1 (C)</cell><cell>Recall (C)</cell></row><row><cell>all</cell><cell>0.9691</cell><cell>0.9634</cell><cell>0.9390</cell><cell>0.9158</cell><cell>0.9709</cell><cell>0.9793</cell><cell>0.9878</cell></row><row><cell>Without DSM</cell><cell>0.9527</cell><cell>0.9606</cell><cell>0.8998</cell><cell>0.8462</cell><cell>0.9483</cell><cell>0.9688</cell><cell>0.9901</cell></row><row><cell>Without NDVI</cell><cell>0.9652</cell><cell>0.9644</cell><cell>0.9307</cell><cell>0.8993</cell><cell>0.9655</cell><cell>0.9768</cell><cell>0.9883</cell></row><row><cell>Only RGB</cell><cell>0.9425</cell><cell>0.9471</cell><cell>0.8819</cell><cell>0.8251</cell><cell>0.9412</cell><cell>0.9621</cell><cell>0.9838</cell></row><row><cell>Elements</cell><cell>OA</cell><cell>Precision (B)</cell><cell>F 1 (B)</cell><cell>Recall (B)</cell><cell>Precision (C)</cell><cell>F 1 (C)</cell><cell>Recall (C)</cell></row><row><cell>all</cell><cell>0.9771</cell><cell>0.9621</cell><cell>0.9515</cell><cell>0.9412</cell><cell>0.9816</cell><cell>0.9850</cell><cell>0.9883</cell></row><row><cell>Without DSM</cell><cell>0.9626</cell><cell>0.9341</cell><cell>0,9273</cell><cell>0.9207</cell><cell>0.9725</cell><cell>0.9749</cell><cell>0.9773</cell></row><row><cell>Without NDVI</cell><cell>0.9552</cell><cell>0.9228</cell><cell>0.9126</cell><cell>0.9026</cell><cell>0.9663</cell><cell>0.9699</cell><cell>0.9737</cell></row><row><cell>Only IRRG</cell><cell>0.9501</cell><cell>0.9181</cell><cell>0.9102</cell><cell>0.9025</cell><cell>0.9618</cell><cell>0.9677</cell><cell>0.9736</cell></row></table><note><p>B stand for buildings and C represent clutter, and OA means overall accuracy.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Compared with the results of proposed method with others methods on Vaihingen and Potsdam datasets.</figDesc><table><row><cell>Dataset</cell><cell>SegNet</cell><cell>FCN</cell><cell>CNN +</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>RF Mulit-Scale Deep Network [33] CNN + RF + CRF Ours</head><label></label><figDesc></figDesc><table><row><cell cols="2">Vaihingen 0.9078 0.9279</cell><cell>0.9423</cell><cell>0.945</cell><cell>0.943</cell><cell>0.9771</cell></row><row><cell>Potsdam</cell><cell>0.9174 0.9127</cell><cell>0.9303</cell><cell>0.9406</cell><cell>0.9392</cell><cell>0.9691</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 .</head><label>3</label><figDesc>Compared with the results of proposed method with others methods on Vaihingen and Potsdam datasets.</figDesc><table><row><cell>Dataset</cell><cell>SegNet</cell><cell>FCN</cell><cell>CNN + RF</cell><cell>Mulit-Scale Deep Network [33]</cell><cell>CNN + RF + CRF</cell><cell>Ours</cell></row><row><cell>Vaihingen</cell><cell>0.9078</cell><cell>0.9279</cell><cell>0.9423</cell><cell>0.945</cell><cell>0.943</cell><cell>0.9771</cell></row><row><cell>Potsdam</cell><cell>0.9174</cell><cell>0.9127</cell><cell>0.9303</cell><cell>0.9406</cell><cell>0.9392</cell><cell>0.9691</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 .</head><label>3</label><figDesc>Compared with the results of proposed method with others methods on Vaihingen and Potsdam datasets.</figDesc><table><row><cell>Dataset</cell><cell>SegNet</cell><cell>FCN</cell><cell cols="4">CNN + RF Mulit-Scale Deep Network [33] CNN + RF + CRF Ours</cell></row><row><cell cols="3">Vaihingen 0.9078 0.9279</cell><cell>0.9423</cell><cell>0.945</cell><cell>0.943</cell><cell>0.9771</cell></row><row><cell>Potsdam</cell><cell cols="2">0.9174 0.9127</cell><cell>0.9303</cell><cell>0.9406</cell><cell>0.9392</cell><cell>0.9691</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments: This study was financially supported by the National Natural Science Foundation of China (41671400, 41701446, 41401443), National key R &amp; D program of China (No. 2017YFC0602204) and Hubei Natural Science Foundation of China (2015CFA012).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Author Contributions: Yongyang Xu, Zhong Xie proposed the network architecture design and the framework of extracting buildings. Yongyang Xu performed the experiments and analyzed the data. Yongyang Xu, Liang Wu wrote the paper. Zhanlong Chen revised the paper and provided valuable advices for the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflicts of Interest:</head><p>The authors declare no conflict of interest.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Land-Cover Mapping by Markov Modeling of Spatial-Contextual Information in Very-High-Resolution Remote Sensing Images</title>
		<author>
			<persName><forename type="first">G</forename><surname>Moser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Serpico</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Benediktsson</surname></persName>
		</author>
		<idno type="DOI">10.1109/JPROC.2012.2211551</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="631" to="651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Very High Resolution Multiangle Urban Classification Analysis</title>
		<author>
			<persName><forename type="first">N</forename><surname>Longbotham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chaapel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bleiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Padwick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Emery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pacifici</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2011.2165548</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="1155" to="1170" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning to Detect Roads in High-Resolution Aerial Images</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Computer Vision-ECCV 2010-European Conference on Computer Vision</title>
		<meeting>the Computer Vision-ECCV 2010-European Conference on Computer Vision<address><addrLine>Heraklion, Crete, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-09">September 2010</date>
			<biblScope unit="page" from="210" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Tree species classification in the Southern Alps based on the fusion of very high geometrical resolution multispectral/hyperspectral images and LiDAR data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dalponte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bruzzone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gianelle</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.rse.2012.03.013</idno>
	</analytic>
	<monogr>
		<title level="j">Remote Sens. Environ</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="page" from="258" to="270" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A Generalized Image Scene Decomposition-Based System for Supervised Classification of Very High Resolution Remote Sensing Imagery</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">Y</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Benediktsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Remote Sens. 2016, 8, 814</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Random Forest Classification of Wetland Landcovers from Multi-Sensor Data in the Arid Region of Xinjiang</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs8110954</idno>
		<imprint>
			<pubPlace>China</pubPlace>
		</imprint>
	</monogr>
	<note>Remote Sens. 2016, 8, 954. [CrossRef</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Improving image classification in a complex wetland ecosystem through image fusion techniques</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="DOI">10.1117/1.JRS.8.083616</idno>
	</analytic>
	<monogr>
		<title level="j">J. Appl. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">83616</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Road network extraction: A neural-dynamic framework based on deep learning and a finite state machine</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1080/01431161.2015.1054049</idno>
	</analytic>
	<monogr>
		<title level="j">Int. J. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="3144" to="3169" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Building Detection Using Enhanced HOG-LBP Features and Region Refinement Processes</title>
		<author>
			<persName><forename type="first">D</forename><surname>Konstantinidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Stathaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Argyriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Grammalidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="888" to="905" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semantic Segmentation of Small Objects and Modeling of Uncertainty in Urban Remote Sensing Images Using Deep Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kampffmeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Salberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the The IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-07-01">26 June-1 July 2016</date>
			<biblScope unit="page" from="680" to="688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">POL-SAR Image Classification Based on Wishart DBN and Local Spatial Information</title>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2016.2514504</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="3292" to="3308" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Marmanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Galliani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Datcu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Stilla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.01337</idno>
		<title level="m">Classification With an Edge: Improving Semantic Image Segmentation with Boundary Detection</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Optimisation of building detection in satellite images by combining multispectral classification and texture filtering</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0924-2716(98)00027-6</idno>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="50" to="60" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A comparative study of spatial approaches for urban mapping using hyperspectral ROSIS images over Pavia City, northern Italy</title>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1080/01431160802559046</idno>
	</analytic>
	<monogr>
		<title level="j">Int. J. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="3205" to="3221" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Morphological Profiles Based on Differently Shaped Structuring Elements for Classification of Images With Very High Spatial Resolution</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">Y</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Benediktsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Z</forename><surname>Shi</surname></persName>
		</author>
		<idno type="DOI">10.1109/JSTARS.2014.2328618</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="4644" to="4652" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Spectral-spatial classification of hyperspectral images using joint bilateral filter and graph cut based model</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs8090748</idno>
		<imprint/>
	</monogr>
	<note>Remote Sens. 2016, 8, 748. [CrossRef</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Features Off-the-Shelf: An Astounding Baseline for Recognition</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
		<author>
			<persName><surname>Cnn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the 2014 IEEE Conference on Computer Vision and Pattern Recognition Workshops<address><addrLine>Columbus, OH, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06">June 2014</date>
			<biblScope unit="page" from="512" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1311.2524</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06">June 2015</date>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs</title>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><surname>Deeplab</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00915</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Conditional random fields as recurrent neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision<address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12">December 2015</date>
			<biblScope unit="page" from="1529" to="1537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Deep Learning for Remote Sensing</title>
		<author>
			<persName><forename type="first">N</forename><surname>Audebert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Boulch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lagrange</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Le Saux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lefevre</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<pubPlace>France</pubPlace>
		</imprint>
		<respStmt>
			<orgName>ONERA The French Aerospace Lab ; DTIM &amp; Univ. Bretagne-Sud &amp; ENSTA ParisTech: Palaiseau</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Do deep features generalize from everyday objects to remote sensing and aerial scenes domains?</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">A B</forename><surname>Penatti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A D</forename><surname>Santos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops<address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06">June 2015</date>
			<biblScope unit="page" from="44" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep learning for urban remote sensing</title>
		<author>
			<persName><forename type="first">N</forename><surname>Audebert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Boulch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Randrianarivo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Le Saux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ferecatu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lefèvre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Marlet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Urban Remote Sensing Event (JURSE)</title>
		<meeting>the Urban Remote Sensing Event (JURSE)<address><addrLine>Dubai, UAE</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-03">March 2017</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Building detection in very high resolution multispectral data with deep learning features</title>
		<author>
			<persName><forename type="first">M</forename><surname>Vakalopoulou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Karantzalos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Paragios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Geoscience and Remote Sensing Symposium (IGARSS)</title>
		<meeting>the Geoscience and Remote Sensing Symposium (IGARSS)<address><addrLine>Milan, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07">July 2015</date>
			<biblScope unit="page" from="1873" to="1876" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning Hierarchical Features for Scene Labeling</title>
		<author>
			<persName><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2012.231</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1915" to="1929" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Results and implications of a study of fifteen years of satellite image classification experiments</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">G</forename><surname>Wilkinson</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2004.837325</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="433" to="440" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Effective semantic pixel labelling with convolutional networks and conditional random fields</title>
		<author>
			<persName><forename type="first">S</forename><surname>Paisitkriangkrai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sherrah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Janney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V.-D</forename><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops<address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06">June 2015</date>
			<biblScope unit="page" from="36" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Semantic Labeling of Aerial and Satellite Imagery</title>
		<author>
			<persName><forename type="first">S</forename><surname>Paisitkriangkrai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sherrah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Janney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V D</forename><surname>Hengel</surname></persName>
		</author>
		<idno type="DOI">10.1109/JSTARS.2016.2582921</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2868" to="2881" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for building and road extraction: Preliminary results</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)</title>
		<meeting>the 2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-07">July 2016</date>
			<biblScope unit="page" from="1591" to="1594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Classification for High Resolution Remote Sensing Imagery Using a Fully Convolutional Network</title>
		<author>
			<persName><forename type="first">G</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs9050498</idno>
		<imprint/>
	</monogr>
	<note>Remote Sens. 2017, 9, 498. [CrossRef</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks for Large-Scale Remote-Sensing Image Classification</title>
		<author>
			<persName><forename type="first">E</forename><surname>Maggiori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tarabalka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Charpiat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Alliez</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2016.2612821</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="645" to="657" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Semantic Segmentation of Earth Observation Data Using Multimodal and Multi-scale Deep Networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Audebert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Saux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lefèvre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Computer Vision-ACCV 2016</title>
		<meeting>the Computer Vision-ACCV 2016<address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-11">November 2016</date>
			<biblScope unit="page" from="180" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Flownet: Learning optical flow with convolutional networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Van Der Smagt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision<address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12">December 2015</date>
			<biblScope unit="page" from="2758" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Semantic Segmentation of Aerial Images with an Ensemble of CNSS</title>
		<author>
			<persName><forename type="first">D</forename><surname>Marmanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Galliani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Datcu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Stilla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Ann. Photogramm. Remote Sens. Spat. Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="473" to="480" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Semantic image segmentation with deep convolutional nets and fully connected CRFs</title>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7062</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Fully Convolutional Networks for Dense Semantic Labelling of High-Resolution Aerial Imagery</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sherrah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02585</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Morphological attribute profiles for the analysis of very high resolution images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dalla Mura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Benediktsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Waske</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bruzzone</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2010.2048116</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="3747" to="3762" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Color Spaces, and Boosting: New Insights on Semantic Classification of Remote Sensing Images</title>
		<author>
			<persName><forename type="first">P</forename><surname>Tokarczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Walk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName><surname>Features</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="280" to="295" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The Cityscapes Dataset for Semantic Urban Scene Understanding</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06">June 2016</date>
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Dense Semantic Labeling of Subdecimeter Resolution Images With Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Volpi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tuia</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2016.2616585</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="881" to="893" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-07-01">26 June-1 July 2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Neural Information Processing Systems</title>
		<meeting>the International Conference on Neural Information Processing Systems<address><addrLine>Lake Tahoe, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-12">December 2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Transferring Deep Convolutional Neural Networks for the Scene Classification of High-Resolution Remote Sensing Imagery</title>
		<author>
			<persName><forename type="first">F</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs71114680</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="14680" to="14707" />
		</imprint>
	</monogr>
	<note>Remote Sens</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Benchmarking classification of earth-observation data: From learning explicit features to convolutional networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lagrange</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Saux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Beaupère</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Boulch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chan-Hon-Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Herbin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Randrianarivo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ferecatu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Geoscience and Remote Sensing Symposium</title>
		<meeting>the Geoscience and Remote Sensing Symposium<address><addrLine>Milan, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07">July 2015</date>
			<biblScope unit="page" from="4173" to="4176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional Networks for Biomedical Image Segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<meeting>the International Conference on Medical Image Computing and Computer-Assisted Intervention<address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-10">October 2015</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Feature Pyramid Networks for Object Detection</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.03144</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Guided Image Filtering</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2012.213</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1397" to="1409" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Multimodal medical image fusion using modified fusion rules and guided filter</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pritika; Budhiraja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computing, Communication &amp; Automation</title>
		<meeting>the International Conference on Computing, Communication &amp; Automation<address><addrLine>Noida, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05">May 2015</date>
			<biblScope unit="page" from="1067" to="1072" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Use of the Stair Vision Library within the ISPRS 2D Semantic Labeling Benchmark (Vaihingen)</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gerke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<pubPlace>The Netherlands</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Twente: Enschede</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Gated Convolutional Neural Network for Semantic Segmentation in High-Resolution Images</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs9050446</idno>
		<imprint/>
	</monogr>
	<note>Remote Sens. 2017, 9, 446. [CrossRef</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
