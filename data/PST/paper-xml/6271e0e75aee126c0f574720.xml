<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SIMD 2 : A Generalized Matrix Instruction Set for Accelerating Tensor Computation beyond GEMM</title>
				<funder ref="#_pc4u7d2 #_FbD3EHn">
					<orgName type="full">National Science Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yunan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Riverside</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Po-An</forename><surname>Tsai</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">NVIDIA Research</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Hung-Wei</forename><surname>Tseng</surname></persName>
							<email>htseng@ucr.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Riverside</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SIMD 2 : A Generalized Matrix Instruction Set for Accelerating Tensor Computation beyond GEMM</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3470496.3527411</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Matrix-multiplication units (MXUs) are now prevalent in every computing platform. The key attribute that makes MXUs so successful is the semiring structure, which allows tiling for both parallelism and data reuse. Nonetheless, matrix-multiplication is not the only algorithm with such attributes. We find that many algorithms share the same structure and differ in only the core operation; for example, using add-minimum instead of multiply-add. Algorithms with a semiring-like structure therefore have potential to be accelerated by a general-purpose matrix operation architecture, instead of common MXUs.</p><p>In this paper, we propose SIMD 2 , a new programming paradigm to support generalized matrix operations with a semiring-like structure. SIMD 2 instructions accelerate eight more types of matrix operations, in addition to matrix multiplications. Since SIMD 2 instructions resemble a matrix-multiplication instruction, we are able to build SIMD 2 architecture on top of any MXU architecture with minimal modifications. We developed a framework that emulates and validates SIMD 2 using NVIDIA GPUs with Tensor Cores. Across 8 applications, SIMD 2 provides up to 38.59? speedup and more than 10.63? on average over optimized CUDA programs, with only 5% of full-chip area overhead.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Matrices are essential data structures at the core of scientific computing, data and graph analytics as well as artificial intelligence (AI) and machine learning (ML) workloads. Due to the stagnating general-purpose processor performance scaling and memory-wall problem <ref type="bibr" target="#b71">[72]</ref>, a recent trend of efficient computing on matrices focuses on building hardware accelerators. Famous examples include NVIDIA's Tensor Cores <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b52">53]</ref>, Google's Tensor Processing Units (TPUs) <ref type="bibr" target="#b26">[27]</ref>, and the recent IBM Power 10 MMA unit <ref type="bibr" target="#b66">[67]</ref>. The demand of matrix-multiplication accelerators is so strong that the upcoming generations of Intel and ARM CPU processors will also provide matrix extensions and integrate MXUs <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>Compared with conventional SIMD processors (e.g., GPGPUs), MXUs are more efficient in general matrix multiplication (GEMM) for two reasons. First, GEMMs are easy to parallelize. Each MXU can take tiles from input matrices and generate an output tile, and multiple MXUs can work together to form a larger GEMM accelerator, temporally or spatially. Second, GEMMs have a higher compute intensity than vector operations (e.g., saxpy <ref type="bibr" target="#b0">[1]</ref>). Such compute intensity alleviates the memory-wall issue in modern throughputoriented SIMD processors and allows architects to simply add more compute throughput to scale the performance of MXU with the same on-chip and off-chip bandwidth limitation.</p><p>Besides GEMM, a wide-spectrum of problems, including all-pairshortest-path, minimum spanning tree as well as graph problems, have matrix-based algorithms/solutions share the same computation pattern. They all follow a semiring-like structure -? ? (? ? ?), where the problem generates results (or intermediate results) by performing two-step operations (? and ?) on three matrix inputs (?, ? and ?). For example, dynamic programming methods for all-pair-shortest-path problems using All Pairs Bellman-Ford or Floyd-Warshall algorithms can be expressed in a semiring-like structure through having the ? operator represent the addition-based distance update operations <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b61">62]</ref>, and the minimum operation replaces ? operator.</p><p>However, as modern MXUs are highly specialized for just GEMM or convolutions, programmers must perform non-trivial algorithm optimizations (e.g., mapping matrix multiplications to convolutions <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b39">40]</ref>) to tailor these applications for supported matrix operations. Besides, the resulting program may still under-utilize MXUs as mapping the original set of matrix operations to GEMMs that require changing the dataflow or data layout of the program before the actual computation can start. Finally, for problems including the dynamic programming algorithms, existing MXUs cannot provide native support for the required ? and ? operations and have to fallback to SIMD processors (e.g., CUDA cores), even though these algorithms share the semiring-like structure with GEMM.</p><p>To address these issues, this paper presents the SIMD 2 architecture to enable more efficient matrix operations for a broader set of applications. SIMD 2 provides a wider set of matrix-based operations that naturally fit the application demands and abstract these functions through an appropriate set of instructions. SIMD 2  reuses and extends the function of existing MXUs and data paths to minimize the overhead in supporting additional matrix operations.</p><p>The SIMD 2 architecture brings the following benefits in accelerating matrix applications. First, programmers or compilers can leverage the richer set of instructions that naturally maps to common matrix operations without sophisticated code transformations, which facilitates matrix-based programming. By performing more matrix operations with a minimum number of instructions, the SIMD 2 instructions further reduce the control and data movement overhead over conventional SIMD instructions by exposing a matrix-based abstraction.</p><p>As an initial step in this direction, our SIMD 2 architecture introduces eight more types of instructions for matrix computation, including (1) min-plus, (2) max-plus, (3) min-mul, (4) max-mul, <ref type="bibr" target="#b4">(5)</ref> min-max, (6) max-min, <ref type="bibr" target="#b6">(7)</ref> or-and, and (8) plus-norm, in addition to existing mul-plus instructions. Similar to existing hardwareaccelerated GEMM operations, these instructions also take tiles of matrices as inputs and update the resulting output tile. Therefore, these instructions can easily share the same infrastructure of an existing MXU, including instruction front-end, memory, and register files. As these SIMD 2 instructions all follow the same data flow and computation pattern, they can also share the operand delivery structure and simply require a modified data path to perform new operations.</p><p>As the necessary hardware support of SIMD 2 resembles existing MXUs, a SIMD 2 architecture can be implemented on top of any matrix-multiplication accelerators, either in standalone applicationspecific integrated circuit (ASICs) or as processing elements in CPUs or GPUs. This paper presents SIMD 2 in the form of extending GPU architectures as this allows us to leverage existing interface/frontend of GPU programming models and mature software stacks, and focus on the benefits of the SIMD 2 model. On the other hand, since modern matrix-based applications still rely on non-matrix operations to complete all computation tasks, this architecture also offers better performance by avoiding data movements across system interconnects and taking advantage of existing high-bandwidth memory hierarchy in GPUs.</p><p>We evaluate the proposed SIMD 2 architecture and hardware units through software emulation and hardware synthesis. We also made the emulation framework and hardware design publicly available through a web-hosted repository <ref type="foot" target="#foot_0">1</ref> . We demonstrate 8 applications that can naturally leverage these operations in their core algorithms. With the proposed SIMD 2 MXUs, these applications enjoy up to 38.59? speedup and more than 10.63? speedup on average. Synthesis results show that over a conventional MXU that supports only multiply-and-accumulate, SIMD 2 MXU adds 69% area overhead while supporting 8 different operations under the same clock period. This area overhead is 5% of the total chip area according to public die shot photos.</p><p>In presenting the SIMD 2 architecture, this paper makes the following contributions.</p><p>(1) It identifies a set of matrix applications with semiring-like structure and reveals strong potential in performance gain if SIMD 2 support is available in hardware.</p><p>(2) It proposes SIMD 2 architecture, programming model, instructions, and hardware units to accelerate semiring-like applications.</p><p>(3) It evaluates the performance benefit of the proposed SIMD 2 architecture, and the cost of SIMD 2 hardware units over a common MXU to demonstrate the opportunity of a SIMD 2 programming paradigm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">THE CASE FOR SIMD 2</head><p>The motivation of proposing SIMD 2 for matrix and tensor problems comes from two sources-A family of matrix algorithms that share the same semiring pattern in computation, and the emergence of GEMM accelerators designed around the semiring pattern. Both motivate the need and the possibility of a single umbrella that covers a large set of matrix algorithms to facilitate efficient use of hardware components. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The Commonality among Matrix Problems</head><p>Matrices provide a natural mathematical expression for linear systems, graphs, geometric transformations, biological datasets, and so on. In addition to data representations, many applications using matrices as inputs and outputs also share the same algebraic structure in their algorithms. This algebraic structure contains two binary operators, ? and ?. The ? operator satisfies properties analogous to addition. The ? operator is associative and typically has a multiplicative identity element analogous to multiplication. In other words, a large set of matrix algorithms can be formalized as:</p><formula xml:id="formula_0">? = ? ? (? ? ?)</formula><p>where ?, ?, ? are input matrices, ? is the output, as well as the two customized operators, ? and ?.</p><p>The above algebraic structure is similar to a semiring, (?, ?, ?), which contains a set ? equipped with two binary operators, ? and ?. The ? operator in a semiring satisfies properties analogous to addition. The ? operator in a semiring has more restrictions as it must be associative, distributive as well as having a multiplicative identity element. Since some algebraic structure of matrix problems is similar, but not mathematically identical to semirings, we use the term semiring-like structure when referring to this identified algebraic structure.</p><p>General matrix multiplication (i.e., GEMM) is one classic example that follows this structure. To simplify the discussion, we use square matrices in the following examples. Let ? be an ? by ? matrix and ?(?, ?) represent the (?, ?)-entry of ?. Then, there also exists two other ? by ? matrice, ? and ?, where ? (?, ?) and ? (?, ?) represent the (?, ?)-entries of ? and ?, respectively. General matrix multiplication consists of a set of computation for the (?, ?)-entry of the resulting matrix ?, ? (?, ?), where ? (?, ?) = ? (?, ?) + ? ?=0 ?(?, ?) ? ? (?, ?). Figure <ref type="figure" target="#fig_0">1</ref>(a) illustrates the code example for matrix multiplication with ? ? ? matrices. The matrix multiplication therefore has a semiring-like structure where the ? operates as pair-wise addition for each pair of elements sharing the same coordinate ?, ? on each side of the operator, matrix ? and the result of ? ??. The ? operates as calculating the value of the (?, ?)-entry in the result matrix ? as ? ?=1 ?(?, ?) ? ? (?, ?) for each ?, ?, ?. With the aforementioned common form, a matrix multiplication problem is ? = ? + ? ? ?.</p><formula xml:id="formula_1">1 f o r ( i = 0 ; i &lt; N ; i + + ) 2 f o r ( j = 0 ; j &lt; N ; j + + ) 3 f o r ( k = 0 ; k &lt; N ; k + + ) { 4 D[ i ] [ j ] = C[ i ] [ j ] 5 + A[ i ] [ k ] * B [ k ] [ j ] ; 6 } 7 ( a ) 1 f o r ( s r c = 0 ; s r c &lt; N ; s r c + + ) 2 f o r ( d s t = 0 ; d s t &lt; N ; d s t + + ) 3 f o r ( k = 0 ; k &lt; N ; k + + ) { 4 D[ s r c ] [ d s t ] = min ( C[ s r c ] [ d s t ] , 5 ( C[ s r c ] [ k ] + A[ k ] [ d s t ] ) ) ; 6 } 7 ( b )</formula><p>Besides matrix multiplications, a wide spectrum of algorithms, especially those for solving graph problems or algorithms that leverage dynamic programming, can also be formulated as a structure similar to matrix multiplications by customizing the ? and ? operators. For example, Figure <ref type="figure" target="#fig_0">1</ref>(b) shows how the inner loops of allpiars Bellman-Ford algorithm <ref type="bibr" target="#b9">[10]</ref> for all-pairs-shortest-path (APSP) problem is similar to the semiring-like algebraic structure as GEMM (Figure <ref type="figure" target="#fig_0">1(a)</ref>). Each iteration in Line 4-5 of Figure <ref type="figure" target="#fig_0">1</ref>(b) performs the computation of ? (?, ?) = ???{? (?, ?), ??? ? ?=0 [? (?, ?) + ?(?, ?)]}, where each ? (?, ?), ? (?, ?), or ?(?, ?) represents the (?, ?)-entry of matrix ?, ? or ?, respectively. The ? matrix is the result of temporal all-pairs distances after the iteration, ? is the result from the last iteration, and ? is the original adjacency matrix. Therefore, we can leverage the semiring-like structure to express the all-pairs Bellman-Ford algorithm for the APSP problem by replacing the ? operator with ??? and the ? operator with +. The core loops become ? = ? ??? (? + ?).</p><p>In addition to the APSP problem, there are other algorithms amenable to such a semiring-like structure. Table <ref type="table" target="#tab_0">1</ref> illustrates a set of problems and their corresponding customizations of ? and ? operators in their algorithms.</p><p>Though a semiring-like structure can serve as a generic programming paradigm for matrix problems, conventional approaches in solving matrix problems require the programmers to transform matrix data into lower-ranked data representations (e.g., scalar numbers or vectors) and redesign algorithms on these data representations to fulfill the programming paradigm that modern CPUs and GPUs can support. Performance optimizations on programs solving these problems is especially challenging as they are intensive in both computation and data accesses on conventional processor architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Hardware Support for Semiring-like Structure in GEMMs Accelerators</head><p>The semiring-like algebraic structure is the key enabler behind modern tensor accelerators, like MXUs for GEMMs, which improves over conventional SIMD processors. From a hardware design point of view, conventional SIMD architectures, shown in Figure <ref type="figure">2</ref>, are bottlenecked by the vector register file bandwidth. Such data transfer bottleneck (von Neumann bottleneck <ref type="bibr" target="#b69">[70]</ref>) limits how many compute units (ALUs) can be fed by the on-chip memory  MXUs, instead, leverage the semiring-like algebraic structure to break such bottleneck. Figure <ref type="figure" target="#fig_1">3</ref> shows an example implementation of MXU, modeled after the matrix unit in TPUs <ref type="bibr" target="#b26">[27]</ref>. In this MXU example, one input matrix is broadcast to multiple ALUs because of the intrinsic data reuse opportunities in algorithms with a semiring-like structure. The output matrix also leverages the structure (associative) and is accumulated across multiple ALUs before being stored into the output matrix buffer. With the same 4-wide memory structure, we can now supply data to 16 ALUs. More importantly, since the computation complexity is ? (? 3 ), and the data transfer is ? (? 2 ) in semiring-like algorithm, the number of ALUs can scale much more than the on-chip memory bandwidth, alleviating the memory wall issue.</p><p>As a result, modern MXUs are designed around the semiring-like structure, instead of optimizing the ALUs for multiply-add. The programming model of these MXUs also leverages the nature of the algorithm to perform work partitioning and tiling to execute a larger GEMM with multiple MXUs in a system <ref type="bibr" target="#b51">[52]</ref> or across systems <ref type="bibr" target="#b25">[26]</ref>. For example, the wmma API for NVIDIA Tensor Core works at the sub-tile granularity (e.g., 16x16), and programmers can combine multiple wmma calls to merge sub-tile into the full problem.</p><p>Our insight is that supporting a wide range of semiring-like algorithms requires minimal changes on top of any systems with GEMM accelerators. It is clear that the ALU in Figure <ref type="figure" target="#fig_1">3</ref>   <ref type="figure">4</ref>: The high-level architecture of how SIMD 2 units are integrated in GPU systems and the design of an SIMD 2 unit.</p><formula xml:id="formula_2">? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? From RegFile To RegFile ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? L2/DRAM Reduction Tree (a) (b) (c) Figure</formula><p>to the hardware support (broadcast and accumulate) for a semiringlike structure. For example, if we enhance the ALU in Figure <ref type="figure" target="#fig_1">3</ref> to support ??? -???????, then the same MXU architecture can now be used to accelerate solving APSP. That is, the recent development of MXUs for GEMM has laid the ground of supporting semiring-like algorithms, and with a better abstraction and hardware support, many more matrix algorithms can be accelerated. This motivates us to propose and design SIMD 2 , a new programming paradigm and architecture for semiring-like algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SIMD 2 ARCHITECTURE</head><p>We propose the SIMD 2 ISA to efficiently support matrix algorithms beyond GEMMs. SIMD 2 provides a programming paradigm and an instruction set to reflect the natural semiring-like structure in solving these matrix problems. The hardware units for SIMD 2 instructions extend existing MXU to support the proposed programming paradigm. This section will introduce both.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The SIMD 2 hardware architecture</head><p>Like GEMM accelerators, SIMD 2 architecture can be implemented as a standalone processor that contains SIMD 2 units only, or functional units embedded with general-purpose scalar/vector processor cores to share the same instruction front-end. In this work, we chose the latter design and prototype SIMD 2 architecture on a GPU as Figure <ref type="figure">4</ref> shows. Specifically, we build on top of the NVIDIA SM architecture <ref type="bibr" target="#b4">[5]</ref>, which integrates Tensor Core as part of the subcore in a GPU SM. The resulting high-level architecture resembles GPU SM with Tensor Cores <ref type="bibr" target="#b58">[59]</ref> as the SIMD 2 units implementing SIMD 2 instructions are part of a streaming multiprocessor, but the rest of the architectural components (front-end, memory-subsystem, etc.) are shared with conventional GPU cores.</p><p>The SIMD 2 unit in Figure <ref type="figure">4</ref>(c) extends conventional MXUs to use different ? and ? operators. Each SIMD 2 unit can perform an SIMD 2 arithmetic instruction using ? operation on fixed-size matrix tiles (e.g., 4x4 in Figure <ref type="figure">4(c)</ref>) and produce an output matrix by reducing the result from ? operation with the ? operator. Unlike tensor cores that only support multiply and accumulation, the ? ALU supports multiply, min/max, add/and, and L2 dist, and the ? ALU supports add, min/max, or, and subtract. Both ALUs are configured by decoding SIMD 2 instructions, as shown in Figure <ref type="figure" target="#fig_2">5</ref>.</p><p>We chose to build SIMD 2 architecture on top of GPUs for the following reasons. First, since matrix operations just serve as the core computation in matrix applications, applications typically rely on scalar or vector processors to preprocess or postprocess matrix data structures. Collocating SIMD 2 units with other processing elements enables efficient and fine-grained data exchange and synchronization among heterogeneous computing units. Second, GPU's memory architecture design is more bandwidth-oriented and serves better for the purpose since each SIMD 2 unit would consume/produce large amounts of data at once. Finally, there already exists Tensor Cores in NVIDIA's GPU architecture that allow us to leverage as a baseline design and an emulation framework. Alternatively, we have also explored implementing the SIMD 2 unit by building a dedicated hardware unit for each semiring-like algorithm. For example, in addition to the MXU for GEMM, we can add a hardware unit for min-add, another unit for add-norm, and so on. Nonetheless, this design introduces 300% area overhead (See Section 6.1) to the GEMM-only MXU, which is &gt; 4? of the overhead introduced by the combined design in Figure <ref type="figure">4</ref>.</p><p>While we chose GPUs as the baseline system, building an SIMD 2 architecture on other GEMM-based accelerators, such as TPUs <ref type="bibr" target="#b26">[27]</ref>, should be straightforward and low overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The SIMD 2 ISA</head><p>The SIMD 2 instruction extension builds on top of the warp-level matrix-multiply-accumulate (wmma <ref type="bibr" target="#b53">[54]</ref>) instructions for GPUs and extends it to support new arithmetic instructions. Table <ref type="table" target="#tab_3">2</ref> lists these SIMD 2 instructions.</p><p>The load instruction moves a chunk of data from the 1D shared memory address space as a fixed-size (16x16) matrix to the perthread register file. Like the wmma abstraction, each thread in the warp stores part of the matrix in the register file and contributes  Fill the target matrix with given value.</p><p>simd2::loadmatrix(simd2::matrix, source, ld) Load value from source memory location to the target matrix, load with the step of leading dimension. simd2::mmo(simd2::matrix, simd2::matrix, simd2::matrix, simd2::matrix, simd2::opcode) Performs the matrix-matrix operation with given opcode.</p><p>simd2::storematrix(target, simd2::matrix, ld) Store value to source memory location from the target matrix, store with the step of leading dimension.</p><p>to the whole warp-level operation. The store instruction instead moves the matrix segments in the register file back to the 1D shared memory address space.</p><p>In our implementation, we assume input operands are always in 16-bit, half-precision floating-point format (fp16), while the output data is always in 32-bit, single-precision floating point format (fp32). While supporting other formats (e.g., int8) is possible, for many algorithms, we find fixed-precision format cannot converge to the same result as baseline fp32 implementations without SIMD 2 instructions.</p><p>For the arithmetic operations, we introduced eight more ?-? ops, in addition to the classic matrix-multiply-accumulate (mma). These nine instructions map to the frequently used matrix problem patterns in Table <ref type="table" target="#tab_0">1</ref>. The SIMD 2 arithmetic instruction shares the same register file as the vector processor, and uses arguments that specify register locations of input and output matrices. The latency of each SIMD 2 instructions depends on the actual hardware implementation of the SIMD 2 unit, and in our implementation, we provision the SIMD 2 unit to be the same throughput as the conventional MXUs so that all SIMD 2 arithmetic instructions have the same latency.</p><p>Similar to our changes for hardware architecture, we expect adding the SIMD 2 instructions to other ISAs that already support GEMMs, such as Intel AMX <ref type="bibr" target="#b22">[23]</ref>, to be straightforward. These matrix extensions already support matrices as input or output operands and provide data movement instructions for matrices (load/store matrix). SIMD 2 simply adds more arithmetic instruction on top of them. We align our SIMD 2 design point with modern GPU architectures to facilitate our evaluation, but this is not fundamental.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">PROGRAMMING MODEL</head><p>The SIMD 2 units in our proposed architecture can perform matrix operations on a set of predefined matrix shapes and data types. Therefore, the native programming interface reflects the abstraction by which these SIMD 2 units expose through the SIMD 2 ISA. To further facilitate programming at the application level, the framework can provide higher-level library functions that decouple the programmability from architecture-dependent parameters.</p><p>Table <ref type="table" target="#tab_4">3</ref> summarizes the available functions from SIMD 2 's lowlevel programming interface. Each of these functions maps directly to a set of instructions that Section 3.2 describes. The exemplary programming interface resembles the C++ warp matrix operations that NVIDIA's Tensor Cores use to smooth the learning curve, but not a restriction from the SIMD 2 architecture.</p><p>Since the low-level interface reflects the architecture of SIMD 2 units, these functions must operate on a set of matrix shapes and data types that the underlying SIMD 2 hardware natively supports. The program needs to first declare the desired matrix shapes and reserve the register resources for input matrices using the simd2::matrix function. Then, the program can load input matrices into these reserved resources using the simd2::loadmatrix function or set values using the simd2::fillmatrix function. The simd2::mmo function receives arguments describing the desired SIMD 2 operation to perform on the input matrices and the location of the destination matrix. After the code finishes necessary computation on these matrices, the simd2::storematrix can reflect the updated values to a memory location. In case the source dataset does not fit the supported formats, the program typically needs to explicitly To facilitate programming and alleviate the burden of programmers, our framework provides a set of high-level functions as an alternative programming interface. Each maps to a specific type of SIMD 2 arithmetic operations. These functions are essentially composed using the aforementioned low-level functions. In contrast to the low-level interface with limitations on inputs, these high-level functions allow the programmer to simply specify the memory locations of datasets and implicitly handle the tiling/partitioning of datasets and algorithms. Figure <ref type="figure">6</ref> provides an example code that implements a high-level interface function that solves the min-plus matrix problems. The compute kernel starts by identifying the logical SIMD 2 unit of the instance itself is occupying (Lines 6-7). The compute kernel then allocates resources on the SIMD 2 units (Lines 9-11). The code then loads the current partial result of the target tile into one of the allocated matrix storage (Line 13). The following for-loop (Lines 15-21) loads different pairs of tile matrices from the raw input (Lines 17-18) and performs min-plus operations (Line 20) on these tile matrices together with tile loaded in Line 13.</p><formula xml:id="formula_3">1 void s i m d 2 _ m i n p l u s ( h a l f * A , h a l f * B , 2 f l o a t * C , f l o a t * D , 3 i n t m, i n t n , i n t k ) { 4 / /</formula><p>To use the compute kernel from Figure <ref type="figure">6</ref> or the low-level SIMD 2 interface, the programming model still requires a host program to control the workflow, coordinate the computation on various types of processors and move datasets among memory locations on heterogeneous computing devices. Figure <ref type="figure">7</ref> shows an example code that solves the all pair shortest path problem using the All-pairs Bellman Ford algorithm. As SIMD 2 units are auxiliary computing resources to a GPU, the program code will need to explicitly allocate GPU device memory (Line 4-10) and move data to the allocated space before invoking the high-level simd2_minplus function that Figure <ref type="figure">6</ref>  Figure <ref type="figure">7</ref>: CUDA kernel implenmentation of APSP using SIMD 2 API of All-pairs Bellman Ford algorithm would require ? iterations of Line 14. The na?ve implementation assumes the diameter of the graph is always the same as the number of vertices, the worst case scenario. However, the diameter of a real-world graph is way lower than that and a majority of iterations in Line 14 repeatedly generate identical results. Therefore, the implementation in Figure <ref type="figure">7</ref> added a convergence check (i.e., the check_convergence function call) in Line 15 to compare if any element in the result matrix changes from the last iteration. If the result remains the same, the algorithm can terminate earlier. The check_convergence (Line 15) is a pure GPU kernel. Because both SIMD 2 units and conventional GPU cores share the same device memory and registers, the program does not need additional data movements between Line 14 and Line 15.</p><p>In Figure <ref type="figure">7</ref>, we use All-pairs Bellman Ford algorithm as the inputs of SIMD 2 computation in this algorithm are easier to understand. In practice, the Leyzorek's Algorithm can solve APSP problem with fewer SIMD 2 operations <ref type="bibr" target="#b34">[35]</ref>. Leyzorek's Algorithm still uses SIMD 2 , but computes ? = ? ? (? ? ?) in Line 14 instead. In this way, Leyzorek's Algorithm only requires ??|? | iterations to solve an APSP problem in the worst case scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTAL METHODOLOGY</head><p>As SIMD 2 promotes matrix-based algorithms, the SIMD 2 -ized implementations of our benchmark applications may use different algorithms compared to their state-of-the-art implementations, typically using vectorized or scalar-based algorithms, on alternative platforms. Therefore, we designed a framework that allows us to validate the correctness of SIMD 2 -ized programs and emulate the performance of SIMD 2 -ized programs with or without SIMD 2 hardware acceleration presented. This section will describe these aspects in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Emulation framework</head><p>To evaluate SIMD 2 , we developed a framework that evaluates the correctness and performance for each program under test on top of a testbed using a state-of-the-art GPU architecture.  <ref type="figure">8</ref> illustrates the workflow of our process in evaluating applications. For baseline applications, we executed each application directly on the hardware platform without any modification to their source code, datasets and invoked library functions. For SIMD 2 -ized applications, their implementations leverage semiring-like algorithms using the programming model and SIMD 2 API functions described in Section 4. The evaluation framework takes three types of inputs: (1) the compiled program and command line arguments, (2) the dataset used in the baseline application, and (3) the output of the baseline application with the input dataset and command line arguments. Once the emulation framework receives these three sets of inputs, the emulation framework can dynamically change the linked library that implements SIMD 2 API functions to perform (1) correctness validation by using a backend that leverages conventional vector processors and compare the output with the output that the baseline version produced, or (2) performance emulation by using a backend that generates instructions to Tensor Cores residing on the hardware platform. The following paragraphs will describe the correctness validation and performance emulation process in detail.</p><p>Correctness validation. In this work, we need to validate correctness in addition to performance emulation for the following reasons. First, as we need to alter the compute kernels to efficiently use SIMD 2 units and in many cases, using a different algorithm (e.g., Semiring-based vs. Kruskal's Algorithm in Minimum Spanning Tree problems), we need to verify if the change of implementation still delivers the same outcome as the baseline implementation. Second, as existing hardware accelerators only support MMA operations that cannot generate the correct output for other SIMD 2 operations this paper proposes to extend, we need to verify if implemented semiring-based algorithms can generate the desired output after mapping the computation into the proposed SIMD 2 units. Finally, this process can help collect the statistics regarding the total amount of various matrix operations and provide the input for performance emulation.</p><p>During the validation process, we linked the backend of the SIMD 2 programming interface to a library that we extended from cuASR <ref type="bibr" target="#b23">[24]</ref>. This library implements exactly the same functionality as the proposed low-level SIMD 2 functions, except that the library can simply leverage CUDA cores through NVIDIA's highperformance CUTLASS library, but not use Tensor Cores. When implementing low-level SIMD 2 functions for validation purposes, we carefully partitioned the inputs and outputs to fit the exact shape of matrix inputs and outputs of proposed SIMD 2 units (i.e., the input/output sizes of each Tensor Core in our testbed) when invoking corresponding SIMD 2 function calls. We also used reduced/mixed precision inputs/outputs to match the data types that our SIMD 2 units support. Therefore, the validation process can help us access the accuracy of SIMD 2 units. For each program under test, we can optionally count the number of iterations, threads, and low-level SIMD 2 function calls that are necessary to finish running the program and compare each program's output with its state-of-the-art implementation on the alternative architecture.</p><p>Performance emulation. The design of SIMD 2 allows this work to leverage existing Tensor Cores that are available on the GPU of our emulation hardware for exact performance evaluation for the two main reasons. First, adding SIMD 2 instructions do not increase the timing of an existing MMA unit (e.g., a Tensor Core) as Section 6.1 reports. Second, the low-level instructions, register files, memory hierarchy as well as the interaction with the host machine can be made almost identical to those of Tensor Cores, except for the exact output after each computation.</p><p>When performing performance emulation, the framework links the backend of the low-level SIMD 2 API library that implements through using equivalent Tensor Cores' WMMA low-level interface. As this paper simply proposes to extend the ALU functions of Tensor Cores, the memory operations remain the same in SIMD 2 units compared with Tensor Cores. Therefore, each simd2::loadmatrix and simd2::storematrix invocation are identical in its counterpart in CUDA's WMMA API. However, since the state-of-the-art Tensor Cores can only perform MMA operations, the performance emulation backend library maps each invocation of simd2::mmo to a CUDA's WMMA::mma function call on the same size of inputs. This is also the main reason why the performance emulation backend cannot produce correct/meaningful computation outcomes. The performance emulation process can optionally receive statistics from the corresponding validation process to compare if the performance emulation backend generates the exact amount of simd2 and WMMA operations as desired. This performance emulation methodology is similar with prior work in extending Tensor Cores <ref type="bibr" target="#b10">[11]</ref> to support different precisions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Applications</head><p>To demonstrate the performance of SIMD 2 , we ran two types of workloads on the aforementioned evaluation framework. The first type is a set of microbenchmark workloads that only iteratively invoke SIMD 2 functions and accept synthetic datasets to help us to understand the pure performance gain of SIMD 2 instructions over alternative implementations. The other is a set of full-fledged benchmark applications where each program contains not only SIMD 2 functional, but also interacts with other types of processors to complete the tasks. These benchmark applications can accept real-world datasets and generate meaningful outputs accordingly for us to assess the quality of results if appropriate.</p><p>For each workload, we evaluate three implementations.</p><p>State-of-the-art GPU baseline. This version of code serves as the baseline of our workloads. We tried our best to collect implementations from publicly available open-source code hosting websites and select the best-performing implementation on our testbed as the state-of-the-art baseline version for each workload. These implementations simply leverage CUDA cores, but not Tensor Cores to accomplish their tasks. In fact, without a work like SIMD 2 , none of the selected benchmark can leverage Tensor Cores due to the limited MMA functions available on such hardware units. SIMD 2 in CUDA cores. This version of code serves as another baseline of our workloads. This set of programs implement SIMD 2ized algorithms only using CUDA cores, but not Tensor Cores. Our implementations try to leverage the highly optimized functions from cuASR or CUTLASS whenever appropriate. Different from backend functions used in Section 5.1.2, this version of code does not manually partition the algorithms based on our proposed SIMD 2 hardware configuration but allow the code to fully exploit the performance from CUDA cores. This version helps us to identify the performance variance by naively applying matrix algorithms without the presence of appropriate matrix accelerations. SIMD 2 using Tensor Cores. This version of code use identical algorithms to the version of SIMD 2 in CUDA cores except that we replace these algorithms' matrix operations to SIMD 2 ones when appropriate. As existing hardware does not support our proposed SIMD 2 operations yet, we evaluate the performance and validate the result of this version through the framework that Section 5.1 describes.</p><p>Table <ref type="table" target="#tab_8">4</ref> lists the set of benchmark applications. Each of these applications represents a use case for a proposed SIMD 2 instruction as follows. All-Pairs Shortest Path (APSP) and All-Pairs Critical (Longest) Path (APLP) APSP and APLP are graph problems that can be solved via min-plus and max-plus SIMD 2 instructions. Without SIMD 2 , the most efficient implementation, ECL-APSP <ref type="bibr" target="#b37">[38]</ref>, applied a phasebased-tiled Floyd Warshall algorithm to exploit massive parallelism using CUDA. We implemented APLP by extending the ECL-APSP with reversing the input weights on DAG to support the desired recurrence relation. For SIMD 2 version, the implementation simply changes the function calls to use min-plus and max-plus. Maximum Capacity Path (MaxCP), Maximum Reliability Path (MaxRP) and Minimum Reliability Path (MinRP) MaxCP, MaxRP and MinRP represent another set of graph problems with solutions based on transitive-closure. We select CUDA-FW as the state-of-theart GPU baseline for these problems and apply different operations in each iteration of their algorithms. These applications' SIMD 2 kernels simply require invoking max-min, max-mul and min-mul instructions. Minimum Spanning Tree (MST) Minimum spanning tree or minimum spanning forest (MSF) has rich applications in real-life network problems. However, conventional MST or MSF algorithms cannot efficiently take advantage of GPU architectures due to limited parallelism. The best-performing GPU implementation that we know of is CUDA MST and we use this one as our baseline. MST and MSF map perfectly to the min-max SIMD 2 instruction. Our SIMD 2 version of code thus leverages min-max instruction to investigate the efficiency of SIMD 2 in this type of problem. Graph Transitive Closure (GTC) GTC is also a graph analytics workload. Unlike other graph algorithms, GTC simply checks the connectivity between all vertices rather than reporting a route to fulfill the goal of optimization. Therefore, GTC can use library functions from cuBool <ref type="bibr" target="#b55">[56]</ref> for efficient implementation on GPUs. In SIMD 2 version, we used or-and instruction to implement the solution. K-Nearest Neighbor (KNN) Solving pair-wise L2 distance is at the core of K-nearest neighbor and K-means problems, and can leverage SIMD 2 's add-norm instruction. For the state-of-the-art GPU baseline, we use KNN-CUDA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RESULTS</head><p>This section summarizes our evaluation of SIMD 2 . SIMD 2 delivered up to 38.59? speedup in benchmark applications with simply 5% of total chip area overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Area and Power</head><p>We implemented the proposed SIMD 2 unit in RTL and synthesize them using Synopsis design compiler and the 45nm FreePDK45 library. We extended a baseline MMA unit that can simply perform MMA functions like conventional MXUs presented in Tensor Cores. The baseline MMA unit features 4x4 matrix multiplications on 16bit input elements and accumulates results in 32-bit elements. This configuration resembles the architecture used by Tensor Cores <ref type="bibr" target="#b51">[52]</ref> and Accel-Sim <ref type="bibr" target="#b29">[30]</ref>. We carefully design the proposed extensions to make the timing of the SIMD 2 unit the same as the baseline. We empirically observe that our the modification for the SIMD 2 unit never increases the critical path delay.</p><p>Table <ref type="table" target="#tab_9">5</ref>(a) lists the area overhead of adding SIMD 2 instructions into the baseline MMA unit. The baseline MMA unit is 11.52 ?? 2 in size. Adding each individual instruction results in 1.34% -21.25% overhead. The full-fledged SIMD 2 unit has an area overhead of 69.23%. We inspected the public die photo of an NVIDIA 3080 GPU and found that SMs account for 50.2% of the 628.4 mm 2 die area, and each SM is 3.75 mm 2 . If we scale the 69.23% overhead from the 45nm process to the Samsung 8N process used for our 3080 NVIDIA GPU baseline, a SIMD 2 unit introduces only 0.378 mm 2 , which is only 10% of the SM area and 5% of the total die area.</p><p>Table <ref type="table" target="#tab_9">5</ref>(b) also lists the case where we only implement a processing element to support a specific SIMD 2 instruction without the MMA function (i.e., as an individual accelerator). If we implement each SIMD 2 instruction separately as an individual accelerator, the total area of these accelerators will require additional 2.96x space of the baseline MMA unit. In contrast, the design of SIMD 2 unit allows these instructions to reuse common hardware components and saves area. For example, we found that for the processing elements supporting Min-Mul and Max-Mul operations, the area is almost the same as an MMA unit. However, combining their functions into a single SIMD 2 unit only results in 11.82% of area overhead, showing these instructions can share a large amount of circuits that were originally used for MMA operations. The baseline MMA unit consumes 3.74 W power. Extending the baseline as a SIMD 2 unit only adds 0.79 W to the active power.</p><p>If we extend the baseline MMA to support 32-bit numbers, the size of the MMA unit becomes 4.03x larger than a 16-bit MMA unit as Table <ref type="table" target="#tab_9">5</ref>(c) lists. A SIMD 2 unit supporting 32-bit inputs occupies 59% more area than the 32-bit MMA unit. If we further extend the 13.9</p><formula xml:id="formula_4">M in -P lu s P lu s -M u l M a x -P lu s M a x -M in M a x -M u l M in -M u l M in -M a x O r -A n d P lu s -N o r m G M E A N</formula><p>7.9 7.9</p><p>12.9   MMA to support 64-bit numbers, the size of the MMA unit becomes 11x larger than the 16-bit MMA. Extending the 64-bit MMA unit as a 64-bit SIMD 2 unit will add 52% area overhead. If we make both the baseline MMA and SIMD 2 units in supporting 8x8 matrix operations in 16-bit inputs, the MMA unit will become 7.5x larger than the 4x4 baseline. The area overhead over the baseline MXU stays constant and scales well.</p><formula xml:id="formula_5">P lu s -M u l M a x -P lu s M a x -M in M a x -M u l M in -M u l M in -M a x O r -A n d P lu s -N o r m G M E A N 0 2<label>4</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Microbenchmarks</head><p>We used microbenchmark workloads that repetitively invoke SIMD 2 the same instructions to gauge the performance gain of using SIMD 2 units compared against equivalent GPU implementations. The result shows up to 15.8? speedup in evaluated scenarios. Figure <ref type="figure" target="#fig_4">9</ref> shows the performance gain of SIMD 2 over the equivalent GPU baseline implementations when using square matrices as inputs. SIMD 2 reveals up to 15.8? speedup compared with using CUDA cores to achieve the desired matrix operation on the same dataset. The geometric mean (gmean) that discounts the outlier also shows a strong 8.7?-10.6? speedup, depending on the input set sizes. When input matrices are larger than 4,096?4,096 ones, the performance gain saturates at about 10?, representing the level of peak performance gain of these instructions. Figure <ref type="figure" target="#fig_5">10</ref> shows the performance gain of SIMD 2 instructions on different shapes of  From both results, SIMD 2 has the largest performance gains for min-max, max-min, and or-and instructions, by up to 15.8?. Such improvement is larger than the peak throughput difference between vector units and SIMD 2 units. We suspect the extra benefit from SIMD 2 units is due to the structural hazard in the GPU SM architecture, where min and max operations share the same hardware resources(e.g., ALU port), and so are or and and operations. By fusing these operations in a single instruction, SIMD 2 unit avoids this bottleneck and results in much higher speedup. The speedups of Plus-Mul and Plus-Norm operations are relatively low compared with others, but still enjoy a 3.1? speedup over using CUDA cores. This is because CUDA cores provide support for fused multiply-add (FMA) that allow the GPU to complete plus-mul operations with a single instruction. We expect that supporting more instructions similar to FMA would also provide similar performance boost to the class of problems that SIMD 2 addresses. Nevertheless, SIMD 2 still has a significant advantage, obtaining a speedup of up to 5.96? for larger matrix operations. We conclude that the SIMD 2 architecture has larger potential than fusing more vector operations, which we leave to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Benchmark Applications</head><p>Figure <ref type="figure" target="#fig_6">11</ref> shows the speedup of kerenl latency of applications using SIMD 2 (SIMD 2 w/ SIMD 2 units) over the baseline, optimized GPU implementations. SIMD 2 achieves a geometric mean of 10.76? -13.96?, with speedup as large as 38.59?. The performance gain of SIMD 2 in 7 out of the 8 applications remains strong even when dataset sizes increased.</p><p>Compared with implementing the same matrix-based algorithms without SIMD 2 presented (SIMD 2 w/ CUDA cores), all applications show significant slow down when SIMD 2 units are absent. For APLP, MST, MaxRP, MinRP, and APSP, these applications can never take advantage of matrix-base algorithms due to their higher computational complexities when SIMD 2 units are absent. This result explains why these algorithms were not favorable in conventional architectures. However, the introduction of SIMD 2 makes these matrix algorithms feasible. The matrix processing power from the SIMD 2 unit can compensate or even improve the performance of the applications as our experimental results tell. In fact, these algorithms can potentially take advantage of the embarrassingly parallel nature of matrix multiplication to parallelize hard-to-parallelize problems.</p><p>For MCP, GTC, and KNN, their SIMD 2 implementations outperform their baseline, state-of-the-art implementations, even without the presence of SIMD 2 units. For KNN, the computational complexity is the same for both SIMD 2 and the baseline implementations. However, the SIMD 2 kernel can still achieve a maximum speedup of 6.55? without the help of SIMD 2 units. This is because the baseline implementation uses customized functions to implement the algorithm, but the backend library of SIMD 2 without SIMD 2 units leverages CUTLASS that is more optimized and adaptive to modern GPU architectures. However, the performance gap between configurations with or without SIMD 2 units ranges between 4.79? and 6.43?. The performance advantage is more significant when we use the largest dataset. Therefore, even we revisit the design of the GPU baseline and make that as efficient as SIMD 2 on CUDA cores, such implementation still has a huge performance gap to catch up with the performance using SIMD 2 units. For MCP and GTC, SIMD 2 w/ CUDA cores can outperform their baseline implementations even though the computational complexity is higher in SIMD 2 implementations for two reasons. The first reason is similar to the case in KNN that SIMD 2 w/ CUDA cores benefits from more optimized library functions than the baseline ones. The other reason is that the rich parallelism of these matrix-based algorithms allow these implementations to scale better on modern GPU architectures -considering that the RTX 3080 GPU has twice as many CUDA cores than that of the previous generation of GPU architecture. However, the state-of-the-art baseline implementation cannot take advantage of this architectural improvement. On the other hand, this result also reveals that SIMD 2 programming model can make programs more adaptive to various underlying architectures since these architectural optimizations on SIMD 2 operations will remain without the demand of further code optimization.</p><p>The performance of APLP and MST using SIMD 2 degrades when datasets become larger. This is because both APLP and MST using SIMD 2 require additional convergence checks that are sensitive to input data values to determine the completion of the solution. As the input dataset grows, the variance in the content also becomes more significant and needs more iterations for the algorithm to converge. However, if the number of iterations do not increase with  the growth of dataset sizes, the program can still show performance gain over conventional CUDA cores since SIMD 2 still makes each iteration faster. For MST, the baseline GPU solution uses Kruskal's algorithm that can solve MST/MSF problems with computational complexity at ? (? log ?) <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b30">31]</ref>, where ? is defined as the number of edges in the input graph. In contrast, each iteration of the matrixbased SIMD 2 solution has the complexity of ? (? 3 ) <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b11">12]</ref>, where ? is the number of vertices in the input graph. Therefore, SIMD 2 becomes slower than the baseline implementation in each iteration for MST when dataset size is larger.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Discussion on algorithmic optimizations</head><p>In Figure <ref type="figure" target="#fig_6">11</ref>, our implementations use Leyzorek's algorithm and convergence checks to optimize the number of SIMD 2 operations, except for KNN. As the proposed SIMD 2 architecture improves the performance of supported semiring-like operations, SIMD 2 still allows these matrix-based algorithms to outperform the baseline state-of-the-art GPU implementations without these algorithmic optimizations.</p><p>The effect of convergence checks is sensitive to inputs. For each compute kernel using Leyzorek's algorithm on graph problems  with ? vertices, the implementation will take ??|? | SIMD 2 operations in the worst-case scenario. To evaluate the worst-case performance, we implemented a version of these applications without convergence checks. Figure <ref type="figure" target="#fig_0">12</ref> illustrates the performance of these implementations with bars labeled as Leyzorek w/o convergence. The baseline remains the same as Figure <ref type="figure" target="#fig_6">11</ref>. Despite the increasing numbers of iterations, all applications still outperform their baseline GPU implementations, ranging from 1.11? to 10.91?.</p><p>In Figure <ref type="figure" target="#fig_0">12</ref>, we also present implementations of these applications using the less efficient all-pair Bellman-Ford algorithm. As Bellman-Ford algorithm can take up to |V| SIMD 2 operations, using Bellman-Ford algorithm can slow down APLP and MST when datasets become large. MINRP can never beat GPU implementations if we use Bellman-Ford algorithm-based implementations. However, the performance gain remains significant for other applications as the advantage of SIMD 2 architecture out-weighted the shortcomings of increased computational complexity.</p><p>6.5 SIMD 2 for Sparse Workloads SIMD 2 on architectural support for sparsity. The idea of SIMD 2 can be applied to architecture support for sparse inputs, too. As an initial look of the SIMD 2 model, we extend our emulation framework and build on top of the cuSparselt library to model the benefit of applying the SIMD 2 idea to the sparse Tensor Cores in the RTX 3080 GPU, which supports structured sparsity and provides up to 2? throughput. We assume the inputs are pre-processed and stored in the format required by the sparse Tensor Core, excluding the processing overhead when reporting the speedup.</p><p>Figure <ref type="figure" target="#fig_7">13</ref> shows the speedup over baseline implementation when using a sparse SIMD 2 unit. On average, sparse SIMD 2 unit improves performance by 21.13?-24.82? and by up to 68.33?. Compared with the baseline SIMD 2 , SIMD 2 on sparse Tensor Cores is 1.60?-2.05? faster.</p><p>SIMD 2 for extremely sparse inputs. Some applications often have extremely sparse inputs, especially for graph algorithms. For these sparse inputs, a dense SIMD 2 unit might provide less performance improvement over implementations that are designed for sparse inputs, such as the cuSparse library. We therefore explore at what sparsity SIMD 2 can still provide benefits, which is illustrated in Figure <ref type="figure" target="#fig_8">14</ref>. The x-axis in Figure <ref type="figure" target="#fig_8">14</ref> shows the sparsity of inputs, meaning the ratio of zeros to non-zeros in each dataset. The y-axis shows the speedup of using NVIDIA's spGemm function, a sparse GEMM function optimized for Tensor Cores, from cuSparse library compared against gemmEx function, a dense GEMM function for Tensor Cores, from cuBlas library. The results show that cuSparse does not outperform cuBlas for matrices of size 1024 ? 1024, and for matrices of size 4096 ? 4096, cuSparse can outperform cuBlas when the sparsity of the input matrix exceeds 99%. This result shows that while many applications need to process sparse inputs, there is still a range of sparsity where SIMD 2 can provide benefit. Such range also covers a number of real graph datasets that do not exceed the sparsity indicated in the results <ref type="bibr" target="#b44">[45]</ref>, implying that it is more efficient to use the dense matrix processing method for these cases if appropriate architectural support for sparse matrix operations are absent.</p><p>To handle extremely sparse inputs (sparsity &gt;= 99%) on larger graphs, we can apply SIMD 2 sparse accelerators for spGEMM, which also use multiply-and-add for the ALU, such as GAMMA <ref type="bibr" target="#b72">[73]</ref>. For example, a GAMMA PE uses FP64 multiplier and adder, and an SIMD 2 GAMMA PE will use two FP64 ALUs, one supports the ? op, and the other supports the ? op. This SIMD 2 GAMMA accelerator would then be able to run APSP on sparse graphs. In fact, extending sparse accelerators with SIMD 2 would incur less overheads, as compute units contribute to less area than dense accelerators. For example, in GAMMA, only 10% of the total area is due to the FP64 MAC unit. We leave this extension to future work.</p><p>It is worth mentioning that while libraries like cuSparse have an advantage in terms of space complexity when dealing with extremely sparse matrices, the compressed matrix format may consume more device memory when storing relatively dense matrices. Experimental results show that cuSparse requires more memory than a single RTX 3080 GPU can provide when processing matrices with sparsity less than 90% (the OOM result in Figure <ref type="figure" target="#fig_8">14</ref>) and size more than 16384 ? 16384. However, when using a dense processing method, a GPU with 10GB of device memory can accommodate a matrix multiplication of at least 32768 ? 32768 in size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">RELATED WORK</head><p>In addition to the related work that motivates SIMD 2 in Section 2, several other lines of research that are relevant to SIMD 2 deserve mention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Matrix extensions and instructions</head><p>Instruction-level support for matrix-matrix multiplication can be dated back to the 90s. MOM <ref type="bibr" target="#b5">[6]</ref> proposes to leverage MXU to accelerate multi-media applications. As neural networks become one of the most critical workloads, commercial general processors now also include matrix instructions as well as MXUs to accelerate tiled-matrix-multiplication. NVIDIA Tensor Core <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b52">53]</ref>, Intel AMX <ref type="bibr" target="#b22">[23]</ref>, and Arm SME <ref type="bibr" target="#b2">[3]</ref> all provide instructions for GEMM. Our SIMD 2 architecture is compatible with these prior work and modern designs. SIMD 2 reuses the existing hardware and software infrastructure to accelerate matrix operations beyond GEMM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Dense tensor accelerators</head><p>SIMD 2 builds on top of recent dense tensor accelerators for matrixmultiplication <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b66">67]</ref> to efficiently share data across datapath and reduce the bandwidth requirement of SIMD 2 instructions. While we implement our SIMD 2 microarchitecture using systolic-array-like hardware structure, other matrix-multiplication accelerator architecture, such as the IBM MMA <ref type="bibr" target="#b66">[67]</ref> unit, can be extended to support SIMD 2 instructions.</p><p>In addition to matrix-multiplication, prior work also proposes accelerators for other dense linear algebra algorithms with different data sharing patterns. For example, Weng et al. <ref type="bibr" target="#b70">[71]</ref> propose a hybrid systolic-dataflow architecture for inductive matrix algorithms (e.g., linear algebra solver). Tithi et al. <ref type="bibr" target="#b67">[68]</ref> propose a spatial accelerator for edit distance algorithms. While these algorithms have a different data sharing pattern than SIMD 2 instructions support, we expect they can be implemented as CISC-SIMD 2 instructions with variable latency. We nonetheless leave this extension to prior work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Sparse tensor accelerators</head><p>Since sparse matrices are common for many applications, such as HPC workloads, there is also ample prior work in sparse tensor accelerators <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b74">[75]</ref><ref type="bibr" target="#b75">[76]</ref><ref type="bibr" target="#b76">[77]</ref>. These accelerators propose various sparse optimizations to skip ineffectual computations to speed up the tensor algorithms with sparse inputs. They leverage various hardware support for gather/scatter operation and intersection to transform sparse tensor algebra into dense tensor algebra, improving conventional dense tensor accelerators. These techniques are therefore orthogonal to SIMD 2 , and we expect SIMD 2 can be extended to support sparse tensor operation by applying similar techniques, as discussed previously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Graph algorithm accelerators</head><p>While many graph algorithms can be expressed as tensor operations and linear algebra <ref type="bibr" target="#b28">[29]</ref> and accelerated by tensor accelerators, prior work has also proposed hardware accelerators to speed up graph algorithms and analytics in their classic form. Graphicionado <ref type="bibr" target="#b15">[16]</ref>, GraphR <ref type="bibr" target="#b63">[64]</ref>, GraphP <ref type="bibr" target="#b73">[74]</ref>, and GraphQ <ref type="bibr" target="#b78">[79]</ref> leverage processing-inmemory (PIM) architecture to alleviate the bandwidth bottleneck in graph algorithms. PHI <ref type="bibr" target="#b49">[50]</ref> and HATS <ref type="bibr" target="#b48">[49]</ref> instead enhance conventional multi-core processors to accelerate common operations in graph analytics, such as commutative reduction and traversal scheduling. These hardware acceleration techniques focus on leveraging properties in graph algorithms to reduce data movement and bandwidth requirement. In contrast, SIMD 2 proposes a new instruction set for tensorized graph algorithms to leverage tensor accelerators ubiquitous in all compute platforms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">Democratizing Domain-Specific Accelerators</head><p>In addition to accelerating NNs, recent projects have demonstrated the strong potential of using NN/MMA accelerators for a broader spectrum of applications. Both Tensor Cores and TPUs can help improve the performance of linear algebra beyond GEMM <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b20">21]</ref>, database queries <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22]</ref>, cryptography <ref type="bibr" target="#b33">[34]</ref> and scientific computing problems <ref type="bibr">[9, 11, 36, 40-42, 48, 51]</ref>. Ray tracing accelerators are also useful for Monte Carlo simulations <ref type="bibr" target="#b60">[61]</ref>, robotics navigation <ref type="bibr" target="#b45">[46]</ref> and nearest neighbor search problems <ref type="bibr" target="#b77">[78]</ref>. However, due to the domain-specific nature of these accelerators, programmers have to intensively re-engineer the algorithm implementation to make use of these hardware accelerators. The resulting program may also incur overhead when transforming data structures to fulfill the demand of the target accelerator. By extending the hardware features, SIMD 2 provides better programmability to reduce the overhead of remapping algorithms and allows applications that are not possible on conventional NN/MMA accelerators. With hardware accelerators lifting the roofline, a critical issue is designing a memory hierarchy that streamlines the data inputs/outputs for computational logic. Potential solutions include bringing hardware accelerators closer to large memory arrays <ref type="bibr" target="#b31">[32]</ref> or using other hardware accelerators to produce the demanding data structures for the target computing resource <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b54">55]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>Recent advance in hardware accelerators that accelerate matrix multiplications in AI/ML workloads encourage us to take a new look at other matrix problems. As many matrix problems share a similar computation pattern with matrix multiplications that existing hardware accelerators already optimize for, a more generalized matrix processor will allow these matrix problems to benefit from hardware acceleration.</p><p>This paper introduces SIMD 2 to investigate the potential of this research avenue. We leverage the common computation pattern of significant matrix problems to design the SIMD 2 instruction set and implement a feasible, exemplary hardware architecture supporting these SIMD 2 instructions with 5% total chip area overhead. We demonstrate the effectiveness of SIMD 2 using a set of benchmark applications, some of them are rewritten with algorithms that are traditionally considered inefficient due to the lack of hardware support like SIMD 2 . Our evaluation results show that the proposed SIMD 2 architecture achieves more than 10.63? speedup on average across eight applications with various tensor computation patterns.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Code snippet of (a) GEMM and (b) APSP. See Section 4 for full APSP implementation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: An example MXU for GEMM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: ? ALU and ? ALU in an SIMD 2 unit.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Performance of microbenchmark with square matrices using SIMD 2 API</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Performance of microbenchmark with nonsquare matrices using SIMD 2 API</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Performance of applications using SIMD 2 API</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Performance of applications using Sparse SIMD 2 unit</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Performance of sparse matrix multiplication</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Exemplary problems with their mappings to semiring-like structures and the corresponding definitions of operators to their solutions.</figDesc><table><row><cell>Type of</cell><cell cols="3">1st OP 2nd OP Representative</cell></row><row><cell>matrix operations</cell><cell>?</cell><cell>?</cell><cell>Algorithm(s)</cell></row><row><cell>Plus-Multiply</cell><cell>+</cell><cell>?</cell><cell>Matrix Multiplications,</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Matrix Inverse</cell></row><row><cell>Min-Plus</cell><cell>???</cell><cell>+</cell><cell>All-pairs shortest paths</cell></row><row><cell></cell><cell></cell><cell></cell><cell>problem</cell></row><row><cell>Max-Plus</cell><cell>???</cell><cell>+</cell><cell>Maximum cost (critical path)</cell></row><row><cell>Min-Multiply</cell><cell>???</cell><cell>?</cell><cell>Minimum reliability paths</cell></row><row><cell>Max-Multiply</cell><cell>???</cell><cell>?</cell><cell>Maximum reliability paths</cell></row><row><cell>Min-Max</cell><cell>???</cell><cell>???</cell><cell>Minimum spanning tree</cell></row><row><cell>Max-Min</cell><cell>???</cell><cell>???</cell><cell>Maximum capacity paths</cell></row><row><cell>Or-And</cell><cell>??</cell><cell>???</cell><cell>Transitive and reflexive</cell></row><row><cell></cell><cell></cell><cell></cell><cell>closure</cell></row><row><cell>Add-Norm</cell><cell>+</cell><cell cols="2">|? -? | 2 L2 Distance</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>A summary of the PTX instruction set architecture for SIMD2   </figDesc><table><row><cell cols="4">Data Movement Instructions Data Types Matrix Shape Source ? Destination</cell></row><row><cell>SIMD 2 .load</cell><cell>fp16</cell><cell>16x16</cell><cell>Shared Memory ? Register File</cell></row><row><cell>SIMD 2 .store</cell><cell>fp32</cell><cell>16x16</cell><cell>Register File ? Share Memory</cell></row><row><cell>Arithmetic Instructions</cell><cell>? OP</cell><cell>? OP</cell><cell>Algorithm</cell></row><row><cell>SIMD 2 .mma</cell><cell>+</cell><cell>?</cell><cell>GEMM</cell></row><row><cell>SIMD 2 .minplus</cell><cell>???</cell><cell>+</cell><cell>All-pairs shortest paths problem</cell></row><row><cell>SIMD 2 .maxplus</cell><cell>???</cell><cell>+</cell><cell>Maximum cost (critical path)</cell></row><row><cell>SIMD 2 .minmul</cell><cell>???</cell><cell>?</cell><cell>Minimum reliability paths</cell></row><row><cell>SIMD 2 .maxmul</cell><cell>???</cell><cell>?</cell><cell>Maximum reliability paths</cell></row><row><cell>SIMD 2 .minmax</cell><cell>???</cell><cell>???</cell><cell>Minimum spanning tree</cell></row><row><cell>SIMD 2 .maxmin</cell><cell>???</cell><cell>???</cell><cell>Maximum capacity paths</cell></row><row><cell>SIMD 2 .orand</cell><cell>??</cell><cell>???</cell><cell>Transitive and reflexive closure</cell></row><row><cell>SIMD 2 .addnorm</cell><cell>+</cell><cell>|? -? | 2</cell><cell>L2 Distance</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Example API of SIMD 2 programming model</figDesc><table><row><cell>Sample Low-level Synopsis</cell><cell>Description</cell></row></table><note><p>simd2::matrix&lt;matrix_type, m, n, k, data_type&gt; Declaration function, declare the matrix will be applied in the m?n?k matrix-matrix operation. simd2::fillmatrix(simd2::matrix, value)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>implements (Line 14). The na?ve SIMD 2 implementation</figDesc><table><row><cell>1</cell><cell>f l o a t  *  a d j _ m a t _ d ;</cell></row><row><cell>2</cell><cell>f l o a t  *  d i s t _ d _ d e l t a ;</cell></row><row><cell>3</cell><cell>f l o a t  *  d i s t _ d ;</cell></row><row><cell>4</cell><cell>c u d a M a l l o c ( . . . , a d j _ m a t _ d , . . . ) ;</cell></row><row><cell>5</cell><cell>c u d a M a l l o c ( . . . , d i s t _ d , . . . ) ;</cell></row><row><cell>6</cell><cell>c u d a M a l l o c ( . . . , d i s t _ d _ d e l t a , . . . ) ;</cell></row><row><cell>7</cell><cell></cell></row><row><cell cols="2">8 cudaMemcpy ( a d j _ m a t _ d , . . . , H2D ) ;</cell></row><row><cell cols="2">9 cudaMemcpy ( d i s t _ d _ d e l t a , . . . , H2D ) ;</cell></row><row><cell cols="2">10 cudaMemcpy ( d i s t _ d , . . . , H2D ) ;</cell></row><row><cell>11</cell><cell></cell></row><row><cell>12</cell><cell>b o o l c o n v e r g e = t r u e ;</cell></row><row><cell>13</cell><cell>while ( c o n v e r g e ) {</cell></row><row><cell>14</cell><cell>s i m d 2 _ m i n p l u s ( a d j _ m a t _ d , d i s t _ d , d i s t _ d ,</cell></row><row><cell></cell><cell>d i s t _ d _ d e l t a , v , v , v ) ;</cell></row><row><cell>15</cell><cell>c o n v e r g e = c h e c k _ c o n v e r g e n c e ( d i s t _ d ,</cell></row><row><cell></cell><cell>d i s t _ d _ d e l t a , . . . ) ;</cell></row><row><cell>16</cell><cell>}</cell></row><row><cell cols="2">17 cudaMemcpy ( . . . , d i s t _ d , . . . , D2H ) ;</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>5.1.1 Hardware configuration. Our validation and emulation framework uses a machine with NVIDIA's RTX 3080 GPU based on the Ampere architecture with 10 GB device memory. This machine has Figure 8: The workflow of the emulation framework for SIMD 2 evaluation. an 8-core, 16 threads AMD RyZen 3700X processor with peak clock rate at 4.4 GHz and 16 GB physical main memory installed. The machine hosts an Ubuntu 20.04 (Linux kernel version 5.13) with NVIDIA's CUDA 11.1 using driver version 470.103.01.</figDesc><table><row><cell cols="2">Application</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">(e.g. APSP) + Input</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Baseline SOTA</cell><cell></cell><cell>???? 2</cell><cell></cell></row><row><cell cols="2">Implementation</cell><cell cols="2">Implementation</cell><cell>CUDA API</cell></row><row><cell cols="2">CUDA library on</cell><cell cols="2">???? 2 library on</cell><cell>???? 2 library on</cell></row><row><cell cols="2">CUDA cores</cell><cell></cell><cell>CUDA cores</cell><cell>???? ? units</cell></row><row><cell cols="2">(e.g., APSP-cuda [21])</cell><cell cols="2">(cuASR, CUTLASS)</cell><cell>(emulated via wmma)</cell></row><row><cell>App</cell><cell>Runtime</cell><cell>App</cell><cell>Runtime</cell><cell>Runtime</cell></row><row><cell>Result</cell><cell></cell><cell>Result</cell><cell></cell><cell></cell></row><row><cell cols="2">Correctness and Accuracy Check</cell><cell></cell><cell cols="2">Speedup (Fig. 11, 12)</cell></row></table><note><p>5.1.2 Evaluation Process. Figure</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Source and input data size of baseline implenmentation for each selected applications.</figDesc><table><row><cell>Application</cell><cell cols="2">Baseline Source Input Dimension</cell></row><row><cell>All Pair Shortest Path (APSP)</cell><cell>ECL-APSP [28, 38]</cell><cell>Small Medium 8192 4096 Large 16384</cell></row><row><cell>All Pair Critical Path (APLP)</cell><cell>ECL-APSP [28, 38]</cell><cell>Small Medium 8192 4096 Large 16384</cell></row><row><cell>Maximum Capacity Path (MCP)</cell><cell>CUDA-FW [43, 44]</cell><cell>Small Medium 8192 4096 Large 16384</cell></row><row><cell>Maximum Reliability Path (MAXRP)</cell><cell>CUDA-FW [43, 44]</cell><cell>Small Medium 8192 4096 Large 16384</cell></row><row><cell>Minimum Reliability Path (MINRP)</cell><cell>CUDA-FW [43, 44]</cell><cell>Small Medium 8192 4096 Large 16384</cell></row><row><cell>Minimum Spanning Tree (MST)</cell><cell>CUDA MST [17, 19, 25, 60, 63]</cell><cell>Small Medium 2048 1024 Large 4096</cell></row><row><cell>Graph Transitive Colsure (GTC)</cell><cell>CUBOOL [56]</cell><cell>Small Medium 4096 1024 Large 8192</cell></row><row><cell>K-Nearest Neighbor (KNN)</cell><cell>KNN-CUDA [69]</cell><cell>Small Medium 8192 4096 Large 16384</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>The area overhead of supporting SIMD 2 instructions through (a) adding instructions to the MMA unit, (b) individual accelerators, (c) extension to the MMA unit with various precisions, compared to the baseline 16-bit MMA Unit.</figDesc><table><row><cell>Supported Ops.</cell><cell>Area</cell><cell cols="3">Supported Ops. Area</cell></row><row><cell cols="2">MMA + All SIMD 2 Insts. 1.69</cell><cell>Min-Plus</cell><cell></cell><cell>0.26</cell></row><row><cell>MMA + Min-Plus</cell><cell>1.21</cell><cell>Max-Plus</cell><cell></cell><cell>0.26</cell></row><row><cell>MMA + Max-Plus</cell><cell>1.21</cell><cell>Min-Mul</cell><cell></cell><cell>1.03</cell></row><row><cell>MMA + Min-Mul</cell><cell>1.12</cell><cell>Max-Mul</cell><cell></cell><cell>1.03</cell></row><row><cell>MMA + Max-Mul</cell><cell>1.12</cell><cell>Min-Max</cell><cell></cell><cell>0.06</cell></row><row><cell>MMA + Min-Max</cell><cell>1.01</cell><cell>Max-Min</cell><cell></cell><cell>0.06</cell></row><row><cell>MMA + Max-Min</cell><cell>1.01</cell><cell>Or-And</cell><cell></cell><cell>0.08</cell></row><row><cell>MMA + Or-And</cell><cell>1.04</cell><cell cols="2">Add-Norm</cell><cell>0.19</cell></row><row><cell>MMA + Add-Norm</cell><cell>1.18</cell><cell>Total</cell><cell></cell><cell>2.96</cell></row><row><cell>(a)</cell><cell></cell><cell></cell><cell>(b)</cell><cell></cell></row><row><cell></cell><cell cols="4">8-bit 16-bit 32-bit 64-bit</cell></row><row><cell>MMA only</cell><cell>0.25</cell><cell>1</cell><cell>4.04</cell><cell>11.17</cell></row><row><cell cols="2">MMA + All SIMD 2 Insts. 0.69</cell><cell>1.69</cell><cell>6.42</cell><cell>17.01</cell></row><row><cell></cell><cell>(c)</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>You may find the code repository at https://github.com/escalab/SIMD2</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>The authors would like to thank the anonymous reviewers for their helpful comments. This work was sponsored by the two <rs type="funder">National Science Foundation</rs> (NSF) awards, <rs type="grantNumber">CNS-1940048</rs> and <rs type="grantNumber">CNS-2007124</rs>. This work was also supported by new faculty start-up funds from <rs type="affiliation">University of California, Riverside</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_pc4u7d2">
					<idno type="grant-number">CNS-1940048</idno>
				</org>
				<org type="funding" xml:id="_FbD3EHn">
					<idno type="grant-number">CNS-2007124</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
		<author>
			<persName><surname>Blas</surname></persName>
		</author>
		<ptr target="http://www.netlib.org/blas/" />
	</analytic>
	<monogr>
		<title level="s">Basic Linear Algebra Subprograms</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Think fast: a tensor streaming processor (TSP) for accelerating deep learning workloads</title>
		<author>
			<persName><forename type="first">Dennis</forename><surname>Abts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Sparling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Wong-Vanharen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Temesghen</forename><surname>Kahsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Garrin</forename><surname>Kimmell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="145" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Introducing the Scalable Matrix Extension for the Armv9-A Architecture</title>
		<ptr target="https://community.arm.com/developer/ip-products/processors/b/processors-ip-blog/posts/scalable-matrix-extension-armv9-a-architecture" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
		<respStmt>
			<orgName>Arm Corporation</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Qualcomm? Cloud Al 100: 12TOPS/W Scalable, High Performance and Low Latency Deep Learning Inference Accelerator</title>
		<author>
			<persName><forename type="first">Karam</forename><surname>Chatha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE Hot Chips 33 Symposium (HCS)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Volta: Performance and Programmability</title>
		<author>
			<persName><forename type="first">Jack</forename><surname>Choquette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Giroux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denis</forename><surname>Foley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>IEEE Micro</publisher>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="42" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">MOM: a matrix SIMD instruction set architecture for multimedia applications</title>
		<author>
			<persName><forename type="first">Jesus</forename><surname>Corbal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><surname>Espasa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateo</forename><surname>Valero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1999 ACM/IEEE conference on Supercomputing</title>
		<meeting>the 1999 ACM/IEEE conference on Supercomputing</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">H</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">E</forename><surname>Cormen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronald</forename><forename type="middle">L</forename><surname>Leiserson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clifford</forename><surname>Rivest</surname></persName>
		</author>
		<author>
			<persName><surname>Stein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
	<note>Introduction to Algorithms, Third Edition. 3rd edition</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Accelerating reduction and scan using tensor core units</title>
		<author>
			<persName><forename type="first">Abdul</forename><surname>Dakkak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isaac</forename><surname>Gelado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Mei</forename><surname>Hwu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Supercomputing, ICS &apos;19</title>
		<meeting>the ACM International Conference on Supercomputing, ICS &apos;19</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="46" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Accelerating fourier and number theoretic transforms using tensor cores and warp shuffles</title>
		<author>
			<persName><forename type="first">Sultan</forename><surname>Durrani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Saad Chughtai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mert</forename><surname>Hidayetoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rashid</forename><surname>Tahir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdul</forename><surname>Dakkak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Rauchwerger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fareed</forename><surname>Zaffar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Mei</forename><surname>Hwu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 30th International Conference on Parallel Architectures and Compilation Techniques (PACT)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="345" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Erickson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Algorithms</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Egemm-tc: Accelerating scientific computing on tensor cores with extended precision</title>
		<author>
			<persName><forename type="first">Boyuan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuke</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufei</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, PPoPP &apos;21</title>
		<meeting>the 26th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, PPoPP &apos;21</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="278" to="291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Algorithm 97: Shortest path</title>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">W</forename><surname>Floyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM, page</title>
		<imprint>
			<biblScope unit="volume">345</biblScope>
			<date type="published" when="1962-06">jun 1962</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">AWB-GCN: A graph convolutional network accelerator with runtime workload rebalancing</title>
		<author>
			<persName><forename type="first">Tong</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Runbin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunshu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanfei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pouya</forename><surname>Haghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonino</forename><surname>Tumeo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Reinhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="922" to="936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">SparTen: A sparse tensor accelerator for convolutional neural networks</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Gondimalla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Chesnut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mithuna</forename><surname>Thottethodi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Vijaykumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 52nd Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">52</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Harnessing gpu tensor cores for fast fp16 arithmetic to speed up mixed-precision iterative refinement solvers</title>
		<author>
			<persName><forename type="first">Azzam</forename><surname>Haidar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanimire</forename><surname>Tomov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Dongarra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><forename type="middle">J</forename><surname>Higham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SC18: International Conference for High Performance Computing, Networking, Storage and Analysis</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="603" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Graphicionado: A high-performance and energy-efficient accelerator for graph analytics</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Tae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Narayanan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadathur</forename><surname>Sundaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Satish</surname></persName>
		</author>
		<author>
			<persName><surname>Martonosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 49th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Chapter 7 -fast minimum spanning tree computation</title>
		<author>
			<persName><forename type="first">Pawan</forename><surname>Harish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vibhav</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suryakant</forename><surname>Patidar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GPU Computing Gems Jade Edition</title>
		<editor>
			<persName><forename type="first">Wen</forename><surname>Mei</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><surname>Hwu</surname></persName>
		</editor>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="77" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">ExTensor: An accelerator for sparse tensor algebra</title>
		<author>
			<persName><forename type="first">Kartik</forename><surname>Hegde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hadi</forename><surname>Asghari-Moghaddam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Pellauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neal</forename><surname>Crago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aamer</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edgar</forename><surname>Solomonik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">W</forename><surname>Fletcher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 52nd Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="319" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Jared</forename><surname>Hoberock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Bell</surname></persName>
		</author>
		<ptr target="http://thrust.github.io/" />
		<title level="m">Thrust: A parallel template library</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Relational queries with a tensor processing unit</title>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Holanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannes</forename><surname>M?hleisen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th International Workshop on Data Management on New Hardware, DaMoN&apos;19</title>
		<meeting>the 15th International Workshop on Data Management on New Hardware, DaMoN&apos;19</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Accelerating Applications using Edge Tensor Processing Units</title>
		<author>
			<persName><forename type="first">Kuan-Chieh</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hung-Wei</forename><surname>Tseng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SC: The International Conference for High Performance Computing, Networking, Storage, and Analysis</title>
		<meeting><address><addrLine>SC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">TCUDB: Accelerating Database with Tensor Processors</title>
		<author>
			<persName><forename type="first">Yu-Ching</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hung-Wei</forename><surname>Tseng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the 2022 ACM SIGMOD/PODS International Conference on Management of Data</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<ptr target="https://software.intel.com/content/www/us/en/develop/documentation/cpp-compiler-developer-guide-and-reference/top/compiler-reference/intrinsics/intrinsics-for-intel-advanced-matrix-extensions-intel-amx-instructions.html" />
		<title level="m">Intrinsics for Intel(R) Advanced Matrix Extensions (Intel(R) AMX) Instructions</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
		<respStmt>
			<orgName>Intel Corporation</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Hammond</surname></persName>
		</author>
		<ptr target="https://github.com/hpcgarage/cuASR" />
		<title level="m">cuASR: CUDA Algebra for Semirings</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jiacheng</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><surname>Cuda Mst</surname></persName>
		</author>
		<ptr target="https://github.com/jiachengpan/cudaMST" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A Domain-specific Supercomputer for Training Deep Neural Networks</title>
		<author>
			<persName><forename type="first">Doe</forename><surname>Norman P Jouppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Hyun Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Kurian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nishant</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cliff</forename><surname>Laudon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><surname>Patterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="67" to="78" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Norman</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cliff</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nishant</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raminder</forename><surname>Bajwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suresh</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Boden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Al</forename><surname>Borchers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rick</forename><surname>Boyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre-Luc</forename><surname>Cantin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clifford</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Coriell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Daley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Dau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Gelb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tara</forename><surname>Vazir Ghaemmaghami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajendra</forename><surname>Gottipati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Gulland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Richard</forename><surname>Robert Hagmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Hogberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hundt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Hurt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Ibarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alek</forename><surname>Jaffey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Jaworski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harshit</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khaitan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Killebrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naveen</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Lacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Laudon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diemthu</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuyuan</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Lucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gordon</forename><surname>Lundin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Mackean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maire</forename><surname>Maggiore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kieran</forename><surname>Mahony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ray</forename><surname>Narayanaswami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathy</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Nix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Norrie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Narayana</forename><surname>Omernick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Penukonda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Phelps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emad</forename><surname>Salek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Samadiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Severn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Sizikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jed</forename><surname>Snelham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Souter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Steinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mercedes</forename><surname>Swing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Thorson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horia</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erick</forename><surname>Toma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Tuttle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Walter</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doe</forename><surname>Wilcox</surname></persName>
		</author>
		<author>
			<persName><surname>Hyun Yoon</surname></persName>
		</author>
		<title level="m">Proceedings of the 44th Annual International Symposium on Computer Architecture, ISCA &apos;17</title>
		<meeting>the 44th Annual International Symposium on Computer Architecture, ISCA &apos;17</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">All-pairs shortest-paths for large graphs on the gpu</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">T</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><surname>Kider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGGRAPH/EUROGRAPHICS Symposium on Graphics Hardware</title>
		<meeting>the 23rd ACM SIGGRAPH/EUROGRAPHICS Symposium on Graphics Hardware</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="47" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Mathematical foundations of the graphblas</title>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Kepner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Aaltonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Bader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aydin</forename><surname>Bulu?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franz</forename><surname>Franchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dylan</forename><surname>Hutchison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manoj</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Lumsdaine</surname></persName>
		</author>
		<author>
			<persName><surname>Henning Meyerhenke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE High Performance Extreme Computing Conference (HPEC)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Accel-sim: An extensible simulation framework for validated gpu modeling</title>
		<author>
			<persName><forename type="first">Mahmoud</forename><surname>Khairy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhesheng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tor</forename><forename type="middle">M</forename><surname>Aamodt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">G</forename><surname>Rogers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="473" to="486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">On the shortest spanning subtree of a graph and the traveling salesman problem. Proceedings of the</title>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">B</forename><surname>Kruskal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1956">1956</date>
			<publisher>American Mathematical Society</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">TensorDIMM: A practical nearmemory processing architecture for embeddings and tensor operations in deep learning</title>
		<author>
			<persName><forename type="first">Youngeun</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunjae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minsoo</forename><surname>Rhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the 52nd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO), MICRO &apos;52</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="740" to="753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">TensorPRAM: Designing a scalable heterogeneous deep learning accelerator with byte-addressable prams</title>
		<author>
			<persName><forename type="first">Sangwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gyuyoung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myoungsoo</forename><surname>Jung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th USENIX Workshop on Hot Topics in Storage and File Systems</title>
		<meeting><address><addrLine>HotStorage</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-07-13">2020. July 13-14, 2020, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Tensorcrypto: High throughput acceleration of lattice-based cryptography using tensor core on gpu</title>
		<author>
			<persName><forename type="first">Wai-Kong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hwajeong</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenfei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seong Oun</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="20616" to="20632" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Investigation of model techniques-first annual report-6 june 1956-1 july 1957-a study of model techniques for communication systems</title>
		<author>
			<persName><surname>Leyzorek</surname></persName>
		</author>
		<author>
			<persName><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><surname>Ladew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Sr Meaker Jr</surname></persName>
		</author>
		<author>
			<persName><surname>Petry</surname></persName>
		</author>
		<author>
			<persName><surname>Seitz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1957">1957</date>
			<pubPlace>Cleveland, Ohio</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Case Institute of Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">tcfft: A fast half-precision fft library for nvidia tensor cores</title>
		<author>
			<persName><forename type="first">Binrui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shenggan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Conference on Cluster Computing (CLUSTER)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Ascend: a scalable and unified architecture for ubiquitous deep neural network computing: Industry track paper</title>
		<author>
			<persName><forename type="first">Heng</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajin</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiping</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honghui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxing</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">ECL-APSP v1</title>
		<author>
			<persName><forename type="first">Yiqian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Burtscher</surname></persName>
		</author>
		<ptr target="https://userweb.cs.txstate.edu/~burtscher/research/ECL-APSP/" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">NDS: N-Dimensional Storage</title>
		<author>
			<persName><forename type="first">Yu-Chia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hung-Wei</forename><surname>Tseng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture, MICRO 2021</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="28" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Large-scale discrete fourier transform on tpus</title>
		<author>
			<persName><forename type="first">Tianjian</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-Fan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blake</forename><surname>Hechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Accelerating mri reconstruction on tpus</title>
		<author>
			<persName><forename type="first">Tianjian</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibault</forename><surname>Marin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-Fan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE High Performance Extreme Computing Conference (HPEC)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Nonuniform fast fourier transform on tpus</title>
		<author>
			<persName><forename type="first">Tianjian</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibault</forename><surname>Marin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-Fan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="783" to="787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Ben</forename><forename type="middle">D</forename><surname>Lund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><forename type="middle">W</forename><surname>Smith</surname></persName>
		</author>
		<idno>ArXiv, abs/1001.4108</idno>
		<title level="m">A multi-stage cuda kernel for floyd-warshall</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Bojanowski</surname></persName>
		</author>
		<ptr target="https://github.com/MTB90/cuda-floyd_warshall" />
		<title level="m">Cuda Floyd Warshall implementation</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Just how dense are dense graphs in the real world? a methodological note</title>
		<author>
			<persName><forename type="first">Guy</forename><surname>Melancon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 AVI Workshop on BEyond Time and Errors: Novel Evaluation Methods for Information Visualization, BELIV &apos;06</title>
		<meeting>the 2006 AVI Workshop on BEyond Time and Errors: Novel Evaluation Methods for Information Visualization, BELIV &apos;06</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Accelerating probabilistic volumetric mapping using ray-tracing graphics hardware</title>
		<author>
			<persName><forename type="first">Heajung</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyung</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Young</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5440" to="5445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Semiring frameworks and algorithms for shortest-distance problems</title>
		<author>
			<persName><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Automata, Languages and Combinatorics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="321" to="350" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">Alan</forename><surname>Morningstar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Hauru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jackson</forename><surname>Beall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Ganahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vedika</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guifre</forename><surname>Khemani</surname></persName>
		</author>
		<author>
			<persName><surname>Vidal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.08044</idno>
		<title level="m">Simulation of Quantum Many-Body Dynamics with Tensor Processing Units: Floquet Prethermalization</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Exploiting Locality in Graph Analytics through Hardware-Accelerated Traversal Scheduling</title>
		<author>
			<persName><forename type="first">Anurag</forename><surname>Mukkara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Beckmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maleen</forename><surname>Abeydeera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaosong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Sanchez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the 51st Annual IEEE/ACM international symposium on Microarchitecture (MICRO)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">PHI: Architectural Support for Synchronization-and Bandwidth-Efficient Commutative Scatter Updates</title>
		<author>
			<persName><forename type="first">Anurag</forename><surname>Mukkara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Beckmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Sanchez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the 52nd Annual IEEE/ACM international symposium on Microarchitecture (MICRO)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Exploring the binary precision capabilities of tensor cores for epistasis detection</title>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Nobre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergio</forename><surname>Santander-Jim?mnez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonel</forename><surname>Sousa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="338" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<ptr target="https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/nvidia-ampere-architecture-whitepaper.pdf" />
		<title level="m">NVIDIA A100 Tensor Core GPU Architecture</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<ptr target="https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/tesla-t4/t4-tensor-core-datasheet-951643.pdf" />
		<title level="m">NVIDIA T4 TENSOR CORE GPU</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
		<respStmt>
			<orgName>NVIDIA Corporation</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<ptr target="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-instructions" />
		<title level="m">Warp Level Matrix Multiply-Accumulate Instructions</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
		<respStmt>
			<orgName>NVIDIA Corporation</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">NVIDIA Hopper Architecture In-Depth</title>
		<ptr target="https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
		<respStmt>
			<orgName>NVIDIA Corporation</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">cuBool: sparse Boolean linear algebra for NVIDIA CUDA</title>
		<author>
			<persName><forename type="first">Egor</forename><surname>Orachyov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Alimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Semyon</forename><surname>Grigorev</surname></persName>
		</author>
		<ptr target="https://github.com/JetBrains-Research/cuBool" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Version 1.2.0</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">SCNN: An accelerator for compressed-sparse convolutional neural networks</title>
		<author>
			<persName><forename type="first">Angshuman</forename><surname>Parashar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minsoo</forename><surname>Rhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anurag</forename><surname>Mukkara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Puglielli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rangharajan</forename><surname>Venkatesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brucek</forename><surname>Khailany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">W</forename><surname>Keckler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 ACM/IEEE 44th Annual International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="27" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">SIGMA: A sparse and irregular gemm accelerator with flexible interconnects for dnn training</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananda</forename><surname>Samajdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyoukjun</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vineet</forename><surname>Nadella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sudarshan</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipankar</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharat</forename><surname>Kaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Krishna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="58" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Modeling deep learning accelerator enabled gpus</title>
		<author>
			<persName><forename type="first">Negar</forename><surname>Md Aamir Raihan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tor</forename><forename type="middle">M</forename><surname>Goli</surname></persName>
		</author>
		<author>
			<persName><surname>Aamodt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="79" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Fast and memory-efficient minimum spanning tree on the gpu</title>
		<author>
			<persName><forename type="first">Scott</forename><surname>Rostrup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shweta</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kishore</forename><surname>Singhal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computational Science and Engineering</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Exploiting hardware-accelerated ray tracing for monte carlo particle transport with openmc</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Salmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Mcintosh-Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Performance Modeling</title>
		<imprint>
			<biblScope unit="page" from="19" to="29" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
		<respStmt>
			<orgName>Benchmarking and Simulation of High Performance Computer Systems (PMBS</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Generalizing matrix multiplication for efficient computations on modern computers</title>
		<author>
			<persName><forename type="first">G</forename><surname>Stanislav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcin</forename><surname>Sedukhin</surname></persName>
		</author>
		<author>
			<persName><surname>Paprzycki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Parallel Processing and Applied Mathematics</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="225" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Brief announcement: The problem based benchmark suite</title>
		<author>
			<persName><forename type="first">Julian</forename><surname>Shun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><forename type="middle">E</forename><surname>Blelloch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><forename type="middle">T</forename><surname>Fineman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><forename type="middle">B</forename><surname>Gibbons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harsha</forename><surname>Vardhan Simhadri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kanat</forename><surname>Tangwongsan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fourth Annual ACM Symposium on Parallelism in Algorithms and Architectures</title>
		<meeting>the Twenty-Fourth Annual ACM Symposium on Parallelism in Algorithms and Architectures</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="68" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Graphr: Accelerating graph processing using reram</title>
		<author>
			<persName><forename type="first">Linghao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youwei</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuehai</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiran</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="531" to="543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Ma-tRaptor: A sparse-sparse matrix multiplication accelerator based on row-wise product</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanchen</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Albonesi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiru</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="766" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Tensaurus: A versatile accelerator for mixed sparse-dense tensor computations</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanchen</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaden</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongbo</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Albonesi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiru</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="689" to="702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Energy Efficiency Boost in the AI-Infused POWER10 Processor</title>
		<author>
			<persName><forename type="first">Brian</forename><forename type="middle">W</forename><surname>Thompto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dung</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jos?</forename><forename type="middle">E</forename><surname>Moreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramon</forename><surname>Bertran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hans</forename><surname>Jacobson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">J</forename><surname>Eickemeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rahul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcy</forename><surname>Goulet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">J</forename><surname>Byers</surname></persName>
		</author>
		<author>
			<persName><surname>Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Exploiting spatial architectures for edit distance algorithms</title>
		<author>
			<persName><forename type="first">Jahan</forename><surname>Jesmin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neal</forename><forename type="middle">C</forename><surname>Tithi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><forename type="middle">S</forename><surname>Crago</surname></persName>
		</author>
		<author>
			<persName><surname>Emer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<author>
			<persName><forename type="first">Michel</forename><surname>Barlaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Garcia</surname></persName>
		</author>
		<ptr target="https://github.com/vincentfpgarcia/kNN-CUDA" />
		<title level="m">?ric Debreuve. kNN-CUDA</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">John</forename><surname>Von</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neumann</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Annals of the History of Computing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
	<note>First draft of a report on the edvac</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">A hybrid systolic-dataflow architecture for inductive matrix algorithms</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sihao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengrong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vidushi</forename><surname>Dadu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><surname>Nowatzki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Hitting the memory wall: Implications of the obvious</title>
		<author>
			<persName><forename type="first">Wm</forename><forename type="middle">A</forename><surname>Wulf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sally</forename><forename type="middle">A</forename><surname>Mckee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGARCH computer architecture news</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="20" to="24" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Gamma: Leveraging Gustavson&apos;s Algorithm to Accelerate Sparse Matrix Multiplication</title>
		<author>
			<persName><forename type="first">Guowei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nithya</forename><surname>Attaluri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><forename type="middle">S</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Sanchez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th international conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-26)</title>
		<meeting>the 26th international conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-26)</meeting>
		<imprint>
			<date type="published" when="2021-04">April 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Graphp: Reducing communication for pim-based graph processing with efficient data partition</title>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youwei</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongwei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Kozyrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuehai</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">SpArch: Efficient architecture for sparse matrix multiplication</title>
		<author>
			<persName><forename type="first">Zhekai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanrui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="261" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Cambricon-S: Addressing irregularity in sparse neural networks through a cooperative software/hardware approach</title>
		<author>
			<persName><forename type="first">Xuda</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zidong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoli</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengsi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuehai</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianshi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunji</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 51st Annual IEEE/ACM International Symposium on Microarchitecture (MI-CRO)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="15" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Sparse tensor core: Algorithm and hardware co-design for vector-wise sparse neural networks on modern GPUs</title>
		<author>
			<persName><forename type="first">Maohua</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the 52nd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="359" to="371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">RTNN: Accelerating Neighbor Search Using Hardware Ray Tracing</title>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, PPoPP &apos;22</title>
		<meeting>the 27th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, PPoPP &apos;22</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="76" to="89" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Graphq: Scalable pim-based graph processing</title>
		<author>
			<persName><forename type="first">Youwei</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimin</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanzhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuehai</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the 52nd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="712" to="725" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
