<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Face-Focused Cross-Stream Network for Deception Detection in Videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mingyu</forename><surname>Ding</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Beijing Key Laboratory of Big Data Management and Analysis Methods School of Information</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<postCode>100872</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">An</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Beijing Key Laboratory of Big Data Management and Analysis Methods School of Information</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<postCode>100872</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhiwu</forename><surname>Lu</surname></persName>
							<email>zhiwu.lu@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Beijing Key Laboratory of Big Data Management and Analysis Methods School of Information</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<postCode>100872</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tao</forename><surname>Xiang</surname></persName>
							<email>t.xiang@surrey.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical and Electronic Engineering</orgName>
								<orgName type="institution">University of Surrey</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Samsung AI Centre</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Beijing Key Laboratory of Big Data Management and Analysis Methods School of Information</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<postCode>100872</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Face-Focused Cross-Stream Network for Deception Detection in Videos</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automated deception detection (ADD) from real-life videos is a challenging task. It specifically needs to address two problems: (1) Both face and body contain useful cues regarding whether a subject is deceptive. How to effectively fuse the two is thus key to the effectiveness of an ADD model. (2) Real-life deceptive samples are hard to collect; learning with limited training data thus challenges most deep learning based ADD models. In this work, both problems are addressed. Specifically, for face-body multimodal learning, a novel face-focused cross-stream network (FFCSN) is proposed. It differs significantly from the popular two-stream networks in that: (a) face detection is added into the spatial stream to capture the facial expressions explicitly, and (b) correlation learning is performed across the spatial and temporal streams for joint deep feature learning across both face and body. To address the training data scarcity problem, our FFCSN model is trained with both meta learning and adversarial learning. Extensive experiments show that our FFCSN model achieves state-of-the-art results. Further, the proposed FFCSN model as well as its robust training strategy are shown to be generally applicable to other human-centric video analysis tasks such as emotion recognition from user-generated videos.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With the recent rapid development of human-centric AI, human-centric video analysis <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b26">27]</ref> has also begun to draw much attention from the computer vision community. Other than the conventional video content analysis that focuses on generic semantic concept analysis of video content, human-centric video analysis aims to extract, describe, and organize a wealth of information regarding the main objects of interest in most videos: humans. This topic covers a wide range of research problems such as deception detection <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref>, emotion recognition in videos <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b17">18]</ref>, personality computing <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b55">56]</ref>, and action recognition <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b58">59]</ref>. For example, it is often important to recognize the deceptive behaviors <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref>, emotions <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b17">18]</ref>, or personality traits <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b55">56]</ref> of the subject of a video in real-world scenarios.</p><p>Deception detection <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref> is a late addition to humancentric video analysis and still under-studied. Deception is defined as an intentional attempt to mislead others <ref type="bibr" target="#b3">[4]</ref>. In our day-to-day life, deceptive behaviors occur in the form of intended lies, fabrications, omissions, misrepresentations, among others. Some deceptive behaviors are simply harmless, but others may have major threats to the society, e.g., those taking place in a courtroom. Detecting real-world human deceptive behaviors is a challenging task even for humans, and often requires well-trained human experts. A large-scale deployment of deception detection thus depends upon automated deception detection (ADD) <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37</ref>]. An ADD system can find applications in many real-world scenarios including airport security screening, court trial, job interview, and personal credit risk assessment.</p><p>The ADD task faces two major challenges. (1) Multimodal fusion: As a subtle human behavioral trait, deception is hard to detect in real-life scenarios. Its reliable detection needs to resort to multiple modalities including the visual, verbal, and acoustic <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b20">21]</ref>. Among them, the visual modality is considered to be the most informative one. Multiple visual cues also exist visually. In particular, facial expressions <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b32">33]</ref> and body motions <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b30">31]</ref> are typically the focus of visual analysis. An important problem thus arises: How to effectively fuse these modalities/cues? Such a fusion is not straightforward because they not only have different strengths in each individual video sequence, but also are temporally asynchronized. An example of the asynchronization between the face and body cues is shown in Figure <ref type="figure" target="#fig_0">1</ref>. (2) Data scarcity: Unlike the conventional physiological and biological methods <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b7">8]</ref>, an ADD model is non-contact and non-invasive. This indirectness means that collecting large quantity of high-quality data containing samples of deceptive behaviors is critical. Earlier data collection efforts focused on human contributors in a lab or in a crowdsourcing setting. In other words, they are staged; the usefulness of these datasets for real-world deployment is thus questionable. Recently, the focus of ADD has been towards detecting deceptive behaviors from reallife data. Particularly, a new multimodal benchmark dataset of real-life videos from court trials is introduced in <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref>. However, with only 121 video clips and half of them containing deception, this dataset is insufficient for training a deep neural network based model that has dominated the recent ADD approaches <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b19">20]</ref>.</p><p>We address both problems in this paper. For the multimodal fusion problem, we propose a novel face-focused cross-stream network (FFCSN). Different from the popular two-stream networks <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b34">35]</ref>, our FFCSN model has two novel components: (a) Face detection is added into the spatial stream subnet to capture the facial expressions explicitly. (b) Correlation learning is performed across the spatial and temporal streams for joint deep feature learning from facial expressions and body motions. Importantly, our model is able to cope with the asynchronization/temporal inconsistency between facial expressions and body motions (see Figure <ref type="figure" target="#fig_0">1</ref>). For the training data scarcity problem, we introduce meta learning <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b24">25]</ref> and adversarial learning <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref> into the training process of our FFCSN. Meta learning, based on the principle of learning to learn, is deployed here to improve the generalization ability of the model and avoid overfitting to the insufficient training data. In the meantime, adversarial learning based feature synthesis is adopted as a data augmentation strategy. When these two are combined, our FFCSN can be trained effectively even with the very sparse data in the existing real-life deception detection benchmarks <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref>.  <ref type="formula" target="#formula_3">3</ref>) We demonstrate that our FFCSN model can be easily extended to other human-centric video analysis problems such as emotion recognition from user-generated videos <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b17">18]</ref>. Extensive experiments are carried out on benchmark datasets and the results show that our model clearly outperforms existing state-of-the-art alternatives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Video-Based Deception Detection. Earlier works on video-based ADD are limited by the datasets which contain only staged deceptive behaviors <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b20">21]</ref>. Their usefulness for detecting real-life deception is thus in doubt. The change towards deception detection with real-life data was first advocated in <ref type="bibr" target="#b6">[7]</ref>, where the identification of deception in statements issued by witnesses and defendants is targeted using a corpus collected from hearings in Italian courts (i.e., no visual data was available). In <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref>, a new multimodal deception dataset of real-life videos from court trials was first introduced, and the combination of features extracted from different modalities is used for deception detection. Thanks to this benchmark dataset, more advancing ADD methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b49">50]</ref> have been developed to leverage multimodal features for detecting deception. Deep Learning for Deception Detection. Recent ADD methods typically benefit from the latest development in deep neural networks <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b19">20]</ref>. However, it is noted in <ref type="bibr" target="#b49">[50]</ref> that, given the small size of the real-life ADD benchmark introduced in <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref>, hand-crafted features are much better than deep features. This is not surprising: deep learning models are known to be data hungry. The real-life ADD dataset in <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref> only provides around 100 video clips, which is a number of magnitudes smaller than, for example, those YouTube-collected action recognition benchmark datasets such as UCF101 <ref type="bibr" target="#b40">[41]</ref>. Our model differs significantly from existing deep ADD models in that the data scarcity problem is addressed explicitly, based on a meta learning and adversarial learning based training strategy. Adversarial learning <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref> has recently been used as a data augmentation strategy to deal with the lack of training data. However, meta-learning <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b24">25]</ref> was originally proposed for transfer learning. Here, we re-purpose it for learning with scarce data and uniquely combine it with adversarial learning to cope with the extreme challenge of data scarcity in ADD. We show in experiments that our model outperforms <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b49">50]</ref> by big margin, thanks to the proposed training strategy (see Tables <ref type="table" target="#tab_1">1 and 2</ref>). Two-Stream Network. Our FFCSN model adopts a twostream network architecture, one for RGB still frame modeling and the other for optical flow extracted from consecutive frames. Such a two-stream architecture was originally proposed for action recognition in videos and has been popular for many human-centric video analysis tasks <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b5">6]</ref>. Various improvements such as temporal segment network (TSN) <ref type="bibr" target="#b46">[47]</ref> and its variants <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b61">62]</ref> have been designed by capturing the long-range temporal structure and learning the ConvNet models with limited training samples. Similarly, <ref type="bibr" target="#b34">[35]</ref> proposed to add faster R-CNN <ref type="bibr" target="#b37">[38]</ref> so that attention can be focused on objects detected in a video. Our FFC-SN model is different from existing two-stream models in that: (1) face detection is added into the spatial stream subnet to capture the facial expressions explicitly; (2) correlation learning is performed across the spatial and temporal streams to cope with the temporal inconsistency between facial expressions and body motions for ADD.</p><p>Video-Based Emotion Recognition. Deception detection is closely related to emotion recognition: deception could be considered as a specific emotion state of humans, albeit it is much more subtle and harder to detect than others such as happy and angry. Note that emotion recognition from usergenerated videos <ref type="bibr" target="#b17">[18]</ref> is a challenging problem. Because of the complicated and unstructured nature of user-generated videos and the sparsity of video frames that express the emotion content, it is often hard to understand emotions conveyed in user-generated videos. To address this problem, multi-modal fusion and knowledge transfer approaches have been proposed in recent works <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b53">54]</ref>. In this paper, we show that our FFCSN model can be easily extended to emotion recognition from user-generated videos, with state-of-the-art results achieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>As illustrated in Figure <ref type="figure">2</ref>, our full FFCSN model for video-based ADD consists of three main modules: facefocused cross-stream network including a facial expression branch as well as a body motion branch, meta learning module, and adversarial learning module. In the following, we give the details of the three main modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Cross-Stream Network Module</head><p>In this work, we focus on joint deep feature learning from facial expressions and body motions for video-based ADD. Different from the traditional video-based action recognition, the facial expressions and body motions of a subject are found to be related to his/her deceptive behaviors <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b30">31]</ref>, rather than the whole frame appearance. Therefore, we choose to modify the original twostream temporal segment network <ref type="bibr" target="#b46">[47]</ref> designed for videobased action recognition by replacing its appearance branch with a face expression branch (see Figure <ref type="figure">2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Cross-Stream Base Network</head><p>The spatial stream (i.e. face expression branch) is a face detection model based on the popular faster R-CNN <ref type="bibr" target="#b37">[38]</ref>. This branch follows the deep learning framework of faster R-CNN, which has been shown to achieve state-of-the-art results in generic object detection. As illustrated in Figure <ref type="figure">2</ref>, it essentially consists of two parts: (1) a region proposal network (RPN) for generating a list of region proposals which may contain objects, called regions of interest (RoIs); (2) a R-CNN network for classifying the regions of each frame into objects and refining the boundaries of these regions. The two parts share common parameters in the convolutional layers used for feature extraction, allowing it to accomplish the face detection task efficiently.</p><p>In our model, faster R-CNN is generalized for both face detection and expression feature extraction. Note that the traditional faster R-CNN takes only 9 anchors, which sometimes fails to recall small objects. For our face detection task, however, small faces tend to be fairly common. We thus add a size group of 64×64 and increase the number of anchors to 12. In this paper, the RPN batchsize is set to 256, and the ResNet50 <ref type="bibr" target="#b12">[13]</ref> is used as the backbone model for the face expression branch.</p><p>The temporal stream (i.e. body motion branch) operates on a stack of consecutive warped optical flow fields to capture the motion information. Inspired by the representative work on improved dense trajectories <ref type="bibr" target="#b45">[46]</ref>, we extract the warped optical flow by first estimating the homography matrix and then compensating the camera motion. This branch can thus avoid concentrating on the camera motion but not on the body motion. As shown in Figure <ref type="figure">2</ref>, ResNet50 is used to compute the temporal feature maps. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Cross-Stream Fusion</head><p>In our cross-stream base network, one stream focuses on the face that is only a part of the whole frame, and the other focuses on the body motion that is captured using multiple whole frames. These two parts are clearly complementary to each other. We thus combine the two branches by crossstream fusion as illustrated in Figure <ref type="figure">3</ref>. Importantly, the fusion is based on deep correlation analysis rather than simply concatenating the feature vectors extracted from the two streams as in conventional two-stream networks. Specifically, to cope with the asynchronization/temporal inconsistency between facial expressions and body motions (see Figure <ref type="figure" target="#fig_0">1</ref>), we choose to learn the correlation among adjacent frames (only 5 adjacent frames are considered here). For the spatial stream, we downsample the feature maps of the final residual block of ResNet50 <ref type="bibr" target="#b12">[13]</ref> in the dimension of depth and obtain a 1024-dimension feature vector. For the temporal stream, given that five motion frames are matched to one face frame, we utilize the reshape pooling to obtain one 5×512-dimension feature vector after the third residual block of ResNet50. The outputs of the two streams are then concatenated and fed into two fully-connected layers (with the dimension of 128 and 5, respectively). Finally, we compute the correlation scores α =[ α 1 , ..., α 5 ] for the five face-motion pairs using the softmax function, and weight them with α for final two-stream fusion.</p><p>To extract deep visual features from a long-term video, our model essentially works on a sequence of short snippets sparsely sampled from the entire video. After each snippet of this sequence predicts its own result, a consensus among all snippets is obtained as the final video-level prediction. For all obtained video-level predictions, we can define a segmental consensus classification loss similar to that of temporal segment network <ref type="bibr" target="#b46">[47]</ref>. Specifically, we divide each video into K segments {S 1 ,S 2 ,...,S K } of equal duration. From each segment S k (k =1 , ..., K), we then randomly sample a short snippet T k . In our problem, the short snippet T k consists of one spatial frame (denoted as T k (sf 1 )) and five temporal frames (denoted as T k (tf 1 ), ..., T k (tf 5 )). Suppose that all snippets/frames have been represented as feature maps here. We thus have</p><formula xml:id="formula_0">T k =[ T k (sf 1 ), 5 j=1 α j T k (tf j )].</formula><p>Let F(T k ; W ) denote the classification probability predicted by our model with parameters W for T k . The outputs of all short snippets are combined by the segmental consensus function E to obtain a consensus of prediction among them. With the softmax loss, the overall loss of our model is defined as:</p><formula xml:id="formula_1">LBASE(y, E)=− Nc i=1 yi(Ei − log Nc j=1 exp Ej),<label>(1)</label></formula><p>where E is the segment consensus computed by E = E(F(T 1 ; W ), F(T 2 ; W ),...,F(T K ; W )), N c is the number of target classes (N c =2in our problem), and y i is the ground truth label with respect to class i. We define the consensus function E with average pooling, as in <ref type="bibr" target="#b46">[47]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Meta Learning Module</head><p>Deception detection is a challenging task due to the subtle differences between truthful and deceptive behaviors. Learning to differentiate the two types of behaviors with only a handful of samples of each is extremely challenging. This is especially true when the behaviors are modeled with deep neural networks with a large number of model parameters. To deal with the data scarcity problem, we propose to use meta learning <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b41">42]</ref> to train our FFCSN (see Figure <ref type="figure">2</ref>). To best utilize the limited training samples, we introduce pair-wise comparison of them. Specifically, our cross-stream base network can be viewed as the encoding submodule f of our meta learning module. A comparison submodule g is then introduced for meta learning. The meta learning pipeline is illustrated in Figure <ref type="figure" target="#fig_3">4</ref>. Examples of the two classes (yellow deceptive and blue truthful) are shown in different colors. In this case, the meta-train set contains five samples (four truthful and one deceptive). The deceptive sample in the meta-validation set is used to form five pairs with the meta-train samples. The final model output, after softmax, is a 5D logit vector supervised to produce a close-to-one value in the third element close-to-zero values in all other elements. This meta-learning pipeline turns a two-class (deceptive/deceptive) classification problem into a multi-case classification problem and makes full use of the limited training samples.</p><p>Formally, in each mini-batch (with the mini-batch size N b ), videos x i (i =1 , 2,...,N b ) are fed through the encoding submodule f , which outputs the concatenated feature maps f (x i )(i =1 , 2,...,N b ). We split the videos in the mini-batch into the meta-train and meta-validation sets. A sample x a is randomly chosen from the meta-validation set. The output f (x a ) is combined with each f (x j )(j = a) in the meta-train set using the operator C(f (x a ),f(x j )).I n our meta learning module, we set C(•, •) as the concatenation of feature maps in the dimension of depth. The combined feature maps of the sample pairs are fed into the comparison submodule g, which produces a pairwise score representing the similarity between x a and x j . We thus generate the pairwise scores for each mini-batch as:</p><formula xml:id="formula_2">ra,j = g(C(f (xa),f(xj))),j = a.<label>(2)</label></formula><p>We train our meta learning module by fitting the pairwise score r a,j to the ground truth pairwise similarity with a cross entropy loss as follows:</p><formula xml:id="formula_3">LML= −1 N b − 1 j =a yj log(ra,j)+(1 − yj)log(1− ra,j),<label>(3)</label></formula><p>where y j =1if (x a ,x j ) is an intra-class sample pair, and y j =0if (x a ,x j ) is an inter-class sample pair.</p><p>As illustrated in Figure <ref type="figure" target="#fig_3">4</ref>, the encoding submodule of our meta learning module is just our cross-stream base network. In the following, we give the details of the comparison submodule of our meta learning module. Specifically, the comparison submodule consists of two convolutional blocks and </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Adversarial Learning Module</head><p>In this paper, we aim to synthesize feature vectors for data augmentation in the ADD task. Note that synthesizing raw videos explicitly is an unsolved problem in itself. Therefore, we choose to generate a 256-dimension feature vector for each synthesized video instead, which is a much easier task. In particular, we propose to synthesize fake feature vectors and attack the classifier for deception detection during training of our full FFCSN model, in order to overcome the training data scarcity problem.</p><p>Adversarial training involves a discriminator and a generator. In our case, the discriminator network aims to classify the inputs into two classes: real or fake. In this paper, the observed variable x is the 256-dimensional vector produced by our cross-stream base network. Given that the discriminator network D consists of 3 fully-connected layers with the ELU activation, D(x) thus denotes the probability that x comes from the real (but not fake) class.</p><p>As for the generator network G of our adversarial learning module, the input 32-dimensional noise z is sampled from a zero-mean Gaussian distribution p z (z) with the standard deviation 1. We use 3 hidden layers to represent G with the size 32, 64, and 256, respectively. The first fullyconnected layer uses the ELU activation, and the second fully-connected layer uses the sigmoid activation. G(z) denotes a generated sample drawn from the data space.</p><p>The adversarial training of D and G can be formulated as the following min-max problem:</p><formula xml:id="formula_4">min G max D LAL(G, D)=E x∼p data (x) [log D(x)] + E z∼pz (z) [log(1 − D(G(z)))],<label>(4)</label></formula><p>where L AL denotes the loss function of our adversarial learning module and p data (x) denotes the data distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training Process</head><p>Our full FFCSN model for video-based ADD is trained using an end-to-end training strategy. The loss function of our full FFCSN model is defined as follows:</p><formula xml:id="formula_5">L = LBASE + β1LML + β2LAL,<label>(5)</label></formula><p>where β 1 and β 2 denote the hyper-parameters. In this paper, we empirically set β 1 = β 2 =1in all experiments. Real-Life Dataset. We evaluate our full FFCSN model for deception detection on a real-life multimodal dataset <ref type="bibr" target="#b35">[36]</ref>. This dataset consists of 121 court room trial video clips. Since videos from this trial dataset are collected under unconstrained conditions, we need to cope with the change of the viewing angle of the person, the variation in video quality, and the background noise. In this paper, we select a subset of 104 videos from the original trial dataset, including 50 truthful videos and 54 deceptive videos, as in <ref type="bibr" target="#b49">[50]</ref>. Evaluation Setting. Our dataset consists of only 58 identities. Since the number of identities is smaller than the number of video clips, the same identity often appear in both deceptive truthful clips. When the videos of the same identity are divided into both the training and test sets, a deception detection method tends to suffer from over-fitting to identities. To address this over-fitting issue, we perform 10-fold cross validation over identities (but not over video samples) as in <ref type="bibr" target="#b49">[50]</ref>, which ensures that the identities in the test set have no overlap with that in the training set. Evaluation Metrics. To evaluate the performance of a deception detection method, we compute two metrics as follows: (1) ACC -the classification accuracy (ACC) over the test video samples; (2) AUC -the area under the precisionrecall curve (AUC) over the test set, which is originally defined to cope with the imbalance of the positive and negative classes. The former has been widely used in previous research on deception detection <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b8">9]</ref>, while the latter is mainly used in recent works <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b19">20]</ref>. Network Initialization. We pretrain the face branch of our cross-stream base network using the WIDER-FACE <ref type="bibr" target="#b54">[55]</ref> and CK+ <ref type="bibr" target="#b28">[29]</ref> datasets, and then pretrain the motion branch of our cross-stream base network as in <ref type="bibr" target="#b46">[47]</ref> on the UCF101 <ref type="bibr" target="#b40">[41]</ref> dataset. Moreover, for G and D of the adversarial learning module, we adopt the Kaiming initialization. All the other layers are randomly initialized by drawing weights from a zero-mean Gaussian distribution with the standard deviation 0.01 (along with zero biases). Implementation Details. After network initialization, our full FFCSN model is trained in an end-to-end manner using back-propagation and stochastic gradient descent. The  learning rate is set to 0.0005 for the first 10 epochs, and then reduced to one tenth with a step size of 10 (epochs). The maximum number of epochs is set to 100. A momentum of 0.9 and a weight decay of 0.01 are also set for model training. We train our full FFCSN model on two Tesla K40 GPUs, with the batch size 12. Our implementation is developed within the PyTorch framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Ablation Study Results</head><p>To show the contribution of each main module of our full FFCSN model, we make comparison to its five simplified versions: (1) Face -Only the face branch of our crossstream base network used for ADD; (2) Motion -Only the motion branch of our cross-stream base network used for ADD; (3) Face+Motion -our cross-stream base network including the face and motion branches (but without cross-stream correlation learning); (4) Face+Motion+CLour cross-stream base network with cross-stream correlation learning (CL); (5) Face+Motion+CL+ML -our crossstream base network further boosted with meta learning (M-L). Our full model including adversarial learning (AL) is denoted as Face+Motion+CL+ML+AL.</p><p>The ablation study results are presented in Table <ref type="table" target="#tab_0">1</ref>. It can be seen that: <ref type="bibr" target="#b0">(1)</ref> The performance continuously increases when more modules are used for ADD, showing the contribution of each module. ( <ref type="formula" target="#formula_2">2</ref>  <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b19">20]</ref>.</p><p>Both ML and AL clearly lead to performance improvements, which provides evidence that they have a good ability of alleviating the training data scarcity. (4) The effectiveness of cross-stream correlation learning is validated by the comparison Face+Motion+CL vs. Face+Motion. This is further supported by Figure <ref type="figure" target="#fig_5">5</ref>, where our cross-stream correlation learning is found to learn quite different correlation distributions for the truthful/deceptive classes. That is, the learned correlations indeed improve the discriminativeness of deep visual features for deception detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Comparative Results</head><p>We further make comparison to the state-of-the-art alternatives <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b19">20]</ref>. Since all of these methods are multimodal, we also include the acoustic and verbal modalities: Acoustic Feature Learning. We extract the spectrum map from each wave audio of 44,100 Hz sampling rate, and convert each spectrum map into images of fixed size using a sliding window with the window size 300. By taking only the last 300 dimensions along the spectrum height, we obtain a set of samples of the size 300*300. These samples are finally used to train ResNet50. For robust training, ML and AL are similarly exploited for acoustic feature learning. Verbal Feature Learning. We segment the transcript of each video to words, and then employ the word2vec technique <ref type="bibr" target="#b9">[10]</ref> to convert each word into a 300-dimensional feature vector. The feature vectors of all words are averaged as the verbal feature vector of a video. The average vector is fed into three layers of fully connected layers (of the size 300*128, 128*64, and 64*32), resulting in a final vector of 32 dimensions. For robust verbal feature learning, ML and AL are also used like visual feature learning.</p><p>The comparative results on the real-life benchmark dataset <ref type="bibr" target="#b35">[36]</ref> are given in Table <ref type="table" target="#tab_1">2</ref>. We observe that: (1) Our robust deep feature learning approach clearly performs the best under the multimodal setting, validating the effectiveness of exploiting ML and AL for addressing the training data scarcity issue associated with real-life ADD. (2) When only the visual modality is concerned, our robust deep feature learning approach even outperforms the state-of-the-art multimodal deception detection method <ref type="bibr" target="#b49">[50]</ref>. (3) Our multimodal approach achieves performance improvements over the latest deep learning methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b19">20]</ref>, due to the extra use of ML and AL in our approach. In addition, we also provide the comparative results of modality fusion for our approach in Figure <ref type="figure">6</ref>. As expected, our approach is shown to obtain more significant improvements when more modalities are used for deception detection in videos.   Alternative Losses for Pairwise Comparison. In this paper, our loss defined in Eq. ( <ref type="formula" target="#formula_3">3</ref>) is used for pairwise comparison. To show its effectiveness, we compare it to two typical pairwise losses under the same setting: loss of Siamese network <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b42">43]</ref>, and loss of triplet network <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b14">15]</ref>. The conventional non-pairwise loss is also included as the baseline. The comparative results in Figure <ref type="figure" target="#fig_9">9</ref> show that: (1) All three pairwise losses clearly lead to better results than the conventional non-pairwise loss, validating the effectiveness of pairwise comparison for deception detection. (2) The loss defined in Eq. ( <ref type="formula" target="#formula_3">3</ref>) performs the best among the three pairwise losses, i.e., the meta learning module is more capable of modelling the complicated relationships among video samples than the Siamese network and triplet network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Video-Based Emotion Recognition</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Dataset and Setting</head><p>The YouTube-8 dataset <ref type="bibr" target="#b17">[18]</ref> is used for performance evaluation. This dataset consists of 1,101 videos (downloaded from YouTube) annotated with 8 basic emotions: anger, anticipation, disgust, fear, joy, sadness, surprise, and trust. We randomly generate 10 train/test splits, each using 2/3 of the dataset for training and 1/3 for testing. used as the evaluation metric. Our FFCSN model is trained exactly the same as in Section 4.1, and only visual features are extracted from raw videos for emotion recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Comparative Results</head><p>We compare our FFCSN model to the state-of-the-art alternatives <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b53">54]</ref>. The comparative results are presented in Table <ref type="table" target="#tab_2">3</ref>. We have the following observations: (1) Our FFCSN model achieves significant improvements over the state-of-the-art models, validating the effectiveness of our face-focused cross-stream network for emotion recognition from user-generated videos. Note that the biggest challenge of this emotion recognition task lies in the complicated and unstructured nature of user-generated videos and the sparsity of video frames that express the emotion content. Our FFCSN model is clearly effective in overcoming this challenge. <ref type="bibr" target="#b1">(2)</ref> The improvements obtained by our FFCSN model are really impressive, given that only visual features are extracted by our model, whilst at least two modalities are used by all other models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have investigated the challenging problem of deception detection from real-life videos. For joint deep feature learning from facial expressions and body motions, we have proposed a novel face-focused cross-stream network (FFCSN). Importantly, different from existing twostream networks, our FFCSN model enables us to cope with the temporal inconsistency between facial expressions and body motions for ADD. Moreover, we have also developed a new training approach for our FFCSN model by inducing meta learning and adversarial learning into the training process of our base model. As a result, our FFCSN model can be trained effectively even with only a handful of training samples. Extensive experiments show that the proposed FFCSN model achieves state-of-the-art results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Illustration of the asynchronization between facial expressions and body motions. It can be seen that a subject who lies tends to first have a surprised expression before she/he is aroused to take body/hand movements. Notation: ∆t = 2-3 frames.</figDesc><graphic url="image-4.png" coords="2,321.83,167.21,93.26,52.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Our contributions are three-fold: (1) We have proposed a novel face-focused cross-stream network (FFCSN) for joint deep feature learning from facial expressions and body motions in real-life videos. Comparing to existing two-stream networks, our FFCSN model is uniquely able to cope with the asynchronization/temporal inconsistency between facial expressions and body motions. (2) To avoid model overfitting and improve generalization ability, meta learning and adversarial learning are introduced into the training process of FFCSN. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .Figure 3 .</head><label>23</label><figDesc>Figure 2. Overview of our full FFCSN model. Note that α collects the correlation scores of matching the spatial and temporal streams. In such cross-stream fusion, we select 5 frames with the same interval. In this figure, we use [t, t+1,..., t+5] for easily understanding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Architecture of the meta learning module used in our full FFCSN model for video-based ADD. See text for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>4. 1 .</head><label>1</label><figDesc>Video-Based Deception Detection 4.1.1 Dataset and Setting</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Illustration of the two mean correlation distributions (i.e. α =[ α1, ..., α5]) obtained with our cross-stream network averaged over the truthful and deceptive test samples, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .Figure 7 .</head><label>67</label><figDesc>Figure 6. Comparative results obtained by multi-modality fusion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>4. 1 . 4</head><label>14</label><figDesc>Further EvaluationsComparison to Temporal Segment Network. Different from the state-of-the-art temporal segment network (TSN)<ref type="bibr" target="#b46">[47]</ref>, our FFCSN model has two novel components: face detection and correlation learning. To show the contribution of these two components, we obtain two variants of our FFCSN model by adding face detection (FD) and correlation learning (CL) into TSN: (1) TSN+FD: face detection is added to the spatial stream of TSN; (2) TSN+FD+CL: cross-stream correlation learning is further used to boost T-SN+FD. The comparative results in Figure7clearly show that both components are effective for ADD. Model Selection for Meta Learning. As illustrated in Figure4, the number of sample pairs in each sampled task in the meta-learning pipeline is empirically set to 5. To evaluate the impact of the task size on the model performance, Figure8compares different task sizes. It can be clearly seen that our model approaches the peak at 5, but it is in general insensitive to the task size selection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Illustration of the effect of the number of sample pairs used for pairwise comparison on the performance of meta learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Comparative results obtained by employing different losses for pairwise comparison.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Ablation study results (%) for our full FFCSN model.</figDesc><table><row><cell>Model</cell><cell>ACC</cell><cell>AUC</cell></row><row><cell>Face</cell><cell>84.33</cell><cell>84.11</cell></row><row><cell>Motion</cell><cell>86.00</cell><cell>88.63</cell></row><row><cell>Face+Motion</cell><cell>88.21</cell><cell>90.57</cell></row><row><cell>Face+Motion+CL</cell><cell>89.16</cell><cell>91.89</cell></row><row><cell>Face+Motion+CL+ML</cell><cell>92.33</cell><cell>95.83</cell></row><row><cell>Face+Motion+CL+ML+AL</cell><cell>93.16</cell><cell>96.71</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>) The improvements achieved by Face+Motion over Face/Motion show that both face expression and body motion are important cues for ADD. (3) Comparative results (%) for video-based ADD. Note that extra human annotated micro-expressions are used in</figDesc><table><row><cell>Model</cell><cell>ACC</cell><cell>AUC</cell></row><row><cell>[36] (visual+verbal)</cell><cell>75.20</cell><cell>-</cell></row><row><cell>[37] (visual+verbal)</cell><cell>77.11</cell><cell>-</cell></row><row><cell>[17] (visual+acoustic+verbal)</cell><cell>78.95</cell><cell>-</cell></row><row><cell>[9] (visual+acoustic+verbal)</cell><cell>96.42</cell><cell>-</cell></row><row><cell>[50] (visual+acoustic+verbal)</cell><cell>-</cell><cell>92.21</cell></row><row><cell>[20] (visual+acoustic+verbal)</cell><cell>96.14</cell><cell>97.99</cell></row><row><cell>Ours (visual)</cell><cell>93.16</cell><cell>96.71</cell></row><row><cell>Ours (visual+acoustic+verbal)</cell><cell>97.00</cell><cell>99.78</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>The averaged recognition accuracy (ACC) over 10 random train/test splits is Comparative results (%) of video-based emotion recognition on the YouTube-8 dataset.</figDesc><table><row><cell>Model</cell><cell>multimodal</cell><cell>ACC</cell></row><row><cell>[18]</cell><cell>visual+acoustic+attribute</cell><cell>46.1</cell></row><row><cell>[34]</cell><cell>visual+acoustic+attribute</cell><cell>51.1</cell></row><row><cell>[57]</cell><cell>visual+attribute</cell><cell>52.5</cell></row><row><cell>[54]</cell><cell>visual+acoustic</cell><cell>52.6</cell></row><row><cell>[53]</cell><cell>visual+acoustic</cell><cell>52.6</cell></row><row><cell>Ours</cell><cell>visual</cell><cell>57.8</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was partially supported by National Natural Science Foundation of China (61573363 and 61832017), and the Fundamental Research Funds for the Central Universities and the Research Funds of Renmin University of China (15XNLQ01).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Detecting deceptive behavior via integration of discriminative features from multiple modalities</title>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Abouelenien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Verónica</forename><surname>Pérez-Rosas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihai</forename><surname>Burzo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Beyond triplet loss: a deep quadruplet network for person re-identification</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaotang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianguo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="403" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="539" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Cues to deception</title>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">J</forename><surname>Bella M Depaulo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><forename type="middle">E</forename><surname>Lindsay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Malone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelly</forename><surname>Muhlenbruck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harris</forename><surname>Charlton</surname></persName>
		</author>
		<author>
			<persName><surname>Cooper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="74" to="118" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Control and resistance in the psychology of lying</title>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Derksen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theory &amp; Psychology</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="196" to="212" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automatic deception detection in Italian court cases</title>
		<author>
			<persName><forename type="first">Tommaso</forename><surname>Fornaciari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimo</forename><surname>Poesio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence and Law</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="303" to="340" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mind reading using neuroimaging: Is this the future of deception detection?</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Gamer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Psychologist</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">172</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep learning driven multimodal fusion for automated deception detection</title>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Gogate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahsan</forename><surname>Adeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Hussain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium Series on Computational Intelligence</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">word2vec explained: deriving Mikolov et al.&apos;s negative-sampling word-embedding method</title>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1402.3722</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Combining prosodic lexical and cepstral systems for deceptive speech detection</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Graciarena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elizabeth</forename><surname>Shriberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Enos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julia</forename><surname>Hirschberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sachin</forename><surname>Kajarekar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
				<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Distinguishing deceptive from non-deceptive speech</title>
		<author>
			<persName><forename type="first">Julia</forename><surname>Hirschberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Benus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">M</forename><surname>Brenier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Speech Communication and Technology</title>
				<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1833" to="1836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep metric learning using triplet network</title>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nir</forename><surname>Ailon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Similarity-Based Pattern Recognition</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="84" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Acoustic correlates of deceptive speech-an exploratory study</title>
		<author>
			<persName><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christin</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><surname>Kirchhübel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Engineering Psychology and Cognitive Ergonomics</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="28" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The truth and nothing but the truth: Multimodal analysis for deception detection</title>
		<author>
			<persName><forename type="first">Mimansa</forename><surname>Jaiswal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sairam</forename><surname>Tabibu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajiv</forename><surname>Bajpai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM Workshops</title>
				<imprint>
			<date type="published" when="2007">2016. 2, 3, 6, 7</date>
			<biblScope unit="page" from="938" to="943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Predicting emotions in user-generated videos</title>
		<author>
			<persName><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baohan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2008">2014. 1, 2, 3, 8</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="73" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Detecting deception using functional magnetic resonance imaging</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Kozel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><forename type="middle">A</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiwen</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><forename type="middle">L</forename><surname>Grenesko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Laken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">S</forename><surname>George</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological Psychiatry</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="605" to="613" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">A deep learning approach for multimodal deception detection</title>
		<author>
			<persName><forename type="first">Gangeshwar</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navonil</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<idno>arXiv preprint arX- iv:1803.00344</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Combining acousticprosodic, lexical, and phonotactic features for automatic deception detection</title>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Ita Levitan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guozhen</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rivka</forename><surname>Levitan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julia</forename><surname>Hirschberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
				<imprint>
			<date type="published" when="2006">2006-2010, 2016</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Cross-cultural production and detection of deception from speech</title>
		<author>
			<persName><forename type="first">Guzhen</forename><surname>Sarah I Levitan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandi</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gideon</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julia</forename><surname>Mendels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michelle</forename><surname>Hirschberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><surname>Rosenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Workshop on Multimodal Deception Detection</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Adversarial learning for neural dialogue generation</title>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sébastien</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06547</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Insightgan: Semi-supervised feature learning with generative adversarial network for drug abuse detection</title>
		<author>
			<persName><forename type="first">Guangzhen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">An</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyu</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqi</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Zero and few shot learning with semantic feature synthesis and competitive learning</title>
		<author>
			<persName><forename type="first">Zhiwu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiechao</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aoxue</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">An</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.08332</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Latent semantic learning by efficient sparse coding with hypergraph regularization</title>
		<author>
			<persName><forename type="first">Zhiwu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="411" to="416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Latent semantic learning with structured sparse representation for human action recognition</title>
		<author>
			<persName><forename type="first">Zhiwu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1799" to="1809" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Spectral learning of latent semantics for action recognition</title>
		<author>
			<persName><forename type="first">Zhiwu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horace</forename><forename type="middle">Hs</forename><surname>Ip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1503" to="1510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The extended Cohn-Kanade dataset (CK+): A complete dataset for action unit and emotion-specified expression</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takeo</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zara</forename><surname>Ambadar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iain</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
				<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="94" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Large-scale affective content analysis: Combining media content features and facial reactions</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Mcduff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Soleymani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Automatic Face &amp; Gesture Recognition</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="339" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Motion profiles for deception detection using visual cues</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Dilsizian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judee</forename><forename type="middle">K</forename><surname>Burgoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Human-centered video feature selection via mRMR-SCMMCCA for preference extraction</title>
		<author>
			<persName><forename type="first">Takahiro</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshiaki</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satoshi</forename><surname>Asamizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miki</forename><surname>Haseyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IE-ICE Trans. Information and Systems</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="409" to="412" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The design and development of a lie detection system using facial microexpressions</title>
		<author>
			<persName><forename type="first">Michel</forename><surname>Owayjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmad</forename><surname>Kashour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nancy</forename><forename type="middle">Al</forename><surname>Haddad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamad</forename><surname>Fadel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ghinwa</forename><forename type="middle">Al</forename><surname>Souki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Advances in Computational Tools for Engineering Applications</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep multimodal learning for affective analysis and retrieval</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiai</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong-Wah</forename><surname>Ngo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2008">2008-2020, 2015. 3, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multi-region twostream R-CNN for action detection</title>
		<author>
			<persName><forename type="first">Xiaojiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deception detection using real-life trial data</title>
		<author>
			<persName><forename type="first">Verónica</forename><surname>Pérez-Rosas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Abouelenien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihai</forename><surname>Burzo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimodal Interaction</title>
				<imprint>
			<date type="published" when="2007">2015. 1, 2, 3, 6, 7</date>
			<biblScope unit="page" from="59" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Verbal and nonverbal clues for real-life deception detection</title>
		<author>
			<persName><forename type="first">Verónica</forename><surname>Pérez-Rosas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Abouelenien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihai</forename><surname>Linton</surname></persName>
		</author>
		<author>
			<persName><surname>Burzo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2007">2015. 1, 2, 3, 6, 7</date>
			<biblScope unit="page" from="2336" to="2346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A simple neural network module for relational reasoning</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">G</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">UCF101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName><forename type="first">Flood</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1199" to="1208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Gated Siamese convolutional neural network architecture for human re-identification</title>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Rama Varior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mrinal</forename><surname>Haloi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="791" to="808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Detecting Lies and Deceit: The Psychology of Lying and Implications for Professional Practice</title>
		<author>
			<persName><forename type="first">Aldert</forename><surname>Vrij</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Wiley Series on the Psychology of Crime, Policing and Law</title>
				<meeting><address><addrLine>Hoboken, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Detecting deception by manipulating cognitive load</title>
		<author>
			<persName><forename type="first">Aldert</forename><surname>Vrij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronald</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samantha</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharon</forename><surname>Leal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in cognitive sciences</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="141" to="142" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3551" to="3558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2007">2016. 1, 2, 3, 4, 6, 7</date>
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Video affective content analysis: a survey of state of the art methods</title>
		<author>
			<persName><forename type="first">Shangfei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Affective Computing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="410" to="430" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deep bimodal regression of apparent personality traits from short video sequences</title>
		<author>
			<persName><forename type="first">Xiu-Shen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen-Lin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Affective Computing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="303" to="315" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deception detection in videos</title>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Subrahmanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2007">2018. 2, 3, 6, 7</date>
			<biblScope unit="page" from="1695" to="1702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deception detection via blob motion pattern analysis</title>
		<author>
			<persName><forename type="first">Fan</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junxian</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Affective Computing and Intelligent Interaction</title>
				<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Comparator networks</title>
		<author>
			<persName><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Video emotion recognition with transferred deep feature encodings</title>
		<author>
			<persName><forename type="first">Baohan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMR</title>
				<imprint>
			<date type="published" when="2008">2016. 3, 8</date>
			<biblScope unit="page" from="15" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Heterogeneous knowledge transfer in video emotion recognition, attribution and summarization</title>
		<author>
			<persName><forename type="first">Baohan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Affective Computing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="255" to="270" />
			<date type="published" when="2008">2018. 1, 2, 3, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">WIDER FACE: A face detection benchmark</title>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen-Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="5525" to="5533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Deep bimodal regression for apparent personality analysis</title>
		<author>
			<persName><forename type="first">Chen-Lin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiu-Shen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="311" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Recognition of emotions in usergenerated videos with kernelized features</title>
		<author>
			<persName><forename type="first">Haimin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2824" to="2835" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Real-time automatic deceit detection from involuntary facial expressions</title>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vartika</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">E</forename><surname>Slowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Venugopal</forename><surname>Govindaraju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Action recognition based on learnt motion semantic vocabulary</title>
		<author>
			<persName><forename type="first">Qiong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horace</forename><forename type="middle">Hs</forename><surname>Ip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific-Rim Conference on Multimedia</title>
				<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="193" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Temporal relational reasoning in videos</title>
		<author>
			<persName><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="803" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Spatial temporal pyramid matching using temporal sparse representation for human motion retrieval. The Visual Computer</title>
		<author>
			<persName><forename type="first">Liuyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Howard</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="845" to="854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">ECO: Efficient convolutional network for online video understanding</title>
		<author>
			<persName><forename type="first">Mohammadreza</forename><surname>Zolfaghari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamaljeet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="695" to="712" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
