<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Simultaneous Way-footprint Prediction and Branch Prediction for Energy Savings in Set-associative Instruction Caches</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Weiyu</forename><surname>Tang</surname></persName>
							<email>fwtang@ics.uci.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information and Computer Science</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>92697-3425</postCode>
									<settlement>Irvine Irvine</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rajesh</forename><surname>Gupta</surname></persName>
							<email>rgupta@ics.uci.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information and Computer Science</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>92697-3425</postCode>
									<settlement>Irvine Irvine</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alexandru</forename><surname>Nicolau</surname></persName>
							<email>nicolau@ics.uci.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information and Computer Science</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>92697-3425</postCode>
									<settlement>Irvine Irvine</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alexander</forename><forename type="middle">V</forename><surname>Veidenbaum</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Information and Computer Science</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>92697-3425</postCode>
									<settlement>Irvine Irvine</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Simultaneous Way-footprint Prediction and Branch Prediction for Energy Savings in Set-associative Instruction Caches</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Caches are partitioned into subarrays for optimal timing. In a set-associative c a c he, if the way holding the data is known before an access, only subarrays for that way need to be accessed. Reduction in cache switching activities results in energy savings.</p><p>In this paper, we propose to extend the branch prediction framework to enable way-footprint prediction. The next fetch address and its way-footprint are predicted simultaneously for one-way instruction cache access. Because the way-footprint prediction shares some prediction hardware with the branch prediction, additional hardware cost is small.</p><p>To enlarge the number of one-way c a c he accesses, we h a ve made modi cations to the branch prediction. Speci cally, w e h a ve i n vestigated three BTB allocation policies. Each policy results in average 29%, 33% and 62% energy savings with normalized execution time 1, 1, and 1.001 respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With advances in semiconductor technology, processor performance continues to grow with increasing clock rates and additional hardware support for instruction level parallelism. The side e ect is that power dissipation also increases signi cantly. With the maturity o f I C t e c hniques for power management, architectural and compiler techniques hold signi cant potential for power management 2 ]. These techniques decrease power dissipation by reducing the number of signal switching activities within the microprocessor.</p><p>High utilization of the instruction memory hierarchy is needed to exploit instruction level parallelism. Thus power dissipation by the on-chip instruction cache is also high. On-chip L1 instruction cache alone can comprise as high as 27% of the CPU <ref type="bibr">power 7]</ref>.</p><p>In this paper, we exploit cache way partitioning in the set-associative c a c hes for instruction cache energy savings. Fo r a c a c he access, if the way holding the instructions is known before the access, then only that particular way needs to be accessed. To know which way holds the instructions before an access, a wayfootprint prediction mechanism can be used. There are similarities between the branch prediction and the way-footprint prediction. One predicts the next fetch address based on current fetch address the other predicts the way-footprint of the next fetch address based on current f e t c h address. Thus we can extend the branch prediction framework to enable way-footprint prediction, which can signi cantly reduce the hardware cost for the way-footprint prediction. The rest of this paper is organized as follows. In Section 2, we present the motivation for this research. Section 3 describes the implementation of way-footprint prediction. The experimental results are given in Section 4. Section 5 compares wayfootprint prediction with related techniques for instruction cache energy savings. The paper is concluded with future work in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Motivation</head><p>For optimal timing, caches are partitioned into several subarrays so that wordline and bitline lengths are short. In high-performance processors, all the data subarrays and tag subarrays in a set-associative c a c he are accessed in parallel to achieve short access time. If the cache way holding the data is known before an access, only data subarrays and tag subarrays for that particular way need to be accessed. This reduces per cache access switching activities and hence results in energy savings.</p><p>One approach to predict the way-footprint for an address is to use the way-footprint of this address when it was accessed last time. For this purpose, his-tory way-footprints should be saved. A simple implementation is to use a way-footprint cache. Each e n try in the way-footprint c a c he is in the following format:</p><p>(addr tag, way-footprint). The size of the way-footprint eld is equal to log (n + 1), where n values are needed for one-way a ccess in a n-way set-associative cache and one value is needed for all-way access. For a 4-way set-associative cache, the size of the way-footprint eld is 3 bits. For a 2k-entry way-footprint c a c he and 4-byte instruction size, the size of addr tag eld is 21 bits. The tag (addr tag) cost is much higher than the data (wayfootprint) cost in terms of area and power.  To support instruction fetch across basic block boundaries, branch prediction is used in modern processors. Figure <ref type="figure" target="#fig_0">1</ref> shows a typical pipeline architecture with a branch predictor. For high-performance processors, multiple instructions are fetched simultaneously and it may take 2 or 3 cycles to access the instruction cache. Whether an instruction is a branch c a n only be determined a few stages later in the pipeline. If the branch predictor only uses a branch address to predict the next fetch address, there will be bubbles in the pipeline for instruction fetch or the branch miss prediction rate will be high. Thus in processors such as <ref type="bibr">G5 5]</ref>, the branch predictor uses current fetch address to predict the next fetch address every cycle.</p><p>Generally, there are three components in a branch predictor: branch direction predictor (BDP), branch target bu ers (BTB) and return address stack (RAS). The BDP predicts whether a branch will take the target path. The BTB predicts the target address for a taken branch. The RAS predicts the return address for a return instruction.</p><p>A BTB is organized as a RAM-based structure and is indexed by the fetch address. Each e n try in the BTB is in the following format:</p><p>(addr tag, target address).</p><p>A RAS is organized as a stack and only the top entry is accessed. Each e n try in the RAS is in the following format: (return address). Note that the same fetch address is used in both branch prediction and way-footprint prediction. If the tag comparison in the BTB fails, then the tag comparison in the way-footprint c a c he will also fail. Thus the tag used in the way-footprint c a c he is redundant and can be eliminated to reduce hardware cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Way-footprint Prediction</head><p>To support way-footprint prediction, a wayfootprint eld is added to the RAS entry. As the number of entries in a RAS is small and only the top entry is accessed during the branch prediction, the RAS access is not on one of the critical path. Consequently, adding the way-footprint eld to the RAS entry is unlikely to a ect the processor cycle time.</p><p>Adding way-footprint elds to the BTB entry will increase the BTB capacity. The BTB access time increases with capacity. T h i s m a y a ect the processor cycle time because the BTB access is often on one of the critical path. Thus a separate Way-Footprint Table (WFT) shown in Figure <ref type="figure" target="#fig_0">1</ref> is used instead. The numb e r o f w ays and the number of sets in the WFT is equal to those in the BTB. Each e n try in the WFT has the following two w ay-footprint elds: target address way-footprint fall-through address way-footprint The WFT access time is shorter than that of the BTB because the WFT capacity i s m uch smaller than the BTB capacity. T h us the WFT access is not on one of the critical path.  Figure <ref type="figure" target="#fig_2">2</ref> shows the way-footprint queue needed for the WFT and the RAS update. Entries \c 1" and \c 2" are reserved for the last two committed fetch addresses. Entries from \u head" to \u tail" are used to keep track o f t h e w ay-footprints for uncommitted fetch addresses.</p><p>When an instruction fetch nishes, the \fetch address" and \way-footprint " e l d s o f e n try \u tail" are updated. The \isCacheMiss" eld is set if this fetch has generated a cache miss.</p><p>When an instruction commits and its address matches the \fetch a d d r e s s " e l d o f e n try \u head", the following elds of entry \u head" are updated:</p><p>\isCall" is set if it is a call instruction \isBTBalloc" is set if a BTB entry is allocated for the instruction \isBrMissPred" is set if the instruction is a miss predicted branch \isTaken" is set if the instruction is a taken branch. Then pointers \u head", \c 1" and \c 2" are updated to re ect the fact that a new entry has committed. If the committed instruction is a miss predicted branch, a wrong path is taken and the way-footprints in the uncommitted entries of the way-footprint queue are useless. All the uncommitted entries are ushed. Note that this ush is done in parallel with the pipeline ush, which is required on a branch prediction miss. Other queue operations are simple. Thus queue operations are unlikely on one of the critical path.</p><p>The WFT is updated in the next cycle if one of the following conditions is satis ed:</p><p>\isCacheMiss" of entry \u 1" is set the wayfootprint f o r a f e t c h address may c hange on an instruction cache miss and WFT update is necessary \isBrMissPred" of entry \u 2" is set the wayfootprint for the next fetch m a y a l s o c hange on a branch prediction miss because a di erent c o n trol path may be taken \isBTBalloc" of entry \u 2" is set an entry will also be allocated in the WFT so that both the target address and the way-footprint can be provided next time the same fetch address is encountered. The \fetch address" eld of entry \u 2" and the \wayfootprint" eld of entry \u 1" are used to update the WFT. Either the \target address way-footprint" eld or the \fall-through address way-footprint" eld is updated depending on whether the \isTaken" eld of entry \u 2" is set.</p><p>Entries in both the BTB and the WFT can be identi ed using (way set). During the branch prediction, the BTB and the WFT are accessed in parallel using the same index function. If the fetch address matches the tag of the BTB entry (w s), then either the \target address way-footprint" or the \fall-through address way-footprint" of the WFT entry (w s) i s p r o vided for the next fetch depending on the branch direction predicted by the BDP.</p><p>If the \isCall" eld of entry \u 2" is set, the RAS update is needed. As the next fetch address is not the return address, the \way-footprint" in entry \u 1" is useless. However, if the call instruction is not on the cache line boundary, the instruction following the call instruction, which will be executed once the call returns, is also in the same cache line, Thus the wayfootprint for the call instruction can be used for the return address if the call instruction is not on the cache line boundary. Otherwise, the way-footprint for allway access will be used.</p><p>During the branch prediction, if a return instruction is predicted to be in the current fetch, then the top entry of the RAS will provide both the return address and the way-footprint for the next fetch.</p><p>Modi cations to the BTB allocation policy can affect the BTB hit rate, which in turn can a ect the number of successful way-footprint predictions because way-footprint prediction can succeed only when the tag comparison in the BTB succeeds. We h a ve i nvestigated the following three BTB allocation policies: taken branch policy (TB): BTB allocation only for a taken branch missing from the BTB any branch policy (AB): BTB allocation for any branch missing from the BTB any fetch address policy (AFA): BTB allocation for any fetch address missing from the BTB.</p><p>When an untaken branch or a non-branch instruction is allocated a BTB entry, the target address is the next continuous fetch address, which is the default address prediction if current fetch address misses from the BTB. The AB and AFA policies can decrease the number of entries available for taken branches and may degrade performance. Thus the TB policy is used in most processors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Performance</head><p>We use the SimpleScalar toolset 3] to model an outof-order speculative processor with a two-level cache hierarchy. The simulation parameters shown in Table <ref type="table" target="#tab_1">1</ref> roughly correspond to those in a high-performance microprocessor. We h a ve s i m ulated 100 million instructions for all SPEC95 benchmarks except Vortex. For the 4-way set-associative instruction cache, we use the <ref type="bibr">Cacti 11]</ref> to obtain the cache partitioning parameters with the optimal timing. The data array i s partitioned into eight subarrays and the tag array i s partitioned into two subarrays. One-way cache access needs only to access two data subarrays and one tag subarray. W e also use the Cacti to derive t h e p o wer parameters. The power per all-way access is normalized to 1. The power per one-way access is 0.2896 and the power per WFT access is 0.054.</p><p>The RAS and the way-footprint queue are small structures and the power dissipation by them is very small comparing to that of the instruction cache. Thus the power dissipation by them is not modeled. Figure <ref type="figure" target="#fig_3">3</ref> shows the BTB allocation rate, calculated as total number of BTB allocations versus total num-ber of instruction fetches. The BTB allocation rate is close to 0 for most benchmarks. For those benchmarks, once a fetch address is allocated an entry, i t is unlikely to be replaced from the BTB because the BTB capacity i s m uch larger than the work set size. Noticeable increase in the allocation rate can be found in apsi, fpppp, gcc and gcc. F or these benchmarks, the work set size is relatively large and the number of BTB con ict misses increases, which leads to more number of BTB allocation. Figure <ref type="figure" target="#fig_4">4</ref> shows branch address prediction hit rate. For most benchmarks, there is virtually no di erence in the hit rate with di erent BTB allocation policies. For gcc and go, the hit rate with the AB and AFA policies is lower than that of the TB policy. The reason is that untaken branches and non-branch instructions are allocated BTB entries. As a consequence, the effective n umb e r o f B T B e n tries for the taken branches with the AB and AFA policies is smaller than that of the TB policy. However, a couple of benchmarks such a s fpppp show slight increase in the hit rate. The branch history is updated if an address has an entry in the BTB. The history update can somehow improve the prediction accuracy for other correlated branches.</p><p>Figure <ref type="figure" target="#fig_5">5</ref> shows the dynamic branch instruction rate. The dynamic branch instruction rate varies widely. The rate ranges from 15% to 35% for 12 benchmarks. For four oat-point b e n c hmarks{applu, apsi, fpppp and turb3d, the rate is lower than 10%. Figure <ref type="figure" target="#fig_6">6</ref> shows percentage of instruction fetches that need only one-way access. For the AFA policy, a BTB entry is allocated for every fetch address missing from the BTB. Thus one-way c a c he access rate is close to 100% for every benchmark and is not a ected by t h e dynamic instruction rate shown in Figure <ref type="figure" target="#fig_5">5</ref>. However, high dynamic instruction rate results in high one-way access rate for the TB and AB policies. One-way a ccess rate for the AB policy is slightly higher than the rate for the TB policy because of additional entries allocated for untaken branches. Figure <ref type="figure">8</ref> shows normalized energy. As our technique can only reduce instruction cache hit energy, hit energy is used in the calculation. The relationship between the hit energy and the miss energy depends on the instruction cache miss rate. For all benchmarks except fpppp, the hit energy is at least ten times the miss energy. Normalized energy highly depends on the one-way c a c he access rate shown in Figure <ref type="figure" target="#fig_6">6</ref>. For TB, AB and AFA policies, the average normalized energy is 70.8%, 66.7% and 37.6% respectively, which translates into 29.2%, 33.3% and 62.4% energy savings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>Calder and Grunwald have proposed a coupled BTB for fast instruction fetch in a set-associative i nstruction cache 4]. A similar scheme is also used in the Alpha <ref type="bibr">21264 8]</ref>. Each c a c he line is in the format:</p><p>(tag, data, next-line, next-way). \Next-line" and \next-way" are used to locate the next fetch c a c he line. Because of the \next-way" prediction, most of the time one cache way is accessed and this can result in low instruction cache energy.</p><p>The e ective n umber of entries in a coupled BTB is smaller than the number of instruction cache lines. In addition, \next-line" and \next-way" can only point to one cache line. Prediction misses often occur when there are multiple branches in a cache line or a branch changes direction. On the contrary, the decoupled BTB shown in Figure <ref type="figure" target="#fig_0">1</ref> can provide accurate prediction in the above scenarios.</p><p>In the coupled BTB, the next fetch cache line is unknown until current instruction fetch nishes. The instruction fetch is serialized and is not scalable. On the other hand, the decoupled BTB is scalable. It can support multiple branch predictions and multiple cache line fetches in one cycle to deliver more instructions <ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12]</ref>. The decoupled BTB can also enable a scalable front-end with asynchronous instruction fetch and BTB prediction for high rate instruction delivery as proposed by Reinman, Austin and <ref type="bibr">Calder 9]</ref>.</p><p>Comparing to a coupled BTB, the energy savings by our way-footprint prediction are higher because the prediction accuracy by the decoupled BTB is higher and we can predict based on RAS. And the framework for energy savings by our way-footprint prediction is scalable.</p><p>Inoue, Ishihara and Murakarni have proposed another kind of \way-prediction" 6]. For each cache set, the way-footprint for the last accessed way is stored in a table. When the same set is accessed next time, the last accessed way is speculatively accessed rst. On a way-prediction miss, the remaining ways are accessed in the next cycle. Way-prediction is stored in a table and this table is accessed before the cache. Because of this kind of access serialization, the processor cycle time may be a ected. In addition, the performance degradation is much higher than our approach.</p><p>Albonesi has proposed \selective c a c he ways" 1] t o turn o some cache ways based on application requirements. He has only investigated energy savings in the data cache. \Selective cache ways" can also be used in the instruction cache. As all the active w ays are accessed simultaneously, the energy savings are much lower than \way-prediction", where only one way i s accessed most of the time. This technique cannot be used in applications with large work set because the number of cache misses, which can incur high energy and performance cost, may increase dramatically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we h a ve proposed a way-footprint prediction technique for energy savings in setassociative instruction caches. The hardware cost is small because it utilizes existent h a r d w are in the branch predictor. And the added hardware is not on one of the critical path. We h a ve i n vestigated three BTB allocation policies for their e ects on perfor-mance and energy. Each of them results in 29%, 33% and 62% instruction cache energy savings with normalized execution time of 1, 1 and 1.001 respectively.</p><p>We are currently investigating the potential performance advantages of the way-footprint prediction. For one-way c a c he access, the access time is shorter because there is no need for way selection. It is likely to take a shorter time for the instructions to go through the pipeline. This may result in early branch miss prediction detection and reduce the branch miss prediction penalties. In addition, the average instruction cache port utilization is decreased because of shorter cache access time. Idle ports can be used by some techniques, such a s t a g c heck during the prefetching, to improve performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Pipeline architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Way-footprint q u e u e</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: BTB allocation rate</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Branch address prediction hit rate</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Dynamic branch instruction rate</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: One-way cache access rate</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :Figure 8 :</head><label>78</label><figDesc>Figure 7: Normalized execution time Figure 7 shows normalized execution time. For</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>System con guration</figDesc><table><row><cell>Parameter branch pred. BTB return address stack RUU/LSQ fetch queue fetch width int./ t. ALUs int./ t. Mult/Div L1 Icache L1 Dcache L2 cache</cell><cell>Value combined, 4K 2-bit chooser, 4k-entry bimodal, 12-bit, 4K-entry global 7-cycle miss prediction penalty 2K-entry, 4 -w ay 32 64/32 16 8 4/2 2/2 32KB, 4-way, 32B block 64KB, 4-way, 32B block 512KB, 4-way, 64B block</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Selective c a c he ways: on-demand cache resource allocation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Albonesi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int&apos;l Symp. Microarchitecture</title>
				<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="248" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">System-level power optimization: techniques and tools</title>
		<author>
			<persName><forename type="first">L</forename><surname>Benini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Micheli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Design Automation fo Electronic Systems</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="115" to="192" />
			<date type="published" when="2000-04">April 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">The simplescalar toolset, version 2.0</title>
		<author>
			<persName><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Austin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
		<respStmt>
			<orgName>University of Wisconsin-Madison</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Next cache line and set prediction</title>
		<author>
			<persName><forename type="first">B</forename><surname>Calder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grunwald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Int&apos;l Symp. Computer Architecture</title>
		<imprint>
			<biblScope unit="page" from="287" to="296" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Custom S/390 G5 and G6 microprocessors</title>
		<author>
			<persName><forename type="first">M</forename><surname>Check</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Slegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Journal of Research and Development</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">5/6</biblScope>
			<biblScope unit="page" from="671" to="680" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Waypredicting set-associative c a c he for high performance and low energy consumption</title>
		<author>
			<persName><forename type="first">K</forename><surname>Inoue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ishihara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murakami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int&apos;l Symp. on Low Power Electronics and Design</title>
				<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="273" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A 160-MHz, 32-b, 0.5-W CMOS RISC microprocessor</title>
		<author>
			<persName><forename type="first">J</forename><surname>Montanaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Solid-State Circuits</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1703" to="1714" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The Alpha 21264 microprocessor</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kessler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="24" to="36" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A scalable front-end architecture for fast instruction delivery</title>
		<author>
			<persName><forename type="first">G</forename><surname>Reinman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Calder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Int&apos;l Symp. Computer Architecture</title>
		<imprint>
			<biblScope unit="page" from="16" to="27" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multiple-block ahead branch predictors</title>
		<author>
			<persName><forename type="first">A</forename><surname>Seznec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jourdan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sainrat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Michaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int&apos;l Symp. Architectural Support for Programming Languages and Operating Systems</title>
				<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">An enhanced access and cycle time model for on-chip caches</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jouppi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
		<respStmt>
			<orgName>Digital Western Research Laboratory</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Increasing the instruction fetch rate via multiple branch prediction and a branch address cache</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Marr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Int&apos;l Symp. Computer Architecture</title>
		<imprint>
			<biblScope unit="page" from="67" to="76" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
