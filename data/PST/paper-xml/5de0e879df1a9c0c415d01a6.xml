<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Depthwise Separable Convolution Neural Network for High-Speed SAR Ship Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-10-24">24 October 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tianwen</forename><surname>Zhang</surname></persName>
							<email>twzhang@std.uestc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Communication Engineering</orgName>
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
								<address>
									<postCode>611731</postCode>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaoling</forename><surname>Zhang</surname></persName>
							<email>xlzhang@uestc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Communication Engineering</orgName>
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
								<address>
									<postCode>611731</postCode>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jun</forename><surname>Shi</surname></persName>
							<email>shijun@uestc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Communication Engineering</orgName>
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
								<address>
									<postCode>611731</postCode>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shunjun</forename><surname>Wei</surname></persName>
							<email>weishunjun@uestc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Communication Engineering</orgName>
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
								<address>
									<postCode>611731</postCode>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Depthwise Separable Convolution Neural Network for High-Speed SAR Ship Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-10-24">24 October 2019</date>
						</imprint>
					</monogr>
					<idno type="MD5">9DBA88EE79CBEE1E180F29A2D144EF20</idno>
					<idno type="DOI">10.3390/rs11212483</idno>
					<note type="submission">Received: 19 September 2019; Accepted: 17 October 2019;</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T04:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>synthetic aperture radar (SAR)</term>
					<term>ship detection</term>
					<term>high-speed</term>
					<term>convolution neural network (CNN)</term>
					<term>depthwise separable convolution neural network (DS-CNN)</term>
					<term>depthwise convolution (D-Conv2D)</term>
					<term>pointwise convolution (P-Conv2D)</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As an active microwave imaging sensor for the high-resolution earth observation, synthetic aperture radar (SAR) has been extensively applied in military, agriculture, geology, ecology, oceanography, etc., due to its prominent advantages of all-weather and all-time working capacity. Especially, in the marine field, SAR can provide numerous high-quality services for fishery management, traffic control, sea-ice monitoring, marine environmental protection, etc. Among them, ship detection in SAR images has attracted more and more attention on account of the urgent requirements of maritime rescue and military strategy formulation. Nowadays, most researches are focusing on improving the ship detection accuracy, while the detection speed is frequently neglected, regardless of traditional feature extraction methods or modern deep learning (DL) methods. However, the high-speed SAR ship detection is of great practical value, because it can provide real-time maritime disaster rescue and emergency military planning. Therefore, in order to address this problem, we proposed a novel high-speed SAR ship detection approach by mainly using depthwise separable convolution neural network (DS-CNN). In this approach, we integrated multi-scale detection mechanism, concatenation mechanism and anchor box mechanism to establish a brand-new light-weight network architecture for the high-speed SAR ship detection. We used DS-CNN, which consists of a depthwise convolution (D-Conv2D) and a pointwise convolution (P-Conv2D), to substitute for the conventional convolution neural network (C-CNN). In this way, the number of network parameters gets obviously decreased, and the ship detection speed gets dramatically improved. We experimented on an open SAR ship detection dataset (SSDD) to validate the correctness and feasibility of the proposed method. To verify the strong migration capacity of our method, we also carried out actual ship detection on a wide-region large-size Sentinel-1 SAR image. Ultimately, under the same hardware platform with NVIDIA RTX2080Ti GPU, the experimental results indicated that the ship detection speed of our proposed method is faster than other methods, meanwhile the detection accuracy is only lightly sacrificed compared with the state-of-art object detectors. Our method has great application value in real-time maritime disaster rescue and emergency military planning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Synthetic aperture radar (SAR), an active microwave remote sensing imaging radar capable of observing the earth's surface all-day and all-weather, has a wide range of applications in military, agriculture, geology, ecology, oceanography, etc. In particular, since the United States launched the first civil SAR satellite to carry out ocean exploration in 1978 <ref type="bibr" target="#b0">[1]</ref>, SAR has begun to be constantly used in the marine field, such as fishery management <ref type="bibr" target="#b1">[2]</ref>, traffic control <ref type="bibr" target="#b2">[3]</ref>, sea-ice monitoring <ref type="bibr" target="#b3">[4]</ref>, marine environmental protection <ref type="bibr" target="#b4">[5]</ref>, ship surveillance <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>, etc. Among them, in recent years, ship detection in SAR images has become a research hotspot for its broad application prospects <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>. On the military side, it is conducive for tactical deployment and ocean defense early warning. On the civil side, it is also beneficial for maritime transport management and maritime distress rescue. However, despite of the wide practical value in the military and civil side, up to now, SAR ship detection technology is still lagging behind optical images due to their different imaging mechanisms <ref type="bibr" target="#b9">[10]</ref>.</p><p>Therefore, many constructive methods are emerging to solve this problem, which promote the development of SAR interpretation technology persistently. According to our investigation, these multifarious methods can be mainly divided into two categories: <ref type="bibr" target="#b0">(1)</ref> Traditional feature extraction methods; (2) modern deep learning (DL) methods.</p><p>The most noteworthy characteristic of the traditional feature extraction methods is the manual feature extraction. In these ways, ships can be distinguished from ports, islands, etc., through gray level, texture, contrast ratio, geometric size, scattering characteristics, the scale-invariant feature transform (SIFT) <ref type="bibr" target="#b10">[11]</ref>, haar-like (Haar) <ref type="bibr" target="#b11">[12]</ref>, histogram of oriented gradient (HOG) <ref type="bibr" target="#b12">[13]</ref>, etc. Among them, the constant false alarm rate (CFAR) is one of the typical algorithms. When performing ship detection, CFAR detectors provide a threshold, which needs to avoid noise background clutter and interference as far as possible, to detect the existence of ships. Therefore, it is essential to establish an accurate clutter statistical model for CFAR detectors. Among them, some frequently-used clutter statistical models are based on Gauss-distribution <ref type="bibr" target="#b13">[14]</ref>, Rayleigh-distribution <ref type="bibr" target="#b14">[15]</ref>, k-distribution <ref type="bibr" target="#b15">[16]</ref>, and Weibull-distribution <ref type="bibr" target="#b16">[17]</ref>. However, for these CFAR detectors, there are still two drawbacks hindering its development and application. On the one hand, it is bitterly challenging to build an accurate clutter model. On the other hand, these established models are inevitably vulnerable to sea clutter and ocean currents. Therefore, the application scenarios are limited and the migration capacity is also weak. Even worse, in the practical applications, it takes a lot of time to solve lots of parameters of the above distribution equations, leading to a slower detection speed. Another common traditional method is the template-based detection <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>. This method considers both the characteristics of specific targets and that of backgrounds, which has achieved good detection performance. However, these detection templates are frequently dependent on the expert experience and deficient in mature theories, which may lead to their poor migration ability. In addition, the template matching in wide-region SAR images is also time-consuming, which undoubtedly declines the detection speed. In summary, these traditional feature extraction methods are complex in computation, weak in generalization, and troublesome in manual feature extraction. Most importantly, the detection speed of these methods is too slow to satisfy real-time ship detection in the practical application occasion.</p><p>The most noteworthy characteristic of the modern DL methods is the automatic feature extraction <ref type="bibr" target="#b19">[20]</ref>. By this means, it is unnecessary to design features via manual participation, simply and efficiently. As long as given a labeled dataset, computers can train and learn like human brains to accomplish accurate object detection tasks. For this reason, nowadays, more and more scholars are following the path of artificial intelligence (AI) to achieve SAR ship detection. Generally, there are two main types of object detectors in the DL field <ref type="bibr" target="#b20">[21]</ref>: (1) Two-stage detectors; (2) one-stage detectors.</p><p>Two-stage detectors usually assign detection tasks into two stages: acquisition of region of interest (ROI) and classification of ROI. Region-convolutional neural network (R-CNN) <ref type="bibr" target="#b21">[22]</ref> is the first one to realize object detection via DL methods. R-CNN used the selective search <ref type="bibr" target="#b22">[23]</ref> to generate ROIs, then extracted the features of ROIs by CNNs, and finally, classified ROIs by support vector machine (SVM) <ref type="bibr" target="#b23">[24]</ref>. It made the mean average precision (mAP) on the Pascal VOC dataset <ref type="bibr" target="#b24">[25]</ref> a great increment compared with previous traditional methods. However, due to its huge amount of calculation, the detection speed of R-CNN is notoriously slow, leading to impracticability in industry. To improve the detection speed, a fast region-convolutional neural network (Fast R-CNN) was proposed by Girshick et al. <ref type="bibr" target="#b25">[26]</ref>, which draws some experience from spatial pyramid pooling network (SPP-net) <ref type="bibr" target="#b26">[27]</ref>. Fast R-CNN added an ROI pooling layer and realized feature sharing, as a result, its speed is 213 times faster than R-CNN <ref type="bibr" target="#b25">[26]</ref>. However, the ROI extraction process still takes up most of the detection time, reducing its detection efficiency. Therefore, faster region-convolutional neural network (Faster R-CNN) <ref type="bibr" target="#b27">[28]</ref> was proposed to simplify this process by a region proposal network (RPN), as a result, its speed is 10 times faster than Fast R-CNN <ref type="bibr" target="#b27">[28]</ref>. So far, Faster R-CNN is still one of the mainstream object detection methods in the DL field <ref type="bibr" target="#b28">[29]</ref>. From these emerged methods, we can summarize that the development tendency of two-stage detectors is to improve the detection speed. Nevertheless, two-stage detectors inherently need to obtain region recommendation boxes in advance, leading to their heavy computation. Therefore, there are some intrinsic technical bottlenecks in improving detection speed, so they scarcely meet the real-time requirement. Hence, one-stage detectors emerged to simplify the detection process.</p><p>One-stage detectors achieve detection tasks directly based on position regression. Redmon et al. <ref type="bibr" target="#b29">[30]</ref> proposed the first one-stage detector, called you only look once (YOLO), which processes the input image only once. In this way, the amount of calculation gets decreased further and the detection speed gets improved further, compared with two-stage detectors. However, its detection performance on neighboring targets and small ones is not ideal, because each grid is responsible for predicting only one target. Therefore, they also proposed an improved version called YOLOv2 <ref type="bibr" target="#b30">[31]</ref> based on anchor box mechanism and feature fusion. In YOLOv2, a more robust backbone network Darknet-19 <ref type="bibr" target="#b30">[31]</ref> was proposed to improve the detection accuracy. However, the accuracy of YOLOv2 is still far inferior to two-stage detectors. Finally, YOLOv3 <ref type="bibr" target="#b31">[32]</ref> was proposed to increase the detection accuracy further, by using Darknet-53 <ref type="bibr" target="#b31">[32]</ref> and multiscale mechanism. In addition, another two common one-stage detectors are single shot multi-box detector (SSD) proposed by Liu et al. <ref type="bibr" target="#b32">[33]</ref> and RetinaNet proposed by Lin et al. <ref type="bibr" target="#b33">[34]</ref>. SSD combined the advantages of YOLO and Faster R-CNN to achieve a balance between accuracy and speed. RetinaNet proposed a focal loss to solve the extreme imbalance between positive and negative sample regions, which can improve accuracy. However, their detection speed is inferior to YOLO. From these emerged methods, we can summarize that the development tendency of one-stage detectors is to improve the detection accuracy.</p><p>From the comparison of two-stage detectors and one-stage detectors, both accuracy and speed are extremely significant for object detection. However, nowadays, in the SAR ship detection field, many types of researches are focusing on improving the ship detection accuracy. Unfortunately, there are few studies on improving the ship detection speed. However, it definitely cannot be ignored as high-speed SAR ship detection is of great practical value, especially in the cases of maritime distress rescue and war emergency scenarios.</p><p>Therefore, in order to address this problem, in this paper, a novel high-speed SAR ship detection approach was proposed by mainly using depthwise separable convolution neural network (DS-CNN). A DS-CNN is composed of a depthwise convolution (D-Conv2D) and a pointwise convolution (P-Conv2D), which substitute for the conventional convolution neural network (C-CNN), by which the number of network parameters get greatly decreased. Consequently, the detection speed gets dramatically improved. Particularly, a brand-new light-weight network architecture, which integrates multi-scale detection mechanism, concatenation mechanism and anchor box mechanism, was exclusively established for the high-speed SAR ship detection. We experimented on an open SAR ship detection dataset (SSDD) <ref type="bibr" target="#b34">[35]</ref> to verify the correctness and feasibility of the proposed method. In addition, we also carried out actual ship detection on a wide-region large-size Sentinel-1 SAR image to verify the strong migration ability of the proposed method. Finally, the experimental results indicated that our method can achieve high-speed SAR ship detection, authentically. On the SSDD dataset, it only takes 9.03 ms per SAR image for ship detection (111 images can be detected per second). The detection speed of the proposed method is faster than other methods, such as Faster R-CNN, RetinaNet, SSD, and YOLO, under the same hardware platform with NVIDIA RTX2080Ti GPU. Our method accelerated SAR ship detection by many times with only a slight accuracy sacrifice compared with the state-of-art object detectors, which is of great application value in real-time maritime disaster rescue and emergency military planning.</p><p>To be clear, in this paper, we did not consider the time of SAR imaging process (most often, decades of minutes up to hours for a wide region) because SAR images to be detected have been acquired previously. Therefore, only the time of SAR image interpretation, namely ship detection, was considered. Another point to note is that the adjective "high-speed" is used to describe the detection speed by a detector, instead of the moving speed of ship.</p><p>The main contributions of our work are as follows:</p><p>1.</p><p>A brand-new light-weight network architecture for the high-speed SAR ship detection was established by mainly using DS-CNN; 2.</p><p>Multi-scale detection mechanism, concatenation mechanism and anchor box mechanism were integrated into our method to improve the detection accuracy; 3.</p><p>The ship detection speed of our proposed method is faster than the current other object detectors, with only a slight accuracy sacrifice compared with the state-of-art object detectors.</p><p>The rest of this paper is organized as follows. Section 2 introduces C-CNN and DS-CNN. Section 3 introduces our network architecture. Section 4 introduces our ship detection model. Section 5 introduces our experiments. Section 6 presents the results of SAR ship detection. Finally, a summary is made in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">CNN</head><p>In this section, firstly, we will introduce the basic structures of C-CNN and DS-CNN. Afterwards, we will make a comparison of theoretical computational complexity between C-CNN and DS-CNN to show that DS-CNN has lower computational cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">C-CNN</head><p>Convolution neural network (CNN) was first proposed by <ref type="bibr">Lecun et al. in 1998 [36]</ref>, which has been successfully applied in speech recognition <ref type="bibr" target="#b36">[37]</ref>, face recognition <ref type="bibr" target="#b37">[38]</ref>, natural language processing <ref type="bibr" target="#b38">[39]</ref>, etc. Especially, in the computer vision (CV) field, CNN has largely surpassed the traditional object detection algorithms in the ImageNet large-scale visual recognition challenge (ILSVRC) <ref type="bibr" target="#b39">[40]</ref>. In general, two main factors are contributing to the success of CNN. On the one hand, it can improve the recognition rate for its receptive field <ref type="bibr" target="#b40">[41]</ref> similar to human visual cells. On the other hand, it can also effectively reduce the number of network parameters and can alleviate over-fitting by local connection and weight sharing, compared with a fully connected deep neural network. Therefore, in the DL object detection field, CNN has become an extremely important research direction. In this paper, we called the above CNN as conventional CNN (C-CNN).</p><p>Figure <ref type="figure" target="#fig_1">1a</ref> is the basic structure of C-CNN. From Figure <ref type="figure" target="#fig_1">1a</ref>, each convolution kernel (K 1 or K 2 or K 3 or K 4 ) needs to convolute all four input channels separately (I 1 , I 2 , I 3 , I 4 ), then add up the above four convolution operation results to obtain the output of one channel (O 1 or O 2 or O 3 or O 4 ), and finally these four outputs from four different kernels are connected into a whole</p><formula xml:id="formula_0">(O 1 O 2 O 3 O 4 ).</formula><p>For example, the rough process for the kernel K 1 can be expressed as follows:</p><formula xml:id="formula_1">O 1 = K 1 ⊗ I 1 + K 1 ⊗ I 2 + K 1 ⊗ I 3 + K 1 ⊗ I 4 (<label>1</label></formula><formula xml:id="formula_2">)</formula><p>where I 1 , I 2 , I 3 , and I 4 are the network inputs, O 1 is one of the network outputs, K 1 is one of the network kernels, and ⊗ denotes the convolution operation. Thus far, many excellent C-CNNs have emerged, such as AlexNet <ref type="bibr" target="#b37">[38]</ref>, VGG <ref type="bibr" target="#b41">[42]</ref>, GoogLeNet <ref type="bibr" target="#b42">[43]</ref>, Darknet <ref type="bibr" target="#b30">[31]</ref>, etc., which are used by most of the object detectors to extract features. However, these C-CNNs simultaneously consider both channels and regions <ref type="bibr" target="#b43">[44]</ref>, leading to a huge number of network parameters. As a result, the detection speed will be inevitably decreased. Therefore, aiming at this problem, DS-CNN emerged.  For example, the rough process for the kernel K1 can be expressed as follows:</p><formula xml:id="formula_3">1 1 1 1 2 1 3 1 4 O K I K I K I K I =  +  +  + <label>(1)</label></formula><p>where I1, I2, I3, and I4 are the network inputs, O1 is one of the network outputs, K1 is one of the network kernels, and  denotes the convolution operation.</p><p>Thus far, many excellent C-CNNs have emerged, such as AlexNet <ref type="bibr" target="#b37">[38]</ref>, VGG <ref type="bibr" target="#b41">[42]</ref>, GoogLeNet <ref type="bibr" target="#b42">[43]</ref>, Darknet <ref type="bibr" target="#b30">[31]</ref>, etc., which are used by most of the object detectors to extract features. However, these C-CNNs simultaneously consider both channels and regions <ref type="bibr" target="#b43">[44]</ref>, leading to a huge number of network parameters. As a result, the detection speed will be inevitably decreased. Therefore, aiming at this problem, DS-CNN emerged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">DS-CNN</head><p>DS-CNN was first proposed by L. Sifre in 2014 <ref type="bibr" target="#b44">[45]</ref>, which has been successfully applied to Xception <ref type="bibr" target="#b45">[46]</ref> and MobileNet <ref type="bibr" target="#b43">[44]</ref>. Especially, DS-CNN has a tremendous application prospect in mobile communication devices and embedded devices for its lighter networks and faster speed. In the DL field, ordinarily, convolution kernels can be regarded as 3D filters (width, height, and channel) for convolution operations, and conventional convolution operations are joint mappings of channel correlations and spatial correlations <ref type="bibr" target="#b45">[46]</ref>. Then, it will be beneficial for the reduction of computational complexity if we decouple channel correlations and spatial correlations <ref type="bibr" target="#b45">[46]</ref>. For this, DS-CNN successfully decouples channel correlations and spatial correlations to reduce computational complexity, meanwhile it does not make great accuracy sacrifice because C-CNN inherently has some redundancy <ref type="bibr" target="#b44">[45]</ref>.</p><p>Figure <ref type="figure" target="#fig_1">1b</ref> is the basic structure of DS-CNN. From Figure <ref type="figure" target="#fig_1">1b</ref>, each convolution kernel (K1 or K2 or K3 or K4) needs to convolute only one input channel (I1 or I2 or I3 or I4), then obtain the output of one channel (O1 or O2 or O3 or O4) without summation process, and finally these four outputs from four different kernels are directly connected into a whole (O1O2O3O4).</p><p>For example, the rough process for the kernel K1 can be expressed as follows:</p><formula xml:id="formula_4">1 1 1 O K I =<label>(2)</label></formula><p>From Equations ( <ref type="formula" target="#formula_1">1</ref>) and ( <ref type="formula" target="#formula_4">2</ref>), C-CNN has four convolution operations and DS-CNN has only one convolution operation, therefore, DS-CNN simplifies traditional convolution operation, which can obviously reduce the amount of calculation. To be clear, Equations ( <ref type="formula" target="#formula_1">1</ref>) and ( <ref type="formula" target="#formula_4">2</ref>) here are only rough expressions and more rigorous expressions will be introduced in Section 2.3. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">DS-CNN</head><p>DS-CNN was first proposed by L. Sifre in 2014 <ref type="bibr" target="#b44">[45]</ref>, which has been successfully applied to Xception <ref type="bibr" target="#b45">[46]</ref> and MobileNet <ref type="bibr" target="#b43">[44]</ref>. Especially, DS-CNN has a tremendous application prospect in mobile communication devices and embedded devices for its lighter networks and faster speed. In the DL field, ordinarily, convolution kernels can be regarded as 3D filters (width, height, and channel) for convolution operations, and conventional convolution operations are joint mappings of channel correlations and spatial correlations <ref type="bibr" target="#b45">[46]</ref>. Then, it will be beneficial for the reduction of computational complexity if we decouple channel correlations and spatial correlations <ref type="bibr" target="#b45">[46]</ref>. For this, DS-CNN successfully decouples channel correlations and spatial correlations to reduce computational complexity, meanwhile it does not make great accuracy sacrifice because C-CNN inherently has some redundancy <ref type="bibr" target="#b44">[45]</ref>.</p><p>Figure <ref type="figure" target="#fig_1">1b</ref> is the basic structure of DS-CNN. From Figure <ref type="figure" target="#fig_1">1b</ref>, each convolution kernel (K 1 or K 2 or K 3 or K 4 ) needs to convolute only one input channel (I 1 or I 2 or I 3 or I 4 ), then obtain the output of one channel (O 1 or O 2 or O 3 or O 4 ) without summation process, and finally these four outputs from four different kernels are directly connected into a whole</p><formula xml:id="formula_5">(O 1 O 2 O 3 O 4 ).</formula><p>For example, the rough process for the kernel K 1 can be expressed as follows:</p><formula xml:id="formula_6">O 1 = K 1 ⊗ I 1<label>(2)</label></formula><p>From Equations ( <ref type="formula" target="#formula_1">1</ref>) and ( <ref type="formula" target="#formula_4">2</ref>), C-CNN has four convolution operations and DS-CNN has only one convolution operation, therefore, DS-CNN simplifies traditional convolution operation, which can obviously reduce the amount of calculation. To be clear, Equations ( <ref type="formula" target="#formula_1">1</ref>) and (2) here are only rough expressions and more rigorous expressions will be introduced in Section 2.3.</p><p>Figure <ref type="figure" target="#fig_2">2</ref> is the detailed internal operation flow of a DS-CNN. From Figure <ref type="figure" target="#fig_2">2</ref>, a DS-CNN consists of a D-Conv2D and a P-Conv2D. For D-Conv2D in Figure <ref type="figure" target="#fig_18">2a</ref>, it performs convolution operations on only one channel, respectively, and generates four results without summing them. Then, for P-Conv2D in Figure <ref type="figure" target="#fig_18">2b</ref>, it uses 1 × 1 kernel to perform conventional convolution operations on all results generated before to ensure the same output shape as C-CNN, then the sum is used to generate an output of one channel, and finally all outputs from the above 1 × 1 kernels are connected into a whole as the final output. Through the above two steps, DS-CNN successfully decouples channel correlation and spatial correlation.</p><p>of a D-Conv2D and a P-Conv2D. For D-Conv2D in Figure <ref type="figure" target="#fig_18">2a</ref>, it performs convolution operations on only one channel, respectively, and generates four results without summing them. Then, for P-Conv2D in Figure <ref type="figure" target="#fig_18">2b</ref>, it uses 1 × 1 kernel to perform conventional convolution operations on all results generated before to ensure the same output shape as C-CNN, then the sum is used to generate an output of one channel, and finally all outputs from the above 1 × 1 kernels are connected into a whole as the final output. Through the above two steps, DS-CNN successfully decouples channel correlation and spatial correlation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Computational Complexity of C-CNN and DS-CNN</head><p>Currently, the mainstream target detectors commonly use C-CNNs to build networks, such as Faster R-CNN, RetinaNet, SSD, YOLO, etc. However, in this paper, we used DS-CNNs to build our network. To verify the lightweight of DS-CNN, we compared the computational complexity of C-CNN and DS-CNN. Figure <ref type="figure" target="#fig_3">3</ref>  Assume that the size of images is L × L, the size of the convolution kernel is Nconv×Nconv×Cin, and the number is Cconv.</p><p>Then, the computational cost of C-CNN in Figure <ref type="figure" target="#fig_8">3a</ref> is:</p><formula xml:id="formula_7">- =      C CNN conv conv in conv Computation L L N N C C (3)</formula><p>If we use DS-CNN, the computational cost of D-Conv2D in Figure <ref type="figure" target="#fig_8">3b</ref> is:</p><formula xml:id="formula_8">=     D-Conv2D conv conv in Computation N N C L L (4)</formula><p>and the computational cost of P-Conv2D in Figure <ref type="figure" target="#fig_8">3b</ref> is: </p><formula xml:id="formula_9">=    P-Conv2D in conv Computation C C L L (5)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Computational Complexity of C-CNN and DS-CNN</head><p>Currently, the mainstream target detectors commonly use C-CNNs to build networks, such as Faster R-CNN, RetinaNet, SSD, YOLO, etc. However, in this paper, we used DS-CNNs to build our network. To verify the lightweight of DS-CNN, we compared the computational complexity of C-CNN and DS-CNN. Figure <ref type="figure" target="#fig_3">3</ref> shows the comparison diagrammatic sketch of C-CNN and DS-CNN. P-Conv2D in Figure <ref type="figure" target="#fig_18">2b</ref>, it uses 1 × 1 kernel to perform conventional convolution operations on all 217 results generated before to ensure the same output shape as C-CNN, then the sum is used to 218 generate an output of one channel, and finally all outputs from the above 1 × 1 kernels are connected   Assume that the size of images is L × L, the size of the convolution kernel is Nconv×Nconv×Cin, and</p><formula xml:id="formula_10">228</formula><p>the number is Cconv.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>229</head><p>Then, the computational cost of C-CNN in Figure <ref type="figure" target="#fig_8">3a</ref> is:</p><formula xml:id="formula_11">230 - =      C CNN conv conv in conv Computation L L N N C C (3)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>231</head><p>If we use DS-CNN, the computational cost of D-Conv2D in Figure <ref type="figure" target="#fig_8">3b</ref> is:</p><formula xml:id="formula_12">232 =     D-Conv2D conv conv in Computation N N C L L (4)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>233</head><p>and the computational cost of P-Conv2D in Figure <ref type="figure" target="#fig_8">3b</ref> is: Assume that the size of images is L × L, the size of the convolution kernel is N conv × N conv × C in , and the number is C conv .</p><formula xml:id="formula_13">234 =    P-Conv2D in conv Computation C C L L (5)</formula><p>Then, the computational cost of C-CNN in Figure <ref type="figure" target="#fig_8">3a</ref> is:</p><formula xml:id="formula_14">Computation C-CNN = L • L • N conv • N conv • C in • C conv<label>(3)</label></formula><p>If we use DS-CNN, the computational cost of D-Conv2D in Figure <ref type="figure" target="#fig_8">3b</ref> is:</p><formula xml:id="formula_15">Computation D-Conv2D = N conv • N conv • C in • L • L (4)</formula><p>and the computational cost of P-Conv2D in Figure <ref type="figure" target="#fig_8">3b</ref> is:</p><formula xml:id="formula_16">Computation P-Conv2D = C in • C conv • L • L (5)</formula><p>Therefore, the computational cost of DS-CNN is:</p><formula xml:id="formula_17">Computation DS-CNN = N conv • N conv • C in • L • L + C in • C conv • L • L (6)</formula><p>Then, their ratio is:</p><formula xml:id="formula_18">ratio = Computation DS-CNN Computation C-CNN = 1 C conv + 1 N 2 conv (7)</formula><p>Remote Sens. 2019, 11, 2483 7 of 37 where C conv &gt;&gt; 1, and N conv &gt; 1.</p><p>Finally,</p><formula xml:id="formula_19">ratio &lt;&lt; 1 (8)</formula><p>Therefore, from Formula (8), DS-CNN can effectively reduce computational cost, which can improve the detection speed. Moreover, we also discussed the time complexity of C-CNN and DS-CNN.</p><p>The time complexity of C-CNN is:</p><formula xml:id="formula_20">Time C-CNN ∼ O N 2 out • N 2 conv • C in • C out<label>(9)</label></formula><p>where N out is the size of the output feature maps, N conv is the size of kernels, C in is the number of the input channels, and C out is the number of the output channels.</p><p>The time complexity of DS-CNN is:</p><formula xml:id="formula_21">Time DS-CNN ∼ O N 2 out • N 2 conv • C in + N 2 out • C in • C out<label>(10)</label></formula><p>From Formulas ( <ref type="formula" target="#formula_20">9</ref>) and ( <ref type="formula" target="#formula_21">10</ref>), DS-CNN essentially converts continuous multiplication into continuous addition, so the redundancy of the network gets reduced. As a result, the computational efficiency of the network has been greatly improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Network Architecture</head><p>Figure <ref type="figure" target="#fig_10">4</ref> shows the network architecture of our proposed high-speed SAR ship detection system. Our network consists of a backbone network and a detection network, inspired from the experience of MobileNet <ref type="bibr" target="#b43">[44]</ref>, YOLO <ref type="bibr" target="#b31">[32]</ref>, SSD <ref type="bibr" target="#b32">[33]</ref>, and DenseNet <ref type="bibr" target="#b46">[47]</ref>. From Figure <ref type="figure" target="#fig_10">4</ref>, in our network, the images to be detected are resized into L × L and the number of channels is 3. In Figure <ref type="figure" target="#fig_10">4</ref>, in our model, to achieve a good tradeoff between accuracy and speed, we set L = 160. Detailed research of the value of L will be introduced in Section 5. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Backbone Network</head><p>A backbone network is used to extract ships' features and it is established by using DS-CNN (marked in a red rectangular frame in Figure <ref type="figure" target="#fig_10">4</ref>) and C-CNN (marked in a blue rectangular frame In addition, the detection network is composed of three parts (detection network-1, detection network-2 and detection network-3), which means that our network will detect an input SAR image under three different scales (L/32, L/16, and L/8), and then obtain the final ship detection results.</p><p>Next, we will introduce the backbone network and the detection network, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Backbone Network</head><p>A backbone network is used to extract ships' features and it is established by using DS-CNN (marked in a red rectangular frame in Figure <ref type="figure" target="#fig_10">4</ref>) and C-CNN (marked in a blue rectangular frame Figure <ref type="figure" target="#fig_10">4</ref>). Here, in order to avoid loss of original image information (make full use of input image information), we intentionally only set the first layer (Conv2D_1) as a C-CNN, inspired by the experience of MobileNet <ref type="bibr" target="#b43">[44]</ref>.</p><p>From Figure <ref type="figure" target="#fig_10">4</ref>, there are 13 DS-CNNs in all in our backbone network, which have enough capability to fully extract ships' features. As is shown in Figure <ref type="figure" target="#fig_10">4</ref>, a D-Conv2D and a P-Conv2D collectively constitute a basic DS-CNN, which has been introduced in Section 2.2. We made all D-Conv2Ds have the same 3 × 3 kernel size to facilitate the analysis of model parameters. Moreover, certainly, the kernel size of all P-Conv2Ds must be 1 × 1 according to the fundamental conception of DS-CNN in Figure <ref type="figure" target="#fig_2">2</ref> of Section 2.2.</p><p>Figure <ref type="figure" target="#fig_12">5a</ref>,b respectively shows the internal implementation of a C-CNN and a DS-CNN in detail. From Figure <ref type="figure" target="#fig_12">5a</ref>, the C-CNN only carries out a Conv2D operation, then passes through batch normalization (BN) layer, and finally enters into the activation function Leaky-ReLU layer to obtain the output of the C-CNN. From Figure <ref type="figure" target="#fig_12">5b</ref>, the DS-CNN carries out D-Conv2D operation first, then passes through BN layer, and finally enters into the Leaky-ReLU layer to obtain the output of the D-Conv2D. After that, the output of the D-Conv2D layer is inputted into the P-Conv2D layer. P-Conv2D carries out a pointwise convolution operation, then passes through batch normalization (BN) layer, and finally enters into the activation function Leaky-ReLU layer to obtain the output of the P-Conv2D. The output of the P-Conv2D is that of the DS-CNN. Here, BN can accelerate deep network training by reducing internal covariate shift <ref type="bibr" target="#b47">[48]</ref>, which can normalize the input data x to [0,1], and make the output data conform to the standard normal distribution. In addition, Leaky-ReLU <ref type="bibr" target="#b48">[49]</ref>, an improved version of ReLU, is an activation function.</p><p>Its definition is as follows:</p><formula xml:id="formula_22">  =       ,0 ,0 xx y x x (<label>11</label></formula><formula xml:id="formula_23">)</formula><p>where α is a constant and α∈(1, + ). In our model, we set α = 5.5, inspired by the experience of reference <ref type="bibr" target="#b48">[49]</ref>. Leaky-ReLU can reduce the possibility of gradient disappearance because it can reduce the occurrence of dead neurons <ref type="bibr" target="#b48">[49]</ref> for its nonzero derivative when x &lt; 0, compared with ReLU. Here, BN can accelerate deep network training by reducing internal covariate shift <ref type="bibr" target="#b47">[48]</ref>, which can normalize the input data x to [0,1], and make the output data conform to the standard normal distribution. In addition, Leaky-ReLU <ref type="bibr" target="#b48">[49]</ref>, an improved version of ReLU, is an activation function. Its definition is as follows:</p><formula xml:id="formula_24">y = x, x ≥ 0 x α , x &lt; 0 (<label>11</label></formula><formula xml:id="formula_25">)</formula><p>where α is a constant and α ∈ (1, +∞). In our model, we set α = 5.5, inspired by the experience of reference <ref type="bibr" target="#b48">[49]</ref>. Leaky-ReLU can reduce the possibility of gradient disappearance because it can reduce the occurrence of dead neurons <ref type="bibr" target="#b48">[49]</ref> for its nonzero derivative when x &lt; 0, compared with ReLU. Finally, the ships' features extracted by the backbone network are transmitted to the detection network for ship detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Detection Network</head><p>A detection network is used to perform ship detection. From Figure <ref type="figure" target="#fig_10">4</ref>, in order to fully absorb the features extracted from the backbone network and improve detection accuracy, the detection network is established by using C-CNNs. (In our experiments, if detection networks all use DS-CNNs, the accuracy is far less than 90%, which cannot meet the practical requirements.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Multi-Scale Detection Mechanism</head><p>Considering the diversity of ship sizes in the SSDD dataset, we set three different detection networks (detection network-1, detection network-2, and detection network-3), inspired by the experience of YOLO <ref type="bibr" target="#b31">[32]</ref> and SSD <ref type="bibr" target="#b32">[33]</ref>, to detect ships under three different scales. Detection network-1 is designed for detecting big size ships, detection network-2 is designed for detecting medium size ships, and detection network-3 is designed for detecting small size ships.</p><p>Figure <ref type="figure" target="#fig_13">6</ref> is the diagram of multi-scale detection. As is shown in Figure <ref type="figure" target="#fig_13">6</ref>, for example, if the size of the input image is L × L, the size of the output feature maps are L/32 × L/32 in the detection network-1, L/16 × L/16 in the detection network-2, and L/8 × L/8 in the detection network-3. Therefore, our network will generate predictive bounding boxes based on these feature maps. By using multi-scale mechanism, the detection accuracy can be greatly improved. Detailed research of multi-scale detection will be introduced in Section 5. <ref type="bibr" target="#b3">4</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Concatenation Mechanism</head><p>In addition, considering that our method has faster detection speed, but may reduce accuracy.</p><p>Therefore, to improve the detection accuracy, we also adopted concatenation mechanism (two blue arrows in Figure <ref type="figure" target="#fig_10">4</ref>), inspired from the experience of DenseNet <ref type="bibr" target="#b46">[47]</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Concatenation Mechanism</head><p>In addition, considering that our method has faster detection speed, but may reduce accuracy. Therefore, to improve the detection accuracy, we also adopted concatenation mechanism (two blue arrows in Figure <ref type="figure" target="#fig_10">4</ref>), inspired from the experience of DenseNet <ref type="bibr" target="#b46">[47]</ref>.</p><p>Figure <ref type="figure" target="#fig_14">7</ref> is the diagram of concatenation mechanism. From Figure <ref type="figure" target="#fig_14">7</ref>, the two inputs of the concatenation (marked in a red rectangular frame) are A and B. We directly concatenated A and B as an output C without any other operations. In addition, B is generated by some subsequent convolution operations of A, which means the feature maps of A are shallow features and the feature maps of B are deep features. Therefore, the feature maps of C are the combination of shallow features and deep features. In this way, our network can achieve feature fusions and improve accuracy. Especially, the sizes of the two input feature maps (A and B) must be the same, while the number of channels need not be equal. The number of the output channels (C) is the sum of the numbers of the two inputs channels (A and B).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Concatenation Mechanism</head><p>In addition, considering that our method has faster detection speed, but may reduce accuracy.</p><p>Therefore, to improve the detection accuracy, we also adopted concatenation mechanism (two blue arrows in Figure <ref type="figure" target="#fig_10">4</ref>), inspired from the experience of DenseNet <ref type="bibr" target="#b46">[47]</ref>.</p><p>Figure <ref type="figure" target="#fig_14">7</ref> is the diagram of concatenation mechanism. From Figure <ref type="figure" target="#fig_14">7</ref>, the two inputs of the concatenation (marked in a red rectangular frame) are A and B. We directly concatenated A and B as an output C without any other operations. In addition, B is generated by some subsequent convolution operations of A, which means the feature maps of A are shallow features and the feature maps of B are deep features. Therefore, the feature maps of C are the combination of shallow features and deep features. In this way, our network can achieve feature fusions and improve accuracy. Especially, the sizes of the two input feature maps (A and B) must be the same, while the number of channels need not be equal. The number of the output channels (C) is the sum of the numbers of the two inputs channels (A and B).  In our experiments, we concatenated the outputs from different depth layers, which can achieve feature fusions and improve accuracy. As is shown in Figure <ref type="figure" target="#fig_10">4</ref>, we concatenated P-Conv2D_11 layer and UpSampling2D_1 layer for the detection network-2, and P-Conv2D_5 layer and UpSampling2D_2 layer for the detection network-3. For Concatenate_1 layer, the P-Conv2D_11 layer (10 × 10 × 512) and the UpSampling2D_1 layer (10 × 10 × 16) are its inputs, so the size of the output is 10 × 10 × 528 (512 + 16 = 528). For Concatenate_2 layer, the P-Conv2D_9 layer (20 × 20 × 256) and the UpSampling2D_2 layer (20 × 20 × 8) are its inputs, so the size of the output is 20 × 20 × 264 (256 + 8 = 264). In particular, the UpSampling2D_1 layer is to ensure the same size between Conv2D_5 layer (5 × 5 × 16) and P-Conv2D_11 layer (10 × 10 × 512), whose up-sampling multiple is 2. Similarly, the UpSampling2D_2 layer is to ensure the same size between Conv2D_9 (10 × 10 × 8) and P-Conv2D_5 (20 × 20 × 256), whose up-sampling multiple is also 2. Detailed research of concatenation mechanism will be introduced in Section 5.4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Ship Detection Model</head><p>In this section, firstly, we will introduce the ship detection process of our method. Then, we will introduce the anchor box mechanism. Finally, we will present the evaluation indexes of SAR ship detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Detection Process</head><p>We designed our high-speed SAR ship detection system utilizing the basic idea of one-stage detectors <ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref> for their faster detection speed than two-stage detectors.</p><p>Figure <ref type="figure" target="#fig_19">8</ref> shows the detection process of our proposed DS-CNN ship detection system. The detailed ship detection process is as follows.</p><p>Step 1: Input SAR images to be detected. See Figure <ref type="figure" target="#fig_20">8a</ref>.</p><p>Step 2: Resize images into L × L. See Figure <ref type="figure" target="#fig_20">8b</ref>.</p><p>In order to make images of different sizes have the same feature dimension, we need to resize all the images into L × L by resampling. Here, L is a variable that can be adjusted to make a tradeoff between accuracy and speed in our experiments. In particular, since our network can be regarded as 32 times down-sampling (Input: L; Detection network-1: L/32), L must be a multiple of 32. Detailed research of L will be introduced in Section 5.4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Detection Process</head><p>We designed our high-speed SAR ship detection system utilizing the basic idea of one-stage detectors <ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref> for their faster detection speed than two-stage detectors.</p><p>Figure <ref type="figure" target="#fig_19">8</ref> shows the detection process of our proposed DS-CNN ship detection system. The detailed ship detection process is as follows. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>High-speed SAR ship detection system</head><p>Step 1</p><p>Step 2</p><p>Step 3</p><p>Step 4</p><p>Step 7</p><p>Step 6</p><p>Step 5 Step 1: Input SAR images to be detected. See Figure <ref type="figure" target="#fig_20">8a</ref>.</p><p>Step 2: Resize images into L × L. See Figure <ref type="figure" target="#fig_20">8b</ref>.</p><p>In order to make images of different sizes have the same feature dimension, we need to resize all the images into L × L by resampling. Here, L is a variable that can be adjusted to make a tradeoff between accuracy and speed in our experiments. In particular, since our network can be regarded as 32 times down-sampling (Input: L; Detection network-1: L/32), L must be a multiple of 32. Detailed research of L will be introduced in Section 5.4.1.</p><p>Step 3: Divide images into S × S grids. See Figure <ref type="figure" target="#fig_20">8c</ref>.</p><p>We divided images into S × S grids inspired by the basic idea of YOLO <ref type="bibr" target="#b29">[30]</ref>. Here, S has three different values for three different detection scales introduced in Figure <ref type="figure" target="#fig_13">6</ref> of Section 3.2.1: Step 3: Divide images into S × S grids. See Figure <ref type="figure" target="#fig_20">8c</ref>.</p><formula xml:id="formula_26">   / 32, / 16, / 8 S L L L (12)</formula><p>We divided images into S × S grids inspired by the basic idea of YOLO <ref type="bibr" target="#b29">[30]</ref>. Here, S has three different values for three different detection scales introduced in Figure <ref type="figure" target="#fig_13">6</ref> of Section 3.2.1:</p><formula xml:id="formula_27">S ∈ {L/32, L/16, L/8}<label>(12)</label></formula><p>For example, if L = 160, then S = 5, 10, 20. Then, the images will be divided into 5 × 5 grids in the detection network-1, 10 × 10 grids in the detection network-2, and 20 × 20 grids in the detection network-3.</p><p>Step 4: Locate ships' center. See Figure <ref type="figure" target="#fig_20">8d</ref>. When performing ship detection, our network can automatically locate the ships' center. This ability can be obtained by learning from the ground truth (real ships) in the training set. If the center of a ship falls into a grid cell, that grid cell is responsible for detecting this ship. For example, in Figure <ref type="figure" target="#fig_20">8d</ref>, cell P is responsible for detecting ship 1, and cell Q is responsible for detecting ship 2.</p><p>Besides, in order to calculate the loss function in Section 5.2, we also defined the probability that cell i contains ships as follows:</p><formula xml:id="formula_28">P cell i (ship) = 1, cell i contains ships 0, cell i does not contain ships<label>(13)</label></formula><p>Equation ( <ref type="formula" target="#formula_28">13</ref>) means that as long as cell i contains a ship or a part of a ship, the probability that cell i contains a ship is 1, otherwise 0. For example, in Figure <ref type="figure" target="#fig_20">8d,</ref><ref type="figure">cell A,</ref><ref type="figure">B,</ref><ref type="figure">C,</ref><ref type="figure">D,</ref><ref type="figure">E,</ref><ref type="figure">F,</ref><ref type="figure">P</ref>, and Q all contain ships, so:</p><formula xml:id="formula_29">P cell A (ship) = 1, P cell B (ship) = 1, P cell C (ship) = 1, P cell D (ship) = 1 P cell E (ship) = 1, P cell F (ship) = 1, P cell P (ship) = 1, P cell Q (ship) = 1</formula><p>For other cells, the probability is zero.</p><p>Step 5: Generate B bounding boxes. See Figure <ref type="figure" target="#fig_20">8e</ref>. Each grid cell generates B bounding boxes and scores for these boxes. Here, B is a variable that can be adjusted to make a tradeoff between accuracy and speed in experiments. Besides, these B bounding boxes in Figure <ref type="figure" target="#fig_20">8e</ref> may have different sizes coming from the use of different features. For example, in Figure <ref type="figure" target="#fig_20">8e</ref>, if B = 3, grid cell P will generate 3 bounding boxes for ship 1, and grid cell Q will generate 3 bounding boxes for ship 2 (marked in blue in Figure <ref type="figure" target="#fig_20">8e</ref>).</p><p>Additionally, each bounding box contains five predictive parameters (x, y, w, h, score), where (x, y) is the coordinate of the top left vertex, w is the width, and h is the height (marked in Figure <ref type="figure" target="#fig_20">8f</ref>).</p><p>The score of each bounding box is defined by <ref type="bibr" target="#b29">[30]</ref>:</p><formula xml:id="formula_30">score = P cell i (ship) • IoU (<label>14</label></formula><formula xml:id="formula_31">)</formula><p>where IoU is the intersection over union.</p><p>IoU is defined by:</p><formula xml:id="formula_32">IoU(B P , B G ) = area(B P ∩ B G ) area(B P ∪ B G )<label>(15)</label></formula><p>where B P is the prediction box and B G is the ground truth box. Detailed research of the value of B will be introduced in Section 5.4.5.</p><p>Step 6: Non-maximum suppression (NMS) <ref type="bibr" target="#b49">[50]</ref>. See Figure <ref type="figure" target="#fig_20">8f</ref>. In Step 5, there are B bounding boxes for each ship. Here, we retained the one with the maximum score from these boxes, and the others B-1 bounding boxes from Step 5 are suppressed.</p><p>Step 7: Coordinate mapping (Detection results). See Figure <ref type="figure" target="#fig_20">8g</ref>.</p><p>From Step 2, our ship detection model detects ships in L × L images, and these L × L images are obtained by resampling the original images. However, the final prediction box should be drawn in the original images. Therefore, we need to map the coordinates of the prediction box obtained from Step 6 to the original images. Here, our system finally completed the task of ship detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Anchor Box Mechanism</head><p>Anchor box mechanism was first proposed by Ren et al. in Faster R-CNN <ref type="bibr" target="#b27">[28]</ref>, which has been used in many object detectors, such as YOLO, SSD, etc. In our model, it is used to address the drawback that each grid cell detects only one ship, which may lead to more missed detection cases. Next, we will take an example to illustrate.</p><p>For example, in Figure <ref type="figure" target="#fig_22">9a</ref>, an original SAR image contains densely distributed small ships. Additionally, from Figure <ref type="figure" target="#fig_22">9b</ref>, cell P simultaneously contains ship A and ship B. However, from Figure <ref type="figure" target="#fig_19">8</ref>, our ship detection system finally generates only one prediction box (the one with the maximum score) for a grid cell. Therefore, one of the ships is bound to be missed. In order to solve this problem, the anchor box mechanism was proposed. drawback that each grid cell detects only one ship, which may lead to more missed detection cases.</p><p>Next, we will take an example to illustrate.</p><p>For example, in Figure <ref type="figure" target="#fig_22">9a</ref>, an original SAR image contains densely distributed small ships.</p><p>Additionally, from Figure <ref type="figure" target="#fig_22">9b</ref>, cell P simultaneously contains ship A and ship B. However, from Figure <ref type="figure" target="#fig_19">8</ref>, our ship detection system finally generates only one prediction box (the one with the maximum score) for a grid cell. Therefore, one of the ships is bound to be missed. In order to solve this problem, the anchor box mechanism was proposed.</p><p>Figure <ref type="figure" target="#fig_22">9b</ref> is a diagram of the anchor box mechanism. We set 3 anchor boxes for each detection scale (detection network-1, detection network-2, and detection network-3). For example, for one of three scales, in Figure <ref type="figure" target="#fig_22">9b</ref>, our system will assign the detection task of ship A to the anchor box N, and the detection task of ship B to the anchor box M. For the anchor box L, it will be deleted finally according to IoU(BP, BG). Each detection scale has 3 anchor boxes, so our system has 9 anchor boxes in all for 3 detection scales, so it can detect up to 9 ships in the same grid cell. In this way, the detection accuracy of small ships with dense distribution gets improved. Besides, 9 anchor boxes is sufficient because in the SSDD dataset, each grid cell contains less than 9 ships. However, for other dataset, the maximum number of anchor boxes may be different. Detailed research of the number of anchor boxes will be introduced in Section 5.4.5.</p><p>In our experiments, we automatically obtained the size of the anchor box by k-means cluster analysis as is shown in Figure <ref type="figure" target="#fig_22">9c</ref>. The cluster centroids were significantly different than hand-picked anchor boxes. There were fewer short, wide boxes and taller, thin boxes <ref type="bibr" target="#b30">[31]</ref>. Additionally, if we use standard k-means with Euclidean distance, larger boxes generate more error than smaller boxes <ref type="bibr" target="#b30">[31]</ref>.</p><p>However, what we really want are priors that lead to good IoU scores, which are independent of the size of the box. Thus, for our distance metric, we use <ref type="bibr" target="#b30">[31]</ref>: Table <ref type="table" target="#tab_3">1</ref>. shows the size of each anchor box for three scales (detection network-1, detection network-2, and detection network-3). More detailed about anchor box can be found in reference <ref type="bibr" target="#b30">[31]</ref>. Figure <ref type="figure" target="#fig_22">9b</ref> is a diagram of the anchor box mechanism. We set 3 anchor boxes for each detection scale (detection network-1, detection network-2, and detection network-3). For example, for one of three scales, in Figure <ref type="figure" target="#fig_22">9b</ref>, our system will assign the detection task of ship A to the anchor box N, and the detection task of ship B to the anchor box M. For the anchor box L, it will be deleted finally according to IoU(B P , B G ). Each detection scale has 3 anchor boxes, so our system has 9 anchor boxes in all for 3 detection scales, so it can detect up to 9 ships in the same grid cell. In this way, the detection accuracy of small ships with dense distribution gets improved. Besides, 9 anchor boxes is sufficient because in the SSDD dataset, each grid cell contains less than 9 ships. However, for other dataset, the maximum number of anchor boxes may be different. Detailed research of the number of anchor boxes will be introduced in Section 5.4.5.</p><formula xml:id="formula_33">( ) ( ) =-</formula><p>In our experiments, we automatically obtained the size of the anchor box by k-means cluster analysis as is shown in Figure <ref type="figure" target="#fig_22">9c</ref>. The cluster centroids were significantly different than hand-picked anchor boxes. There were fewer short, wide boxes and taller, thin boxes <ref type="bibr" target="#b30">[31]</ref>. Additionally, if we use standard k-means with Euclidean distance, larger boxes generate more error than smaller boxes <ref type="bibr" target="#b30">[31]</ref>.</p><p>However, what we really want are priors that lead to good IoU scores, which are independent of the size of the box. Thus, for our distance metric, we use <ref type="bibr" target="#b30">[31]</ref>:</p><formula xml:id="formula_34">d(anchor box, cluster centroid) = 1 -IoU(anchor box, cluster centroid)<label>(16)</label></formula><p>Table <ref type="table" target="#tab_3">1</ref> shows the size of each anchor box for three scales (detection network-1, detection network-2, and detection network-3). More detailed about anchor box can be found in reference <ref type="bibr" target="#b30">[31]</ref>. </p><formula xml:id="formula_35">Precision = TP TP + FP (<label>17</label></formula><formula xml:id="formula_36">)</formula><p>where TP is the number of the True Positive and FP is the number of the False Positive. In addition, TP can be understood that real ships are correctly detected, and FP can be understood as missed detection.</p><p>Recall is defined by:</p><formula xml:id="formula_37">Recall = TP TP + FN (<label>18</label></formula><formula xml:id="formula_38">)</formula><p>where FN is the number of the False Negative. In addition, FN can be understood as false alarm. Mean average precision (mAP) is defined by:</p><formula xml:id="formula_39">mAP = 1 0 P(R)dR (<label>19</label></formula><formula xml:id="formula_40">)</formula><p>where P is precision, R is recall, and P(R) is precision-recall (P-R) curve. Apparently, mAP is a comprehensive evaluation index, which combines precision and recall. Therefore, in our work, we used mAP to represent detection accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.">Detection Speed</head><p>The ship detection time of each SAR image is expressed as:</p><formula xml:id="formula_41">Time (ms)<label>(20)</label></formula><p>Frames per second (FPS) is defined by:</p><formula xml:id="formula_42">FPS = 1 s Time<label>(21)</label></formula><p>FPS means that object detectors can detect the number of images in one second (mathematical reciprocal of time). Therefore, in our work, we used FPS to represent detection speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, firstly, we will introduce the SSDD dataset used in this paper. Then, we will introduce the loss function. Afterwards, we will introduce our training strategies. Finally, we will introduce the establishment process of the SAR ship detection model through five types of researches.</p><p>Our experimental hardware platform is a personal computer (PC) with the hardware configuration of Intel(R) i9-9900K CPU @3.60GHz, NVIDIA RTX2080Ti GPU, and 32G memory. Our experiments are performed on PyCharm <ref type="bibr" target="#b50">[51]</ref> software platform, with Python 3.5 language. Our programs are written based on the Keras framework <ref type="bibr" target="#b51">[52]</ref>, a Python-based deep learning library with TensorFlow <ref type="bibr" target="#b52">[53]</ref> as backend. In addition, we call GPU through CUDA 10.0 and cuDNN 7.6 for training acceleration.</p><p>In our experiments, to achieve better detection accuracy, we set IoU = 0.5 and score = 0.5 as detection thresholds, which means that if the probability of a bounding box containing a ship is greater than or equal to 50%, it is retained. In fact, these two detection thresholds can be dynamically adjusted as a good tradeoff between false alarm and missed detection according to the actual detection results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Dataset</head><p>In the DL field, a dataset that has been correctly labeled is a momentous prerequisite for research. Therefore, we chose an open SSDD dataset <ref type="bibr" target="#b34">[35]</ref> to verify the correctness and feasibility of our proposed method.</p><p>Table <ref type="table" target="#tab_4">2</ref> shows the detailed descriptions of SSDD. From Table <ref type="table" target="#tab_4">2</ref>, there are 1160 SAR images in SSDD dataset coming from three different sensors and there are 2358 ships in these images, with 2.03 ships in one image on average. These 1160 SAR images are labeled by Li et al. <ref type="bibr" target="#b34">[35]</ref> by using LabelImg <ref type="bibr" target="#b53">[54]</ref>, an image annotation software. SAR images in this dataset possess different satellite sensors, various polarization modes, multiple resolutions, different scenes, and abundant ship sizes, so it can verify the robustness of methods. Therefore, many scholars <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b54">[55]</ref><ref type="bibr" target="#b55">[56]</ref><ref type="bibr" target="#b56">[57]</ref><ref type="bibr" target="#b57">[58]</ref><ref type="bibr" target="#b58">[59]</ref><ref type="bibr" target="#b59">[60]</ref><ref type="bibr" target="#b60">[61]</ref><ref type="bibr" target="#b61">[62]</ref><ref type="bibr" target="#b62">[63]</ref> conducted research based on it for a better comparison. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Loss Function</head><p>The task of ship detection is to provide five parameters (x, y, w, h, score) of a prediction box. Therefore, our loss function is mainly composed of the errors of these five parameters.</p><p>Loss function of the predictive coordinates (x, y) is defined by:</p><formula xml:id="formula_43">loss (x,y) = B i=0 S 2 j=0 P cell j (ship) • x i -xi,j<label>2</label></formula><formula xml:id="formula_44">+ y i -ŷi,j 2<label>(22)</label></formula><p>where (x i , y i ) is the coordinate of the i-th ship's ground truth box, ( xi,j , ŷi,j ) is the coordinate of the i-th ship's prediction box of the j-th grid cell. Loss function of the predictive width and height (w, h) is defined by:</p><formula xml:id="formula_45">loss (w,h) = B i=0 S 2 j=0 P cell j (ship) • √ w i -ŵi,j<label>2</label></formula><formula xml:id="formula_46">+ h i -ĥi,j 2<label>(23)</label></formula><p>where (w i , h i ) is the width and height of the i-th ship's ground truth box, ( ŵi,j , ĥi,j ) is the width and height of the i-th ship's prediction box of the j-th grid cell. Loss function of the predictive confidence (score) is defined by:</p><formula xml:id="formula_47">loss (score) = 1 γ • B i=0 S 2 j=0 P cell j (ship) • ŝi,j -IoU(B P , B G ) + B i=0 S 2 j=0 1 -P cell j (ship) • ŝi,j 2<label>(24)</label></formula><p>where ŝi,j is the score of the i-th ship's prediction box of the j-th grid cell, and γ is a weight coefficient. Therefore, the total loss function is:</p><formula xml:id="formula_48">loss = α • loss (x,y) +β • loss (w,h) +γ • loss (score)<label>(25)</label></formula><p>where α, β and γ are the weight coefficients, which indicate the weight of various types of loss in the total loss. In particular, here, γ is the same as that in Equation ( <ref type="formula" target="#formula_47">24</ref>), inspired from the experience of reference <ref type="bibr" target="#b29">[30]</ref>.</p><p>In our ship detection model, we set α = β = 5 and γ = 0.5, inspired by the experience of YOLO <ref type="bibr" target="#b29">[30]</ref>. By using Equation <ref type="bibr" target="#b24">(25)</ref>, our model will have a good convergence in training process, which is a critical premise for our work. The good convergence in training process will be presented in Figure <ref type="figure" target="#fig_23">10</ref> of Section 5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Training Strategies</head><p>We randomly divided the dataset into a training set, a validation set and a test set according to the ratio of 7:2:1. The training set is used to train the model, the verification set is used to adjust the model to avoid over-fitting, and the test set is used to evaluate the performance of the model.</p><p>We used adaptive moment estimation (Adam) algorithm <ref type="bibr" target="#b63">[64]</ref> to realize the iteration of network parameters. We trained the network for 100 epochs with batch size = 8, which means that every eight samples complete a parameter update. For the first 50 epochs, the learning rate was set to 0.001; for the last 50 epochs, the learning rate was changed to 0.0001 in order to further reduce the loss. In the last 50 epochs, if the loss of verification set does not decrease for three consecutive times, the learning rate will automatically decrease.</p><p>For the deeper layers of our backbone network, we adopted dropout mechanism <ref type="bibr" target="#b64">[65]</ref> to accelerate the learning speed of the network and avoid over-fitting, by which each neuron loses its activity at a 0.1% probability. In addition, in order to further avoid over-fitting of the whole network, we also adopted early stopping mechanism <ref type="bibr" target="#b65">[66]</ref>, by which if the loss of the verification set does not decrease for 10 consecutive times, the network is forced to stop training.</p><p>detection model was retained and others were deleted.</p><p>Figure <ref type="figure" target="#fig_23">10</ref> shows the loss curves of the training set and the validation set. From Figure <ref type="figure" target="#fig_23">10</ref>, for one thing, the loss can be reduced rapidly, which shows that the loss function we set in Section 5.2 is effective (only need 10 epochs from 3500 loss to 45 loss, a good convergence). For another thing, the gap between the loss of the validation set and the training set is very narrow, which indicates that there is not an over-fitting phenomenon in our network, so our training strategies are effective and feasible. In particular, in Figure <ref type="figure" target="#fig_23">10</ref>, this network trained only 77 epochs due to the early stopping mechanism. In fact, in our experiments, almost every training was forced to stop (&lt;100 epochs), which reflected the powerful and effective role of early stopping mechanism.  Especially, in order to reduce the number of iterations and avoid falling into local optimum, we first pre-trained our network on ImageNet dataset <ref type="bibr" target="#b39">[40]</ref>. In the DL field, ImageNet pre-training <ref type="bibr" target="#b66">[67]</ref> is a normal practice, which has adopted by many other object detectors. Certainly, we can also start training from scratch <ref type="bibr" target="#b67">[68]</ref>, but it reduces the accuracy by 4% in our experiments. Detailed research of pre-training will be introduced in Section 5.4.2.</p><p>We also used TensorBoard <ref type="bibr" target="#b52">[53]</ref>, a visualization tool of TensorFlow, to facilitate the monitor of the training process. In our experiments, we saved the model only when the loss of the verification set of the current epoch is better than that of the previous epoch. Finally, the optimal SAR ship detection model was retained and others were deleted.</p><p>Figure <ref type="figure" target="#fig_23">10</ref> shows the loss curves of the training set and the validation set. From Figure <ref type="figure" target="#fig_23">10</ref>, for one thing, the loss can be reduced rapidly, which shows that the loss function we set in Section 5.2 is effective (only need 10 epochs from 3500 loss to 45 loss, a good convergence). For another thing, the gap between the loss of the validation set and the training set is very narrow, which indicates that there is not an over-fitting phenomenon in our network, so our training strategies are effective and feasible. In particular, in Figure <ref type="figure" target="#fig_23">10</ref>, this network trained only 77 epochs due to the early stopping mechanism. In fact, in our experiments, almost every training was forced to stop (&lt;100 epochs), which reflected the powerful and effective role of early stopping mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Establishment of Model</head><p>Next, we will establish the most suitable high-speed SAR ship detection model through the following five types of researches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1.">Research on Image Size</head><p>From Figure <ref type="figure" target="#fig_19">8</ref>, the images to be detected are resized into L × L by resampling. Then, we need to determine the final exact value of L to obtain a better detection performance. Therefore, we studied the influence of different values of L on detection performance. In particular, since our network can be regarded as 32 times down-sampling (Input: L; Detection network-1: L/32), L must be a multiple of 32.</p><p>Thus, we set:</p><formula xml:id="formula_49">L = 32k, k = 1, 2, • • • , 10<label>(26)</label></formula><p>to conduct comparative experiments. In addition, these ten groups of experiments were conducted under the following same conditions: Table <ref type="table" target="#tab_5">3</ref> shows the evaluation indexes of research on image size. Figure <ref type="figure" target="#fig_28">11a</ref> shows the precision-recall (P-R) curves and Figure <ref type="figure" target="#fig_28">11b</ref> is the bar graph of mAP (accuracy) and FPS (speed). to conduct comparative experiments.</p><p>In addition, these ten groups of experiments were conducted under the following same Table <ref type="table" target="#tab_5">3</ref> shows the evaluation indexes of research on image size. Figure <ref type="figure" target="#fig_28">11a</ref> shows the precision-recall (P-R) curves and Figure <ref type="figure" target="#fig_28">11b</ref> is the bar graph of mAP (accuracy) and FPS (speed).   From Table <ref type="table" target="#tab_5">3</ref> and Figure <ref type="figure" target="#fig_27">11</ref>, we can draw the following conclusions:</p><p>(1) The detection accuracy becomes higher with the increase of L when L &lt; 160 (marked in green in Figure <ref type="figure" target="#fig_28">11b</ref>). The detection accuracy is the highest when L = 160. The detection accuracy does not increase any more, but fluctuates slightly when L &gt; 160;</p><p>(2) The detection speed becomes lower with the increase of L (marked in pink in Figure <ref type="figure" target="#fig_28">11b</ref>). This phenomenon is in line with common sense because large-size images have more pixels, leading to more computation.</p><p>Finally, we chose L = 160 as a tradeoff between accuracy and speed in our final model. For one thing, it has relatively high accuracy (94.13% mAP). For another thing, its detection speed does not decrease too much, compared with L = 32 (from 124 FPS to 111 FPS).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2.">Research on Pretraining</head><p>In the DL field, in recent years, a common practice is to pre-train the model on some large-scale datasets (such as ImageNet dataset) and then fine-tune the model on other target tasks with less training data <ref type="bibr" target="#b66">[67]</ref>. Compared with 1.26 million images in ImageNet, the number of SAR images in SSDD dataset is very small (only 1160 images). Therefore, we studied the effect of pretraining on ImageNet on detection performance.</p><p>We arranged two groups of experiments, one for pretraining on ImageNet and the other for non-pretraining on ImageNet, and these two groups of experiments were conducted under the following same conditions: Table <ref type="table" target="#tab_7">4</ref> shows the evaluation indexes of research on pretraining. Figure <ref type="figure" target="#fig_32">12a</ref> shows the precision-recall (P-R) curves and Figure <ref type="figure" target="#fig_32">12b</ref> is the bar graph of mAP (accuracy) and FPS (speed). Finally, we chose L = 160 as a tradeoff between accuracy and speed in our final model. For one thing, it has relatively high accuracy (94.13% mAP). For another thing, its detection speed does not decrease too much, compared with L = 32 (from 124 FPS to 111 FPS).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2.">Research on Pretraining</head><p>In the DL field, in recent years, a common practice is to pre-train the model on some large-scale datasets (such as ImageNet dataset) and then fine-tune the model on other target tasks with less training data <ref type="bibr" target="#b66">[67]</ref>. Compared with 1.26 million images in ImageNet, the number of SAR images in SSDD dataset is very small (only 1160 images). Therefore, we studied the effect of pretraining on ImageNet on detection performance.</p><p>We arranged two groups of experiments, one for pretraining on ImageNet and the other for non-pretraining on ImageNet, and these two groups of experiments were conducted under the following same conditions: Table <ref type="table" target="#tab_7">4</ref> shows the evaluation indexes of research on pretraining. Figure <ref type="figure" target="#fig_32">12a</ref> shows the precision-recall (P-R) curves and Figure <ref type="figure" target="#fig_32">12b</ref> is the bar graph of mAP (accuracy) and FPS (speed).  From Table <ref type="table" target="#tab_7">4</ref> and Figure <ref type="figure" target="#fig_31">12</ref>, we can draw the following conclusions: From Table <ref type="table" target="#tab_7">4</ref> and Figure <ref type="figure" target="#fig_31">12</ref>, we can draw the following conclusions:</p><p>(1) The detection accuracy of pretraining on ImageNet is superior to that of non-pretraining.</p><p>This phenomenon shows that pretraining is an effective way to improve the detection accuracy; (2) The detection speed of pretraining and non-pretraining is similar (110 FPS and 111 FPS) because the pre-training is only carried out in the training process, it does not affect the subsequent actual detection.</p><p>Although pretraining on ImageNet is time-consuming before establishing the model, it can improve the accuracy considerably. Therefore, finally, we still chose pretraining on ImageNet in our final model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.3.">Research on Multi-Scale Detection Mechanism</head><p>In our network, we set three different detection networks (detection network-1, detection network-2, and detection network-3) to detect ships with different sizes (L/32, L/16, and L/8), as is shown in Figure <ref type="figure" target="#fig_13">6</ref>.</p><p>Therefore, we made a further research on multi-scale detection to confirm that it can indeed improve the detection accuracy.</p><p>We arranged seven groups of comparative experiments under the following same conditions: Table <ref type="table" target="#tab_9">5</ref> shows the evaluation indexes of research on multi-scale detection mechanism. Figure <ref type="figure" target="#fig_36">13a</ref> shows the precision-recall (P-R) curves and Figure <ref type="figure" target="#fig_36">13b</ref> is the bar graph of mAP (accuracy) and FPS (speed). shown in Figure <ref type="figure" target="#fig_13">6</ref>. Therefore, we made a further research on multi-scale detection to confirm that it can indeed improve the detection accuracy.</p><p>We arranged seven groups of comparative experiments under the following same conditions: Table <ref type="table" target="#tab_9">5</ref> shows the evaluation indexes of research on multi-scale detection mechanism. Figure <ref type="figure" target="#fig_36">13a</ref> shows the precision-recall (P-R) curves and Figure <ref type="figure" target="#fig_36">13b</ref> is the bar graph of mAP (accuracy) and FPS (speed).  From Table <ref type="table" target="#tab_9">5</ref> and Figure <ref type="figure" target="#fig_35">13</ref>, we can draw the following conclusions: From Table <ref type="table" target="#tab_9">5</ref> and Figure <ref type="figure" target="#fig_35">13</ref>, we can draw the following conclusions:</p><p>(1) The detection accuracy shows a upward trend with the increase of the number of detection scales. If using only one scale, the detection accuracy is the lowest. If using two scales, the detection accuracy is better than that of one scale. However, it is still too poor to meet the actual needs (mAP &lt; 90%). If using all three scales, the detection accuracy is the highest (mAP = 94.13%). This phenomenon shows that multi-scale detection mechanism can indeed improve detection accuracy; (2) The detection speed shows a downward trend with the increase of the number of detection scales.</p><p>This phenomenon is in line with common sense because the size of the network has increased.</p><p>In addition, in the SSDD dataset, the smallest size of ship is 4 × 7 and the biggest size of ship is 211 × 298. Ships from the smallest to the biggest can all be detected successfully by these three different detection scales. In other words, three different detection scales are sufficient. Certainly, if more detection scales are used, the detection accuracy may be higher while the detection speed is bound to decline.</p><p>Therefore, finally, we chose three detection scales in our final model, which can achieve the best detection accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.4.">Research on Concatenation Mechanism</head><p>In our detection network, we set two concatenations to achieve feature fusion, as is shown in Figure <ref type="figure" target="#fig_10">4</ref>. Therefore, we studied the effect of concatenation and non-concatenation.</p><p>We arranged two groups of comparative experiments under the following same conditions: Table <ref type="table" target="#tab_13">6</ref> shows the evaluation indexes of research on concatenation mechanism. Figure <ref type="figure" target="#fig_46">14a</ref> shows the precision-recall (P-R) curves and Figure <ref type="figure" target="#fig_46">14b</ref> is the bar graph of mAP (accuracy) and FPS (speed). 2) The detection speed shows a downward trend with the increase of the number of detection scales. This phenomenon is in line with common sense because the size of the network has increased.</p><p>In addition, in the SSDD dataset, the smallest size of ship is 4×7 and the biggest size of ship is 211×298. Ships from the smallest to the biggest can all be detected successfully by these three different detection scales. In other words, three different detection scales are sufficient. Certainly, if more detection scales are used, the detection accuracy may be higher while the detection speed is bound to decline.</p><p>Therefore, finally, we chose three detection scales in our final model, which can achieve the best detection accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.4.">Research on Concatenation Mechanism</head><p>In our detection network, we set two concatenations to achieve feature fusion, as is shown in Table <ref type="table" target="#tab_13">6</ref> shows the evaluation indexes of research on concatenation mechanism. Figure <ref type="figure" target="#fig_46">14a</ref> shows the precision-recall (P-R) curves and Figure <ref type="figure" target="#fig_46">14b</ref> is the bar graph of mAP (accuracy) and FPS (speed).  From Table <ref type="table" target="#tab_13">6</ref> and Figure <ref type="figure" target="#fig_39">14</ref>, we can draw the following conclusions: From Table <ref type="table" target="#tab_13">6</ref> and Figure <ref type="figure" target="#fig_39">14</ref>, we can draw the following conclusions:</p><p>(1) The detection accuracy of concatenation is superior to that of non-concatenation (94.13% mAP &gt; 92.43% mAP). This phenomenon shows that concatenation mechanism can indeed improve detection accuracy because the shallow features and deep features have been fully integrated; (2) The detection speed of concatenation and non-concatenation is almost equal (112 FPS and 111 FPS), because only two concatenations does not increase network parameters too much.</p><p>Finally, in order to obtain better detection accuracy, we adopted the concatenation mechanism in our final model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.5.">Research on Anchor Box Mechanism</head><p>Finally, we also studied the influence of the number of anchor boxes on the detection performance. Therefore, we arranged three groups of experiments, and the number of their anchor boxes were 3, 6, and 9, respectively. If there are 3 anchor boxes, then each detection scale generates only one bounding box. If there are 6 anchor boxes, then each detection scale generates two bounding boxes. If there are 9 anchor boxes, then each detection scale generates three bounding boxes. Besides, 9 anchor boxes are sufficient, because, in the SSDD dataset, a grid cell contains less than 9 ships. However, for other dataset, the maximum number of anchor boxes may be different.</p><p>In addition, the three groups of experiments were conducted under the following same conditions: Table <ref type="table" target="#tab_15">7</ref> shows the evaluation indexes of research on anchor box mechanism. Figure <ref type="figure" target="#fig_44">15a</ref> shows the precision-recall (P-R) curves and Figure <ref type="figure" target="#fig_44">15b</ref> is the bar graph of mAP (accuracy) and FPS (speed). 2) The detection speed of concatenation and non-concatenation is almost equal (112 FPS and 111 FPS), because only two concatenations does not increase network parameters too much.</p><p>Finally, in order to obtain better detection accuracy, we adopted the concatenation mechanism in our final model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.5.">Research on Anchor Box Mechanism</head><p>Finally, we also studied the influence of the number of anchor boxes on the detection performance. Therefore, we arranged three groups of experiments, and the number of their anchor boxes were 3, 6, and 9, respectively. If there are 3 anchor boxes, then each detection scale generates only one bounding box. If there are 6 anchor boxes, then each detection scale generates two bounding boxes. If there are 9 anchor boxes, then each detection scale generates three bounding boxes. Besides, 9 anchor boxes are sufficient, because, in the SSDD dataset, a grid cell contains less than 9 ships. However, for other dataset, the maximum number of anchor boxes may be different.</p><p>In Table <ref type="table" target="#tab_15">7</ref> shows the evaluation indexes of research on anchor box mechanism. Figure <ref type="figure" target="#fig_44">15a</ref> shows the precision-recall (P-R) curves and Figure <ref type="figure" target="#fig_44">15b</ref> is the bar graph of mAP (accuracy) and FPS (speed).  From Table <ref type="table" target="#tab_15">7</ref> and Figure <ref type="figure" target="#fig_43">15</ref>, we can draw the following conclusions: From Table <ref type="table" target="#tab_15">7</ref> and Figure <ref type="figure" target="#fig_43">15</ref>, we can draw the following conclusions:</p><p>(1) The detection accuracy becomes higher with the increase of the number of anchor boxes. One possible reason is that more anchor boxes can improve the detection accuracy of densely distributed small ships; (2) The detection speed is not greatly affected by the number of anchor boxes, because the number of the network parameters are invariable for these three cases and only a small amount of follow-up processing is added with the increase of the number of anchor boxes.</p><p>In addition, for these three cases, we also compared the detection results of some small ships with the dense distribution. Figure <ref type="figure" target="#fig_45">16</ref> shows the ship detection results of a SAR image with densely distributed small ships. In Figure <ref type="figure" target="#fig_47">16a</ref>, there are 27 real ships in the image. In Figure <ref type="figure" target="#fig_47">16b</ref>, for 3 anchor boxes, 15 ships are detected successfully and 12 ships are missed detected. In Figure <ref type="figure" target="#fig_47">16c</ref>, for 6 anchor boxes, 21 ships are detected successfully and 6 ships are missed detected. In Figure <ref type="figure" target="#fig_47">16d</ref>, for 9 anchor boxes, 25 ships are detected successfully and 1 ship is missed detected, meanwhile a false alarm case appears. In brief, if there are more anchor boxes, our model has a lower missed detection rate for those small and densely distributed ships.</p><p>One possible reason is that more anchor boxes can improve the detection accuracy of densely distributed small ships;</p><p>2) The detection speed is not greatly affected by the number of anchor boxes, because the number of the network parameters are invariable for these three cases and only a small amount of follow-up processing is added with the increase of the number of anchor boxes.</p><p>In addition, for these three cases, we also compared the detection results of some small ships with the dense distribution. Figure <ref type="figure" target="#fig_45">16</ref> shows the ship detection results of a SAR image with densely distributed small ships. In Figure <ref type="figure" target="#fig_47">16a</ref>, there are 27 real ships in the image. In Figure <ref type="figure" target="#fig_47">16b</ref>, for 3 anchor boxes, 15 ships are detected successfully and 12 ships are missed detected. In Figure <ref type="figure" target="#fig_47">16c</ref>, for 6 anchor boxes, 21 ships are detected successfully and 6 ships are missed detected. In Figure <ref type="figure" target="#fig_47">16d</ref>, for 9 anchor boxes, 25 ships are detected successfully and 1 ship is missed detected, meanwhile a false alarm case appears. In brief, if there are more anchor boxes, our model has a lower missed detection rate for those small and densely distributed ships. Therefore, finally, we chose 9 anchor boxes in our final model, which can improve the accuracy of small ships.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Visualization of Feature Maps</head><p>Through the above five types of researches in Section 5. After building the model, in order to more vividly present the processing of the input image by deep neural networks, we visualized some important feature maps of our network (input, output of the 1 st layer, output of backbone network, output of detection network-1, output of detection network-2, and output of detection network-3). Besides, the visualization of feature maps is also convenient to understand the process of feature extraction by deep neural networks.</p><p>Figure <ref type="figure" target="#fig_49">17</ref> shows the visualization of some important feature maps. In Figure <ref type="figure" target="#fig_50">17a</ref>, the image is resized into 160×160, and the number of channels is 3. In Figure <ref type="figure" target="#fig_50">17b</ref>, after the convolution operation of the 1 st layer (Conv2D_1 in Figure <ref type="figure" target="#fig_10">4</ref>), the size of all feature maps is 80×80×32, where 80 is the number of width pixels and height pixels and 32 is the number of channels. From Figure <ref type="figure" target="#fig_50">17b</ref>, the features extracted from deep networks are abstract and difficult to explain (maybe texture, edge, shape, etc.), which is a consensus in the DL field <ref type="bibr">[20] [69]</ref>. Even this, computers can surprisingly accurately detect objects based on these abstract features, which subverts the feature extraction theory of traditional methods. In Figure <ref type="figure" target="#fig_50">17c</ref>, after processing of the backbone network, the outputs consist of 1024 feature maps with the size of 5×5 (P-Conv2D_13 in Figure <ref type="figure" target="#fig_10">4</ref>). Then, these 1024 Therefore, finally, we chose 9 anchor boxes in our final model, which can improve the accuracy of small ships.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Visualization of Feature Maps</head><p>Through the above five types of researches in Section 5.4, we determined our final high-speed SAR ship detection model, and the specific parameters are as follows: After building the model, in order to more vividly present the processing of the input image by deep neural networks, we visualized some important feature maps of our network (input, output of the 1 st layer, output of backbone network, output of detection network-1, output of detection network-2, and output of detection network-3). Besides, the visualization of feature maps is also convenient to understand the process of feature extraction by deep neural networks.</p><p>Figure <ref type="figure" target="#fig_49">17</ref> shows the visualization of some important feature maps. In Figure <ref type="figure" target="#fig_50">17a</ref>, the image is resized into 160 × 160, and the number of channels is 3. In Figure <ref type="figure" target="#fig_50">17b</ref>, after the convolution operation of the 1 st layer (Conv2D_1 in Figure <ref type="figure" target="#fig_10">4</ref>), the size of all feature maps is 80 × 80 × 32, where 80 is the number of width pixels and height pixels and 32 is the number of channels. From Figure <ref type="figure" target="#fig_50">17b</ref>, the features extracted from deep networks are abstract and difficult to explain (maybe texture, edge, shape, etc.), which is a consensus in the DL field <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b68">69]</ref>. Even this, computers can surprisingly accurately detect objects based on these abstract features, which subverts the feature extraction theory of traditional methods. In Figure <ref type="figure" target="#fig_50">17c</ref>, after processing of the backbone network, the outputs consist of 1024 feature maps with the size of 5 × 5 (P-Conv2D_13 in Figure <ref type="figure" target="#fig_10">4</ref>). Then, these 1024 feature maps are inputted into three detection networks for ship detection. In Figure <ref type="figure" target="#fig_50">17d</ref>, there are 18 feature maps with the size of 5 × 5 (L/32) for detecting big size ships. In Figure <ref type="figure" target="#fig_50">17e</ref>, there are 18 feature maps with the size of 10 × 10 (L/16) for detecting medium size ships. In Figure <ref type="figure" target="#fig_50">17f</ref>, there are 18 feature maps with the size of 20 × 20 (L/8) for detecting small size ships. Finally, our model will generate prediction boxes based on the feature maps of Figure <ref type="figure" target="#fig_50">17d-f</ref>. More details about the visualization of feature maps can be found in reference <ref type="bibr" target="#b68">[69]</ref>.</p><p>feature maps are inputted into three detection networks for ship detection. In Figure <ref type="figure" target="#fig_50">17d</ref>, there are 18 feature maps with the size of 5×5 (L/32) for detecting big size ships. In Figure <ref type="figure" target="#fig_50">17e</ref>, there are 18 feature maps with the size of 10×10 (L/16) for detecting medium size ships. In Figure <ref type="figure" target="#fig_50">17f</ref>, there are 18 feature maps with the size of 20×20 (L/8) for detecting small size ships. Finally, our model will generate prediction boxes based on the feature maps of Figure <ref type="figure" target="#fig_50">17d-f</ref>. More details about the visualization of feature maps can be found in reference <ref type="bibr" target="#b68">[69]</ref>. (f) output of detection network-3.</p><formula xml:id="formula_50">(b) (c)<label>(d) (e) (f)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Results</head><p>In this section, firstly, we will carry out actual ship detection on the test set of SSDD dataset.</p><p>Then, to verify the migration capability of the model, we will also carry out actual ship detection in a wide-region large-size Sentinel-1 SAR image. Finally, we will make a performance comparison between our method and other methods on the SSDD dataset. In addition, we did not show the performance comparison results on the Sentinel-1 SAR image, because of: 1) the similar conclusions to that on the SSDD dataset; 2) page limit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Results of Ship Detection on Test Set</head><p>Figure <ref type="figure" target="#fig_51">18</ref> shows the SAR ship detection results of some sample images in the SSDD dataset.</p><p>From Figure <ref type="figure" target="#fig_51">18</ref>, real ships in various backgrounds can almost be detected correctly, which shows that our ship detection system has strong robustness. In addition, there are also some false alarm and missed detection cases. For the former, one possible reason is the high similarity between other port facilities and ships. For the latter, one possible reason is that these ships are densely distributed and too small, which bring some obstacles to the system for their lower IoU scores. In a word, from a subjective point of view in Figure <ref type="figure" target="#fig_51">18</ref>, our model achieves satisfactory ship detection results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Results</head><p>In this section, firstly, we will carry out actual ship detection on the test set of SSDD dataset. Then, to verify the migration capability of the model, we will also carry out actual ship detection in a wide-region large-size Sentinel-1 SAR image. Finally, we will make a performance comparison between our method and other methods on the SSDD dataset. In addition, we did not show the performance comparison results on the Sentinel-1 SAR image, because of: (1) the similar conclusions to that on the SSDD dataset; (2) page limit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Results of Ship Detection on Test Set</head><p>Figure <ref type="figure" target="#fig_51">18</ref> shows the SAR ship detection results of some sample images in the SSDD dataset. From Figure <ref type="figure" target="#fig_51">18</ref>, real ships in various backgrounds can almost be detected correctly, which shows that our ship detection system has strong robustness. In addition, there are also some false alarm and missed detection cases. For the former, one possible reason is the high similarity between other port facilities and ships. For the latter, one possible reason is that these ships are densely distributed and too small, which bring some obstacles to the system for their lower IoU scores. In a word, from a subjective point of view in Figure <ref type="figure" target="#fig_51">18</ref>, our model achieves satisfactory ship detection results.</p><p>Then, we objectively evaluated our model on the test set of SSDD. Table <ref type="table" target="#tab_18">8</ref> shows the evaluation indexes of the test set of SSDD. Figure <ref type="figure" target="#fig_55">19a</ref> shows the precision-recall (P-R) curves and Figure <ref type="figure" target="#fig_55">19b</ref> is the bar graph of mAP (accuracy) and FPS (speed). Then, we objectively evaluated our model on the test set of SSDD. Table <ref type="table" target="#tab_18">8</ref> shows the evaluation indexes of the test set of SSDD. Figure <ref type="figure" target="#fig_55">19a</ref> shows the precision-recall (P-R) curves and Figure <ref type="figure" target="#fig_55">19b</ref> is the bar graph of mAP (accuracy) and FPS (speed).    From Figure <ref type="figure" target="#fig_53">19</ref> and Table <ref type="table" target="#tab_18">8</ref>, we can draw the following conclusions:</p><p>1) The detection accuracy of our method is 94.13% mAP, which can meet the requirement of SAR ship detection;</p><p>2) The detection speed of our method is 111 FPS, which can detect 111 SAR images in the SSDD dataset per second. The number of SAR images in the test set of the SSDD dataset is From Figure <ref type="figure" target="#fig_53">19</ref> and Table <ref type="table" target="#tab_18">8</ref>, we can draw the following conclusions:</p><p>(1) The detection accuracy of our method is 94.13% mAP, which can meet the requirement of SAR ship detection; (2) The detection speed of our method is 111 FPS, which can detect 111 SAR images in the SSDD dataset per second. The number of SAR images in the test set of the SSDD dataset is 116 (10% of 1160), and DS-CNN averagely takes 9.03 ms to complete the ship detection of one image. Therefore, DS-CNN takes 1047.48 ms (116 × 9.03 ms) to complete the ship detection of the whole test set. Moreover, there are 182 ships in the test set of the SSDD dataset, then the detection average speed is 5.76 ms/ship (1047.48 ms/182).</p><p>Finally, we also compared the detection results of images with severe speckle noise and clear images. Table <ref type="table" target="#tab_20">9</ref> shows the ship detection evaluation indexes of images with severe speckle noise and clear images. Figure <ref type="figure" target="#fig_58">20a</ref> shows the precision-recall (P-R) curves and Figure <ref type="figure" target="#fig_58">20b</ref> is the bar graph of mAP (accuracy) and FPS (speed). Figure <ref type="figure" target="#fig_59">21</ref> shows the ship detection results of images with severe speckle noise and clear images.  From Figure <ref type="figure" target="#fig_53">19</ref> and Table <ref type="table" target="#tab_18">8</ref>, we can draw the following conclusions:</p><p>1) The detection accuracy of our method is 94.13% mAP, which can meet the requirement of SAR ship detection;</p><p>2) The detection speed of our method is 111 FPS, which can detect 111 SAR images in the Finally, we also compared the detection results of images with severe speckle noise and clear images. Table <ref type="table" target="#tab_20">9</ref> shows the ship detection evaluation indexes of images with severe speckle noise and clear images. Figure <ref type="figure" target="#fig_58">20a</ref> shows the precision-recall (P-R) curves and Figure <ref type="figure" target="#fig_58">20b</ref> is the bar graph of mAP (accuracy) and FPS (speed). Figure <ref type="figure" target="#fig_59">21</ref> shows the ship detection results of images with severe speckle noise and clear images.   From Table <ref type="table" target="#tab_20">9</ref>, Figures <ref type="figure" target="#fig_58">20</ref> and<ref type="figure" target="#fig_59">21</ref>, we can draw the following conclusions:</p><p>(1) The detection accuracy of the nosiy images is superior to that of the clear images (96.91% mAP &gt; 93.63% mAP). This phenomenon seems to be out of line with common sense, because usually, clear images should have higher accuracy than noisy ones. In fact, the possible reasons for this phenomenon are that (1) in the test set of the SSDD dataset (1160 × 10% = 116), the number of moisy images is less than that of clear images (28 of 116 &lt; 88 of 116), which may lead to greater accuracy randomness; (2) noisy images have a single background and contain offshore ships (See Figure <ref type="figure" target="#fig_60">21a</ref>) while clear images have more complex backgrounds and contain onshore ships (See Figure <ref type="figure" target="#fig_60">21b</ref>), and evidently, it is more difficulty to detect the onshore ships than the offshore ships; (2) The detection speed of the clear images is similar to that of the nosiy images, because the size of images is the same. (Ground truth is marked in green, missed detection is marked in red, and false alarm is marked in yellow).</p><p>From Table <ref type="table" target="#tab_20">9</ref>, Figure <ref type="figure" target="#fig_57">20</ref>-21, we can draw the following conclusions:</p><p>1) The detection accuracy of the nosiy images is superior to that of the clear images (96.91% mAP &gt; 93.63% mAP). This phenomenon seems to be out of line with common sense, because usually, clear images should have higher accuracy than noisy ones. In fact, the possible reasons for this phenomenon are that 1) in the test set of the SSDD dataset (1160×10% = 116), the number of moisy images is less than that of clear images (28 of 116 &lt; 88 of 116), which may lead to greater accuracy randomness; 2) noisy images have a single background and contain offshore ships (See Figure <ref type="figure" target="#fig_60">21a</ref>) while clear images have more complex backgrounds and contain onshore ships (See Figure <ref type="figure" target="#fig_60">21b</ref>), and evidently, it is more difficulty to detect the onshore ships than the offshore ships;</p><p>2) The detection speed of the clear images is similar to that of the nosiy images, because the size of images is the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Results of Ship Detection on Sentinel-1</head><p>In order to verify that our ship detection model has strong migration ability, we carried out actual ship detection on a wide-region large-size Sentinel-1 SAR image with multiple targets and complex backgrounds. We obtained this SAR image from the website of reference <ref type="bibr" target="#b69">[70]</ref>, and obtained the ground truth information from the Association for Information Systems (AIS) <ref type="bibr" target="#b70">[71,</ref><ref type="bibr">72]</ref>.</p><p>Table <ref type="table" target="#tab_22">10</ref> shows the descriptions of the Sentinel-1 SAR image to be detected. Observation with Progressive Scans SAR (TOPSAR) <ref type="bibr" target="#b71">[73]</ref>.</p><p>We directly divided the original wide-region large-size SAR image (6333 × 4185) into 900 sub-images on average, with 211×140 size for each sub-image, and then resized these 900 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Results of Ship Detection on Sentinel-1</head><p>In order to verify that our ship detection model has strong migration ability, we carried out actual ship detection on a wide-region large-size Sentinel-1 SAR image with multiple targets and complex backgrounds. We obtained this SAR image from the website of reference <ref type="bibr" target="#b69">[70]</ref>, and obtained the ground truth information from the Association for Information Systems (AIS) <ref type="bibr" target="#b70">[71,</ref><ref type="bibr">72]</ref>. Table <ref type="table" target="#tab_22">10</ref> shows the descriptions of the Sentinel-1 SAR image to be detected. We directly divided the original wide-region large-size SAR image (6333 × 4185) into 900 sub-images on average, with 211 × 140 size for each sub-image, and then resized these 900 sub-images into 160 × 160. Finally, these sub-images were inputted into our high-speed SAR ship detection system to carry out the actual ship detection.</p><p>Figure <ref type="figure" target="#fig_61">22</ref> shows the ship detection results of the Sentinel-1 SAR image. From Figure <ref type="figure" target="#fig_61">22</ref>, our method can detect almost all ships (marked in blue). There are some false alarm cases on land (marked in yellow), which is due to the high similarity between these land facilities and real ships. Besides, there are some missed detection cases (marked in red), which may be due to the dense distribution or parallel parking of ships, causing certain difficulties for detection. In short, our method has strong migration ability which can be applied in practical SAR ship detection.</p><p>Then, we evaluated our model on the Sentinel-1 SAR image. Table <ref type="table" target="#tab_24">11</ref> shows the evaluation indexes of the Sentinel-1 SAR image. Figure <ref type="figure" target="#fig_66">23a</ref> shows the precision-recall (P-R) curves and Figure <ref type="figure" target="#fig_66">23b</ref> is the bar graph of mAP (accuracy) and FPS (speed).</p><p>Figure <ref type="figure" target="#fig_61">22</ref> shows the ship detection results of the Sentinel-1 SAR image. From Figure <ref type="figure" target="#fig_61">22</ref>, our method can detect almost all ships (marked in blue). There are some false alarm cases on land (marked in yellow), which is due to the high similarity between these land facilities and real ships.</p><p>Besides, there are some missed detection cases (marked in red), which may be due to the dense distribution or parallel parking of ships, causing certain difficulties for detection. In short, our method has strong migration ability which can be applied in practical SAR ship detection. Then, we evaluated our model on the Sentinel-1 SAR image. Table <ref type="table" target="#tab_24">11</ref> shows the evaluation indexes of the Sentinel-1 SAR image. Figure <ref type="figure" target="#fig_66">23a</ref> shows the precision-recall (P-R) curves and Figure <ref type="figure" target="#fig_66">23b</ref> is the bar graph of mAP (accuracy) and FPS (speed).     Then, we evaluated our model on the Sentinel-1 SAR image. Table <ref type="table" target="#tab_24">11</ref> shows the evaluation 784 indexes of the Sentinel-1 SAR image. Figure <ref type="figure" target="#fig_66">23a</ref> shows the precision-recall (P-R) curves and Figure <ref type="figure" target="#fig_20">785</ref> 23b is the bar graph of mAP (accuracy) and FPS (speed).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>786</head><p>Table <ref type="table" target="#tab_24">11</ref>. Evaluation indexes of the ship detection results of the Sentinel-1 SAR image.  From Table <ref type="table" target="#tab_24">11</ref> and Figure <ref type="figure" target="#fig_62">23</ref>, our method has high accuracy with 90.36% mAP, which can meet the practical application requirements. The time to divide the wide-region large-size SAR image into small-size sub-images (time of preprocessing) is 54.85 × 10 -3 s by using Python Imaging Library (PIL), an image preprocessing tool. Additionally, our method takes only 8.92 × 10 -3 s for one sub-image (160 × 160) similar to the detection time in SSDD dataset (9.03 × 10 -3 s), and takes only 8.09 s (8.92 × 10 -3 s × 900 + 54.85 × 10 -3 s = 8.09 s) to complete the whole wide-region large-size SAR image detection (6333 × 4185). Therefore, our method is of value in practical application from the above detection results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Compared with C-CNN</head><p>From Figure <ref type="figure" target="#fig_10">4</ref>, there are 13 DS-CNNs in our backbone network, which are used to extract ships' features. To confirm that DS-CNN can really speed up ship detection and sacrifice little accuracy, we replaced these 13 DS-CNNs in Figure <ref type="figure" target="#fig_10">4</ref> with 13 C-CNNs to carry out further research, as is shown in Figure <ref type="figure" target="#fig_67">24</ref>.</p><p>the practical application requirements. The time to divide the wide-region large-size SAR image into small-size sub-images (time of preprocessing) is 54.85×10 -3 s by using Python Imaging Library (PIL), an image preprocessing tool. Additionally, our method takes only 8.92×10 -3 s for one sub-image (160×160) similar to the detection time in SSDD dataset (9.03×10 -3 s), and takes only 8.09 s (8.92×10 -3 s×900 + 54.85×10 -3 s =8.09s) to complete the whole wide-region large-size SAR image detection (6333 × 4185). Therefore, our method is of value in practical application from the above detection results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Compared with C-CNN</head><p>From Figure <ref type="figure" target="#fig_10">4</ref>, there are 13 DS-CNNs in our backbone network, which are used to extract ships' features. To confirm that DS-CNN can really speed up ship detection and sacrifice little accuracy, we replaced these 13 DS-CNNs in Figure <ref type="figure" target="#fig_10">4</ref> with 13 C-CNNs to carry out further research, as is shown in Figure <ref type="figure" target="#fig_67">24</ref>. In particular, we only compared the detection speed of small-size sub-images in the test set of the SSDD dataset, because the faster detection speed of small-size sub-images will naturally bring about the increase of detection speed of wide-region large-size images.</p><p>Table <ref type="table" target="#tab_27">12</ref> shows the evaluation indexes of C-CNN and DS-CNN. Figure <ref type="figure" target="#fig_18">25a</ref> shows the precision-recall (P-R) curves and Figure <ref type="figure" target="#fig_18">25b</ref> is the bar graph of mAP (accuracy) and FPS (speed).</p><p>From Table <ref type="table" target="#tab_27">12</ref> and Figure <ref type="figure" target="#fig_18">25</ref>, we can draw the following conclusions:</p><p>1) The detection accuracy of DS-CNN is slightly lower than C-CNN (94.13% mAP &lt; 94.39% mAP). This phenomenon shows that using DS-CNN to build a network is feasible and does not decline the detection accuracy too much;</p><p>2) The detection speed of DS-CNN is 2.7 times than C-CNN (111 FPS &gt; 41 FPS). This phenomenon shows that DS-CNN can indeed improve the detection speed by using D-Conv2D and P-Conv2D to replace C-CNN.  In addition, in order to ensure the rationality of the study, we arranged the following two groups of experiments under the following same conditions: In particular, we only compared the detection speed of small-size sub-images in the test set of the SSDD dataset, because the faster detection speed of small-size sub-images will naturally bring about the increase of detection speed of wide-region large-size images.</p><p>Table <ref type="table" target="#tab_27">12</ref> shows the evaluation indexes of C-CNN and DS-CNN. Figure <ref type="figure" target="#fig_18">25a</ref> shows the precision-recall (P-R) curves and Figure <ref type="figure" target="#fig_18">25b</ref> is the bar graph of mAP (accuracy) and FPS (speed).  From Table <ref type="table" target="#tab_27">12</ref> and Figure <ref type="figure" target="#fig_18">25</ref>, we can draw the following conclusions:</p><p>(1) The detection accuracy of DS-CNN is slightly lower than C-CNN (94.13% mAP &lt; 94.39% mAP). This phenomenon shows that using DS-CNN to build a network is feasible and does not decline the detection accuracy too much; (2) The detection speed of DS-CNN is 2.7 times than C-CNN (111 FPS &gt; 41 FPS). This phenomenon shows that DS-CNN can indeed improve the detection speed by using D-Conv2D and P-Conv2D to replace C-CNN.</p><p>Especially, it is meaningful to increase the detection speed from 41 FPS to 111 FPS for 160 × 160 images. For example, DS-CNN only takes 10.45 s to complete the ship detection task of 1160 SAR images in the SSDD dataset, while C-CNN needs 28.2 s. In general, even millisecond level time saving is meaningful and valuable in the field of image processing. In addition, for the wide-region large-size SAR image (6333 × 4185) in Figure <ref type="figure" target="#fig_61">22</ref>, the detection time of DS-CNN is 8.09 s, while that of C-CNN is 21.8 s. From the perspective of image processing, such much time saving is remarkable, which can facilitate subsequent reprocessing (such as ship classification) and improve efficiency of full-link SAR application. Most noteworthy, generally, the size of SAR images acquired from spaceborne SAR satellites is often larger (tens of thousands of pixels × tens of thousands of pixels), so in this case, the advantages of DS-CNN will be better reflected.</p><p>We also compared the network sizes of C-CNN and DS-CNN. In the DL field, the number of network parameters, model file size, and weight file size are often used to measure the size of the network. Therefore, we made a comparison from the above three perspectives to illustrate the lightweight nature of our network. Table <ref type="table" target="#tab_29">13</ref> shows the network sizes of C-CNN and DS-CNN. From Table <ref type="table" target="#tab_29">13</ref>, the number of network parameters of our proposed DS-CNN is only 3,299,862, which is about one-ninth of the C-CNN's <ref type="bibr" target="#b27">(28,</ref><ref type="bibr">352,</ref><ref type="bibr">054)</ref>. Similarly, the model file size and weight file size are nearly one-tenth of the C-CNN's. Therefore, our network is lighter, leading to higher training efficiency and faster detection speed, which accords with the theoretical analysis before in Section 2.3.</p><p>Figure <ref type="figure" target="#fig_71">26</ref> shows the ship detection results of C-CNN and DS-CNN. From Figure <ref type="figure" target="#fig_71">26</ref>, both C-CNN and DS-CNN can detect almost all ships successfully, with similar accuracy. In addition, for the second image in Figure <ref type="figure" target="#fig_71">26</ref>, DS-CNN generates more missed detection cases (the number of missed detection is (3) than C-CNN (the number of missed detection is (1). One possible reason is that DS-CNN does not adequately extract features from the inshore ships.</p><p>Therefore, our method is effective and brilliant. Despite of the slight sacrifice of accuracy for the inshore ships, the detection speed has got largely improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Compared with Other Object Detectors</head><p>In order to prove that our method can authentically achieve high-speed ship detection, we also compared the performance with other object detectors, such as Faster R-CNN, RetinaNet, SSD, YOLOv1, YOLOv2, YOLOv2-tiny, YOLOv3, and YOLOv3-tiny. Here, YOLOv2-tiny and YOLOv3-tiny are two lightweight networks that improve YOLOv2 and YOLOv3 respectively, proposed by Redmon et al. <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>. Moreover, in order to ensure the rationality of experiments, these other object detectors are implemented on the same platform, and the training mechanism is basically consistent. (Ground truth is marked in green, missed detection is marked in red, and false alarm is marked in yellow.).</p><p>Therefore, our method is effective and brilliant. Despite of the slight sacrifice of accuracy for the inshore ships, the detection speed has got largely improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Compared with Other Object Detectors</head><p>In order to prove that our method can authentically achieve high-speed ship detection, we also compared the performance with other object detectors, such as Faster R-CNN, RetinaNet, SSD, YOLOv1, YOLOv2, YOLOv2-tiny, YOLOv3, and YOLOv3-tiny. Here, YOLOv2-tiny and YOLOv3-tiny are two lightweight networks that improve YOLOv2 and YOLOv3 respectively, proposed by Redmon. et.al <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>. Moreover, in order to ensure the rationality of experiments, these other object detectors are implemented on the same platform, and the training mechanism is basically consistent.</p><p>Especially, the detection speed of the traditional feature extraction methods is quite slow, whose detection time reaches several seconds or more per image, while the modern DL methods only need tens or hundreds of milliseconds per image. Therefore, we ignored the comparison with the traditional feature extraction methods.</p><p>Table <ref type="table" target="#tab_30">14</ref> shows the evaluation indexes of different object detectors. Figure <ref type="figure" target="#fig_73">27</ref> shows the precision-recall (P-R) curves and Figure <ref type="figure" target="#fig_74">28</ref> is the bar graph of mAP (accuracy) and FPS (speed).  Especially, the detection speed of the traditional feature extraction methods is quite slow, whose detection time reaches several seconds or more per image, while the modern DL methods only need tens or hundreds of milliseconds per image. Therefore, we ignored the comparison with the traditional feature extraction methods.</p><p>Table <ref type="table" target="#tab_30">14</ref> shows the evaluation indexes of different object detectors. Figure <ref type="figure" target="#fig_73">27</ref> shows the precision-recall (P-R) curves and Figure <ref type="figure" target="#fig_74">28</ref> is the bar graph of mAP (accuracy) and FPS (speed). From Table <ref type="table" target="#tab_30">14</ref> and Figures <ref type="figure" target="#fig_73">27</ref> and<ref type="figure" target="#fig_74">28</ref>, we can draw the following conclusions:</p><p>(1) The detection accuracy of our method is slightly lower than RetinaNet (94.13% mAP &lt; 95.68% mAP). However, our detection speed is almost 37 times faster than RetinaNet. Therefore, it is of far-reaching significance that a little sacrifice of accuracy brings about a multiplier increase in speed; (2) The detection accuracy of our method is also slightly lower than YOLOv3 (94.13% mAP &lt; 95.34% mAP). However, our detection speed is 2.47 times faster than YOLOv3. Therefore, our approach is still excellent because we traded 2.1% accuracy for a 147% speed increase;</p><p>(3) The detection accuracy and speed of our method are all superior to Faster R-CNN, SSD, YOLOv1, YOLOv2, and YOLOv3-tiny; (4) The detection speed of our method is slightly faster than YOLOv2-tiny (111 FPS &gt; 107 FPS).</p><p>However, the detection accuracy of YOLOv2-tiny is too poor to meet the actual detection requirements at all (44.40% mAP), which is far lower than that of our method (94.13% mAP); (5) The detection speed of our method is superior to all other methods (9.03 ms per image and 111 FPS); (6) Our method has not only the fastest detection speed but also satisfactory accuracy (Both mAP and FPS are high in Figure <ref type="figure" target="#fig_74">28</ref>), while other methods cannot maintain a good balance between accuracy and speed.   From Table <ref type="table" target="#tab_30">14</ref> and Figure <ref type="figure" target="#fig_73">27</ref>-28, we can draw the following conclusions: 867</p><p>1) The detection accuracy of our method is slightly lower than RetinaNet (94.13% mAP &lt; 868 95.68% mAP). However, our detection speed is almost 37 times faster than RetinaNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>869</head><p>Therefore, it is of far-reaching significance that a little sacrifice of accuracy brings about a 870 multiplier increase in speed;</p><p>871</p><p>2) The detection accuracy of our method is also slightly lower than YOLOv3 (94.13% mAP &lt; 872 95.34% mAP). However, our detection speed is 2.47 times faster than YOLOv3. Therefore,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>873</head><p>our approach is still excellent because we traded 2.1% accuracy for a 147% speed increase;</p><p>874</p><p>3) The detection accuracy and speed of our method are all superior to Faster R-CNN, SSD,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>875</head><p>YOLOv1, YOLOv2, and YOLOv3-tiny;</p><p>876  From Table <ref type="table" target="#tab_30">14</ref> and Figure <ref type="figure" target="#fig_73">27</ref>-28, we can draw the following conclusions:</p><p>1) The detection accuracy of our method is slightly lower than RetinaNet (94.13% mAP &lt; 95.68% mAP). However, our detection speed is almost 37 times faster than RetinaNet.</p><p>Therefore, it is of far-reaching significance that a little sacrifice of accuracy brings about a multiplier increase in speed;</p><p>2) The detection accuracy of our method is also slightly lower than YOLOv3 (94.13% mAP &lt; 95.34% mAP). However, our detection speed is 2.47 times faster than YOLOv3. Therefore, our approach is still excellent because we traded 2.1% accuracy for a 147% speed increase;</p><p>3) The detection accuracy and speed of our method are all superior to Faster R-CNN, SSD, YOLOv1, YOLOv2, and YOLOv3-tiny; Figure <ref type="figure" target="#fig_78">29</ref> shows the ship detection results of some sample images of different object detectors. From Figure <ref type="figure" target="#fig_78">29</ref>, we can draw the following conclusions:</p><p>(1) All methods can successfully detect ships in simple sample images, except YOLOv3-tiny and YOLOv2-tiny; (2) YOLOv3-tiny and YOLOv2-tiny generate more missed detection cases;</p><p>(3) RetinaNet and YOLOv2 have a better detection performance for small ships with dense distribution, with only one missed detection case; (4) Compared with RetinaNet and YOLOv2, YOLOv3 have similar detection performance for small ships with dense distribution, with only one missed detection case and only one false alarm case; (5) Compared with all other methods, from a subjective point of view in Figure <ref type="figure" target="#fig_78">29</ref>, our method has obtained satisfactory ship detection results, although not the best. We also compared the network sizes of other object detectors. Table <ref type="table" target="#tab_33">15</ref> shows the network sizes of different object detectors. From Table <ref type="table" target="#tab_33">15</ref>, our model has the fewest network parameters only 3,299,862, while the numbers of network parameters of other methods are several or even tens of times than that of our method. Moreover, the model file size and the weight file size are also minimal compared with other methods. Therefore, our network is lighter, leading to higher training efficiency and faster detection speed, which also accords with the theoretical analysis before in Section 2.3.</p><p>In summary, by comparing the actual network size, it further more forcefully indicated that our method can achieve high-speed SAR ship detection. We also compared the network sizes of other object detectors. Table <ref type="table" target="#tab_33">15</ref> shows the network sizes of different object detectors. From Table <ref type="table" target="#tab_33">15</ref>, our model has the fewest network parameters only 3,299,862, while the numbers of network parameters of other methods are several or even tens of times than that of our method. Moreover, the model file size and the weight file size are also minimal compared with other methods. Therefore, our network is lighter, leading to higher training efficiency and faster detection speed, which also accords with the theoretical analysis before in Section 2.3. In summary, by comparing the actual network size, it further more forcefully indicated that our method can achieve high-speed SAR ship detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.">Compared with References</head><p>We also compared our methods with some previous open studies which use the same SSDD dataset. To be clear, here, we replaced NVIDIA RTX2080Ti GPU with NVIDIA GTX1080 GPU to keep the hardware environment basically the same as previous other open studies (References <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b58">[59]</ref><ref type="bibr" target="#b59">[60]</ref><ref type="bibr" target="#b60">[61]</ref><ref type="bibr" target="#b61">[62]</ref> used NVIDIA GTX1080 GPU.). For different performances of different types of GPUs, we have to make such a replacement in order to consider the rationality of the comparison experiment.</p><p>Figure <ref type="figure" target="#fig_80">30</ref> shows the contrast results of ship detection speed (FPS, Frames Per Second) between our method and previous other open studies. From Figure <ref type="figure" target="#fig_80">30</ref>, the ship detection speed of our method is 53 FPS under the NVIDIA GTX1080 GPU, which is the half of that under the NVIDIA RTX2080Ti GPU (111 FPS introduced before). This phenomenon is in line with common sense, because the performance of RTX2080Ti GPU is superior to that of GTX1080 GPU. In Figure <ref type="figure" target="#fig_80">30</ref>, our method has the fastest detection speed compared with the other references under the similar hardware environment with NVIDIA GTX1080 GPU. Most noteworthy, the detection speed of reference <ref type="bibr" target="#b9">[10]</ref> is 48 FPS, which is close to that of the method in this paper (53 FPS). However, the accuracy of reference <ref type="bibr" target="#b9">[10]</ref> is lower than ours (Reference <ref type="bibr" target="#b9">[10]</ref>: From Figure <ref type="figure" target="#fig_80">30</ref>, the ship detection speed of our method is 53 FPS under the NVIDIA GTX1080 GPU, which is the half of that under the NVIDIA RTX2080Ti GPU (111 FPS introduced before). This phenomenon is in line with common sense, because the performance of RTX2080Ti GPU is superior to that of GTX1080 GPU. In Figure <ref type="figure" target="#fig_80">30</ref>, our method has the fastest detection speed compared with the other references under the similar hardware environment with NVIDIA GTX1080 GPU. Most noteworthy, the detection speed of reference <ref type="bibr" target="#b9">[10]</ref> is 48 FPS, which is close to that of the method in this paper (53 FPS). However, the accuracy of reference <ref type="bibr" target="#b9">[10]</ref> is lower than ours (Reference <ref type="bibr" target="#b9">[10]</ref>: 90.16% mAP; Ours: 94.13% mAP).</p><p>Therefore, our method can reliably realize high-speed SAR ship detection. In addition, due to the lacking of specific type GPUs, we cannot make full comparisons between our method and the remaining studies using the same SSDD dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions</head><p>Aiming at the problem that the detection speed of SAR ship is often neglected at present, in this paper, a brand-new light-weight network was established with fewer network parameters by mainly using DS-CNN to achieve high-speed SAR ship detection. DS-CNN is composed of D-Conv2D and P-Conv2D, which substitute for the C-CNN, by which the number of network parameters get greatly decreased. Consequently, the detection speed gets dramatically improved. In addition, multi-scale detection mechanism, concatenation mechanism and anchor box mechanism are integrated into our network to improve the detection accuracy. We verified the correctness and effectiveness of the proposed method on an open SSDD dataset. The experimental results indicated that our method has the faster ship detection speed than other object detectors, under the same hardware platform, with 9.03 ms per image and 111 FPS, meanwhile the detection accuracy is only lightly sacrificed compared with the state-of-art object detectors. Besides, we also discussed the reasons why the proposed method can achieve high-speed ship detection via theoretical complexity analysis and calculation of network size. Our method can achieve high-speed and accurate ship detection simultaneously (111 FPS and 94.13% mAP) compared with other methods, which has great application value in real-time maritime disaster rescue and emergency military planning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1 K 2 K 3 K 4 K 1 K 2 K 3 K 1 I 2 I 3 I 4 I 1 I 2 I 3 I 4 I 1 O 2 O 3 O 4 O 1 O 2 O 3 O 4 O 4 KFigure 1 .</head><label>1234123123412341234123441</label><figDesc>Figure 1. Convolutional neural network. (a) Conventional convolution neural network (C-CNN); (b) depthwise separable convolution neural network (DS-CNN).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Convolutional neural network. (a) Conventional convolution neural network (C-CNN); (b) depthwise separable convolution neural network (DS-CNN).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Detailed internal operation flow of a DS-CNN. (a) Depthwise convolution (D-Conv2D); (b) pointwise convolution (P-Conv2D).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Computational complexity. (a) C-CNN; (b) DS-CNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Detailed internal operation flow of a DS-CNN. (a) Depthwise convolution (D-Conv2D); (b) pointwise convolution (P-Conv2D).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>219</head><label></label><figDesc>into a whole as the final output. Through the above two steps, DS-CNN successfully decouples 220 channel correlation and spatial correlation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Detailed internal operation flow of a DS-CNN. (a) Depthwise convolution (D-Conv2D); (b) pointwise convolution (P-Conv2D).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Computational complexity. (a) C-CNN; (b) DS-CNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Computational complexity. (a) C-CNN; (b) DS-CNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>4 . 1 .Figure 4 .</head><label>414</label><figDesc>Figure4shows the network architecture of our proposed high-speed SAR ship detection system. Our network consists of a backbone network and a detection network, inspired from the experience of MobileNet<ref type="bibr" target="#b43">[44]</ref>, YOLO<ref type="bibr" target="#b31">[32]</ref>, SSD<ref type="bibr" target="#b32">[33]</ref>, and DenseNet<ref type="bibr" target="#b46">[47]</ref>. From Figure4, in our network, the images to be detected are resized into L × L and the number of channels is 3. In Figure4, in our model, to achieve a good tradeoff between accuracy and speed, we set L = 160. Detailed research of the value of L will be introduced in Section 5.4.1.Remote Sens. 2019, 11, x FOR PEER REVIEW 8 of 38</figDesc><graphic coords="7,386.19,520.95,122.49,56.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Network Architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Internal implementation. (a) C-CNN; (b) DS-CNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Internal implementation. (a) C-CNN; (b) DS-CNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Multi-scale detection mechanism.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 7</head><label>7</label><figDesc>Figure 7  is the diagram of concatenation mechanism. From Figure7, the two inputs of the concatenation (marked in a red rectangular frame) are A and B. We directly concatenated A and B as an output C without any other operations. In addition, B is generated by some subsequent convolution operations of A, which means the feature maps of A are shallow features and the feature maps of B are deep features. Therefore, the feature maps of C are the combination of shallow features and deep features. In this way, our network can achieve feature fusions and improve accuracy. Especially, the sizes of the two input feature maps (A and B) must be the same, while the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Multi-scale detection mechanism.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>fusion: shallow features + deep features .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 7 .Figure 7 .</head><label>77</label><figDesc>Figure 7. Concatenation mechanism.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>2 B</head><label>2</label><figDesc>bounding boxes for ship 1. (marked in blue) B bounding boxes for ship 2.(marked in blue)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Detection process of high-speed SAR ship detection system. (a) Original images; (b) resize images; (c) divide images; (d) locate ships' center; (e) generate bounding boxes; (f) non-maximum suppression; (g) detection results.</figDesc><graphic coords="11,88.24,88.90,419.30,258.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Detection process of high-speed SAR ship detection system. (a) Original images; (b) resize images; (c) divide images; (d) locate ships' center; (e) generate bounding boxes; (f) non-maximum suppression; (g) detection results.</figDesc><graphic coords="11,89.81,93.60,413.55,252.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Anchor box. (a) An original SAR image with densely distributed small ships (Ground truth is marked in green.); (b) diagram of anchor box mechanism. (c) cluster results of SSDD. (The x axis is the width of ground truth and y axis is the height of ground truth.).</figDesc><graphic coords="12,349.65,604.40,166.19,123.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Anchor box. (a) An original SAR image with densely distributed small ships (Ground truth is marked in green.); (b) diagram of anchor box mechanism. (c) cluster results of SSDD. (The x axis is the width of ground truth and y axis is the height of ground truth.).</figDesc><graphic coords="12,100.33,608.50,97.24,95.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. Loss curves of the training set and the validation set.Figure 10. Loss curves of the training set and the validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. Loss curves of the training set and the validation set.Figure 10. Loss curves of the training set and the validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head></head><label></label><figDesc>Three detection scales; (d) Nine anchor boxes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. Research on image size. (a) P-R curves; (b) bar graph of mAP (accuracy) and FPS (speed).Figure 11. Research on image size. (a) P-R curves; (b) bar graph of mAP (accuracy) and FPS (speed).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. Research on image size. (a) P-R curves; (b) bar graph of mAP (accuracy) and FPS (speed).Figure 11. Research on image size. (a) P-R curves; (b) bar graph of mAP (accuracy) and FPS (speed).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_29"><head></head><label></label><figDesc>(a) L = 160; (b) Two concatenations; (c) Three detection scales; (d) Nine anchor boxes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_31"><head>Figure 12 .</head><label>12</label><figDesc>Figure 12. Research on pretraining. (a) P-R curves; (b) bar graph of mAP (accuracy) and FPS (speed).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_32"><head>Figure 12 .</head><label>12</label><figDesc>Figure 12. Research on pretraining. (a) P-R curves; (b) bar graph of mAP (accuracy) and FPS (speed).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_35"><head>FPSFigure 13 .</head><label>13</label><figDesc>Figure 13. Research on multi-scale detection mechanism. (a) P-R curves; (b) bar graph of mAP (accuracy) and FPS (speed).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_36"><head>Figure 13 .</head><label>13</label><figDesc>Figure 13. Research on multi-scale detection mechanism. (a) P-R curves; (b) bar graph of mAP (accuracy) and FPS (speed).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_38"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Therefore, we studied the effect of concatenation and non-concatenation. We arranged two groups of comparative experiments under the following same conditions: a) L = 160; b) Pretraining; c) Three detection scales; d) Nine anchor boxes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_39"><head>Figure 14 .</head><label>14</label><figDesc>Figure 14. Research on concatenation mechanism. (a) P-R curves; (b) bar graph of mAP (accuracy) and FPS (speed).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_40"><head>Figure 14 .</head><label>14</label><figDesc>Figure 14. Research on concatenation mechanism. (a) P-R curves; (b) bar graph of mAP (accuracy) and FPS (speed).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_42"><head></head><label></label><figDesc>addition, the three groups of experiments were conducted under the following same</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_43"><head>Figure 15 .</head><label>15</label><figDesc>Figure 15. Research on anchor box mechanism. (a) P-R curves; (b) bar graph of mAP (accuracy) and FPS (speed).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_44"><head>Figure 15 .</head><label>15</label><figDesc>Figure 15. Research on anchor box mechanism. (a) P-R curves; (b) bar graph of mAP (accuracy) and FPS (speed).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_45"><head>Figure 16 .</head><label>16</label><figDesc>Figure 16. Ship detection result of an image with densely distributed small ships. (a) Ground truth; (b) 3 anchor boxes; (c) 6 anchor boxes; (d) 9 anchor boxes. (Ground truth is marked in green, missed detection is marked in red, and false alarm is marked in yellow.).</figDesc><graphic coords="22,224.14,186.20,70.28,72.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_46"><head>4 ,</head><label>4</label><figDesc>we determined our final high-speed SAR ship detection model, and the specific parameters are as follows: a) L = 160; b) Pretraining; c) Two concatenations; d) Three scales detection; e) Nine anchor boxes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_47"><head>Figure 16 .</head><label>16</label><figDesc>Figure 16. Ship detection result of an image with densely distributed small ships. (a) Ground truth; (b) 3 anchor boxes; (c) 6 anchor boxes; (d) 9 anchor boxes. (Ground truth is marked in green, missed detection is marked in red, and false alarm is marked in yellow.).</figDesc><graphic coords="22,144.90,185.97,74.05,74.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_48"><head></head><label></label><figDesc>(a) L = 160; (b) Pretraining; (c) Two concatenations; (d) Three scales detection; (e) Nine anchor boxes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_49"><head>Figure 17 .</head><label>17</label><figDesc>Figure 17. Visualization of feature maps. (a) An original image; (b) output of the 1 st layer; (c) output of backbone network; (d) output of detection network-1; (e) output of detection network-2; (f) output of detection network-3.</figDesc><graphic coords="23,98.55,292.18,118.58,59.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_50"><head>Figure 17 .</head><label>17</label><figDesc>Figure 17. Visualization of feature maps. (a) An original image; (b) output of the 1 st layer; (c) output of backbone network; (d) output of detection network-1; (e) output of detection network-2; (f) output of detection network-3.</figDesc><graphic coords="23,236.72,292.18,118.58,59.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_51"><head>Figure 18 .</head><label>18</label><figDesc>Figure 18. SAR ship detection results. (Ground truth is marked in green, missed detection is marked in red, and false alarm is marked in yellow).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_52"><head>Figure 18 .</head><label>18</label><figDesc>Figure 18. SAR ship detection results. (Ground truth is marked in green, missed detection is marked in red, and false alarm is marked in yellow).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_53"><head>Figure 19 .</head><label>19</label><figDesc>Figure 19. Results of the test set of SSDD. (a) P-R curves; (b) bar graph of mAP (accuracy) and FPS (speed).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_54"><head>Figure 19 .</head><label>19</label><figDesc>Figure 19. Results of the test set of SSDD. (a) P-R curves; (b) bar graph of mAP (accuracy) and FPS (speed).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_55"><head>Figure 19 .</head><label>19</label><figDesc>Figure 19. Results of the test set of SSDD. (a) P-R curves; (b) bar graph of mAP (accuracy) and FPS (speed).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_56"><head></head><label></label><figDesc>SSDD dataset per second. The number of SAR images in the test set of the SSDD dataset is 116 (10% of 1160), and DS-CNN averagely takes 9.03 ms to complete the ship detection of one image. Therefore, DS-CNN takes 1047.48 ms (116×9.03 ms) to complete the ship detection of the whole test set. Moreover, there are 182 ships in the test set of the SSDD dataset, then the detection average speed is 5.76 ms/ship (1047.48 ms/182).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_57"><head>Figure 20 .</head><label>20</label><figDesc>Figure 20. Results of images of with severe speckle noise and clear images. (a) P-R curves; (b) bar graph of mAP (accuracy) and FPS (speed).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_58"><head>Figure 20 .</head><label>20</label><figDesc>Figure 20. Results of images of with severe speckle noise and clear images. (a) P-R curves; (b) bar graph of mAP (accuracy) and FPS (speed).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_59"><head>Figure 21 .</head><label>21</label><figDesc>Figure 21. SAR ship detection results of images of with severe speckle noise and clear images.(Ground truth is marked in green, missed detection is marked in red, and false alarm is marked in yellow).</figDesc><graphic coords="26,226.16,223.92,58.33,58.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_60"><head>Figure 21 .</head><label>21</label><figDesc>Figure 21. SAR ship detection results of images of with severe speckle noise and clear images. (Ground truth is marked in green, missed detection is marked in red, and false alarm is marked in yellow).</figDesc><graphic coords="26,162.80,223.92,58.33,58.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_61"><head>Figure 22 .</head><label>22</label><figDesc>Figure 22. Ship detection results of the Sentinel-1 SAR image. (Detection results are marked in blue, missed detection is marked in red, and false alarm is marked in yellow).</figDesc><graphic coords="27,80.67,190.08,78.00,77.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_62"><head>FPSFigure 23 .</head><label>23</label><figDesc>Figure 23. Results of the Sentinel-1 image. (a) P-R curves; (b) bar graph of mAP (accuracy) and FPS (speed).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_63"><head>Figure 22 .</head><label>22</label><figDesc>Figure 22. Ship detection results of the Sentinel-1 SAR image. (Detection results are marked in blue, missed detection is marked in red, and false alarm is marked in yellow).</figDesc><graphic coords="27,441.80,190.12,78.00,77.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_64"><head>783Figure 22 .</head><label>22</label><figDesc>Figure 22. Ship detection results of the Sentinel-1 SAR image. (Detection results are marked in blue, missed detection is marked in red, and false alarm is marked in yellow).</figDesc><graphic coords="27,165.42,90.45,269.65,177.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_65"><head>Figure 23 .</head><label>23</label><figDesc>Figure 23. Results of the Sentinel-1 image. (a) P-R curves; (b) bar graph of mAP (accuracy) and FPS (speed).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_66"><head>Figure 23 .</head><label>23</label><figDesc>Figure 23. Results of the Sentinel-1 image. (a) P-R curves; (b) bar graph of mAP (accuracy) and FPS (speed).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_67"><head>Figure 24 .</head><label>24</label><figDesc>Figure 24. Replace DS-CNN in Figure 4 with C-CNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_68"><head>Figure 24 .</head><label>24</label><figDesc>Figure 24. Replace DS-CNN in Figure 4 with C-CNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_69"><head></head><label></label><figDesc>(a) L = 160; (b) Pretraining; (c) Two concatenations; (d) Three scales detection; (e) Nine anchor boxes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_70"><head>Figure 25 .Figure 25 .</head><label>2525</label><figDesc>Figure 25. Results of C-CNN and DS-CNN. (a) P-R curves; (b) bar graph of mAP (accuracy) and FPS (speed).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_71"><head>Figure 26 .</head><label>26</label><figDesc>Figure 26. Ship detection results of C-CNN and DS-CNN (a) Ground truth; (b) C-CNN; (c) DS-CNN.(Ground truth is marked in green, missed detection is marked in red, and false alarm is marked in yellow.).</figDesc><graphic coords="30,200.10,286.25,56.23,56.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_72"><head>Figure 26 .</head><label>26</label><figDesc>Figure 26. Ship detection results of C-CNN and DS-CNN (a) Ground truth; (b) C-CNN; (c) DS-CNN.(Ground truth is marked in green, missed detection is marked in red, and false alarm is marked in yellow.).</figDesc><graphic coords="30,270.02,286.25,56.23,56.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_73"><head>Figure 27 .</head><label>27</label><figDesc>Figure 27. P-R curves of different object detectors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_74"><head>FFigure 28 .</head><label>28</label><figDesc>Figure 28. Bar graph of mAP (accuracy) and FPS (speed) of different object detectors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_75"><head>Figure 27 .Figure 27 .</head><label>2727</label><figDesc>Figure 27. P-R curves of different object detectors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_76"><head>FFigure 28 .</head><label>28</label><figDesc>Figure 28. Bar graph of mAP (accuracy) and FPS (speed) of different object detectors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_77"><head>Figure 28 .</head><label>28</label><figDesc>Figure 28. Bar graph of mAP (accuracy) and FPS (speed) of different object detectors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_78"><head>Figure 29 .</head><label>29</label><figDesc>Figure 29. SAR ship detection results of different object detectors. (a) Faster R-CNN; (b) RetinaNet; (c) SSD; (d) YOLOv3; (e) YOLOv3-tiny; (f) YOLOv2; (g) YOLOv2-tiny; (h) YOLOv1; (i) Our method. (Missed detection is marked in red, and false alarm is marked in yellow).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_79"><head>Figure 29 .</head><label>29</label><figDesc>Figure 29. SAR ship detection results of different object detectors. (a) Faster R-CNN; (b) RetinaNet; (c) SSD; (d) YOLOv3; (e) YOLOv3-tiny; (f) YOLOv2; (g) YOLOv2-tiny; (h) YOLOv1; (i) Our method. (Missed detection is marked in red, and false alarm is marked in yellow).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_80"><head>Figure 30 .</head><label>30</label><figDesc>Figure<ref type="bibr" target="#b29">30</ref>. Detection speed comparison between our method and references under the similar hardware environment with NVIDIA GTX1080 GPU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_81"><head>Figure 30 .</head><label>30</label><figDesc>Figure<ref type="bibr" target="#b29">30</ref>. Detection speed comparison between our method and references under the similar hardware environment with NVIDIA GTX1080 GPU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>.3.</figDesc><table><row><cell>Remote Sens. 2019, 11, x FOR PEER REVIEW</cell><cell></cell><cell></cell><cell>10 of 38</cell></row><row><cell>L</cell><cell cols="3">Backbone Network</cell></row><row><cell></cell><cell cols="3">Detection Network-1</cell></row><row><cell>L/2</cell><cell cols="3">Detection Network-2</cell></row><row><cell></cell><cell cols="3">Detection Network-3</cell></row><row><cell>L/4</cell><cell></cell><cell></cell><cell></cell></row><row><cell>L/8</cell><cell>L/16 L/32</cell><cell>L/16</cell><cell>L/8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 .</head><label>1</label><figDesc>Size of anchor box in different detection networks.</figDesc><table><row><cell>Name</cell><cell>Scale</cell><cell>Anchor Boxes (Width, Height)</cell></row><row><cell>Detection network-1</cell><cell>L/32</cell><cell>(9,12), (12,25), (17,12)</cell></row><row><cell>Detection network-2</cell><cell>L/16</cell><cell>(21,45), (27,17), (36,64)</cell></row><row><cell>Detection network-3</cell><cell>L/8</cell><cell>(50,25), (59,115), (105,45)</cell></row><row><cell>4.3. Evaluation Index</cell><cell></cell><cell></cell></row><row><cell>4.3.1. Detection Accuracy</cell><cell></cell><cell></cell></row><row><cell>Precision is defined by:</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>Detailed descriptions of SSDD. H: Horizontal; V: Vertical.</figDesc><table><row><cell>Sensors</cell><cell>Sentinel-1, RadarSat-2, TerraSAR-X</cell></row><row><cell>Place</cell><cell>Visakhapatnam, India; Yantai, China</cell></row><row><cell>Polarization</cell><cell>HH, VV, HV, VH</cell></row><row><cell>Average size (pixel × pixel)</cell><cell>500 × 500</cell></row><row><cell>Resolution</cell><cell>1 m-10 m</cell></row><row><cell>Scene</cell><cell>Inshore, offshore</cell></row><row><cell>Number of Images</cell><cell>1160</cell></row><row><cell>Number of Ships</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>Evaluation indexes of research on image size.</figDesc><table><row><cell>of 32. Thus, we set:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>L</cell><cell cols="2">== 32 , k k</cell><cell>1,2, ,10</cell><cell>(26)</cell></row><row><cell>L</cell><cell>Precision</cell><cell cols="2">Recall</cell><cell></cell><cell>mAP</cell><cell>Time (ms)</cell><cell>FPS</cell></row><row><cell>32</cell><cell>75.63%</cell><cell cols="2">49.45%</cell><cell cols="2">45.54%</cell><cell>8.06</cell><cell>124</cell></row><row><cell>64</cell><cell>83.87%</cell><cell cols="2">85.71%</cell><cell cols="2">83.35%</cell><cell>8.55</cell><cell>117</cell></row><row><cell>96</cell><cell>80.49%</cell><cell cols="2">90.66%</cell><cell cols="2">88.29%</cell><cell>8.70</cell><cell>115</cell></row><row><cell>128</cell><cell>87.63%</cell><cell cols="2">93.41%</cell><cell cols="2">91.36%</cell><cell>8.90</cell><cell>112</cell></row><row><cell>160</cell><cell>87.94%</cell><cell cols="2">96.15%</cell><cell cols="2">94.13%</cell><cell>9.03</cell><cell>111</cell></row><row><cell>192</cell><cell>88.72%</cell><cell cols="2">95.05%</cell><cell cols="2">93.12%</cell><cell>9.37</cell><cell>107</cell></row><row><cell>224</cell><cell>88.21%</cell><cell cols="2">94.51%</cell><cell cols="2">92.55%</cell><cell>9.85</cell><cell>102</cell></row><row><cell>256</cell><cell>88.72%</cell><cell cols="2">95.05%</cell><cell cols="2">92.06%</cell><cell>10.37</cell><cell>96</cell></row><row><cell>288</cell><cell>85.57%</cell><cell cols="2">94.51%</cell><cell cols="2">91.96%</cell><cell>11.00</cell><cell>91</cell></row><row><cell>320</cell><cell>87.88%</cell><cell cols="2">95.60%</cell><cell cols="2">92.54%</cell><cell>11.76</cell><cell>85</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 .</head><label>3</label><figDesc>Evaluation indexes of research on image size.</figDesc><table><row><cell>L</cell><cell cols="2">Precision Recall</cell><cell>mAP</cell><cell cols="2">Time (ms) FPS</cell></row><row><cell>32</cell><cell>75.63%</cell><cell cols="2">49.45% 45.54%</cell><cell>8.06</cell><cell>124</cell></row><row><cell>64</cell><cell>83.87%</cell><cell cols="2">85.71% 83.35%</cell><cell>8.55</cell><cell>117</cell></row><row><cell>96</cell><cell>80.49%</cell><cell cols="2">90.66% 88.29%</cell><cell>8.70</cell><cell>115</cell></row><row><cell>128</cell><cell>87.63%</cell><cell cols="2">93.41% 91.36%</cell><cell>8.90</cell><cell>112</cell></row><row><cell>160</cell><cell>87.94%</cell><cell cols="2">96.15% 94.13%</cell><cell>9.03</cell><cell>111</cell></row><row><cell>192</cell><cell>88.72%</cell><cell cols="2">95.05% 93.12%</cell><cell>9.37</cell><cell>107</cell></row><row><cell>224</cell><cell>88.21%</cell><cell cols="2">94.51% 92.55%</cell><cell>9.85</cell><cell>102</cell></row><row><cell>256</cell><cell>88.72%</cell><cell cols="2">95.05% 92.06%</cell><cell>10.37</cell><cell>96</cell></row><row><cell>288</cell><cell>85.57%</cell><cell cols="2">94.51% 91.96%</cell><cell>11.00</cell><cell>91</cell></row><row><cell>320</cell><cell>87.88%</cell><cell cols="2">95.60% 92.54%</cell><cell>11.76</cell><cell>85</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 .</head><label>4</label><figDesc>Evaluation indexes of research on pretraining.</figDesc><table><row><cell>Pretraining?</cell><cell>Precision</cell><cell>Recall</cell><cell>mAP</cell><cell>Time (ms)</cell><cell>FPS</cell></row><row><cell>× √</cell><cell>84.58%</cell><cell>93.41%</cell><cell>90.14%</cell><cell>9.13</cell><cell>110</cell></row><row><cell></cell><cell>87.94%</cell><cell>96.15%</cell><cell>94.13%</cell><cell>9.03</cell><cell>111</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 .</head><label>4</label><figDesc>Evaluation indexes of research on pretraining.</figDesc><table><row><cell cols="3">Pretraining? Precision Recall</cell><cell>mAP</cell><cell cols="2">Time (ms) FPS</cell></row><row><cell>×</cell><cell>84.58%</cell><cell cols="2">93.41% 90.14%</cell><cell>9.13</cell><cell>110</cell></row><row><cell>√</cell><cell>87.94%</cell><cell cols="2">96.15% 94.13%</cell><cell>9.03</cell><cell>111</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 .</head><label>5</label><figDesc>Evaluation indexes of research on multi-scale detection mechanism.</figDesc><table><row><cell>L/32</cell><cell>L/16</cell><cell>L/8</cell><cell>Precision</cell><cell>Recall</cell><cell>mAP</cell><cell>Time (ms)</cell><cell>FPS</cell></row><row><cell>√</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>√</cell><cell></cell><cell>76.32%</cell><cell>47.80%</cell><cell>45.11%</cell><cell>7.15</cell><cell>140</cell></row><row><cell></cell><cell></cell><cell>√</cell><cell>76.67%</cell><cell>50.50%</cell><cell>48.21%</cell><cell>7.56</cell><cell>132</cell></row><row><cell>√</cell><cell>√</cell><cell></cell><cell>73.68%</cell><cell>46.15%</cell><cell>43.39%</cell><cell>7.96</cell><cell>126</cell></row><row><cell>√</cell><cell></cell><cell>√</cell><cell>85.47%</cell><cell>70.33%</cell><cell>67.21%</cell><cell>8.56</cell><cell>117</cell></row><row><cell></cell><cell>√</cell><cell>√</cell><cell>79.59%</cell><cell>85.71%</cell><cell>82.75%</cell><cell>8.87</cell><cell>113</cell></row><row><cell>√</cell><cell>√</cell><cell>√</cell><cell>84.47%</cell><cell>74.73%</cell><cell>71.23%</cell><cell>8.91</cell><cell>112</cell></row><row><cell></cell><cell></cell><cell></cell><cell>87.94%</cell><cell>96.15%</cell><cell>94.13%</cell><cell>9.03</cell><cell>111</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 .</head><label>5</label><figDesc>Evaluation indexes of research on multi-scale detection mechanism.</figDesc><table><row><cell></cell><cell></cell><cell cols="5">L/32 L/16 L/8 Precision Recall</cell><cell>mAP</cell><cell>Time (ms) FPS</cell></row><row><cell></cell><cell></cell><cell>√</cell><cell></cell><cell></cell><cell>76.32%</cell><cell cols="2">47.80% 45.11%</cell><cell>7.15</cell><cell>140</cell></row><row><cell></cell><cell></cell><cell></cell><cell>√</cell><cell></cell><cell>76.67%</cell><cell cols="2">50.50% 48.21%</cell><cell>7.56</cell><cell>132</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>√</cell><cell>73.68%</cell><cell cols="2">46.15% 43.39%</cell><cell>7.96</cell><cell>126</cell></row><row><cell></cell><cell></cell><cell>√</cell><cell>√</cell><cell></cell><cell>85.47%</cell><cell cols="2">70.33% 67.21%</cell><cell>8.56</cell><cell>117</cell></row><row><cell></cell><cell></cell><cell>√</cell><cell></cell><cell>√</cell><cell>79.59%</cell><cell cols="2">85.71% 82.75%</cell><cell>8.87</cell><cell>113</cell></row><row><cell></cell><cell></cell><cell></cell><cell>√</cell><cell>√</cell><cell>84.47%</cell><cell cols="2">74.73% 71.23%</cell><cell>8.91</cell><cell>112</cell></row><row><cell></cell><cell></cell><cell>√</cell><cell>√</cell><cell>√</cell><cell>87.94%</cell><cell cols="2">96.15% 94.13%</cell><cell>9.03</cell><cell>111</cell></row><row><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.6 Precision</cell><cell></cell><cell></cell><cell></cell><cell cols="2">L/32</cell><cell></cell></row><row><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell cols="2">L/16</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>L/8</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>L/</cell><cell></cell><cell></cell></row><row><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.0</cell><cell>0.2</cell><cell>0.4</cell><cell>Recall</cell><cell>0.6</cell><cell>0.8</cell><cell>1.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>32, L/16 L/32, L/8 L/16, L/8 L/32, L/16, L/8 (a)</head><label></label><figDesc></figDesc><table><row><cell>140</cell><cell></cell><cell></cell></row><row><cell></cell><cell>132</cell><cell></cell></row><row><cell></cell><cell></cell><cell>126</cell></row><row><cell></cell><cell></cell><cell>117</cell></row><row><cell></cell><cell></cell><cell>113</cell><cell>112</cell><cell>111</cell></row><row><cell>L /3 2</cell><cell>L /1 6</cell><cell>L /8</cell></row><row><cell></cell><cell></cell><cell>L /3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>2 , L /1 6 L /3 2 , L /8 L /1 6 , L /8 L /3 2 , L /1 6 , L /8</head><label></label><figDesc></figDesc><table><row><cell>100%</cell><cell></cell><cell>150</cell></row><row><cell></cell><cell></cell><cell>94.13%</cell></row><row><cell></cell><cell></cell><cell>mAP</cell></row><row><cell></cell><cell></cell><cell>FPS</cell></row><row><cell></cell><cell></cell><cell>82.75%</cell></row><row><cell>80%</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>71.23%</cell></row><row><cell></cell><cell></cell><cell>67.21%</cell></row><row><cell></cell><cell></cell><cell>100</cell></row><row><cell>60%</cell><cell></cell><cell></cell></row><row><cell>mAP</cell><cell>48.21%</cell><cell>43.39%</cell></row><row><cell>40%</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>50</cell></row><row><cell>20%</cell><cell></cell><cell></cell></row><row><cell>0%</cell><cell></cell><cell>0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 6 .</head><label>6</label><figDesc>Evaluation indexes of concatenation mechanism.</figDesc><table><row><cell cols="2">Concatenation? Precision</cell><cell>Recall</cell><cell>mAP</cell><cell>Time (ms)</cell><cell>FPS</cell></row><row><cell>× √</cell><cell>89.18%</cell><cell>95.05%</cell><cell>92.43%</cell><cell>8.93</cell><cell>112</cell></row><row><cell></cell><cell>87.94%</cell><cell>96.15%</cell><cell>94.13%</cell><cell>9.03</cell><cell>111</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 6 .</head><label>6</label><figDesc>Evaluation indexes of concatenation mechanism.</figDesc><table><row><cell></cell><cell></cell><cell cols="5">Concatenation? Precision Recall</cell><cell>mAP</cell><cell>Time (ms) FPS</cell></row><row><cell></cell><cell></cell><cell></cell><cell>×</cell><cell></cell><cell>89.18%</cell><cell cols="2">95.05% 92.43%</cell><cell>8.93</cell><cell>112</cell></row><row><cell></cell><cell></cell><cell></cell><cell>√</cell><cell></cell><cell>87.94%</cell><cell cols="2">96.15% 94.13%</cell><cell>9.03</cell><cell>111</cell></row><row><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.6 Precision</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Concatenation</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Non-concatenation</cell><cell></cell></row><row><cell>0.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.0</cell><cell>0.2</cell><cell>0.4</cell><cell>Recall</cell><cell>0.6</cell><cell>0.8</cell><cell>1.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 7 .</head><label>7</label><figDesc>Evaluation indexes of anchor box mechanism.</figDesc><table><row><cell>Number</cell><cell>Precision</cell><cell>Recall</cell><cell>mAP</cell><cell>Time (ms)</cell><cell>FPS</cell></row><row><cell>3</cell><cell>79.90%</cell><cell>89.56%</cell><cell>86.58%</cell><cell>8.80</cell><cell>114</cell></row><row><cell>6</cell><cell>85.86%</cell><cell>93.41%</cell><cell>90.92%</cell><cell>8.95</cell><cell>112</cell></row><row><cell>9</cell><cell>87.94%</cell><cell>96.15%</cell><cell>94.13%</cell><cell>9.03</cell><cell>111</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 7 .</head><label>7</label><figDesc>Evaluation indexes of anchor box mechanism.</figDesc><table><row><cell cols="3">Number Precision Recall</cell><cell>mAP</cell><cell cols="2">Time (ms) FPS</cell></row><row><cell>3</cell><cell>79.90%</cell><cell cols="2">89.56% 86.58%</cell><cell>8.80</cell><cell>114</cell></row><row><cell>6</cell><cell>85.86%</cell><cell cols="2">93.41% 90.92%</cell><cell>8.95</cell><cell>112</cell></row><row><cell>9</cell><cell>87.94%</cell><cell cols="2">96.15% 94.13%</cell><cell>9.03</cell><cell>111</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Precision Recall 3 anchor boxes 6 anchor boxes 9 anchor boxes</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 8 .</head><label>8</label><figDesc>Evaluation indexes of the test set of SSDD.</figDesc><table><row><cell cols="5">Ground Truth TP FP FN Precision Recall</cell><cell>mAP</cell><cell cols="2">Time (ms) FPS</cell></row><row><cell>182</cell><cell>175 24</cell><cell>7</cell><cell>87.94%</cell><cell cols="2">96.15% 94.13%</cell><cell>9.03</cell><cell>111</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 8 .</head><label>8</label><figDesc>Evaluation indexes of the test set of SSDD.</figDesc><table><row><cell>Ground Truth</cell><cell>TP</cell><cell>FP</cell><cell>FN</cell><cell>Precision</cell><cell>Recall</cell><cell>mAP</cell><cell>Time (ms)</cell><cell>FPS</cell></row><row><cell>182</cell><cell>175</cell><cell>24</cell><cell>7</cell><cell>87.94%</cell><cell>96.15%</cell><cell>94.13%</cell><cell>9.03</cell><cell>111</cell></row></table><note><p>Remote Sens. 2019, 11, x FOR PEER REVIEW 25 of 38</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 9 .</head><label>9</label><figDesc>Ship detection evaluation indexes of images with severe speckle noise and clear images.</figDesc><table><row><cell>Type</cell><cell>Precision</cell><cell>Recall</cell><cell>mAP</cell><cell>Time (ms)</cell><cell>FPS</cell></row><row><cell>Clear</cell><cell>88.68%</cell><cell>95.92%</cell><cell>93.63%</cell><cell>9.23</cell><cell>108</cell></row><row><cell>Noisy</cell><cell>85.00%</cell><cell>97.14%</cell><cell>96.91%</cell><cell>9.05</cell><cell>110</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 9 .</head><label>9</label><figDesc>Ship detection evaluation indexes of images with severe speckle noise and clear images.</figDesc><table><row><cell>Type</cell><cell cols="2">Precision Recall</cell><cell>mAP</cell><cell cols="2">Time (ms) FPS</cell></row><row><cell>Clear</cell><cell>88.68%</cell><cell cols="2">95.92% 93.63%</cell><cell>9.23</cell><cell>108</cell></row><row><cell>Noisy</cell><cell>85.00%</cell><cell cols="2">97.14% 96.91%</cell><cell>9.05</cell><cell>110</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 10 .</head><label>10</label><figDesc>Descriptions of the Sentinel-1 SAR image. Az.: Azimuth; Rg.: Range. H: Horizontal; V:</figDesc><table><row><cell>Vertical.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Cover Area</cell><cell>Resolution Az. × Rg.</cell><cell>Imaging Mode</cell><cell>Incident Angle</cell><cell>Image Size Pixel × Pixel</cell><cell>Band</cell><cell>Time</cell><cell>Polarization</cell></row><row><cell>Zhe Jiang, China</cell><cell>10m×10m</cell><cell>IW 1</cell><cell>34~46°</cell><cell>6333×4185</cell><cell>C</cell><cell>8 July 2015</cell><cell>VV, VH</cell></row><row><cell cols="8">1 The Interferometric Wide (IW) swath mode is the main acquisition mode over land and</cell></row><row><cell cols="8">satisfies the majority of service requirements [72]. It acquires data with a 250 km swath at 5 m by 20</cell></row><row><cell cols="8">m spatial resolution (single look) [72]. IW mode captures three sub-swaths using Terrain</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table 10 .</head><label>10</label><figDesc>Descriptions of the Sentinel-1 SAR image. Az.: Azimuth; Rg.: Range. H: Horizontal; V: Vertical.</figDesc><table><row><cell>Cover Area</cell><cell>Resolution Az. × Rg.</cell><cell>Imaging Mode</cell><cell>Incident Angle</cell><cell>Image Size Pixel × Pixel</cell><cell>Band</cell><cell>Time</cell><cell>Polarization</cell></row><row><cell cols="2">Zhe Jiang, China 10 m × 10 m</cell><cell>IW 1</cell><cell>34~46 •</cell><cell>6333 × 4185</cell><cell>C</cell><cell>8 July 2015</cell><cell>VV, VH</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head>Table 11 .</head><label>11</label><figDesc>Evaluation indexes of the ship detection results of the Sentinel-1 SAR image.</figDesc><table><row><cell>0.0 0.2 0.4 0.6 0.8 1.0 Precision</cell><cell>Precision Recall 91.15% 89.63% 90.36% mAP 0.0 0.2 0.4 0.6 Recall</cell><cell>Time of Preprocessing (s) 54.85×10 -3 0.8 1.0 mAP 20% Sub-image (s) Time of 8.92×10 -3 0% 40% 60% 80% 100%</cell><cell>FPS of Sub-image 116 Our method 90.36%</cell><cell>Time of Whole Image (s) 8.09 116 mAP FPS</cell><cell>0 50 100 150</cell></row><row><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head>Table 11 .</head><label>11</label><figDesc>Evaluation indexes of the ship detection results of the Sentinel-1 SAR image.</figDesc><table><row><cell>Precision</cell><cell>Recall</cell><cell>mAP</cell><cell>Time of Preprocessing (s</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_26"><head>) Time of Sub-Image (s) FPS of Sub-Image Time of Whole Image (s)</head><label></label><figDesc>method has strong migration ability which can be applied in practical SAR ship detection.</figDesc><table><row><cell>91.15%</cell><cell>89.63%</cell><cell>90.36%</cell><cell>54.85 × 10 -3</cell><cell>8.92 × 10 -3</cell><cell>116</cell><cell>8.09</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_27"><head>Table 12 .</head><label>12</label><figDesc>Evaluation indexes of C-CNN and DS-CNN.</figDesc><table><row><cell>Name</cell><cell cols="2">Precision Recall</cell><cell>mAP</cell><cell cols="2">Time (ms) FPS</cell></row><row><cell>C-CNN</cell><cell>89.29%</cell><cell cols="2">96.15% 94.39%</cell><cell>24.17</cell><cell>41</cell></row><row><cell>DS-CNN</cell><cell>87.94%</cell><cell cols="2">96.15% 94.13%</cell><cell>9.03</cell><cell>111</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_28"><head>Table 12 .</head><label>12</label><figDesc>Evaluation indexes of C-CNN and DS-CNN.</figDesc><table><row><cell>Name</cell><cell>Precision</cell><cell>Recall</cell><cell>mAP</cell><cell>Time (ms)</cell><cell>FPS</cell></row><row><cell>C-CNN</cell><cell>89.29%</cell><cell>96.15%</cell><cell>94.39%</cell><cell>24.17</cell><cell>41</cell></row><row><cell cols="2">DS-CNN Remote Sens. 2019, 11, x FOR PEER REVIEW 87.94%</cell><cell>96.15%</cell><cell>94.13%</cell><cell>9.03</cell><cell>111</cell><cell>29 of 38</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_29"><head>Table 13 .</head><label>13</label><figDesc>Network sizes of C-CNN and DS-CNN.</figDesc><table><row><cell>Method</cell><cell>Number of Parameters</cell><cell>Model File Size (KByte)</cell><cell>Weight File Size (KByte)</cell></row><row><cell>C-CNN</cell><cell>28,352,054</cell><cell>334,783</cell><cell>119,986</cell></row><row><cell>DS-CNN</cell><cell>3,299,862</cell><cell>38,965</cell><cell>13,159</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_30"><head>Table 14 .</head><label>14</label><figDesc>Evaluation indexes of different object detectors.</figDesc><table><row><cell>Method</cell><cell cols="2">Precision Recall</cell><cell>mAP</cell><cell cols="2">Time (ms) FPS</cell></row><row><cell>Faster R-CNN</cell><cell>81.15%</cell><cell cols="2">85.16% 82.66%</cell><cell>327.48</cell><cell>3</cell></row><row><cell>RetinaNet</cell><cell>93.12%</cell><cell cols="2">96.70% 95.68%</cell><cell>314.43</cell><cell>3</cell></row><row><cell>SSD</cell><cell>85.15%</cell><cell cols="2">94.51% 92.67%</cell><cell>48.86</cell><cell>20</cell></row><row><cell>YOLOv3</cell><cell>93.62%</cell><cell cols="2">96.70% 95.34%</cell><cell>22.30</cell><cell>45</cell></row><row><cell>YOLOv3-tiny</cell><cell>77.58%</cell><cell cols="2">70.33% 64.64%</cell><cell>10.25</cell><cell>98</cell></row><row><cell>YOLOv2</cell><cell>84.92%</cell><cell cols="2">92.86% 90.09%</cell><cell>19.01</cell><cell>53</cell></row><row><cell>YOLOv2-tiny</cell><cell>73.73%</cell><cell cols="2">47.80% 44.40%</cell><cell>9.43</cell><cell>107</cell></row><row><cell>YOLOv1</cell><cell>84.53%</cell><cell cols="2">84.07% 81.24%</cell><cell>21.95</cell><cell>46</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_31"><head>Table 14 .</head><label>14</label><figDesc>Evaluation indexes of different object detectors.</figDesc><table><row><cell>Method</cell><cell>Precision</cell><cell>Recall</cell><cell>mAP</cell><cell>Time (ms)</cell><cell>FPS</cell></row><row><cell>Faster R-CNN</cell><cell>81.15%</cell><cell>85.16%</cell><cell>82.66%</cell><cell>327.48</cell><cell>3</cell></row><row><cell>RetinaNet</cell><cell>93.12%</cell><cell>96.70%</cell><cell>95.68%</cell><cell>314.43</cell><cell>3</cell></row><row><cell>SSD</cell><cell>85.15%</cell><cell>94.51%</cell><cell>92.67%</cell><cell>48.86</cell><cell>20</cell></row><row><cell>YOLOv3</cell><cell>93.62%</cell><cell>96.70%</cell><cell>95.34%</cell><cell>22.30</cell><cell>45</cell></row><row><cell>YOLOv3-tiny</cell><cell>77.58%</cell><cell>70.33%</cell><cell>64.64%</cell><cell>10.25</cell><cell>98</cell></row><row><cell>YOLOv2</cell><cell>84.92%</cell><cell>92.86%</cell><cell>90.09%</cell><cell>19.01</cell><cell>53</cell></row><row><cell>YOLOv2-tiny</cell><cell>73.73%</cell><cell>47.80%</cell><cell>44.40%</cell><cell>9.43</cell><cell>107</cell></row><row><cell>YOLOv1</cell><cell>84.53%</cell><cell>84.07%</cell><cell>81.24%</cell><cell>21.95</cell><cell>46</cell></row><row><cell>Our method</cell><cell>87.94%</cell><cell>96.15%</cell><cell>94.13%</cell><cell>9.03</cell><cell>111</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_33"><head>Table 15 .</head><label>15</label><figDesc>Network sizes of different object detectors.</figDesc><table><row><cell>Method</cell><cell>Number of Parameters</cell><cell>Model File Size (KByte)</cell><cell>Weight File Size (KByte)</cell></row><row><cell>YOLOv1</cell><cell>272,746,867</cell><cell>2,308,877</cell><cell>770,814</cell></row><row><cell>YOLOv3</cell><cell>61,576,342</cell><cell>722,131</cell><cell>241,082</cell></row><row><cell>YOLOv2</cell><cell>50,578,686</cell><cell>592,777</cell><cell>197,668</cell></row><row><cell>RetinaNet</cell><cell>36,382,957</cell><cell>426,195</cell><cell>142,573</cell></row><row><cell>Faster R-CNN</cell><cell>28,342,195</cell><cell>310,112</cell><cell>111,144</cell></row><row><cell>SSD</cell><cell>23,745,908</cell><cell>278,550</cell><cell>92,904</cell></row><row><cell>YOLOv2-tiny</cell><cell>15,770,510</cell><cell>184,858</cell><cell>61,646</cell></row><row><cell>YOLOv3-tiny</cell><cell>8,676,244</cell><cell>101,799</cell><cell>33,990</cell></row><row><cell>Our method</cell><cell>3,299,862</cell><cell>38,965</cell><cell>13,159</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Remote Sens. 2019, 11, 2483; doi:10.3390/rs11212483 www.mdpi.com/journal/remotesensing</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>The Interferometric Wide (IW) swath mode is the main acquisition mode over land and satisfies the majority of service requirements[72]. It acquires data with a</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="250" xml:id="foot_2"><p>km swath at 5 m by 20 m spatial resolution (single look)[72]. IW mode captures three sub-swaths using Terrain Observation with Progressive Scans SAR (TOPSAR)<ref type="bibr" target="#b71">[73]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_3"><p>images in the SSDD dataset, while C-CNN needs 28.2 s. In general, even millisecond level time</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_4"><p>saving is meaningful and valuable in the field of image processing. In addition, for the wide-region</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_5"><p>large-size SAR image (6333×4185) in Figure22, the detection time of DS-CNN is 8.09 s, while that of</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments:</head><p>We thank anonymous reviewers for their comments towards improving this manuscript. The authors would also like to thank Durga Kumar for his linguistic assistance during the preparation of this manuscript.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Funding: This work was supported in part by the National Key R&amp;D Program of China under Grant 2017YFB0502700 and in part by the National Natural Science Foundation of China under Grants 61571099, 61501098, and 61671113.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Author Contributions: Conceptualization, T.Z.; methodology, T.Z.; software, T.Z.; validation, X.Z.; formal analysis, X.Z.; investigation, T.Z.; resources, T.Z.; data curation, X.Z.; writing-original draft preparation, T.Z.; writing-review &amp; editing, T.Z. and X.Z.; visualization, T.Z.; supervision, J.S. and S.W.; project administration, X.Z.; funding acquisition, X.Z.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflicts of Interest:</head><p>The authors declare no conflict of interest.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Seasat mission overview</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Born</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Dunne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Lame</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.204.4400.1405</idno>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">204</biblScope>
			<biblScope unit="page" from="1405" to="1406" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Synthetic aperture radar imaging of sea surface life and fishing activities</title>
		<author>
			<persName><forename type="first">M</forename><surname>Petit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Stretta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Farrugio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wadsworth</surname></persName>
		</author>
		<idno type="DOI">10.1109/36.175346</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1085" to="1089" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Wide-area traffic monitoring with the SAR/GMTI system pamir</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cerutti-Maori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Klare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Brenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Ender</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2008.923026</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="3019" to="3030" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Sea ice monitoring by L-band SAR: An assessment based on literature and comparisons of JERS-1 and ERS-1 imagery</title>
		<author>
			<persName><forename type="first">W</forename><surname>Dierking</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Busche</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2005.861745</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="957" to="970" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Automatic detection of oil spills in ERS SAR images</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H S</forename><surname>Solberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Storvik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Solberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Volden</surname></persName>
		</author>
		<idno type="DOI">10.1109/36.774704</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1916" to="1924" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Ship surveillance with TerraSAR-X</title>
		<author>
			<persName><forename type="first">S</forename><surname>Brusch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lehner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Soccorsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Soloviev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Schie</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2010.2071879</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="1092" to="1103" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Ship surveillance by integration of space-borne SAR and AIS-Review of current research</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.1017/S0373463313000659</idno>
	</analytic>
	<monogr>
		<title level="j">J. Navig</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="177" to="189" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Validation of ship detection by the RADARSAT synthetic aperture radar and the ocean monitoring workstation</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Vachon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cranton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Edel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Henschel</surname></persName>
		</author>
		<idno type="DOI">10.1080/07038992.2000.10874770</idno>
	</analytic>
	<monogr>
		<title level="j">Can. J. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="200" to="212" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Automatic detection of ships in RADARSAT-1 SAR imagery</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Wackerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">G</forename><surname>Pichel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Clemente-Colón</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1080/07038992.2001.10854896</idno>
	</analytic>
	<monogr>
		<title level="j">Can. J. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="568" to="577" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">High-Speed Ship Detection in SAR Images Based on a Grid Convolutional Neural Network</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs11101206</idno>
	</analytic>
	<monogr>
		<title level="j">Remote Sens</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<date type="published" when="1206">2019. 1206</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A novel SAR target detection algorithm via multi-scale SIFT features</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Northwest. Polytech. Univ</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="867" to="873" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Ship detection in South African oceans using SAR, CFAR and a Haar-like feature classifier</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Schwegmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Kleynhans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P</forename><surname>Salmon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 IEEE Geoscience and Remote Sensing Symposium</title>
		<meeting>the 2014 IEEE Geoscience and Remote Sensing Symposium<address><addrLine>Quebec City, QC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-07">July 2014</date>
			<biblScope unit="page" from="13" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Ship detection and extraction using visual saliency and bar graph of oriented gradient</title>
		<author>
			<persName><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11801-016-6179-y</idno>
	</analytic>
	<monogr>
		<title level="j">Optoelectron. Lett</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="473" to="477" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Object detection in SAR image based on bandlet transform</title>
		<author>
			<persName><forename type="first">N</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sethunadh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Aparna</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jvcir.2016.07.010</idno>
	</analytic>
	<monogr>
		<title level="j">J. Vis. Commun. Image Represent</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="376" to="383" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Performance analysis of order statistic constant false alarm rate (CFAR) detectors in generalized Rayleigh environment</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Blasch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SPIE; The International Society for Optical Engineering</title>
		<meeting>SPIE; The International Society for Optical Engineering<address><addrLine>Bellingham, WA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Radar detection prediction in sea clutter using the compound k-distribution model</title>
		<author>
			<persName><forename type="first">S</forename><surname>Watts</surname></persName>
		</author>
		<idno type="DOI">10.1049/ip-f-1.1985.0115</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Proc. Commun. Radar Signal Process</title>
		<imprint>
			<biblScope unit="volume">132</biblScope>
			<biblScope unit="page">613</biblScope>
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Optimal CFAR detection in weibull clutter</title>
		<author>
			<persName><forename type="first">V</forename><surname>Anastassopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Lampropoulos</surname></persName>
		</author>
		<idno type="DOI">10.1109/7.366292</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Aerosp. Electron. Syst</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="52" to="64" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A novel threshold template algorithm for ship detection in high-resolution SAR images</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 IEEE Geoscience &amp; Remote Sensing Symposium</title>
		<meeting>the 2016 IEEE Geoscience &amp; Remote Sensing Symposium<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-07">July 2016</date>
			<biblScope unit="page" from="10" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Projection shape template-based ship target recognition in TerraSAR-X images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lei</surname></persName>
		</author>
		<idno type="DOI">10.1109/LGRS.2016.2635699</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="222" to="226" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="DOI">10.1038/nature14539</idno>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fusion Light-Head Detector for SAR Ship Detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><surname>Multilayer</surname></persName>
		</author>
		<idno type="DOI">10.3390/s19051124</idno>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<date type="published" when="1124">2019. 1124</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Columbus, OH, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06">June 2014</date>
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E A V D</forename><surname>Sande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-013-0620-5</idno>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Support vector machine</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Adankon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cheriet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="28" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The Pascal Visual Object Classes Challenge: A Retrospective</title>
		<author>
			<persName><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-014-0733-5</idno>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R-Cnn</forename><surname>Fast</surname></persName>
		</author>
		<title level="m">Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the 2015 IEEE International Conference on Computer Vision (ICCV)<address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12">December 2015</date>
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2015.2389824</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1904" to="1916" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Towards Real-Time Object Detection with Region Proposal Networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R-Cnn</forename><surname>Faster</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2016.2577031</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A review of object detection based on convolutional neural network</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE 2017 36th Chinese Control Conference (CCC)</title>
		<meeting>the IEEE 2017 36th Chinese Control Conference (CCC)<address><addrLine>Dalian, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07">July 2017</date>
			<biblScope unit="page" from="26" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">You Only Look Once: Unified, Real-Time Object Detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06">June 2016</date>
			<biblScope unit="page" from="27" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">YOLO9000: Better, Faster, Stronger</title>
		<author>
			<persName><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07">July 2017</date>
			<biblScope unit="page" from="6517" to="6525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">YOLOv3: An incremental improvement</title>
		<author>
			<persName><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Single Shot MultiBox Detector</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><surname>Ssd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)<address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-10">October 2016</date>
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2018.2858826</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="page" from="2999" to="3007" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Ship detection in SAR images based on an improved faster R-CNN</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 SAR in Big Data Era: Models, Methods and Applications (BIGSARDATA)</title>
		<meeting>the 2017 SAR in Big Data Era: Models, Methods and Applications (BIGSARDATA)<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-11">November 2017</date>
			<biblScope unit="page" from="13" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<idno type="DOI">10.1109/5.726791</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 1998</title>
		<meeting>IEEE 1998</meeting>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Continuous speech recognition by convolutional neural networks</title>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chin. J. Eng</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="85" to="95" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">ImageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep learning for natural language processing: Advantages and challenges</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1093/nsr/nwx110</idno>
	</analytic>
	<monogr>
		<title level="j">Natl. Sci. Rev</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="24" to="26" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">ImageNet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-015-0816-y</idno>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Receptive fields of single neurones in the cat&apos;s striate cortex</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Hubel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Wiesel</surname></persName>
		</author>
		<idno type="DOI">10.1113/jphysiol.1959.sp006308</idno>
	</analytic>
	<monogr>
		<title level="j">J. Physiol</title>
		<imprint>
			<biblScope unit="volume">148</biblScope>
			<date type="published" when="1959">1959</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Going Deeper with Convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06">June 2015</date>
			<biblScope unit="page" from="7" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><surname>Mobilenets</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Efficient Convolutional Neural Networks for Mobile Vision Applications</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Rigid-Motion Scattering for Image Classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<pubPlace>Palaiseau, France</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Ecole Polytechnique</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. Thesis</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02357v2</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">D M</forename><surname>Laurens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.06993</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on International Conference on Machine Learning</title>
		<meeting>the International Conference on International Conference on Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07">July 2015</date>
			<biblScope unit="page" from="6" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Empirical evaluation of rectified activations in convolutional network</title>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00853</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning Non-maximum Suppression</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 2017 IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07">July 2017</date>
			<biblScope unit="page" from="6469" to="6477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title/>
		<author>
			<persName><surname>Pycharm</surname></persName>
		</author>
		<ptr target="http://www.jetbrains.com/pycharm/" />
		<imprint>
			<date type="published" when="2019-09-06">6 September 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Understanding and Working with Keras</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Manaswi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning with Applications Using Python</title>
		<meeting><address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Apress</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Tensorflow: Large-scale Machine Learning on Heterogeneous Systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<ptr target="https://www.tensorflow.org" />
		<imprint>
			<date type="published" when="2015-10-24">2015. 24 October 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title/>
		<author>
			<persName><surname>Labelimg</surname></persName>
		</author>
		<ptr target="https://github.com/tzutalin/labelImg" />
		<imprint>
			<date type="published" when="2019-09-06">6 September 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Dense Attention Pyramid Networks for Multi-Scale Ship Detection in SAR Images</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2019.2923988</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A densely connected end-to-end neural network for multiscale and multiscene sar ship detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Xun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/ACCESS.2018.2825376</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="20881" to="20892" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Multi-Scale Proposal Generation for Ship Detection in SAR Images</title>
		<author>
			<persName><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dang</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs11050526</idno>
		<imprint/>
	</monogr>
	<note>Remote Sens. 2019, 11, 526. [CrossRef</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Ship Detection Based on YOLOv2 for SAR Imagery</title>
		<author>
			<persName><forename type="first">Y.-L</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anagaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Hsiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-H</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs11070786</idno>
		<imprint/>
	</monogr>
	<note>Remote Sens. 2019, 11, 786. [CrossRef</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Ship Detection in SAR images Based on Generative Adversarial Network and Online Hard Examples Mining</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Electron. Inf. Technol</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="143" to="149" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Ship detection in SAR images based on convolutional neural network</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Syst. Eng. Electron</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1953" to="1959" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Combining a single shot multibox detector with transfer learning for ship detection using sentinel-1 SAR images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hong</surname></persName>
		</author>
		<idno type="DOI">10.1080/2150704X.2018.1475770</idno>
	</analytic>
	<monogr>
		<title level="j">Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="780" to="788" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Simultaneous Ship Detection and Orientation Estimation in SAR Images Based on Attention Module and Angle Regression</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="DOI">10.3390/s18092851</idno>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<date type="published" when="2018">2018. 2851</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">A Deep Neural Network Based on an Attention Mechanism for SAR Ship Detection in Multiscale and Complex Scenarios</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
		<idno type="DOI">10.1109/ACCESS.2019.2930939</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="104848" to="104863" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">A Method for Stochastic Optimization. arXiv</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">On early stopping in gradient descent learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Caponnetto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Constr. Approx</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="289" to="315" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Rethinking ImageNet pre-training</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08883</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Learning Deep Ship Detector in SAR Images from Scratch</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2018.2889353</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="4021" to="4039" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Visualizing and Understanding Convolutional Networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1311.2901v3</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title/>
		<author>
			<persName><surname>Opensar</surname></persName>
		</author>
		<ptr target="http://opensar.sjtu.edu.cn/" />
		<imprint>
			<date type="published" when="2019-09-06">6 September 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">A dataset dedicated to sentinel-1 ship interpretation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><surname>Opensarship</surname></persName>
		</author>
		<idno type="DOI">10.1109/JSTARS.2017.2755672</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="195" to="208" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Terrain observation by progressive scans</title>
		<author>
			<persName><forename type="first">F</forename><surname>De Zan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Guarnieri</surname></persName>
		</author>
		<author>
			<persName><surname>Topsar</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2006.873853</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="2352" to="2360" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
