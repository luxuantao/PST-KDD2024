<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">UNIFORMER: UNIFIED TRANSFORMER FOR EFFICIENT SPATIOTEMPORAL REPRESENTATION LEARNING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-01-12">12 Jan 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kunchang</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yali</forename><surname>Wang</surname></persName>
							<email>yl.wang@siat.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Peng</forename><surname>Gao</surname></persName>
							<email>gaopeng@pjlab.org.cn</email>
							<affiliation key="aff2">
								<orgName type="department">Shanghai AI Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guanglu</forename><surname>Song</surname></persName>
							<email>songguanglu@sensetime.com</email>
							<affiliation key="aff3">
								<orgName type="institution">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yu</forename><surname>Liu</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
							<email>hsli@ee.cuhk.edu.hk</email>
							<affiliation key="aff4">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
							<email>qiaoyu@pjlab.org.cn</email>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">UNIFORMER: UNIFIED TRANSFORMER FOR EFFICIENT SPATIOTEMPORAL REPRESENTATION LEARNING</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-01-12">12 Jan 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2201.04676v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>It is a challenging task to learn rich and multi-scale spatiotemporal semantics from high-dimensional videos, due to large local redundancy and complex global dependency between video frames. The recent advances in this research have been mainly driven by 3D convolutional neural networks and vision transformers. Although 3D convolution can efficiently aggregate local context to suppress local redundancy from a small 3D neighborhood, it lacks the capability to capture global dependency because of the limited receptive field. Alternatively, vision transformers can effectively capture long-range dependency by self-attention mechanism, while having the limitation on reducing local redundancy with blind similarity comparison among all the tokens in each layer. Based on these observations, we propose a novel Unified transFormer (UniFormer) which seamlessly integrates merits of 3D convolution and spatiotemporal self-attention in a concise transformer format, and achieves a preferable balance between computation and accuracy. Different from traditional transformers, our relation aggregator can tackle both spatiotemporal redundancy and dependency, by learning local and global token affinity respectively in shallow and deep layers. We conduct extensive experiments on the popular video benchmarks, e.g., Kinetics-400, Kinetics-600, and Something-Something V1&amp;V2. With only ImageNet-1K pretraining, our UniFormer achieves 82.9%/84.8% top-1 accuracy on Kinetics-400/Kinetics-600, while requiring 10× fewer GFLOPs than other state-of-the-art methods. For Something-Something V1 and V2, our UniFormer achieves new state-of-the-art performances of 60.9% and 71.2% top-1 accuracy respectively. Code is available at https://github.com/Sense-X/UniFormer.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Learning spatiotemporal representations is a fundamental task for video understanding. Basically, there are two distinct challenges. On the one hand, videos contain large spatiotemporal redundancy, where target motions across local neighboring frames are subtle. On the other hand, videos contain complex spatiotemporal dependency, since target relations across long-range frames are dynamic.</p><p>The advances in video classification have mostly driven by 3D convolutional neural networks <ref type="bibr" target="#b51">(Tran et al., 2015;</ref><ref type="bibr" target="#b6">Carreira &amp; Zisserman, 2017b;</ref><ref type="bibr" target="#b15">Feichtenhofer et al., 2019)</ref> and spatiotemporal transformers <ref type="bibr">(Bertasius et al., 2021;</ref><ref type="bibr" target="#b0">Arnab et al., 2021)</ref>. Unfortunately, each of these two frameworks focuses on one of the aforementioned challenges. 3D convolution can capture detailed and local spatiotemporal features, by processing each pixel with context from a small 3D neighborhood (e.g., 3×3×3). Hence, it can reduce spatiotemporal redundancy across adjacent frames. However, due to the limited receptive field, 3D convolution suffers from difficulty in learning long-range dependency <ref type="bibr">(Wang Preprint</ref> Figure <ref type="figure">1</ref>: Some visualizations of TimeSformer. We respectively show the feature, spatial and temporal attention from the 3rd layer of TimeSformer <ref type="bibr">(Bertasius et al., 2021)</ref>. We find that, such transformer learns local representations with redundant global attention. For an anchor token (green box), spatial/temporal attention compares it with all the contextual tokens for aggregation, while only its neighboring tokens (boxes filled with red color) actually work. Hence, it wastes large computation to encode only very local spatiotemporal representations.  <ref type="bibr">et al., 2018;</ref><ref type="bibr" target="#b27">Li et al., 2020a)</ref>. Alternatively, vision transformers are good at capturing global dependency, with the help of self-attention among visual tokens <ref type="bibr" target="#b12">(Dosovitskiy et al., 2021)</ref>. Recently, this design has been introduced in video classification via spatiotemporal attention mechanism <ref type="bibr">(Bertasius et al., 2021)</ref>. However, we observe that, video transformers are often inefficient to encode local spatiotemporal features in the shallow layers. We take the well-known and typical TimeSformer <ref type="bibr">(Bertasius et al., 2021)</ref> for illustration. As shown in Figure <ref type="figure">1</ref>, TimeSformer indeed learns detailed video representations in the early layers, but with very redundant spatial and temporal attention. Specifically, spatial attention mainly focuses on the neighbor tokens (mostly in 3×3 local regions), while learning nothing from the rest tokens in the same frame. Similarly, temporal attention mostly only aggregates tokens in the adjacent frames, while ignoring the rest in the distant frames. More importantly, such local representations are learned from global token-to-token similarity comparison in all layers, requiring large computation cost. This fact clearly deteriorates computation-accuracy balance of such video transformer (Figure <ref type="figure" target="#fig_0">2</ref>).</p><p>To tackle these difficulties, we propose to effectively unify 3D convolution and spatiotemporal selfattention in a concise transformer format, thus we name the network Unified transFormer (Uni-Former), which can achieve a preferable balance between efficiency and effectiveness. More specif-Preprint ically, our UniFormer consists of three core modules, i.e., Dynamic Position Embedding (DPE), Multi-Head Relation Aggregator (MHRA), and Feed-Forward Network (FFN). The key difference between our UniFormer and traditional video transformers is the distinct design of our relation aggregator. First, instead of utilizing a self-attention mechanism in all layers, our proposed relation aggregator tackles video redundancy and dependency respectively. In the shallow layers, our aggregator learns local relation with a small learnable parameter matrix, which can largely reduce computation burden by aggregating context from adjacent tokens in a small 3D neighborhood. In the deep layers, our aggregator learns global relation with similarity comparison, which can flexibly build long-range token dependencies from distant frames in the video. Second, different from spatial and temporal attention separation in the traditional transformers <ref type="bibr">(Bertasius et al., 2021;</ref><ref type="bibr" target="#b0">Arnab et al., 2021)</ref>, our relation aggregator jointly encodes spatiotemporal context in all the layers, which can further boost video representations in a joint learning manner. Finally, we build up our model by progressively integrating UniFormer blocks in a hierarchical manner. In this case, we enlarge the cooperative power of local and global UniFormer blocks for efficient spatiotemporal representation learning in videos. We conduct extensive experiments on the popular video benchmarks, e.g., Kineticss-400 <ref type="bibr" target="#b4">(Carreira &amp; Zisserman, 2017a)</ref>, Kinetics-600 <ref type="bibr" target="#b5">(Carreira et al., 2018)</ref> and Something-Something V1&amp;V2 <ref type="bibr" target="#b18">(Goyal et al., 2017b</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Convolution-based Video Networks. 3D Convolution Neural Networks (CNNs) have been dominant in video understanding <ref type="bibr" target="#b51">(Tran et al., 2015;</ref><ref type="bibr" target="#b15">Feichtenhofer et al., 2019)</ref>. However, they suffer from the difficult optimization problem and large computation cost. To resolve this issue, I3D (Carreira &amp; Zisserman, 2017b) inflates the pre-trained 2D convolution kernels for better optimization. Other prior works <ref type="bibr" target="#b52">(Tran et al., 2018;</ref><ref type="bibr" target="#b39">Qiu et al., 2017;</ref><ref type="bibr" target="#b53">Tran et al., 2019;</ref><ref type="bibr" target="#b14">Feichtenhofer, 2020;</ref><ref type="bibr" target="#b55">Wang et al., 2020a)</ref> try to factorize 3D convolution kernel in different dimensions to reduce complexity.</p><p>Recent methods propose well-designed modules to enhance the temporal modeling ability for 2D CNNs <ref type="bibr" target="#b56">(Wang et al., 2016;</ref><ref type="bibr" target="#b30">Lin et al., 2019;</ref><ref type="bibr" target="#b36">Luo &amp; Yuille, 2019;</ref><ref type="bibr" target="#b22">Jiang et al., 2019;</ref><ref type="bibr" target="#b33">Liu et al., 2020a;</ref><ref type="bibr" target="#b29">Li et al., 2020b;</ref><ref type="bibr" target="#b25">Kwon et al., 2020;</ref><ref type="bibr" target="#b27">Li et al., 2020a;</ref><ref type="bibr">2021a;</ref><ref type="bibr" target="#b57">Wang et al., 2020b)</ref>. However, 3D convolution struggles to capture long-range dependency, due to the limited receptive field.</p><p>Transformer-based Video Networks. Vision Transformers <ref type="bibr" target="#b12">(Dosovitskiy et al., 2021;</ref><ref type="bibr" target="#b49">Touvron et al., 2021a;</ref><ref type="bibr">b;</ref><ref type="bibr" target="#b31">Liu et al., 2021a)</ref> have been popular for vision tasks and outperform many CNNs.</p><p>Based on ViT, several prior works <ref type="bibr">(Bertasius et al., 2021;</ref><ref type="bibr" target="#b37">Neimark et al., 2021;</ref><ref type="bibr" target="#b45">Sharir et al., 2021;</ref><ref type="bibr" target="#b28">Li et al., 2021b;</ref><ref type="bibr" target="#b0">Arnab et al., 2021;</ref><ref type="bibr" target="#b3">Bulat et al., 2021;</ref><ref type="bibr" target="#b38">Patrick et al., 2021;</ref><ref type="bibr" target="#b63">Zha et al., 2021)</ref> propose different variants for spatiotemporal learning, verifying the outstanding ability of the transformer to capture long-term dependencies. To reduce high dot-product computation, <ref type="bibr">MViT (Fan et al., 2021)</ref> introduces the hierarchical structure and pooling self-attention, while Video Swin <ref type="bibr" target="#b32">(Liu et al., 2021b)</ref> advocates an inductive bias of locality for video. Nevertheless, the self-attention mechanism is inefficient to encode low-level features, hindering their high potential. To tackle this challenge, different from Video Swin that applies self-attention in a local 3D window, we adopt 3D convolution in a concise transformer format to encode local features. Besides, we follow their hierarchical designs and propose our UniFormer, achieving powerful performance for video understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head><p>In this section, we describe our UniFormer in detail. First, we introduce the overall architecture of the UniFormer block. Then, we explain the vital designs of our UniFormer for spatiotemporal modeling, i.e., multi-head relation aggregator and dynamic position embedding. Finally, we hierarchically stack UniFormer blocks to build up our video network.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">OVERVIEW OF UNIFORMER BLOCK</head><p>To overcome problems of spatiotemporal redundancy and dependency, we propose a novel and concise Unified transFormer (UniFormer) shown in Figure <ref type="figure" target="#fig_1">3</ref>. We utilize a basic transformer format <ref type="bibr" target="#b54">(Vaswani et al., 2017)</ref> but specially design it for efficient and effective spatiotemporal representation learning. Specifically, our UniFormer block consists of three key modules: Dynamic Position Embedding (DPE), Multi-Head Relation Aggregator (MHRA), and Feed-Forward Network (FFN):</p><formula xml:id="formula_0">X = DPE (X in ) + X in ,<label>(1)</label></formula><formula xml:id="formula_1">Y = MHRA (Norm (X)) + X,<label>(2)</label></formula><formula xml:id="formula_2">Z = FFN (Norm (Y)) + Y.</formula><p>(3)</p><p>Considering the input token tensor (frame volumes) X in ∈ R C×T ×H×W , we first introduce DPE to dynamically integrate 3D position information into all the tokens (Eq. 1), which effectively makes use of spatiotemporal order of the tokens for video modeling. Then, we leverage MHRA to aggregate each token with its contextual tokens (Eq. 2). Different from the regular Multi-Head Self-Attention (MHSA), our MHRA smartly tackles local video redundancy and global video dependency, by flexible designs of token affinity learning in the shallow and deep layers. Finally, we add FFN with two linear layers for pointwise enhancement of each token (Eq. 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">MULTI-HEAD RELATION AGGREGATOR</head><p>As discussed above, we should solve large local redundancy and complex global dependency, for efficient and effective spatiotemporal representation learning. Unfortunately, the popular 3D CNNs and spatiotemporal transformers only focus on one of these two challenges. For this reason, we design an alternative Relation Aggregator (RA), which can flexibly unify 3D convolution and spatiotemporal self-attention in a concise transformer format, solving video redundancy and dependency in the shallow layers and deep layers respectively. Specifically, our MHRA conducts token relation learning via multi-head fusion:</p><formula xml:id="formula_3">R n (X) = A n V n (X), (4) MHRA(X) = Concat(R 1 (X); R 2 (X); • • • ; R N (X))U.</formula><p>(5)</p><p>Given the input tensor X ∈ R C×T ×H×W , we first reshape it to a sequence of tokens X ∈ R L×C , L=T ×H×W . R n (•) is the relation aggregator (RA) in the n-th head, and U ∈ R C×C is a learnable parameter matrix to integrate N heads. Moreover, each RA consists of token context encoding and token affinity learning. Via a linear transformation, one can transform the original token into context</p><formula xml:id="formula_4">V n (X) ∈ R L× C N .</formula><p>Subsequently, the relation aggregator can summarize context with the guidance of the token affinity A n ∈ R L×L . The key in our RA is how to learn A n in videos.</p><p>Local MHRA. In the shallow layers, we aim at learning detailed video representation from the local spatiotemporal context in small 3D neighborhoods. This coincidentally shares a similar insight with the design of a 3D convolution filter. As a result, we design the token affinity as a learnable parameter matrix operated in the local 3D neighborhood, i.e., given one anchor token X i , RA learns local spatiotemporal affinity between this token and other tokens in the small tube Ω t×h×w i :</p><formula xml:id="formula_5">A local n (X i , X j ) = a i−j n , where j ∈ Ω t×h×w i ,<label>(6)</label></formula><p>where a n ∈ R t×h×w and X j refers to any neighbor token in Ω t×h×w i . (i − j) means the relative token index that determines the aggregating weight (Appendix A shows more details). In the shallow layers, video contents between adjacent tokens vary subtly, it is significant to encode detailed features with local operator to reduce redundancy. Hence, the token affinity is designed as a local learnable parameter matrix, whose values only depend on the relative 3D position between tokens.</p><p>Comparison to 3D Convolution Block. Interestingly, we find that our local MHRA can be interpreted as a spatiotemporal extension of MobileNet block <ref type="bibr" target="#b43">(Sandler et al., 2018;</ref><ref type="bibr" target="#b53">Tran et al., 2019;</ref><ref type="bibr" target="#b14">Feichtenhofer, 2020)</ref>. Specifically, the linear transformation V(•) can be instantiated as pointwise convolution (PWConv). Furthermore, the local token affinity A local n is a spatiotemporal matrix that operated on each output channel (or head) V n (X), thus the relation aggregator</p><formula xml:id="formula_6">R n (X) = A local n V n (X)</formula><p>can be explained as a depthwise convolution (DWConv). Finally, all heads are concatenated and fused by a linear matrix U, which can also be instantiated as pointwise convolution (PWConv). As a result, this local MHRA can be reformulated with a manner of PWConv-DWConv-PWConv in the MobileNet block. In our experiments, we flexibly instantiate our local MHRA as such channelseparated spatiotemporal convolution, so that our UniFormer can inherit computation efficiency for light-weight video classification. Different from the MobileNet block, our UniFormer block is designed as a generic transformer format, thus an extra FFN is inserted after MHRA, which can further mix token context at each spatiotemporal position to boost classification accuracy.</p><p>Global MHRA. In the deep layers, we focus on capturing long-term token dependency in the global video clip. This naturally shares a similar insight with the design of self-attention. Hence, we design the token affinity via comparing content similarity among all the tokens in the global view:</p><formula xml:id="formula_7">A global n (X i , X j ) = e Qn(Xi) T Kn(Xj ) j ∈Ω T ×H×W e Qn(Xi) T Kn(X j ) ,<label>(7)</label></formula><p>where X j can be any token in the global 3D tube with size of T ×H×W , while Q n (•) and K n (•) are two different linear transformations. Most video transformers apply self-attention in all stages, introducing a large amount of calculation. To reduce the dot-product computation, the prior works tend to divide spatial and temporal attention <ref type="bibr">(Bertasius et al., 2021;</ref><ref type="bibr" target="#b0">Arnab et al., 2021)</ref>, but it deteriorates the spatiotemporal relation among tokens. In contrast, our MHRA performs local relation aggregation in the early layers, which largely saves the computation of token comparison. Hence, instead of factorizing spatiotemporal attention, we jointly encode spatiotemporal relation in our MHRA for all the stages, in order to achieve a preferable computation-accuracy balance.</p><p>Comparison to Transformer Block. In the deep layers, our UniFormer block is equipped with a global MHRA A global n (Eq. 7). It can be instantiated as a spatiotemporal self attention, where Q n (•), K n (•) and V n (•) become Query, Key and Value in the transformer <ref type="bibr" target="#b12">(Dosovitskiy et al., 2021)</ref>. Hence, it can effectively learn long-term dependency. Instead of spatial and temporal factorization in the previous video transformers <ref type="bibr">(Bertasius et al., 2021;</ref><ref type="bibr" target="#b0">Arnab et al., 2021)</ref> Table <ref type="table">2</ref>: Comparison with the state-of-the-art on Kinetics-400&amp;600. Our UniFormer outperforms most of the current methods with much fewer computation cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">DYNAMIC POSITION EMBEDDING</head><p>Since videos are both spatial and temporal variant, it is necessary to encode spatiotemporal position information for token representations. The previous methods mainly adapt the absolute or relative position embedding of image tasks to tackle this problem <ref type="bibr">(Bertasius et al., 2021;</ref><ref type="bibr" target="#b0">Arnab et al., 2021)</ref>. However, when testing with longer input clips, the absolute one should be interpolated to target input size with fine-tuning. Besides, the relative version modifies the self-attention and performs worse due to lack of absolute position information <ref type="bibr" target="#b21">(Islam et al., 2020)</ref>. To overcome the above problems, we extend the conditional position encoding (CPE) <ref type="bibr" target="#b7">(Chu et al., 2021)</ref> to design our DPE:</p><formula xml:id="formula_8">DPE(X in ) = DWConv(X in ),<label>(8)</label></formula><p>where DWConv means simple 3D depthwise convolution with zero paddings. Thanks to the shared parameters and locality of convolution, DPE can overcome permutation-invariance and is friendly to arbitrary input lengths. Moreover, it has been proven in CPE that zero paddings help the tokens on the borders be aware of their absolute positions, thus all tokens can progressively encode their absolute spatiotemporal position information via querying their neighbor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">MODEL ARCHITECTURE</head><p>We hierarchically stack UniFormer blocks to build up our network for spatiotemporal learning. As shown in Figure <ref type="figure" target="#fig_1">3</ref>, our network consists of four stages, the channel numbers of which are 64, 128, 256 and 512 respectively. We provide two model variants depending on the number of UniFormer blocks in these stages: {3, 4, 8, 3} for UniFormer-S and {5, 8, 20, 7} for UniFormer-B. In the first two stages, we utilize MHRA with local token affinity (Eq. 6) to reduce the short-term spatiotemporal redundancy. The tube size is set to 5×5×5 and the head number N is equal to the corresponding channel number. In the last two stages, we apply MHRA with global token affinity (Eq. 7) to capture long-term dependency, the head dimension of which is 64. We utilize BN <ref type="bibr" target="#b20">(Ioffe &amp; Szegedy, 2015)</ref> for local MHRA and LN <ref type="bibr" target="#b1">(Ba et al., 2016)</ref>  Table <ref type="table">3</ref>: Comparison with the state-of-the-art on Something-Something V1&amp;V2. Our Uni-Former achieves new state-of-the-art performances on both datasets.</p><p>Comparison to Convolution+Transformer Network. The prior works have demonstrate that selfattention can perform convolution <ref type="bibr" target="#b42">(Ramachandran et al., 2019;</ref><ref type="bibr" target="#b8">Cordonnier et al., 2020)</ref>, but they propose to replace convolution instead of combining them. Recent works have attempted to introduce convolution to vision transformers <ref type="bibr" target="#b61">(Wu et al., 2021;</ref><ref type="bibr" target="#b9">Dai et al., 2021;</ref><ref type="bibr" target="#b16">Gao et al., 2021;</ref><ref type="bibr" target="#b46">Srinivas et al., 2021)</ref>, but they mainly focus on image recognition, without any spatiotemporal consideration for video understanding. Moreover, the combination is almost straightforward in the prior video transformers, e.g., using transformer as global attention <ref type="bibr" target="#b60">(Wang et al., 2018)</ref> or using convolution as patch stem <ref type="bibr" target="#b34">(Liu et al., 2020b)</ref>. In contrast, our UniFormer tackles both video redundancy and dependency with an insightful unified framework (Table <ref type="table" target="#tab_2">1</ref>). Via local and global token affinity learning, we can achieve a preferable computation-accuracy balance for video classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">DATASETS AND EXPERIMENTAL SETUP</head><p>We conduct experiments on widely-used Kinetics-400 <ref type="bibr" target="#b4">(Carreira &amp; Zisserman, 2017a</ref>) and larger benchmark Kinetics-600 <ref type="bibr" target="#b5">(Carreira et al., 2018)</ref>. We further verify the transfer learning performance on temporal-related datasets Something-Something V1&amp;V2 <ref type="bibr" target="#b18">(Goyal et al., 2017b)</ref>. For training, we utilize the dense sampling strategy <ref type="bibr" target="#b60">(Wang et al., 2018)</ref> for Kinetics and uniform sampling strategy <ref type="bibr" target="#b56">(Wang et al., 2016)</ref> for Something-Something. We adopt the same training recipe as <ref type="bibr">MViT (Fan et al., 2021)</ref> by default, but the random horizontal flip is not applied for Something-Something. To reduce the total training cost, we inflate the 2D convolution kernels pre-trained on ImageNet for Kinetics <ref type="bibr" target="#b6">(Carreira &amp; Zisserman, 2017b)</ref>. More implementation specifics are shown in Appendix C. For testing, we explore the sampling strategies in our experiments. To obtain a preferable computationaccuracy balance, we adopt multi-clip testing for Kinetics and multi-crop testing for Something-Something. All scores are averaged for the final prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">COMPARISON TO STATE-OF-THE-ART</head><p>Kinetics-400&amp;600.  Something-Something V1&amp;V2. Results on Something-Something V1&amp;V2 are shown in Table <ref type="table">3</ref>. Since these datasets depend on temporal relation modeling, it is difficult for the CNN-based methods to capture long-term dependencies, which leads to their worse results. In contrast, transformerbased backbones are good at processing long sequential data and demonstrate better transfer learning capabilities <ref type="bibr" target="#b64">(Zhou et al., 2021)</ref>. Our UniFormer pre-trained from Kinetis-600 outperforms all the current methods under the same settings. In fact, our best model achieves the new state-of-the-art results: 61.0% top-1 accuracy on Something-Something V1 (4.2% higher than TDN EN ) <ref type="bibr" target="#b57">(Wang et al., 2020b)</ref> and 71.2% top-1 accuracy on Something-Something V2 (1.6% higher than Swin-B <ref type="bibr" target="#b32">(Liu et al., 2021b)</ref>). Such results verify the capability of spatiotemporal learning for UniFormer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">ABLATION STUDIES</head><p>UniFormer vs. Convolution: Does transformer-style FFN help? As mentioned in Section 3.2, our UniFormer block in the shallow layers can be interpreted as a transformer-style spatiotemporal MobileNet block <ref type="bibr" target="#b53">(Tran et al., 2019)</ref> with extra FFN. Hence, we first investigate its effectiveness by replacing our UniFormer blocks in shallow layers with MobileNet blocks (the expand ratios are set to 3 for similar parameters). As expected, our default UniFormer outperforms such spatiotemporal MobileNet block in Table <ref type="table" target="#tab_6">4a</ref>. It shows that, FFN in our UniFormer can further mix token context at each spatiotemporal position to boost classification accuracy.</p><p>UniFormer vs. Transformer: Is joint or divided spatiotemporal attention better? As discussed in Section 3.2, our UniFormer block in the deep layers can be interpreted as a transformer block, but our attention is jointly learned in a spatiotemporal manner, instead of dividing spatial and temporal attention <ref type="bibr">(Bertasius et al., 2021;</ref><ref type="bibr" target="#b0">Arnab et al., 2021)</ref>. As shown in Table <ref type="table" target="#tab_6">4a</ref>, the joint version is more powerful than the separate one, showing that joint spatiotemporal attention can learn more discrim-  <ref type="table" target="#tab_6">4c</ref>, when the model is gradually pre-trained from ImageNet to Kinetics-400, the performance of our UniFormer becomes better. Such distinct characteristic is not observed in the pure local MHRA structure (LLLL) and the splitting version. It demonstrates that the joint learning manner is preferable for video representation learning.</p><p>Does dynamic position embedding matter to UniFormer? With dynamic position embedding, our UniFormer improve the top-1 accuracy by 0.5% and 1.7% on ImageNet and Kinetics-400. It shows that via encoding the position information, our DPE can maintain spatiotemporal order, contributing to better spatiotemporal representation learning.</p><p>How much does local MHRA help? Since our UniFormer is equipped with local and global token affinity respectively in the shallow and deep layers, we investigate the configuration of our network stage by stage. As shown in Table <ref type="table" target="#tab_6">4a</ref>, when we only use local MHRA (LLLL), the computation cost will be light. However, the accuracy is largely dropped, since the network lacks the capacity of learning long-term dependency without global MHRA. When we gradually replace local MHRA with global MHRA, the accuracy becomes better as expected. However, the accuracy is dramatically dropped with a heavy computation load when all the layers apply global MHRA (GGGG). It is mainly because that, without local MHRA, the network lacks the capacity of extracting detailed video representations, leading to severe model overfitting with redundant spatiotemporal attention.</p><p>In our experiments, we choose local MHRA and global MHRA in the first two stages and the last two stages respectively, in order to achieve a preferable computation-accuracy balance.</p><p>Is our UniFormer more transferable? We further verify the transfer learning ability of our Uni-Former in Table <ref type="table" target="#tab_6">4c</ref>. All models share the same stage numbers but the stage types are different.</p><p>Compared with pre-training from ImgeNet, pre-training from Kinetics-400 will further improve the top-1 accuracy by 1.8%. However, such distinct characteristic is not observed in the pure local MHRA structure and UniFormer with divided spatiotemporal attention. It demonstrates that the joint learning manner is preferable for transfer learning.</p><p>Empirical investigation on model parameters. We further evaluate the robustness of our Uni-Former network to several important model parameters.</p><p>(1) size of local tube: In our local token affinity (Eq. 6), we aggregate spatiotemporal context from a small local tube. Hence, we investigate the influence of this tube by changing its 3D size (Table <ref type="table" target="#tab_6">4b</ref>). Our network is robust to the tube size. We choose 5×5×5 for better accuracy.</p><p>(2) sampling method: We explore the vital sampling method shown in Table <ref type="table" target="#tab_6">4d</ref>. For training, 16×4 means that we sample 16 frames with frame stride 4. For testing, 4×1 means four-clip testing. As expected, sparser sampling method achieves a higher single-clip result. For multi-clip testing, dense sampling is slightly better when sampling a few frames. However, when sampling more frames, sparse sampling is obviously better.</p><p>(3) testing strategy: We evaluate our network with different numbers of clips and crops for the validation videos. As shown in Figure <ref type="figure" target="#fig_2">4</ref>, since Kinetics is a scene-related dataset and trained with dense sampling, multi-clip testing is preferable to cover more frames for boosting performance. Alternatively, Something-Something is a temporal-related dataset and trained with uniform sampling, so multi-crop testing is better for capturing the discriminative motion for boosting performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">VISUALIZATION</head><p>To further verify the effectiveness of UniFormer, we conduct some visualizations of different structures (see Appendix D). In Figure <ref type="figure" target="#fig_5">5</ref>, We apply Grad-CAM <ref type="bibr" target="#b44">(Selvaraju et al., 2019)</ref> to show the areas Preprint of the greatest concern in the last layer. It reveals that GGGG struggles to focus on the key object, i.e., the skateboard and the football, as it blindly compares the similarity of all tokens in all layers. Alternatively, LLLL only performs local aggregation. Hence, its attention tends to be coarse and inaccurate without a global view. Different from both cases, our UniFormer with LLGG can cooperatively learn local and global contexts in a joint manner. As a result, it can effectively capture the most discriminative information, by paying precise attention to the skateboard and the football. In Figure <ref type="figure">6</ref>, we present the accuracies of different structures on Kinetics-400 <ref type="bibr" target="#b4">(Carreira &amp; Zisserman, 2017a)</ref>. It shows that LLGG outperforms other structures in most categories, which demonstrates that our UniFormer takes advantage of both 3D convolution and spatiotemporal self-attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preprint</head><p>A MORE DETAILS ABOUT LOCAL MHRA</p><p>For local MHRA, it is vital to determine the neighbor tokens. Considering any token X k (k ∈ [0, L − 1]), we can calculate its index (t k , h k , w k ) as follows:</p><formula xml:id="formula_9">t k = k H × W ,<label>(9)</label></formula><formula xml:id="formula_10">h k = k − t k × H × W W ,<label>(10)</label></formula><formula xml:id="formula_11">w k = (k − t k × H × W ) mod W.<label>(11)</label></formula><p>Therefore, for an anchor token X i , any of its neighbor tokens X j in Ω t×h×w i should satisfy</p><formula xml:id="formula_12">|t i − t j | ≤ t 2 , (<label>12</label></formula><formula xml:id="formula_13">)</formula><formula xml:id="formula_14">|h i − h j | ≤ h 2 , (<label>13</label></formula><formula xml:id="formula_15">)</formula><formula xml:id="formula_16">|w i − w j | ≤ w 2 . (<label>14</label></formula><formula xml:id="formula_17">)</formula><p>Thus the local spatiotemporal affinity in Eq. 6 can be calculated as follows:</p><formula xml:id="formula_18">A local n (X i , X j ) = a i−j (15) = a[t i − t j , h i − h j , w i − w j ].<label>(16)</label></formula><p>For other tokens not in</p><formula xml:id="formula_19">Ω t×h×w i , A local n (X i , X j ) = 0.</formula><p>B MORE DETAILS ABOUT FFN.</p><p>We adopt the standard FFN (Eq. 3) in vision transformers <ref type="bibr" target="#b12">(Dosovitskiy et al., 2021)</ref>,</p><formula xml:id="formula_20">Z = Linear 2 (GELU (Linear 1 (Z ))) ,<label>(17)</label></formula><p>where GELU is a non-linear function. The channel number will be first expanded by ratio 4 and then reduced. All token representations will be enhanced after performing FFN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C ADDITIONAL IMPLEMENTATION DETAILS</head><p>Architecture details. As in ViT <ref type="bibr" target="#b12">(Dosovitskiy et al., 2021)</ref>, we adopt the pre-normalization configuration <ref type="bibr" target="#b58">(Wang et al., 2019)</ref> that applies norm layer at the beginning of the residual function <ref type="bibr" target="#b19">(He et al., 2016)</ref>. Differently, we utilize BN <ref type="bibr" target="#b20">(Ioffe &amp; Szegedy, 2015)</ref> for local MHRA and LN <ref type="bibr" target="#b1">(Ba et al., 2016)</ref> for global MHRA. Moreover, we add an extra layer normalization in the downsampling layers.</p><p>Training details. We adopt AdamW <ref type="bibr" target="#b35">(Loshchilov &amp; Hutter, 2017a)</ref> optimizer with cosine learning rate schedule <ref type="bibr" target="#b35">(Loshchilov &amp; Hutter, 2017b)</ref> to train the entire network. The first 5 or 10 epochs are used for warm-up <ref type="bibr" target="#b17">(Goyal et al., 2017a)</ref> to overcome early optimization difficulty. For UniFormer-S, the warmup epoch, total epoch, stochastic depth rate, weight decay are set to 10, 110, 0.1 and 0.05 respectively for Kinetics and 5, 50, 0.3 and 0.05 respectively for Something-Something. For UniFormer-B, all the hyper-parameters are the same unless the stochastic depth rates are doubled. We linearly scale the base learning rates according to the batch size, which are 1e −4 × batchsize </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D VISUALIZATION</head><p>We choose three structures used in our experiments (Table <ref type="table" target="#tab_6">4a</ref>) to make comparisons: LLGG, LLLL and GGGG. The stage numbers of them are {3, 4, 8, 3}, {3, 5, 10, 4} and {2, 2, 7, 3} respectively.</p><p>In Figure <ref type="figure" target="#fig_5">5</ref>, we conduct attention visualization of different structures. Part1 shows the input videos selected from Kinetics-400 <ref type="bibr" target="#b4">(Carreira &amp; Zisserman, 2017a</ref>  <ref type="bibr">et al., 2019)</ref> to generate the corresponding attention in the last layer. Since GGGG blindly compares the similarity of all tokens in all, it struggles to focus on the key object, i.e., the skateboard and the football. Alternatively, LLLL only performs local aggregation without a global view, leading to coarse and inaccurate attention. Different from both cases, our UniFormer with LLGG can cooperatively learn local and global contexts in a joint manner. As a result, it can effectively capture the most discriminative information, by paying precise attention to the skateboard and the football.</p><p>Additionally, in Figure <ref type="figure">6</ref>, we show the top-1 accuracies of different structures on <ref type="bibr">Kinetics-400 (Carreira &amp; Zisserman, 2017a)</ref>. It demonstrates that GGGG surpasses the other two structures in most categories. Furthermore, we analyze the prediction results of several categories in Figure <ref type="figure">7</ref>. It shows that gargling is often misjudged as brushing teeth, while swing dancing is often misjudged as other types of dancing. We argue that these categories are easier to be discriminated against based on the spatial details. For example, toothbrush often exists in brushing teeth but not in gargling, and the people's poses are different in different dancing. Therefore, LLLL performs better than GGGG in these categories thanks to the capacity of encoding detailed spatiotemporal features. What's more, playing guitar and strumming guitar are difficult to be classified, since their spatial contents are almost the same. They require the long-range dependency between objects, e.g., the interaction between the people's hand and the guitar, thus GGGG does better. More importantly, our UniFormer with LLGG is competitive with the other two methods in these categories, which means it takes advantage of both 3D convolution and spatiotemporal self-attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E ADDITIONAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 MORE RESULTS ON KINETICS</head><p>Table <ref type="table" target="#tab_7">5</ref> shows more results on Kinetics-400 <ref type="bibr" target="#b4">(Carreira &amp; Zisserman, 2017a)</ref> and <ref type="bibr">Kinetics-600 (Carreira et al., 2018)</ref>. The trends of the results on both datasets are similar. When sampling with a large frame stride, the corresponding single-clip testing result will be better. It is mainly because sparser sampling covers a larger time range. For multi-clip testing, sampling with frame stride 4 always performs better, thus we adopt frame stride 4 by default.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 MORE RESULTS ON SOMETHING-SOMETHING</head><p>Table <ref type="table">6</ref> presents more results on Something-Something V1&amp;V2 <ref type="bibr" target="#b18">(Goyal et al., 2017b)</ref>. For UniFormer-S, pre-training with Kinetics-600 is better than pre-training with Kinetics-400, improving the top-1 accuracy by approximately 1.5%. However, for UniFormer-B, the improvement is not obvious. We claim that the small model is difficult to fit, thus larger dataset pre-training can help it fit better. Besides, UniFormer-B with 16 frames performs better than UniFormer-S with 32 frames.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: Accuracy vs. per-video GFLOPs on Kinetics-400 and Something-Something V2. B-32(4) means we test UniFormer-B 32f with 4 clips and S-16(3) means we test UniFormer-S 16f with 3 crops (more testing details can be found in Section 4.3). Our UniFormer achieves the best balance between accuracy and computation on both datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Overall architecture of our Unified transFormer (UniFormer). A UniFormer block consists of three key modules, i.e., Dynamic Position Embedding (DPE), Multi-Head Relation Aggregrator (MHRA), and Feed Forward Network (FFN). Detailed explanations can be found in Section 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Multi-clip/crop testing comparison on different datasets. Multi-clip testing is better for Kinetics and multi-crop testing is better for Something-Something.</figDesc><graphic url="image-6.png" coords="9,108.73,81.86,392.05,86.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Preprint (a) hoverboarding. (b) passing American football (not in game).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Attention visualization of different structures. Videos are chosen from Kinetics-400 (Carreira &amp; Zisserman, 2017a).</figDesc><graphic url="image-8.png" coords="16,108.00,394.34,396.00,280.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-9.png" coords="17,108.00,90.83,395.99,135.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Comparison to different methods. 'Local Redundancy' means the redundant computation for capturing local features. 'Global Dependency' means the long-range dependency among frames.</figDesc><table><row><cell>Preprint</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Basic Operation</cell><cell cols="2">Tackle Local Capture Global Redundancy Dependency GFLOPs Top-1 Efficiency</cell></row><row><cell>X3D (Feichtenhofer, 2020)</cell><cell>PWConv-DWConv-PWConv</cell><cell>5823</cell><cell>80.4</cell></row><row><cell>TimeSformer (Bertasius et al., 2021)</cell><cell>Divided MHSA</cell><cell>7140</cell><cell>80.7</cell></row><row><cell>Our UniFormer</cell><cell>Joint MHRA</cell><cell>168</cell><cell>80.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>, our global MHRA is based on joint spatiotemporal learning to generate more discriminative video representation. Moreover, we adopt dynamic position embedding (DPE, see Section 3.3) to overcome permutation-invariance, which can maintain translation-invariance and is friendly to different input clip lengths.</figDesc><table><row><cell>Preprint</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Pretrain</cell><cell>#Frame</cell><cell>GFLOPs</cell><cell cols="4">K400 Top-1 Top-5 Top-1 Top-5 K600</cell></row><row><cell>LGD(Qiu et al., 2019)</cell><cell>IN-1K</cell><cell cols="2">128×N/A N/A</cell><cell>79.4</cell><cell>94.4</cell><cell>81.5</cell><cell>95.6</cell></row><row><cell cols="2">SlowFast+NL(Feichtenhofer et al., 2019) -</cell><cell cols="2">16×3×10 7020</cell><cell>79.8</cell><cell>93.9</cell><cell>81.8</cell><cell>95.1</cell></row><row><cell>ip-CSN(Tran et al., 2019)</cell><cell cols="3">Sports1M 32×3×10 3270</cell><cell>79.2</cell><cell>93.8</cell><cell>-</cell><cell>-</cell></row><row><cell>CorrNet(Wang et al., 2020a)</cell><cell cols="3">Sports1M 32×3×10 6720</cell><cell>81.0</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>X3D-M(Feichtenhofer, 2020)</cell><cell>-</cell><cell cols="2">16×3×10 186</cell><cell>76.0</cell><cell>92.3</cell><cell>78.8</cell><cell>94.5</cell></row><row><cell>X3D-XL(Feichtenhofer, 2020)</cell><cell>-</cell><cell cols="2">16×3×10 1452</cell><cell>79.1</cell><cell>93.9</cell><cell>81.9</cell><cell>95.5</cell></row><row><cell>MoViNet-A5(Kondratyuk et al., 2021)</cell><cell>-</cell><cell cols="2">120×1×1 281</cell><cell>80.9</cell><cell>94.9</cell><cell>82.7</cell><cell>95.7</cell></row><row><cell>MoViNet-A6(Kondratyuk et al., 2021)</cell><cell>-</cell><cell cols="2">120×1×1 386</cell><cell>81.5</cell><cell>95.3</cell><cell>83.5</cell><cell>96.2</cell></row><row><cell>ViT-B-VTN (Neimark et al., 2021)</cell><cell>IN-21K</cell><cell cols="2">250×1×1 3992</cell><cell>78.6</cell><cell>93.7</cell><cell>-</cell><cell>-</cell></row><row><cell>TimeSformer-L(Bertasius et al., 2021)</cell><cell>IN-21K</cell><cell>96×3×1</cell><cell>7140</cell><cell>80.7</cell><cell>94.7</cell><cell>82.2</cell><cell>95.5</cell></row><row><cell>STAM (Sharir et al., 2021)</cell><cell>IN-21K</cell><cell>64×1×1</cell><cell>1040</cell><cell>79.2</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>X-ViT(Bulat et al., 2021)</cell><cell>IN-21K</cell><cell>16×3×1</cell><cell>850</cell><cell>80.2</cell><cell>94.7</cell><cell>84.5</cell><cell>96.3</cell></row><row><cell>Mformer-HR(Patrick et al., 2021)</cell><cell>IN-21K</cell><cell cols="2">16×3×10 28764</cell><cell>81.1</cell><cell>95.2</cell><cell>82.7</cell><cell>96.1</cell></row><row><cell>MViT-B,16×4(Fan et al., 2021)</cell><cell>-</cell><cell>16×1×5</cell><cell>353</cell><cell>78.4</cell><cell>93.5</cell><cell>82.1</cell><cell>95.7</cell></row><row><cell>MViT-B,32×3(Fan et al., 2021)</cell><cell>-</cell><cell>32×1×5</cell><cell>850</cell><cell>80.2</cell><cell>94.4</cell><cell>83.4</cell><cell>96.3</cell></row><row><cell>ViViT-L(Arnab et al., 2021)</cell><cell>IN-21K</cell><cell>16×3×4</cell><cell>17352</cell><cell>80.6</cell><cell>94.7</cell><cell>82.5</cell><cell>95.6</cell></row><row><cell>ViViT-L(Arnab et al., 2021)</cell><cell cols="2">JFT-300M 16×3×4</cell><cell>17352</cell><cell>82.8</cell><cell>95.3</cell><cell>84.3</cell><cell>96.2</cell></row><row><cell>Swin-T(Liu et al., 2021b)</cell><cell>IN-1K</cell><cell>32×3×4</cell><cell>1056</cell><cell>78.8</cell><cell>93.6</cell><cell>-</cell><cell>-</cell></row><row><cell>Swin-B(Liu et al., 2021b)</cell><cell>IN-1K</cell><cell>32×3×4</cell><cell>3384</cell><cell>80.6</cell><cell>94.6</cell><cell>-</cell><cell>-</cell></row><row><cell>Swin-B(Liu et al., 2021b)</cell><cell>IN-21K</cell><cell>32×3×4</cell><cell>3384</cell><cell>82.7</cell><cell>95.5</cell><cell>84.0</cell><cell>96.5</cell></row><row><cell>Our UniFormer-S</cell><cell>IN-1K</cell><cell>16×1×4</cell><cell>167</cell><cell>80.8</cell><cell>94.7</cell><cell>82.8</cell><cell>95.8</cell></row><row><cell>Our UniFormer-B</cell><cell>IN-1K</cell><cell>16×1×4</cell><cell>389</cell><cell>82.0</cell><cell>95.1</cell><cell>84.0</cell><cell>96.4</cell></row><row><cell>Our UniFormer-B</cell><cell>IN-1K</cell><cell>32×1×4</cell><cell>1036</cell><cell>82.9</cell><cell>95.4</cell><cell>84.8</cell><cell>96.7</cell></row><row><cell>Our UniFormer-B</cell><cell>IN-1K</cell><cell>32×3×4</cell><cell>3108</cell><cell>83.0</cell><cell>95.4</cell><cell>84.9</cell><cell>96.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Structure design. All models are trained for 50 epochs on Kinetics-400. To guarantee the parameters and computation of all the models are similar, when modifying the stage types, we modify the stage numbers and reduce the computation of self-attention asMViT (Fan et al., 2021)  for LGGG and GGGG.</figDesc><table><row><cell>Preprint</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Unified Joint DPE</cell><cell>Type</cell><cell cols="8">ImageNet GFLOPs #Param Top-1 Top-5 GFLOPs #Param Top-1 Top-5 K400 1×4</cell></row><row><cell></cell><cell>LLGG</cell><cell>3.6</cell><cell>21.5</cell><cell>82.9</cell><cell>96.2</cell><cell>41.8</cell><cell>21.4</cell><cell>79.3</cell><cell>94.3</cell></row><row><cell></cell><cell>LLGG</cell><cell>3.3</cell><cell>21.3</cell><cell>82.6</cell><cell>96.1</cell><cell>41.0</cell><cell>21.3</cell><cell>78.6</cell><cell>93.6</cell></row><row><cell></cell><cell>LLGG</cell><cell>3.6</cell><cell>21.5</cell><cell>82.9</cell><cell>96.2</cell><cell>36.8</cell><cell>27.7</cell><cell>78.7</cell><cell>94.1</cell></row><row><cell></cell><cell>LLGG</cell><cell>3.6</cell><cell>21.5</cell><cell>82.4</cell><cell>96.0</cell><cell>41.4</cell><cell>21.3</cell><cell>77.6</cell><cell>93.5</cell></row><row><cell></cell><cell>LLLL</cell><cell>3.7</cell><cell>23.3</cell><cell>81.9</cell><cell>95.9</cell><cell>31.6</cell><cell>23.7</cell><cell>77.2</cell><cell>92.9</cell></row><row><cell></cell><cell>LLLG</cell><cell>3.7</cell><cell>22.2</cell><cell>82.5</cell><cell>96.1</cell><cell>31.6</cell><cell>22.4</cell><cell>78.4</cell><cell>93.3</cell></row><row><cell></cell><cell>LGGG</cell><cell>3.6</cell><cell>21.6</cell><cell>82.7</cell><cell>96.1</cell><cell>39.0</cell><cell>21.4</cell><cell>79.0</cell><cell>94.1</cell></row><row><cell></cell><cell>GGGG</cell><cell>3.7</cell><cell>20.1</cell><cell>82.1</cell><cell>95.9</cell><cell>72.0</cell><cell>19.8</cell><cell>75.3</cell><cell>92.4</cell></row><row><cell>(a) Size 3 5 7 9 (b) Tube size. Our net-K400 1×4 GFLOPs Top1 41.0 79.0 41.8 79.3 43.6 79.1 46.6 78.9 work is basically robust to the tube size.</cell><cell cols="5">Type LLLL LLGG LLGG (c) Transfer learning. Jointly manner performs Joint GFLOPs Pretrain SSV1 Top-1 ImageNet 49.2 26.1 K400 49.2(+0.0) ImageNet 51.9 36.8 K400 51.8(−0.1) ImageNet 52.0 41.8 K400 53.8(+1.8) better when pre-training from larger dataset.</cell><cell cols="4">Sampling K400 Top-1 Method 1×1 4×1 16×4 76.2 80.8 16×8 78.4 80.7 16×4 78.1 82.0 16×8 79.3 81.7 32×2 77.3 81.2 32×4 79.8 82.0 (d) Sampling method. Model Small Base Small</cell></row></table><note>Table 2 presents comparisons to the state-of-the-art methods on Kinetics-400 and Kinetics-600. The first part shows the prior works using CNN. Compared with SlowFast (Fe-</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Ablation studies. 'Unified' means whether to use our local MHRA ( means to use Mo-</figDesc><table /><note>bileNet block). 'Joint' means whether to use joint attention. 'L'/'G' refers to local/globalMHRA.   ichtenhofer et al., 2019), our UniFormer-S 16f requires 42× fewer GFLOPs but obtains 1.0% performance gain on both datasets. Even compared with MoViNet<ref type="bibr" target="#b24">(Kondratyuk et al., 2021)</ref>, which is designed through extensive neural architecture search, our model achieves slightly better results with fewer input frames (16f ×4 vs. 120f ). The second part lists the recent works based on vision transformers. With only ImageNet-1K pre-training, UniFormer-B 16f surpasses most of the other backbones with large dataset pre-training. For example, compared with ViViT-L pre-trained from JFT-300M and Swin-B pre-trained from ImageNet-21K, UniFormer-B 32f obtains comparable performance with 16.7× and 3.3× fewer computation on both Kinetics-400 and Kinetics-600.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>). In part2, we use Grad-CAM (Selvaraju More results on Kinetics-400&amp;600.</figDesc><table><row><cell>Preprint</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="4">Sampling #Frame GFLOPs #Param Method</cell><cell cols="4">K400 Top-1 Top-5 Top-1 Top-5 K600</cell></row><row><cell></cell><cell>16×4</cell><cell>16×1×1 16×1×4</cell><cell>41.8 167.2</cell><cell>21.4 21.4</cell><cell>76.2 80.8</cell><cell>92.2 94.7</cell><cell>79.0 82.8</cell><cell>93.6 95.8</cell></row><row><cell>UniFormer-S</cell><cell>16×8 32×2</cell><cell>16×1×1 16×1×4 32×1×1 32×1×4</cell><cell>41.8 167.2 109.6 438.4</cell><cell>21.4 21.4 21.4 21.4</cell><cell>78.4 80.8 77.3 81.2</cell><cell>92.9 94.4 92.4 94.7</cell><cell>80.8 82.7 --</cell><cell>94.7 95.7 --</cell></row><row><cell></cell><cell>32×4</cell><cell>32×1×1 32×1×4</cell><cell>109.6 438.4</cell><cell>21.4 21.4</cell><cell>79.8 82.0</cell><cell>93.4 95.1</cell><cell>--</cell><cell>--</cell></row><row><cell></cell><cell>16×4</cell><cell>16×1×1 16×1×4</cell><cell>96.7 386.8</cell><cell>49.8 49.8</cell><cell>78.1 82.0</cell><cell>92.8 95.1</cell><cell>80.3 84.0</cell><cell>94.5 96.4</cell></row><row><cell>UniFormer-B</cell><cell>16×8</cell><cell>16×1×1 16×1×4</cell><cell>96.7 386.8</cell><cell>49.8 49.8</cell><cell>79.3 81.7</cell><cell>93.4 94.8</cell><cell>81.7 83.4</cell><cell>95.0 96.0</cell></row><row><cell></cell><cell>32×4</cell><cell>32×1×1 32×1×4</cell><cell>259 1036</cell><cell>49.8 49.8</cell><cell>80.9 82.9</cell><cell>94.0 95.4</cell><cell>82.7 84.8</cell><cell>95.7 96.7</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we propose a novel UniFormer, which can effectively unify 3D convolution and spatiotemporal self-attention in a concise transformer format to overcome video redundancy and dependency. We adopt local MHRA in shallow layers to largely reduce computation burden and global MHRA in deep layers to learn global token relation. Extensive experiments demonstrate that our UniFormer achieves a preferable balance between accuracy and efficiency on popular video benchmarks, Kinetics-400/600 and Something-Something V1/V2.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preprint</head><p>Figure <ref type="figure">6</ref>: Accuracy of different structures on Kinetics-400.   <ref type="bibr" target="#b10">(Deng et al., 2009)</ref>. We design four model variants as follows:</p><p>• UniFormer-S: channel numbers={64, 128, 256, 512}, stage numbers={3, 4, 8, 3}</p><p>• UniFormer-S †: channel numbers={64, 128, 256, 512}, stage numbers={3, 5, 9, 3}  <ref type="bibr" target="#b23">(Jiang et al., 2021)</ref>. We group the models based on their parameters.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Vivit: A video vision transformer</title>
		<author>
			<persName><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<idno>ArXiv, abs/2103.15691</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Is space-time attention all you need for video understanding?</title>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Ryan Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<idno>ArXiv, abs/2102.05095</idno>
	</analytic>
	<monogr>
		<title level="j">Gedas Bertasius</title>
		<imprint>
			<date type="published" when="2016">2016. 2021</date>
		</imprint>
	</monogr>
	<note>Layer normalization</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">High-performance large-scale image recognition without normalization</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soham</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<idno>ArXiv, abs/2102.06171</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Brais Martínez, and Georgios Tzimiropoulos. Space-time mixing attention for video transformer</title>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan-Manuel</forename><surname>Pérez-Rúa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swathikiran</forename><surname>Sudhakaran</surname></persName>
		</author>
		<idno>ArXiv, abs/2106.05968</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName><forename type="first">João</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2017">2017. 2017a</date>
			<biblScope unit="page" from="4724" to="4733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A short note about kinetics-600</title>
		<author>
			<persName><forename type="first">João</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Noland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andras</forename><surname>Banki-Horvath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>ArXiv, abs/1808.01340</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName><forename type="first">João</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2017">2017. 2017b</date>
			<biblScope unit="page" from="4724" to="4733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Do we really need explicit position encodings for vision transformers?</title>
		<author>
			<persName><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaxia</forename><surname>Xia</surname></persName>
		</author>
		<idno>ArXiv, abs/2102.10882</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">On the relationship between selfattention and convolutional layers</title>
		<author>
			<persName><forename type="first">Jean-Baptiste</forename><surname>Cordonnier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Loukas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
		<idno>ArXiv, abs/1911.03584</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Coatnet: Marrying convolution and attention for all data sizes</title>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<idno>ArXiv, abs/2106.04803</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
				<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Cswin transformer: A general vision transformer backbone with cross-shaped windows</title>
		<author>
			<persName><forename type="first">Preprint</forename><surname>Xiaoyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<idno>ArXiv, abs/2107.00652</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<idno>ArXiv, abs/2010.11929</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Multiscale vision transformers</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Haoqi Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karttikeya</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanghao</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><surname>Feichtenhofer</surname></persName>
		</author>
		<idno>ArXiv, abs/2104.11227</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">X3d: Expanding architectures for efficient video recognition</title>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="200" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="6201" to="6210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Container: Context aggregation network</title>
		<author>
			<persName><forename type="first">Peng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<idno>ArXiv, abs/2106.01401</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno>ArXiv, abs/1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
				<imprint>
			<date type="published" when="2017">2017a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The &quot;something something&quot; video database for learning and evaluating visual common sense</title>
		<author>
			<persName><forename type="first">Raghav</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susanne</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heuna</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valentin</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ingo</forename><surname>Fründ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Mueller-Freitag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Thurau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ingo</forename><surname>Bax</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2017">2017. 2017b</date>
			<biblScope unit="page" from="5843" to="5851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">X</forename><surname>Kaiming He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>ArXiv, abs/1502.03167</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">How much position information do convolutional neural networks encode?</title>
		<author>
			<persName><forename type="middle">Amirul</forename><surname>Md</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sen</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><forename type="middle">D B</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><surname>Bruce</surname></persName>
		</author>
		<idno>ArXiv, abs/2001.08248</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Stm: Spatiotemporal and motion encoding for action recognition</title>
		<author>
			<persName><forename type="first">Boyuan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengmeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihao</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="2000" to="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">All tokens matter: Token labeling for training better vision transformers</title>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daquan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojie</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.10858</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Movinets: Mobile video networks for efficient video recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kondratyuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangzhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yandong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">A</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<idno>ArXiv, abs/2103.11511</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Motionsqueeze: Neural motion feature learning for video understanding</title>
		<author>
			<persName><forename type="first">Heeseung</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manjin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suha</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Ct-net: Channel tensorization network for video classification</title>
		<author>
			<persName><forename type="first">Kunchang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianhang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<idno>ArXiv, abs/2106.01603</idno>
		<imprint>
			<date type="published" when="2021">2021a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Smallbignet: Integrating core and contextual views for video classification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhipeng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2020">2020. 2020a</date>
			<biblScope unit="page" from="1089" to="1098" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Vidtr: Video transformer without convolutions</title>
		<author>
			<persName><forename type="first">Preprint</forename><surname>Xinyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunhui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Biagio</forename><surname>Brattoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Marsic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Tighe</surname></persName>
		</author>
		<idno>ArXiv, abs/2104.11746</idno>
		<imprint>
			<date type="published" when="2021">2021b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Tea: Temporal excitation and aggregation for action recognition</title>
		<author>
			<persName><forename type="first">Yinong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianguo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<idno>ArXiv, abs/2004.01398</idno>
		<imprint>
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Tsm: Temporal shift module for efficient video understanding</title>
		<author>
			<persName><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="7082" to="7092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<idno>ArXiv, abs/2103.14030</idno>
		<imprint>
			<date type="published" when="2021">2021a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Video swin transformer</title>
		<author>
			<persName><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<idno>ArXiv, abs/2106.13230</idno>
		<imprint>
			<date type="published" when="2021">2021b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Teinet: Towards an efficient architecture for video recognition</title>
		<author>
			<persName><forename type="first">Zhaoyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jilin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<idno>ArXiv, abs/1911.09435</idno>
		<imprint>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Convtransformer: A convolutional transformer network for video frame synthesis</title>
		<author>
			<persName><forename type="first">Zhouyong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wubin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingben</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luxi</forename><surname>Yang</surname></persName>
		</author>
		<idno>ArXiv, abs/2011.10185</idno>
		<imprint>
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Fixing weight decay regularization in adam</title>
		<author>
			<persName><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno>ArXiv, abs/1711.05101</idno>
	</analytic>
	<monogr>
		<title level="m">Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv: Learning</title>
				<imprint>
			<date type="published" when="2017">2017a. 2017b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Grouped spatial-temporal aggregation for efficient action recognition</title>
		<author>
			<persName><forename type="first">Chenxu</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="5511" to="5520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Neimark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omri</forename><surname>Bar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maya</forename><surname>Zohar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dotan</forename><surname>Asselmann</surname></persName>
		</author>
		<idno>ArXiv, abs/2102.00719</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Keeping your eye on the ball: Trajectory attention in video transformers</title>
		<author>
			<persName><forename type="first">Mandela</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dylan</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuki</forename><forename type="middle">M</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra Florian Metze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">João</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<idno>ArXiv, abs/2106.05392</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal representation with pseudo-3d residual networks</title>
		<author>
			<persName><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="5534" to="5542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal representation with local and global diffusion</title>
		<author>
			<persName><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinmei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="12048" to="12057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Designing network design spaces</title>
		<author>
			<persName><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raj</forename><forename type="middle">Prateek</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="10425" to="10433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Stand-alone self-attention in vision models</title>
		<author>
			<persName><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Mo-bilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="page" from="336" to="359" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words</title>
		<author>
			<persName><forename type="first">Gilad</forename><surname>Sharir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asaf</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihi</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<idno>ArXiv, abs/2103.13915</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Bottleneck transformers for visual recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<idno>ArXiv, abs/2101.11605</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Efficientnet</surname></persName>
		</author>
		<idno>ArXiv, abs/1905.11946</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Smaller models and faster training</title>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Efficientnetv2</surname></persName>
		</author>
		<idno>ArXiv, abs/2104.00298</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Alexandre Sablayrolles, and Herv&apos;e J&apos;egou. Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hugo Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><surname>Massa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2021">2021a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Going deeper with image transformers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hugo Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><surname>Herv'e J'egou</surname></persName>
		</author>
		<idno>ArXiv, abs/2103.17239</idno>
		<imprint>
			<date type="published" when="2021">2021b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lubomir</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Hong Xiu Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manohar</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Video classification with channel-separated convolutional networks</title>
		<author>
			<persName><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="5551" to="5560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><forename type="middle">M</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno>ArXiv, abs/1706.03762</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Video modeling with correlation networks</title>
		<author>
			<persName><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2020">2020. 2020a</date>
			<biblScope unit="page" from="349" to="358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Tdn: Temporal difference networks for efficient action recognition</title>
		<author>
			<persName><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhan</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gangshan</forename><surname>Wu</surname></persName>
		</author>
		<idno>ArXiv, abs/2012.10071</idno>
		<imprint>
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Learning deep transformer models for machine translation</title>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingbo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><forename type="middle">F</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lidia</forename><forename type="middle">S</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<idno>ArXiv, abs/2102.12122</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Cvt: Introducing convolutions to vision transformers</title>
		<author>
			<persName><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<idno>ArXiv, abs/2103.15808</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Tokens-to-token vit: Training vision transformers from scratch on imagenet</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><forename type="middle">E H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuicheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><surname>Yan</surname></persName>
		</author>
		<idno>ArXiv, abs/2101.11986</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Shifted chunk transformer for spatiotemporal representational learning</title>
		<author>
			<persName><forename type="first">Xuefan</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingxun</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Liu</surname></persName>
		</author>
		<idno>ArXiv, abs/2108.11575</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Convnets vs. transformers: Whose visual representations are more transferable?</title>
		<author>
			<persName><forename type="first">Hong-Yu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chixiang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sibei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
		</author>
		<idno>ArXiv, abs/2108.05305</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
