<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Under review as a conference paper at ICLR 2021 ZERO-COST PROXIES FOR LIGHTWEIGHT NAS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<title level="a" type="main">Under review as a conference paper at ICLR 2021 ZERO-COST PROXIES FOR LIGHTWEIGHT NAS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neural Architecture Search (NAS) is quickly becoming the standard methodology to design neural network models. However, NAS is typically compute-intensive because multiple models need to be evaluated before choosing the best one. To reduce the computational power and time needed, a proxy task is often used for evaluating each model instead of full training. In this paper, we evaluate conventional reduced-training proxies and quantify how well they preserve ranking between multiple models during search when compared with the rankings produced by final trained accuracy. We propose a series of zero-cost proxies, based on recent pruning literature, that use just a single minibatch of training data to compute a model's score. Our zero-cost proxies use 3 orders of magnitude less computation but can match and even outperform conventional proxies. For example, Spearman's rank correlation coefficient between final validation accuracy and our best zero-cost proxy on NAS-Bench-201 is 0.82, compared to 0.61 for EcoNAS (a recently proposed reduced-training proxy). Finally, we use these zerocost proxies to enhance existing NAS search algorithms such as random search, reinforcement learning, evolutionary search and predictor-based search. For all search methodologies and across three different NAS datasets, we are able to significantly improve sample efficiency, and thereby decrease computation, by using our zero-cost proxies. For example on NAS-Bench-101, we achieved the same accuracy 4× quicker than the best previous result.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Instead of manually designing neural networks, neural architecture search (NAS) algorithms are used to automatically discover the best ones <ref type="bibr" target="#b27">(Tan &amp; Le, 2019a;</ref><ref type="bibr" target="#b18">Liu et al., 2019;</ref><ref type="bibr" target="#b0">Bender et al., 2018)</ref>. Early work by <ref type="bibr" target="#b37">Zoph &amp; Le (2017)</ref> proposed using a reinforcement learning (RL) controller that constructs candidate architectures, these are evaluated and then feedback is provided to the controller based on the performance of the candidate. One major problem with this basic NAS methodology is that each evaluation is very costly -typically on the order of hours or days to train a single neural network fully. We focus on this evaluation phase -we propose using proxies that require a single minibatch of data and a single forward/backward propagation pass to score a neural network. This is inspired by recent pruning-at-initialization work by <ref type="bibr" target="#b17">Lee et al. (2019)</ref>, <ref type="bibr" target="#b32">Wang et al. (2020)</ref> and <ref type="bibr" target="#b29">Tanaka et al. (2020)</ref> wherein a per-parameter saliency metric is computed before training to inform parameter pruning. Can we use such saliency metrics to score an entire neural network? Furthermore, can we use these "single minibatch" metrics to rank and compare multiple neural networks for use within NAS? If so, how do we best integrate these metrics within existing NAS algorithms such as RL or evolutionary search? These are the questions that we hope to (empirically) tackle in this work with the goal of making NAS less compute-hungry. Our contributions are:</p><p>• Zero-cost proxies We adapt pruning-at-initialization metrics for use with NAS. This requires these metrics to operate at the granularity of an entire network rather than individual parameters -we devise and validate approaches that aggregate parameter-level metrics in a manner suitable for ranking candidates during NAS search. • Comparison to conventional proxies We perform a detailed comparison between zerocost and conventional NAS proxies that use a form of reduced-computation training. First, we quantify the rank consistency of conventional proxies on large-scale datasets: 15k models vs. 50 models used in <ref type="bibr" target="#b36">(Zhou et al., 2020)</ref>. Second, we show that zero-cost proxies can match or exceed the rank consistency of conventional proxies. • Ablations on NAS benchmarks We perform ablations of our zero-cost proxies on five different NAS benchmarks (NAS-Bench-101/201/NLP/ASR and PyTorchCV) to both test the zero-cost metrics under different settings, and expose properties of successful metrics.</p><p>• Integration with NAS Finally, we propose two ways to use zero-cost metrics effectively within NAS algorithms: random search, reinforcement learning, aging evolution and predictor-based search. For all algorithms and three NAS datasets we show significant speedups, up to 4× for NAS-Bench-101 compared to current state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>NAS Efficiency To decrease NAS search time, various techniques were used in the literature. <ref type="bibr" target="#b24">Pham et al. (2018)</ref> and <ref type="bibr" target="#b1">Cai et al. (2018)</ref> use weight sharing between candidate models to decrease the training time during evaluation. <ref type="bibr" target="#b18">Liu et al. (2019)</ref> and others use smaller datasets (CIFAR-10) as a proxy to the full task (ImageNet1k). In EcoNAS, <ref type="bibr" target="#b36">Zhou et al. (2020)</ref> extensively investigated reduced-training proxies wherein input size, model size, number of training samples and number of epochs were reduced in the NAS evaluation phase. We compare to EcoNAS in this work to elucidate how well our zero-cost proxies perform compared to familiar and widely-used conventional proxies.</p><p>Pruning The goal is to reduce the number of parameters in a neural network, one way to do this is by identifying a saliency (importance) metric for each parameter, and the less-important parameters are removed. For example, <ref type="bibr" target="#b9">Han et al. (2015)</ref>, <ref type="bibr" target="#b7">Frankle &amp; Carbin (2019)</ref> and others use parameter magnitudes as the criterion while <ref type="bibr" target="#b16">LeCun et al. (1990)</ref>, <ref type="bibr" target="#b10">Hassibi &amp; Stork (1993)</ref>   <ref type="formula">2019</ref>) and extended by <ref type="bibr" target="#b32">Wang et al. (2020)</ref> and <ref type="bibr" target="#b29">Tanaka et al. (2020)</ref>. A single forward/backward propagation pass is used to compute a saliency criterion which is successfully used to heavily prune neural networks before training. We extend these pruning-at-initialization criteria towards scoring entire neural networks and we investigate their use with NAS algorithms.</p><p>Intersection between pruning and NAS Concepts from pruning have been used within NAS multiple times. For example, <ref type="bibr" target="#b19">Mei et al. (2020)</ref> use channel pruning in their AtomNAS work to arrive at customized multi-kernel-size convolutions (mixconvs as introduced by Tan &amp; Le (2019b)). In their Blockswap work, <ref type="bibr" target="#b31">Turner et al. (2020)</ref> use Fisher information at initialization to score different lightweight primitives that are substituted into a neural network to decrease computation. This is the earliest work we could find that attempts to perform a type of NAS by scoring neural networks without training using a pruning criterion, More recently, <ref type="bibr" target="#b20">Mellor et al. (2020)</ref> introduced a new metric for scoring neural networks at initialization based on the correlation of Jacobians with different inputs. They perform "NAS without training" by performing random search with their zero-cost metric (jacob cov) to rank neural networks instead of using accuracy. We include jacob cov in our analysis and we introduce five more zero-cost metrics in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROXIES FOR NEURAL NETWORK ACCURACY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">CONVENTIONAL NAS PROXIES (ECONAS)</head><p>In conventional sample-based NAS, a proxy training regime is often used to predict a model's accuracy instead of full training. <ref type="bibr" target="#b36">Zhou et al. (2020)</ref> investigate conventional proxies in depth by computing the Spearman rank correlation coefficient (Spearman ρ) of a proxy task to final validation accuracy. The proxy used is a reduced-computation training, wherein one of the following four variables is reduced: (1) number of epochs, (2) number of training samples, (3) input resolution (4) model size (controlled through the number of channels after the first convolution). Even though such proxies were used in many prior works, EcoNAS is the first systematic study of conventional proxy tasks that we found. One main finding by <ref type="bibr" target="#b36">Zhou et al. (2020)</ref> is that using approximately 1 4 of the model size and input resolution, all training samples, and 1 10 the number of epochs was a reasonable proxy which yielded the best results for their experiment <ref type="bibr" target="#b36">(Zhou et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">ZERO-COST NAS PROXIES</head><p>We present alternative proxies for network accuracy that can be used to speed up NAS. A simple proxy that we use is grad norm in which we sum the Euclidean norm of the gradients after a single minibatch of data training data. Other metrics listed below were previously introduced in the context of parameter pruning at the granularity of a single parameter -a saliency is computed to rank parameters and remove the least important ones. We adapt these metrics to score and rank entire neural network models for NAS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">SNIP, GRASP AND SYNAPTIC FLOW</head><p>In their snip work, <ref type="bibr" target="#b17">Lee et al. (2019)</ref> proposed performing parameter pruning based on a saliency metric computed at initialization using a single minibatch of data. This saliency criteria approximates the change in loss when a specific parameter is removed. <ref type="bibr" target="#b32">Wang et al. (2020)</ref> attempted to improve on the snip metric by approximating the change in gradient norm (instead of loss) when a parameter is pruned in their grasp objective. Finally, <ref type="bibr" target="#b29">Tanaka et al. (2020)</ref> generalized these so-called synaptic saliency scores and proposed a modified version (synflow) which avoids layer collapse when performing parameter pruning. Instead of using a minibatch of training data and cross-entropy loss (as in snip or grasp), with synflow we compute a loss which is simply the product of all parameters in the network; therefore, no data is needed to compute this loss or the synflow metric itself. These are the three metrics:</p><formula xml:id="formula_0">snip : S p (θ) = ∂L ∂θ θ , grasp : S p (θ) = −(H ∂L ∂θ ) θ, synflow : S p (θ) = ∂L ∂θ θ</formula><p>(1) where L is the loss function of a neural network with parameters θ, H is the Hessian<ref type="foot" target="#foot_0">1</ref> , S p is the per-parameter saliency and is the Hadamard product. We extend these saliency metrics to score an entire neural network by summing over all parameters N in the model: <ref type="bibr" target="#b30">Theis et al. (2018)</ref> perform channel pruning by removing activation channels (and their corresponding parameters) that are estimated to have the least effect on the loss. They build on the work of <ref type="bibr" target="#b21">Molchanov et al. (2017)</ref> and <ref type="bibr" target="#b6">Figurnov et al. (2016)</ref>. More recently, <ref type="bibr" target="#b31">Turner et al. (2020)</ref> aggregated this fisher metric for all channels in a convolution primitive to quantify the importance of that primitive when it is replaced by a more efficient alternative. We further aggregate the fisher metric for all layers in a neural network to score an entire network as shown in the following equations:</p><formula xml:id="formula_1">S n = N i S p (θ) i . 3.2.2 FISHER</formula><formula xml:id="formula_2">fisher : S z (z) = ∂L ∂z z 2 , S n = M i=1 S z (z i ) (2)</formula><p>where S z is the saliency per activation z, and M is the length of the vectorized feature map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">JACOBIAN COVARIANCE</head><p>This metric was purpose-designed to score neural networks in the context of NAS -we refer the reader to the original paper for detailed reasoning and derivation of the metric which we call jacob cov <ref type="bibr" target="#b20">(Mellor et al., 2020)</ref>. In brief, this metric captures the correlation of activations within a network when subject to different inputs within a minibatch of data -the lower the correlation, the better the network is expected to perform as it can differentiate between different inputs well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EMPIRICAL EVALUATION OF PROXY TASKS</head><p>Generally, most of the proxies presented in the previous section try to capture how trainable a neural network is by inspecting the gradients at the beginning of training. In this work, we refrain from attempting to explain precisely why each metric works (or does not work) and instead focus on the empirical evaluation of those metrics in different scenarios. We use the Spearman rank correlation coefficient (Spearman ρ) to quantify how well a proxy ranks models compared to the ground-truth ranking produced by final test accuracy <ref type="bibr" target="#b2">(Daniel, 1990)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">NAS-BENCH-201</head><p>NAS-Bench-201 is a purpose-built benchmark for prototyping NAS algorithms <ref type="bibr" target="#b4">(Dong &amp; Yang, 2020)</ref>. It contains 15,625 CNN models from a cell-based search space and corresponding training statistics. We first use NAS-Bench-201 to evaluate conventional proxies from EcoNAS, then we evaluate our zero-cost proxies and compare the two approaches.  According to the EcoNAS study, the most effective configuration divides both the input resolution and stem channels by ~4 and the number of epochs by 10, that is, r 8 c 4 e 20 for NAS-Bench-201 models. Keeping that in mind we investigate r 8 c 4 in Fig. <ref type="figure" target="#fig_0">1</ref> (labeled econas); however, this proxy training seems to suffer from overfitting as correlation to final accuracy started to drop after 20 epochs. Additionally, the Spearman ρ was a modest 0.61 when evaluated on all 15,625 models in NAS-Bench-201 -a far cry from the 0.87 achieved on the 50 models in the EcoNAS paper <ref type="bibr" target="#b36">(Zhou et al., 2020)</ref>. We additionally explore r 8 c 8 , r 16 c 4 and r 16 c 8 and find a very good proxy with r 16 c 8 e 15 , labeled in Fig. <ref type="figure" target="#fig_0">1</ref> as econas+. From the plots in Fig. <ref type="figure" target="#fig_0">1</ref>, we would like to highlight that:</p><p>1. A reduced-training proxy that works well on one search space may not work well on another as highlighted by the difference in Spearman ρ between econas and econas+. This occurs even though both tasks in this case were CIFAR-10 image classification.</p><p>2. Even though econas-style proxies reduce computation load by a large factor (as seen in the middle plot in Fig. <ref type="figure" target="#fig_0">1</ref>, this does not translate fully into actual runtime improvement when run on a nominal desktop GPU<ref type="foot" target="#foot_1">2</ref> . We therefore plot actual GPU speedup in the third subplot in Fig. <ref type="figure" target="#fig_0">1</ref>. For example, notice that the point labeled econas (r 8 c 4 e 20 ) has the same FLOPS as ~1 10 of a full training epoch, but when measured on a GPU, takes time equivalent to 5 full training epochs -a 50× gap between theoretical and actual speedup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">ZERO-COST PROXIES ON NAS-BENCH-201</head><p>We now shift our focus towards our zero-cost NAS proxies which rely on gradient computations using a single minibatch of data at initialization. A clear advantage of zero-cost proxies is that they take very little time to compute -the forward/backward pass using a single minibatch of data.</p><p>We ran the zero-cost proxies on all 15,625 models in NAS-Bench-201 for three datasets and we summarize the results in Table <ref type="table" target="#tab_1">1</ref>.</p><p>The synflow metric performed the best on all three datasets with a Spearman ρ consistently above 0.73, jacob cov was second best but was also very well-correlated to final accuracy. Next came grad norm and snip with a Spearman ρ close to 0.6. We add another metric that we simply label with vote that takes a majority vote between the three metrics synflow, jacob cov and snip when ranking two models. This performed better than any single metric with a Spearman ρ consistently above 0.8. At the cost of just 3 minibatches instead of ~1000, this is already performing slightly better than econas+, and much better than econas as shown in Fig. <ref type="figure">2a</ref>. In Fig. <ref type="figure">2</ref> we also plot the rank correlation of validation accuracy (without any reduced training) over the first 10 epochs of training for the three datasets available in NAS-Bench-201. Having set a comparison point with EcoNAS and normal training proxies, we have shown that zero-cost proxies can match and outperform these conventional methods in a large-scale empirical analysis. However, different NAS search spaces may behave differently, so in the remainder of this section, we test the zero-cost proxies on different search spaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">MODELS IN THE WILD (PYTORCHCV)</head><p>To study zero-cost proxies in a different setting, we scored the models in the PyTorchCV database <ref type="bibr" target="#b26">(Sémery, 2020)</ref>. PytorchCV contains common state-of-the-art neural networks such as ResNets <ref type="bibr" target="#b11">(He et al., 2016)</ref>, DenseNets <ref type="bibr" target="#b13">(Huang et al., 2017)</ref>, MobileNets <ref type="bibr" target="#b12">(Howard et al., 2017)</ref> and EfficientNets <ref type="bibr" target="#b27">(Tan &amp; Le, 2019a</ref>) -a representative assortment of top-performing models. We evaluated ~50 models for CIFAR-10, CIFAR-100 <ref type="bibr" target="#b15">(Krizhevsky, 2009)</ref> and SVHN <ref type="bibr" target="#b22">(Netzer et al., 2011)</ref>, and ~200 models for ImageNet1k <ref type="bibr" target="#b3">(Deng et al., 2009)</ref>. Fig. <ref type="figure">3</ref> shows the resulting correlation for the zero-cost metrics. synflow, snip, fisher and grad norm all perform similarly well on all datasets, with the exception of SVHN where synflow outperforms other metrics by a large margin. However, grasp failed in this setting completely as shown by the low mean Spearman ρ and high variance as shown in Fig. <ref type="figure">3</ref>. Curiously, jacob cov also failed in this setting even though it performed well on NAS-Bench-201. This suggests that this metric is better at scoring models from within a search space (similar topology), but becomes worse when scoring unrelated models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">OTHER SEARCH SPACES</head><p>We investigate these metrics with other NAS datasets. Our goal is to empirically find a good metric to speed up NAS algorithms reliably -we evaluate our zero-cost proxies on the following datasets.</p><p>• NAS-Bench-101: This is the first and largest NAS benchmark available with over 423k</p><p>CNN models and training statistics on CIFAR-10 ( <ref type="bibr" target="#b35">Ying et al., 2019)</ref>.</p><p>• NAS-Bench-NLP: <ref type="bibr" target="#b14">Klyuchnikov et al. (2020)</ref> investigate the architectures of 14k different recurrent cells in natural language processing (NLP) tasks such as next word prediction.</p><p>• NAS-Bench-ASR: This is our in-house dataset for convolution-based automatic speech recognition models evaluated on the TIMIT dataset <ref type="bibr" target="#b8">(Garofolo et al., 1993)</ref>. The search space includes linear, convolution, zeroize and skip-connections, forming 8242 models.  <ref type="table" target="#tab_2">2</ref> we would like to highlight that the synflow metric is the only consistent one across all analyzed benchmarks. Additionally, even for the synflow metric, rank correlation is quite a bit lower than that for NAS-Bench-201 (~0.3 vs. ~0.8). Other than global rank correlation, we posit that ranking of top models from a search space is also critically important for NAS algorithms -this is because we ultimately care about finding those top models. In Section A.3 we perform an analysis of how top models are ranked by zero-cost proxies. Top models (according to validation accuracy) need to be ranked correctly by a zero-cost metric. Additionally, local rank correlation of top models could be important for NAS algorithms when two good models are compared using their proxy metric value. Tables <ref type="table" target="#tab_11">10 and 9</ref> show that the only metric that maintains correct ranking among top models consistently well across all NAS benchmarks is synflow. In Section 5 we deliberately evaluate 3 benchmarks that exhibit different levels of rank correlation: NAS-Bench-201/101/ASR to see if we can integrate synflow within NAS and achieve consistent gains for all three search spaces.</p><p>5 ZERO-COST NAS <ref type="bibr" target="#b20">Mellor et al. (2020)</ref> proposed using jacob cov to score a set of randomly-sampled models and to greedily choose the model with the highest score. This "NAS without training" methodology is very attractive thanks to its simplicity and low computational cost. In this section, we evaluate our metrics in this setting that we simply call "random search" (RAND). We extend this methodology slightly: instead of just training the top model, we keep training models (from best to worst as ranked by the zero-cost metric) until the desired accuracy is achieved. However, this approach can only produce results that are as good as the metric being used -and we have no guarantees (just empirical evidence) that these metrics will perform well on all datasets. Therefore, we also investigate how to integrate zero-cost metrics within existing NAS algorithms such as reinforcement learning (RL) <ref type="bibr" target="#b37">(Zoph &amp; Le, 2017)</ref>, aging evolution (AE) search <ref type="bibr" target="#b25">(Real et al., 2019)</ref> and predictor-based search <ref type="bibr" target="#b5">(Dudziak et al., 2020)</ref>. More specifically, we investigate enhancing these search algorithms through either (a) zero-cost warmup phase or (b) zero-cost move proposal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">ZERO-COST WARMUP</head><p>Generally speaking, by warmup we mean using the zero-cost proxies at the beginning of the search process to initialize the search algorithm without training any models or using accuracy. The main parameter in zero-cost warmup is the number of models for which we compute and use the zero-cost metric (N ), and the potential gain comes from the fact that this number can be usually much larger than the number of models we can afford to train (T N ).</p><p>Aging Evolution We score N random models with our proxy metric and choose the ones ranked highest as the initial population (pool) in the aging evolution (AE) algorithm <ref type="bibr" target="#b25">(Real et al., 2019)</ref>.  Binary Predictor We warm up a binary graph convolutional network (GCN) predictor (from Dudziak et al. ( <ref type="formula">2020</ref>)) by training it to predict relative performance of two models by considering their zero-cost scores instead of accuracy. For N warmup points, we use the relative rankings (according to the zero-cost metric) of all pairs of models (0.5N (N − 1) pairs) when performing warmup training for the predictor. As in <ref type="bibr" target="#b5">(Dudziak et al., 2020)</ref>, models ranked by the predictor after each training round (including the warmup phase) and the top models are evaluated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">ZERO-COST MOVE PROPOSAL</head><p>Whereas warmup tries to leverage global correlation of the proxy metrics to the accuracy of models, move proposal focuses on a local neighborhood at each step. A common parameter for move proposal algorithms is denoted as R and means sample ratio, i.e., how many models can be checked using zero-cost metrics each time we select a model to train.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Aging Evolution</head><p>The algorithm is enhanced by performing "guided" mutations. More specifically, each time a model is being mutated (in the baseline algorithm this is done randomly) we consider all possible mutations with edit distance 1 from the current model, score them using the zero-cost proxies and select the best one to add to the pool.</p><p>Reinforcement Learning In the case of REINFORCE, move proposal is similar to warmupinstead of rewarding a controller N time before the search begins, we interleave R zero-cost rewards for each accuracy reward (R N ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">RESULTS</head><p>For all NAS experiments, we repeat experiments 32 times and we plot the median and shade between the lower/upper quartiles. Our baselines are already heavily tuned and achieve the same or better results than those reported in the original NAS-Bench-101/201 papers. When adding zero-cost warmup or move proposal with synflow, we leave all search hyper-parameters unchanged.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NAS-Bench-201</head><p>The global/top-10% rank correlations of synflow for this dataset are (0.76/0.42) so we expect this proxy to perform quite well. Indeed, as Figure <ref type="figure" target="#fig_2">4</ref> and Table <ref type="table" target="#tab_5">3</ref> show, we improve search speed on all four types of searches using zero-cost warmup and move proposal. RAND and RL are both significantly improved, both in terms of sample efficiency and final achieved accuracy. But even more powerful algorithms like AE and BP exhibit 5.2× and 1.5× speedups respectively to arrive at 73% accuracy. Generally, the more zero-cost warmup, the better the results. This holds true for all algorithms except RL which degrades at 15k warmup points, suggesting that the controller is overfitting to the synflow metric instead of learning to optimize for accuracy. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NAS-Bench-101</head><p>This dataset is an order of magnitude larger than NAS-Bench-201 and has lower global/top-10% rank correlations of (0.37/0.14). In many ways, this provides a true test as to whether these lower correlations are still useful with zero-cost warmup and move proposal. Table <ref type="table" target="#tab_6">4</ref> shows a summary of the results and Figure <ref type="figure" target="#fig_5">7</ref> (in Section A.4) shows the full plots. As the table shows, even with modest correlations, there is a major boost to all searching algorithms thus outperforming the best previously published result by a large margin and setting a new state-of-the-art result on this dataset. However, it is worth noting that the binary predictor exhibits no improvement (but also no degradation). Perhaps this is because it was already very sample-efficient and synflow warmup couldn't help further due to its relatively poor correlation on this dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSION</head><p>In this section we investigate why zero-cost NAS is effective in improving the sample efficiency of NAS algorithms by looking more closely at how top models are selected by the synflow proxy.  <ref type="table" target="#tab_8">6</ref> shows the number of top-5% most-accurate models ranked within the top 64 models by the synflow metric. If we compare random warmup versus zero-cost warmup with synflow, random warmup will only return 5% or ~3 models out of 64 that are within the top 5% of models whereas synflow warmup returns a higher number of top-5% models as listed in Table <ref type="table" target="#tab_8">6</ref>. This is key to the improvements observed when adding zero-cost warmup to algorithms like random search or AE. For example, with AE, the numbers in Table <ref type="table" target="#tab_8">6</ref> are indicative of the models that may end up in the initial AE pool. By initializing the AE pool with many good models, it becomes more likely that a random mutation will lead to an even better model, thus allowing the search to find a top model more quickly. Note that synflow is able to rank many good models in its top 64 models even when global/local correlation is low (as it is the case for NAS-Bench-ASR). Move Proposal For a search algorithm like AE, search moves consist of random mutations (with edit distance 1 for our experiments) for a model from the AE pool. Zero-cost move proposal enhances this by trying out all possible mutations and selecting the best one according to synflow.</p><p>To investigate how this improves search efficiency, we took 1000 random points and explored their local neighbourhood cluster of possible mutations. Table <ref type="table">7</ref> shows the probability that the synflow proxy correctly identifies the top model. Indeed, synflow improves the chance of selecting the best mutation from ~4% for NAS-Bench-201/101 to &gt;30% and 12%. Even for NAS-Bench-ASR a random mutation has a 7.7% chance (= 1/13) to select the best mutation, but this increases to 10% with the synflow proxy thus speeding up convergence to top models.</p><p>Table <ref type="table">7</ref>: For 1000 clusters of models with edit distance 1, we empirically measure the probability that the synflow proxy will select the most accurate model from each cluster. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>In this paper, we introduced six zero-cost proxies, mainly based on recent pruning-at-initialization work, that are used to rank neural network models in NAS. First, we compared to conventional proxies (EcoNAS) that perform reduced-computation training and we found that zero-cost proxies such as synflow can outperform EcoNAS in maintaining rank consistency. Next, we verified our zero-cost metrics on four additional datasets of varying sizes and tasks and found that indeed out of the six initially-considered zero-cost metrics, only synflow was robust across all datasets for both global and top-10% rank correlation. Finally, we proposed two ways to integrate synflow within NAS algorithms: zero-cost warmup and zero-cost move proposal. Both methods demonstrated significant speedups across four search algorithms and three NAS benchmarks, setting new state-of-the-art results for both NAS-Bench-101 and NAS-Bench-201 datasets. Our strong and consistent empirical results suggest that the synflow metric, when combined with warmup and move proposal can be an effective and reliable methodology for speeding up different NAS algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX</head><p>A.1 EXPERIMENTAL DETAILS</p><p>In Table <ref type="table" target="#tab_10">8</ref> we list the hyper-parameters used in training the EcoNAS proxies to produce Figure <ref type="figure" target="#fig_0">1</ref>.</p><p>The only difference to the standard NAS-Bench-201 training pipeline <ref type="bibr" target="#b4">(Dong &amp; Yang, 2020)</ref> is our use of fewer epochs for the learning rate annealing schedule -we anneal the learning rate to zero over 40 epochs instead of 200. This is a common technique used in speeding up convergence for training proxies <ref type="bibr" target="#b36">Zhou et al. (2020)</ref>. We acknowledge that slightly better correlations could have been achieved for econas and econas+ proxies in Figure <ref type="figure" target="#fig_0">1</ref> if the learning rate was annealed to zero over fewer epochs (20 and 15 epochs respectively). However, we do not anticipate the results to change significantly.  <ref type="figure" target="#fig_4">6</ref> shows the speedup of different EcoNAS proxies compared to baseline training. Even though r 8 c 4 has 64× less computation compared to r 32 c 16 , it achieves a maximum of 4× real speedup even when the batch size is increased. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 ANALYSIS OF THE TOP 10% OF MODELS</head><p>In the main text we pointed to the fact that only synflow achieves consistent rank correlation for the top-10% of models across different datasets. Here, in Table <ref type="table" target="#tab_11">9</ref> we provide the full results. Additionally, we hypothesized that a successful metric will rank many of the most-accurate models in its top models. In Table <ref type="table" target="#tab_1">10</ref> we enumerate the percentage of top-10% most accurate models ranked as top-10% by each proxy metric. Again, synflow is the only consistent metric for all datasets, and performs best on average. Table <ref type="table" target="#tab_12">11</ref> shows the number of top-5% models ranked in the top 64 models by each metric. This is an extension to Table <ref type="table" target="#tab_8">6</ref> in the main text that only shows the results for synflow.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 SENSITIVITY ANALYSIS</head><p>We performed some sensitivity analysis to investigate how the zero-cost metrics perform on all points within NAS-Bench-201 with different initialization seed, initialization method and minibatch size. We comment on each table in its caption; however, to summarize, all metrics seem to be relatively unaffected when initialization and minibatch size are varied. The one exception can be seen in Table <ref type="table" target="#tab_15">15</ref> where fisher benefits when biases are initialized with zeroes. Figure <ref type="figure">8</ref>: Evaluation of all zero-cost proxies on different datasets and search algorithms: random search (RAND) and aging evolution (AE). RAND benefits greatly from a strong metric (such as synflow) but may deteriorate with a weaker metric as shown in the plot. However, AE benefits when a strong metric is used and is resilient to weaker metrics as well -it is able to recover and achieve the top accuracy in most cases.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Evaluation of different econas proxies on NAS-Bench-201 CIFAR-10. FLOPS and runtime are normalized to the FLOPS/runtime of a single baseline (full training) epoch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: Correlation of validation accuracy to final test accuracy during the first 12 epochs of training for three datasets on the NAS-Bench-201 search space. Zero-cost and EcoNAS proxies are also labeled for comparison.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Search speedup with the synflow zero-cost proxy on NAS-Bench-201 CIFAR-100.Reinforcement Learning In the REINFORCE algorithm<ref type="bibr" target="#b37">(Zoph &amp; Le (2017)</ref>), we sample N random models and use their zero-cost scores to reward the controller, thus biasing it towards selecting architectures which are likely to have higher values of the chosen metrics. During warmup, reward for the controller is calculated by linearly normalizing values returned by the proxy functions to the range [−1, 1] (with online adjustment of min and max).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Search speedup with the synflow zero-cost proxy on NAS-Bench-ASR TIMIT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Higher batch sizes when training econas proxies have diminishing returns in terms of measured speedup. This measurement is done for 10 randomly-sampled NAS-Bench-201 models on the CIFAR-10 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Search speedup with the synflow zero-cost proxy on NAS-Bench-101 CIFAR-10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Spearman ρ of zero-cost proxies on NAS-Bench-201.</figDesc><table><row><cell>Dataset</cell><cell cols="6">grad norm snip grasp fisher synflow jacob cov vote</cell></row><row><cell>CIFAR-10</cell><cell>0.58</cell><cell>0.58 0.48</cell><cell>0.36</cell><cell>0.74</cell><cell>0.73</cell><cell>0.82</cell></row><row><cell>CIFAR-100</cell><cell>0.64</cell><cell>0.63 0.54</cell><cell>0.39</cell><cell>0.76</cell><cell>0.71</cell><cell>0.83</cell></row><row><cell>ImageNet16-120</cell><cell>0.58</cell><cell>0.58 0.56</cell><cell>0.33</cell><cell>0.75</cell><cell>0.71</cell><cell>0.82</cell></row><row><cell>4.1.1 ECONAS PROXY</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>ON NAS-BENCH-201Even though<ref type="bibr" target="#b36">Zhou et al. (2020)</ref> thoroughly investigated reduced-training proxies, they only evaluated a small model zoo consisting of 50 models. To study EcoNAS more extensively we evaluate it on all 15,625 models in NAS-Bench-201 search space (training details in A.1). The full configuration training of NAS-Bench-201 on CIFAR-10 uses input resolution r=32, number of channels in the stem convolution c=16 and number of epochs e=200 -we summarize this as: r 32 c 16 e 200 .</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Spearman ρ of zero-cost proxies on other NAS search spaces.</figDesc><table><row><cell></cell><cell cols="6">grad norm snip grasp fisher synflow jacob cov</cell></row><row><cell>NAS-Bench-101</cell><cell>0.20</cell><cell>0.16</cell><cell>0.45</cell><cell>0.26</cell><cell>0.37</cell><cell>0.38</cell></row><row><cell>NAS-Bench-NLP</cell><cell>-0.21</cell><cell cols="2">-0.19 0.16</cell><cell>-</cell><cell>0.34</cell><cell>0.56</cell></row><row><cell>NAS-Bench-ASR</cell><cell>0.06</cell><cell>-0.01</cell><cell>-</cell><cell>0.01</cell><cell>0.38</cell><cell>-0.38</cell></row></table><note>Compared to NAS-Bench-201, these datasets are either much larger (NAS-Bench-101) or based on a different task (NAS-Bench-NLP/ASR). From Table</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Zero-cost NAS comparison with baseline algorithms on NAS-Bench-201 CIFAR-100. We show accuracy after 50 trained models and the number of models to reach 73% accuracy.</figDesc><table><row><cell>Baseline</cell><cell>Warmup 1000 (BP=256) 3000 (BP=512)</cell><cell>15k</cell><cell>10</cell><cell>Move</cell><cell>100</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Comparison to prior work on NAS-Bench-101 dataset.</figDesc><table><row><cell></cell><cell>Wen et al.</cell><cell>Wei et al.</cell><cell>Dudziak</cell><cell></cell><cell>Ours</cell><cell></cell></row><row><cell></cell><cell>(2019)</cell><cell>(2020)</cell><cell>et al. (2020)</cell><cell cols="3">RL+M (100) AE+W (15k) RAND+W (3k)</cell></row><row><cell># Trained Models</cell><cell>256</cell><cell>150</cell><cell>140</cell><cell>51</cell><cell>50</cell><cell>34</cell></row><row><cell>Test Accuracy [%]</cell><cell>94.17</cell><cell>94.14</cell><cell>94.22</cell><cell>94.22</cell><cell>94.22</cell><cell>94.22</cell></row><row><cell cols="7">NAS-Bench-ASR We repeat our evaluation on NAS-Bench-ASR with global/top-10% correla-</cell></row><row><cell cols="7">tions (0.38/0.03). Even though this is a different task (speech recognition) and the top-10% cor-</cell></row><row><cell cols="7">relation is effectively nil, synflow warmup and move proposal both yield large improvements in</cell></row><row><cell cols="7">search speeds compared to all baselines in Figure 5 and Table 5. For example, to achieve a phoneme</cell></row><row><cell cols="7">error rate (PER) of 21.3%, baseline RAND, AE and RL required 560, 177 and &gt;1000 trained mod-</cell></row></table><note>els; however, this is reduced to 92, 78 and 72 samples with 2000 models of zero-cost warmup.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Zero-cost NAS comparison with baseline algorithms on NAS-Bench-ASR. We show PER after 50 trained models and the number of models to reach PER=21.3%.</figDesc><table><row><cell></cell><cell>Baseline</cell><cell>500</cell><cell cols="2">Warmup</cell><cell>2000</cell><cell>10</cell><cell>Move</cell><cell>100</cell></row><row><cell cols="2">RAND 21.63 / 560</cell><cell cols="4">21.37 / 509 21.36 / 92 -</cell><cell></cell><cell>-</cell></row><row><cell>RL</cell><cell cols="7">21.69 / 1000+ 21.52 / 349 21.45 / 78 21.60 / 1000+ 21.45 / 1000+</cell></row><row><cell>AE</cell><cell>21.76 / 177</cell><cell cols="2">21.40 / 87</cell><cell cols="3">21.36 / 72 21.59 / 142</cell><cell>-</cell></row><row><cell>Warmup Table</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Number of top-5% most-accurate models within the top 64 models returned by synflow.</figDesc><table><row><cell></cell><cell>NAS-Bench-201</cell><cell></cell><cell cols="2">NAS-Bench-101 NAS-Bench-ASR</cell></row><row><cell cols="3">CIFAR-10 CIFAR-100 ImageNet16-120</cell><cell></cell><cell></cell></row><row><cell>44</cell><cell>54</cell><cell>56</cell><cell>12</cell><cell>16</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>EcoNAS training hyper-parameters for NAS-Bench-201.</figDesc><table><row><cell>optimizer</cell><cell>SGD</cell><cell>initial LR</cell><cell>0.1</cell></row><row><cell>Nesterov</cell><cell></cell><cell>final LR</cell><cell>0</cell></row><row><cell>momentum</cell><cell>0.9</cell><cell cols="2">LR schedule cosine</cell></row><row><cell>weight decay</cell><cell>0.0005</cell><cell>epochs</cell><cell>40</cell></row><row><cell>random flip</cell><cell cols="2">(p=0.5) batch size</cell><cell>256</cell></row><row><cell>random crop</cell><cell></cell><cell></cell><cell></cell></row><row><cell>A.2 GPU RUNTIME FOR ECONAS</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Figure</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc>Spearman ρ of zero-cost proxies for the top 10% of points on all NAS search spaces.</figDesc><table><row><cell></cell><cell cols="6">grad norm snip grasp fisher synflow jacob cov</cell></row><row><cell>NB2-CIFAR-10</cell><cell>-0.38</cell><cell cols="3">-0.38 -0.37 -0.38</cell><cell>0.18</cell><cell>0.17</cell></row><row><cell>NB2-CIFAR-100</cell><cell>-0.09</cell><cell cols="3">-0.09 -0.11 -0.16</cell><cell>0.42</cell><cell>0.08</cell></row><row><cell>NB2-ImageNet16-120</cell><cell>0.13</cell><cell>0.13</cell><cell>0.10</cell><cell>0.02</cell><cell>0.55</cell><cell>0.05</cell></row><row><cell>NAS-Bench-101</cell><cell>0.05</cell><cell cols="3">-0.01 -0.01 0.07</cell><cell>0.14</cell><cell>0.08</cell></row><row><cell>NAS-Bench-NLP</cell><cell>-0.03</cell><cell cols="2">-0.02 0.04</cell><cell>-</cell><cell>0.10</cell><cell>0.04</cell></row><row><cell>NAS-Bench-ASR</cell><cell>-0.04</cell><cell>-0.17</cell><cell>-</cell><cell>-0.03</cell><cell>0.03</cell><cell>0.02</cell></row><row><cell cols="7">Table 10: Percentage of top-10% most-accurate models within the top-10% of models ranked by</cell></row><row><cell>each zero-cost metric.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="6">grad norm snip grasp fisher synflow jacob cov</cell></row><row><cell>NB2-CIFAR-10</cell><cell>30%</cell><cell cols="2">31% 30%</cell><cell>5%</cell><cell>46%</cell><cell>25%</cell></row><row><cell>NB2-CIFAR-100</cell><cell>35%</cell><cell cols="2">36% 34%</cell><cell>4%</cell><cell>50%</cell><cell>24%</cell></row><row><cell>NB2-ImageNet16-120</cell><cell>31%</cell><cell cols="2">31% 32%</cell><cell>5%</cell><cell>44%</cell><cell>30%</cell></row><row><cell>NAS-Bench-101</cell><cell>2%</cell><cell>3%</cell><cell>26%</cell><cell>3%</cell><cell>23%</cell><cell>2%</cell></row><row><cell>NAS-Bench-NLP</cell><cell>10%</cell><cell>10%</cell><cell>4%</cell><cell>-</cell><cell>22%</cell><cell>38%</cell></row><row><cell>NAS-Bench-ASR</cell><cell>0%</cell><cell>0%</cell><cell>-</cell><cell>0%</cell><cell>15%</cell><cell>44%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 11 :</head><label>11</label><figDesc>Number of top-5% most-accurate models within the top-64 models returned by each metric.</figDesc><table><row><cell></cell><cell cols="6">grad norm snip grasp fisher synflow jacob cov</cell></row><row><cell>NB2-CIFAR-10</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>44</cell><cell>15</cell></row><row><cell>NB2-CIFAR-100</cell><cell>4</cell><cell>4</cell><cell>4</cell><cell>0</cell><cell>54</cell><cell>16</cell></row><row><cell>NB2-ImageNet16-120</cell><cell>13</cell><cell>13</cell><cell>14</cell><cell>0</cell><cell>56</cell><cell>15</cell></row><row><cell>NAS-Bench-101</cell><cell>0</cell><cell>0</cell><cell>6</cell><cell>0</cell><cell>12</cell><cell>0</cell></row><row><cell>NAS-Bench-ASR</cell><cell>1</cell><cell>0</cell><cell>-</cell><cell>1</cell><cell>16</cell><cell>13</cell></row><row><cell cols="7">Table 12: Rank correlation coefficient for the local neighbourhoods (edit distance = 1) of 1000 points</cell></row><row><cell>in each search space.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="6">grad norm snip grasp fisher synflow jacob cov</cell></row><row><cell>NB2-CIFAR-10</cell><cell>0.51</cell><cell cols="2">0.51 0.37</cell><cell>0.37</cell><cell>0.66</cell><cell>0.62</cell></row><row><cell>NB2-CIFAR-100</cell><cell>0.58</cell><cell cols="2">0.58 0.44</cell><cell>0.41</cell><cell>0.69</cell><cell>0.61</cell></row><row><cell>NB2-ImageNet16-120</cell><cell>0.56</cell><cell>0.57</cell><cell>0.5</cell><cell>0.4</cell><cell>0.67</cell><cell>0.61</cell></row><row><cell>NAS-Bench-101</cell><cell>0.23</cell><cell cols="2">0.21 0.44</cell><cell>0.27</cell><cell>0.36</cell><cell>0.37</cell></row><row><cell>NAS-Bench-ASR</cell><cell>0.59</cell><cell>0.4</cell><cell>-</cell><cell>0.56</cell><cell>0.38</cell><cell>0.28</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 13 :</head><label>13</label><figDesc>For 1000 clusters of points with edit distance = 1. We count the number of times wherein the top model returned by a zero-cost metric matches the top model according to validation accuracy. This represents the probability that zero-cost move proposal will perform the best possible mutation.Figure7shows the NAS search curves for all considered algorithms on NAS-Bench-101 dataset. Important points from this plot are summarized in Table4in the main text.</figDesc><table><row><cell></cell><cell>grad norm</cell><cell>snip</cell><cell>grasp</cell><cell cols="3">fisher synflow jacob cov</cell></row><row><cell>NB2-CIFAR-10</cell><cell>14.8%</cell><cell cols="3">14.8% 12.7% 5.7%</cell><cell>32.2%</cell><cell>14.5%</cell></row><row><cell>NB2-CIFAR-100</cell><cell>19.1%</cell><cell cols="3">18.5% 14.2% 6.0%</cell><cell>35.4%</cell><cell>13.8%</cell></row><row><cell>NB2-ImageNet16-120</cell><cell>17.5%</cell><cell cols="3">18.5% 15.7% 5.5%</cell><cell>33.4%</cell><cell>16.7%</cell></row><row><cell>NAS-Bench-101</cell><cell>0.4%</cell><cell>0.9%</cell><cell>7.4%</cell><cell>0.5%</cell><cell>12.3%</cell><cell>0.5%</cell></row><row><cell>NAS-Bench-ASR</cell><cell>11.0%</cell><cell>9.8%</cell><cell>-</cell><cell cols="2">10.3% 10.3%</cell><cell>10.5%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 14 :</head><label>14</label><figDesc>All metrics remain fairly constant when varying the initialization seed -the variations are only observed at the third significant digit. Dataload is random with 128 samples and initialization is done with default PyTorch initialization scheme.</figDesc><table><row><cell>seed</cell><cell cols="2">grad norm snip</cell><cell cols="3">grasp fisher synflow jacob cov</cell></row><row><cell>1</cell><cell>0.578</cell><cell cols="2">0.581 0.487 0.361</cell><cell>0.737</cell><cell>0.735</cell></row><row><cell>2</cell><cell>0.580</cell><cell cols="2">0.583 0.488 0.354</cell><cell>0.740</cell><cell>0.728</cell></row><row><cell>3</cell><cell>0.582</cell><cell cols="2">0.584 0.486 0.358</cell><cell>0.738</cell><cell>0.726</cell></row><row><cell>4</cell><cell>0.581</cell><cell cols="2">0.584 0.491 0.356</cell><cell>0.738</cell><cell>0.73</cell></row><row><cell>5</cell><cell>0.581</cell><cell cols="2">0.583 0.486 0.356</cell><cell>0.738</cell><cell>0.727</cell></row><row><cell>Average</cell><cell>0.580</cell><cell cols="2">0.583 0.488 0.357</cell><cell>0.738</cell><cell>0.729</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 15 :</head><label>15</label><figDesc>fisher becomes noticeably better when biases are initialized to zero; otherwise, metrics seem to perform independently of initialization method. Results averaged over 3 seeds.</figDesc><table><row><cell cols="4">Weights init Bias init grad norm snip</cell><cell cols="3">grasp fisher synflow jacob cov</cell></row><row><cell>default</cell><cell>default</cell><cell>0.580</cell><cell cols="2">0.583 0.488 0.357</cell><cell>0.738</cell><cell>0.729</cell></row><row><cell>kaiming</cell><cell>default</cell><cell>0.548</cell><cell cols="2">0.558 0.364 0.332</cell><cell>0.731</cell><cell>0.723</cell></row><row><cell>xavier</cell><cell>default</cell><cell>0.543</cell><cell cols="2">0.568 0.424 0.345</cell><cell>0.736</cell><cell>0.729</cell></row><row><cell>default</cell><cell>zero</cell><cell>0.581</cell><cell cols="2">0.583 0.488 0.509</cell><cell>0.738</cell><cell>0.729</cell></row><row><cell>kaiming</cell><cell>zero</cell><cell>0.542</cell><cell cols="2">0.551 0.370 0.479</cell><cell>0.730</cell><cell>0.723</cell></row><row><cell>xavier</cell><cell>zero</cell><cell>0.540</cell><cell cols="2">0.566 0.412 0.495</cell><cell>0.735</cell><cell>0.730</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 16 :</head><label>16</label><figDesc>Surprisingly, grasp becomes worse with more (random) data, while grad norm and snip degrade very slightly. Other metrics seem to perform independently of the number of samples in the minibatch. Initialization is done with default PyTorch initialization scheme. RESULTS FOR ALL ZERO-COST METRICS Here we provide some NAS search results using all considered metrics for both RAND and AE searches on NAS-Bench-101/201 datasets.</figDesc><table><row><cell cols="3">Number of Samples grad norm snip</cell><cell cols="3">grasp fisher synflow jacob cov</cell></row><row><cell>32</cell><cell>0.595</cell><cell cols="2">0.596 0.511 0.362</cell><cell>0.737</cell><cell>0.732</cell></row><row><cell>64</cell><cell>0.589</cell><cell cols="2">0.59 0.509 0.361</cell><cell>0.737</cell><cell>0.735</cell></row><row><cell>128</cell><cell>0.578</cell><cell cols="2">0.581 0.487 0.361</cell><cell>0.737</cell><cell>0.735</cell></row><row><cell>256</cell><cell>0.564</cell><cell cols="2">0.569 0.447 0.361</cell><cell>0.737</cell><cell>0.731</cell></row><row><cell>512</cell><cell>0.547</cell><cell cols="2">0.552 0.381 0.361</cell><cell>0.737</cell><cell>0.724</cell></row><row><cell>A.6</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">The full Hessian does not need to be explicitly constructed as explained by<ref type="bibr" target="#b23">Pearlmutter (1993)</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">We used Nvidia Geforce GTX 1080 Ti and ran a random sample of 10 models for 10 epochs to get an average time-per-epoch for each proxy at different batch sizes. We discuss this further in Section A.2</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Understanding and simplifying one-shot architecture search</title>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Efficient architecture search by network transformation</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Applied Nonparametric Statistics</title>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>PWS-Kent</publisher>
			<pubPlace>Boston</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">NAS-Bench-201: Extending the Scope of Reproducible Neural Architecture Search</title>
		<author>
			<persName><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">BRP-NAS: Prediction-based NAS using GCNs</title>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Dudziak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Chau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><forename type="middle">S</forename><surname>Abdelfattah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Royson</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyeji</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><forename type="middle">D</forename><surname>Lane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Perforatedcnns: Acceleration through elimination of redundant convolutions</title>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Figurnov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aizhan</forename><surname>Ibraimova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><forename type="middle">P</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The lottery ticket hypothesis: Finding sparse, trainable neural networks</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Carbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Timit acoustic phonetic continuous speech corpus</title>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">S</forename><surname>Garofolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lori</forename><forename type="middle">F</forename><surname>Lamel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">M</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">G</forename><surname>Fiscus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">S</forename><surname>Pallett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nancy</forename><forename type="middle">L</forename><surname>Dahlgren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Zue</surname></persName>
		</author>
		<ptr target="https://catalog.ldc.upenn.edu/LDC93S1" />
	</analytic>
	<monogr>
		<title level="m">Linguistic Data Consortium</title>
				<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning both weights and connections for efficient neural network</title>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Second order derivatives for network pruning: Optimal brain surgeon</title>
		<author>
			<persName><forename type="first">Babak</forename><surname>Hassibi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">G</forename><surname>Stork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 5</title>
				<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2016-06">June 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2017-07">July 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Nas-bench-nlp: Neural architecture search benchmark for natural language processing</title>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Klyuchnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Trofimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ekaterina</forename><surname>Artemova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Salnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgeny</forename><surname>Burnaev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07116</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Learning Multiple Layers of Features from Tiny Images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<ptr target="https://www.cs.toronto.edu/˜kriz/cifar.html" />
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Optimal brain damage</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><forename type="middle">A</forename><surname>Solla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Snip: Single-shot network pruning based on connection sensitivity</title>
		<author>
			<persName><forename type="first">Namhoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thalaiyasingam</forename><surname>Ajanthan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Hs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torr</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">DARTS: Differentiable architecture search</title>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">AtomNAS: Fine-grained end-to-end neural architecture search</title>
		<author>
			<persName><forename type="first">Jieru</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaochen</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojie</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Mellor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amos</forename><surname>Storkey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elliot</forename><forename type="middle">J</forename><surname>Crowley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04647</idno>
		<title level="m">Neural architecture search without training</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pruning convolutional neural networks for resource efficient inference</title>
		<author>
			<persName><forename type="first">Pavlo</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Reading Digits in Natural Images with Unsupervised Feature Learning</title>
		<author>
			<persName><forename type="first">Yuval</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS Workshop on Deep Learning and Unsupervised Feature Learning</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fast Exact Multiplication by the Hessian</title>
		<author>
			<persName><forename type="first">A</forename><surname>Barak</surname></persName>
		</author>
		<author>
			<persName><surname>Pearlmutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Efficient neural architecture search via parameter sharing</title>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melody</forename><forename type="middle">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Regularized Evolution for Image Classifier Architecture Search</title>
		<author>
			<persName><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alok</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">PyTorchCV Convolutional neural networks for computer vision</title>
		<author>
			<persName><forename type="first">Oleg</forename><surname>Sémery</surname></persName>
		</author>
		<ptr target="https://github.com/osmr/imgclsmob" />
		<imprint>
			<date type="published" when="2020-08">August 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">EfficientNet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2019">2019a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Mixconv</surname></persName>
		</author>
		<title level="m">Mixed depthwise convolutional kernels</title>
				<imprint>
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Pruning neural networks without any data by iteratively conserving synaptic flow</title>
		<author>
			<persName><forename type="first">Hidenori</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Kunin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Yamins</surname></persName>
		</author>
		<author>
			<persName><surname>Ganguli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.05467</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Faster gaze prediction with dense networks and fisher pruning</title>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Korshunova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ferenc</forename><surname>Huszár</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.05787</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Blockswap: Fisher-guided block substitution for network compression on a budget</title>
		<author>
			<persName><forename type="first">Jack</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elliot</forename><forename type="middle">J</forename><surname>Crowley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O'</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amos</forename><surname>Boyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gavin</forename><surname>Storkey</surname></persName>
		</author>
		<author>
			<persName><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Picking winning tickets before training by preserving gradient flow</title>
		<author>
			<persName><forename type="first">Chaoqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><surname>Grosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">NPENAS: Neural predictor guided evolution for neural architecture search</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiping</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimin</forename><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003.12857, 03 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiran</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.00848</idno>
		<title level="m">Neural predictor for neural architecture search</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">NASbench-101: Towards reproducible neural architecture search</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Christiansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Econas: Finding proxies for economical neural architecture search</title>
		<author>
			<persName><forename type="first">Dongzhan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinchi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuesen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2020-06">June 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
