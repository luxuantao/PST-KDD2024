<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Simplifier for Propositional Formulas with Many Binary Clauses</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ronen</forename><forename type="middle">I</forename><surname>Brafman</surname></persName>
						</author>
						<title level="a" type="main">A Simplifier for Propositional Formulas with Many Binary Clauses</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D9E6CB8452284567F8B96E6F415CAC89</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deciding whether a propositional formula in conjunctive normal form is satisfiable (SAT) is an NP-complete problem. The problem becomes linear when the formula contains binary clauses only. Interestingly, the reduction to SAT of a number of well-known and important problems -such as classical AI planning and automatic test pattern generation for circuits -yields formulas containing many binary clauses. In this paper we introduce and experiment with 2-SIMPLIFY, a formula simplifier targeted at such problems. 2-SIMPLIFY constructs the transitive closure of the implication graph corresponding to the binary clauses in the formula and uses this graph to deduce new unit literals. The deduced literals are used to simplify the formula and update the graph, and so on, until stabilization. Finally, we use the graph to construct an equivalent, simpler set of binary clauses. Experimental evaluation of this simplifier on a number of bench-mark formulas produced by encoding AI planning problems prove 2-SIMPLIFY to be a useful tool in many circumstances.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>P ROPOSITIONAL satisfiability (SAT) is the problem of de- ciding whether a propositional formula in conjunctive normal form (CNF) is satisfiable. SAT was the first problem shown to be NP-complete <ref type="bibr" target="#b5">[6]</ref> and has important practical applications. In the last decade we have witnessed great progress in SAT solution methods, first with the introduction of efficient stochastic local search algorithms <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, and more recently with a number of efficient systematic solvers, such as REL-SAT <ref type="bibr" target="#b3">[4]</ref> and SATZ <ref type="bibr" target="#b15">[16]</ref> and their randomized versions <ref type="bibr" target="#b9">[10]</ref>, and more recently, CHAFF <ref type="bibr" target="#b17">[18]</ref>.</p><p>Among AI researchers, interest in SAT solution algorithms has increased since Kautz and Selman showed that some classical planning problems can be solved more quickly when they are reduced to SAT problems <ref type="bibr" target="#b11">[12]</ref>. Kautz and Selman's planning as satisfiability approach is based on generic SAT technology, and, aside from the translation process itself, makes no use of properties specific to planning problems. However, SAT-encoded planning problems have an important syntactic property: they contain a large fraction of binary clauses. Interestingly, this property is found in other important domainsautomatic test-pattern generation for circuits <ref type="bibr" target="#b14">[15]</ref> and bounded model checking <ref type="bibr" target="#b22">[23]</ref>.</p><p>Unlike the general SAT problem, 2-SAT, the problem of deciding whether a propositional formula containing binary clauses only has a satisfying assignment is (constructively) solvable in linear time. One would hope that this property would make it easier to solve SAT instances containing a large Author's address: Department of Computer Science, Ben-Gurion University, Beer-Sheva, Israel (email :brafman@cs.bgu.ac.il) fraction of binary clauses. In this article we describe the 2-SIMPLIFY preprocessor, a simplifier that is geared to such formulas. Like other simplifiers (e.g., Crawford's COMPACT), this algorithm takes a propositional formula in CNF as input and outputs a new propositional formula. Naturally, for this process to be worthwhile, the new formula should be easier to solve, and the overall time required for simplification and solution of the simplified formula should be less than the time required to solve the original SAT instance. Experiments on a number of bench-mark formulas derived from planning problems show that 2-SIMPLIFY's performance depends on the difficulty of the problem, whether it is satisfiable or not, and the solver used. In many classes of problems, 2-SIMPLIFY leads to a combined simplification and solution time that is lesser than the original solution time.</p><p>2-SIMPLIFY efficiently implements and combines wellknown 2-SAT techniques, a limited form of hyper-resolution, and novel use of transitive reduction to reduce formula size. The basic idea is as follows: each clause of the form Ô Õ is equivalent to two implications: Ô Õ and Õ Ô. We use this property to construct a graph (known as the implication graph <ref type="bibr" target="#b0">[1]</ref>) from the set of binary clauses. This graph contains a node for each literal in the language and a directed edge from a literal Ð to another literal Ð ¼ if the (disjunction equivalent to the) implication Ð Ð ¼ appears among the set of binary clauses. After constructing this graph, we compute its transitive closure and check each literal to see whether its negation appears among its descendants. If Ð is a descendant of Ð, we can immediately conclude that Ð is a consequence of the original formula. Once we know that Ð holds, we can simplify the original formula. The simplified formula may contain new binary clauses, which are immediately added to the graph. 2-SIMPLIFY utilizes these and other ideas to quickly derive unit literals from the original formula and produce a simpler equivalent formula as its output.</p><p>In the next section we discuss the background of this work in more detail. In Section 3 we present the simplification algorithm used by 2-SIMPLIFY. In Section 4 we present some experimental results, and we conclude in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND</head><p>We start with some SAT background and then we briefly explain the planning as satisfiability approach, which motivated this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The SAT Problem</head><p>The SAT problem is defined as follows: given a propositional formula in conjunctive normal form (CNF) output YES if the formula is satisfiable and NO otherwise. In practice, a positive answer is accompanied by some satisfying assignment.</p><p>There are two classes of SAT algorithms: stochastic and systematic. Stochastic methods, such as G-SAT <ref type="bibr" target="#b20">[21]</ref> and WALK-SAT <ref type="bibr" target="#b21">[22]</ref>, perform stochastic local search in the space of truth assignment. Often, they can find solutions quickly, but they cannot identify an unsatisfiable instance. Their performance is extremely sensitive to the choice of heuristic and various other parameters. Systematic methods systematically search the space of truth assignments. Thus, they can identify unsatisfiable instances. Modern systematic algorithms are quite fast and stable thanks to improved branch choice heuristics and backtracking techniques. In addition, systematic solvers can be improved by introducing some randomization into their search procedure, e.g., their choice of branch variable. See <ref type="bibr" target="#b9">[10]</ref> for more information on this topic.</p><p>Often, a formula simplifier is applied before the SAT solver. Simplifiers use specialized, efficient deductive methods to reduce the original formula into a simpler formula which is typically easier to solve. The best known simplification method is unit propagation. When one of the clauses in the formula contains a single literal, it must be assigned the value true in any satisfying assignment. For example, if a formula contains the clause Ô then Ô must be true in any satisfying assignment, i.e., Ô must be false. Once we deduce this fact, we can use it to simplify other clauses: clauses that contain Ô can be removed since their satisfaction is guaranteed when Ô is false, and the literal Ô can be removed from any clause containing it (e.g., Ô × will be transformed into × ) because it is equivalent to false. As we just saw, the simplification process can yield additional unit clauses, which are used to produce additional simplifications. If during the simplification process an empty clause is discovered (e.g., if we assigned × the value true and there is a unit clause × ) we can conclude that the formula is unsatisfiable.</p><p>There are a number of additional simplification methods, such as failed unit literal and failed binary literal, where one or two unit clauses are added to the current formula and we attempt to show (e.g., using unit propagation) that the resulting formula is inconsistent. In that case, the negation of the added clause is implied by the original formula, and we update the truth assignment accordingly. For example, if our original formula becomes inconsistent once we add the clauses Ô Õ , we know that either Ô or Õ must be assigned false, i.e., that Ô Õ is implied by the formula.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. 2-SAT</head><p>2-SAT is a subclass of SAT in which clauses contain no more than two literals. While SAT is NP-complete <ref type="bibr" target="#b5">[6]</ref>, 2-SAT can be solved in linear time. The key step in solving 2-SAT problems is the construction of the implication graph <ref type="bibr" target="#b0">[1]</ref>). The nodes of the implication graph correspond to the literals in the formula. The graph contains an edge between the literal Ð and the literal Ð ¼ if the clause Ð Ð ¼ appears in the formula. That is, edges in the graph correspond to implications (since between node Ð ½ and node Ð ¾ . In particular, if we have a path between Ð ½ and Ð ½ , we know that Ð ½ cannot hold. Therefore, Ð ½ is implied by the formula. If, in addition, we have a path from Ð ½ to Ð ½ , then neither Ð ½ nor Ð ½ can hold, and so the formula is unsatisfiable. Finally, we know that in every satisfying truth assignment, if Ð is assigned true then any literal implied by Ð, i.e., any descendant of Ð in the graph, must be assigned true as well.</p><formula xml:id="formula_0">Ð Ð ¼ is equivalent to Ð Ð ¼ ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Planning As Satisfiability</head><p>The planning problem is defined as follows: given a description of an initial state, a goal state, and a set of operators (=Actions) for changing the state of the world, find a sequence of operators that, when applied in the initial state, yield the goal state. An important development in planning algorithms was Kautz and Selman's planning as satisfiability approach <ref type="bibr" target="#b11">[12]</ref>. Kautz and Selman showed that by reducing planning problems to satisfiability problems, we can often solve them more quickly than by using standard planning algorithms. Planning problems can be encoded as satisfiability problems in a number of ways (e.g., see <ref type="bibr" target="#b7">[8]</ref> for a description and analysis of some of these methods).</p><p>As <ref type="bibr" target="#b4">[5]</ref> points outs, encoded planning problems contain a large number of binary clauses. In Table <ref type="table">1</ref> we show this for a number of instances of SAT-encoded planning problems. This is no accidental phenomenon. Close inspection of the types of constraints expressed within encoded planning problems makes it apparent that many classes of these constraints generate binary formulas. For example, the constraint that if an action is executed at some time point then all its preconditions must hold prior, produces binary clauses. Similarly, the constraint asserting that if an action is executed at some point then all its effects must hold after the execution yields binary clauses as well. In the encoding used by the BLACKBOX planner <ref type="bibr" target="#b12">[13]</ref> mutual exclusion constraints (on actions and on state variables) play a prominent role. These constraints are expressed using binary clauses as well.</p><p>Interestingly, it turns out that the SAT encodings of other important problems exhibit the same large percentage of binary clauses. These include test-pattern generation for circuits <ref type="bibr" target="#b14">[15]</ref> and bounded model checking <ref type="bibr" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. THE 2-SIMPLIFY PREPROCESSOR</head><p>We now explain the algorithm implemented by the 2-SIMPLIFY preprocessor using the following formula:</p><formula xml:id="formula_1">Ô Õ Ô Ö Ö × Û × Ô Ø Ø Ô Ù Ø Ô × Ô × Õ Õ × Ô Ù Û Õ Õ Ö × × Ú Ñ</formula><p>(1) Construct Implication Graph. A graph containing all literals in the language is constructed with directed edges from Ð to Ð ¼ if Ð Ð ¼ is a binary clause. Figure <ref type="figure" target="#fig_0">1</ref> shows the implication graph for the formula above. Note that in the figures, we use Ô to denote Ô.</p><p>(2) Collapse Strongly Connected Components. A subgraph in which there is a path between every pair of nodes is called a strongly connected component (SCC). When a path from node Ð to node Ð ¼ exists, we know that Ð Ð ¼ is a consequence of our formula. Therefore, all nodes within an SCC imply each other, and they must all be assigned the same value.</p><p>Once we discover an SCC we replace it by a single node. The children of this node are the children of the nodes in the SCC, and the parents of this node are the parents of the nodes in this SCC. In addition, all literals in the SCC must be replaced by the literal corresponding to this new node within all non-binary clauses.</p><p>Because of the symmetric nature of the implication graph, for every SCC we discover, another SCC containing the negation of the literals of this SCC exists. Thus, whenever we replace an SCC by a new node labelled by the literal Ð, we replace the symmetric SCC by a new node that is labelled by Ð.</p><p>In our example, the nodes Ø and Ô form a strongly connected component, and so do their negations, Ø and Ô. We choose (3) Generate Transitive Closure. Now, we generate the transitive closure of the graph. This can be done with one traversal of the (now acyclic) graph in reverse topological order (i.e., by adding to the adjacency list of each node the children of its children). We know that if Ð ¼ is a child of Ð then Ð Ð ¼ is implied by the original formula. We can deduce Ð if either:</p><p>1) for some proposition Ô, both Ô and Ô are children of Ð.</p><p>2) Ð is a child of Ð.</p><p>Once we deduce Ð, we can perform unit propagation: all children of Ð are assigned the value true, and we can remove all occurrences of Ð from within the clauses of our formula. If the reduced formula contains new binary clauses, we add the appropriate edges to the graph and update the transitive closure.</p><p>In Figure <ref type="figure">3</ref> we can see the effect of this step. First, we compute the transitive closure of the current graph, shown in Fig- <ref type="figure">ure 3A</ref>. In this graph, we see that Ù has Ù as a descendant and that Ô has Ô as a descendant. Therefore, we conclude that Ô and Ù must be assigned the value false. We can remove nodes that correspond to assigned propositions (i.e., Ô Ù Ô Ù in our case). The resulting graph is shown in Figure <ref type="figure">3B</ref>. Next, we perform unit propagation, and our initial ternary clauses:</p><formula xml:id="formula_2">Ô × Õ Õ × Ô Ù Û Õ Õ Ö × are reduced to Õ × Û Õ Õ Ö ×</formula><p>The first clause was removed because it is satisfied, and a (false) literal was removed from the next two clauses. Since we have new binary clauses, we can update the graph, as shown in Figure <ref type="figure">3C</ref>, making sure it is transitively closed. In the resulting graph, Û is a child of Û, and we can deduce that Û false. The reduced graph is shown in Figure <ref type="figure">3D</ref>. ´ Õµ ´ Ö × Õµ ´× Õµ, respectively.</p><p>Their intersection contains Õ. Hence, we can deduce that Õ is false.</p><formula xml:id="formula_3">Subsumption Elimination. A clause Ð ½ Ð is subsumed by ¼ Ð ¼ ½ Ð ¼ Ñ if Ñ and Ð ¼ ¾ Ð ½ Ð</formula><p>for every ½ Ñ. If ¼ subsumes , then the constraint ¼ is stronger than . Thus, is redundant.</p><p>We use the following method to quickly detect whether one non-binary clause is subsumed by some binary clause implied by the implication graph. Given a clause Ð ½ Ð , we generate a set Ë containing all the children of the negations of the literals in , i.e., the children of Ð ½</p><p>Ð . If any of the literals in appears in Ë , we know that is subsumed.</p><p>(6) Pure-literal Removal. A pure literal is a literal whose negation does not occur in any of the clauses in the formula. In that case, we can assign that literal the value true without affecting the satisfiability of the formula. ( <ref type="formula">7</ref>) Compute Transitive Reduction. The transitive reduction of a graph is a graph ¼ with the same nodes as but with a minimal set of edges such that a path between Ð and Ð ¼ exists in iff a path between Ð and Ð ¼ exists in ¼ . Thus, ¼ is a minimal sub-graph of that maintains node connectivity.</p><p>We compute the transitive reduction of the current graph in order to reduce the size of the formula. Our implementation of this step relies on the fact that we start with a transitively closed graph. Thus, if we remove from the list of children of a node all of its grandchildren, starting at the root nodes and progressing in topological order, we obtain a reduced graph.</p><p>(8) Output Simplified Formula. We output a formula whose clauses consist of the non-binary clauses remaining at this stage, and all binary clauses corresponding to edges in the transitive reduction of the graph. Given the assignments deduced so far and the mappings between elements of strongly connected components, the simplified formula is equivalent to the original formula. For example, the output for our original formula will be: Ö × × Ú Ñ together with the partial assignment</p><formula xml:id="formula_4">Ù false Ô false Û false Õ false.</formula><p>Step ( <ref type="formula">4</ref>) is a novel implementation of an old technique (hyper-resolution <ref type="bibr" target="#b19">[20]</ref>) and step ( <ref type="formula">7</ref>) is new. Both have important impact on 2-SIMPLIFY's performance. The Derive Shared Implications step enhances the ability of 2-SIMPLIFY to derive unit literals. In some cases, it can derive hundreds of new unit literals quickly. In fact, 2-SIMPLIFY uses a more sophisticated version of this procedure: if no shared unit literals exist, we attempt to derive new binary clauses by intersecting the implications of all literals but one. These binary clauses are then added to the implication graph. The Compute Transitive Reduction step leads to a minimal sufficient set of binary clauses, leading to smaller and simpler formulas. We have found this reduction to have an important positive influence on systematic solvers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL EVALUATION</head><p>We run extensive tests that examine different aspects of 2-SIMPLIFY on a set of bench-mark instances of encoded planning problems which were used to test the REL-SAT solver. In addition, to see the usefulness of 2-SIMPLIFY on other problems we checked its performance on a host of verification bench-mark problems.</p><p>The experiments in the first section below, dealing with encoded planning problem were carried out on a DELL Latitude CP notebook with a Pentium II-400 processor with 64MB RAM running Linux. These problems were obtained from tt ftp://ftp.research.att.com/dist/ai/logistics/tar. <ref type="bibr">Z</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Encoded Planning Problems</head><p>First, we examined 2-SIMPLIFY's ability to deduce unit literals. In Table <ref type="table">2</ref> we show 2-SIMPLIFY's running time on each of the instances, the number of variables it was able to assign or map, and the ratio between the number of clauses in the simplified formula and the original formula.</p><p>To assess the utility of 2-SIMPLIFY we generated simplified formulas for each of the instances and compared the solution time of the original formulas with the combined simplification and solution times for the simplified formulas. We performed this comparison using two systematic solvers: SATZ and REL-SAT. The results for SATZ are shown in Table <ref type="table">3</ref>. In 5 out of the 14 problems, the use of 2-SIMPLIFY leads to degraded performance. This occurs on the relatively smaller problems, and typically, with a small overhead that stems from simplification costs. On 9 out of the 14 problems, 2-SIMPLIFY leads to improved performance. Of particular importance is the fact that 2-SIMPLIFY performs better on the problems that harder for SATZ and on those that SATZ cannot solve without simplification.</p><p>The results for REL-SAT are shown in Table <ref type="table">4</ref>. The simplified formulas were solved using REL-SAT, but with its preprocessor disabled. In many cases, this leads to improved performance, a natural consequence of the fact that 2-SIMPLIFY provides stronger simplification capabilities. For example, in the largest instance, bw-dir.d, REL-SAT without the preprocessor took 571 seconds, instead of 847. However, there are cases in which the REL-SAT preprocessor leads to better running times on the simplified formulas. For instance, in the log-gp and log-un instances, it was always better. Overall, we see that  2-SIMPLIFY+REL-SAT is almost always faster than REL-SAT alone, with three exceptions that stem from relatively long simplification times. The improvement is especially significant in the hardest instances. We note that the REL-SAT figures represent average running times (because REL-SAT has a stochastic element) and that in the case of bw-dir.d, REL-SAT timed out on the original problem in some of the iterations and the result provided is a lower bound on its true average running time.</p><p>We run another sequence of experiments to compare 2-SIMPLIFY with Crawford's COMPACT simplifier. COMPACT provides various simplification options. The basic COMPACT simplifier performs unit resolution, removes satisfied clauses, and renames variables to be contiguous. In addition, there are a number of optional flags. With the Ô flag, COMPACT performs pure-literal elimination, with × it resolves away literals that occur only once, with Ð it performs the unit-failed test -that is, it checks for each literal whether adding this literal to the formula leads to an inconsistency (using unit propagation). If so, it assigns that literal the value false. Finally, the option adds the binary-failed test. In this case, pairs of literals are added each time, and unit resolution is performed. If an inconsistency is detected, the negation of the conjunction of this pair of literals  is added to the formula. This latter test is quite powerful, but it is almost always too costly to be worthwhile. After testing various combinations of options on the above instances, we found that the best performance is obtained almost always using either no flags or using the Ô×Ð flags, and this is what we show here.</p><p>In Tables <ref type="table">5</ref> and<ref type="table">6</ref> below, we compare the simplification + running times of SATZ and REL-SAT on COMPACT simplified formulas and on the 2-SIMPLIFY simplified formulas. In Table 5, we see the results for SATZ. The columns correspond to the combined running time of SATZ and the simplification algorithms on each of the instances. There are three cases in which COMPACT with no options leads to better performance, but overall, 2-SIMPLIFY leads to much better results, especially on the more difficult instances. The results when the Ô×Ð option is used are more varied. 2-SIMPLIFY still does better in more cases, but on the last two instances, COMPACT with Ô×Ð does much better. This may indicate that an enhanced version of 2-SIMPLIFY with Ô×Ð-like capabilities could perform better than either simplifiers. However, as we shall see below, the relative performance is greatly affected by the solver used.</p><p>In Table <ref type="table">6</ref> we show the corresponding results for REL-SAT (without its preprocessor). Again, we see that 2-SIMPLIFY is better than COMPACT with no options, and that 2-SIMPLIFY and Ô×Ð succeed on different instances. However, notice that Ô×Ð does noticeably better only on 3 instances. Moreover, notice the large change in performance on the two problems that were most difficult for SATZ -log-un.b/c. Again, it would be excellent if we could get the best of both worlds.</p><p>Finally, we examined 2-SIMPLIFY's influence on the performance of WALKSAT, a stochastic solver. As noted, stochastic solvers require tuning, and we tried to find the best parameters in each case. As Table <ref type="table">7</ref> shows, the results are, again, quite positive. Although there are three instances in which 2-SIMPLIFY leads to reduced performance, on most instances it leads to improved performance. Moreover, on all the harder instances, 2-SIMPLIFY leads to noticeable improvements. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Verification Problems</head><p>A natural question is whether the performance of 2-SIMPLIFY carries over to other SAT-encoded problems. In particular, those originating from the verification community. To check this, we tested 2-SIMPLIFY on a host of model-checking and bounded model-checking problems. These problems are much larger than the planning bench-marks, and SATZ and REL-SAT have a very difficult time solving them. Luckily, a recent solver, CHAFF, is able to solve these problems rather easily. Thus, we compared the running times of CHAFF with and without 2-SIMPLIFY on these problems.</p><p>In Table <ref type="table">8</ref> we see the performance of 2-SIMPLIFY on satisfiable instances. CHAFF solves these problems without difficulty, usually in less than a second. Although CHAFF works faster on the simplified problems, the simplification time is roughly 40 seconds, making the use 2-SIMPLIFY inappropriate.</p><p>In Table <ref type="table">9</ref>, we see the results on a set of unsatisfiable problems. Here, we see precisely the opposite picture. Except for two cases, on all instances that require more than 10 seconds, 2-SIMPLIFY leads to considerable reduction in combined running time. The hardest problem in this class, fvp-7pipe, was not solvable by CHAFF within over 1000 minutes, whereas its simplified version was solved within less than an hour.  The explanation for the performance differences of 2-SIMPLIFY with respect to the class of satisfiable and unsatisfiable instances most likely lies in the different nature of the search required. CHAFF seems to be able to generate a single solution, when one exists, quickly. This makes the advantages of the reduction relatively small -even if we save during solution time, the difference is too small to compensate for the simplification cost. On the other hand, to prove that a problem is unsatisfiable, an exhaustive search of the space of assignments is required. Now, the smaller search space of the simplified formula pays off.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION AND RELATED WORK</head><p>SAT instances with many binary clauses arise naturally in a number of important applications. The abundance of binary clauses in such problems can be exploited using 2-SAT solution methods and other specialized inference algorithms. Here, we presented 2-SIMPLIFY, a principled and efficient simplification algorithm that uses the transitive closure of the implication graph together with a novel implementation of hyper-resolution (i.e, the derive shared implications step) and transitive reduction to obtain a smaller equivalent formula. This leads to an approach that is faster, more powerful, and more efficient than the ad-hoc resolution of binary clauses used in <ref type="bibr" target="#b4">[5]</ref>.</p><p>Our experiments show that the performance of 2-SIMPLIFY depends crucially on the solver used, the satisfiability of the problem, and its difficulty for the solver. Such irregular behavior of simplifiers was noticed by <ref type="bibr" target="#b16">[17]</ref>. However, it seems safe to say that on problems that are difficult for a solver, 2-SIMPLIFY is quite useful. Indeed, we saw that on the encoded planning problems 2-SIMPLIFY was beneficial in conjunction with both SATZ and REL-SAT in the bulk of cases. On the larger satisfiable verification problems we saw that 2-SIMPLIFY's simplification time was much larger than the solution times. However, on the larger unsatisfiable verification times, we saw that 2-SIMPLIFY leads to two to three-fold improvement, and even more. Another interesting observation is the complementary effect of COMPACT's Ô×Ð options and 2-SIMPLIFY, raising the natural question of whether the capabilities of these two simpli-fiers can be combined effectively. Finally, we note that the current implementation of 2-SIMPLIFY leaves much for improvement, and we believe that a more careful design can lead to much improved simplification times. This is of particular importance when one recalls that in almost all cases the simplified formulas require less time to solve. Thus, reduced performance is often due to the simplification time overhead.</p><p>We are not the first to utilize binary resolution in this area. Larrabee used the implication graph to devise a SAT algorithm in the context of test-pattern generation <ref type="bibr" target="#b14">[15]</ref>. Larrabee systematically generates satisfying assignments consistent with the implication graph. Any assignment that satisfies the non-binary clauses is a satisfying assignment for the whole formula. This method exploits the binary portion of the formula, but it does not utilize the power of contemporary variable ordering and search techniques.</p><p>2CL <ref type="bibr" target="#b8">[9]</ref> is a solver based on the Davis-Putnam-Logemann-Loveland algorithm <ref type="bibr" target="#b6">[7]</ref>. At each branch point, 2CL constructs the transitive-closure of the current implication graph and uses it to choose the next branching variable. Thus, 2CL is a dynamic extension of a key aspect of 2-SIMPLIFY. It does not incorporate our derive shared implications step. We did not experiment with 2CL but currently, it is not considered a competitive solver.</p><p>Indeed, extending 2-SIMPLIFY to a full solver would seem to be a natural next step. This solver will be based on the DPLL algorithm. Every time an assignment is made, the implication graph will be used to detect all of its immediate implications. New binary clauses resulting from the reduction of ternary clauses will be added to the implication graph, and the derive shared implications step will be executed. Moreover, the implication graph may be able to provide us with valuable information for branch selection.</p><p>Unfortunately, our initial efforts in this direction were not successful. There appear to be two reasons for this. First, in our implementation, maintaining the graph transitively closed after each assignment (which typically results in a number of new binary clauses) appeared to be a serious bottleneck. Second, we were not able to come up with a quick, yet powerful branch selection heuristics that is competitive with that used by current solvers. For instance, it appears that SATZ is able to gain much information quickly using its unit propagation step, while we have not been able to emulate that using the information in our implication graph. Very recently, Bacchus reported on his effort to extend our approach to a full solver <ref type="bibr" target="#b1">[2]</ref>. The resulting solver is competitive with CHAFF on some SAT instances.</p><p>In particular, Bacchus suggests that repeated application of our derive shared implications step can be very useful. Our use of graph-based techniques, motivated by well-known 2-SAT technique is somewhat reminiscent of graph-based simplification techniques used in work on automated theorem proving such as Kowalski's Clausal Graphs <ref type="bibr" target="#b13">[14]</ref> which formed the basis of the Markgraf Karl Refutation Procedure <ref type="bibr" target="#b18">[19]</ref> (the implication graph can be viewed as a special case of this graph), and some rewrite-based simplification methods <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b2">[3]</ref>. However, the methods used there are much more sophisticated and focus on aspects of first-order theories that do not come up in the propositional setting in which we work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The Implication Graph</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Removing Strongly Connected Components</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>( 4 )Fig. 3 .</head><label>43</label><figDesc>Fig. 3. (A) Initial Transitive Closure (B) Removal of Assigned Nodes (C) Update with New Binary Clauses (D) Removal of Assigned Nodes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Since implication is transitive, we have that Ð ½</figDesc><table><row><cell cols="2">ÁÒ×Ø Ò % Binary Clauses</cell></row><row><cell>log-dir.a</cell><cell>49%</cell></row><row><cell>log-dir.b</cell><cell>55%</cell></row><row><cell>log-dir.c</cell><cell>55%</cell></row><row><cell>log.d</cell><cell>80%</cell></row><row><cell>log-gp.a</cell><cell>98%</cell></row><row><cell>log-gp.b</cell><cell>98%</cell></row><row><cell>log-gp.c</cell><cell>98%</cell></row><row><cell>log-un.a</cell><cell>98%</cell></row><row><cell>log-un.b</cell><cell>98%</cell></row><row><cell>log-un.c</cell><cell>99%</cell></row><row><cell>bw-dir.a</cell><cell>70%</cell></row><row><cell>bw-dir.b</cell><cell>71%</cell></row><row><cell>bw-dir.c</cell><cell>74%</cell></row><row><cell>bw-dir.d</cell><cell>78%</cell></row><row><cell></cell><cell>TABLE I</cell></row><row><cell cols="2">PERCENTAGE OF BINARY CLAUSES IN SAT-ENCODED PLANNING</cell></row><row><cell></cell><cell>PROBLEMS</cell></row><row><cell>Ð ¾</cell><cell></cell></row><row><cell>is implied by the formula whenever there is a path in the graph</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>and satplan.data.tar.Z. The experiments in the second section, dealing with different encoded verification problems</figDesc><table><row><cell>Instance</cell><cell cols="3">Time Assigned+Mapped Clause Number Ratio</cell></row><row><cell>bw-dir.a</cell><cell>0.16</cell><cell>198 out of 459</cell><cell>1944/4675</cell></row><row><cell>bw-dir.b</cell><cell>0.57</cell><cell>391 out of 1087</cell><cell>6814/13772</cell></row><row><cell>bw-dir.c</cell><cell>2.63</cell><cell>890 out of 3016</cell><cell>29770/50457</cell></row><row><cell>bw-dir.d</cell><cell>9.28</cell><cell>1760 out of 6325</cell><cell>82453/131973</cell></row><row><cell>log-dir.a</cell><cell>0.07</cell><cell>238 out of 828</cell><cell>5019/6718</cell></row><row><cell>log-dir.b</cell><cell>0.08</cell><cell>269 out of 843</cell><cell>5325/7301</cell></row><row><cell>log-dir.c</cell><cell>0.13</cell><cell>302 out of 1141</cell><cell>8309/10719</cell></row><row><cell>log.d</cell><cell>4.05</cell><cell>967 out of 4713</cell><cell>16602/21991</cell></row><row><cell>log-gp.a</cell><cell>0.31</cell><cell>281 out of 1782</cell><cell>5511/20895</cell></row><row><cell>log-gp.b</cell><cell>0.49</cell><cell>323 out of 2069</cell><cell>6725/29508</cell></row><row><cell>log-gp.c</cell><cell>0.85</cell><cell>374 out of 2809</cell><cell>9915/48920</cell></row><row><cell>log-un.a</cell><cell>0.21</cell><cell>416 out of 1415</cell><cell>3725/14346</cell></row><row><cell>log-un.b</cell><cell>0.34</cell><cell>307 out of 1729</cell><cell>5355/21943</cell></row><row><cell>log-un.c</cell><cell>0.58</cell><cell>347 out of 2353</cell><cell>7975/37121</cell></row><row><cell></cell><cell></cell><cell>TABLE II</cell><cell></cell></row><row><cell cols="4">RUNNING TIME AND DEDUCTION POWER OF 2-SIMPLIFY</cell></row><row><cell cols="4">(which were much more difficult) were performed on a PC</cell></row><row><cell cols="4">running Linux with a Pentium 4, 180Ghz processor and a</cell></row><row><cell cols="4">256KB cache. These problems were taken from the bench-</cell></row><row><cell cols="4">mark of Miroslav Velev at www.ece.cmu.edu/˜mvelev</cell></row><row><cell cols="4">(the fvp and 2dlx instances), Ofer Shtrichman (ibm in-</cell></row><row><cell cols="4">stances) obtainable from satlib (www.satlib.org), and</cell></row><row><cell cols="4">the BMC generated instances of Biere, Cimatti, Clark, and</cell></row><row><cell cols="4">Zhu, www-2.cs.cmu.edu/˜modelcheck/bmc/bmc-</cell></row><row><cell cols="4">benchmarks.html. 2-SIMPLIFY is written in C++ and all</cell></row><row><cell cols="3">time measurements refer to CPU time.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV</head><label>IV</label><figDesc></figDesc><table /><note><p>SOLUTION TIMES FOR REL-SAT, 2-SIMPLIFY, AND 2-SIMPLIFY+REL-SAT.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V SIMPLIFICATION</head><label>V</label><figDesc>WITH COMPACT AND 2-SIMPLIFY, SOLUTION WITH SATZ.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>ÁÒ×Ø Ò COMPACT+REL-SAT 2-SIM.+REL-SAT psl + REL-SAT</figDesc><table><row><cell>bw-dir.a</cell><cell>0.24</cell><cell>0.21</cell><cell>0.18</cell></row><row><cell>bw-dir.b</cell><cell>0.89</cell><cell>0.98</cell><cell>1.03</cell></row><row><cell>bw-dir.c</cell><cell>17.42</cell><cell>14.07</cell><cell>15.42</cell></row><row><cell>bw-dir.d</cell><cell>579.51</cell><cell>580.26</cell><cell>283.41</cell></row><row><cell>log-dir.a</cell><cell>11.15</cell><cell>0.27</cell><cell>6.48</cell></row><row><cell>log-dir.b</cell><cell>0.33</cell><cell>0.21</cell><cell>0.62</cell></row><row><cell>log-dir.c</cell><cell>2.59</cell><cell>1.66</cell><cell>0.88</cell></row><row><cell>log.d</cell><cell>7.2</cell><cell>10.28</cell><cell>2.41</cell></row><row><cell>log-gp.a</cell><cell>2.18</cell><cell>1.43</cell><cell>6.04</cell></row><row><cell>log-gp.b</cell><cell>3.58</cell><cell>2.4</cell><cell>10.37</cell></row><row><cell>log-gp.c</cell><cell>7.26</cell><cell>5.00</cell><cell>25.07</cell></row><row><cell>log-un.a</cell><cell>1.11</cell><cell>0.52</cell><cell>3.08</cell></row><row><cell>log-un.b</cell><cell>10.08</cell><cell>8.09</cell><cell>11.49</cell></row><row><cell>log-un.c</cell><cell>23.1</cell><cell>17.3</cell><cell>34.47</cell></row><row><cell></cell><cell>TABLE VI</cell><cell></cell><cell></cell></row><row><cell cols="4">SIMPLIFICATION WITH COMPACT AND 2-SIMPLIFY, SOLUTION WITH</cell></row><row><cell></cell><cell>REL-SAT.</cell><cell></cell><cell></cell></row><row><cell>Instance</cell><cell cols="2">WALKSAT 2-SIMPLIFY+WALKSAT</cell><cell></cell></row><row><cell>bw-dir.a</cell><cell>0.04</cell><cell>0.19</cell><cell></cell></row><row><cell>bw-dir.b</cell><cell>0.83</cell><cell>1.49</cell><cell></cell></row><row><cell>bw-dir.c</cell><cell>84.18</cell><cell>38.26</cell><cell></cell></row><row><cell>bw-dir.d</cell><cell>321.90</cell><cell>186.72</cell><cell></cell></row><row><cell>log-dir.a</cell><cell>0.29</cell><cell>0.14</cell><cell></cell></row><row><cell>log-dir.b</cell><cell>0.39</cell><cell>0.21</cell><cell></cell></row><row><cell>log-dir.c</cell><cell>0.78</cell><cell>0.30</cell><cell></cell></row><row><cell>log.d</cell><cell>0.7</cell><cell>4.26</cell><cell></cell></row><row><cell>log-gp.a</cell><cell>3.93</cell><cell>1.06</cell><cell></cell></row><row><cell>log-gp.b</cell><cell>5.92</cell><cell>1.55</cell><cell></cell></row><row><cell>log-gp.c</cell><cell>35.19</cell><cell>3.45</cell><cell></cell></row><row><cell></cell><cell>TABLE VII</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">SOLUTION TIMES USING WALKSAT.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VIII SOLUTION</head><label>VIII</label><figDesc>TIMES FOR CHAFF ON ORIGINAL AND SIMPLIFIED SATISFIABLE PROBLEMS(OOM STANDS FOR OUT OF MEMORY).</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments: I am grateful to Yefim Dinitz and Avraham Melkman for their help and advice on graph algorithms and for important comments on previous versions of this paper, and to the anonymous reviewers for their useful suggestions and comments. This work was supported in part by the Paul Ivanier Center for Robotics and Production Management.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A linear-time algorithm for testing the truth of certain quantified boolean formulas</title>
		<author>
			<persName><forename type="first">B</forename><surname>Aspvall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Plass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tarjan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing Letters</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="121" to="123" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Enhancing davis putnam with extended binary clause reasoning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bacchus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI&apos;02</title>
		<meeting>AAAI&apos;02</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Rewrite-based equational theorem proving with selection and simplification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bachmair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ganzinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Logic and Computation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="217" to="247" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Using CSP look-back techniques to solve real-world SAT instances</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Bayardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Schrag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI-97</title>
		<meeting>AAAI-97</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="203" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Reachability, relevance, resolution, and the planning as satisfiability approach</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">I</forename><surname>Brafman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI&apos;99</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="976" to="981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The complexity of theorem proving procedures</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 3rd ACM Symposium on Theory of Computing</title>
		<meeting>of the 3rd ACM Symposium on Theory of Computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A machine program for theorem proving</title>
		<author>
			<persName><forename type="first">M</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Logemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Loveland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communication of the ACM</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="394" to="397" />
			<date type="published" when="1962-07">July 1962</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Automatic SAT-compilation of planning problems</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Millstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Artificial Intelligence</title>
		<meeting>the International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Satisfiability testing with more reasoning and less guessing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Van Gelder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">K</forename><surname>Tsuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cliques, Coloring, and Satisfiability: Second DIMACS Implementation Challenge</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Johnson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Trick</surname></persName>
		</editor>
		<imprint>
			<publisher>American Mathematical Society</publisher>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Boosting combinatorial search through randomization</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Selman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 15th Nat. Conf. AI</title>
		<meeting>of 15th Nat. Conf. AI</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="431" to="437" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An overview of rewrite rule laboratory (rrl)</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kapur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of RTA&apos;89</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting>of RTA&apos;89</meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1989">1989</date>
			<biblScope unit="volume">355</biblScope>
			<biblScope unit="page" from="559" to="563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Pushing the envelope: Planning, propositional logic, and stochastic search</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Selman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 13th National Conference on AI (AAAI&apos;96)</title>
		<meeting>of the 13th National Conference on AI (AAAI&apos;96)</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="1194" to="1201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unifying sat-based and graph-based planning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Selman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 16th Intl. Joint Conf. on AI (IJCAI&apos;99)</title>
		<meeting>16th Intl. Joint Conf. on AI (IJCAI&apos;99)</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="318" to="325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A proof procedure using connection graphs</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kowalski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="572" to="595" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Test pattern generation using boolean satisfiability</title>
		<author>
			<persName><forename type="first">T</forename><surname>Larrabee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computer-Aided Design</title>
		<imprint>
			<biblScope unit="page" from="4" to="15" />
			<date type="published" when="1992-01">January 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Heuristics based on unit propagation for satisfiability problems</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anbulagan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IJCAI-97</title>
		<meeting>IJCAI-97</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The interaction between simplification and search in propositional satisfiability</title>
		<author>
			<persName><forename type="first">I</forename><surname>Lynce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Marques-Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CP&apos;01 Workshop on Modeling and Problem Formulation</title>
		<meeting>of CP&apos;01 Workshop on Modeling and Problem Formulation</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Chaff: Engineering an efficient sat solver</title>
		<author>
			<persName><forename type="first">M</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Madigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 39th Design Automation Conference</title>
		<meeting>of 39th Design Automation Conference</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The Markgraf Karl refutation procedure</title>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Hans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jörg</forename><forename type="middle">H</forename><surname>Ohlbach</surname></persName>
		</author>
		<author>
			<persName><surname>Siekmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Logic, Essays in Honor of Alan Robinson</title>
		<editor>
			<persName><forename type="first">J.-L</forename><surname>Lassez</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Plotkin</surname></persName>
		</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="41" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Automatic deduction with hyper-resolution</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Robinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. of Com. Math</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="227" to="234" />
			<date type="published" when="1965">1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Gsat: A new method for solving hard satisfiability problems</title>
		<author>
			<persName><forename type="first">B</forename><surname>Selman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Levesque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 10th National Conf. on AI (AAAI &apos;92)</title>
		<meeting>of the 10th National Conf. on AI (AAAI &apos;92)</meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="440" to="446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Noise strategies for improving local search</title>
		<author>
			<persName><forename type="first">Bart</forename><surname>Selman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henry</forename><forename type="middle">A</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Nat. Conf. on AI</title>
		<meeting>Nat. Conf. on AI</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="337" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Tuning sat checkers for bounded model checking</title>
		<author>
			<persName><forename type="first">O</forename><surname>Shtrichman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Aided Verification</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Emerson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Sistla</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2000">2000. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
