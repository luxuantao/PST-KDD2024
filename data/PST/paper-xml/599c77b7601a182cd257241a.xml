<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Empirical Mode Decomposition based Ensemble Deep Learning for Load Demand Time Series Forecasting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2016-10-02">October 2, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xueheng</forename><surname>Qiu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical and Electronic Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<addrLine>50 Nanyang Avenue</addrLine>
									<postCode>639798</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ye</forename><surname>Ren</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical and Electronic Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<addrLine>50 Nanyang Avenue</addrLine>
									<postCode>639798</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nagaratnam</forename><surname>Suganthan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical and Electronic Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<addrLine>50 Nanyang Avenue</addrLine>
									<postCode>639798</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gehan</forename><forename type="middle">A J</forename><surname>Amaratunga</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Centre for Advanced Photonics and Electronics</orgName>
								<orgName type="department" key="dep2">Electrical Engineering Division, Engineering Department</orgName>
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<postCode>CB3 0FA</postCode>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Empirical Mode Decomposition based Ensemble Deep Learning for Load Demand Time Series Forecasting</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2016-10-02">October 2, 2016</date>
						</imprint>
					</monogr>
					<idno type="MD5">54B013C6042BF5BC8AB9378D1AB099AB</idno>
					<idno type="DOI">10.1016/j.asoc.2017.01.015</idno>
					<note type="submission">Received date: 6-7-2016 Revised date: 2-10-2016 Accepted date: 3-1-2017 Preprint submitted to Applied Soft Computing Time series forecasting, Load demand forecasting, Neural networks, Support vector regression, Random forests Table 1: Nomenclature RF Random Forest ANN Artificial Neural Network SVM Support Vector Machine SVR Support Vector Regression DBN Deep Belief Network RBM Restricted Boltzmann Machine EMD Empirical Mode Decomposition IMF Intrinsic Mode Function ACF Autocorrelation Function SLFN Single-hidden Layer Feedforward Neural network MAPE Mean Absolute Percentage Error RMSE Root Mean Square Error ARMA Auto Regressive Moving Average ARIMA Auto Regressive Integrated Moving Average</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>id=&quot;aut0005&quot; author-id=&quot;S1568494617300273-6c3995605dce92efba8bf69f8a61caa6&quot; biographyid=&quot;vt0005&quot;&gt; Xueheng Qiu id=&quot;aut0010&quot; author-id=&quot;S1568494617300273-5d58496303658b4798877c8fabd58081&quot; biographyid=&quot;vt0010&quot;&gt; Ye Ren id=&quot;aut0015&quot; orcid=&quot;0000-0003-0901-5105&quot; author-id=&quot;S1568494617300273-16e5d600d6f497dd629cf6fb535467bd&quot; biographyid=&quot;vt0015&quot;&gt; Ponnuthurai Nagaratnam Suganthan id=&quot;aut0020&quot; author-id=&quot;S1568494617300273-bf2cf1500b1013b37aa7e55bfd72fb66&quot; biographyid=&quot;vt0020&quot;&gt; Gehan A.J. Amaratunga Applied Soft Computing Empirical mode decomposition, Deep learning, Ensemble method, 1.011e-12</term>
					<term>Different</term>
					<term>CritDist: 3.0</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Research highlights:</p><p>1. An ensemble deep learning method has been proposed for load demand forecasting.</p><p>2. The hybrid method composes of Empirical Mode Decomposition and Deep Belief Network.</p><p>3. Empirical Mode Decomposition based methods outperform the single structure models.</p><p>4. Deep learning shows more advantages when the forecasting horizon increases.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This is a PDF file of an unedited manuscript that has been accepted for publication. As a service to our customers we are providing this early version of the manuscript. The manuscript will undergo copyediting, typesetting, and review of the resulting proof before it is published in its final form. Please note that during the production process errors may be discovered which could affect the content, and all legal disclaimers that apply to the journal pertain.</p><p>A c c e p t e d M a n u s c r i p t Machines (RBMs) was used to model each of the extracted IMFs, so that the tendencies of these IMFs can be accurately predicted. Finally, the prediction results of all IMFs can be combined by either unbiased or weighted summation to obtain an aggregated output for load demand. The electricity load demand data sets from Australian Energy Market Operator (AEMO) are used to test the effectiveness of the proposed EMD-based DBN approach. Simulation results demonstrated attractiveness of the proposed method compared with nine forecasting methods.</p><p>A c c e p t e d M a n u s c r i p t</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Electricity load demand forecasting is one of the most important tasks in the management of modern power systems. Improving the accuracy and efficiency of load demand forecasting can help power companies develop reasonable grid construction planning which will lead to the improvement of economic and social benefits of the systems. Moreover, the forecasting results with high accuracy can also be effective to predict the potential faults in the power systems, and thus A c c e p t e d M a n u s c r i p t provide a reliable safety basis for the grid operation. In another word, the goal of load demand forecasting is to provide reliable power supply while keeping the operational costs as low as possible.</p><p>Time series (TS) analysis is a hot research field, which aims to extract meaningful statistics and other characteristics of TS data by analyzing the data itself.</p><p>Methods of time series analysis can be divided into two categories: univariate and multivariate. For example, Raza has developed an exponentially weighted moving average (EWMA) based shift-detection methods for detecting covariate shifts in non-stationary environments <ref type="bibr" target="#b0">[1]</ref>. In the testing stage, Kolmogorov-Smirnov statistical hypothesis test is applied for univariate TS, and the Hotelling T-Squared multivariate statistical hypothesis test is used in the case of multivariate TS. Moreover, many TS datasets have cyclical or seasonal characteristic, which influences TS analysis. Many models have been designed to deal with the cyclic characteristics. For example, Gharehbaghi has developed a pattern recognition framework for detecting dynamic changes on cyclic time series, which combines the discriminant analysis and k-means clustering method <ref type="bibr" target="#b1">[2]</ref>.</p><p>As load demand forecasting belongs to time series forecasting paradigm, there are four types based on the forecasting horizon: long-term (years ahead), mediumterm (months to a year ahead), short-term (a day to weeks ahead) and very shortterm (minutes to hours ahead) <ref type="bibr" target="#b2">[3]</ref>. In this paper, we mainly focus on short term as well as very short term load forecasting. Electricity load demand forecasting with high accuracy is a challenging task. There are many external factors such as climate change and social activities which cause the data to be highly nonlinear and unpredictable <ref type="bibr" target="#b3">[4]</ref>.</p><p>Since the 1940s, various statistical based linear time series forecasting ap-A c c e p t e d M a n u s c r i p t proaches have been proposed. The common goal of these linear models is to use time series analysis for extrapolating the future energy requirement. <ref type="bibr">Bargur</ref> and Mandel have examined the energy consumption and economic growth using trend analysis for Israel <ref type="bibr" target="#b4">[5]</ref>. Moreover, the most successful methods are based on Holt-Winters exponential smoothing <ref type="bibr" target="#b5">[6]</ref> and Autoregressive Integrated Moving Average (ARIMA) <ref type="bibr" target="#b6">[7]</ref>, as well as Linear Regression <ref type="bibr" target="#b7">[8]</ref>.</p><p>In the recent years, with the rapid development of computational intelligence, artificial neural network (ANN) <ref type="bibr" target="#b8">[9]</ref>, fuzzy comprehensive evaluation <ref type="bibr" target="#b9">[10]</ref> and support vector machine (SVM) methods <ref type="bibr" target="#b10">[11]</ref> have been widely used for short-term load forecasting. Luis Hernández presented a solution for short-term load forecasting in micro-grids. The proposed system includes pattern recognition, a kmeans clustering algorithm, and demand forecasting using ANN <ref type="bibr" target="#b11">[12]</ref>. Wavelet analysis also can be used for short term load forecasting <ref type="bibr" target="#b12">[13]</ref>.</p><p>ANN has been successfully applied in the fields of classification and regression, but still fell out of fashion as it is often trapped in a local minimum <ref type="bibr" target="#b13">[14]</ref>. In 2006, Geoffrey Hinton et al. <ref type="bibr" target="#b14">[15]</ref> rekindled interest in neural networks by show-  <ref type="bibr" target="#b17">[18]</ref>, handwriting recognition <ref type="bibr" target="#b18">[19]</ref> and so on. He has also summarized the deep learning related works in his survey paper <ref type="bibr" target="#b19">[20]</ref>.</p><p>Ensemble learning methods, which obtain better forecasting performance by strategically combining multiple learning algorithms, has been widely applied in various research fields including pattern classification, regression and time series forecasting. Dietterich has concluded three fundamental reasons for the success of ensemble methods: statistical, computational and representational <ref type="bibr" target="#b20">[21]</ref>. In addition, Bias-variance decomposition <ref type="bibr" target="#b21">[22]</ref> and strength-correlation also explain why ensemble methods have better performance than their non-ensemble counterparts.</p><p>Fast algorithms are commonly used with ensembles, such as decision trees.</p><p>The ensemble of decision trees is called "random forests" introduced by <ref type="bibr" target="#b22">[23]</ref>.</p><p>RF increases the variance of base learning models by combining the concept of bagging and random subspaces <ref type="bibr" target="#b23">[24]</ref>, thereby improving the performance of this learning model. Manuel Fernández-Delgado et al. have compared 179 learning models from 17 families using 121 classification datasets in their survey paper, among which RF has achieved the best performance <ref type="bibr" target="#b24">[25]</ref>.</p><p>Among the various ensemble methods <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref>, divide and conquer <ref type="bibr" target="#b30">[31]</ref> is a concept which is often applied in time series forecasting. Wavelet transform is a commonly used time series decomposition algorithm. It decomposes the original time series into certain orthonormal sub series by looking at the time frequency domain. <ref type="bibr" target="#b31">[32]</ref>  Empirical mode decomposition (EMD) <ref type="bibr" target="#b32">[33]</ref> is another decomposition method suitable for time series forecasting, which is a part of Hilbert-Huang Transform (HHT). EMD is different from wavelet transform as EMD processes the time series data in time domain. A comparative study on different variations of EMD for wind speed forecasting was reported in <ref type="bibr" target="#b33">[34]</ref>.</p><p>In <ref type="bibr" target="#b34">[35]</ref>, a survey paper explores the application of machine learning methods to energy-based time series forecasting with two main objectives: (i) providing a compact mathematical formulation of the mainly used techniques; (ii) reviewing the latest works of time series forecasting related to electricity price and demand markets. A wide variety of data mining approaches are discussed in this work, including linear models, non-linear machine learning methods, and ensemble models. Several common points are concluded, such as the horizon of prediction normally equals to one day, and the accuracy measures mainly used are MAPE and RMSE, which are consistent with the experiments in our paper. Moreover, the survey work states that: "the current trend in electricity forecasting points to the development of ensembles, thus highlighting single strengths of every method". Therefore, interested readers should view this survey paper as a good guidance for time series forecasting.</p><p>In <ref type="bibr" target="#b26">[27]</ref>, we have proposed an ensemble deep learning algorithm for regres- The remaining of this paper is organized as follows: Section 2 explains the theoretical background on forecasting methods. Section 3 presents the proposed EMD based ensemble deep learning method. Section 4 shows the procedures for experiment setup, followed by the discussion about experiment results in Section 5. In Section 6, two comparative experiments are implemented to evaluate the performance of the proposed method. Finally in Section 7, the conclusions and future works are stated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Theoretical Background on Forecasting Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Artificial Neural Network</head><p>ANN is a learning model inspired by human brain, especially the central nervous system <ref type="bibr" target="#b35">[36]</ref>. The simplest model of ANN is called single-hidden layer feedforward neural network (SLFN). There are three fundamental layers in an SLFN: an input layer with the same number of neurons as the dimension of input features; a hidden layer comprised of neurons with nonlinear activation function; and an output layer which aggregates the outputs from the hidden layer neurons. The output from SLFN is:</p><formula xml:id="formula_0">y = g( h j=1 w jo v j + b j )<label>(1)</label></formula><p>A c c e p t e d M a n u s c r i p t</p><formula xml:id="formula_1">v j = f ( n i=1 w ij x i + b i )<label>(2)</label></formula><p>where x i is the input to the neuron; f () and g() are nonlinear activation functions; v j is the output of hidden layer neuron j; y is the output of this SLFN; n and h are the number of input features and the number of the hidden layer neurons, respectively; w ij is the weight of the connection between the input variable i and the neuron j of the hidden layer; w jo is the weight of the connection between the hidden layer neuron j and the output; b i and b j are the biases.</p><p>To train an SLFN, random values are assigned to the weights, then the weights are tuned by certain method such as back-propagation (BP) <ref type="bibr" target="#b36">[37]</ref> or using a closed form solution <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Support Vector Regression</head><p>The Support Vector Machine (SVM) is a statistical learning theory based machine learning method which proposed by <ref type="bibr" target="#b10">[11]</ref>, using structural risk minimization as its fundamental concept. The Support Vector Regression (SVR) is a regression method which shared the same theoretical background as SVM. It has been widely</p><p>applied in time series prediction such as load demand forecasting.</p><p>Suppose a time series data set is given as</p><formula xml:id="formula_2">D = {(X i , y i )} , 1 i N (3)</formula><p>where X i is the input vector at time i with m elements and y i is the corresponding output data. The regression function can be defined as</p><formula xml:id="formula_3">f (X i ) = W T φ(X i ) + b<label>(4)</label></formula><p>A c c e p t e d M a n u s c r i p t</p><p>where W is the weight vector, b is the bias, and φ(X) maps the input vector X to a higher dimensional feature space. W and b can be obtained by solving the following optimization problem:</p><formula xml:id="formula_4">Min 1 2 W 2 + C N i=1 (ε i + ε * i )<label>(5)</label></formula><p>Subject to:</p><formula xml:id="formula_5">y i -W T (φ(x)) -b ≤ ξ + ε i W T (φ(x)) + b -y i ≤ ξ + ε * i ε i , ε * i ≥ 0 (<label>6</label></formula><formula xml:id="formula_6">)</formula><p>where C is a predefined positive trade-off parameter between model simplicity and generalization ability, ξ is a free parameter that serves as a threshold, ε i and ε * i are the slack variables measuring the cost of the errors. For nonlinear input data set, kernel functions can be used to map from original space onto a higher dimensional feature space in which a linear regression model can be built. The most frequently used kernel function is the Gaussian radial</p><formula xml:id="formula_7">function (RBF) with a width of σ K(X i , X j ) = exp(-X i -X j 2 /(2σ 2 ))<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Random Forests</head><p>Random forests, or random decision forests <ref type="bibr" target="#b39">[40,</ref><ref type="bibr">41]</ref>, proposed by <ref type="bibr" target="#b22">[23]</ref>, is an ensemble learning method for both of classification and regression problems.</p><p>Random forests combine bagging and random subspace method (RSM) by conducting random feature subspace at each node of the classification and regression tree (CART) <ref type="bibr" target="#b23">[24]</ref>. Bagging (bootstrap aggregating), developed by <ref type="bibr" target="#b41">[42]</ref>, is a widely A c c e p t e d M a n u s c r i p t used ensemble method. In bagging ensemble method,one trains each weak learning machine on bootstrap samples of the original training samples, then aggregating the outputs. RSM is a combining method which trains the learning machines on randomly chosen subspaces of the original input space, and combines the outputs by a majority vote or median <ref type="bibr">[41]</ref>. More specifically, at each node of the decision tree in random forest, m features from totally n input features are randomly selected. Then according to an impurity criterion, one of these features is used to perform a partition along the feature axis <ref type="bibr" target="#b23">[24]</ref>. The algorithm of RF is presented in Table 2 <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44]</ref>. Given:</p><p>X is the training dataset with dimension N × n, where N is the number of observations, n is the number of input features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Y is the target values of the training dataset with dimension</head><formula xml:id="formula_8">N × 1.</formula><p>L is the number of trees in RF.</p><p>T i refers to each decision tree in RF, where i = 1...L.</p><p>m is the number of features randomly selected in each node of decision tree.</p><p>1). In each decision tree T i in RF, generate the training set by sampling N times from all observations with replacement.</p><p>2). In each node of one decision tree, m randomly selected features are used to calculate the best split criterion for T i .</p><p>3). Repeat step 2 until the decision tree T i is fully grown.</p><p>4). Aggregate the outputs given by all the decision trees to obtain final result. For classification, the output value is determined by majority vote. For regression, the mean or median of all the outputs is treated as the predicted value. The DBN proposed by <ref type="bibr" target="#b14">[15]</ref> provides a new way to train deep generative models, which is called layer-wise greedy pre-training algorithm. Figure <ref type="figure">1</ref> shows the flowchart of a DBN. There is no inter-connection between units in each layer.</p><p>An restricted Boltzmann machine (RBM) is a neural network which can learn the probability distribution over the input dataset. The DBN pre-training procedure treats each consecutive pair of layers in the MLP as a restricted Boltzmann machine (RBM) <ref type="bibr" target="#b46">[47]</ref> whose joint probability is defined as</p><formula xml:id="formula_9">P h,v (h, v) = 1 Z h,v • e (v T W h+v T b+a T h)<label>(8)</label></formula><p>for the Bernoulli-Bernoulli RBM applied to binary v with a second bias vector b and normalization term Z h,v , and</p><formula xml:id="formula_10">P h,v (h, v) = 1 Z h,v • e (v T W h+(v-b) T (v-b)+a T h)<label>(9)</label></formula><p>for the Gaussion-Bernoulli RBM applied to continuous variable v <ref type="bibr" target="#b47">[48]</ref>. In both cases the conditional probability P h|v (h|v) has the same form as that in an MLP To train multiple layers, one trains the first layer, freezes it, and uses the conditional expectation of the output as the input to the next layer and continues training next layers. Hinton and many others have found that initializing MLPs with pretrained parameters never hurts and often helps <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b49">50]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Empirical Mode Decomposition</head><p>EMD <ref type="bibr" target="#b32">[33]</ref>, also known as HHT, is a method to decompose a signal into several intrinsic mode functions (IMF) along with a residue which stands for the trend.</p><p>EMD is an empirical approach to obtain instantaneous frequency data from nonstationary and nonlinear data sets.</p><p>A c c e p t e d M a n u s c r i p t</p><p>The system load is a random non-stationary process composed of thousands of individual components. The system load behavior is influenced by a number of factors, which can be classified as: economic factors, time, day, season, weather and random effects. Thus, EMD algorithm can be very effective for load demand</p><p>forecasting.</p><p>An IMF is a function that has only one extreme between zero crossings, along with a mean value of zero. The procedures of algorithm inside EMD are shown as follows:</p><p>1. With a given time series signal x(t), create its upper and lower envelopes by a cubic-spline interpolation of local maxima and minima.</p><p>2. Calculate the mean of the upper and lower envelopes m 1 .</p><p>3. Subtract the mean from the original time series to obtain the first component</p><formula xml:id="formula_11">h(t) = x(t) -m(t).</formula><p>4. Repeat steps 1 to 3 by considering h(t) as new x(t) until one of the following stopping criteria is satisfied: i) m(t) approaches zero, ii) the numbers of zero-crossings and extrema of h(t) differs at most by one, or iii) the predefined maximum iteration is reached.</p><p>5. Treat h(t) as an IMF and compute residue signal: r(t) = x(t) -h(t).</p><p>6. Use the residual signal r(t) as new x(t) to find next IMF. Repeat steps 1 to 5 until all IMFs are obtained.</p><p>Finally the original TS signal is decomposed as:</p><formula xml:id="formula_12">x(t) = n i=1 (c i ) + r n (<label>10</label></formula><formula xml:id="formula_13">)</formula><p>where the number of functions n in the set depends on the original TS signal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A c c e p t e d M a n u s c r i p t</head><p>End data points extending problem is an important issue which should be considered in EMD algorithm. In usual case, the end points are not the local extrema, so it is necessary to extend two maximum extrema and two minimum extrema to get the envelope of extrema. In <ref type="bibr" target="#b32">[33]</ref>, the characteristic waves were used to get the maximum extrema and the minimum extrema. However, different characteristic waves will cause different results, and it is difficult to choose the proper waves for every iteration. In this work, the Matlab package for EMD, which is implemented by G. Rilling, was used to decompose the TS signal. According to <ref type="bibr" target="#b50">[51]</ref>, good results can be obtained by just mirroring the extrema close to the edges (or mirror extending method). Several publications in the literature present some improved methods for mitigating of end effect in EMD. For example, in <ref type="bibr" target="#b51">[52]</ref>, end mirror extending is used in high frequency, while least square polynomial extending is used in low frequency. However, no complete solution is in sight currently, which means that there is room for improving the solution of the endpoint effect of EMD method.</p><p>Figure <ref type="figure" target="#fig_4">2</ref> shows an example of the decomposed load demand TS signal with a time window of one month.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed EMD based Deep Learning Method</head><p>A divide and conquer algorithm works by recursively breaking down a problem into two or more sub-problems of the same (or related) type, until these become simple enough to be solved directly. The solutions to the sub-problems are then combined to give a solution to the original problem.</p><p>In the proposed method, the load demand data is decomposed into several ANN is applied to each IMF including the residue. As the completeness of the forecasting for all sub series, the prediction results can be aggregated by single learning machine or simply summed to obtain the final prediction. The procedure of this proposed method is shown as follows:</p><p>1. The time series data is decomposed by EMD into several IMFs and one residue.  4. Combine all the prediction results by summation or with a linear neural network to formulate an ensemble output for TS.</p><p>Figure <ref type="figure" target="#fig_6">3</ref> shows the overall schematic of this ensemble method.</p><p>Time Series Data EMD</p><formula xml:id="formula_14">IM F 1 IM F 2 • • • IM F n R n Input 1 Input 2 • • • Input n Input n+1 DBN 1 DBN 2 • • • DBN n DBN n+1 Output 1 Output 2 • • • Output n Output n+1</formula><p>Prediction Results The electricity load demand data sets from Australian Energy Market Operator (AEMO) were used for the comparison <ref type="bibr" target="#b52">[53]</ref>. Especially, the data sets of year 2013 from New South Wales (NSW), Tasmania (TAS), Queensland (QLD), South Australia (SA) and Victoria (VIC) were chosen to train and test the proposed method.</p><p>For each area, four months were chosen to reflect the factors of different seasons:</p><p>January, April, July and October. During the simulation, the first three weeks was used to train the model, and the remaining one week was used for testing.</p><p>Thus, there are totally 1008 examples for training and 336 examples for testing.</p><p>Moreover, six fold cross-validation was employed during training to improve the generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Characteristics of Data</head><p>The load demand data is sampled every half an hour, which means there are 48 data points for one day. Due to the influence from climate and social activities, the electricity load data shows three main nest cycles: daily, weekly and yearly.</p><p>To identify cycles and patterns in load demand time series data, autocorrelation function (ACF) can be applied as a guidance for informative feature subset selection <ref type="bibr" target="#b2">[3]</ref>. Suppose a time series data set is given as X = {X t : t ∈ T }, where T is the index set. The lag k autocorrelation coefficient r k can be computed by:</p><formula xml:id="formula_15">r k = r(X t , X t-k ) = n t=k+1 (X t -X)(X t-k -X) n t=1 (X t -X) 2<label>(11)</label></formula><p>where X is the mean value of all X in the given time series, r k measures the linear correlation of the time series at times t and t -k.</p><p>Three strongest dependent lag variables can be identified from the ACF of electricity load demand TS: the value at half-an-hour before (X t-1 ), the value at Page 20 of 41</p><p>A c c e p t e d M a n u s c r i p t the same time in the previous week (X t-336 ), as well as the value at the same time in the previous day (X t-48 ). Therefore, for one day head load demand forecasting in this work, the input feature set is composed by the data points of the whole previous day (X t-48 to X t-96 ) and the same day in the previous week (X t-336 to X t-384 ), which include all the most informative lag variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Methodology</head><p>For the time series load demand datasets, all the training and testing values are linearly scaled to [0, 1]. The scaling formula is:</p><formula xml:id="formula_16">ȳi = y max -y i y max -y min<label>(12)</label></formula><p>To implement the simulation, LIBSVM toolbox was used for the SVR model <ref type="bibr" target="#b53">[54]</ref>, while deep learning toolbox was used for neural networks, including ANN, DBN, EDBN <ref type="bibr" target="#b26">[27]</ref>, EMD-ANN and the proposed method <ref type="bibr" target="#b54">[55]</ref>. RF and EMD-RF are developed from the function "TreeBagger" in Matlab. We set the parameter "NumPredictorsToSample" as one third of the number of input features to invoke RF algorithm. For RF and EMD based RF, the number of decision trees is set as 500.</p><p>A c c e p t e d M a n u s c r i p t</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Performance Estimation</head><p>In this paper, two error measures are used to examine the accuracy of a prediction model: Root Mean Square Error (RMSE) and Mean Absolute Percentage Error (MAPE). They are defined as</p><formula xml:id="formula_17">RM SE = 1 n n i=1 (y ′ i -y i ) 2 M AP E = 1 n n i=1 y ′ i -y i y i (<label>13</label></formula><formula xml:id="formula_18">)</formula><p>where y ′ i is the predicted value of corresponding y i , and n is the number of data points in the testing time series.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results and Comparison</head><p>In this study, two forecasting horizons are adopted for comparison: half an hour (very short term) and one day ahead (short term).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Performance Comparison for Half-an-hour ahead Load Forecasting</head><p>The simplest forecasting method is persistence method, which assumes that the conditions at the future time of forecast are the same as the current values.</p><p>The persistence method works well for very short term load demand forecasting since the temperature and human factors change little during a short time period. Therefore, persistence method can be treated as a baseline for evaluating the effectiveness of machine learning models. The one step ahead (half an hour) forecasting results of persistence method are shown in Table <ref type="table" target="#tab_2">3</ref>. We can see that all of the machine-learning algorithms outperform the persistence method for half an hour ahead forecasting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A c c e p t e d M a n u s c r i p t</head><p>The original load demand time series data was modeled by SVR, SLFN and RF without decomposition to reveal the advantages of EMD based hybrid approach. Comparing the forecasting results listed in Table <ref type="table" target="#tab_2">3</ref>, we can conclude that the EMD based hybrid approach generally outperforms the single structure machine learning algorithms most of the time. Moreover, EMD-SVR, EMD-SLFN and EMD-RF model has comparable performance with each other. In addition, the proposed EMD-DBN model has the best performance for half-an-hour ahead forecasting in most cases.</p><p>The comparison results of Nemenyi test among all the learning methods based on RMSE and MAPE are shown in Figure <ref type="figure" target="#fig_9">4</ref>. The Nemenyi test is a post-hoc test which is used when all classifiers are compared to each other <ref type="bibr" target="#b55">[56]</ref>. As shown, the methods with better ranks are at the top whereas the methods with worse ranks are at the bottom. The methods within a vertical line whose length is less than or equal to a critical distance have statistically the same performance. The title of the graphs shows Friedman p-value. If it is smaller than 0.05, there exists significant difference among these models. The critical difference is calculated by:</p><formula xml:id="formula_19">CD = q α k(k + 1) 6N (<label>14</label></formula><formula xml:id="formula_20">)</formula><p>where k is the number of algorithms, N is the number of data sets, and q α is the critical value based on the studentized range statistic divided by √ 2 <ref type="bibr" target="#b55">[56]</ref>.</p><p>Therefore, we can conclude from the results of statistical testing that our proposed method has the best rank and significantly outperforms the non-ensemble methods with a 95% confidence. It is also worth noting that the proposed EMD-DBN method has better rank compared with EDBN, which shows the advantages of divide and conquer concept.</p><p>A c c e p t e d M a n u s c r i p t and MAPE (right). The critical distance is 3.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Performance Comparison for One-day ahead Load Forecasting</head><p>The prediction results for one day ahead load forecasting are shown in Table <ref type="table" target="#tab_3">4</ref>.</p><p>Similar to very short term load forecasting, the performance comparison includes the persistence method. In this case, the time horizon is 24 hour, which means that we assume the predicted value is the same as the value 24 hours ago. Due to the daily seasonality of the load demand data, the accuracy of persistence method falls in an acceptable range. Therefore, the effectiveness of the involved machine learning algorithms can be verified by outperforming the persistence methods.</p><p>Same as the previous experiment, the Nemenyi test is also used to compare the one-day ahead forecasting performances. The results based on RMSE and MAPE are shown in Figure <ref type="figure" target="#fig_10">5</ref>.</p><p>By analyzing the forecasting outputs of SVR and ANN listed in Table <ref type="table" target="#tab_3">4</ref>, we can find that these two methods have comparable performances. This phenomenon may be due to the reason that both models have similar network structure with one hidden layer <ref type="bibr" target="#b58">[59]</ref>. It is also worth noting that the DBN model out-</p><p>A c c e p t e d M a n u s c r i p t we can conclude that the proposed EMD-based DBN approach has successfully outperformed all the benchmark methods in both experiments on both forecasting horizons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Comparative Experiments</head><p>In this section, three comparative experiments were implemented to evaluate the performance of our proposed method. The comparison conditions such as dataset partitioning and cross-validation were kept the same for the reported methods <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b61">62]</ref> and the proposed method.</p><p>A c c e p t e d M a n u s c r i p t</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Comparative Experiment with SRSVRCABC Model</head><p>The first experiment uses historical monthly electric load demand data of Northeast China to compare with two benchmark methods: seasonal recurrent SVR with chaotic artificial bee colony (SRSVRCABC) model in <ref type="bibr" target="#b60">[61]</ref> and TF-ε-SVR-SA model in <ref type="bibr" target="#b59">[60]</ref>. Table <ref type="table">5</ref> shows the actual values and the forecast values obtained using all benchmark methods. Obviously, the proposed method has the smallest MAPE values compared with ARIMA, TF-ε-SVR-SA and SRSVRCABC models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Comparative Experiment with AFCM</head><p>In the second comparative experiment, the electricity load demand data of May 2007 from New South Wales, Australia is used for the simulations. According to <ref type="bibr" target="#b61">[62]</ref>, this experiment is divided into two parts: one part with small sample size, and another part with large sample size. In the first part, the data set contains the load demand data from 00:00 on May 2 to 23:30 on May 8 with the same interval From the forecasting results listed in Table <ref type="table">6</ref>, some observations can be made from comparison. First of all, all the learning methods are effective for short time load demand forecasting since all of them can give reasonable results. Second, by comparing the differences between small and large sample size parts, the proposed EMD-DBN method can reduce the influence caused by redundant information in the large size data set to give better performance. Finally, it is clear that the EMD based deep learning method has outperformed the benchmark methods in both experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Comparative Experiment with PSF-NN</head><p>For the third comparative experiment, according to <ref type="bibr" target="#b62">[63]</ref>, we use electricity load demand data for the state of NSW in Australia for three years: 2009, 2010 and 2011. The data from the first two years is used to train the prediction model, while the remaining data for 2011 is used for testing. The forecasting horizon is still one day. There are four benchmark methods, the pattern sequence-based forecasting (PSF) method and three combined PSF-NN models with three different feature sets. Table <ref type="table">7</ref> shows the comparative results. The proposed EMD based deep learning approach outperforms all the benchmark methods on both error measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper, we proposed an ensemble deep learning method based on EMD and DBN. The proposed method has been evaluated with three electricity load demand datasets from AEMO. Nine benchmark methods have been compared to      </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>ing substantially better performance by a "deep" neural network. Since then, deep learning has become popular in machine learning field. Takashi Kuremoto proposed a time series forecasting predictor model using Deep Belief Network (DBN) with multiple restricted Boltzmann machines (RBMs) [16]. The CATS benchmark data has been used in the form of 5 blocks with 20 missing and 980 known in each block. The model was then optimized by particle swarm optimization (PSO) algorithm. This work has shown DBN's superiority over conventional multilayer perceptron (MLP) neural network model and statistical model ARIMA. Busseti also conducted simulations to compare deep learning methods with traditional A c c e p t e d M a n u s c r i p t shallow neural networks [17]. The work successfully showed the advantages of deep learning architectures to the problems of electricity load demand forecasting. Moreover, since 2009, Juergen Schmidhuber and his deep learning team have designed effective deep neural networks for image classification</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>use wavelet based nonlinear multistate decom-A c c e p t e d M a n u s c r i p t position model for electricity load forecasting. Adaptive wavelet neural network model is used for forecasting short term electric load with feed forward neurons.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>sion and time series forecasting, which composed of DBNs trained using different number of BP epochs and an SVR applied to analyze the relationship between these outputs and target values. It is worth noting that the input to each DBN is the original time series data. To further improve the ensemble learning architec-A c c e p t e d M a n u s c r i p t ture, in this paper, we adopt the concept of "divide and conquer", and construct a novel electricity load demand forecasting method based on EMD and deep learning algorithms. The advantages of the proposed method are demonstrated on real world datasets compared with nine benchmark learning algorithms: Persistence, SVR, ANN, DBN, RF, EMD based SVR, EMD based ANN, EMD based RF, as well as the ensemble DBN proposed in the previous work.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>A c c e p t e d M a n u s c r i p t 2 . 4 .</head><label>24</label><figDesc>Deep Belief Network (DBN)Deep learning is a branch of machine learning algorithms that attempts to model high-level abstractions in data by using model architectures, with complex structures with multiple non-linear transformations<ref type="bibr" target="#b44">[45]</ref>. Deep learning algorithms are fundamentally based on distributed representations, which means that observed data can be represented by interactions of many different factors on different levels. The main promise of deep learning is replacing handcrafted features with efficient algorithms for unsupervised feature extraction<ref type="bibr" target="#b45">[46]</ref>. In another word, deep learning attempts to abstract important features in input data set by deep architecture in an unsupervised way.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>ResidueFigure 2 :</head><label>2</label><figDesc>Figure 2: Example of the obtained IMF components after EMD with a time window of one month.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>2 .</head><label>2</label><figDesc>For each IMF and residue, we construct one training matrix as the input for one DBN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>A c c e p t e d M a n u s c r i p t 3 .</head><label>3</label><figDesc>Train DBN to obtain the predicted results for each of the extracted IMF and residue.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Schematic Diagram of the Proposed EMD based Deep Learning Approach</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>For</head><label></label><figDesc>SVR and EMD based SVR, we choose the RBF kernel function with parameters chosen by a grid search. As suggested by the authors of LIBSVM toolbox, exponentially growing sequences of C and σ is used for parameter selection, where the range of C is [2 -4 , 2 4 ], and the range of σ is [10 -3 ,10 -1 ]. For ANN and EMD-ANN, the size of neural networks is determined by the size of input vector. For DBN and the proposed method, two RBMs are stacked for pre-training with the size of [100 100]. The number of iterations for back propagation is set as 500.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Nemenyi testing results for half-an-hour ahead load forecasting based on RMSE (left)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Nemenyi testing results for one-day ahead load forecasting based on RMSE (left) and MAPE (right). The critical distance is 3.0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>According to Hong's paper, there are totally 64 monthly electric load data points from January 2004 to April 2009, which are divided into three parts: the training set (32 data points, December 2004 to July 2007), the validation data set (14 data points, August 2007 to September 2008), and the testing data sets (7 data points, from October 2008 to April 2009). Moreover, based on the same comparison conditions, 25 data points are fed in as input matrix to predict the following monthly load data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>of 30 min. The data set is divided into two parts: one is the training set which contains the historical data of first six days; another one is the testing set which contains the remaining one day's data. In the second part with large sample size, 1104 data points from May 2 to May 24 are used to train the model to predict the load demand in the following one week from May 18 to May 24. The adaptive A c c e p t e d M a n u s c r i p t fuzzy combination model (AFCM) from [62] along with SVR are implemented to compare with the proposed model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>A c c e p t e d M a n u s c r i p t verify the effectiveness 4 .</head><label>4</label><figDesc>of the proposed method: Persistence, SVR, ANN, DBN, RF, EDBN, EMD-SVR, EMD-SLFN and EMD-RF. Two error measures (RMSE and MAPE) were used to evaluate the performance of these prediction models. Moreover, two comparative experiments are also implemented to verify the effectiveness of the proposed method. According to the prediction results, several observations can be concluded:1. EMD based hybrid methods normally outperform the corresponding single structure models for load demand time series forecasting.2. Deep learning algorithms show their advantages in dealing with nonlinearfeatures when the forecasting horizon increases.3. Random Forests, as a decision tree based method, is effective for load demand forecasting with the advantage of fast training. The proposed EMD based ensemble deep learning approach has the best performance according to the statistical testing.For future work, additional nonlinear features such as climate and human activities need to be considered in constructing a more complex model. The potential learning ability of deep learning methods will be well suited to such complex problems. Moreover, since the ensemble deep learning model is more time consuming when compared with the single structure model, optimization techniques can be designed to simplify the structure and increase the efficiency of deep learning model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>s c r i p t the 1966 Professor of Engineering and Head of Electronics, Power, and Energy Conversion at the University of Cambridge. Since 2009, he has been a Visiting Professor in the School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore. He has authored or coauthored more than 500 papers. He holds 25 patents. His current research interests include novel materials and device structures for nanotechnology-enhanced batteries, supercapacitors, solar cells, and power electronics for optimum grid connection of photovoltaic electricity generation systems. Dr. Amaratunga is a Fellow of the Royal Academy of Engineering. He was the recipient of awards from the Royal Academy of Engineering, the Institution of Engineering and Technology, and the Royal Society.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>A c c e p t e d M a n u s c r i p t</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>A c c e p t e d M a n u s c r i p t</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="3,584.53,195.00,435.47,231.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="3,178.23,232.59,290.26,185.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="3,177.74,27.23,290.26,185.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>Random Forests Algorithm</figDesc><table /><note><p>Random Forests Algorithm:</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Prediction results for half-an-hour ahead load forecasting</figDesc><table><row><cell cols="3">Dataset Month Metrics</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Prediction model</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Persistence SVR</cell><cell>ANN</cell><cell>DBN</cell><cell>RF</cell><cell cols="5">EDBN EMD-SVR EMD-ANN EMD-RF Proposed</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>[11]</cell><cell>[36]</cell><cell>[15]</cell><cell>[23]</cell><cell>[27]</cell><cell>[57]</cell><cell>[58]</cell><cell></cell><cell></cell></row><row><cell>NSW</cell><cell>Jan</cell><cell>RMSE</cell><cell>164.02</cell><cell>94.24</cell><cell>96.66</cell><cell>79.16</cell><cell>93.36</cell><cell>75.42</cell><cell>78.56</cell><cell>82.11</cell><cell>76.10</cell><cell>49.86</cell></row><row><cell></cell><cell></cell><cell>MAPE</cell><cell>1.64%</cell><cell cols="5">0.93% 0.98% 0.78% 0.88% 0.70%</cell><cell>0.78%</cell><cell>0.88%</cell><cell>0.76%</cell><cell>0.53%</cell></row><row><cell></cell><cell>Apr</cell><cell>RMSE</cell><cell>248.14</cell><cell cols="5">162.57 140.74 70.36 142.85 134.47</cell><cell>114.09</cell><cell>87.76</cell><cell>120.16</cell><cell>69.55</cell></row><row><cell></cell><cell></cell><cell>MAPE</cell><cell>2.43%</cell><cell cols="5">1.88% 1.26% 0.64% 1.18% 1.14%</cell><cell>1.07%</cell><cell>0.82%</cell><cell>1.10%</cell><cell>0.65%</cell></row><row><cell></cell><cell>Jul</cell><cell>RMSE</cell><cell>235.66</cell><cell cols="5">117.87 165.42 105.63 114.83 78.09</cell><cell>74.29</cell><cell>81.22</cell><cell>120.77</cell><cell>75.09</cell></row><row><cell></cell><cell></cell><cell>MAPE</cell><cell>2.31%</cell><cell cols="5">1.20% 1.68% 1.09% 1.20% 0.67%</cell><cell>0.67%</cell><cell>0.81%</cell><cell>1.22%</cell><cell>0.70%</cell></row><row><cell></cell><cell>Oct</cell><cell>RMSE</cell><cell>159.98</cell><cell>58.26</cell><cell>76.64</cell><cell>62.58</cell><cell>69.06</cell><cell>64.36</cell><cell>54.58</cell><cell>66.00</cell><cell>76.58</cell><cell>51.68</cell></row><row><cell></cell><cell></cell><cell>MAPE</cell><cell>1.65%</cell><cell cols="5">0.64% 0.82% 0.70% 0.75% 0.66%</cell><cell>0.60%</cell><cell>0.74%</cell><cell>0.82%</cell><cell>0.55%</cell></row><row><cell>TAS</cell><cell>Jan</cell><cell>RMSE</cell><cell>17.80</cell><cell>13.87</cell><cell>13.84</cell><cell>12.90</cell><cell>11.80</cell><cell>13.24</cell><cell>12.54</cell><cell>11.39</cell><cell>13.52</cell><cell>11.59</cell></row><row><cell></cell><cell></cell><cell>MAPE</cell><cell>1.27%</cell><cell cols="5">1.09% 1.09% 1.01% 1.03% 1.06%</cell><cell>0.97%</cell><cell>0.78%</cell><cell>1.09%</cell><cell>0.74%</cell></row><row><cell></cell><cell>Apr</cell><cell>RMSE</cell><cell>37.42</cell><cell>25.26</cell><cell>27.03</cell><cell>19.53</cell><cell>24.82</cell><cell>21.35</cell><cell>21.76</cell><cell>18.03</cell><cell>25.19</cell><cell>16.23</cell></row><row><cell></cell><cell></cell><cell>MAPE</cell><cell>2.32%</cell><cell cols="5">1.49% 1.63% 1.25% 1.26% 1.21%</cell><cell>1.36%</cell><cell>1.16%</cell><cell>1.45%</cell><cell>1.07%</cell></row><row><cell></cell><cell>Jul</cell><cell>RMSE</cell><cell>43.84</cell><cell>34.03</cell><cell>33.97</cell><cell>30.43</cell><cell>33.59</cell><cell>22.62</cell><cell>30.90</cell><cell>29.14</cell><cell>32.34</cell><cell>24.44</cell></row><row><cell></cell><cell></cell><cell>MAPE</cell><cell>2.73%</cell><cell cols="5">2.10% 2.03% 1.76% 2.07% 1.36%</cell><cell>1.91%</cell><cell>1.98%</cell><cell>2.17%</cell><cell>1.54%</cell></row><row><cell></cell><cell>Oct</cell><cell>RMSE</cell><cell>22.80</cell><cell>15.89</cell><cell>18.49</cell><cell>16.80</cell><cell>16.94</cell><cell>20.41</cell><cell>14.94</cell><cell>9.37</cell><cell>13.69</cell><cell>8.81</cell></row><row><cell></cell><cell></cell><cell>MAPE</cell><cell>1.63%</cell><cell cols="5">1.09% 1.36% 1.19% 1.19% 1.34%</cell><cell>1.06%</cell><cell>0.70%</cell><cell>0.97%</cell><cell>0.66%</cell></row><row><cell>QLD</cell><cell>Jan</cell><cell>RMSE</cell><cell>109.39</cell><cell>51.31</cell><cell>62.97</cell><cell>44.95</cell><cell>40.30</cell><cell>51.03</cell><cell>42.12</cell><cell>33.88</cell><cell>33.34</cell><cell>25.39</cell></row><row><cell></cell><cell></cell><cell>MAPE</cell><cell>1.50%</cell><cell cols="5">0.70% 0.85% 0.62% 0.54% 0.63%</cell><cell>0.57%</cell><cell>0.48%</cell><cell>0.44%</cell><cell>0.34%</cell></row><row><cell></cell><cell>Apr</cell><cell>RMSE</cell><cell>137.11</cell><cell>71.30</cell><cell>65.84</cell><cell>48.48</cell><cell>57.20</cell><cell>60.27</cell><cell>51.30</cell><cell>56.02</cell><cell>54.78</cell><cell>48.34</cell></row><row><cell></cell><cell></cell><cell>MAPE</cell><cell>1.91%</cell><cell cols="5">0.94% 0.93% 0.66% 0.86% 0.75%</cell><cell>0.61%</cell><cell>0.73%</cell><cell>0.81%</cell><cell>0.67%</cell></row><row><cell></cell><cell>Jul</cell><cell>RMSE</cell><cell>127.23</cell><cell>46.07</cell><cell>51.79</cell><cell>38.45</cell><cell>45.52</cell><cell>42.00</cell><cell>35.53</cell><cell>41.48</cell><cell>45.39</cell><cell>30.61</cell></row><row><cell></cell><cell></cell><cell>MAPE</cell><cell>1.92%</cell><cell cols="5">0.72% 0.82% 0.54% 0.85% 0.60%</cell><cell>0.53%</cell><cell>0.62%</cell><cell>0.69%</cell><cell>0.44%</cell></row><row><cell></cell><cell>Oct</cell><cell>RMSE</cell><cell>110.19</cell><cell>57.90</cell><cell>63.17</cell><cell>61.03</cell><cell>61.37</cell><cell>55.49</cell><cell>54.89</cell><cell>46.78</cell><cell>48.41</cell><cell>40.46</cell></row><row><cell></cell><cell></cell><cell>MAPE</cell><cell>1.61%</cell><cell cols="5">0.80% 0.92% 0.86% 0.77% 0.71%</cell><cell>0.75%</cell><cell>0.69%</cell><cell>0.67%</cell><cell>0.56%</cell></row><row><cell>VIC</cell><cell>Jan</cell><cell>RMSE</cell><cell>174.95</cell><cell cols="5">117.79 114.73 106.25 120.34 78.58</cell><cell>82.96</cell><cell>117.45</cell><cell>115.66</cell><cell>98.75</cell></row><row><cell></cell><cell></cell><cell>MAPE</cell><cell>2.52%</cell><cell cols="5">1.54% 1.55% 1.32% 1.58% 1.05%</cell><cell>1.09%</cell><cell>1.56%</cell><cell>1.59%</cell><cell>1.35%</cell></row><row><cell></cell><cell>Apr</cell><cell>RMSE</cell><cell>162.18</cell><cell cols="5">148.89 149.20 104.48 102.34 75.59</cell><cell>96.24</cell><cell>77.02</cell><cell>68.30</cell><cell>64.11</cell></row><row><cell></cell><cell></cell><cell>MAPE</cell><cell>2.15%</cell><cell cols="5">1.97% 1.99% 1.49% 1.53% 0.98%</cell><cell>1.33%</cell><cell>0.99%</cell><cell>0.92%</cell><cell>0.87%</cell></row><row><cell></cell><cell>Jul</cell><cell>RMSE</cell><cell>171.99</cell><cell cols="3">69.41 119.43 86.63</cell><cell>62.57</cell><cell>66.36</cell><cell>119.27</cell><cell>114.34</cell><cell>61.84</cell><cell>58.70</cell></row><row><cell></cell><cell></cell><cell>MAPE</cell><cell>2.44%</cell><cell cols="5">1.01% 1.74% 1.26% 1.00% 0.91%</cell><cell>1.73%</cell><cell>1.67%</cell><cell>0.88%</cell><cell>0.88%</cell></row><row><cell></cell><cell>Oct</cell><cell>RMSE</cell><cell>139.47</cell><cell>62.88</cell><cell>96.63</cell><cell>91.20</cell><cell>87.11</cell><cell>68.50</cell><cell>90.49</cell><cell>91.38</cell><cell>55.19</cell><cell>57.95</cell></row><row><cell></cell><cell></cell><cell>MAPE</cell><cell>1.97%</cell><cell cols="5">0.92% 1.42% 1.35% 1.20% 0.89%</cell><cell>1.23%</cell><cell>1.23%</cell><cell>0.85%</cell><cell>0.84%</cell></row><row><cell>SA</cell><cell>Jan</cell><cell>RMSE</cell><cell>72.11</cell><cell>50.37</cell><cell>55.65</cell><cell>59.17</cell><cell>53.92</cell><cell>39.87</cell><cell>58.29</cell><cell>46.34</cell><cell>42.52</cell><cell>51.91</cell></row><row><cell></cell><cell></cell><cell>MAPE</cell><cell>3.04%</cell><cell cols="5">1.92% 2.33% 2.54% 2.23% 1.69%</cell><cell>2.29%</cell><cell>1.73%</cell><cell>1.55%</cell><cell>1.69%</cell></row><row><cell></cell><cell>Apr</cell><cell>RMSE</cell><cell>58.53</cell><cell>45.40</cell><cell>39.55</cell><cell>44.85</cell><cell>46.42</cell><cell>35.65</cell><cell>26.14</cell><cell>27.98</cell><cell>31.27</cell><cell>33.44</cell></row><row><cell></cell><cell></cell><cell>MAPE</cell><cell>3.03%</cell><cell cols="5">1.93% 2.33% 2.54% 2.50% 1.75%</cell><cell>1.37%</cell><cell>1.54%</cell><cell>1.83%</cell><cell>1.89%</cell></row><row><cell></cell><cell>Jul</cell><cell>RMSE</cell><cell>75.05</cell><cell>47.68</cell><cell>56.12</cell><cell>48.55</cell><cell>59.99</cell><cell>38.03</cell><cell>37.38</cell><cell>39.16</cell><cell>31.93</cell><cell>29.18</cell></row><row><cell></cell><cell></cell><cell>MAPE</cell><cell>4.25%</cell><cell cols="5">2.34% 3.53% 3.07% 3.49% 1.98%</cell><cell>2.17%</cell><cell>2.33%</cell><cell>1.86%</cell><cell>1.70%</cell></row><row><cell></cell><cell>Oct</cell><cell>RMSE</cell><cell>48.49</cell><cell>42.74</cell><cell>37.94</cell><cell>40.56</cell><cell>42.48</cell><cell>43.59</cell><cell>30.16</cell><cell>25.14</cell><cell>26.59</cell><cell>34.17</cell></row><row><cell></cell><cell></cell><cell>MAPE</cell><cell>2.62%</cell><cell cols="5">1.77% 2.23% 2.17% 2.30% 1.92%</cell><cell>1.67%</cell><cell>1.69%</cell><cell>1.71%</cell><cell>1.62%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Prediction results for one day ahead load forecasting</figDesc><table><row><cell cols="3">Dataset Month Metrics</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Prediction model</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Persistence</cell><cell>SVR</cell><cell>ANN</cell><cell>DBN</cell><cell>RF</cell><cell cols="5">EDBN EMD-SVR EMD-ANN EMD-RF Proposed</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>[11]</cell><cell>[36]</cell><cell>[15]</cell><cell>[23]</cell><cell>[27]</cell><cell>[57]</cell><cell>[58]</cell><cell></cell><cell></cell></row><row><cell>NSW</cell><cell>Jan</cell><cell>RMSE</cell><cell>978.24</cell><cell cols="5">703.43 750.53 639.75 521.14 636.03</cell><cell>611.20</cell><cell>748.30</cell><cell>544.17</cell><cell>541.53</cell></row><row><cell></cell><cell></cell><cell>MAPE</cell><cell>8.55%</cell><cell>6.23%</cell><cell>7.2%</cell><cell>5.95%</cell><cell>4.26%</cell><cell>5.70%</cell><cell>5.19%</cell><cell>6.66%</cell><cell>4.54%</cell><cell>4.62%</cell></row><row><cell></cell><cell>Apr</cell><cell>RMSE</cell><cell>729.50</cell><cell cols="5">474.38 578.05 361.63 500.70 551.74</cell><cell>569.28</cell><cell>512.59</cell><cell>495.28</cell><cell>377.63</cell></row><row><cell></cell><cell></cell><cell>MAPE</cell><cell>6.71%</cell><cell>4.27%</cell><cell>5.41%</cell><cell>3.36%</cell><cell>4.25%</cell><cell>4.78%</cell><cell>5.27%</cell><cell>4.57%</cell><cell>4.21%</cell><cell>3.22%</cell></row><row><cell></cell><cell>Jul</cell><cell>RMSE</cell><cell>609.82</cell><cell cols="5">574.30 534.75 415.81 387.15 414.90</cell><cell>402.69</cell><cell>345.90</cell><cell>353.90</cell><cell>322.04</cell></row><row><cell></cell><cell></cell><cell>MAPE</cell><cell>6.22%</cell><cell>5.86%</cell><cell>5.38%</cell><cell>4.11%</cell><cell>4.01%</cell><cell>4.07%</cell><cell>3.95%</cell><cell>3.09%</cell><cell>3.67%</cell><cell>3.08%</cell></row><row><cell></cell><cell>Oct</cell><cell>RMSE</cell><cell>587.14</cell><cell cols="5">393.32 345.07 350.82 296.53 334.12</cell><cell>272.01</cell><cell>299.34</cell><cell>333.82</cell><cell>282.34</cell></row><row><cell></cell><cell></cell><cell>MAPE</cell><cell>5.36%</cell><cell>3.74%</cell><cell>3.48%</cell><cell>3.41%</cell><cell>2.78%</cell><cell>3.14%</cell><cell>2.76%</cell><cell>2.90%</cell><cell>3.17%</cell><cell>2.71%</cell></row><row><cell>TAS</cell><cell>Jan</cell><cell>RMSE</cell><cell>89.82</cell><cell>60.97</cell><cell>69.92</cell><cell>63.96</cell><cell>65.90</cell><cell>60.68</cell><cell>61.73</cell><cell>63.38</cell><cell>58.51</cell><cell>56.10</cell></row><row><cell></cell><cell></cell><cell>MAPE</cell><cell>7.24%</cell><cell>4.81%</cell><cell>5.42%</cell><cell>4.98%</cell><cell>4.77%</cell><cell>4.82%</cell><cell>4.49%</cell><cell>4.87%</cell><cell>4.67%</cell><cell>4.05%</cell></row><row><cell></cell><cell>Apr</cell><cell>RMSE</cell><cell>157.73</cell><cell>111.89</cell><cell>94.40</cell><cell>93.81</cell><cell>92.64</cell><cell>109.78</cell><cell>104.59</cell><cell>87.41</cell><cell>86.61</cell><cell>85.13</cell></row><row><cell></cell><cell></cell><cell>MAPE</cell><cell>10.22%</cell><cell>7.48%</cell><cell>6.3%</cell><cell>6.12%</cell><cell>6.10%</cell><cell>7.28%</cell><cell>6.87%</cell><cell>5.92%</cell><cell>5.80%</cell><cell>5.80%</cell></row><row><cell></cell><cell>Jul</cell><cell>RMSE</cell><cell>120.47</cell><cell>90.99</cell><cell>89.17</cell><cell>87.30</cell><cell>90.48</cell><cell>85.19</cell><cell>92.54</cell><cell>82.92</cell><cell>81.34</cell><cell>73.91</cell></row><row><cell></cell><cell></cell><cell>MAPE</cell><cell>8.11%</cell><cell>5.89%</cell><cell>6.28%</cell><cell>6.04%</cell><cell>6.17%</cell><cell>6.04</cell><cell>6.09%</cell><cell>5.50%</cell><cell>5.54%</cell><cell>4.93%</cell></row><row><cell></cell><cell>Oct</cell><cell>RMSE</cell><cell>109.46</cell><cell>79.45</cell><cell>72.86</cell><cell>75.73</cell><cell>69.80</cell><cell>80.81</cell><cell>82.85</cell><cell>80.85</cell><cell>73.86</cell><cell>68.26</cell></row><row><cell></cell><cell></cell><cell>MAPE</cell><cell>7.48%</cell><cell>5.55%</cell><cell>5.24%</cell><cell>5.15%</cell><cell>4.63%</cell><cell>5.05</cell><cell>5.60%</cell><cell>5.63%</cell><cell>4.88%</cell><cell>4.75%</cell></row><row><cell>QLD</cell><cell>Jan</cell><cell>RMSE</cell><cell>461.09</cell><cell cols="5">282.07 299.32 228.86 195.85 218.55</cell><cell>196.20</cell><cell>273.70</cell><cell>178.63</cell><cell>191.22</cell></row><row><cell></cell><cell></cell><cell>MAPE</cell><cell>5.25%</cell><cell>3.65%</cell><cell>3.61%</cell><cell>2.78%</cell><cell>2.41%</cell><cell>2.69%</cell><cell>2.56%</cell><cell>3.28%</cell><cell>2.21%</cell><cell>2.56%</cell></row><row><cell></cell><cell>Apr</cell><cell>RMSE</cell><cell>489.63</cell><cell cols="5">266.39 339.93 247.56 231.01 259.34</cell><cell>264.00</cell><cell>237.58</cell><cell>201.74</cell><cell>243.68</cell></row><row><cell></cell><cell></cell><cell>MAPE</cell><cell>6.25%</cell><cell>3.53%</cell><cell>3.77%</cell><cell>2.99%</cell><cell>2.78%</cell><cell>3.33%</cell><cell>3.47%</cell><cell>3.11%</cell><cell>2.44%</cell><cell>2.93%</cell></row><row><cell></cell><cell>Jul</cell><cell>RMSE</cell><cell>430.46</cell><cell cols="5">223.17 203.00 213.20 156.08 159.45</cell><cell>164.68</cell><cell>174.64</cell><cell>150.01</cell><cell>142.84</cell></row><row><cell></cell><cell></cell><cell>MAPE</cell><cell>5.90%</cell><cell>3.10%</cell><cell>3.03%</cell><cell>2.95%</cell><cell>2.32%</cell><cell>2.32%</cell><cell>2.46%</cell><cell>2.45%</cell><cell>2.29%</cell><cell>2.08%</cell></row><row><cell></cell><cell>Oct</cell><cell>RMSE</cell><cell>417.33</cell><cell cols="5">298.76 263.12 251.34 236.50 292.93</cell><cell>218.71</cell><cell>248.55</cell><cell>260.94</cell><cell>219.19</cell></row><row><cell></cell><cell></cell><cell>MAPE</cell><cell>5.54%</cell><cell>3.93%</cell><cell>3.46%</cell><cell>3.40%</cell><cell>2.88%</cell><cell>3.53%</cell><cell>2.82%</cell><cell>3.27%</cell><cell>3.15%</cell><cell>2.88%</cell></row><row><cell>VIC</cell><cell>Jan</cell><cell>RMSE</cell><cell>990.74</cell><cell cols="5">587.98 811.43 915.21 739.65 762.16</cell><cell>806.29</cell><cell>781.17</cell><cell>783.58</cell><cell>762.57</cell></row><row><cell></cell><cell></cell><cell>MAPE</cell><cell>9.48%</cell><cell>7.16%</cell><cell>9.32%</cell><cell>8.79%</cell><cell>8.77%</cell><cell>9.14%</cell><cell>9.48%</cell><cell>9.07%</cell><cell>9.32%</cell><cell>8.86%</cell></row><row><cell></cell><cell>Apr</cell><cell>RMSE</cell><cell>669.87</cell><cell cols="5">330.93 359.03 353.02 366.16 343.18</cell><cell>363.50</cell><cell>376.12</cell><cell>393.63</cell><cell>321.59</cell></row><row><cell></cell><cell></cell><cell>MAPE</cell><cell>8.40%</cell><cell>4.43%</cell><cell>4.95%</cell><cell>4.55%</cell><cell>4.65%</cell><cell>4.49%</cell><cell>4.67%</cell><cell>4.79%</cell><cell>5.04%</cell><cell>4.35%</cell></row><row><cell></cell><cell>Jul</cell><cell>RMSE</cell><cell>721.85</cell><cell cols="5">297.07 305.88 276.25 302.15 285.14</cell><cell>298.12</cell><cell>386.64</cell><cell>300.65</cell><cell>285.45</cell></row><row><cell></cell><cell></cell><cell>MAPE</cell><cell>9.76%</cell><cell>4.38%</cell><cell>4.29%</cell><cell>3.72%</cell><cell>4.29%</cell><cell>3.65%</cell><cell>4.27%</cell><cell>5.29%</cell><cell>4.26%</cell><cell>3.83%</cell></row><row><cell></cell><cell>Oct</cell><cell>RMSE</cell><cell>577.70</cell><cell cols="5">391.11 347.91 389.06 364.32 401.02</cell><cell>309.63</cell><cell>332.44</cell><cell>344.06</cell><cell>322.91</cell></row><row><cell></cell><cell></cell><cell>MAPE</cell><cell>8.30%</cell><cell>4.50%</cell><cell>4.79%</cell><cell>4.85%</cell><cell>4.16%</cell><cell>4.72%</cell><cell>3.78%</cell><cell>4.15%</cell><cell>3.92%</cell><cell>3.73%</cell></row><row><cell>SA</cell><cell>Jan</cell><cell>RMSE</cell><cell>433.57</cell><cell cols="5">337.10 411.66 401.25 349.87 363.49</cell><cell>280.70</cell><cell>397.66</cell><cell>288.85</cell><cell>238.09</cell></row><row><cell></cell><cell></cell><cell>MAPE</cell><cell>14.32%</cell><cell cols="5">13.34% 13.72% 13.62% 13.41% 14.43%</cell><cell>11.13%</cell><cell>13.80%</cell><cell>13.03%</cell><cell>10.46%</cell></row><row><cell></cell><cell>Apr</cell><cell>RMSE</cell><cell>180.20</cell><cell>124.43</cell><cell>119.4</cell><cell cols="3">117.61 127.90 105.39</cell><cell>121.60</cell><cell>126.78</cell><cell>124.60</cell><cell>125.31</cell></row><row><cell></cell><cell></cell><cell>MAPE</cell><cell>9.36%</cell><cell>6.71%</cell><cell>6.42%</cell><cell>6.67%</cell><cell>6.88%</cell><cell>6.56%</cell><cell>6.78%</cell><cell>6.87%</cell><cell>6.65%</cell><cell>6.76%</cell></row><row><cell></cell><cell>Jul</cell><cell>RMSE</cell><cell>289.94</cell><cell cols="5">150.84 151.06 148.23 154.67 148.55</cell><cell>141.78</cell><cell>153.22</cell><cell>161.71</cell><cell>160.82</cell></row><row><cell></cell><cell></cell><cell>MAPE</cell><cell>16.84%</cell><cell>8.54%</cell><cell>8.66%</cell><cell>8.50%</cell><cell>8.95%</cell><cell>8.59%</cell><cell>8.48%</cell><cell>9.53%</cell><cell>9.12%</cell><cell>9.60%</cell></row><row><cell></cell><cell>Oct</cell><cell>RMSE</cell><cell>240.53</cell><cell cols="5">210.72 233.48 204.16 218.30 203.53</cell><cell>203.38</cell><cell>199.77</cell><cell>209.70</cell><cell>192.74</cell></row><row><cell></cell><cell></cell><cell>MAPE</cell><cell>11.54%</cell><cell cols="3">8.94% 10.03% 9.33%</cell><cell>9.11%</cell><cell>9.32%</cell><cell>8.39%</cell><cell>8.54%</cell><cell>8.22%</cell><cell>8.11%</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This project is funded by the National Research Foundation Singapore under its Campus for Research Excellence and Technological Enterprise (CREATE)</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">EWMA model based shift-detection methods for detecting covariate shifts in non-stationary environments</title>
		<author>
			<persName><forename type="first">H</forename><surname>Raza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="659" to="669" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A pattern recognition framework for detecting dynamic changes on cyclic time series</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gharehbaghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ask</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Babic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="696" to="708" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Correlation and instance based feature selection for electricity load forecasting</title>
		<author>
			<persName><forename type="first">I</forename><surname>Koprinska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">G</forename><surname>Agelidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page" from="29" to="40" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Short-term load forecasting based on support vector regression and load profiling</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Sousa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Jorge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Neves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Energy Research</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="350" to="362" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Bargur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mandel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ha-</surname></persName>
		</author>
		<title level="m">Tekhniyon le-mehkar u-fituah (Haifa Israel), I. M. ha-energyah veha tashtit, Energy Consumption and Economic Growth in Israel: Trend Analysis</title>
		<imprint>
			<date type="published" when="1960">1960-1979. 1981</date>
		</imprint>
	</monogr>
	<note>Ministry of Energy and Infrastructure</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Forecasting seasonals and trends by exponentially weighted moving averages</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Holt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Forecasting</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="5" to="10" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Time series analysis: forecasting and control, Holden-Day series in time series analysis and digital processing</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E P</forename><surname>Box</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Jenkins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1976">1976</date>
			<publisher>Holden-Day</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A regression-based approach to short-term system load forecasting</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Papalexopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Hesterberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Power Systems</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1535" to="1547" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Forecasting the short-term demand for electricity: Do neural networks stand a better chance?</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Darbellay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Slama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Forecasting</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="71" to="83" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Using adaptive network based fuzzy inference system to forecast regional electricity loads</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Energy Conversion and Management</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="205" to="211" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Support-vector networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="297" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Artificial neural networks for short-term load forecasting in microgrids environment</title>
		<author>
			<persName><forename type="first">L</forename><surname>Hernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Baladrón</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Aguiar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Carro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sánchez-Esguevillas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lloret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Energy</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="252" to="264" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A hybrid short-term load forecasting with a new data preprocessing framework</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G M</forename><surname>Ghayekhloo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Menhaj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electric Power Systems Research</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="138" to="148" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Chaotic particle swarm optimization algorithm in a support vector regression electric load forecasting model</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">C</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Energy Conversation and Management</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="105" to="117" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Time series forecasting using restricted boltzmann machine</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kuremoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kimura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kobayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Obayashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Emerging Intelligent Computing Technology and Applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="17" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep learning for time series modeling</title>
		<author>
			<persName><forename type="first">E</forename><surname>Busseti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Osband</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CS</title>
		<imprint>
			<biblScope unit="volume">229</biblScope>
			<date type="published" when="2012">2012</date>
			<publisher>Stanford University</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-column deep neural networks for image classification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="page" from="3642" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep, big, simple neural nets for handwritten digit recognition</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Gambardella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3207" to="3220" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep learning in neural networks: An overview</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="85" to="117" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Ensemble methods in machine learning</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
	<note>Multiple classifier systems</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<author>
			<persName><forename type="first">S</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bienenstock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Doursat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural networks and the bias/variance dilemma</title>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Random forests</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="32" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Classification and Regression Trees, The Wadsworth and Brooks-Cole statistics-probability series</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Freidman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Olshen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Stone</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984">1984</date>
			<publisher>Taylor &amp; Francis</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Do we need hundreds of classifiers to solve real world classification problems?</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fernández-Delgado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cernadas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Barro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amorim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="3133" to="3181" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Comprehensive learning particle swarm optimization based memetic algorithm for model selection in short-term load forecasting using support vector regression</title>
		<author>
			<persName><forename type="first">L.-Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="15" to="25" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Ensemble deep learning for regression and time series forecasting</title>
		<author>
			<persName><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Suganthan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Amaratunga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Symposium on Computational Intelligence in Ensemble Learning (CIEL)</title>
		<meeting>IEEE Symposium on Computational Intelligence in Ensemble Learning (CIEL)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A hybrid anfis model based on empirical mode decomposition for stock time series forecasting</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="368" to="376" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">EMD based AdaBoost-BPNN method for wind speed forecasting</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Suganthan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Symposium on Computational Intelligence and Ensemble Learning</title>
		<meeting>IEEE Symposium on Computational Intelligence and Ensemble Learning<address><addrLine>Orlando, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A novel empirical mode decomposition with support vector regression for wind speed forecasting</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Suganthan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srikanth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Network and Learning Systems PP</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Cormen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Leiserson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Rivest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stein</surname></persName>
		</author>
		<title level="m">Introduction to Algorithms</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Wavelet-based nonlinear multiscale decomposition model for electricity load forecasting</title>
		<author>
			<persName><forename type="first">D</forename><surname>Benaouda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Murtagh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Starck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Renaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="139" to="154" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">The empirical mode decomposition and the hilbert spectrum for nonlinear and non-stationary time series analysis</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">E</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N.-C</forename><surname>Yen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Roy. Soc. London A</publisher>
			<biblScope unit="volume">454</biblScope>
			<biblScope unit="page" from="903" to="995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A comparative study of empirical mode decomposition-based short-term wind speed forecasting methods</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Suganthan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srikanth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Sustainable Energy</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="236" to="244" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A survey on data mining techniques applied to electricity-related time series forecasting</title>
		<author>
			<persName><forename type="first">F</forename><surname>Martínez-Álvarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Troncoso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Asencio-Cortés</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Riquelme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Energies</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="13162" to="13193" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Haykin</surname></persName>
		</author>
		<title level="m">Neural Networks: A Comprehensive Foundation</title>
		<imprint>
			<publisher>Prentice Hall</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
	<note>International edition</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Short-term load forecasting using SVR (support vector regression)-based radial basis function neural network with dual extended kalman filter</title>
		<author>
			<persName><forename type="first">C.-N</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Energy</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="413" to="422" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A survey of randomized algorithms for training neural networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Suganthan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">000</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Random vector functional link network for short-term electricity load demand forecasting</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Suganthan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srikanth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Amaratunga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">000</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Random decision forests</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Ho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the Third International Conference on Document Analysis and Recognition, Montreal, Que</title>
		<meeting>the Third International Conference on Document Analysis and Recognition, Montreal, Que</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="278" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The random subspace method for constructing decision forests</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Ho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="832" to="844" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Bagging predictors</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="123" to="140" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Random forests with ensemble of feature spaces</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Suganthan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="3429" to="3437" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Oblique decision tree ensemble via multisurface proximal support vector machine</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Suganthan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2165" to="2176" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning deep architectures for ai</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="127" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Hierarchical representation using nmf</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-Y</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Information Processing</title>
		<imprint>
			<biblScope unit="volume">8226</biblScope>
			<biblScope unit="page" from="466" to="473" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">5786</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Yamashita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yoshida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yamauchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fujiyoshi</surname></persName>
		</author>
		<title level="m">To be Bernoulli or to be Gaussian, for a restricted Boltzmann machine</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1520" to="1525" />
		</imprint>
	</monogr>
	<note>22nd International Conference on Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Training products of experts by minimizing contrastive divergence</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1771" to="1800" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A Practical Guide to Training Restricted Boltzmann Machines</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade: Second Edition</title>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="599" to="619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">On empirical mode decomposition and its algorithms</title>
		<author>
			<persName><forename type="first">G</forename><surname>Rilling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Flandrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gonc ¸alves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE-EURASIP Workshop on Nonlinear Signal and Image Processing NSIP-03</title>
		<meeting>IEEE-EURASIP Workshop on Nonlinear Signal and Image Processing NSIP-03<address><addrLine>Grado (Italy)</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A new method for mitigation of end effect in empirical mode decomposition</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">nd International Asia Conference on Informatics in Control, Automation and Robotics</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><surname>Aemo</surname></persName>
		</author>
		<ptr target="http://www.aemo.com.au/" />
		<title level="m">Australian energy market operator</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">LIBSVM: a library for support vector machines</title>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology (TIST)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Prediction as a candidate for learning deep hierarchical models of data, Master&apos;s thesis</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Palm</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
		<respStmt>
			<orgName>Technical University of Denmark</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Statistical comparisons of classifiers over multiple data sets</title>
		<author>
			<persName><forename type="first">J</forename><surname>Demšar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Combined model based on emd-svm for short-term wind power prediction</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Chinese Society for Electrical Engineering (CSEE)</title>
		<meeting>Chinese Society for Electrical Engineering (CSEE)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="102" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A hybrid model for wind speed prediction using empirical mode decomposition and artificial neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Renewable Energy</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="545" to="556" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Comparing support vector machines and feedforward neural networks with similar hidden-layer weights</title>
		<author>
			<persName><forename type="first">E</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Toppo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="959" to="963" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A trend fixed on firstly and seasonal adjustment model combined with the ε-SVR for short-term forecasting of electricity demand</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Energy Policy</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="4901" to="4909" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Electric load forecasting by seasonal recurrent svr (support vector regression) with chaotic artificial bee colony algorithm</title>
		<author>
			<persName><forename type="first">W.-C</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Energy</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="5568" to="5578" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">An adaptive fuzzy combination model based on self-organizing map and support vector regression for electric load forecasting</title>
		<author>
			<persName><forename type="first">J</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Energy</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="657" to="664" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Combining pattern sequence similarity with neural networks for forecasting electricity de-A in 1996-99. He moved to NTU in 1999</title>
		<author>
			<persName><forename type="first">I</forename><surname>Koprinska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Troncoso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Martínez-Álvarez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition</title>
		<imprint>
			<publisher>Elsevier) and Int</publisher>
		</imprint>
	</monogr>
	<note>Information Sciences</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">His research interests include evolutionary computation, pattern recognition, multi-objective evolutionary algorithms, applications of evolutionary computation and neural networks. His publications have been well cited according to Googlescholar Citations. His SCI indexed publications attracted over 1000 SCI citations in each calendar year 2013</title>
		<author>
			<persName><forename type="first">J</forename><surname>Of</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Swarm Intelligence Research Journals</title>
		<imprint>
			<publisher>IEEE Computational Intelligence Society</publisher>
			<date type="published" when="2014">2014. 2014 and 2015. 2014-2016</date>
		</imprint>
	</monogr>
	<note>He is a founding co-editor-in-chief of Swarm and Evolutionary Computation, an Elsevier Journal. His co-authored SaDE paper published in April 2009 won &quot;IEEE Trans. on Evolutionary Computation&quot; outstanding paper award in 2012. Dr Jane Jing Liang (his former PhD student) won the IEEE CIS Outstanding PhD dissertation award</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">He has long collaborations with industry partners in Europe, the U.S., and Asia. He is also a Co-Founder of four spin-out companies, including Nanoinstruments</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Gehan</surname></persName>
		</author>
		<author>
			<persName><surname>Wales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">K</forename><surname>Cardiff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1979, and the Ph.D. degree from the University of Cambridge</title>
		<meeting><address><addrLine>Cambridge, U.K.; California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1983">1983</date>
		</imprint>
		<respStmt>
			<orgName>Southampton University, the University of Liverpool, and Stanford University</orgName>
		</respStmt>
	</monogr>
	<note>He was at academic and research positions at. which is now part of Aixtron. He is one of four Founding Advisers to the Sri Lanka Institute of Nanotechnology. He is currently</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
