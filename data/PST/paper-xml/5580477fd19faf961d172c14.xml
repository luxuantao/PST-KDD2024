<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Machine Learning Approaches to Estimating Software</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Krishnamoorthy</forename><surname>Srinivasan</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">D</forename><surname>Fisher</surname></persName>
							<email>dfisher@vuse.vanderbilt.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Computer Consultants, Inc</orgName>
								<address>
									<settlement>Washington</settlement>
									<region>D.C</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Vanderbilt Univer-sity</orgName>
								<address>
									<settlement>Nashville</settlement>
									<region>Tennessee</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Madras</orgName>
								<address>
									<settlement>Madras</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Machine Learning Approaches to Estimating Software</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note type="submission">received October 1992; revised October 1993 and October 1994.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Software development effort</term>
					<term>machine learning</term>
					<term>decision trees</term>
					<term>regression trees</term>
					<term>and neural networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Accurate estimation of software development effort is critical in software engineering. Underestimates lead to time pressures that may compromise full functional development and thorough testing of software. In contrast, overestimates can result in noncompetitive contract bids and/or over allocation of development resources and personnel. As a result, many models for estimating software development effort have been proposed. This article describes two methods of machine learning, which we use to build estimators of software development effort from historical data. Our experiments indicate that these techniques are competitive with traditional estimators on one dataset, but also illustrate that these methods are sensitive to the data on which they are trained. This cautionary note applies to any model-construction strategy that relies on historical data. All such models for software effort estimation should be evaluated by exploring model sensitivity on a variety of historical data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION CCURATE estimation of software development effort</head><p>A has major implications for the management of software development. If management's estimate is too low, then the software development team will be under considerable pressure to finish the product quickly, and hence the resulting software may not be fully functional or tested. Thus, the product may contain residual errors that need to be corrected during a later part of the software life cycle, in which the cost of corrective maintenance is greater. On the other hand, if a manager's estimate is too high, then too many resources will be committed to the project. Furthermore, if the company is engaged in contract software development, then too high an estimate may fail to secure a contract.</p><p>The importance of software effort estimation has motivated considerable research in recent years. Parametric models such as C o c o M o [3], FUNCTION POINTS <ref type="bibr" target="#b1">[2]</ref>, and SLIM <ref type="bibr">[16]</ref> "calibrate" prespecified formulas for estimating development effort from historical data. Inputs to these models may include the experience of the development team, the required reliability of the software, the programming language in which the software is to be written, and an estimate of the final number Development Effort Douglas Fisher, Member, IEEE of delivered source lines of code (SLOC). In contrast, many methods of machine learning make no or minimal assumptions about the form of the function under study (e.g., development effort), but as with other approaches they depend on historical data. In particular, over a known set of training data, the leaming algorithm constructs "rules" that fit the data, and which hopefully fit previously unseen data in a reasonable manner as well. This article illustrates machine learning approaches to estimating software development effort using an algorithm for building regression trees <ref type="bibr">[4]</ref>, and a neural-network learning approach known as BACKPROPAGATION [ 191. Our experiments, using established case libraries <ref type="bibr">[3]</ref>, [ l l ] , indicate possible advantages of the approach relative to traditional models, but also point to limitations that motivate continued research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">MODELS FOR ESTIMATING SOFTWARE DEVELOPMENT EFFORT</head><p>Many models have been developed to estimate software development effort. Many of these models are parametric, in that they predict development effort using a formula of fixed form that kparameterized from historical data. In preparation for later discussion we summarize three such models that were highlighted in a previous study by Kemerer [ 111.  P u t " <ref type="bibr">[16]</ref> developed an early model known as SLIM, which estimates the cost of software by using SLOC as the major input. The underlying assumption of this model is that resource consumption, including personnel, varies with time and can be modeled with some degree of accuracy by the Rayleigh distribution:</p><p>where R, is the instananeous resource consumption, t is the time into the development effort, and IC is the time at which consumption is at its peak. The parameter IC and other "management parameters" are estimated by characteristics of a particular software project, notably estimated SLOC. The general relationship between inputs such as SLOC and management parameters can be determined from historical data.</p><p>The Constructive Cost Model (COCOMO) was developed by <ref type="bibr">Boehm [3]</ref> based on a regression analysis of 63 completed projects. COCOMO relates the effort required to develop a software project (in terms of person-months) to Delivered Source Instructions (DSI). Thus, like SLIM, C o c o ~o assumes SLOC as a major input. If the software project is judged to be straightforward, then the basic COCOMO model (COCOMObasic) relates the nominal development effort <ref type="bibr">( N )</ref> and DSI as follows:</p><formula xml:id="formula_0">N = 3.2 x (KDS1)1-05,</formula><p>where K D S I is the DSI in 1000s. However, the prediction of the basic C o c o ~o model can be modified using cost drivers .</p><p>Cost drivers are classified under four major headings relating to attributes of the product (e.g., required software reliability), computer platform (e.g., main memory limitations), personnel (e.g., analyst capability), and the project (e.g., use of modem programming practices). These factors serve to adjust the nominal effort up or down. These cost drivers and other considerations extend the basic model to intermediate and final forms.</p><p>The Function Point method was developed by Albrecht <ref type="bibr" target="#b1">[2]</ref>. Function points are based on characteristics of the project that are at a higher descriptive level than SLOC, such as the number of input transaction types and number of reports. A notable advantage of this approach is that it does not rely on SLOC, which facilitates estimation early in the project life cycle (i.e., during requirements definition), and by nontechnical personnel. To count function points requires that one count user functions and then make adjustments for processing complexity. There are five types of user function that are included in the function point calculation: external input types, extemal output types, logical internal file types, external interface file types, and extemal inquiry types. In addition, there are 14 processing complexity characteristics such as transaction rates and online updating. A function point is calculated based on the number of transactions and complexity characteristics. The development effort estimate given the function point, F , is:</p><formula xml:id="formula_1">N = 54 x F -13390.</formula><p>Recently, a case-based approach called ESTOR was developed for software effort estimation. This model was developed by Vicinanza et al. <ref type="bibr">[23]</ref> by obtaining protocols from a human expert. From a library of cases developed from expert-supplied protocols, an instance called the source is retrieved that is most "similar" to the target problem to be solved.</p><p>The solution of the most similar problem retrieved from the case library is adapted to account for differences between the source problem and the target problem using rules inferred from analysis of the human expert's protocols. An example of an adjustment rule is:</p><p>IF staff size of Source project is small, AND staff size of Target is large by 20%.</p><p>THEN increase effort estimate of Target In sum, there have been a variety of models developed for estimating development effort. With the exception of ESTOR these are parametric approaches that assume that an initial estimate can be provided by a formula that has been fit to historical data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="111.">MACHINE LEARNING APPROACHES TO ESTIMATING DEVELOPMENT EFFORT</head><p>This section describes two machine learning strategies that we use to estimate software development effort, which we assume is measured in development months (Ad). In many respects this work stems from a more general methodology for developing expert systems. Traditionally, expert systems have been developed by extracting the rules that experts apparently use by an interview process or protocol analysis (e.g., ESTOR), but an altemate approach is to allow machine learning programs to formulate rulebases from historical data. This methodology requires historical data on which to apply learning strategies.</p><p>There are several aspects of software development effort estimation that make it amenable to machine learning analysis. Most important, previous researchers have identified at least some of the attributes relevant to software development effort estimation, and historical databases defined over these relevant attributes have been accumulated. The following sections describe two very different learning algorithms that we use to test the machine learning approach. Other research using machine learning techniques for software resource estimation are found in [5], [14], [15], [22], which we will discuss throughout the paper. In short, our work adds to the collection of machine learning techniques available to software engineers, and our analysis stresses the sensitivity of these approaches to the nature of historical data and other factors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Learning Decision and Regression Trees</head><p>Many learning approaches have been developed that construct decision trees for classifying data [4], [17]. Fig. <ref type="figure" target="#fig_0">1</ref> illustrates a partial decision tree over Boehm's original 63 projects from which C o c o ~o was developed. Each project is described over dimensions such as AKDSI (i.e., adjusted delivered source instructions), TIME (i.e., the required system response time), and STOR (i.e., main memory limitations). The complete set of attributes used to describe these data is given in Appendix A. The mean of actual project development months labels each leaf of the tree. Predicting development effort for a project requires that one descend the decision tree along an appropriate path, and the leaf value along that path gives the estimate of development effort of the new project. The decision tree in Fig. <ref type="figure" target="#fig_0">1</ref> is referred to as a regression tree, because the intent of categorization is to generate a prediction along a continuous dependent dimension (here, software development effort).</p><p>There are many automatic methods for constructing decision and regression trees from data, but these techniques are typically variations on one simple strategy. A "top-down'' strategy examines the data and selects an attribute that best divides the data into disjoint subpopulations. The most important aspect of decision and regression tree learners is the criterion used to select a "divisive" attribute during tree construction. In one variation the system selects the attribute with values that maximally reduce the mean squared error ( M S E ) of the dependent dimension (e.g., software development effort) observed in the training data. The M S E of any set, S , where is the mean of the Y k values exhibited in S . The values of each attribute, A; partition the entire training data set, T , into subsets, T;j, where every example in Tij takes on the same value, say V, for attribute A;. The attribute, A;, that maximizes the difference:</p><formula xml:id="formula_2">A M S E = M S E ( T ) -</formula><p>M S E ( T ; j ) j is selected to divide the tree. Intuitively, the attribute that minimizes the error over the dependent dimension is used. While M S E values are computed over the training data, the inductive assumption is that selected attributes will similarly reduce error over future cases as well. This basic procedure of attribute selection is easily extended to allow continuously-valued attributes: all ordered 2-partitions of the observed values in the training data are examined. In essence, the dimension is split around each observed value. The effect is to 2-partition the dimension in IC -1 altemate ways (where IC is the number of observed values), and the binary "split" that is best according to A M S E is considered along with other possible attributes to divide a regression-tree node. Such "splitting" is common in the tree of Fig. <ref type="figure" target="#fig_0">1</ref>; see AKDSI, for example. Approaches have also been developed that split a continuous dimension into more than two ranges 191, 1151, though we will assume 2-partitions only. Similarly, techniques that 2-partition all attribute domains, for both continuous and nominally-valued (i.e., finite, unordered) attributes, have been explored (e.g., <ref type="bibr">[24]</ref>). For continuous attributes this bisection process operates as we have just described, but for a nominally-valued attribute all ways to group values of the attribute into two disjoint sets are considered. Suffice it to say that treating all attributes as though they had the same number of values (e.g., 2) for purposes of attribute selection mitigates certain biases that are present in some attribute selection measures (e.g., A M S E ) . As we will note again in Section IV, we ensure that all attributes are either continuous or binary-valued at the outset of regression-tree construction.</p><p>The basic regression-tree learning algorithm is summarized in Fig. <ref type="figure">2</ref>. The data set is first tested to see whether tree construction is worthwhile; if all the data are classified identically or some other statistically-based criterion is satisfied, then expansion ceases. In this case, the algorithm simply retums a leaf labeled by the mean value of the dependent dimension found in the training data. If the data are not sufficiently distinguished, then the best divisive attribute according to A M S E is selected, the attribute's values are used to partition the data into subsets, and the procedure is recursively called on these subsets to expand the tree. When used to construct predictors along continuous dimensions, this general procedure is referred to as recursive-partitioning regression. Our experiments use a partial reimplementation of a system known as CART <ref type="bibr">[4]</ref>. We refer to our reimplementation as CARTX.</p><p>Previously, Porter and Selby [14], 1151, <ref type="bibr">[22]</ref>, have investigated the use of decision-tree induction for estimating development effort and other resource-related dimensions. Their work assumes that if predictions over a continuous dependent dimension are required, then the continuous dimension is "discretized" by breaking it into mutually-exclusive ranges. More commonly used decision-tree induction algorithms, which assume discrete-valued dependent dimensions, are then applied to the appropriately classified data. In many cases this preprocessing of a continuous dependent dimension may be profitable, though regression-tree induction demonstrates that the general tree-construction approach can be adapted for direct manipulation of a continuous dependent dimension. This is also the case with the learning approach that we describe next. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. A Neural Network Approach to Learning</head><p>A learning approach that is very different from that outlined above is BACKPROPAGATION, which operates on a network of simple processing elements as illustrated in Fig. <ref type="figure" target="#fig_1">3</ref>. This basic architecture is inspired by biological nerve nets, and is thus called an artificial neural network. Each line between processing elements has a corresponding and distinct weight. Each processing unit in this network computes a nonlinear function of its inputs and passes the resultant value along as its output. The favored function is</p><formula xml:id="formula_3">1 \ l</formula><p>where xi wzfi is a weighted sum of the inputs, f i , to a processing element [ 191, [25].</p><p>The network generates output by propagating the initial inputs, shown on the lefthand side of Fig. <ref type="figure" target="#fig_1">3</ref>, through subsequent layers of processing elements to the final output layer. This net illustrates the kind of mapping that we will use for estimating software development effort, with inputs corresponding to various project attributes, and the output line corresponding to the estimated development effort. The inputs and output are restricted to numeric values. For numerically-valued attributes this mapping is natural, but for nominal data such as LANG (implementation language), a numeric representation must be found. In this domain, each value of a nominal attribute is given its own input line. If the value is present in an observation then the input line is set to 1.0, and if the value is absent then it is set to 0.0. Thus, for a given observation the input line corresponding to an observed nominal value (e.g., COB) will be 1.0, and the others (e.g., FTN) will be 0.0. Our application requires only one network output, but other applications may require more than one.</p><p>Details of the BACKPROPAGATION learning procedure are beyond the scope of this article, but intuitively the goal of learning is to train the network to generate appropriate output patterns for corresponding input patterns. To accomplish this, X An example of function approximation by a regression tree. comparisons are made between a network's actual output pattern and an a priori known correct output pattern. The difference or error between each output line and its correct corresponding value is "backpropagated" through the net and guides the modification of weights in a manner that will tend to reduce the collective error between actual and correct outputs on training patterns. This procedure has been shown to converge on accurate mappings between input and output patterns in a variety of domains [211, 1251.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Approximating Arbitrary Functions</head><p>In trying to approximate an arbitrary function like development effort, regression trees approximate a function with a "staircase" function. Fig. <ref type="figure">4</ref> illustrates a function of one continuous, independent variable. A regression tree decomposes this function's domain so that the mean at each leaf reflects the function's range within a local region. The "hidden" processing elements that reside between the input and output layers of a neural network do roughly the same thing, though the approximating function is generally smoothed. The granularity of this partitioning of the function is modulated by the depth of a regression tree or the number of hidden units in a network.</p><p>Each learning approach is nonparametric, since it makes no a priori assumptions about the form of the function being approximated. There are a wide variety of parametric methods for function approximation such as regression methods of statistics and polynomial interpolation methods of numerical analysis [ 101. Other nonparametric methods include genetic algorithms <ref type="bibr">[7]</ref> and nearest neighbor approaches <ref type="bibr">[ 11,</ref> though we will not elaborate on any of these alternatives here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. SensitiviQ to Conjiguration Choices</head><p>Both BACKPROPAGATION and CARTX require that the analyst make certain decisions about algorithm implementation. For example, BACKPROPAGATION can be used to train networks with differing numbers of hidden units. Too few hidden units can compromise the ability of the network to approximate a desired function. In contrast, too many hidden units can lead to "overfitting," whereby the leaming system fits the "noisE" present in the training data, as well as the meaningful trends that we would like to capture. BACKPROPAGATION is also typically trained by iterating through the training data many times. In general, the greater the number of iterations, the greater the reduction in error over the training sample, though there is no general guarantee of this. Finally, BACKPROPAGATION assumes that weights in the neural network are initialized to small, random values prior to training. The initial random weight settings can also impact learning success, though in many applications this is not a significant factor. There are other parameters that can effect BACKPROPAGATION 's performance, but we will not explore these here.</p><p>In CARTX, the primary dimension under control by the experimenter is the depth to which the regression tree is allowed to grow. Growth to too great a deptkcan lead to overfitting, and too little growth can lead to underfitting. Experimental results of Section IV-B illustrate the sensitivity of each learning system to certain configuration choices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I v . OVERVIEW OF EXPERIMENTAL STUDIES</head><p>We conducted several experiments with CARTX and BACKPROPAGATION for the task of estimating software development effort. In general, each of our experiments partitions historical data into samples used to train our learning systems, and disjoint samples used to test the accuracy of the trained classifier in predicting development effort.</p><p>For purposes of comparison, we refer to previous experimental results by Kemerer [ 1 I]. He conducted comparative analyses between SLIM, COCOMO, and FUNCTION POINTS on a database of 15 projects.' These projects consist mainly of business applications with a dominant proportion of them (12115) written in the COBOL language. In contrast, the COCOMO database includes instances of business, scientific, and system software projects, written in a variety of languages including COBOL, PL1, HMI, and FORTRAN. For comparisons involving COCOMO, Kemerer coded his 15 projects using the same attributes used by Boehm.</p><p>One way that Kemerer characterized the fit between the predicted (Mest) and actual (Matt&gt; development person-months was by the magnitude of relative error ( M R E ) :</p><p>This measure normalizes the difference between actual and predicted development months, and supplies an analyst with a measure of the reliability of estimates by different models. However, when using a model developed at one site for estimation at another site, there may be local factors that are not modeled, but which nonetheless impact development effort in a systematic way. Thus, following earlier work by Albrecht <ref type="bibr" target="#b1">[2]</ref>, Kemerer did a linear regression/correlation analysis to "calibrate" the predictions, with Mest treated as the independent variable and Matt treated as the dependent linear relationship and those close to 0.0 suggest no such relationship. Our experiments will characterize the abilities of BACKPROPAGATION and CARTX using the same dimensions as Kemerer: M R E and R 2 .</p><p>As we noted, each system imposes certain constraints on the representation of data. There are a number of nominally-valued attributes in the project databases, including implementation language. BACKPROPAGATION requires that each value of such an attribute was treated as a binary-valued attribute that was either present (1) or absent (0) in each project. Thus, each value of a nominal attribute corresponded to a unique input to the neural network as noted in Section 111-B. We represent each nominal attribute as a set of binary-valued attributes for CARTX as well. As we noted in Section 111-A this mitigates certain biases in attribute selection measures such as A M S E .</p><p>In contrast, each continuous attribute identified by Boehm corresponded to one input to the neural network. There was one output unit, which reflected a prediction of development effort and was also continuous. Preprocessing for the neural network normalized these values between 0.0 and 1.0. A simple scheme was used where each value was divided by the maximum of the values for that attribute in the training data. It has been shown empirically that neural networks converge relatively quickly if all the values for the attributes are between zero and one <ref type="bibr">[12]</ref>. No such normalization was done for CARTX, since it would have no effect on CARTX'S performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experiment I : Comparison with Kemerer 's Results</head><p>Our first experiment compares the performance of machine learning algorithms with standard models of software development estimation using Kemerer's data as a test sample. To test CARTX and BACKPROPAGATION, we trained each system on COCOMO'S database of 63 projects and tested on Kemerer's 15 projects. For BACKPROPAGATION we initially configured the network with 33 input units, 10 hidden units, and 1 output unit, and required that the training set error reach 0.00001 or continue for a maximum of 12 000 presentations of the training data. Training ceased after 12 000 presentations without converging to the required error criterion. The -experiment was done on an AT&amp;T PC 386 under DOS. It required about 6-7 hours for 12 000 presentations of the training pattems. We actually repeated this experiment 10 times, though we only report the results of one run here; we summarize the complete set of experiments in Section IV-B.</p><p>In our initial configuration of CARTX, we allowed the regression tree to grow to a "maximum" depth, where each leaf represented a single software project description from the C o c o ~o data. We were motivated initially to extend the tree to singleton leaves, because the data is very sparse relative to the number of dimensions used to describe each data point; our concern is not so much with overfitting, as it is with underfitting the data. Experiments with the regression tree learner were performed on a SUN 3/60 under UNIX, and required about a minute. The predictions obtained from the learning algorithms (after training on the COCOMO data) are shown in Table <ref type="table" target="#tab_1">I</ref> with the actual person-months of Kemerer's  the R2 dimension learning methods provide significant fits to the data. Unfortunately, a primary weakness of these learning approaches is that their performance is sensitive to a number of implementation decisions. Experiment 2 illustrates some of these sensitivities.</p><p>15 projects. We note that some predictions of CARTX do not correspond to exact person-month values of any COCoMO (training set) project, even though the regression tree was developed to singleton leaves. This stems from the presence of missing values for some attributes in Kemerer's data. If, during classification of a test project, we encounter a decision node that tests an attribute with an unknown value in the test project, both subtrees under the decision node are explored.</p><p>In such a case, the system's final prediction of development effort is a weighted mean of the predictions stemming from each subtree. The approach is similar to that described in [ 171. In sum, Experiment 1 illustrates two points. In an absolute sense, none of the models does particularly well at estimating software development effort, particularly along the M R E dimension, but in a relative sense both learning approaches are competitive with traditional models examined by Kemerer on one dataset. In general, even though M R E is high in the 2Results are reported for COCOMO-BASIC (i.e., without cost drivers), which was comparable to the intermediate and detailed models on this data. In addition, Kemerer actually reported x2, which is R2 adjusted for degrees of freedom, and which is slightly lower than the unadjusted R' values that we report. x2 values reported by Kemerer are 0.55.0.68, and 0.88 for FUNCTION POINTS, COCOMO, and SLIM, respectively. 3Both the slope and R value are significant at the 99% confidence level.</p><p>The t coefficients for determining the significance of slope are 8.048 and 7.25 for CARTX and BACKPROPAGATION, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experiment 2: Sensitivity of the Learning Algorithms</head><p>We have noted that each learning system assumes a number of important choices such as depth to which to "grow" regression trees, or the number of hidden units included in the neural network. These choices can significantly impact the success of learning. Experiment 2 illustrates the sensitivity of our two learning systems relative to different choices along these dimensions. In particular, we repeated Experiment 1 using BACKPROPAGATION with differing numbers of hidden units and using CARTX with differing constraints on regression-tree growth.</p><p>Table <ref type="table" target="#tab_5">I11</ref> illustrates our results with BACKPROPAGATION. Each cell summarizes results over 10 experimental trials, rather than one trial, which was reported in Section IV-A for presentation purposes. Thus, Max, and Min values of R2 and M R E in each cell of Table <ref type="table" target="#tab_5">I11</ref> suggest the sensitivity of BACKPROPAGATION to initial random weight settings, which were different in each of the 10 experimental trials. The experimental results of Section IV-A reflect the "best" among the 10 trials summarized in Table <ref type="table">111</ref>'s 10hidden-unit column. In general, however, for 5, 10, and 15 hidden units, M R E scores are still comparable or superior to some of the other models summarized in Table <ref type="table">11</ref>, and mean R2 scores suggest that significant linear relationships between predicted and actual development months are often found. Poor results obtained with no hidden units indicate the importance of these for accurate function approximation.</p><p>The performance of CARTX can vary with the depth to which we extend the regression tree. The results of Experiment 1 are repeated here, and represent the case where required accuracy over the training data is 0%-that is, the tree is decomposed to singleton leaves. However, we experimented with more conservative tree expansion policies, where CARTX extended the tree only to the point where an error threshold (relative to the training data) is satisfied. In particular, trees were grown to leaves where the mean M R E among projects at a leaf was less than or equal to a prespecified threshold that ranged from 0% to 500%. The M R E of each project at a leaf 0 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE IV C U T X RESULTS WITH VARYING TRAINING ERROR THRESHOLDS</head><p>was calculated by where 2 is the mean person-months development effort of projects at that node.</p><p>Table <ref type="table" target="#tab_6">IV</ref> shows CARTX'S performance when we vary the required accuracy of the tree over the training data. Table <ref type="table" target="#tab_1">entries</ref> correspond to the M R E and R2 scores of the learned trees over the Kemerer test data. In general, there is degradation in performance as one tightens the requirement for regressiontree expansion, though there are applications in which this would not be the case. Importantly, other design decisions in decision and regression-tree systems, such as the manner in which continuous attributes are "split" and the criteria used to select divisive attributes, might also influence prediction accuracy. Selby and Porter [22] have evaluated different design choices along a number of dimensions on the success of decision-tree induction systems using NASA software project descriptions as a test-bed. Their evaluation of decision trees, not regression trees, limits the applicability of their findings to the evaluation reported here, but their work sets an excellent example of how sensitivity to various design decisions can be evaluated.</p><p>The performance of both systems is sensitive to certain configuration choices, though we have only examined sensitivity relative to one or two dimensions for each system. Thus, it seems important to posit some intuition about how learning systems can be configured to yield good results on new data, given only knowledge of performance on training data. In cases where more training data is available a holdout method can be used for selecting an appropriate network or regression- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experiment 3: Sensitivity to Training and Test Data</head><p>Thus far. our results suggest that using learning algorithms to discover regularities in a historical database can facilitate predictions on new cases. In particular, comparisons between our experimental results and those of Kemerer indicate that relatively speaking, learning system performance is competitive with some traditional approaches on one common data set. However, Kemerer found that performance of algorithmic approaches was sensitive to the test data. For example, when a selected subset of 9 of the 15 cases was used to test the models, each considerably improved along the R2 dimension. By implication, performance on the other 6 projects was likely poorer. We did not repeat this experiment, but we did perform similarly-intended experiments in which the C o c o ~O and Kemerer data sets were combined into a single dataset of 78 projects; 60 projects were randomly selected for training the leaming algorithms and the remaining 18 projects were used for test. Table <ref type="table" target="#tab_6">V</ref> summarizes the results over 20 such randomized trials. The low average R2 should not mask the fact that many runs yielded strong linear relationships. For example, on 9 of the 20 CARTX runs, R2 was above 0.80.</p><p>We also ran 20 randomized trials in which 10 of Kemerer's cases were used to train each leaming algorithm, and 5 were used for test. The results are summarized in Table <ref type="table" target="#tab_6">VI</ref>. This experiment was motivated by a study with ESTOR [23], a casebased approach that we summarized in Section 11: an expert's protocols from 10 of Kemerer's projects were used to construct a "case library" and the remaining 5 cases were used to test the model's predictions; the particular cases used for test were not reported, but ESTOR outperformed C o c o ~o and FUNCTION POINTS on this set.</p><p>We do not know the robustness of ESTOR in the face of the kind of variation experienced in our 20 randomized trials (Table <ref type="table" target="#tab_6">VI</ref>), but we might guess that rules inferred from expert problem solving, which ideally stem from human learning over a larger set of historical data, would render ESTOR more robust along this dimension. However, our experiments and those of Kemerer with selected subsets of his 15 cases suggest that care must be taken in evaluating the robustness of any model with such sparse data. In defense of Vicinanza's et aZ.P methodology, we should note that the creation of.. a case library depended on an analysis of expert protocols and the derivation of expert-like rules for modifying the predictions of best matching cases, thus increasing the "cost" of model construction to a point that precluded more complete randomized trials. Vicinanza et al. also point out that their study is best viewed as indicating ESTOR'S "plausibility" as a good estimator, while broader claims require further study.</p><p>In addition to experiments with the combined C o c o ~o and Kemerer data, and the Kemerer data alone, we experimented with the CocoMo data alone for completeness. When experimenting with Kemerer's data alone, our intent was to weakly explore the kind of variation faced by ESTOR. Using the C o c O ~o data we have no such goal in mind. Thus, this analysis uses an N-fold cross validation or a "leave-one-out" methodology, which is another form of resampling. In particular, if a data sample is relatively sparse, as ours is, then for each of N (i.e., 63) projects, we remove it from the sample set, train the learning system with the remaining N -1 samples, and then test on the removed project. M R E and R2 are computed over the N tests. CARTX'S R2 value was 0.56 (144.48+0.74~, t = 8.82) and MRE was 125.2%. In this experiment we only report results obtained with CARTX, since a fair and comprehensive exploration of BACKPROPAGATION across possible network configurations is computationally expensive and of limited relevance. Suffice it to say that over the CocoMo data alone, which probably reflects a more uniform sample than the mixed CocoMo/Kemerer data, CARTX provides a significant linear fit to the data with markedly smaller M R E than its performance on Kemerer's data.</p><p>In sum, our initial results indicating the relative merits of a learning approach to software development effort estimation must be tempered. In fact, a variety of randomized experiments reveal that there is considerable variation in the performance of these systems as the nature of historical training data changes. This variation probably stems from a number of factors. Notably, there are many projects in both the CocoMo and Kemerer datasets that differ greatly in their actual development effort, but are very similar in other respects, including SLOC. Other characteristics, which are currently unmeasured in the COCOMO scheme, are probably responsible for this variation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. GENERAL DISCUSSION</head><p>Our experimental comparisons of CARTX and BACK-PROPAGATION with traditional approaches to development effort estimation suggest the promise of an automated learning approach to the task. Both learning techniques performed well on the R2 and MRE dimensions relative to some other approaches on the same data. Beyond this cursory summary, our experimental results and the previous literature suggest several issues that merit discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Limitations of Learning from Historical Data</head><p>There are well-known limitations of models constructed using historical data. In particular, attributes used to predict software development effort can change over time andfor differ between software development environments. Mohanty [ 131 makes this point in comparisons between the predictions of a wide variety of models on a single hypothetical software project. In particular, Mohanty surveyed approximately 15 models and methods for predicting software development effort. These models were used to predict software development effort of a single hypothetical software project. Mohanty's main finding was that estimated effort on this single project varied significantly over models. Mohanty points out that each model was developed and calibrated with data collected within a unique software environment. The predictions of these models, in part, reflect underlying assumptions that are not explicitly represented in the data. For example, software development sites may use different development tools. These tools are constant within a facility, and thus not represented explicitly in data collected by that facility, but this environmental factor is not constant across facilities.</p><p>Differing environmental factors not reflected in data are undoubtedly responsible for much of the unexplained variance in our experiments. To some extent, the R2 derived from linear regression is intended to provide a better measure of a model's "fit" to arbitrary new data than M R E in cases where the environment from which a model was derived is different from the environment from which new data was drawn. Even so, these environmental differences may not be systematic in a way that is well accounted for by a linear model. In sum, great care must be taken when using a model constructed from data from one environment to make predictions about data from another environment. Even within a site, the environment may evolve over time, thus compromising the benefits of previously-derived models. Machine learning research has recently focussed on the problem of tracking the accuracy of a learned model over time, which triggers relearning when experience with new data suggests that the environment has changed [6]. However, in an application such as software development effort estimation, there are probably explicit indicators that an environmental change is occurring or will occur (e.g., when new development tools or quality control practices are implemented).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Engineering the Dejinition of Data</head><p>If environmental factors are relatively constant, then there is little need to explicitly represent these in the description of data. However, when the environment exhibits variance along some dimension, it often becomes critical that this variance be codified and included in data description. In this way, differences across data points can be observed and used in model construction. For example, Mohanty argues that the desired quality of the finished product should be taken into account when estimating development effort. A comprehensive survey by Scacchi [20] of previous software production studies leads to considerable discussion on the pros and cons of many attributes for software project representation.</p><p>Thus, one of the major tasks is deciding upon the proper codification of factors judged to be relevant. Consider the dimension of response time requirements (i.e., TIME) which was included by Boehm in project descriptions. This attribute was selected by CARTX during regression-tree construction. However, is TIME an "optimal" codification of some aspect of software projects that impacts development effort? Consider that strict response time requirements may motivate greater coupling of software modules, thereby necessitating greater communication among developers and in general increasing development effort. If predictions of deielopment effort must be made at the time of requirements analysis, then perhaps TIME is a realistic dimension of measurement, but better predictive models might be obtained and used given some measure of software component coupling.</p><p>In sum, when building models via machine learning or statistical methods, it is rarely the case that the set of descriptive attributes is static. Rather, in real-world success stories involving machine learning tools the set of descriptive attributes evolves over time as attributes are identified as relevant or irrelevant, the reasons for relevance are analyzed, and additional or replacement attributes are added in response to this analysis [8]. This "model" for using learning systems in the real world is consistent with a long-term goal of Scacchi [20], which is to develop a knowledge-based "corporate memory" of software production practices that is used for both estimating and controlling software development. The machine-learning tools that we have described, and other tools such as ESTOR, might be added to the repertoire of knowledge-acquisition strategies that Scacchi suggests. In fact, Porter and Selby [ 141 make a similar proposal by outlining the use of decision-tree induction methods as tools for software development.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. The Limitations of Selected Learning Methods</head><p>Despite the promising results on Kemerer's common database, there are some important limitations of CARTX and BACKFJROPAGATION. We have touched upon the sensitivity to certain configuration choices. In addition to these practical limitations, there are also some important theoretical limitations, primarily concerning CARTX. Perhaps the most important of these is that CARTX cannot estimate a value along a dimension (e.g., software development effort) that is outside the range of values encountered in the training data. Similar limitations apply to a variety of other techniques as well (e.g., nearest neighbor approaches of machine learning and statistics). In part, this limitation appears responsible for a sizable amount of error on test data. For example, in the experiment illustrating CARTX'S sensitivity to training data using 10/5 splits of Kemerer's projects (Section IV-C), CARTX is doomed to being at least a factor of 3 off the mark when estimating the person-month effort required for the project requiring 23.20 M or the project requiring 1107.31 M; the projects closest to each among the remaining 14 projects are 69.90 M and 336.30 M, respectively.</p><p>The root of CARTX'S difficulties lies in its labeling of each leaf by the mean of development months of projects classified at the leaf. An alternative approach that would enable CARTX to extrapolate beyond the training data, would label each leaf by an equation derived through regression-e.g., a linear regression. After classifying a project to a leaf, the regression equation labeling that leaf would then be used to predict development effort given the object's values along the independent variables. In addition, the criterion for selecting divisive attributes would be changed as well. To illustrate, consider only two independent attributes, development team experience and KDSI, and the dependent variable of software development effort. CARTX would undoubtedly select KDSI, since lower (higher) values of KDSI tend to imply lower (higher) means of development effort. In contrast, development team experience might not provide as good a fit using CARTX'S error criterion. However, consider a CART-like system that divides data up by an independent variable, finds a best fitting linear equation that predicts development effort given development team experience and KDSI, and assesses error in terms of the differences between predictions using this best fitting equation and actual development months. Using this strategy, development team experience might actually be preferred; even though lesser (greater) experience does not imply lesser (greater) development effort, development team experience does imply subpopulations for which strong linear relationships might exist between independent and dependent variables. For example, teams with lesser experience may not adjust as well to larger projects as do teams with greater experience; that is, as KDSI increases, development effort increases are larger for less experienced teams than more experienced teams. Recently, machine learning systems have been developed that have this flavor [ 181. We have not yet experimented with these systems, but the approach appears promising.</p><p>The success of CARTX, and decisiodregression-tree learners generally, may also be limited by two other processing characteristics. First, CARTX uses a greedy attribute selection strategy-tree construction assesses the informativeness of a single attribute at a time. This greedy strategy might overlook attributes that participate in more accurate regression trees, particularly when attributes interact in subtle ways. Second, CARTX builds one classifier over a training set of software projects. This classifier is static relative to the test projects; any subsequent test project description will match exactly one conjunctive pattern, which is represented by a path in the regression tree. If there is noise in the data (e.g., an error in the recording of an attribute value), then the prediction stemming from the regression-tree path matching a particular test project may be very misleading. It is possible that other conjunctive patterns of attribute values matching a particular test project, but which are not represented in the regression tree, could ameliorate CARTX'S sensitivity to errorful or otherwise noisy project descriptions.</p><p>The Optimized Set Reduction (OSR) strategy of <ref type="bibr">Briand,</ref><ref type="bibr">Basili,</ref><ref type="bibr">and Thomas [5]</ref> is related to the CARTX approach in several important ways, but may mitigate problems associated with CARTX-OSR conducts a more extensive search for multiple patterns that match each test observation. In contrast to CARTX'S construction of a single classifier that is static relative to the test projects, OSR can be viewed as dynamically building a different classifier for each test project. The specifics of OSR are beyond the scope of this paper, but suffice it to say that OSR looks for multiple patterns that are statistically justified by the training project descriptions and that match a given test project. The predictions stemming from different patterns (say, for software development effort) are then combined into a single, global prediction for the test project.</p><p>OSR was also evaluated in [5] using Kemerer's data for test, and C o c o ~o data as a (partial) training ample.^ The authors report an average M R E of 94% on Kemerer's data.</p><p>However, there are important differences in experimental design that make a comparison between results with OSR, BACKPROPAGATION, and CARTX unreliable. In particular, when OSR was used to predict software development effort for a particular Kemerer project, the C o c o ~o data and the remaining 14 Kemerer projects were used as training examples. In addition, recognizing that Kemerer's projects were selected from the same development environment, OSR was configured to weight evidence stemming from these projects more heavily than those in the Cocomo data set. The sensitivity of results to this "weighting factor" is not described. We should note that the experimental conditions assumed in <ref type="bibr">[5]</ref> are quite reasonable from a pragmatic standpoint, particularly the decision to weight projects more heavily that are drawn from the same environment as the test project. These different training assumptions simply confound comparisons between experimental results, and OSR's robustness across differing training and test sets is not reported. In addition, like the work of Porter and Selby [14], [15], <ref type="bibr">[22]</ref>, OSR assumes that the dependent dimension of software development effort is nominally-valued for purposes of learning. Thus, this dimension is partitioned into a number of collectivelyexhaustive and mutually-exclusive ranges prior to learning. Neither BACKPROPAGATION nor CARTX requires this kind of preprocessing. In any case, OSR appears unique relative to other machine learning systems in that it does not learn a static classifier; rather, it combines predictions from multiple, dynamically-constructed patterns. Whether one is interested in software development effort estimation or not, this latter facility appears to have merits that are worth further exploration.</p><p>In sum, CARTX suffers from certain theoretical limitations: it cannot extrapolate beyond the data on which it was trained, it uses a greedy tree expansion strategy, and the resultant classifier generates predictions by matching a project against a single conjunctive pattern of attribute values. However, there appear to be extensions that might mitigate these problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUDING REMARKS</head><p>This article has compared the CARTX and BACKPROPAGATION learning methods to traditional approaches for software effort estimation. We found that the learning approaches were competitive with SLIM, COCOMO, and FUNCTION POINTS as represented in a previous study by Kemerer. Nonetheless, further experiments showed the sensitivity of learning to various aspects of data selection and representation. Mohanty and Kemerer indicate that traditional models are quite sensitive as well.</p><p>A primary advantage of learning systems is that they are adaptable and nonparametric; predictive models can be tailored to, the data at a particular site. Decision and regression trees are particularly well-suited to this task because they make explicit the attributes (e.g., TIME) that appear relevant to the prediction task. Once implicated, a process that engineers the data definition is often required to explain relevant and irrelevant aspects of the data, and to encode it accordingly. This process is best done locally, within a software shop, where the idiosyncrasies of that environment can be factored in or out. In such a setting analysts may want to investigate the behavior of systems like BACKPROPAGATION, CART, and related approaches [5], <ref type="bibr">[14]</ref>, [ 151, <ref type="bibr">[22]</ref> over a range of permissible configurations, thus obtaining performance that is optimal in their environment. A. Product Attributes attribute measures how reliable the software should be. For example, if serious financial consequences stem from a software fault, then the required reliability should be high.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">) Database Size (DATA):</head><p>The size of the database to be used by software may effect development effort. Larger databases generally suggest that more time will be required to develop the software product.</p><p>3 ) Product Complexity ( C P U ) : The application area has a bearing on the software development effort. For example, communications software will likely have greater complexity than software developed for payroll processing.</p><p>4 ) Adaptation Adjustment Factor (AAF): In many cases software is not developed entirely from scratch. This factor reflects the extent that previous designs are reused in the new project.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Computer Attributes</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">) Execution Time Constraint (TIME):</head><p>If there are constraints on processing time, then the development time may be greater.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Main Storage Constraint (STOR):</head><p>If there are memory constraints, then the development effort will tend to be high.</p><p>3) Virtual Machine Volatility (VIRT): If the underlying hardware andor system software change frequently, then development effort will be high.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Personnel Attributes</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">) Analyst Capability (ACAP):</head><p>If the analysts working on the software project are highly skilled, then the development effort of the software will be less than projects with less-skilled analysts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Applications Experience (AEXP):</head><p>The experience of project personnel influences the software development effort.</p><p>3) Programmer Capability (PCAP): This is similar to ACAP, but it applies to programmers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Virtual Machine Experience (VEXP):</head><p>Programmer experience with the underlying hardware and the operating system has a bearing on development effort.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">) Language Experience (LEXP):</head><p>Experience of the programmers with the implementation language affects the software development effort.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6) Personnel Continuity Turnover (CONT):</head><p>If the same personnel work on the project from beginning to end, then the development effort will tend to be less than similar projects experiencing greater personnel turnover.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D: Project Attributes</head><p>programming practices like structured software design reduces the development effort.</p><p>2) Use of Software Tools (TOOL): Extensive use of soft- ware tools like source-line debuggers and syntax-directed editors reduces the software development effort.</p><p>3) Required Development Schedule (SCED): If the development schedule of the software project is highly constrained, then the development effort will tend to be high.</p><p>Apart from the attributes mentioned above, other attributes that influence the development are: programming language, and the estimated lines of code (unadjusted and adjusted for the use of existing software).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I ) Modern Programming Practices (MODP): Modern</head><p>Douglas Fisher (M'92) received his Ph.D. in information and computer science from the University of California at Irvine in 1987.</p><p>He is currently an Associate Professor in computer science at Vanderbilt University. He is an Associate Editor of Machine Learning, and IEEE Expert, and serves on the editorial board of the Joumal ofAn$cial Intelligence Research. His research interests include machine learning, cognitive modeling, data analysis, and cluster analysis. An electronic addendu to this article, which reports any subsequent analysis, can be found at (http://www.vuse.vanderbilt.edurdfisher/dfisher.html).</p><p>Dr. Fisher is a member of the ACM and AAAI.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 2. Decisiodregression-tree learning algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. A network architecture for software development effort estimation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig</head><label></label><figDesc>Fig. 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>variable. The R2 value indicates the amount of variation in the actual values accounted for by a linear relationship with the estimated values. R2 values close to 1.0 suggest a strong "We thank Professor Chris Kemerer for supplying this dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>the C o c o ~o and Kemerer databases were used to develop the C o c o ~o model. The following is a brief description of the attributes and some of their suspected influences on development effort. The interested reader is referred to [3] for a detailed exposition of them. These attributes can be classified under four major headings. They are Product Attributes; Computer Attributes; Personnel Attributes; and Project Attributes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I CARTX</head><label>I</label><figDesc>AND BACKPROPAGATION ESTIMATES ON KEMERERS DATA -</figDesc><table><row><cell cols="2">Actual -CARTX</cell><cell>3AC K P ROP</cell></row><row><cell>287.00</cell><cell>1893.30</cell><cell>86.45</cell></row><row><cell>82.50</cell><cell>162.03</cell><cell>14.14</cell></row><row><cell>1107.31</cell><cell>11400.00</cell><cell>1000.43</cell></row><row><cell>86.90</cell><cell>243.00</cell><cell>88.37</cell></row><row><cell>336.30</cell><cell>6600.00</cell><cell>540.4?</cell></row><row><cell>84.00</cell><cell>129.17</cell><cell>13.16</cell></row><row><cell>23.20</cell><cell>129.17</cell><cell>45.38</cell></row><row><cell>130.30</cell><cell>243.00</cell><cell>78.92</cell></row><row><cell>116.00</cell><cell>1272.00</cell><cell>113.18</cell></row><row><cell>72.00</cell><cell>129.17</cell><cell>15.72</cell></row><row><cell>258.70</cell><cell>243.00</cell><cell>80.87</cell></row><row><cell>230.70</cell><cell>243.00</cell><cell>28.65</cell></row><row><cell>157.00</cell><cell>243.00</cell><cell>44.29</cell></row><row><cell>246.90</cell><cell>243.00</cell><cell>39.17</cell></row><row><cell>69.90</cell><cell>129.17</cell><cell>214.71</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I1 A</head><label>I1</label><figDesc>COMPARISON OF LEARNING AND ALGOMTHMIC APPROACHES. THE REGRESSION EQUATIONS GIVE &amp;fact AS A FUNCTION OF M</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>e s t ( Z )</head><label></label><figDesc></figDesc><table><row><cell>BACKPROP</cell><cell>78.13 t 0.882</cell></row><row><cell cols="2">FUNC. PTS. 103 I I : t : :::: ~ 49.9 t0.0822. 1 0.58 -37+0.96S 27.7 + 0.1561</cell></row><row><cell cols="2">case of all models, Kemerer argues that high R2 suggests that</cell></row><row><cell cols="2">by "calibrating" a model's predictions in a new environment,</cell></row><row><cell cols="2">the adjusted model predictions can be reliably used. Along</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table I1</head><label>I1</label><figDesc></figDesc><table /><note>summarizes the M R E and R2 values resulting from a linear regression of Mest and Matt values for the two learning algorithms, and results obtained by Kemerer with COCOMO-BASIC, FUNCTION POINTS, and SLIM.' These results indicate that CARTX'S and BACKPROPAGATION ' s predictions show a strong linear relationship with the actual development effort values for the 15 test project^.^ On this dimension, the performance of the learning systems is less than SLIM'S performance in Kemerer's experiments, but better than the other two models. In terms of mean M R E , BACKPROPACATION does strikingly well compared to the other approaches, and CARTX'S M R E is approximately one-half that of SLIM and COCOMO.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE I11 BACKPROPAGATION RESULTS WITH VARYING NUMBERS OF HIDDEN NODES</head><label>I11</label><figDesc></figDesc><table><row><cell>5</cell><cell>10</cell><cell>15</cell></row><row><cell>Mean R2</cell><cell></cell><cell></cell></row><row><cell>Max R2</cell><cell></cell><cell></cell></row><row><cell>sfin R2</cell><cell></cell><cell></cell></row><row><cell>Mean MRE(%)</cell><cell></cell><cell></cell></row></table><note>Max MRE(%)MinMRE(W)    </note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V SENSITIVITY OVER 20 RANDOMIZED TRIALS ON COMBINED COCOMO AND KEMERER'S DATA CARTX TABLE VI SENSITIVITY OVER 20 RANDOMIZED TRIALS ON KEMERERS DATA</head><label>V</label><figDesc></figDesc><table><row><cell>I 1</cell><cell>I Min RZ I Mean R* 1 Max R2 /I I 0.00 1 0.26 1 0.90 / I BACKPROPAGATION 0.03 CARTX 0.39 0.90</cell></row><row><cell cols="2">tree configuration. The holdout method divides the available</cell></row><row><cell cols="2">data into two sets; one set, generally the larger, is used to build</cell></row><row><cell cols="2">decisionlregression trees or train networks under different</cell></row><row><cell cols="2">configurations. The second subset is then classified using each</cell></row><row><cell cols="2">alternative configuration, and the configuration yielding the</cell></row><row><cell cols="2">best results over this second subset is selected as the final</cell></row><row><cell cols="2">configuration. Better yet, a choice of configuration may rest on</cell></row><row><cell cols="2">a form of resampling that exploits many randomized holdout</cell></row><row><cell cols="2">trials. Holdout could have been used in this case by dividing</cell></row></table><note>the Coco~o data, but the COCOMO dataset is very small as is.Thus, we have satisfied ourselves with a demonstration of the sensitivity of each learning algorithm to certain configuration decisions. A more complete treatment of resampling and other strategies for making configuration choices can be found in Weiss and Kulikowski[24].</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0">0 ~r choice of using COCOMO data for training and Kemerer's data for test was made independently of 151.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1">) Required SofhYare Reliability (RELY): This</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank the three reviewers and the action editor for their many useful comments.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recommended by D. Wile. D. Fisher's work was supported by NASA Ames Grant NAG 2-834. received the M B A, in management information systems from the Owen Graduate School of Management, Vanderbilt University, and the M.S in computer science from Vanderbilt University He also received the Post Graduate Diploma in industnal engineenng from the National Institute for Trluning in Industnal Engineering, Bombay, India, and the B E from the</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Instance-based learning algorithms</title>
		<author>
			<persName><forename type="first">D</forename><surname>Aha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kibler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Albert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
				<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="37" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Software function, source lines of code, and development effort prediction: A software science validation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Albrecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gaffney</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Olshen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Briand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Basili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Thomas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Software Engineering Economics</title>
		<editor>B. W. Boehm</editor>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="931" to="942" />
			<date type="published" when="1981-11">1983. 1981. Nov. 1992</date>
			<publisher>Prentice-Hall</publisher>
		</imprint>
	</monogr>
	<note>IEEE Trans. Sojiware Eng.</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Measuring concept change</title>
		<author>
			<persName><forename type="first">C</forename><surname>Brodley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rissland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">M I Spring Symp. Training Issues in Incremental Learning</title>
				<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="98" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Nonlinear signal prediction using neural networks: Prediction and system modeling</title>
		<author>
			<persName><forename type="first">K</forename><surname>Dejong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Fayyad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Kemerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Farber</surname></persName>
		</author>
		<idno>LA-UR-87-2662</idno>
	</analytic>
	<monogr>
		<title level="m">Numerical Analysis</title>
				<editor>
			<persName><forename type="first">L</forename><surname>Johnson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Riess</surname></persName>
		</editor>
		<meeting><address><addrLine>Reading, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Addison-Wesley</publisher>
			<date type="published" when="1982-05">1988. Feb. 1994. 1991. 1982. May 1987. 1987</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="416" to="429" />
		</imprint>
		<respStmt>
			<orgName>EECS Dep., Univ. of Michigan ; Los Alamos National Laboratory</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
	<note>On the induction of decision trees for multiple concept learning</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A general empirical solution to the macro software sizing and estimating problem</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mohanty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Selby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Selby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Putnam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">C4.5: Programs for Machine Learning</title>
				<editor>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Quinlan</surname></persName>
		</editor>
		<meeting><address><addrLine>San Mateo, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1978">1981. Mar. 1990. July 1990. 1978. 1993</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="345" to="361" />
		</imprint>
	</monogr>
	<note>J. Syst. Software</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Combining instance-based and model-based learning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Quinlan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the loth Int. Machine Learning Con$</title>
				<meeting>the loth Int. Machine Learning Con$</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="236" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning internal representations by error propagation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Parallel Distributed Processing</title>
				<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning from examples: Generation and evaluation of decision trees for software resource analysis</title>
		<author>
			<persName><forename type="first">W</forename><surname>Scacchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Eng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Sejnowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Selby</surname></persName>
		</author>
		<author>
			<persName><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Software Eng. and Knowledge</title>
		<imprint>
			<biblScope unit="volume">I</biblScope>
			<biblScope unit="page" from="1743" to="1757" />
			<date type="published" when="1987">1987. 1988</date>
		</imprint>
	</monogr>
	<note>IEEE Trans. Software Eng.</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Case-based reasoning in software effort estimation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Vicinanza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Prietulla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mukhopadhyay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 11th Int. Con$ Info. SJsr</title>
				<meeting>11th Int. Con$ Info. SJsr</meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="149" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Computer Systems that Learn</title>
		<author>
			<persName><forename type="first">S</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kulikowski</surname></persName>
		</author>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<biblScope unit="volume">199</biblScope>
			<biblScope unit="page">1</biblScope>
			<pubPlace>San Mateo, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Introduction to Artrjclcial Neural Networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zaruda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984">1992. 1984. 1991</date>
			<publisher>Wadsworth International</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="293" to="320" />
			<pubPlace>St. Paul, MN; Belmont, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
