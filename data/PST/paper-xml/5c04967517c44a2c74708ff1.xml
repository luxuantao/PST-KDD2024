<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">WAVEGLOW: A FLOW-BASED GENERATIVE NETWORK FOR SPEECH SYNTHESIS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-10-31">31 Oct 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ryan</forename><surname>Prenger</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">NVIDIA Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rafael</forename><surname>Valle</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">NVIDIA Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">NVIDIA Corporation</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">WAVEGLOW: A FLOW-BASED GENERATIVE NETWORK FOR SPEECH SYNTHESIS</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-10-31">31 Oct 2018</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1811.00002v1[cs.SD]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Audio Synthesis</term>
					<term>Text-to-speech</term>
					<term>Generative models</term>
					<term>Deep Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we propose WaveGlow: a flow-based network capable of generating high quality speech from melspectrograms. WaveGlow combines insights from Glow <ref type="bibr" target="#b1">[1]</ref> and WaveNet [2]  in order to provide fast, efficient and highquality audio synthesis, without the need for auto-regression. WaveGlow is implemented using only a single network, trained using only a single cost function: maximizing the likelihood of the training data, which makes the training procedure simple and stable. Our PyTorch implementation produces audio samples at a rate of more than 500 kHz on an NVIDIA V100 GPU. Mean Opinion Scores show that it delivers audio quality as good as the best publicly available WaveNet implementation. All code will be made publicly available online <ref type="bibr" target="#b3">[3]</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>As voice interactions with machines become increasingly useful, efficiently synthesizing high quality speech becomes increasingly important. Small changes in voice quality or latency have large impacts on customer experience and customer preferences. However, high quality, real-time speech synthesis remains a challenging task. Speech synthesis requires generating very high dimensional samples with strong long term dependencies. Additionally, humans are sensitive to statistical imperfections in audio samples. Beyond the quality challenges, real-time speech synthesis has challenging speed and computation constraints. Perceived speech quality drops significantly when the audio sampling rate is less than 16kHz, and higher sampling rates generate even higher quality speech. Furthermore, many applications require synthesis rates much faster than 16kHz. For example, when synthesizing speech on remote servers, strict interactivity requirements mean the utterances must be synthesized quickly at sample rates far exceeding real-time requirements.</p><p>Currently, state of the art speech synthesis models are based on parametric neural networks. Text-to-speech synthesis is typically done in two steps. The first step transforms the text into time-aligned features, such as a mel-spectrogram <ref type="bibr" target="#b4">[4,</ref><ref type="bibr">5]</ref>, or F0 frequencies and other linguistic features <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b6">6]</ref>. A second model transforms these time-aligned features into audio samples. This second model, sometimes referred to as a vocoder, is computationally challenging and affects quality as well. We focus on this second model in this work. Most of the neural network based models for speech synthesis are autoregressive, meaning that they condition future audio samples on previous samples in order to model long term dependencies. These approaches are relatively simple to implement and train. However, they are inherently serial, and hence can't fully utilize parallel processors like GPUs or TPUs. Models in this group often have difficulty synthesizing audio faster than 16kHz without sacrificing quality.</p><p>At this time we know of three neural network based models that can synthesize speech without auto-regression: Parallel WaveNet <ref type="bibr" target="#b2">[2]</ref>, Clarinet <ref type="bibr" target="#b7">[7]</ref>, and MCNN for spectrogram inversion <ref type="bibr" target="#b8">[8]</ref>. These techniques can synthesize audio at more than 500kHz on a GPU. However, these models are more difficult to train and implement than the auto-regressive models. All three require compound loss functions to improve audio quality or problems with mode collapse <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b8">8]</ref>. In addition, Parallel WaveNet and Clarinet require two networks, a student network and teacher network. The student networks underlying both Parallel WaveNet and Clarinet use Inverse Auto-regressive Flows (IAF) <ref type="bibr" target="#b10">[10]</ref>. Though the IAF networks can be run in parallel at inference time, the auto-regressive nature of the flow itself makes calculation of the IAF inefficient. To overcome this, these works use a teacher network to train a student network on a approximation to the true likelihood. These approaches are hard to reproduce and deploy because of the difficulty of training these models successfully to convergence.</p><p>In this work, we show that an auto-regressive flow is unnecessary for synthesizing speech. Our contribution is a flowbased network capable of generating high quality speech from mel-spectrograms. We refer to this network as WaveGlow, as it combines ideas from Glow <ref type="bibr" target="#b1">[1]</ref> and WaveNet <ref type="bibr" target="#b2">[2]</ref>. Wave-Glow is simple to implement and train, using only a single network, trained using only the likelihood loss function. Despite the simplicity of the model, our PyTorch implementation synthesizes speech at more than 500kHz on an NVIDIA V100 GPU: more than 25 times faster than real time. Mean Opinion Scores show that it delivers audio quality as good as the best publicly available WaveNet implementation trained on the same dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">WAVEGLOW</head><p>WaveGlow is a generative model that generates audio by sampling from a distribution. To use a neural network as a generative model, we take samples from a simple distribution, in our case, a zero mean spherical Gaussian with the same number of dimensions as our desired output, and put those samples through a series of layers that transforms the simple distribution to one which has the desired distribution. In this case, we model the distribution of audio samples conditioned on a mel-spectrogram.</p><formula xml:id="formula_0">z ? N (z; 0, I) (1) x = f 0 ? f 1 ? . . . f k (z)<label>(2)</label></formula><p>We would like to train this model by directly minimizing the negative log-likelihood of the data. If we use an arbitrary neural network this is intractable. Flow-based networks <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b1">1]</ref> solve this problem by ensuring the neural network mapping is invertible. By restricting each layer to be bijective, the likelihood can be calculated directly using a change of variables:</p><formula xml:id="formula_1">log p ? (x) = log p ? (z) + k i=1 log | det(J (f -1 i (x)))| (3) z = f -1 k ? f -1 k-1 ? . . . f -1 0 (x)<label>(4)</label></formula><p>In our case, the first term is the log-likelihood of the spherical Gaussian. This term penalizes the l 2 norm of the transformed sample. The second term arises from the change of variables, and the J is the Jacobian. The log-determinant of the Jacobian rewards any layer for increasing the volume of the space during the forward pass. This term also keeps a layer from just multiplying the x terms by zero to optimize the l 2 norm. The sequence of transformations is also referred to as a normalizing flow <ref type="bibr" target="#b13">[13]</ref>.</p><p>Our model is most similar to the recent Glow work <ref type="bibr" target="#b1">[1]</ref>, and is depicted in figure <ref type="figure">1</ref>. For the forward pass through the network, we take groups of 8 audio samples as vectors, which we call the "squeeze" operation, as in <ref type="bibr" target="#b1">[1]</ref>. We then process these vectors through several "steps of flow". A step of flow here consists of an invertible 1 ? 1 convolution followed by an affine coupling layer, described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Affine Coupling Layer</head><p>Invertible neural networks are typically constructed using coupling layers <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b1">1]</ref>. In our case, we use an affine coupling layer <ref type="bibr" target="#b12">[12]</ref>. Half of the channels serve as inputs, which then produce multiplicative and additive terms that are used to scale and translate the remaining channels: </p><formula xml:id="formula_2">x a , x b = split(x) (5) (log s, t) = W N (x a , mel -spectrogram) (6) x b = s x b + t (7) f -1 coupling (x) = concat(x a , x b )<label>(8)</label></formula><p>Here W N () can be any transformation. The coupling layer preserves invertibility for the overall network, even though W N () does not need to be invertible. This follows because the channels used as the inputs to W N (), in this case x a , are passed through unchanged to the output of the layer. Accordingly, when inverting the network, we can compute s and t from the output x a , and then invert x b to compute x b , by simply recomputing W N (x a , mel -spectrogram). In our case, W N () uses layers of dilated convolutions with gatedtanh nonlinearities, as well as residual connections and skip connections. This W N architecture is similar to WaveNet <ref type="bibr" target="#b2">[2]</ref> and Parallel WaveNet <ref type="bibr" target="#b9">[9]</ref>, but our convolutions have 3 taps and are not causal. The affine coupling layer is also where we include the mel-spectrogram in order to condition the generated result on the input. The upsampled mel-spectrograms are added before the gated-tanh nonlinearites of each layer as in WaveNet <ref type="bibr" target="#b2">[2]</ref>.</p><p>With an affine coupling layer, only the s term changes the volume of the mapping and adds a change of variables term to the loss. This term also serves to penalize the model for non-invertible affine mappings.</p><formula xml:id="formula_3">log | det(J (f -1 coupling (x)))| = log |s| (9)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">1x1 Invertible Convolution</head><p>In the affine coupling layer, channels in the same half never directly modify one another. Without mixing information across channels, this would be a severe restriction. Following Glow <ref type="bibr" target="#b1">[1]</ref>, we mix information across channels by adding an invertible 1x1 convolution layer before each affine coupling layer. The W weights of these convolutions are initialized to be orthonormal and hence invertible. The log-determinant of the Jacobian of this transformation joins the loss function due to the change of variables, and also serves to keep these convolutions invertible as the network is trained.</p><formula xml:id="formula_4">f -1 conv = W x (10) log | det(J (f -1 conv (x)))| = log | det W |<label>(11)</label></formula><p>After adding all the terms from the coupling layers, the final likelihood becomes:</p><formula xml:id="formula_5">log p ? (x) = - z(x) T z(x) 2? 2 + #coupling j=0 log s j (x, mel -spectrogram) + #conv k=0 log det |W k |<label>(12)</label></formula><p>Where the first term comes from the log-likelihood of a spherical Gaussian. The ? 2 term is the assumed variance of the Gaussian distribution, and the remaining terms account for the change of variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Early outputs</head><p>Rather than having all channels go through all the layers, we found it useful to output 2 of the channels to the loss function after every 4 coupling layers. After going through all the layers of the network, the final vectors are concatenated with all of the previously output channels to make the final z. Outputting some dimensions early makes it easier for the network to add information at multiple time scales, and helps gradients propagate to earlier layers, much like skip connections. This approach is similar to the multi-scale architecture used in <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b12">12]</ref>, though we do not add additional squeeze operations, so vectors get shorter throughout the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Inference</head><p>Once the network is trained, doing inference is simply a matter of randomly sampling z values from a Gaussian and running them through the network. As suggested in <ref type="bibr" target="#b1">[1]</ref>, and earlier work on likelihood-based generative models <ref type="bibr" target="#b14">[14]</ref>, we found that sampling zs from a Gaussian with a lower standard deviation from that assumed during training resulted in slightxly quality higher audio. During training we used ? = ? 0.5, and during inference we sampled zs from a Gaussian with standard deviation 0.6. Inverting the 1x1 convolutions is just a matter of inverting the weight matrices. The inverse is guaranteed by the loss. The mel-spectrograms are included at each of the coupling layers as before, but now the affine transforms are inverted, and these inverses are also guaranteed by the loss.</p><p>x a = x a -t s (13)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTS</head><p>For all the experiments we trained on the LJ speech data <ref type="bibr" target="#b15">[15]</ref>. This data set consists of 13,100 short audio clips of a single speaker reading passages from 7 non-fiction books. The data consists of roughly 24 hours of speech data recorded on a MacBook Pro using its built-in microphone in a home environment. We use a sampling rate of 22,050kHz.</p><p>We use the mel-spectrogram of the original audio as the input to the WaveNet and WaveGlow networks. For WaveGlow, we use mel-spectrograms with 80 bins using librosa mel filter defaults, i.e. each bin is normalized by the filter length and the scale is the same as HTK. The parameters of the melspectrograms are FFT size 1024, hop size 256, and window size 1024.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Griffin-Lim</head><p>As baseline for mean opinion score we compare the popular Griffin-Lim algorithm <ref type="bibr" target="#b16">[16]</ref>. Griffin-Lim takes the entire spectrogram (rather than the reduced mel-spectrogram) and iteratively estimates the missing phase information by repeatedly converting between frequency and time domain. For our experiments we use 60 iterations from frequency to time domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">WaveNet</head><p>We compare against the popular open source WaveNet implementation <ref type="bibr" target="#b17">[17]</ref>. The network has 24 layers, 4 dilation doubling cycles, and uses 512/512/256, for number of residual, gating, and skip channels respectively. The network upsamples the mel-spectrogram to full time resolution using 4 separate upsampling layers. The network was trained for 1 ? 10 6 iterations using the Adam optimizer <ref type="bibr" target="#b18">[18]</ref>. The melspectrogram for this network is still 80 dimensions but was processed slightly differently from the mel-spectrogram we used in the WaveGlow network. Qualitatively, we did not find these differences had an audible effect when changed in the WaveGlow network. The full list of hyperparameters is available online.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">WaveGlow</head><p>The WaveGlow network we use has 12 coupling layers and 12 invertible 1x1 convolutions. The coupling layer networks (W N ) each have 8 layers of dilated convolutions as described in Section 2, with 512 channels used as residual connections and 256 channels in the skip connections. We also output 2 of the channels after every 4 coupling layers. The WaveGlow network was trained on 8 Nvidia GV100 GPU's using randomly chosen clips of 16,000 samples for 580,000 iterations using weight normalization <ref type="bibr" target="#b19">[19]</ref> and the Adam optimizer <ref type="bibr" target="#b18">[18]</ref>, with a batch size of 24 and a step size of 1 ? 10 -4</p><p>When training appeared to plateau, the learning rate was further reduced to 5 ? 10 -5 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Audio quality comparison</head><p>We crowd-sourced Mean Opinion Score (MOS) tests on Amazon Mechanical Turk. Raters first had to pass a hearing test to be eligible. Then they listened to an utterance, after which they rated pleasantness on a five-point scale. We used 40 volume normalized utterances disjoint from the training set for evaluation, and randomly chose the utterances for each subject. After completing the rating, each rater was excluded from further tests to avoid anchoring effects.</p><p>The MOS scores are shown in Table <ref type="table" target="#tab_0">1</ref> with 95% confidence intervals. Though MOS scores of synthesized samples are close on an absolute scale, none of the methods reach the MOS score of real audio. Though WaveGlow has the highest MOS, all the methods have similar scores with only weakly significant differences after collecting approximately 1,000 samples. This roughly matches our subjective qualitative assessment. Samples of the test utterances can be found online <ref type="bibr" target="#b3">[3]</ref>. The larger advantage of WaveGlow is in training simplicity and inference speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Mean Opinion Score (MOS) The inference implementation of the WaveNet we compare against synthesizes speech at 0.11kHz, significantly slower than the real time.</p><p>Our unoptimized PyTorch implementation of WaveGlow synthesizes a 10 second utterance at approximately 520kHz on an NVIDIA V100 GPU. This is slightly faster than the 500kHz reported by Parallel WaveNet <ref type="bibr" target="#b9">[9]</ref>, although they tested on an older GPU. For shorter utterances, the speed per sample goes down because we have the same number of serial steps, but less audio produced. Similar effects should be seen for Griffin-Lim and Parallel WaveNet. This speed could be increased with further optimization. Based on the arithmetic cost of computing WaveGlow, we estimate that the upper bound of a fully optimized implementation is approximately 2,000kHz on an Nvidia GV100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">DISCUSSION</head><p>Existing neural network based approaches to speech synthesis fall into two groups. The first group conditions future audio samples on previous samples in order to model long term dependencies. The first of these auto-regressive neural network models was WaveNet <ref type="bibr" target="#b2">[2]</ref> which produced high quality audio. However, WaveNet inference is challenging computationally. Since then, several auto-regressive models have attempted to speed up inference while retaining quality <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b21">21]</ref>. As of this writing, the fastest auto-regressive network is <ref type="bibr" target="#b22">[22]</ref>, which uses a variety of techniques to speed up an auto-regressive RNN. Using customized GPU kernels, <ref type="bibr" target="#b22">[22]</ref> was able to produce audio at 240kHz on an Nvidia P100 GPU, making it the fastest auto-regressive model.</p><p>In the second group, Parallel WaveNet <ref type="bibr" target="#b9">[9]</ref> and ClariNet <ref type="bibr" target="#b7">[7]</ref> are discussed in Section 1. MCNN for spectrogram inversion <ref type="bibr" target="#b8">[8]</ref> produces audio using one multi-headed convolutional network. This network is capable of producing samples at over 5,000kHz, but their training procedure is complicated due to four hand-engineered losses, and it operates on the full spectrogram rather than a reduced mel-spectrogram or other features. It is not clear how a non-generative approach like MCNN would generate realistic audio from a more underspecified representation like mel-spectrograms or linguistic features without some kind of additional sampling procedure to add information.</p><p>Flow-based models give us a tractable likelihood for a wide variety of generative modeling problems, by constraining the network to be invertible. We take the flow-based approach of <ref type="bibr" target="#b1">[1]</ref> and include the architectural insights of WaveNet. Parallel WaveNet and ClariNet use flow-based models as well. The inverse auto-regressive flows used in Parallel WaveNet <ref type="bibr" target="#b9">[9]</ref> and ClariNet <ref type="bibr" target="#b7">[7]</ref> are capable of capturing strong long-term dependencies in one individual pass. This is likely why Parallel WaveNet was structured with only 4 passes through the IAF, as opposed to the 12 steps of flow used by WaveGlow. However, the resulting complexity of two networks and corresponding mode-collapse issues may not be worth it for all users.</p><p>WaveGlow networks enable efficient speech synthesis with a simple model that is easy to train. We believe that this will help in the deployment of high quality audio synthesis.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>9 Fig. 1 :</head><label>91</label><figDesc>Fig. 1: WaveGlow network</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Mean Opinion Scores3.5. Speed of inference comparisonOur implementation of Griffin-Lim can synthesize speech at 507kHz for 60 iterations of the algorithm. Note that Griffin-Lim requires the full spectrogram rather than the reduced mel-spectrogram like the other vocoders in this comparison.</figDesc><table><row><cell>Griffin-Lim</cell><cell>3.823 ? 0.1349</cell></row><row><cell>WaveNet</cell><cell>3.885 ? 0.1238</cell></row><row><cell>WaveGlow</cell><cell>3.961 ? 0.1343</cell></row><row><cell>Ground Truth</cell><cell>4.274 ? 0.1340</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>The authors would like to thank <rs type="person">Ryuichi Yamamoto</rs>, <rs type="person">Brian Pharris</rs>, <rs type="person">Marek Kolodziej</rs>, <rs type="person">Andrew Gibiansky</rs>, <rs type="person">Sercan Arik</rs>, <rs type="person">Kainan Peng</rs>, <rs type="person">Prafulla Dhariwal</rs>, and <rs type="person">Durk Kingma</rs>.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Dhariwal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03039</idno>
		<title level="m">Glow: Generative flow with invertible 1x1 convolutions</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Wavenet: A generative model for raw audio</title>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03499</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Valle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">"</forename><surname>Waveglow</surname></persName>
		</author>
		<ptr target="https://nv-adlr.github.io/WaveGlow" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Yuxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Skerry-Ryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daisy</forename><surname>Stanton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10135</idno>
		<title level="m">Tacotron: A fully end-to-end text-to-speech synthesis model</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Natural tts synthesis by conditioning wavenet on mel spectrogram predictions</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Skerry-Ryan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.05884</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Mike</forename><surname>Sercan O Arik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Chrzanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongguo</forename><surname>Gibiansky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><surname>Raiman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.07825</idno>
		<title level="m">Deep voice: Real-time neural text-to-speech</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Clarinet: Parallel wave generation in end-to-end text-to-speech</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kainan</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitong</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.07281</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Fast spectrogram inversion using multi-head convolutional neural networks</title>
		<author>
			<persName><forename type="first">Heewoo</forename><surname>Sercan O Arik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><surname>Diamos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06719</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Parallel WaveNet: Fast high-fidelity speech synthesis</title>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Lockhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Cobo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Stimberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Norman</forename><surname>Casagrande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Grewe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seb</forename><surname>Noury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helen</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Walters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Belov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Demis</forename><surname>Hassabis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">Jennifer</forename><surname>Dy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Andreas</forename><surname>Krause</surname></persName>
		</editor>
		<meeting>the 35th International Conference on Machine Learning<address><addrLine>Stockholmsmssan, Stockholm Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018-07-15">10-15 Jul 2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="3918" to="3926" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improved variational inference with inverse autoregressive flow</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Diederik P Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafal</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4743" to="4751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Nice: Non-linear independent components estimation</title>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.8516</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Density estimation using real nvp</title>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.08803</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Jimenez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rezende</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.05770</idno>
		<title level="m">Variational inference with normalizing flows</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Image transformer</title>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05751</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The LJ speech dataset</title>
		<author>
			<persName><forename type="first">Keith</forename><surname>Ito</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Signal estimation from modified short-time fourier transform</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jae</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="236" to="243" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Wavenet vocoder</title>
		<author>
			<persName><forename type="first">Ryuichi</forename><surname>Yamamoto</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.1472609</idno>
		<ptr target="https://doi.org/10.5281/zenodo.1472609" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Weight normalization: A simple reparameterization to accelerate training of deep neural networks</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="901" to="909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Deep voice 2: Multi-speaker neural textto-speech</title>
		<author>
			<persName><forename type="first">Sercan</forename><surname>Arik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Gibiansky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kainan</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Raiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.08947</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">FFTNet: a real-time speaker-dependent neural vocoder</title>
		<author>
			<persName><forename type="first">Zeyu</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gautham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingwan</forename><surname>Mysore</surname></persName>
		</author>
		<author>
			<persName><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 43rd IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2018-04">Apr. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Efficient neural audio synthesis</title>
		<author>
			<persName><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seb</forename><surname>Noury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Norman</forename><surname>Casagrande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Lockhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Stimberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sander</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08435</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
