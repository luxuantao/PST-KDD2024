<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Modeling Intent Graph for Search Result Diversification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhan</forename><surname>Su</surname></persName>
							<email>suzhan@ruc.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhicheng</forename><surname>Dou</surname></persName>
							<email>*dou@ruc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yutao</forename><surname>Zhu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Université de Montréal</orgName>
								<address>
									<settlement>Montréal</settlement>
									<region>Québec</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xubo</forename><surname>Qin</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
							<affiliation key="aff3">
								<orgName type="laboratory">Beijing Key Laboratory of Big Data Management and Analysis Methods</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution" key="instit1">Key Laboratory of Data Engineering and Knowledge Engineering</orgName>
								<orgName type="institution" key="instit2">MOE</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Modeling Intent Graph for Search Result Diversification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3404835.3462872</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS</term>
					<term>Information systems → Information retrieval diversity Intent Graph</term>
					<term>Search Result Diversification</term>
					<term>Graph Neural Network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Search result diversification aims to offer diverse documents that cover as many intents as possible. Most existing implicit diversification approaches model diversity through the similarity of document representation, which is indirect and unnatural. To handle the diversity more precisely, we measure the similarity of documents by their similarity of the intent coverage. Specifically, we build a classifier to judge whether two different documents contain the same intent based on the document's content. Then we construct an intent graph to present the complicated relationship of documents and the query. On the intent graph, documents are connected if they are similar, while the query and the document are gradually connected based on the document selection result. Then we employ graph convolutional networks (GCNs) to update the representation of the query and each document by aggregating its neighbors. By this means, we can obtain the context-aware query representation and the intent-aware document representations through the dynamic intent graph during the document selection process. Furthermore, these representations and intent graph features are fused into diversity features. Combined with the traditional relevance features, we obtain the final ranking score that balances the relevance and the diversity. Experimental results show that this implicit diversification model significantly outperforms all existing implicit diversification methods, and it can even beat the state-of-the-art explicit models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Search result diversification can efficiently alleviate the influence brought by the ambiguous query. In addition to improving the hit rate of different users' information needs, diversification can also meet the user's intrinsic diversity need. For example, by issuing a query "cook noodles", a user may look for different recipes for cooking noodles. To fulfill such kinds of needs, diversification approaches are expected to display documents with various subtopics while considering the relevance of the documents.</p><p>Existing approaches to search result diversification can be roughly categorized into explicit methods and implicit methods. Since the diversity ranking task is NP-hard, most approaches adopt greedy selection strategies as a compromise <ref type="bibr" target="#b31">[32]</ref>, i.e., iteratively selecting the most diverse document from the candidate set at each step. Explicit approaches explicitly leverage subtopics distribution as the substitution of real intents to measure diversity of each document <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b30">31]</ref>, while implicit approaches focus on the document's novelty based on the similarity between documents and do not rely on subtopics <ref type="bibr" target="#b2">[3]</ref>. As subtopic mining itself is a very challenging task, the implicit result diversification methods have received much attention in real search services <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b47">48]</ref>.</p><p>Although many implicit methods <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b47">48]</ref> have been proposed in recent years, most existing methods measure the document's novelty based on the dissimilarity between the candidate documents and the selected documents. For example, the typical implicit approach NTN <ref type="bibr" target="#b38">[39]</ref> automatically learns a novelty function based on the preliminary representation (e.g., doc2vec or PLSA) of the candidate document and the selected documents. There are two main drawbacks of these methods: <ref type="bibr" target="#b0">(1)</ref> With the only diversity ranking loss, it is often difficult to tell if a wrong ranking stems from a wrong combination of features or the incompetent document novelty features, thus the document's novelty cannot always be learned to its best. Meanwhile, merely computing the document's novelty based on the preliminary representation is also inaccurate, since the document's content is an essential source for deriving the document's diversity information. <ref type="bibr" target="#b1">(2)</ref> The novelty of a candidate document is measured by its dissimilarity with the selected documents. The intent coverage of selected documents on the query and the similarity among candidate documents are neglected. Under this circumstance, it is difficult to select an optimal document from the candidates to satisfy the user's intent.</p><p>From our perspective: (1) Diversity ranking models try to offer documents covering as many intents as possible. Therefore, the real intents of the documents are essential parts when considering the similarity of documents. For example, two documents can be treated as more similar if they share more subtopics. Indeed, the real user intent is often hidden in the document content. So, our first challenge is how to take both the documents' content and their intent coverage into account for computing their similarity. <ref type="bibr" target="#b1">(2)</ref> The information needs of the query may be partly satisfied according to the subtopics contained in the selected documents. Hence, diversity ranking models should capture the dynamic diversity needs of the query timely. Besides, the novelty of all the candidate documents is not independent. When a candidate document is selected, the novelty of the remnant documents will be affected. Therefore, our second challenge is how to consider the complicated and dynamic relation of the query and documents during the document selection process.</p><p>To tackle the challenges above, we propose to model the document's similarity through the similarity of the document's intent coverage directly rather than the similarity of document representations. However, due to the subtopic mining is a very challenging task in explicit methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b31">32]</ref>, we dedicate to implicit methods in this paper. In fact, we can derive two document's intent similarity from the document's content without exactly knowing what the subtopics are. To fully leverage the abundant information of the document's content, we design a document relation classifier to judge whether two documents cover the same intent based on the content. Additionally, to further enhance the weak relation of documents extracted from the classifier and model the similarity of documents with a global view, we present the complicated relationship of documents and the query on the graph. Specifically, we build an intent graph where two documents are connected if they share the same intent. The selected document is also connected to the query so as to distinguish it from remnant candidate documents. Moreover, the intent graph could be dynamically adjusted according to the selected documents for a better representation of the query's information needs. With the help of the graph structure, we can derive the local diversity features when we focus on the document nodes and their neighbors, while the global features are also easy to obtain by aggregating features of the entire intent graph. Motivated by the powerful aggregating capability of the graph convolutional networks <ref type="bibr" target="#b17">[18]</ref> (GCN), we adapt GCN to this dynamic intent graph for learning the intent-aware document representations and the context-aware query representation. Combining the features extracted from the intent graph, we can model the document's diversity in a direct and precise way. In general, we leverage the Graph to represent the intent coverage of the documents for search result diversification. Therefore, our method is named Graph4DIV. The experimental results show that our approach outperforms the state-of-the-art implicit method by 12.2% in terms of 𝛼-nDCG@20.</p><p>The main contributions of this paper are summarized as follows:</p><p>(1) We propose to model documents' similarity directly through their intent coverage. This brand new idea offers a better way for capturing the essence of the document's novelty and result diversity without the explicit use of subtopics.</p><p>(2) We use a dynamic intent graph to model the complicated query-document and document-document relationships simultaneously and timely. We leverage GCN to learn better representations of the query and documents from the intent graph. As far as we know, this is the first method of adapting GCN to search result diversification.</p><p>(3) Our implicit diversification approach largely improves the state-of-the-art performance of implicit methods and outperforms all explicit diversity approaches, which makes diversification approaches more suitable to be applied in real scenarios.</p><p>The rest of the paper is organized as follows. We review some related work in Section 2. Then we introduce the Graph4DIV framework in Section 3. Section 4 presents the experimental settings and results. In Section 5, we analyse different settings and influences of the experiment. Finally, we conclude the paper in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Search Result Diversification</head><p>Search result diversification can be categorized into explicit approaches and implicit approaches depending on whether they use subtopics or not. From another perspective, diversification methods include heuristic (unsupervised) methods and supervised methods as shown in Table <ref type="table" target="#tab_0">1</ref>. In this section, we will briefly introduce the major diversification approaches in terms of the features they use.</p><p>Implicit Diversification Approaches Most implicit methods obey the framework of MMR <ref type="bibr" target="#b2">[3]</ref>, which balances the relevance and novelty of the document with a parameter 𝜆. The novelty is mainly measured by dissimilarity between retrieved documents. It provides a balanced strategy for ranking the documents returned by search engines, which becomes the foundation of many implicit and explicit approaches <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b44">45]</ref>. Yue and Joachims <ref type="bibr" target="#b42">[43]</ref> proposed SVM-DIV that uses structural SVM to measure the diversity of the documents. R-LTR <ref type="bibr" target="#b47">[48]</ref> was a relational learning-to-rank approach based on the relation of documents. To solve the problem that loss functions loosely related to the evaluation measures, Xia et al. <ref type="bibr" target="#b37">[38]</ref> proposed the PAMM approach to directly optimize diversity evaluation measures. Then neural tensor network (NTN) <ref type="bibr" target="#b38">[39]</ref> was introduced to automatically learn the relation functions of the documents. As an implicit approach, our model also follows the framework of MMR. Different from the previous implicit methods, we obtain the diversity features automatically learned from the graph structure that contains the intent information.</p><p>Explicit Diversification Approaches Instead of the similarity between documents, most explicit models leverage subtopic coverage to measure documents' diversity. The representative traditional explicit approaches are xQuAD <ref type="bibr" target="#b31">[32]</ref> and PM2 <ref type="bibr" target="#b8">[9]</ref>. Many further studies are carried out based on them, such as HxQuAD, HPM2 <ref type="bibr" target="#b13">[14]</ref>, TxQuAD, and TPM2 <ref type="bibr" target="#b9">[10]</ref>. To avoid handcrafted functions and parameters, several supervised approaches have been proposed recently. For example, DSSA <ref type="bibr" target="#b15">[16]</ref> proposed a list-pairwise loss for training the diversity ranking model. Besides, DSSA also introduced recurrent neural networks (RNNs) and the attention mechanism to model the subtopic coverage of the document sequence.</p><p>Ensemble Diversification Approaches Recently, researchers also consider using explicit (subtopic) features and implicit features together in the ranking process, which could be categorized into explicit methods. For instance, DVGAN <ref type="bibr" target="#b20">[21]</ref> combined ranking signals learned by the generator and the discriminator in order to obtain a better ranking model. DESA <ref type="bibr" target="#b28">[29]</ref> leveraged both document novelty and subtopic coverage based on self-attention. Compared to these models, our method also leverages the strength of supervised learning but without depending on extra subtopics, and thus it is an implicit method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Graph in IR</head><p>The graph structure is a very common and natural way to present the relationship of documents, queries, and intents in IR literature <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b45">46]</ref>. For example, PageRank <ref type="bibr" target="#b27">[28]</ref> turns out to be a powerful and typical algorithm to measure the importance of the web pages based on the graph structure. Jiang et al. <ref type="bibr" target="#b14">[15]</ref> learned the relevance of query and documents through the Web-scale Click Graph that presents user behavior, which demonstrates that the abundant information contained in the graph helps to improve the search result and the intent is also suitable to present on the graph.</p><p>Graph neural networks (GNNs) can efficiently leverage the structural information extracted from the graph. Owing to its aggregation and representation capability, it quickly becomes a powerful tool in many fields, such as computer vision <ref type="bibr" target="#b10">[11]</ref>, social network analysis <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b34">35]</ref> and natural language processing <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b48">49]</ref>.</p><p>Recently, graph-based learning methods make breakthroughs in the IR literature. Researchers leverage graph structure to enhance the representation of documents and queries. For example, Li et al. <ref type="bibr" target="#b19">[20]</ref> learned text representation with graph structure that contains click behavior information. Zhang et al. <ref type="bibr" target="#b46">[47]</ref> used graph embedding techniques to learn the representation of query and items in the product search.</p><p>Graph convolutional networks (GCNs) <ref type="bibr" target="#b17">[18]</ref> can collect the neighbors' information by generalizing traditional convolutional operation from nodes with a fixed degree to ones with a scalable degree. The representations of the nodes on the graph will be enhanced by their neighbors after the GCN. Since implicit search result diversification approaches model document's novelty based on the dissimilarity of documents, we believe GCN could be a suitable tool to refine the document representation with its similar documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROPOSED METHOD: GRAPH4DIV</head><p>Diversity ranking aims to offer diverse documents that cover as many intents as possible, while most existing implicit methods measure diversity by the dissimilarity of the document representations indirectly and roughly.</p><p>In this paper, we hope to directly model the diversity according to the intent contained in the documents. However, it is still a challenging task to mine the precise intent or subtopics from the documents or other data sources. Instead of using explicit subtopics, we propose to implicitly leverage the hidden query intents covered by the top results of the query. We build a classifier to judge whether two documents share the same intent and present the relation on </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation</head><p>The notations in this paper and their descriptions are shown in Table <ref type="table" target="#tab_1">2</ref>. Supposing 𝑞 is the current query and D is a list of 𝑛 candidate documents for 𝑞, the task of search result diversification is to generate a new ranked document list R based on the initial ad-hoc ranking list D, where diverse documents are ranked higher in R and redundant ones are ranked lower. Different from the ad-hoc retrieval task, which aims at returning relevant documents, search result diversification needs to consider both <ref type="bibr" target="#b0">(1)</ref> the relevance between the query and the document; and</p><p>(2) the similarity among the documents. As introduced in Section 1, most existing diversification methods apply the greedy selection strategy <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b47">48]</ref>, i.e., iteratively selecting the next document by measuring its relevance with the current query and its novelty compared with the documents that have already been selected in the early iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Overview of Graph4DIV</head><p>The overall structure of our proposed Graph4DIV is shown in Figure <ref type="figure">1</ref>. Formally, at the time step 𝑡, supposing S is the set of documents already been selected, Graph4DIV determines the next document 𝑑 * by measuring the ranking score 𝑓 (𝑑 𝑖 ) of each remained candidate document 𝑑 𝑖 and selecting the document with the highest ranking score. 𝑓 (𝑑 𝑖 ) is comprised of relevance and novelty of the document given the current query 𝑞, document set D, and selected document sequence S:</p><formula xml:id="formula_0">𝑓 (𝑑 𝑖 , D, S) = 𝜆𝑓 rel (𝑑 𝑖 ) + (1 − 𝜆) 𝑓 div (𝑑 𝑖 , D, S),<label>(1)</label></formula><p>where 𝑓 (𝑑 𝑖 , D, S) denotes document 𝑑 𝑖 's ranking score that consists of relevance score 𝑓 rel (𝑑 𝑖 ) and diversity score 𝑓 div (𝑑 𝑖 , D, S) <ref type="foot" target="#foot_0">1</ref> .</p><p>𝜆 is the parameter to control the balance between relevance and diversity. This is the common format of most search result diversification models. As for the relevance part, Graph4DIV uses the same relevance features R 𝑖 as those used in previous work <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b47">48]</ref>.</p><formula xml:id="formula_1">⨁ 𝑋 ! 𝑋 " 𝑋 # 𝑋 $ 𝑋 % 𝑋 &amp; Graph Adjust Conv 1 Conv 2 𝑍 ! 𝑍 " 𝑍 # 𝑍 $ 𝑍 % 𝑍 &amp; MLP ••• 𝐑 ! Relevance Features 𝑍 ! 𝑍 $ 𝐷 $ 𝑇 ' Graph Sum Aggregate Adjusted Graph Intent Graph Diversity Features Graph Convolutional Layers MLP = 𝑋 ! 𝑋 " 𝑋 # 𝑋 $ 𝑋 % 𝑋 &amp; 𝑑 # Selected document 𝑓 &amp;'( (𝑑 ) ) 𝑓 *+, (𝑑 ) ) 𝐇 𝒊 𝑓(𝑑 ) )</formula><p>Figure The relevance score 𝑓 rel (𝑑 𝑖 ) is calculated from the relevance feature R 𝑖 with an MLP layer:</p><formula xml:id="formula_2">𝑓 rel (𝑑 𝑖 ) = MLP(R 𝑖 ).<label>(2)</label></formula><p>The details will be introduced in Section 3.4.2.</p><p>The computation of the diversity score 𝑓 div (𝑑 𝑖 , D, S) is the focus of this paper. We propose building an intent graph G and extract diversity features H based on the graph. The diversity score is then computed as:</p><formula xml:id="formula_3">𝑓 div (𝑑 𝑖 , D, S) = MLP (H (𝑑 𝑖 , D, S)) ,<label>(3)</label></formula><formula xml:id="formula_4">H(𝑑 𝑖 , D, S) = F (𝑑 𝑖 , D, S, G 𝐷,𝑆 ),<label>(4)</label></formula><p>where G 𝐷,𝑆 is the corresponding intent graph for query 𝑞 that is updated after S is selected from D. Note that 𝑞 also belongs to the nodes of this graph but the notation is omitted here for simplification and space saving. The diversity features H 𝑖 of the document 𝑑 𝑖 is dynamically changing at each step 𝑡 in the document selection process, and we also omit the notation 𝑡 for convenience. The function F describes how our model produces the representation of document 𝑑 𝑖 and related diversity features when given the intent graph G 𝐷,𝑆 , the selected documents set S, and document set D.</p><p>The key components of our Graph4DIV for computing H 𝑖 are briefly introduced as follows:</p><p>(1) Graph Building and Adjustment (Section 3.3). We build an intent graph for each query 𝑞 based on the result of the documents relation classifier (introduced in Section 3.3.3). In the intent graph, the query and its all candidate documents are represented as nodes. The query node is only connected to the selected documents in order to obtain a context-aware query representation. For the remaining candidate documents, there will be an edge between two candidate document nodes only when they share the same intent of the query. The graph is dynamically adjusted according to the selection of documents at each step. For example, as shown in Figure <ref type="figure" target="#fig_0">2</ref>, at the time step 𝑡 = 2, given the previous selected document 𝑑 2 , we adjust the graph by dropping the edges between the selected document node 𝑣 2 and the remaining candidate document nodes Algorithm 1 Diversity Ranking algorithm of Graph4DIV  (2) Graph-based Diversity Features (Section 3.4). We then compute the diversity features based on the current intent graph. Specifically, considering the initial node representations X = [X 𝑞 , X 1 , • • • , X 𝑛 ] of all the nodes on the graph, they are updated after a two-layer graph convolutional network. As a result, we can get the new representations Z = [Z 𝑞 , Z 1 , • • • , Z 𝑛 ] for them. To compute the diversity features H 𝑖 , we consider the query's representation Z 𝑞 , document 𝑑 𝑖 's representation Z 𝑖 , the degree 𝐷 𝑖 of the node 𝑣 𝑖 , and the representation T 𝑔 of the entire intent graph. The diversity features of 𝑑 𝑖 are calculated as the assemble of these features</p><formula xml:id="formula_5">H 𝑖 = [Z 𝑞 ; Z 𝑖 ; 𝐷 𝑖 ; T 𝑔 ] (illustrated in Section 3.4.2).</formula><p>The overall process of our proposed GraphDIV for search result diversification is summarized as Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Intent Graph</head><p>Measuring the similarity of two documents is the foundation of the implicit diversity approaches. In the search result diversification task, we treat the similarity of documents as the similarity of subtopic coverage. To model the relationship of multiple document pairs simultaneously and extract more comprehensive diversity features containing both local and global information, we present all the documents 𝑑 𝑖 ∈ D and the query 𝑞 on the graph, which is called the intent graph.</p><p>Algorithm 2 Graph Adjustment algorithm used by Graph4DIV The intent graph is an essential part of our approach to model the document-document and query-document relationship for diversification. We build one intent graph G = (𝑁 , 𝐸) for each query 𝑞, 𝑞 ∈ Q, where 𝑁 denotes the nodes, and 𝐸 denotes the edges. G is an undirected graph and its nodes 𝑁 are comprised of the current query 𝑞 and all documents contained in D. The edges will be dynamically adjusted after a new document is selected and added to S.</p><p>The building and adjustment procedure of the intent graph are shown in Figure <ref type="figure" target="#fig_0">2</ref>. We build a document relation classifier to judge the subtopic covering relationship of documents. Such a relation is represented as edges between document nodes. Based on the result of the classifier, the graph builder will build the initial intent graph with query node and document nodes. Then the graph adjustment algorithm will refine the intent graph according to the document selection result at each step. Next, we will introduce the critical parts of our workflow in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Graph Builder.</head><p>First of all, we create an intent graph G 0 with the current query 𝑞 and all documents contained in D as the nodes, and an empty edge set, i.e., 𝑁 (G 0 ) = {𝑣 𝑞 , 𝑣 1 , • • • , 𝑣 𝑛 } and 𝐸 (G 0 ) = 𝜙. Then, we build a document-document relation classifier to predict the relationship between two documents. As the target of search result diversification is to improve result diversity, and the common way to measure diversity is based on intent <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b44">45]</ref>. Inspired by this, we train a classifier to explicitly judge whether two documents belong to the same intent and we consider this is a simple but effective way to predict the connection between documents. More details will be elaborated in Section 3.3.3. After getting the prediction result of all the pairs of candidate documents, the graph builder will connect the document nodes that are predicted to belong to the same intent and get the initial graph G D,S and currently we have S = 𝜙. In our approach, we treat edge weight between documents as a binary value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Graph</head><p>Adjustment after Document Selection. Given the current intent graph G 𝐷,S , we will employ the document scoring algorithm (introduced in Section 3.4) to assess each document in the remaining documents C = D \ S. Consistent with the diversification algorithm, we divide the document nodes of 𝑁 into two sets: the selected documents S and the remnant documents C.</p><p>Assuming that the best document 𝑑 * with the highest score is selected and appended to S, we use Algorithm 2 to adjust the intent graph. Considering that some parts of the user's information needs might be met when the document 𝑑 * is selected, we hope the model to focus more on the intents that have not been covered by the selected documents set S yet. Motivated by this, we propose leveraging S to update the context-aware query representation. We connect the query node with the nodes within S. With these edges, the representation of the current information need in the query can be updated based on the selected documents via a graph neural network (e.g., GCN). Furthermore, we mainly exploit the remnant documents to obtain the dependent representation of remaining documents, hence we drop all the edges between documents in S and those in C. More specifically, after 𝑑 * is selected, we add an edge to connect 𝑑 * and 𝑞 with the relevant score as edge weight in order to help update the context-aware query representation. The relevant score is the normalized form of the initial ranking score without considering diversity. We then remove all the edges connecting 𝑑 * and other documents in C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Documents Relation Classifier.</head><p>To convert the complicated relationship of the query and documents to the edges on the intent graph, we design a classifier to explicitly judge whether two documents cover the same subtopic according to the document's content. Instead of getting the document's relationship from the document's representation, we hope our model can integrate the relation of documents and the query into their representations. Such relation information of the documents comes from the prediction results of the documents relation classifier.</p><p>The main structure of the classifier is shown in Figure <ref type="figure" target="#fig_1">3</ref>. Given a query 𝑞 and its document set D, we sample all the document pairs from D and send them to the relation classifier. Supposing that a pair of documents (𝑑 𝑖 , 𝑑 𝑗 ) is given, the documents relation classifier is expected to judge whether 𝑑 𝑖 and 𝑑 𝑗 share the same subtopic. To mine the subtopic information from the documents, we leverage BERT <ref type="bibr" target="#b29">[30]</ref> to extract the representation of documents 𝑑 𝑖 and 𝑑 𝑗 . Here we employ BERT because it is pre-trained on the large corpus and has achieved great performance on several natural language processing tasks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25]</ref>. Other advanced text matching models can also be a potential choice for building such a classifier. For the convenience of processing, the two documents are tokenized into a fixed length, say 𝑀. Therefore, we can get the token sequences</p><formula xml:id="formula_6">x 𝑖 = [[CLS], 𝑤 1 , 𝑤 2 , • • • , 𝑤 𝑀 ] and x 𝑗 = [[CLS], 𝑡 1 , 𝑡 2 , • • • , 𝑡 𝑀 ]</formula><p>standing for documents 𝑑 𝑖 and 𝑑 𝑗 , respectively, where "[CLS]" is a special token. Thereafter, we obtain the representation x 𝑖 and x 𝑗 based on the representations of the "[CLS]" tokens computed by BERT. Considering that the difference of two document's representations may contain the useful information for the classifier, we use the feature x 𝑖 𝑗 = [x 𝑖 ; x 𝑗 ; |x 𝑖 − x 𝑗 |] as the joint representation of document 𝑑 𝑖 and 𝑑 𝑗 . Furthermore, we can derive 𝑐 𝑖 𝑗 = MLP(x 𝑖 𝑗 ), which is the judge of 𝑑 𝑖 and 𝑑 𝑗 given by the documents relation classifier. 𝑐 𝑖 𝑗 = 1 denotes that the document 𝑑 𝑖 and 𝑑 𝑗 might cover the same intent, while 𝑐 𝑖 𝑗 = 0 implies that the document 𝑑 𝑖 and 𝑑 𝑗 are less likely to share the joint intent.</p><p>Assuming that the number of all documents is 𝑛 = |D|, the total number of intent graph's nodes is 𝑛 + 1 since we present the query and all documents on the graph. Based on the result of the document relation classifier, we can derive the adjacent matrix A for the initial intent graph G 𝐷,𝜙 , where A ∈ R (𝑛+1)×(𝑛+1) . The adjacent matrix A is defined as:</p><formula xml:id="formula_7">A[𝑖, 𝑗] = 0, if 𝑖 = 1 or 𝑗 = 1;</formula><p>𝑐 (𝑖−1) ( 𝑗−1) , else.</p><p>(</p><p>Here A[𝑖, 𝑗] is the 𝑖-th row and 𝑗-th column element of A, which stands for the relation of document 𝑑 𝑖−1 and 𝑑 𝑗−1 (𝑖 ≥ 1 and 𝑗 ≥ 1).</p><p>According to the Algorithm 2, the adjacent matrix A dynamically changes in the document selection process. Given the selected document 𝑑 𝑘 at step 𝑡, we set A[𝑖, 𝑘] = A[𝑘, 𝑖] = 0 for 𝑖 ∈ [2, 𝑛 + 1] to drop all the edges between 𝑑 𝑘 and other documents. Then we set A[1, 𝑘] = A[𝑘, 1] = 𝑟 𝑘 to connect the query node and the document node 𝑣 𝑘 , where 𝑟 𝑘 is the relevance score of initial ranking without considering diversity.</p><p>It is worth noting that our classifier is only trained to predict whether two documents belong to the same intent. We do not predict whether a single document contains a specific intent, which is still a challenging task. This is also why our method is considered as an implicit method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Diversified Scoring based on Graph</head><p>As introduced in Section 3.1, in order to make a better document selection, we take both the relevance feature of the document and the diversity feature extracted from the intent graph into consideration. As we want to take the global document's relationship into account and represent the dynamic information need of the query, we propose leveraging the dynamic intent graph in the duration of document selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Representation Learning via GCN. Given the initial represen</head><formula xml:id="formula_9">- tation X = [X 𝑞 , X 1 , • • • , X 𝑛 ]</formula><p>of the query and document nodes, X 𝑞 is the distributed representation of query 𝑞, while X 𝑖 is the initial representation of the document 𝑑 𝑖 . Then we can update the representation using the information presented on the intent graph and get the new feature vectors [Z 𝑞 , Z 1 , • • • , Z 𝑛 ] of each node with local and global information. Instead of using document representations to calculate similarity, we hope to use similarity to generate document representations. Specifically, we leverage graph convolutional network (GCN) to aggregate neighbor's intent information to produce new document representation. With the help of GCN, the representations of documents will be enhanced by their neighbors with similar intents. The diversity features extracted by the GCN will be used to produce the diversity score of the documents.</p><p>In the first stage, documents nodes on the graph aggregate all the neighbors' feature vectors within a predefined scope 𝐾. Then the document nodes update their representation by the information collected from their neighbors. The procedure is conducted layer by layer. In this work, the scope 𝐾 is determined by the layer num 𝐿 of the GCN, namely, 𝐾 = 𝐿. According to our experiment, we set 𝐿 = 2. Concretely, supposing A is the corresponding adjacent matrix for the current intent graph G 𝐷,𝑆 , Z (0) = X is the initial representation of the nodes on the graph, we use GCN to calculate the features of the current nodes as follows:</p><formula xml:id="formula_10">Z (𝑙+1) = 𝜎 ( D− 1 2 Ã D− 1 2 Z (𝑙) W (𝑙) ),<label>(6)</label></formula><formula xml:id="formula_11">Ã = A + I 𝑁 ,<label>(7)</label></formula><p>where 𝑙 ∈ [0, 𝐿) is the identify of each layer in the GCN; A ∈ R (𝑛+1)×(𝑛+1) is the current adjacency matrix of the undirected intent graph and D[𝑖, 𝑖] = 𝑗 Ã[𝑖, 𝑗]; 𝑛 is the number of all candidate documents of query 𝑞; I 𝑁 is the identity matrix; Z (𝑙) ∈ R (𝑛+1)×𝐷 is the matrix of node features where 𝐷 is the dimension size of the node features; W (𝑙) is a layer-specific trainable weight matrix for 𝑙-th layer; and 𝜎 (•) is an activation function, e.g., ReLU(•) = max (0, •) or tanh(•).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Relevance and Diversity Features.</head><p>As shown in Equation ( <ref type="formula" target="#formula_0">1</ref>), we score each candidate document 𝑑 𝑖 based on relevance and diversity. Following previous work <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b47">48]</ref>, the relevance score <ref type="formula" target="#formula_2">2</ref>)) by the traditional relevance features R 𝑖 , including BM25, TF-IDF, PageRank, etc. The whole list of features is shown in Table <ref type="table" target="#tab_8">3</ref> and is consistent with that in <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b28">29]</ref>.</p><formula xml:id="formula_12">𝑓 rel (𝑑 𝑖 ) is produced (illustrated in Equation (</formula><p>The diversity score 𝑓 div (𝑑 𝑖 , D, S) is calculated (illustrated in Equation ( <ref type="formula" target="#formula_3">3</ref>)) based on the diversity features H(𝑑 𝑖 , D, S) extracted from the current intent graph G 𝐷,𝑆 :</p><formula xml:id="formula_13">H 𝑖 = [Z 𝑞 ; Z 𝑖 ; 𝐷 𝑖 ; T 𝑔 ],<label>(8)</label></formula><p>where H 𝑖 consists of the current query embedding Z 𝑞 , document embedding Z 𝑖 , degree feature 𝐷 𝑖 , and the whole graph representation T 𝑔 .</p><p>[; ] means the concatenation operation. We have:</p><formula xml:id="formula_14">Z 𝑞 = Z (𝐿) [1], Z 𝑖 = Z (𝐿) [𝑖 + 1],<label>(9)</label></formula><p>Z 𝑞 : The representation of query 𝑞. To make a fair comparison with the previous work <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b47">48]</ref>, we use doc2vec embedding as the initial query and the documents representation. Based on the intent graph, the representation of the query contains the information of selected documents, which can dynamically change when the graph is adjusted. With the dynamic representation of query 𝑞, our model can model the information needs of the query precisely and timely.</p><p>Z 𝑖 : The representation Z 𝑖 of document 𝑑 𝑖 , which contains the local information by aggregating the neighbor's features of document node 𝑣 𝑖 . We have Z 𝑖 = Z (𝐿) [𝑖 + 1] from the GCN.</p><p>𝐷 𝑖 : The degree of document 𝑑 𝑖 on the intent graph. For the diverse documents may share more edges with other documents, the degree of the node 𝑣 𝑖 in the intent graph is an essential measure to evaluate the diversity of document 𝑑 𝑖 . We have 𝐷 𝑖 = 𝑛+1 𝑗=2 A[𝑖, 𝑗]. Since we only use the diversity features of the remaining candidate documents, we omit the edges connecting the query node and selected document nodes when generating 𝐷 𝑖 .</p><p>T 𝑔 : The representation of the whole graph obtained by summing the representations of all documents. We have T 𝑔 = 𝑛+1 𝑖=1 Z (𝐿) [𝑖]. Derived from all the nodes vectors, the feature T 𝑔 is the global feature of the entire intent graph. Combined with local and global features, our approach can consider the comprehensive information in the diversification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Training and Optimization</head><p>The training for our model can be divided into two phases, the training for document relation classifier and the training for diversity ranking. They will be introduced as follows.</p><p>(1) Classifier Training. We use the diversity judgements to generate the training data for document relation classifier. For example, if 𝑑 1 and 𝑑 2 cover one intent, and 𝑑 2 and 𝑑 3 cover another intent, we can obtain (𝑑 1 , 𝑑 2 , 1) and (𝑑 2 , 𝑑 3 , 1) as positive samples, while (𝑑 1 , 𝑑 3 , 0) as a negative sample. With the generated data, we use a binary cross-entropy to train the classifier. In our experiment, the positive samples are much fewer than the negative samples (about 1:8). To accelerate the training process and avoid bias, we randomly discard negative samples to keep the ratio as 1:1.</p><p>(2) Diversity Ranking. Given a query set Q, the diversity ranking R 𝑞 is produced based on Algorithm 1 (Graph4DIV):</p><formula xml:id="formula_15">R 𝑞 = Graph4DIV(𝑞, D 𝑞 , G 𝐷,𝜙 ),<label>(10)</label></formula><formula xml:id="formula_16">𝑓 = arg min ∑︁ 𝑞 ∈ Q ∑︁ 𝑜 ∈ O L (R 𝑞 , 𝑌 𝑜 ),<label>(11)</label></formula><p>where 𝑞 ∈ Q, 𝑌 𝑜 is a ground-truth ranking of training sample of query 𝑞, and L is the loss function of the model. We follow the previous studies <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b28">29]</ref> and utilize the list-pairwise loss function for optimization. The generation of ground-truth ranking 𝑌 𝑜 follows the procedure of list-pairwise training <ref type="bibr" target="#b15">[16]</ref>. This loss function is computed based on the same sampling strategy proposed by Jiang et al. <ref type="bibr" target="#b15">[16]</ref>. The sample pair (𝑟 1 , 𝑟 2 ) could be presented as (𝐶 𝑡 −1 , 𝑑 1 , 𝑑 2 ), where C 𝑡 −1 is the same previous document sequence shared by 𝑟 1 and 𝑟 2 . Ranking 𝑟 1 and 𝑟 2 are different only at 𝑡-th position. The list-pairwise loss can be defined as:</p><formula xml:id="formula_17">L = ∑︁ 𝑞 ∈Q | O 𝑞 | ∑︁ 𝑜=1 𝑤 𝑜 𝑦 𝑜 log(𝑃 (𝑟 𝑜 1,2 )) + (1 − 𝑦 𝑜 ) log 1 − 𝑃 (𝑟 𝑜 1,2 ) ,</formula><p>where O 𝑞 is the pair samples set of query 𝑞. 𝑃 (𝑟 𝑜 1,2 ) is the probability of pair (𝑟 1 , 𝑟 2 ) to be positive, and 𝑤 𝑜 is the weight of pair sample (𝑟 1 , 𝑟 2 ) based on the metric function 𝑀 as:</p><formula xml:id="formula_18">𝑃 (𝑟 𝑜 1,2 ) = 1 1 + exp(𝑠 𝑜 𝑟 2 − 𝑠 𝑜 𝑟 1 ) ,<label>(12)</label></formula><formula xml:id="formula_19">𝑤 𝑜 = |𝑀 (𝑟 𝑜 1 ) − 𝑀 (𝑟 𝑜 2 )|,<label>(13)</label></formula><p>where 𝑀 is the metric function that evaluates the quality of model's diversity ranking. 𝑀 (𝑟 1 ) &gt; 𝑀 (𝑟 2 ) shows that (𝑟 1 , 𝑟 2 ) is a positive pair, while 𝑀 (𝑟 1 ) &lt; 𝑀 (𝑟 2 ) implies (𝑟 1 , 𝑟 2 ) to be negative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS 4.1 Data Collections</head><p>We use the ClueWeb09 dataset <ref type="bibr" target="#b1">[2]</ref>, which is the same as previous work <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b28">29]</ref>. The ClueWeb09 contains 200 queries of Web Track dataset from TREC 2009 to 2012. However, because query #95 and #100 have no diversity judgements, only 198 queries are used in our experiment. There are 3 to 8 subtopics for each query. In the experiment, the subtopic features are only used to train explicit baseline methods and they are not used in our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Metrics</head><p>We use various metrics that are used by lots of previous research, including 𝛼-nDCG <ref type="bibr" target="#b6">[7]</ref>, ERR-IA <ref type="bibr" target="#b3">[4]</ref>, and NRBP <ref type="bibr" target="#b7">[8]</ref>, which are official diversity evaluation metrics used in Web Track. They measure the diversity by explicitly rewarding novelty and penalizing redundancy. Besides, we also use the diversity measure Subtopic Recall (denoted as S-rec, a.k.a. I-rec) <ref type="bibr" target="#b44">[45]</ref>. Consistent with previous diversification models <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b47">48]</ref> and TREC Web Track, we adopt the top 50 results of Lemur<ref type="foot" target="#foot_2">2</ref> for diversity re-ranking, and all evaluation metrics are computed on the top 20 results of a document ranking list. Two-tailed paired t-test is used to conduct significance testing with 𝑝-value &lt; 0.05.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baseline Models</head><p>We compare our Graph4DIV with various methods including:</p><p>(1) Non-diversified methods. Lemur: We use the same adhoc results as <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16]</ref> for fair comparison. Results of Lemur are produced by Indri engine using language model. ListMLE <ref type="bibr" target="#b36">[37]</ref> is a learningto-ranking method without considering diversity.</p><p>(2) Explicit methods. xQuAD <ref type="bibr" target="#b31">[32]</ref>, PM2 <ref type="bibr" target="#b8">[9]</ref>, TxQuAD, TPM2 <ref type="bibr" target="#b9">[10]</ref>, HxQuAD, and HPM2 <ref type="bibr" target="#b13">[14]</ref> are some representative unsupervised explicit baseline methods. Similar to our method, all these methods balance the importance of relevance and diversity by a parameter 𝜆. Based on the hierarchical structure, HxQuAD and HPM2 adopt an additional parameter 𝛼 to control the weight of subtopic layers. DSSA <ref type="bibr" target="#b15">[16]</ref> is a supervised explicit diversification method, which models the diversity of the documents with subtopic attention at each step in the document selection process using RNNs. Explicit methods are shown to be more effective than the implicit methods in existing studies <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b28">29]</ref>. Note that our proposed Graph4DIV does not use subtopics and it is an implicit method.</p><p>(3) Implicit methods. R-LTR <ref type="bibr" target="#b47">[48]</ref>, PAMM <ref type="bibr" target="#b37">[38]</ref>, and NTN <ref type="bibr" target="#b38">[39]</ref> are the representative supervised implicit methods. For PAMM, we use 𝛼-nDCG@20 as the optimization metrics and tune the number of positive rankings 𝑙 + and negative rankings 𝑙 − per query. The neural tensor network (NTN) is used on both R-LTR and PAMM, denoted as R-LTR-NTN and PAMM-NTN, respectively.</p><p>(4) Ensemble methods. DESA <ref type="bibr" target="#b28">[29]</ref> and DVGAN <ref type="bibr" target="#b20">[21]</ref> are two ensemble methods that use both explicit (subtopic) features and implicit features. DESA leverages self-attention based encoder-decoder structure to model the interactions between documents and subtopics. With the framework of the generative adversarial network, DVGAN is able to generate training data that combine both explicit and implicit features. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Implementation Details</head><p>For building the relation classifier in our model, we use the pretrained BERT provided by HuggingFace <ref type="bibr" target="#b35">[36]</ref> and fine tune it for our relation classification. The maximum token sequence length 𝑀 is 512 in BERT. The batch size is 16. We use AdamW <ref type="bibr" target="#b23">[24]</ref> as the optimizer with the learning rate of 3e-5. The number of epochs is set as 3. As we use 5-fold cross-validation for training and testing our Graph4DIV, we also train 5 classifiers correspondingly. For training Graph4DIV model, we adopt doc2vec embeddings with the dimension of 100 as the initial document representations on the intent graph, which is the same as previous work <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b47">48]</ref>. The number of GCN layers is tuned in {1,2,3}, and the learning rate is tuned from 1e-10 to 1e-2 and set as 8e-4. For balancing the weights of relevance score and diversity score in Equation (1), we tune the 𝜆 in {0.1, 0.2, • • • , 0.9} and finally set 𝜆 = 0.5. All hyper-parameters are selected by 5-fold cross-validation based on the result of 𝛼-nDCG@20. Our code is is available at https://github.com/su-zhan/ Graph4DIV.git.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Experimental Results</head><p>The overall results are shown in Table <ref type="table" target="#tab_9">4</ref>. We find that GraphDIV outperforms all explicit, implicit, and ensemble methods. This result clearly demonstrates the superiority of our method.</p><p>(1) Graph4DIV significantly outperforms all implicit methods (R-LTR, PAMM, R-LTR-NTN, and PAMM-NTN) in terms of ERR-IA, 𝛼-nDCG, and NRBP (t-test with 𝑝-value &lt; 0.05). This result proves the effectiveness of our proposed Graph4DIV on search result diversification. Specifically, R-LTR-NTN and PAMM-NTN are two state-ofthe-art implicit supervised methods. They calculate the document's novelty by using a neural tensor network based on the document representations. Compared with these two methods, Graph4DIV improves the absolute value of 𝛼-nDCG by more than 5%. This indicates that our proposed intent graph can better represent the complicated relationship among several documents than only considering the pairwise similarity between the single candidate document and each of the selected documents. Besides, our GraphDIV does not rely on many artificial features used in R-LTR, which improves its applicability in real scenarios.</p><p>(2) Graph4DIV also outperforms explicit methods by a large margin, including DSSA, which is the state-of-the-art explicit method. Indeed, either the methods based on xQuAD or those based on PM2 are unsupervised approaches. The better performance obtained by DSSA and Graph4DIV proves the great advantage of using the supervised method for learning the ranking function in search result diversification. Furthermore, compared with DSSA which explicitly models subtopic coverage, our Graph4DIV only measures the novelty of each document implicitly based on our proposed intent graph through a GCN. Surprisingly, Graph4DIV can still achieve better performance than DSSA regarding all metrics. This demonstrates that the intent graph can well reflect the relation between documents by considering their underlying intents, which is extremely helpful for diversifying the search results.</p><p>(3) Interestingly, we also find Graph4DIV can perform slightly better than ensemble methods, i.e., DESA and DVGAN. DESA leverages the self-attention mechanism to measure the similarity between documents and enhances the document representations. The similarity computed by the self-attention mechanism is only tuned by the final diversification loss. On the contrary, the similarity of documents used in Graph4DIV is computed by a fine-tuned BERT with supervision signals of shared intents. With such separated and clear supervision, the similarity between documents can be better captured. On the other hand, we employ a GCN to integrate the similarity information into document representations. Only the documents within a predefined scale can be aggregated, thus avoiding the noise in irrelevant documents. As future work, we plan to equip our Graph4DIV with explicit subtopic features and make it as an ensemble model, which may bring further improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DISCUSSIONS</head><p>To better analyze our model, we further investigate two research questions: (1) How is the performance of the relation classifier and what is the influence on the final results? (2) What is the effect of each component in Graph4DIV and how is the performance of it with other settings?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Performance and Influence of Classifier</head><p>In our model, the intent graph is built based on the predicted relations of documents. Therefore, the performance of this classifier may influence the final diversification results. Since the whole experiment is conducted with the 5-fold cross-validation, we build five document relation classifiers for the corresponding folds, respectively. The average Accuracy of five fold-classifiers is 0.792 and the average F1 is 0.884. Due to the difficulty of judging two documents whether belong to the same subtopic using only the content, we believe it is a good performance for the classifier and it is also a meaningful exploration for a better utility of document's relation in diversification task. Compared with the excellent performance  of Graph4DIV, it implies the robustness and generalization of our approach.</p><p>To further investigate the upper bound of using the intent graph in the search result diversification task, we use the ground-truth label for building the graph, where the edge between two documents certainly indicates they belong to the same subtopic. The result is shown in Table <ref type="table" target="#tab_10">5</ref>, which is denoted as Ground-truth Intent Graph. According to the results, we can observe that there is still a large gap between Graph4DIV and that using the ground-truth intent graph. This demonstrates the potential of using the intent graph for diversification. As the average accuracy of our document relation classifier is around 0.8, we speculate that building a better classifier may bring further improvements for Graph4DIV. As a reference, we also provide the result of the Ground-truth Ranking. It is clear to see that only leveraging the intent graph is not enough for the diversification task. The potential reasons include: (1) The intent graph only reflects whether two documents belong to the same intent, but does not contain the information of the specific subtopics that the document belongs to. Missing such information, it is still hard to select the optimal document at each step. <ref type="bibr" target="#b1">(2)</ref> The GCN can aggregate documents that may share the same user intents and update their representations. However, it is difficult for GCN to infer the subtopic coverage since the novelty of each document can only be measured implicitly by its similarity with other documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Effects of Different Settings</head><p>We also investigate the influence of different settings on the performance of Graph4DIV. The results are shown in Table <ref type="table" target="#tab_11">6</ref>.</p><p>(1) Ablation of diversity features. Since the diversity features contain various features extracted from the intent graph, we explore the effectiveness of them by removing them one by one from the full model. The document representations are basic features, we only remove the other three ones, namely, the query representation Z 𝑞 (w/o Query Rep.), the graph features T 𝑔 (w/o Graph Features), and the document degrees 𝐷 𝑖 (w/o Doc. Degree). In general, removing any one of them leads to performance degradation. This demonstrates all three kinds of features are effective in our method. Specifically, the performance drops most when removing the graph features. The potential reason is that the graph features can provide a global view of all documents, which helps to detect novelty and determine the next document.</p><p>(2) Graph Adjustment. To validate the effect of our proposed graph adjustment strategy, we replace it by only using the initial intent graph for training. This variant is denoted as w/o Graph Adjustment. In this model, the document diversity ranking is directly obtained based on the score derived from the initial graph. It is clear to see that the performance degrades when the graph is static. In our graph adjustment algorithm, the selected document is connected with the query to update its representation. The updated query representation can reflect how many subtopics have already been covered. On the other hand, we break the edge between the selected document and the remaining ones, so that the candidate document cannot affect the representation of the query, which further reduce the noise.</p><p>(3) Different GNNs. As there are several graph neural networks, we also try other ones, such as Graph Isomorphism Network (GIN) <ref type="bibr" target="#b39">[40]</ref>, in Graph4DIV. All hyper-parameters are kept the same without careful tuning. We can observe the results with GINs (w/ GIN) are worse compared to the model with GCNs but still competitive. This shows that other GNN models could also be used on our intent graphs and demonstrates the scalability of our method.</p><p>(4) Different number of layers in GNN. The number of layers in GCNs is also important in our method. It determines how many neighbors are considered when updating the representation of a specific document. Based on the result, we can observe that it is not enough to aggregate documents within only one hop (1-layer-GCN). On the contrary, introducing more layers also hurts the performance. This may stem from the over-smoothing problem, which is often observed in multi-layer GCNs <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b40">41]</ref>. According to the experimental results, it is suitable for Graph4DIV to use the 2-layer-GCN structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS</head><p>In this paper, we propose an implicit supervised approach that models the relationship of multiple document pairs simultaneously with graph structure for search result diversification. We further use the graph convolutional network to extract the diversity features that contain both local and global information. To capture the dynamic information needs of the query, we design a graph adjust algorithm for the intent graph to timely present the situation during the document selection process. The experimental results also confirm that our dynamic intent graph is beneficial and meaningful to generate diversity features for the documents in the diversification task.</p><p>In the future, we plan to improve the accuracy of the classifier by combining more information and apply the intent graph to the explicit search result diversification methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Adjustment of intent graph based on the selection result of candidate documents.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Documents Relation Classifier based on BERT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Session 3E :</head><label>3E</label><figDesc>Diversity and Novelty SIGIR '21, July 11-15, 2021, Virtual Event, Canada</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Categorization of diversification approaches.</figDesc><table><row><cell></cell><cell>Unsupervised</cell><cell>Supervised</cell></row><row><cell>Explicit</cell><cell>IA-Select, HxQuAD,</cell><cell>DSSA, DESA, DVGAN</cell></row><row><cell></cell><cell>PM2, TPM2, TxQuAD,</cell><cell></cell></row><row><cell></cell><cell>xQuAD, HPM2</cell><cell></cell></row><row><cell>Implicit</cell><cell>MMR</cell><cell>SVM-DIV, R-LTR, PAMM, NTN,</cell></row><row><cell></cell><cell></cell><cell>Graph4DIV (our approach)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Notations in Graph4DIV 𝐸 the edges set of the intent graph 𝑣 𝑞 the node of the query 𝑞 in the intent graph 𝑣 𝑖 the node of the document 𝑑 𝑖 in the intent graph R ranking sequence of the query 𝑞 R 𝑖 the relevance feature of 𝑖-th document 𝑑 𝑖 H 𝑖 the diversity feature of 𝑖-th document 𝑑 𝑖 the graph. With the graph structure, our approach can model the complicated relation of multiple documents based on these hidden intents simultaneously and extract both global and local features via GCN.</figDesc><table><row><cell cols="2">Notation Definition</cell></row><row><cell>Q, 𝑞</cell><cell>the query set, the query in the set, 𝑞 ∈ Q</cell></row><row><cell>D</cell><cell>documents set for the query 𝑞, | D | = 𝑛</cell></row><row><cell>S</cell><cell>selected document sequence for the query 𝑞</cell></row><row><cell>C</cell><cell>remaining documents for query 𝑞, C = D \ S</cell></row><row><cell>G 𝐷,𝑆</cell><cell></cell></row></table><note>the intent graph after S is selected from D 𝑁 the nodes set of the intent graph, |𝑁 | = 𝑛 + 1</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>1 :</head><label>1</label><figDesc>Procedure Graph4DIV Diversity Ranking 2: Input: query 𝑞, document set D, and initial intent graph G 𝐷,𝜙 . 3: Output: diversity ranking sequence R for query 𝑞.</figDesc><table><row><cell cols="2">4: S ← ∅</cell></row><row><cell cols="2">5: C ← D //initial state C = D \ S = D</cell></row><row><cell cols="2">6: 𝑡 ← 0</cell></row><row><cell cols="2">7: while C do</cell></row><row><cell>8:</cell><cell>𝑡 ← 𝑡 + 1</cell></row><row><cell>9:</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>P</head><label></label><figDesc>𝑡 ← ∅ //P 𝑡 is the score set of the candidates at 𝑡 step P 𝑡 ← P 𝑡 ∪ { 𝑓 (𝑑 𝑖 , D, S) } //append the score of 𝑑 𝑖</figDesc><table><row><cell>10:</cell><cell>for document 𝑑 𝑖 ∈ C do</cell></row><row><cell>11:</cell><cell></cell></row><row><cell>12:</cell><cell>end for</cell></row><row><cell>13:</cell><cell>𝑑  *  = getbest( P 𝑡 )</cell></row><row><cell>14:</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>G</head><label></label><figDesc>𝐷,𝑆 ← GraphAdjust(𝑞, 𝑑 * , G 𝐷,𝑆 )</figDesc><table><row><cell>15:</cell><cell>S ← S ∪ {𝑑  *  }</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>16:</cell><cell>C ← C \ {𝑑  *  }</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">17: end while</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">18: R ← S</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">19: return R</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>𝑑" 𝑑 !</cell><cell>Relation Classifier</cell><cell>𝑑 !</cell><cell>𝑑 #</cell><cell>Select Evaluate and</cell><cell>𝑑 "</cell><cell>𝑑!</cell><cell>𝑑 #</cell></row><row><cell>𝑑#</cell><cell>Graph</cell><cell>𝑞</cell><cell>𝑑%</cell><cell></cell><cell></cell><cell>𝑞</cell><cell>𝑑%</cell></row><row><cell>𝑑 $</cell><cell>Builder</cell><cell>𝑑 "</cell><cell>𝑑$</cell><cell>Algorithm Graph Adjust</cell><cell></cell><cell>𝑑 "</cell><cell>𝑑 $</cell></row><row><cell>𝑑%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>𝑞</cell><cell cols="2">Initial Graph</cell><cell></cell><cell></cell><cell cols="2">Adjusted Graph</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>1 :</head><label>1</label><figDesc>Procedure GraphAdjust 2: Input: query 𝑞, selected document 𝑑 𝑘 and the intent graph G 𝐷,𝑆 for query 𝑞 at step 𝑡 . 3: Output: Adjusted intent graph G 𝐷,𝑆 . 4: N 𝑘 ← getNeighbors(G 𝐷,𝑆 , 𝑑 𝑘 ) 5: for document 𝑑 𝑖 ∈ N 𝑘 do</figDesc><table><row><cell>6:</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>G</head><label></label><figDesc>𝐷,𝑆 ← removeLink(G 𝐷,𝑆 , 𝑑 𝑖 , 𝑑 𝑘 ) 7: end for 8: G 𝐷,𝑆 ← addLinktoQuery( G 𝐷,𝑆 , 𝑞, 𝑑 𝑘 ) 9: return G 𝐷,𝑆</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 3 :</head><label>3</label><figDesc>Relevance features used by previous methods</figDesc><table><row><cell>Name</cell><cell>Description</cell><cell># Features</cell></row><row><cell>TF-IDF</cell><cell>TF-IDF model</cell><cell>5</cell></row><row><cell>BM25</cell><cell>BM25 with default parameters</cell><cell>5</cell></row><row><cell>LMIR</cell><cell>LMIR with Dirichlet smoothing</cell><cell>5</cell></row><row><cell cols="2">PageRank PageRank score</cell><cell>1</cell></row><row><cell>#Inlinks</cell><cell>Number of inlinks</cell><cell>1</cell></row><row><cell cols="2">#Outlinks Number of outlinks</cell><cell>1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>Performance comparison of all methods. The baselines include: (1) non-diversed methods; (2) explicit methods; (3) implicit methods; and (4) ensemble methods. The best result is in bold. † indicates significant improvements obtained by GraphDIV in t-test with 𝑝-value&lt; 0.05.</figDesc><table><row><cell></cell><cell>ERR-IA</cell><cell>𝛼-nDCG</cell><cell>NRBP</cell><cell>S-rec</cell></row><row><cell>(1) Lemur</cell><cell>.271  †</cell><cell>.369  †</cell><cell>.232  †</cell><cell>.621  †</cell></row><row><cell>(1) ListMLE</cell><cell>.287  †</cell><cell>.387  †</cell><cell>.249  †</cell><cell>.619  †</cell></row><row><cell>(2) xQuAD</cell><cell>.317  †</cell><cell>.413  †</cell><cell>.284  †</cell><cell>.622  †</cell></row><row><cell>(2) TxQuAD</cell><cell>.308  †</cell><cell>.410  †</cell><cell>.272  †</cell><cell>.634</cell></row><row><cell>(2) HxQuAD</cell><cell>.326  †</cell><cell>.421  †</cell><cell>.294  †</cell><cell>.629</cell></row><row><cell>(2) PM2</cell><cell>.306  †</cell><cell>.411  †</cell><cell>.267  †</cell><cell>.643</cell></row><row><cell>(2) TPM2</cell><cell>.291  †</cell><cell>.399  †</cell><cell>.250  †</cell><cell>.639</cell></row><row><cell>(2) HPM2</cell><cell>.317  †</cell><cell>.420  †</cell><cell>.279  †</cell><cell>.645</cell></row><row><cell>(2) DSSA</cell><cell>.356</cell><cell>.456</cell><cell>.326</cell><cell>.649</cell></row><row><cell>(3) R-LTR</cell><cell>.303  †</cell><cell>.403  †</cell><cell>.267  †</cell><cell>.631</cell></row><row><cell>(3) PAMM</cell><cell>.309  †</cell><cell>.411  †</cell><cell>.271  †</cell><cell>.643</cell></row><row><cell>(3) R-LTR-NTN</cell><cell>.312  †</cell><cell>.415  †</cell><cell>.275  †</cell><cell>.644</cell></row><row><cell>(3) PAMM-NTN</cell><cell>.311  †</cell><cell>.417  †</cell><cell>.272  †</cell><cell>.648</cell></row><row><cell>(3) Graph4DIV (Ours)</cell><cell>.370</cell><cell>.468</cell><cell>.338</cell><cell>.666</cell></row><row><cell>(4) DESA</cell><cell>.363</cell><cell>.464</cell><cell>.332</cell><cell>.653</cell></row><row><cell>(4) DVGAN</cell><cell>.367</cell><cell>.465</cell><cell>.334</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>Comparison between Graph4DIV and Ground-truth.</figDesc><table><row><cell></cell><cell cols="4">ERR-IA 𝛼-nDCG NRBP S-rec</cell></row><row><cell>Graph4DIV</cell><cell>.370</cell><cell>.468</cell><cell>.338</cell><cell>.666</cell></row><row><cell>Ground-truth Intent Graph</cell><cell>.477</cell><cell>.563</cell><cell>.461</cell><cell>.673</cell></row><row><cell>Ground-truth Ranking</cell><cell>.574</cell><cell>.657</cell><cell>.568</cell><cell>.706</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>Performance of Graph4DIV with different settings.</figDesc><table><row><cell></cell><cell cols="4">ERR-IA 𝛼-nDCG NRBP S-rec</cell></row><row><cell>Graph4DIV</cell><cell>.370</cell><cell>.468</cell><cell>.338</cell><cell>.666</cell></row><row><cell>w/o Query Rep.</cell><cell>.355</cell><cell>.455</cell><cell>.322</cell><cell>.653</cell></row><row><cell>w/o Graph Features</cell><cell>.347</cell><cell>.446</cell><cell>.315</cell><cell>.650</cell></row><row><cell>w/o Doc. Degree</cell><cell>.352</cell><cell>.452</cell><cell>.319</cell><cell>.657</cell></row><row><cell>w/o Graph Adjustment</cell><cell>.361</cell><cell>.461</cell><cell>.328</cell><cell>.666</cell></row><row><cell>w/ GIN</cell><cell>.361</cell><cell>.458</cell><cell>.330</cell><cell>.655</cell></row><row><cell>1-layer-GCN</cell><cell>.353</cell><cell>.453</cell><cell>.318</cell><cell>.653</cell></row><row><cell>3-layer-GCN</cell><cell>.355</cell><cell>.458</cell><cell>.320</cell><cell>.666</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">To reduce the notation redundancy, we omit the query 𝑞 in all equations.Session 3E: Diversity and Novelty SIGIR<ref type="bibr" target="#b20">'21,</ref>  </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2021" xml:id="foot_1">, Virtual Event, Canada</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2">Lemur service: http://boston.lti.cs.cmu.edu/Services/clueweb09 batch/ Session</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3">3E: Diversity and Novelty SIGIR '21, July 11-15, 2021, Virtual Event, Canada</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>Zhicheng Dou is the corresponding author. This work was supported by National Natural Science Foundation of China No. 61872370 and No. 61832017, Beijing Outstanding Young Scientist Program NO. BJJWZYJH012019100020098, and Shandong Provincial Natural Science Foundation under Grant ZR2019ZD06. Session 3E: Diversity and Novelty SIGIR '21, July 11-15, 2021, Virtual Event, Canada</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Diversifying search results</title>
		<author>
			<persName><forename type="first">Rakesh</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sreenivas</forename><surname>Gollapudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Halverson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Ieong</surname></persName>
		</author>
		<idno type="DOI">10.1145/1498759.1498766</idno>
		<ptr target="https://doi.org/10.1145/1498759.1498766" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second International Conference on Web Search and Web Data Mining</title>
				<editor>
			<persName><forename type="first">Paolo</forename><surname>Boldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Berthier</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Berkant</forename><surname>Ribeiro-Neto</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Cambazoglu</forename><surname>Barla</surname></persName>
		</editor>
		<meeting>the Second International Conference on Web Search and Web Data Mining<address><addrLine>Barcelona, Spain; Ricardo Baeza-Yates</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009-02-09">2009. 2009. February 9-11, 2009</date>
			<biblScope unit="page" from="5" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Hoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changkuk</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Zhao</surname></persName>
		</author>
		<ptr target="https://boston.lti.cs.cmu.edu/Data/clueweb09/" />
		<title level="m">Clueweb09 data set</title>
				<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The Use of MMR, Diversity-Based Reranking for Reordering Documents and Producing Summaries</title>
		<author>
			<persName><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jade</forename><surname>Goldstein</surname></persName>
		</author>
		<idno type="DOI">10.1145/290941.291025</idno>
		<ptr target="https://doi.org/10.1145/290941.291025" />
	</analytic>
	<monogr>
		<title level="m">SIGIR &apos;98: Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
				<editor>
			<persName><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Alistair</forename><surname>Moffat</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Van Rijsbergen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ross</forename><surname>Wilkinson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Justin</forename><surname>Zobel</surname></persName>
		</editor>
		<meeting><address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998-08-24">1998. August 24-28 1998</date>
			<biblScope unit="page" from="335" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Expected reciprocal rank for graded relevance</title>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Metlzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Grinspan</surname></persName>
		</author>
		<idno type="DOI">10.1145/1645953.1646033</idno>
		<ptr target="https://doi.org/10.1145/1645953.1646033" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM Conference on Information and Knowledge Management</title>
				<editor>
			<persName><forename type="first">David</forename><surname>Wai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">-Lok</forename><surname>Cheung</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Il-Yeol</forename><surname>Song</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Wesley</forename><forename type="middle">W</forename><surname>Chu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Xiaohua</forename><surname>Hu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jimmy</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</editor>
		<meeting>the 18th ACM Conference on Information and Knowledge Management<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009-11-02">2009. November 2-6, 2009</date>
			<biblScope unit="page" from="621" to="630" />
		</imprint>
	</monogr>
	<note>CIKM 2009</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Measuring and Relieving the Over-Smoothing Problem for Graph Neural Networks from the Topological View</title>
		<author>
			<persName><forename type="first">Deli</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<ptr target="https://aaai.org/ojs/index.php/AAAI/article/view/5747" />
	</analytic>
	<monogr>
		<title level="m">The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference</title>
				<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020-02-07">2020. February 7-12, 2020</date>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="3438" to="3445" />
		</imprint>
	</monogr>
	<note>The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">BAM! Born-Again Multi-Task Networks for Natural Language Understanding</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Urvashi</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p19-1595</idno>
		<ptr target="https://doi.org/10.18653/v1/p19-1595" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019</title>
				<editor>
			<persName><forename type="first">Long</forename><surname>Papers</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">David</forename><forename type="middle">R</forename><surname>Traum</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Lluís</forename><surname>Màrquez</surname></persName>
		</editor>
		<meeting>the 57th Conference of the Association for Computational Linguistics, ACL 2019<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-07-28">2019. July 28-August 2, 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5931" to="5937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Novelty and diversity in information retrieval evaluation</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maheedhar</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Kolla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Azin</forename><surname>Vechtomova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Ashkan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Büttcher</surname></persName>
		</author>
		<author>
			<persName><surname>Mackinnon</surname></persName>
		</author>
		<idno type="DOI">10.1145/1390334.1390446</idno>
		<ptr target="https://doi.org/10.1145/1390334.1390446" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2008</title>
				<editor>
			<persName><forename type="first">Hyon</forename><surname>Myaeng</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Fabrizio</forename><surname>Sebastiani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mun-Kew</forename><surname>Leong</surname></persName>
		</editor>
		<meeting>the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2008<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008-07-20">2008. July 20-24, 2008</date>
			<biblScope unit="page" from="659" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An Effectiveness Measure for Ambiguous and Underspecified Queries</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maheedhar</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Kolla</surname></persName>
		</author>
		<author>
			<persName><surname>Vechtomova</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-04417-5_17</idno>
		<idno>ICTIR 2009</idno>
		<ptr target="https://doi.org/10.1007/978-3-642-04417-5_17" />
	</analytic>
	<monogr>
		<title level="m">Advances in Information Retrieval Theory, Second International Conference on the Theory of Information Retrieval</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Leif</forename><surname>Azzopardi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Gabriella</forename><surname>Kazai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Stephen</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Stefan</forename><forename type="middle">M</forename><surname>Rüger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Milad</forename><surname>Shokouhi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Dawei</forename><surname>Song</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Emine</forename><surname>Yilmaz</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009-09-10">2009. September 10-12, 2009</date>
			<biblScope unit="volume">5766</biblScope>
			<biblScope unit="page" from="188" to="199" />
		</imprint>
	</monogr>
	<note>Proceedings</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Diversity by proportionality: an electionbased approach to search result diversification</title>
		<author>
			<persName><forename type="first">Van</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
		<idno type="DOI">10.1145/2348283.2348296</idno>
		<ptr target="https://doi.org/10.1145/2348283.2348296" />
	</analytic>
	<monogr>
		<title level="m">The 35th International ACM SI-GIR conference on research and development in Information Retrieval, SIGIR &apos;12</title>
				<editor>
			<persName><forename type="first">William</forename><forename type="middle">R</forename><surname>Hersh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yoelle</forename><surname>Maarek</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mark</forename><surname>Sanderson</surname></persName>
		</editor>
		<meeting><address><addrLine>Portland, OR, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012-08-12">2012. August 12-16, 2012</date>
			<biblScope unit="page" from="65" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Term level search result diversification</title>
		<author>
			<persName><forename type="first">Van</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
		<idno type="DOI">10.1145/2484028.2484095</idno>
		<ptr target="https://doi.org/10.1145/2484028.2484095" />
	</analytic>
	<monogr>
		<title level="m">The 36th International ACM SIGIR conference on research and development in Information Retrieval, SIGIR &apos;13</title>
				<editor>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Gareth</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Paraic</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Diane</forename><surname>Sheridan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Maarten</forename><surname>Kelly</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tetsuya</forename><surname>De Rijke</surname></persName>
		</editor>
		<editor>
			<persName><surname>Sakai</surname></persName>
		</editor>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013-07-28">2013. July 28 -August 01, 2013</date>
			<biblScope unit="page" from="603" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Benchmarking Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Prakash Dwivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaitanya</forename><forename type="middle">K</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00982</idno>
		<ptr target="https://arxiv.org/abs/2003.00982" />
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning to Diversify for E-commerce Search with Multi-Armed Bandit</title>
		<author>
			<persName><forename type="first">Anjan</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prasant</forename><surname>Mohapatra</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2410/paper18.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SI-GIR 2019 Workshop on eCommerce, co-located with the 42st International ACM SIGIR Conference on Research and Development in Information Retrieval, eCom@SIGIR 2019</title>
		<title level="s">CEUR Workshop Proceedings</title>
		<editor>
			<persName><forename type="first">Jon</forename><surname>Degenhardt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Surya</forename><surname>Kallumadi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Utkarsh</forename><surname>Porwal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Andrew</forename><surname>Trotman</surname></persName>
		</editor>
		<meeting>the SI-GIR 2019 Workshop on eCommerce, co-located with the 42st International ACM SIGIR Conference on Research and Development in Information Retrieval, eCom@SIGIR 2019<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-07-25">2019. July 25, 2019</date>
			<biblScope unit="volume">2410</biblScope>
		</imprint>
	</monogr>
	<note>CEUR-WS.org</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Speaker-Aware BERT for Multi-Turn Response Selection in Retrieval-Based Chatbots</title>
		<author>
			<persName><forename type="first">Jia-Chen</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianda</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen-Hua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiming</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3340531.3412330</idno>
		<ptr target="https://doi.org/10.1145/3340531.3412330" />
	</analytic>
	<monogr>
		<title level="m">CIKM &apos;20: The 29th ACM International Conference on Information and Knowledge Management, Virtual Event, Ireland</title>
				<editor>
			<persName><forename type="first">Stefan</forename><surname>Mathieu D'aquin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Claudia</forename><surname>Dietze</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Edward</forename><surname>Hauff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Philippe</forename><surname>Curry</surname></persName>
		</editor>
		<editor>
			<persName><surname>Cudré-Mauroux</surname></persName>
		</editor>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020-10-19">2020. October 19-23, 2020</date>
			<biblScope unit="page" from="2041" to="2044" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Search Result Diversification Based on Hierarchical Intents</title>
		<author>
			<persName><forename type="first">Sha</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Jie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tetsuya</forename><surname>Sakai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<idno type="DOI">10.1145/2806416.2806455</idno>
		<ptr target="https://doi.org/10.1145/2806416.2806455" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM International Conference on Information and Knowledge Management, CIKM 2015</title>
				<editor>
			<persName><forename type="first">James</forename><surname>Bailey</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Alistair</forename><surname>Moffat</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Charu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Maarten</forename><surname>Aggarwal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ravi</forename><surname>De Rijke</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Vanessa</forename><surname>Kumar</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Timos</forename><forename type="middle">K</forename><surname>Murdock</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jeffrey</forename><surname>Sellis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yu</forename><surname>Xu</surname></persName>
		</editor>
		<meeting>the 24th ACM International Conference on Information and Knowledge Management, CIKM 2015<address><addrLine>Melbourne, VIC, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015-10-19">2015. October 19 -23, 2015</date>
			<biblScope unit="page" from="63" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning Query and Document Relevance from a Webscale Click Graph</title>
		<author>
			<persName><forename type="first">Shan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuening</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changsung</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Daly</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Dawei Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengxiang</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><surname>Zhai</surname></persName>
		</author>
		<idno type="DOI">10.1145/2911451.2911531</idno>
		<ptr target="https://doi.org/10.1145/2911451.2911531" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval, SIGIR 2016</title>
				<editor>
			<persName><forename type="first">Raffaele</forename><surname>Perego</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Fabrizio</forename><surname>Sebastiani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Javed</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ian</forename><surname>Aslam</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Justin</forename><surname>Ruthven</surname></persName>
		</editor>
		<editor>
			<persName><surname>Zobel</surname></persName>
		</editor>
		<meeting>the 39th International ACM SIGIR conference on Research and Development in Information Retrieval, SIGIR 2016<address><addrLine>Pisa, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016-07-17">2016. July 17-21, 2016</date>
			<biblScope unit="page" from="185" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning to Diversify Search Results via Subtopic Attention</title>
		<author>
			<persName><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Yue</surname></persName>
		</author>
		<idno type="DOI">10.1145/3077136.3080805</idno>
		<ptr target="https://doi.org/10.1145/3077136.3080805" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
				<editor>
			<persName><forename type="first">Noriko</forename><surname>Kando</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tetsuya</forename><surname>Sakai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Hideo</forename><surname>Joho</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Hang</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Arjen</forename><forename type="middle">P</forename><surname>De Vries</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ryen</forename><forename type="middle">W</forename><surname>White</surname></persName>
		</editor>
		<meeting>the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Shinjuku, Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017-08-07">2017. August 7-11, 2017</date>
			<biblScope unit="page" from="545" to="554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">UnifiedQA: Crossing Format Boundaries With a Single QA System</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/2020.findings-emnlp.171/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, EMNLP 2020</title>
				<editor>
			<persName><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yulan</forename><surname>He</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</editor>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, EMNLP 2020</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11-20">2020. Online Event, 16-20 November 2020</date>
			<biblScope unit="page" from="1896" to="1907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SJU4ayYgl" />
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
				<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24">2017. 2017. April 24-26, 2017</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Authoritative Sources in a Hyperlinked Environment</title>
		<author>
			<persName><forename type="first">Jon</forename><forename type="middle">M</forename><surname>Kleinberg</surname></persName>
		</author>
		<idno type="DOI">10.1145/324133.324140</idno>
		<ptr target="https://doi.org/10.1145/324133.324140" />
	</analytic>
	<monogr>
		<title level="j">J. ACM</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="604" to="632" />
			<date type="published" when="1999">1999. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning Better Representations for Neural Information Retrieval with Graph Information</title>
		<author>
			<persName><forename type="first">Xiangsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxin</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoping</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.1145/3340531.3411957</idno>
		<ptr target="https://doi.org/10.1145/3340531.3411957" />
	</analytic>
	<monogr>
		<title level="m">CIKM &apos;20: The 29th ACM International Conference on Information and Knowledge Management, Virtual Event</title>
				<editor>
			<persName><forename type="first">Stefan</forename><surname>Mathieu D'aquin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Claudia</forename><surname>Dietze</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Edward</forename><surname>Hauff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Philippe</forename><surname>Curry</surname></persName>
		</editor>
		<editor>
			<persName><surname>Cudré-Mauroux</surname></persName>
		</editor>
		<meeting><address><addrLine>Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020-10-19">2020. October 19-23, 2020</date>
			<biblScope unit="page" from="795" to="804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">DVGAN: A Minimax Game for Search Result Diversification Combining Explicit and Implicit Features</title>
		<author>
			<persName><forename type="first">Jiongnan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Jie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuqi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<idno type="DOI">10.1145/3397271.3401084</idno>
		<ptr target="https://doi.org/10.1145/3397271.3401084" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020, Virtual Event</title>
				<editor>
			<persName><forename type="first">Jimmy</forename><surname>Huang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yi</forename><surname>Chang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jaap</forename><surname>Kamps</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Vanessa</forename><surname>Murdock</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yiqun</forename><surname>Liu</surname></persName>
		</editor>
		<meeting>the 43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020, Virtual Event<address><addrLine>China</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020-07-25">2020. July 25-30, 2020</date>
			<biblScope unit="page" from="479" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multi-Task Deep Neural Networks for Natural Language Understanding</title>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p19-1441</idno>
		<ptr target="https://doi.org/10.18653/v1/p19-1441" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019</title>
				<editor>
			<persName><forename type="first">Long</forename><surname>Papers</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">David</forename><forename type="middle">R</forename><surname>Traum</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Lluís</forename><surname>Màrquez</surname></persName>
		</editor>
		<meeting>the 57th Conference of the Association for Computational Linguistics, ACL 2019<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-07-28">2019. July 28-August 2, 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4487" to="4496" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Jointly Multiple Events Extraction via Attention-based Graph Information Aggregation</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhunchen</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heyan</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d18-1156</idno>
		<ptr target="https://doi.org/10.18653/v1/d18-1156" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<editor>
			<persName><forename type="first">Ellen</forename><surname>Riloff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">David</forename><surname>Chiang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jun'ichi</forename><surname>Tsujii</surname></persName>
		</editor>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-10-31">2018. October 31 -November 4, 2018</date>
			<biblScope unit="page" from="1247" to="1256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Decoupled Weight Decay Regularization</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno>ICLR 2019</idno>
		<ptr target="https://openreview.net/forum?id=Bkg6RiCqY7" />
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
				<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06">2019. May 6-9, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Improving Contextual Language Models for Response Retrieval in Multi-Turn Conversation</title>
		<author>
			<persName><forename type="first">Junyu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiancong</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yazhou</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zenglin</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3397271.3401255</idno>
		<ptr target="https://doi.org/10.1145/3397271.3401255" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020, Virtual Event</title>
				<editor>
			<persName><forename type="first">Jimmy</forename><surname>Huang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yi</forename><surname>Chang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jaap</forename><surname>Kamps</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Vanessa</forename><surname>Murdock</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yiqun</forename><surname>Liu</surname></persName>
		</editor>
		<meeting>the 43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020, Virtual Event<address><addrLine>China</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020-07-25">2020. July 25-30, 2020</date>
			<biblScope unit="page" from="1805" to="1808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Scattering GCN: Overcoming Oversmoothness in Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Yimeng</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederik</forename><surname>Wenkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><surname>Wolf</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/hash/a6b964c0bb675116a15ef1325b01ff45-Abstract.html" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Marc</forename><forename type="middle">'</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">Aurelio</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Maria-Florina</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Hsuan-Tien</forename><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2020-12-06">2020. 2020. 2020. December 6-12, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Graph Convolutional Networks With Argument-Aware Pooling for Event Detection</title>
		<author>
			<persName><forename type="first">Huu</forename><surname>Thien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><surname>Grishman</surname></persName>
		</author>
		<ptr target="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16329" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)</title>
				<editor>
			<persName><forename type="first">Sheila</forename><forename type="middle">A</forename><surname>Mcilraith</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018-02-02">2018. February 2-7, 2018</date>
			<biblScope unit="page" from="5900" to="5907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">The PageRank citation ranking: Bringing order to the web</title>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajeev</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terry</forename><surname>Winograd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Stanford InfoLab</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Diversifying Search Results using Self-Attention Network</title>
		<author>
			<persName><forename type="first">Xubo</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<idno type="DOI">10.1145/3340531.3411914</idno>
		<ptr target="https://doi.org/10.1145/3340531.3411914" />
	</analytic>
	<monogr>
		<title level="m">CIKM &apos;20: The 29th ACM International Conference on Information and Knowledge Management, Virtual Event, Ireland, October 19-23</title>
				<editor>
			<persName><forename type="first">Stefan</forename><surname>Mathieu D'aquin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Claudia</forename><surname>Dietze</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Edward</forename><surname>Hauff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Philippe</forename><surname>Curry</surname></persName>
		</editor>
		<editor>
			<persName><surname>Cudré-Mauroux</surname></persName>
		</editor>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="1265" to="1274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks</title>
		<author>
			<persName><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1410</idno>
		<ptr target="https://doi.org/10.18653/v1/D19-1410" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019</title>
				<editor>
			<persName><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</editor>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11-03">2019. November 3-7, 2019</date>
			<biblScope unit="page" from="3980" to="3990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Explicit web search result diversification</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">T</forename><surname>Rodrygo</surname></persName>
		</author>
		<author>
			<persName><surname>Santos</surname></persName>
		</author>
		<idno type="DOI">10.1145/2492189.2492205</idno>
		<ptr target="https://doi.org/10.1145/2492189.2492205" />
	</analytic>
	<monogr>
		<title level="j">SIGIR Forum</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="67" to="68" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Exploiting query reformulations for web search result diversification</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">T</forename><surname>Rodrygo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Craig</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iadh</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName><surname>Ounis</surname></persName>
		</author>
		<idno type="DOI">10.1145/1772690.1772780</idno>
		<ptr target="https://doi.org/10.1145/1772690.1772780" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on World Wide Web, WWW 2010</title>
				<editor>
			<persName><forename type="first">Michael</forename><surname>Rappa</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Paul</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Juliana</forename><surname>Freire</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Soumen</forename><surname>Chakrabarti</surname></persName>
		</editor>
		<meeting>the 19th International Conference on World Wide Web, WWW 2010<address><addrLine>Raleigh, North Carolina, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010-04-26">2010. April 26-30, 2010</date>
			<biblScope unit="page" from="881" to="890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">E-commerce product search: personalization, diversification, and beyond</title>
		<author>
			<persName><forename type="first">Atish</forename><surname>Das Sarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nish</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neel</forename><surname>Sundaresan</surname></persName>
		</author>
		<idno type="DOI">10.1145/2567948.2577272</idno>
		<ptr target="https://doi.org/10.1145/2567948.2577272" />
	</analytic>
	<monogr>
		<title level="m">23rd International World Wide Web Conference, WWW &apos;14</title>
				<editor>
			<persName><forename type="first">Andrei</forename><forename type="middle">Z</forename><surname>Chung</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kyuseok</forename><surname>Broder</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Torsten</forename><surname>Shim</surname></persName>
		</editor>
		<editor>
			<persName><surname>Suel</surname></persName>
		</editor>
		<meeting><address><addrLine>Seoul, Republic of Korea</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014-04-07">2014. April 7-11, 2014</date>
			<biblScope unit="page" from="189" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A Graph-to-Sequence Model for AMR-to-Text Generation</title>
		<author>
			<persName><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1150</idno>
		<ptr target="https://doi.org/10.18653/v1/P18-1150" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018</title>
				<editor>
			<persName><forename type="first">Long</forename><surname>Papers</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
		</editor>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-07-15">2018. July 15-20. 2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1616" to="1626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<ptr target="http://arxiv.org/abs/1710.10903" />
		<title level="m">Graph Attention Networks. CoRR abs/1710</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page">10903</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-Art Natural Language Processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.6</idno>
		<ptr target="https://doi.org/10.18653/v1/2020.emnlp-demos.6" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, EMNLP 2020 -Demos</title>
				<editor>
			<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">David</forename><surname>Schlangen</surname></persName>
		</editor>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, EMNLP 2020 -Demos</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11-16">2020. November 16-20, 2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Listwise approach to learning to rank: theory and algorithm</title>
		<author>
			<persName><forename type="first">Fen</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wensheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1145/1390156.1390306</idno>
		<ptr target="https://doi.org/10.1145/1390156.1390306" />
	</analytic>
	<monogr>
		<title level="m">Machine Learning, Proceedings of the Twenty-Fifth International Conference (ICML 2008)</title>
		<title level="s">ACM International Conference Proceeding Series</title>
		<editor>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sam</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</editor>
		<meeting><address><addrLine>Helsinki, Finland</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008-06-05">2008. June 5-9, 2008</date>
			<biblScope unit="volume">307</biblScope>
			<biblScope unit="page" from="1192" to="1199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning Maximal Marginal Relevance Model via Directly Optimizing Diversity Evaluation Measures</title>
		<author>
			<persName><forename type="first">Long</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="DOI">10.1145/2766462.2767710</idno>
		<ptr target="https://doi.org/10.1145/2766462.2767710" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
				<editor>
			<persName><forename type="first">Ricardo</forename><surname>Baeza-Yates</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mounia</forename><surname>Lalmas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Alistair</forename><surname>Moffat</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Berthier</forename><forename type="middle">A</forename><surname>Ribeiro-Neto</surname></persName>
		</editor>
		<meeting>the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015-08-09">2015. August 9-13, 2015</date>
			<biblScope unit="page" from="113" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Modeling Document Novelty with Neural Tensor Network for Search Result Diversification</title>
		<author>
			<persName><forename type="first">Long</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="DOI">10.1145/2911451.2911498</idno>
		<ptr target="https://doi.org/10.1145/2911451.2911498" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval, SIGIR 2016</title>
				<editor>
			<persName><forename type="first">Fabrizio</forename><surname>Perego</surname></persName>
		</editor>
		<editor>
			<persName><surname>Sebastiani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Javed</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ian</forename><surname>Aslam</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Justin</forename><surname>Ruthven</surname></persName>
		</editor>
		<editor>
			<persName><surname>Zobel</surname></persName>
		</editor>
		<meeting>the 39th International ACM SIGIR conference on Research and Development in Information Retrieval, SIGIR 2016<address><addrLine>Pisa, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016-07-17">2016. July 17-21, 2016</date>
			<biblScope unit="page" from="395" to="404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">How Powerful are Graph Neural Networks?</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno>ICLR 2019</idno>
		<ptr target="https://openreview.net/forum?id=ryGs6iA5Km" />
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
				<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06">2019. May 6-9, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">What Can Neural Networks Reason About?</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingling</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mozhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rJxbJeHFPS" />
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
				<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26">2020. April 26-30, 2020</date>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation based diversified retrieval for e-commerce search</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunil</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duangmanee</forename><surname>Putthividhya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weng-Keen</forename><surname>Wong</surname></persName>
		</author>
		<idno type="DOI">10.1145/2556195.2556215</idno>
		<ptr target="https://doi.org/10.1145/2556195.2556215" />
	</analytic>
	<monogr>
		<title level="m">Seventh ACM International Conference on Web Search and Data Mining</title>
				<editor>
			<persName><forename type="first">Ben</forename><surname>Carterette</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Fernando</forename><surname>Diaz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Carlos</forename><surname>Castillo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</editor>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014-02-24">2014. 2014. February 24-28, 2014</date>
			<biblScope unit="page" from="463" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Predicting diverse subsets using structural SVMs</title>
		<author>
			<persName><forename type="first">Yisong</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
		<idno type="DOI">10.1145/1390156.1390310</idno>
		<ptr target="https://doi.org/10.1145/1390156.1390310" />
	</analytic>
	<monogr>
		<title level="m">Machine Learning, Proceedings of the Twenty-Fifth International Conference (ICML 2008)</title>
		<title level="s">ACM International Conference Proceeding Series</title>
		<editor>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sam</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</editor>
		<meeting><address><addrLine>Helsinki, Finland</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008-06-05">2008. June 5-9, 2008</date>
			<biblScope unit="volume">307</biblScope>
			<biblScope unit="page" from="1224" to="1231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Conversation Modeling on Reddit Using a Graph-Structured LSTM</title>
		<author>
			<persName><forename type="first">Victoria</forename><surname>Zayats</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<ptr target="https://transacl.org/ojs/index.php/tacl/article/view/1083" />
	</analytic>
	<monogr>
		<title level="j">Trans. Assoc. Comput. Linguistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="121" to="132" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Beyond independent relevance: methods and evaluation metrics for subtopic retrieval</title>
		<author>
			<persName><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<idno type="DOI">10.1145/860435.860440</idno>
		<ptr target="https://doi.org/10.1145/860435.860440" />
	</analytic>
	<monogr>
		<title level="m">SIGIR 2003: Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
				<editor>
			<persName><forename type="first">David</forename><surname>Hawking</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Alan</forename><forename type="middle">F</forename><surname>Smeaton</surname></persName>
		</editor>
		<meeting><address><addrLine>Toronto, Canada, Charles L. A. Clarke, Gordon V. Cormack, Jamie Callan</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003-07-28">2003. July 28 -August 1, 2003</date>
			<biblScope unit="page" from="10" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Improving Web Search Results Using Affinity Graph</title>
		<author>
			<persName><forename type="first">Benyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wensi</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiguo</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.1145/1076034.1076120</idno>
		<ptr target="https://doi.org/10.1145/1076034.1076120" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
				<meeting>the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Salvador, Brazil; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="504" to="511" />
		</imprint>
	</monogr>
	<note>SIGIR &apos;05)</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Neural IR Meets Graph Embedding: A Ranking Model for Product Search</title>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3308558.3313468</idno>
		<ptr target="https://doi.org/10.1145/3308558.3313468" />
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
				<editor>
			<persName><forename type="first">Ling</forename><surname>Liu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ryen</forename><forename type="middle">W</forename><surname>White</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Amin</forename><surname>Mantrach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Fabrizio</forename><surname>Silvestri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Julian</forename><forename type="middle">J</forename><surname>Mcauley</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ricardo</forename><surname>Baeza-Yates</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Leila</forename><surname>Zia</surname></persName>
		</editor>
		<meeting><address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019-05-13">2019. 2019. May 13-17, 2019</date>
			<biblScope unit="page" from="2390" to="2400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning for search result diversification</title>
		<author>
			<persName><forename type="first">Yadong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuzi</forename><surname>Niu</surname></persName>
		</author>
		<idno type="DOI">10.1145/2600428.2609634</idno>
		<ptr target="https://doi.org/10.1145/2600428.2609634" />
	</analytic>
	<monogr>
		<title level="m">The 37th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;14</title>
				<editor>
			<persName><forename type="first">Shlomo</forename><surname>Geva</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Andrew</forename><surname>Trotman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Peter</forename><surname>Bruza</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Charles</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kalervo</forename><surname>Clarke</surname></persName>
		</editor>
		<editor>
			<persName><surname>Järvelin</surname></persName>
		</editor>
		<meeting><address><addrLine>Gold Coast , QLD, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014-07-06">2014. July 06 -11, 2014</date>
			<biblScope unit="page" from="293" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Neural Sentence Ordering Based on Constraint Graphs</title>
		<author>
			<persName><forename type="first">Yutao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengchao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Dou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11178</idno>
		<ptr target="https://arxiv.org/abs/2101.11178" />
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
