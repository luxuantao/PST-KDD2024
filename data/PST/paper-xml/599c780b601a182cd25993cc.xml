<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Switching Delayed PSO Optimized Extreme Learning Machine for Short-Term Load Forecasting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-01-20">20 January 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Nianyin</forename><surname>Zeng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hong</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Weibo</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jinling</forename><surname>Liang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Fuad</forename><forename type="middle">E</forename><surname>Alsaadi</surname></persName>
						</author>
						<title level="a" type="main">A Switching Delayed PSO Optimized Extreme Learning Machine for Short-Term Load Forecasting</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-01-20">20 January 2017</date>
						</imprint>
					</monogr>
					<idno type="MD5">F09E7803D8A0F18F41C83F55251A2A13</idno>
					<idno type="DOI">10.1016/j.neucom.2017.01.090</idno>
					<note type="submission">Received date: 7 October 2016</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T05:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, a hybrid learning approach, which combines the extreme learning machine (ELM) with a new switching delayed PSO (SDPSO) algorithm, is proposed for the problem of the short-term load forecasting (STLF). In particular, the input weights and biases of ELM are optimized by a new developed SDPSO algorithm, where the delayed information of locally best particle and globally best particle are exploited to update the velocity of particle. By testing the proposed SDPSO-ELM in a comprehensive manner on a tanh function, this approach obtain better generalization performance and can also avoid adding unnecessary hidden nodes and overtraining problems. Moreover, it has shown outstanding performance than other state-of-the-art ELMs. Finally, the proposed SDPSO-ELM algorithm is successfully applied to the STLF of power system. Experiment results demonstrate that the proposed learning algorithm can get better forecasting results in comparison with the radial basis function neural network (RBFNN) algorithm.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T I. INTRODUCTION</head><p>Load forecasting, which aims to predict the future load demand with satisfactory accuracy, plays an importance role in the generation scheduling, system reliability and power optimization and economical running of the smart grid. Based on the prediction time, load forecasting can be divided into three types, which are short-term, medium-term and long-term forecasting <ref type="bibr" target="#b26">[27]</ref>. In this paper, we focus on the problem of the short-term load forecasting (STLF), which generally refers to the period of its prediction from one hour to one week.</p><p>In the past few years, many efforts have been made, and quite a considerable number of methods have been proposed for STLF. Especially, recent research has been going mainly toward two categories: one is the statistical methods, see, e.g., <ref type="bibr">[1]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b20">[21]</ref>, and the other is artificial intelligence methods, see, e.g., <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b26">[27]</ref>. Among various forecasting approaches, artificial neural networks (ANNs) have become the popular ones for the STLF due primarily to its attractive properties such as strong ability</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>in pattern recognition, fault tolerance and distributed associative memory <ref type="bibr" target="#b26">[27]</ref>. Up to date, a variety of ANN-based algorithms have been under especial intensive investigations for the STLF problem <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b26">[27]</ref>. However, studies have shown that those ANN-based algorithms which are trained by gradient-based methods such as back propagation (BP) or its variants have some limitations when applied to a variety of complex fields like power load forecasting <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b26">[27]</ref>.</p><p>Recently, a powerful learning algorithm called extreme learning machine (ELM) was proposed by Huang in 2004 <ref type="bibr" target="#b3">[4]</ref> for single-hidden-layer feedforward neural networks (SLFNs). The ELM, which can overcome some limitations faced by gradient-based methods, has been gaining particular research attention because of fast learning speed and competitive generalization performance when applied to real-word problems <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>. With rapid development of smart grid, large volumes of data can be acquired and applied to the STLF of power system using ELMs. Notably, unnecessary hidden neurons may be added and ill-condition problems may occur when the input weights and hidden biases are selected randomly in the ELM <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b26">[27]</ref>. Hence, with maintained performance, the evolutionary algorithm, particularly the particle swarm optimization (PSO) algorithm is chosen to select the optimum input weights and hidden biases of the ELM.</p><p>Inspired by the swarm behaviours of birds flocking or fish schooling <ref type="bibr" target="#b7">[8]</ref>, PSO is a swarm intelligencebased optimization algorithm which has been widely used in many complex optimization problems because of easily implementation. However, the basic PSO algorithm suffers from certain shortcomings such as easily trapped in the local optimum and appeared premature convergence <ref type="bibr" target="#b19">[20]</ref>. Hence, a great number of strategies have been introduced into the basic PSO algorithm to improve its performance, see e.g. <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b27">[28]</ref>- <ref type="bibr" target="#b29">[30]</ref>. In particular, a novel switching delayed PSO (SDPSO) algorithm, which can adjust the model according to an evolutionary factor and a Markov chain has been recently introduced and analyzed in <ref type="bibr" target="#b29">[30]</ref>, where the delayed information of locally best particle and globally best particle are utilized to update the velocity of particle. It has been proven that the SDPSO can not only enhance the ability of global searching but also improve the possibility of eventually reaching the global optimum solution.</p><p>Based on the above discussions, a hybrid learning approach, which combines the ELM with a new SDPSO algorithm, is proposed for the STLF of power system in this paper. Especially, the SDPSO algorithm is exploited to optimize the hidden nodes parameters (including the input weights and biases) of ELM. Through comprehensive analysis of the proposed method, the results demonstrate that the performance of the SDPSO-ELM algorithm is superior over other competitive learning algorithms. The main contributions of this paper can be summarized as follows. (1) A hybrid learning algorithm called SDPSO-ELM is proposed for the problem of STLF. The hidden nodes parameters of ELM algorithm are optimized by the new developed SDPSO algorithm. Hence, the SDPSO-ELM can avoid adding unnecessary hidden nodes and overtraining problems, and also improve the forecasting accuracy. <ref type="bibr" target="#b1">(2)</ref> The SDPSO-ELM is tested in a comprehensive and systematic manner on a tanch function and its performance outperforms other state-of-the-art ELMs significantly in terms of generalization performance. <ref type="bibr" target="#b2">(3)</ref> The novel SDPSO-ELM algorithm is successfully applied to the STLF of power system. The results demonstrate that the proposed method can get better forecasting results in comparison with the competitive algorithms.</p><p>The rest of this paper is organized as follows. In section II, we presents a detailed introduction on the extreme learning machine and the new developed switching delayed particle swarm optimization algorithm. A hybrid learning method called SPSO-ELM is proposed in Section III. In Section IV, we successfully implement the proposed SDPSO-ELM approach to the STLF for electric power system and a series of experiments are carried out to demonstrate the effectiveness of the approach. Finally, conclusions are summarized in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PRELIMINARIES</head><p>In this section, we mainly introduce the extreme learning machine (ELM) and the new developed switching delayed particle swarm optimization (SDPSO) algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Extreme Learning Machine</head><p>The single-hidden-layer feedforward neural networks (SLFNs), as shown in Fig. <ref type="figure" target="#fig_1">1</ref>, is a three-layer neural network with the ability of approximating complex nonlinearity of the data, which has been extensively investigated and widely applied in the past few decades <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b25">[26]</ref>. Employed in generalized SLFNs, ELM, a novel learning algorithm with highly competitive performance has attracted broad attentions <ref type="bibr" target="#b6">[7]</ref>.</p><p>...  For a SLFN (d input nodes, L hidden nodes and m output nodes), the output function of the SLFN can be denoted as <ref type="bibr" target="#b25">[26]</ref>:</p><formula xml:id="formula_0">f L (x) = L i=1 β i g i (x) = L i=1 β i G(a i , b i , x), x ∈ R d , β i ∈ R m (1)</formula><p>where g i and G(a i , b i , x) represent the activation function of ith hidden node. (a i , b i ) is the hidden layer parameters which connect the hidden neurons and input neurons, and β i is the output weight which connects the hidden neurons and output neurons. For additive hidden nodes, the activation function g i can be described as:</p><formula xml:id="formula_1">g i = G(a i , b i , x) = g(a i x + b i ), a i ∈ R d , b i ∈ R<label>(2)</label></formula><p>And for RBF hidden nodes, the activation function g i can be described as:</p><formula xml:id="formula_2">g i = G(a i , b i , x) = g(b i x -a i ), a i ∈ R d , b i ∈ R +<label>(3)</label></formula><p>Suppose that the training set consists of N samples {(x i , t i )} N i=1 , where x i = [x i1 , x i2 , ..., x id ] T ∈ R d and t i = [t i1 , t i2 , ..., t im ] T ∈ R m , theoretically, the SLFNs can approximate these N samples with zero error, which can be represented as:</p><formula xml:id="formula_3">N j=1 y j -t j = 0<label>(4)</label></formula><p>where y denotes the actual output of the SLFN. Therefore, there exist parameters (a i , b i ) and β i satisfying that:</p><formula xml:id="formula_4">L i=1 β i G(a i , b i , x j ) = t j , j = 1, 2, ..., N<label>(5)</label></formula><p>The matrix form of the above N equations is briefly represented by:</p><formula xml:id="formula_5">Hβ = T<label>(6)</label></formula><p>where</p><formula xml:id="formula_6">H =      h(x 1 ) h(x 2 ) . . . h(x N )      =      G(a 1 , b 1 , x 1 ) • • • G(a L , b L , x 1 ) G(a 1 , b 1 , x 2 ) • • • G(a L , b L , x 2 ) . . . G(a 1 , b 1 , x N ) • • • G(a L , b L , x N )      N ×L<label>(7)</label></formula><formula xml:id="formula_7">β =      β T 1 β T 2 . . . β T L      L×m and T =      t T 1 t T 2 . . . t T N      N ×m<label>(8)</label></formula><p>Unlike the traditional learning algorithms, the highlight of ELM is that the hidden layer parameters (a i , b i ) are generated randomly and kept fixed instead of tuned by iteration. In ELM, the most important step is to figure out the output wights β i , which can be solved by minimizing the training error as well as the norm of the output weights. One thing to note here is that the generalization performance of SLFNs tends to get better if the training error and norms of weights are smaller. The objective function can be described as:</p><formula xml:id="formula_8">F = min β Hβ -T 2 + λ β 2<label>(9)</label></formula><p>The method of smallest norm least-squares is used to solve the above problem:</p><formula xml:id="formula_9">β = H † T<label>(10)</label></formula><p>where H † represents the Moore-Penrose generalized inverse of matrix H.</p><p>Here, various of approaches can be adopted to compute H † , such as the orthogonal projection approach, orthogonalization approach, the iterative approach, and the singular value decomposition approach (SVD). Especially, the orthogonal projection method can be efficiently used in two cases:</p><formula xml:id="formula_10">H † = (H T H) -1 H T when H T H is nonsingular, or H † = H T (HH T ) -1</formula><p>when HH T is nonsingular. Based on the ridge regression theory, the stability and generalization performance of SLFNs can be improved by adding a positive value to the diagonal of H T H or HH T . In this way, the output wights β can be calculated by:</p><formula xml:id="formula_11">β = ( I λ + H T H) -1 H T T if H T H is nonsingular<label>(11)</label></formula><p>or</p><formula xml:id="formula_12">β = H T ( I λ + HH T ) -1 T if HH T is nonsingular (12) A C C E P T E D M A N U S C R I P T B.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Switching Delayed Particle Swarm Optimization Algorithm</head><p>PSO, first proposed by Kennedy and Eberhart in 1995 <ref type="bibr" target="#b7">[8]</ref>, is a global optimization algorithm motivated by the group behaviors in nature such as fish schooling or birds flocking, etc. The basic PSO and its variants have been successfully applied to various real-word applications due to its effectiveness in performing difficult optimization tasks and its convenience for implementation with fast convergence to a reasonably good solution <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b29">[30]</ref>.</p><p>Recently, a novel switching delayed PSO (SDPSO) algorithm is developed and analyzed in <ref type="bibr" target="#b29">[30]</ref>, in which the update formula of the velocity adaptively changes based on an evolutionary factor and the Markov chain. Furthermore, the delayed information of pbest and gbest are employed to the update formula of the velocity in accordance with different evolutionary states. The SDPSO algorithm can effectively overcome the problem of local optimum and premature convergence, and it is especially proficient in handling with the high-dimensional and multi-modal problems.</p><p>The velocity and position equations of the new developed SDPSO algorithm are given as follows:</p><formula xml:id="formula_13">v i,j (k + 1) = w(k)v i,j (k) + c 1 (ξ(k))r 1 (p i,j (k -τ 1 (ξ(k))) -x i,j +c 2 (ξ(k))r 2 (p g,j (k -τ 2 (ξ(k)) -x i,j (k)), x i,j (k + 1) = x i,j (k) + v i,j (k + 1),<label>(13)</label></formula><p>where c 1 (ξ(k)) and c 2 (ξ(k)) are the acceleration coefficients, τ 1 (ξ(k)) and τ 2 (ξ(k)) represent the timedelay, respectively. Especially, these four parameters are decided by a non-homogeneous Markov chain ξ(k) (k ≥ 0), which takes values in a finite state space S= {1, 2, • • • , N } according to the probability transition matrix</p><formula xml:id="formula_14">Π (k) = (π (k) ij ) N ×N , where π (k) ij ≥ 0 (i, j ∈ S) and N j=1 π (k) ij = 1.</formula><p>The searching process towards to the global optimum can be divided into four states: convergence, exploration, exploitation and jumping out, which are denoted by ξ(k) = 1, ξ(k) = 2, ξ(k) = 3 and ξ(k) = 4, respectively <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b29">[30]</ref>. Particularly, these four states can be discriminatively defined by an evolutionary factor (EF) which can be defined as follows <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b29">[30]</ref>:</p><formula xml:id="formula_15">E f = d g -d min d max -d min<label>(14)</label></formula><p>where d g represents the globally best particle among the mean distance d i , which represents the distance between each particle and the other particles. d max and d min represent the maximum and minimum distances in d i , respectively.</p><p>According to the value of E f , the state of Markov chain can be determined by:</p><formula xml:id="formula_16">ξ(k) =            1, 0 ≤ E f &lt; 0.25, 2, 0.25 ≤ E f &lt; 0.5, 3, 0.5 ≤ E f &lt; 0.75, 4, 0.75 ≤ E f &lt; 1,<label>(15)</label></formula><p>where the probability transition matrix is obtained as follows:</p><formula xml:id="formula_17">Π =     χ 1 -χ 0 0 1-χ 2 χ 1-χ 2 0 0 1-χ 2 0 1-χ 2 0 0 1 -χ χ    <label>(16)</label></formula><p>Therefore, the Markov chain may change from one state to another or remain the same state during the iteration, which is depended on the value of probability transition matrix Π. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><formula xml:id="formula_18">(ξ(k)) τ2(ξ(k)) Convergence ξ(k) = 1 2 2 pi(k) pg(k) 0 0 Exploitation ξ(k) = 2 2.1 1.9 pi(k -τ1(ξ(k))) pg(k) k • rand1 0 Exploration ξ(k) = 3 2.2 1.8 pi(k) pg(k -τ2(ξ(k))) 0 k • rand2 Jumping-out ξ(k) = 4 1.8 2.2 pi(k -τ1(ξ(k))) pg(k -τ2(ξ(k))) k • rand1 k • rand2</formula><p>In addition, the delayed information of pbest and gbest are exploited to update the velocity equation according to the evolutionary state. The methods for selecting the acceleration coefficients (c 1 and c 2 ) and delayed information are given in the Table <ref type="table" target="#tab_0">I</ref>. Furthermore, the inertia weight w has similar tendency as the evolutionary factor E f during the iteration process, which can be expressed as follows <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b29">[30]</ref>:</p><formula xml:id="formula_19">w(E f ) = 0.5E f + 0.4 ∈ [0.4, 0.9], ∀E f ∈ [0, 1].<label>(17)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. SWITCHING DELAYED PSO OPTIMIZED EXTREME LEARNING MACHINE (SDPSO-ELM)</head><p>In ELM, the parameters of input weights and hidden biases are randomly generated instead of using the conventional tuning-based approaches, which saves much training time. Nevertheless, as the output weights are calculated on the basis of input weights and hidden biases, there inevitably exists some nonoptimal or unnecessary input weights and hidden biases. Generally, the approach of randomly selecting parameters may cause two main issues. On one hand, the ELM may need more hidden neurons than the traditional tuning-based neural networks, which can lead to a slow response to the unknown testing data. On the other hand, it tends to appear an ill-conditioned hidden output matrix H, especially in the case that a great number of hidden neurons are used, which may result in a worse generalization performance.</p><p>A condition number is introduced to qualitatively describe the condition of a matrix <ref type="bibr" target="#b24">[25]</ref>. In general, a small condition number is expected because it stands for good conditioning. On the contrary, a large one may indicate that the matrix is close to ill-conditioned. Based on the literatures, the 2-norm condition number of the matrix H can be defined by <ref type="bibr" target="#b24">[25]</ref>:</p><formula xml:id="formula_20">K 2 (H) = λ max (H T H) λ min (H T H)<label>(18)</label></formula><p>where λ max (H T H) and λ min (H T H) represent the the largest and smallest eigenvalues of the matrix H T H, respectively. In order to overcome the above mentioned limitations, a hybrid method, named SDPSO-ELM, which combines the SDPSO algorithm with an ELM is proposed in this paper. In particular, the SDPSO algorithm is used to optimize the parameters of input weights and hidden biases of ELM.</p><p>Firstly, initialize particles of the swarm and the parameters of the SDPSO. Each particle formed by a set of inputs weights and hidden biases is represented by θ i = [w 11 , w 12 , ..., w 1K , w 21 , w 22 , ..., w 2K , ..., w n1 , w n2 , ..., w nK , .. Secondly, calculate the corresponding output weights of particles according to the Eqn. <ref type="bibr" target="#b9">(10)</ref> and evaluate the fitness value of each particle. In order to avoid over-fitting of the SLFN, the fitness value of each particle is calculated by the root mean square error (RMSE) on the validation set rather than the whole</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>training set. The fitness function is defined as follows:</p><formula xml:id="formula_21">f () = nν j=1 K i=1 β i g(w i • x j + b i ) -t j 2 2 n ν (19)</formula><p>where n ν is the number of the validation samples. Thirdly, determine the the local best pbest for all particles and the globally best gbest for the swarm. With the mean distance value of each particle, the evolutionary factor can be figured out by the Eqn. <ref type="bibr" target="#b13">(14)</ref>, and then the state in the next generation is updated based on the current state and the probability transition matrix. Subsequently, the inertia weight is calculated by the Eqn. <ref type="bibr" target="#b16">(17)</ref>, as well as the acceleration coefficients and the pbest, gbest are selected according to the Table <ref type="table" target="#tab_0">I</ref>. Especially, neural networks are apt to have better generalization performance with smaller norm of weights <ref type="bibr" target="#b22">[23]</ref>. Hence, for the sake of further enhancing the generalization performance of SLFN, the norm of output weights as well as the RMSE are both taken into account to determine the local best pbest for all particles and the global best gbest for the swarm. The corresponding equations are denoted as follows <ref type="bibr" target="#b22">[23]</ref>:</p><formula xml:id="formula_22">p ib = p i (f (p ib ) -f (p i ) ≥ εf (p ib )) or (|f (p ib ) -f (p i )| &lt; εf (p ib ) and β p i &lt; β p ib ) p ib else (<label>20</label></formula><formula xml:id="formula_23">)</formula><formula xml:id="formula_24">p g = p i (f (p g ) -f (p i ) ≥ εf (p g )) or (|f (p g ) -f (p i )| &lt; εf (p g ) and β p i &lt; β pg ) p g else (<label>21</label></formula><formula xml:id="formula_25">)</formula><p>where f (p i ), f (p ib ) and p g are the corresponding fitness values for the ith particle, the best position of ith particle and global best position of the swarm, respectively. β p i , β p ib and β pg are the corresponding output wights obtained by Eqn. <ref type="bibr" target="#b9">(10)</ref> when the parameters of input weights and hidden biases are set as the ith particle, the best position of ith particle and global best position of the swarm, respectively. ε &gt; 0 is the tolerance rate. Fourthly, each particle updates the velocity and position according to Eqn. <ref type="bibr" target="#b12">(13)</ref>, and the next generation is produced.</p><p>Finally, the above iteration process continues until the optimal solution or the maximum iteration is reached. Therefore, the optimal parameters of input weights and hidden biases are achieved, and eventually the corresponding ELM with the optimal parameters can be utilized to the testing data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS AND DISCUSSIONS</head><p>In this section, a series of experiments are carried out to demonstrate the effectiveness of the proposed SDPSO-ELM approach. To illustrate the superiority of the SDPSO-ELM, three well-known ELM algorithms are compared with the SDPSO-ELM algorithm which consists of the IPSO-ELM algorithm <ref type="bibr" target="#b5">[6]</ref>, the E-ELM algorithm <ref type="bibr" target="#b22">[23]</ref> and the basic ELM <ref type="bibr" target="#b3">[4]</ref>. At first, the four algorithms are used to reconstruct a given nonlinear function, in order to discuss their properties in all respects. And then, we try to apply the proposed SDPSO-ELM approach to the STLF for electric power system.</p><p>The parameters of four algorithms in the experiments are set as: for ELM, the hidden nodes L = 30; for the other three algorithms, the hidden nodes L = 10, the particle number N = 20, the tolerance rate for fitness function ε = 0.1, and the maximum iteration number is set to 200. To obtain the mean results, the experiment is repeated 50 trails independently for each algorithm in order to avoid the influence of random factor. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Function Approximation</head><p>Here, the tanh function is selected for the four algorithms to approximate, which is represented as:</p><formula xml:id="formula_26">f (x) = e x -e -x e x + e -x<label>(22)</label></formula><p>The training set and testing set are formed by 1000 data, respectively, where the inputs are uniformly distributed in <ref type="bibr">[-5,5]</ref>. In addition, a large uniform noise distributed in [-0.2,0.2] is added to the training data while the testing data remain clean. The corresponding results are shown in Table <ref type="table" target="#tab_1">II</ref> and Fig. <ref type="figure" target="#fig_4">2</ref>.  As shown in Fig. <ref type="figure" target="#fig_4">2</ref>, the output of the SDPSO-ELM algorithm for "tanh" function approximation is extremely close to the expected value, which demonstrates the effectiveness of the proposed SDPSO-ELM approach.</p><p>From the Table <ref type="table" target="#tab_1">II</ref>, we firstly observe that all three improved ELMs of SDPSO-ELM, IPSO-ELM and E-ELM achieve smaller testing RMSE than the ELM, where the SDPSO-ELM achieves the best performance. Meanwhile, only 10 hidden neurons are assigned for the three improved ELMs, while 30 hidden neurons are used in ELM. Therefore, a more compact SLFN with good generalization performance can be obtained by the three improved ELMs. Moreover, the norm values of the output weights in the E-ELM, IPSO-ELM and SDPSO-ELM are much smaller than the ELM, which further confirms that these three improved ELMs enhance the SLFN generalization performance. However, the condition values of IPSO-ELM and E-ELM are not so satisfactory, for the reason that a few sharp spikes are contained in the total 50 trials. In spite of this, the SDPSO-ELM also achieves a stable and low condition value of SLFN. In addition, the standard deviation (Dev) of SDPSO-ELM and ELM are much better than the Dev of E-ELM and IPSO-ELM, possibly because of the volatility of 50 trials in E-ELM and IPSO-ELM. Note that the three improved ELMs sacrifice more times for selecting the parameters of input weights and basis. In conclusion, the proposed SDPSO-ELM algorithm outperforms the other three algorithms of IPSO-ELM, E-ELM and ELM, in terms of the RMSE, Dev, norm and condition value. Furthermore, as shown in Fig. <ref type="figure">3</ref> and Fig. <ref type="figure">4</ref>, the norm of the output weights and condition values of four ELMs are analyzed respectively. From Fig. <ref type="figure">3</ref>(a), we notice that the algorithms of SDPSO-ELM, IPSO-ELM and E-ELM achieve much lower value of the norm of the output weights than the ELM in each trial. Similarly, as observed in Fig. <ref type="figure">4</ref>(a), the condition values of SLFN obtained by the SDPSO-ELM, IPSO-ELM and ELM are much smaller and more steady than those of the E-ELM.</p><p>Especially, in order to provide a proper performance comparison among the algorithms, we delete the worst performance of four ELMs. Then, the results are shown in Fig. <ref type="figure">3</ref> we can obviously observe the improvements on the condition and the norm of the output weights of the SLFN. As Fig. <ref type="figure">3(b)</ref> shows, the curves of norm value for E-ELM and SDPSO-ELM are more stable, while the curve of IPSO-ELM contains several sharp spikes. From Fig. <ref type="figure">4</ref>(b), the SDPSO-ELM also achieves the most stable performance of condition value among the three algorithms of SDPSO-ELM, IPSO-ELM and ELM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Short-term Load Forecasting for Electric Power System</head><p>As a basic but indispensable task of power system, STLF generally predicts the load for a lead time ranging from one hour to several days, which plays an important role in the generation scheduling and reserving activities. In this section, the proposed SDPSO-ELM algorithm is applied to the short-term load forecasting of power system, where the 24 hourly loads of the forecasting day are required to make prediction.</p><p>1) Architecture: The architecture of the proposed SDPSO-ELM based STLF is shown in Fig. <ref type="figure" target="#fig_7">5</ref>. As shown, the SDPSO algorithm is introduced to optimize the parameters of input weights and hidden biases of ELM, where the norm of output weights and the RMSE on the validation set are both considered as the criterion. The combination of the SDPSO and ELM makes the selection of input weights and hidden biases in ELM more reasonable and effective, instead of the conventional randomly generating. With the optimized parameters, the SDPSO-ELM algorithm achieves stronger generalization performance as well as better condition of the SLFN. Therefore, the forecasting results with high accuracy are obtained using the SDPSO-ELM when the testing data inputs. In particular, in order to obtain more precise forecasting data, the STLF model is designed as the combination of the prediction results of 24 hours, which means the load of each hour in the day is predicted by the SDPSO-ELM, independently. For the load of hour t in the day k, the input variables are chosen according to the correlation studies and literatures, as shown in Table <ref type="table" target="#tab_1">III</ref>.</p><p>As for the performance indicator, mean absolute percentage error (MAPE) and mean absolute error (MAE) are introduced, which are computed as follows:</p><formula xml:id="formula_27">MAPE = 1 N N i=1 |Y k (i) -Ŷk (i)| Y k (i) × 100<label>(23)</label></formula><formula xml:id="formula_28">MAE = 1 N N i=1 |Y k (i) -Ŷk (i)|<label>(24)</label></formula><p>where Y k (i) and Ŷk (i) are the real and prediction load value of hour i, respectively.</p><p>2) Test results: The historical load data from January to June in 2010 of the Ningde City in Fujian Province of China are analyzed using the proposed SDPSO-ELM approach. The data from January to May are used as the training set, and then we forecast the hourly load on any one day in June. Furthermore, a comparison between the SDPSO-ELM and the state-of-the-art ANN method, radial basis function neural network (RBFNN), is conducted in order to demonstrate the performance of the proposed method. It should be pointed out that under the same circumstance, we can easily confirm the parameters of SDPSO-ELM while requiring much more time to tune the parameters of the RBFNN.</p><p>The forecasting results are shown in the Fig. <ref type="figure" target="#fig_8">6</ref> and the generalization performances are listed in the Table <ref type="table" target="#tab_0">IV</ref>.</p><p>Fig. <ref type="figure" target="#fig_8">6</ref> describes that the output of the SDPSO-ELM algorithm for STLF is obviously more approximate to the real load than that of the RBFNN, which indicates the proposed SDPSO-ELM can efficiently forecast the hourly load with remarkable accuracy.</p><p>Meanwhile, from Table <ref type="table" target="#tab_0">IV</ref>, the MAPE obtained by the SDPSO-ELM decreases 0.72% compared with the MAPE of RBFNN. It is of great significance to improve the prediction accuracy for the power system generation scheduling and reserving activities. Similarly, the SDPSO-ELM algorithm achieves better MAE as much as 7.566MW than the MAE of RBFNN.   In this paper, we have presented a hybrid learning approach for the problem of the STLF based on an improved ELM which is optimized by a new developed switching delayed PSO algorithm. That is, the input weights and biases of ELM are optimized by the SDPSO algorithm. The proposed SDPSO-ELM is firstly verified on a tanh function, which shows that the performance of the proposed method is superior to other popular ELMs. Finally, the SDPSO-ELM is successfully applied to the STLF of power system. Experiment results show that the proposed method can significantly improve the forecasting accuracy in comparison with the RBFNN algorithm. In the near feature, some latest adaptively control strategies (e.g. <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b12">[13]</ref>- <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>) will be exploited for further improving the performance of the ELM algorithm, and also some advanced computational intelligent methods (e.g. <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b30">[31]</ref>) will be applied to the problem of the STLF. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Schematic diagram of a SLFN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>., b 1 , b 2 , ..., b K ], where w ij and b j are randomly initialized from [-1,1].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>output on the testing set Actual output on the testing set</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The output of the SDPSO-ELM algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 3 .Fig. 4 .</head><label>34</label><figDesc>Fig. 3. The norm value of the output weights obtained by ELMs</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(b) and Fig. 4(b), from which A C C E P T E D M A N U S C R I P T</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Architecture of proposed SDPSO-ELM based STLF model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Forecasting results of two methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>S C R I P T</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I STRATEGIES</head><label>I</label><figDesc>FOR SELECTING c1 , c2 AND DELAYED INFORMATION</figDesc><table><row><cell>State</cell><cell>Mode</cell><cell>c1</cell><cell>c2</cell><cell>pbest</cell><cell>gbest</cell><cell>τ1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II THE</head><label>II</label><figDesc>APPROXIMATION PERFORMANCE OF FUNCTION tanh WITH FOUR ELMS.</figDesc><table><row><cell>Algorithms</cell><cell cols="5">Training RMSE Testing RMSE Testing Dev Training time Hidden nodes</cell><cell>Norm</cell><cell>Condition</cell></row><row><cell>ELM</cell><cell>0.0938</cell><cell>0.0157</cell><cell>0.0031</cell><cell>0.0031</cell><cell>30</cell><cell>3.8330e+09</cell><cell>3.6579e+16</cell></row><row><cell>E-ELM</cell><cell>0.0949</cell><cell>0.0144</cell><cell>0.0075</cell><cell>4.8541</cell><cell>10</cell><cell cols="2">1.0245e+03 4.16604e+31</cell></row><row><cell>IPSO-ELM</cell><cell>0.0944</cell><cell>0.0121</cell><cell>0.0043</cell><cell>5.5040</cell><cell>10</cell><cell cols="2">4.4097e+03 7.56568e+15</cell></row><row><cell>SDPSO-ELM</cell><cell>0.0943</cell><cell>0.0118</cell><cell>0.0029</cell><cell>8.7688</cell><cell>10</cell><cell>1.3820e+03</cell><cell>4.0069e+05</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Short-term hourly load forecasting using time-series modeling with peak load estimation capability</title>
		<author>
			<persName><forename type="first">N</forename><surname>Amjady</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Power Systems</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="498" to="505" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">MalikShort term load forecast using fuzzy logic and wavelet transform integrated generalized neural network</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chaturvedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Electrical Power and Energy Systems</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="230" to="237" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Short-term load forecasting via ARMA model identification including non-Gaussian process considerations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Power Systems</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="673" to="679" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Extreme learning machine: a new learning scheme of feedforward neural networks</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Siew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="985" to="990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Universal approximation using incremental constructive feedforward networks with random hidden nodes</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Siew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="879" to="892" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An improved evolutionary extreme learning machine based on particle swarm optimization</title>
		<author>
			<persName><forename type="first">F</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page" from="87" to="93" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An Insight into Extreme learning machines: random neurons, random features and kernels</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Computation</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="376" to="390" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Particle swarm optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Eberhart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference On Neural Network</title>
		<meeting>IEEE International Conference On Neural Network</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="1942" to="1948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A novel wavelet-based ensemble method for short-term load forecasting with hybrid neural networks and feature selection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Goel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Power Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1788" to="1798" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Extended Kalman filtering for stochastic nonlinear systems with randomly occurring cyber attacks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">207</biblScope>
			<biblScope unit="page" from="708" to="716" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Exponential stability of Markovian jumping Cohen-Grossberg neural networks with mixed mode-dependent time-delays</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Obaid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">A</forename><surname>Abbas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">177</biblScope>
			<biblScope unit="page" from="409" to="415" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Weighted average consensus-based unscented Kalman filtering</title>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="558" to="567" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Error-constrained reliable tracking control for discrete time-varying systems subject to quantization effects</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">174</biblScope>
			<biblScope unit="page" from="897" to="905" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Event-triggered H∞ state estimation for discrete-time stochastic genetic regulatory networks with Markovian jumping parameters and time-varying delays</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">E</forename><surname>Alsaadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">174</biblScope>
			<biblScope unit="page" from="912" to="920" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A new framework for output feedback controller design for a class of discrete-time stochastic nonlinear system with quantization and missing measurement</title>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">E</forename><surname>Alsaadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of General Systems</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="517" to="531" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improving short term load forecast accuracy via combining sister forecasts</title>
		<author>
			<persName><forename type="first">J</forename><surname>Nowotarski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Energy</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page" from="40" to="49" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">One-hour-ahead load forecasting using neural network</title>
		<author>
			<persName><forename type="first">T</forename><surname>Senjyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Takara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Uezato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Funabashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Power Systems</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="113" to="118" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sales forecasting using extreme learning machine with applications in fashion retailing</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Au</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Decision Support Systems</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="411" to="419" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unknown input and state estimation for linear discrete-time systems with missing measurements and correlated noises</title>
		<author>
			<persName><forename type="first">H</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of General Systems</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="648" to="661" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Parameters identification of unknown delayed genetic regulatory networks by a switching particle swarm optimization algorithm</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="2523" to="2535" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hybrid demand model for load estimation and short-term load forecasting in distribution electrical systems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Villalba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Power Delivery</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="764" to="769" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A reduced-order approach to filtering for systems with linear equality constraints</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">193</biblScope>
			<biblScope unit="page" from="219" to="226" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Evolutionary extreme learning machine</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Suganthan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1759" to="1763" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adaptive particle swarm optimization</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on System, Man and Cybernetics-B</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1362" to="1381" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">On improving the conditioning of extreme learning machine: a linear aase</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Man</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">th International Conference on Information, Communications and Signal Processing</title>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Extreme learning machines: a survey</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Machine Learning and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="107" to="122" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Short-term load forecasting of Australian National Electricity Market by an ensemble model of extreme learning machine</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IET Generation, Transmission &amp; Distribution</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="391" to="397" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Image-based quantitative analysis of gold immunochromatographic strip via cellular neural network approach</title>
		<author>
			<persName><forename type="first">N</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zineddin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1129" to="1136" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Inferring nonlinear lateral flow immunoassay state-space models via an unscented Kalman filter</title>
		<author>
			<persName><forename type="first">N</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SCIENCE CHINA Information Sciences</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">112204</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A novel switching delayed PSO algorithm for estimating unknown parameters of lateral flow immunoassay</title>
		<author>
			<persName><forename type="first">N</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Alsaadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Computation</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="143" to="152" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep belief networks for quantitative analysis of gold immunochromatographic strip</title>
		<author>
			<persName><forename type="first">N</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Alsaadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Computation</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="684" to="692" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Event-based state estimation for a class of complex networks with time-varying delays: a comparison principle approach</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">E</forename><surname>Alsaadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physics Letters A</title>
		<imprint>
			<biblScope unit="volume">381</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="10" to="18" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Passivity analysis for discrete-time neural networks with mixed time-delays and randomly occurring quantization effects</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">216</biblScope>
			<biblScope unit="page" from="657" to="665" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">he was a RA in the Department of Electrical and Electronic Engineering, the University of Hong Kong. Currently, he is an Assistant Professor with the Department of Instrumental &amp; Electrical Engineering of Xiamen University. His current research interests include intelligent data analysis, computational intelligent, time-series modeling and applications. He is the author or co-author of several technical papers and also a very active reviewer for many international journals and conferences</title>
	</analytic>
	<monogr>
		<title level="m">Nianyin Zeng was born in Fujian Province</title>
		<meeting><address><addrLine>China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1986-10">1986. October 2012 to March 2013</date>
		</imprint>
	</monogr>
	<note>He received the B.Eng. degree in electrical engineering and automation in 2008 and the Ph. D. degree in electrical engineering in 2013, both from Fuzhou University</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Journal of Advances in Biomedical Engineering and Technology, and Smart Healthcare. Hong Zhang received her Bachelor&apos;s degree in electrical engineering and automation from the Department of Mechanical &amp; Electrical Engineering</title>
		<author>
			<persName><surname>Dr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<pubPlace>Xiamen, China; Xiamen, China</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Xiamen University ; Electrical Testing Technology and Instruments at Xiamen University</orgName>
		</respStmt>
	</monogr>
	<note>She is currently pursuing the Master&apos;s degree in. Her research interests include image processing and deep learning techniques</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">She is currently a Professor in the Department of Mathematics, Southeast University. Prof. Liang has published around 80 papers in refereed international journals. She serves as an associate editor for several international journals such as</title>
	</analytic>
	<monogr>
		<title level="m">Jinling Liang received the B.Sc. and M.Sc. degrees in mathematics from Northwest University, Xian, China, in 1997 and 1999, respectively, and the Ph.D. degree in applied mathematics from Southeast University</title>
		<meeting><address><addrLine>Liverpool, UK; Nanjing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Weibo Liu received his B. S</publisher>
			<date type="published" when="2006">2015. 2006</date>
		</imprint>
		<respStmt>
			<orgName>Department of Electrical Engineering &amp; Electronics, University of Liverpool</orgName>
		</respStmt>
	</monogr>
	<note>Her current research interests include stochastic systems, complex networks, robust filtering and bioinformatics</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">He published widely in the top IEEE communications conferences and journals and has received the Carter award, University of Leeds for the best PhD. He has research interests in optical systems and networks, signal processing, synchronization and systems design</title>
		<author>
			<persName><forename type="first">Fuad</forename><forename type="middle">E</forename></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sc</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996. 2002. 2011. 1996 and 2005</date>
			<pubPlace>Jeddah, Saudi Arabia; Leeds, UK; Jeddah, Saudi Arabia</pubPlace>
		</imprint>
		<respStmt>
			<orgName>King AbdulAziz University ; King Abdulaziz University</orgName>
		</respStmt>
	</monogr>
	<note>Jeddah as a communication instructor in the College of Electronics and Communication. He is currently an associate professor of the Electrical and Computer Engineering Department within the Faculty of Engineering</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
