<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Predicting Personalized Image Emotion Perceptions in Social Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sicheng</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hongxun</forename><surname>Yao</surname></persName>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Yue</forename><surname>Gao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
						</author>
						<title level="a" type="main">Predicting Personalized Image Emotion Perceptions in Social Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F7E957AEF4B7AA118BBDE13AC6FB7520</idno>
					<idno type="DOI">10.1109/TAFFC.2016.2628787</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2628787, IEEE Transactions on Affective Computing This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2628787, IEEE Transactions on Affective Computing</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T07:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Personalized image emotion</term>
					<term>social context</term>
					<term>temporal evolution</term>
					<term>location influence</term>
					<term>heypergraph learning 1. Specifically</term>
					<term>image emotion is often called image sentiment for binary classification (positive or negative) [1]</term>
					<term>[16]</term>
					<term>[28]</term>
					<term>[29]</term>
					<term>IEEE Transactions on Affective Computing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Images can convey rich semantics and induce various emotions to viewers. Most existing works on affective image analysis focused on predicting the dominant emotions for the majority of viewers. However, such dominant emotion is often insufficient in realworld applications, as the emotions that are induced by an image are highly subjective and different with respect to different viewers. In this paper, we propose to predict the personalized emotion perceptions of images for each individual viewer. Different types of factors that may affect personalized image emotion perceptions, including visual content, social context, temporal evolution, and location influence, are jointly investigated. Rolling multi-task hypergraph learning (RMTHG) is presented to consistently combine these factors and a learning algorithm is designed for automatic optimization. For evaluation, we set up a large scale image emotion dataset from Flickr, named Image-Emotion-Social-Net, on both dimensional and categorical emotion representations with over 1 million images and about 8,000 users. Experiments conducted on this dataset demonstrate that the proposed method can achieve significant performance gains on personalized emotion classification, as compared to several state-of-the-art approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>With the rapid development of digital photography technology and wide-spread popularity of social networks, people have become used to sharing their lives and expressing their opinions using images and videos together with text. The explosively growing volume of online social data have greatly motivated and promoted the research on large-scale multimedia analysis. As what people feel may directly determine their decision making, the understanding of these data at the emotional level is of great importance, which can benefit social communication and enable wide applications <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, ranging from marketing <ref type="bibr" target="#b2">[3]</ref> to political voting forecasts <ref type="bibr" target="#b3">[4]</ref>.</p><p>Despite the promising progress of textual sentiment analysis <ref type="bibr" target="#b4">[5]</ref>, emotion analysis of social images remains an open problem <ref type="bibr" target="#b5">[6]</ref>. The main challenges that limit the development of image emotion analysis lie in the so-called affective gap, as well as the subjective evaluation <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b6">[7]</ref>. Trying to find features that can express emotions better to bridge the affective gap, previous works mainly focused on predicting the dominant emotions for the majority of viewers, without considering the subjective evaluation. However, predicting personalized emotion is usually more practical, as the emotions that are induced in viewers by an image are highly subjective and different, due to the influence of social, educational and cultural backgrounds <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, as shown in Figure <ref type="figure">1</ref>. Under such a circumstance, Fig. <ref type="figure">1</ref>. Illustration of personalized image emotion perceptions in social networks. (a) The uploaded image to Flickr. (b) The title, tags and description given by the uploader to the image. (c) The expected emotion labels that we assign to the uploader using the keywords in (b) in red with both categorical and dimensional representations. (d) The comments to this image from different viewers and the personalized emotion labels that we obtain using the keywords in red. We can find that the emotions perceived by the viewers are truly subjective.</p><p>traditional dominant emotion based methods may not work well for personalized emotion prediction <ref type="bibr" target="#b12">[13]</ref>. So far, little progress has been made on predicting personalized emotion perceptions (termed PEP), mainly due to two key challenges:</p><p>The Lack of Benchmarks. To the best of our knowledge, there is no public dataset on PEP of images, though a few works have been done on emotion or sentiment analysis of social multimedia data <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>. To avoid the tedious manual labeling of large scale social images, existing works on emotion analysis mainly used two methods to obtain the emotion labels based on different emotion representation models. First, from the dimensional emotion perspective, traditional lexicon-based methods are used to find out the polarity values of the comments with a predefined word dictionary. Yang et al. <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b16">[17]</ref> adopted this method for Chinese microblog analysis with emotions represented by one dimension. However, they just classified the emotions into two discrete categories by taking threshold of the continuous polarities. No large scale dataset on social image emotions using dimensional representation based on English dictionary has been released. Second, from the categorical emotion perspective, keywords or synonyms based retrieval of specified emotions are used. The works in <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b17">[18]</ref> adopted this strategy. The emotion categories used for classification are ad hoc and arbitrary with categories numbers ranging from six to tens <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b18">[19]</ref>. Besides, the numbers of positive and negative emotions are unbalanced <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b17">[18]</ref>.</p><p>Multi-factor influence. Besides the visual content of images, there are many other factors that may influence the personalized perception of image emotions. Personal interest may directly influence the emotion perceptions <ref type="bibr" target="#b19">[20]</ref>. Viewers' emotions are often temporally influenced by their recent past emotions <ref type="bibr" target="#b20">[21]</ref>. In social networks, the emotions are widely influenced by the social connections. Whitfield <ref type="bibr" target="#b21">[22]</ref> showed that how happy you are is influenced by your social links to people even you've never heard of and never met. How a viewer's emotion is influenced by their friends on social networks is quantitatively studied in <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b17">[18]</ref>. The locations of social images, if known, can also be used for visual data analysis <ref type="bibr" target="#b22">[23]</ref>. How to consistently and effectively combine these factors for robust personalized emotion analysis is a challenging problem.</p><p>In this paper, we make the first attempt in predicting PEP of social images to tackle the two challenges above. For the first challenge, we set up a large scale image dataset of personalized emotions, named Image-Emotion-Social-Net, which are represented by both dimensional and categorical emotion models. We use 8 categories as the categorical emotion states which are defined in a rigorous psychological study <ref type="bibr" target="#b23">[24]</ref>, including anger, disgust, fear, sadness as negative emotions, and amusement, awe, contentment, excitement as positive emotions. Tens of keywords for each emotion category are used to search the titles, tags and descriptions of images or the comments to the images to obtain the personalized emotion labels. Then we computed the average value of valence, arousal and dominance (VAD) for dimensional emotion space using recently published VAD norms of 13,915 English lemmas <ref type="bibr" target="#b24">[25]</ref>. Besides, we give the sentiment category (positive or negative).</p><p>For the second challenge, we take different types of factors into account, including visual content, social context, temporal evolution and location influence. We propose a rolling multitask hypergraph learning (RMTHG) to formalize the personalized emotion perception prediction problem by modelling these various factors. Experiments are conducted on our collected dataset and the results demonstrate that by incorporating the various factors, the proposed model can significantly improve the prediction performance compared with the state-of-the-art approaches. We also design one novel application based on the personalized emotion prediction results. Please note that in this paper we do not clearly distinguish expressed, perceived and induced emotions as in music <ref type="bibr" target="#b25">[26]</ref>. We use "perceive (perception)" and "induce (induction)" from the perspective of emotion subjects, such as "User A perceives fear from image B" and "Image B induces fear in user A".</p><p>The contributions of this paper can be summarized in three aspects as follows:</p><p>1) We propose to distinguish personalized emotions from traditional dominant emotions. What's more, we also consider different types of factors, including visual content, social context, temporal evolution and location influence. 2) We present a compound vertex hypergraph to model all the different factors in a consistent and expandable way. To enable efficient inference in this framework, we devise a rolling multi-task hypergraph learning algorithm, which can simultaneously predict individual emotions of different users. 3) We set up a large scale personalized emotion dataset of social images from Flickr with over 1 million images and 1.4 million labels for about 8,000 users. The dataset, containing images, metadata, social connections and personalized emotions, will be released along with this work.</p><p>One preliminary conference version on personalized image emotion perception prediction has been accepted by ACM Multimedia <ref type="bibr" target="#b26">[27]</ref>. Our new improvement compared with the conference version lies in the following aspects. First, we add a data analysis section to inspire the motivation of modelling personalized emotion perceptions. Second, we extend the hyperedge construction in more detail. Finally, we conduct more comparative experiments and enrich the analysis of the results.</p><p>The rest of this paper is organized as follows. Section 2 introduces related work of image emotion and sentiment analysis, social media analysis and (hyper)graph based learning. Section 3 presents data analysis based justification to motivate the proposed idea. Section 4 introduces the constructed Image-Emotion-Social-Net dataset. Section 5 gives an overview of the proposed method. Hypergraph construction and rolling multi-task hypergraph learning are described in Section 6 and Section 7, respectively. Experimental evaluation and analysis are presented in Section 8, followed by conclusion and future work in Section 9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Image emotion and sentiment analysis. Some research efforts have been dedicated to improving the accuracy of image emotion prediction. Related works can be divided into different types, according to the adopted emotion models, the required tasks, the extracted features and the models used.</p><p>There are two kinds of emotion representation models: categorical emotion states (CES) and dimensional emotion space (DES). CES methods model emotions as one of a few basic categories 1 <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>, while DES methods employ 3-D or 2-D space to represent emotions, such as valence-arousal-dominance <ref type="bibr" target="#b33">[34]</ref>, natural-temporal-energetic <ref type="bibr" target="#b34">[35]</ref> and valence-arousal <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b31">[32]</ref>. Accordingly, related works on image emotion analysis can be classified into three different tasks: affective image classification <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>, regression <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b31">[32]</ref> and retrieval <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b29">[30]</ref>. We model image emotions using both representation models.</p><p>From a feature's view point, different levels of visual features are extracted for image emotion analysis. Low level holistic image features including Wiccest features and Gabor features are extracted to classify image emotions in <ref type="bibr" target="#b30">[31]</ref>. Lu et al. <ref type="bibr" target="#b31">[32]</ref> investigated the computability of emotion through shape features. Machajdik et al. <ref type="bibr" target="#b18">[19]</ref> extracted features inspired from psychology and art theory, including color, texture as low level features, composition as mid level features while face and skin as high level features. Zhao et al. <ref type="bibr" target="#b1">[2]</ref> proposed to extract mid level principles-of-art based emotion features, which are demonstrated to be more interpretable by humans and have stronger link to emotions than the elements-of-art based ones. Yuan et al. <ref type="bibr" target="#b28">[29]</ref> used mid level scene attributes for binary sentiment classification. Image based global sparse representation and region based local sparse representation are proposed to define the similarities of a test image and all training images <ref type="bibr" target="#b32">[33]</ref>. Visual sentiment ontology and detectors are proposed to detect high level adjective noun pairs (ANP) based on large-scale social multimedia data <ref type="bibr" target="#b15">[16]</ref>. Similarly, Chen et al. <ref type="bibr" target="#b0">[1]</ref> used object based methods to detect ANP. We extract features of different levels and investigate the performance on personalized emotion prediction.</p><p>The commonly used models are based on machine learning methods, such as Naive Bayes <ref type="bibr" target="#b18">[19]</ref>, SVM or SVR <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b31">[32]</ref>, sparse learning <ref type="bibr" target="#b32">[33]</ref> and multi-graph learning <ref type="bibr" target="#b5">[6]</ref>. These methods may perform well for traditional affective image classification, regression or retrieval, but they are difficult to incorporate different factors, such as social connections, temporal evolution, etc. We attempt to combine different kinds of factors with visual content to predict personalized image emotions.</p><p>Note that affective content analysis has also been widely studied based on other types of input data, such as text <ref type="bibr" target="#b4">[5]</ref>, speech <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, music <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref> and videos <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>.</p><p>Social media analysis. The extremely large volume of data in social networks have motivated various research topics related to multimedia, computer vision and data mining, such as brand data analysis <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b46">[47]</ref>, outbreak prediction <ref type="bibr" target="#b47">[48]</ref>, social event detection <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref>, cross-view retrieval <ref type="bibr" target="#b50">[51]</ref> and emotion related analysis <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b51">[52]</ref>. Among all these works, the emotion related analysis is similar to our work. Jia et al. <ref type="bibr" target="#b13">[14]</ref> simply used the uploaded time of images and the ID of image owner as social features, while no social features are used in <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b27">[28]</ref>. The social connections between different users are modelled <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>. The works in <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b17">[18]</ref> used social connections to model emotion influence of one user to another. Yang et al. <ref type="bibr" target="#b16">[17]</ref> utilized social factors together with visual and textual ones to discover representative images for social events. Social connections are used for predicting emotions for individuals <ref type="bibr" target="#b12">[13]</ref>, which is similar to our work. But they just simply classified the sentiments of Chinese microblogs and did not consider the temporal influence of emotions.</p><p>(Hyper)graph based learning. As the structure of graph model is similar to that of social networks, it is widely used for social media analysis <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b16">[17]</ref>. Factor graph model is used to analyze individual emotional states in <ref type="bibr" target="#b14">[15]</ref>. Social influence and user interest are modelled by a hybrid graph <ref type="bibr" target="#b12">[13]</ref>. Emotionally similar images were retrieved via multi-graph learning <ref type="bibr" target="#b5">[6]</ref> which was firstly used in video retrieval <ref type="bibr" target="#b52">[53]</ref>. Compared with conventional graph, hypergraph can reflect the higher order information <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b55">[56]</ref> and is widely used in music recommendation <ref type="bibr" target="#b54">[55]</ref>, 3D object retrieval <ref type="bibr" target="#b55">[56]</ref>, image retrieval <ref type="bibr" target="#b56">[57]</ref> and brand data gathering <ref type="bibr" target="#b22">[23]</ref>. However, traditional single vertex hypergraph <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b55">[56]</ref> cannot well model the temporal emotion influence. Further, the constructed social context hyperedge <ref type="bibr" target="#b22">[23]</ref> is of little significance for personalized emotion prediction, since for one user all the involved images are connected together. In this paper, we present a compound vertex hypergraph to model the various factors that may contribute to personalized emotion perception and devise a rolling multi-task hypergraph learning algorithm to simultaneously predict individual emotions of different users.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROBLEM JUSTIFICATION</head><p>To further motivate the problem, we conduct a data analysis based justification as in <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b59">[60]</ref> of existing datasets related to emotional subjective perception.</p><p>First, we analyze the distribution of dimensional emotions in the IAPS dataset <ref type="bibr" target="#b57">[58]</ref>, which consists of 1,182 documentarystyle natural color images depicting complex scenes, such as portraits, babies, animals, landscapes, etc. Each image was rated by approximately 100 college students (half female) on valance, arousal, and dominance in a 9-point rating scale. The mean value and standard deviation (STD) are assigned to each image. We illustrate the distribution of the STD of valence and arousal in Figure <ref type="figure" target="#fig_0">2</ref>. It is clear that the STD for the majority of images is larger than 1.5 and 2 on valence and arousal, respectively. We can conclude that the perceived emotions in dimensional form may greatly differ among these participants.</p><p>Second, we conduct the statistics of categorical emotions in the Abstract dataset <ref type="bibr" target="#b18">[19]</ref> and Emotion6 dataset <ref type="bibr" target="#b9">[10]</ref>. The Abstract dataset includes 279 peer rated abstract paintings without contextual content <ref type="bibr" target="#b18">[19]</ref>. Each image was rated about 14 people into 8 emotion categories <ref type="bibr" target="#b23">[24]</ref>. The Emotion6 dataset is composed of 1,980 images collected from Flickr by using emotion keywords and synonyms as search terms <ref type="bibr" target="#b9">[10]</ref>. Each image is scored by 15 subjects into Ekman's 6 emotion categories <ref type="bibr" target="#b60">[61]</ref>. The distributions of the 8 and 6 emotion categories for the Abstract and Emotion6 datasets are reported in Figure <ref type="figure" target="#fig_2">3</ref>. We can find that in the Abstract dataset, more than 81% images were assigned with more than 5 emotions, while in the Emotion6 dataset, more than 87% images induced at least 3 emotions. We can conclude that the perceived emotions in categorical form are indeed subjective and different.</p><p>Motivated by these analysis, it is more reasonable and significant to change the study from image-centric dominant emotion recognition to user-centric personalized emotion recognition or image-centric emotion distribution prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">THE IMAGE-EMOTION-SOCIAL-NET DATASET</head><p>In this section, we introduce the dataset (Image-Emotion-Social-Net, IESN<ref type="foot" target="#foot_1">2</ref> ) on emotions of social images, including the con- struction process, quality validation, dataset statistics and the challenging tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset Construction</head><p>We downloaded 21,066,920 images from Flickr with 2,060,357 users belonging to 264,683 groups. Each image is associated with the metadata, such as the title, tags, taken time and location if available. Each user is associated with the personal information, the contact list and the group list they joined in. As how to measure emotions is still far from consensus in research community <ref type="bibr" target="#b61">[62]</ref>, we defined emotions using both categorical and dimensional representations. For CES, we used the 8 categories rigorously defined in psychology <ref type="bibr" target="#b23">[24]</ref>, including 4 negative and 4 positive emotions. To get the ground truth labels, we adopted keywords based searching strategy as in <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b17">[18]</ref>. Tens of keywords for each emotion category are obtained from a public synonym searching site 3 and are manually verified, with examples shown in Table <ref type="table" target="#tab_0">1</ref>. Expected emotions of the image uploaders are firstly considered. The keywords are searched from the title, tags and descriptions given by the uploaders. The emotion category with the most frequent keywords is set as the ground truth of expected emotions from the uploaders.</p><p>As we are focusing on PEP, we then searched from all the comments of the images tackled above to get the personalized emotion labels of each viewer. As in <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b17">[18]</ref>, we removed the images if the searched title, tags or descriptions contain negation adjacent and prior to the target keywords, such as "I am not fear". Similarly, we also removed the comments with negation adjacent and prior to the target keywords. This is because the antonym of an emotion is not so clear. Note that the labels of an image for a specific user are allowed to have different emotion categories (such as fear, disgust) but must have only one sentiment (positive or negative). Then we computed the average value of valence, arousal and dominance of the segmented text (metadata or comments) as ground truth for dimensional emotion representation based on recently published VAD norms of 13,915 English lemmas <ref type="bibr" target="#b24">[25]</ref>. Besides, we also gave the sentiment categories (positive or negative). We combined the expected emotions and actual emotions of all involved images for each user. This process resulted in a dataset containing 1,012,901 images uploaded by 11,347 users; and 1,060,636 comments on these images commented by 106,688 users. We chose 7,723 active users with more than 50 involved images. Finally we obtained 1,434,080 emotion labels of three types, including 8 emotion categories, 2 sentiment categories and continuous VAD values. Note that all the involved images of one user are labelled with sentiment categories and VAD values, while a tiny proportion of 3. http://www.thesaurus.com/browse/synonym/  If one user is the uploader of an image, then the emotion of the metadata text (title, tags and descriptions) is the personalized emotion of this user, which is also the expected emotion that is expected to induce in other viewers by this user. If one user is a viewer of an image, then the emotion of the comment is the personalized emotion of this user.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A m u s e m e n t A n g e r A w e C o n te n tm e n t D is g u s t E x c it e m e n t F e a r S a d n e s s</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Dataset Validation</head><p>To validate the quality of the dataset, we did a crowdsourcing experiment on discrete emotions. For each emotion category, we randomly selected 200 images with associated titles, tags and descriptions for expected emotions, and 200 comments with corresponding images for personalized emotions. 5 graduate students (3 males, 2 females) were invited to judge whether the text was used to express the assigned emotions of related images. To facilitate this judgement, they were asked simple question like "Do you think that the text is used to express excitement for this image?", and they just needed to choose YES or NO. Each image was judged by all the 5 annotators. The result is shown in Figure <ref type="figure" target="#fig_3">4</ref>. We can find that for both expected and personalized emotions, on average more than 88% of emotion labels receive at least 3 Yeses, which verifies the quality of the constructed dataset. In such cases, the expected emotion labels are 3.5% more accurately assigned than personalized emotions. To assess the inter-rater agreement, we also calculate the Fleiss' kappa 4 of the 5 annotators. The average Fleiss' kappa (the standard deviation) for the 8 emotion categories of expected emotions and personalized emotions are 0.2297 (0.0748) and 0.3224 (0.1411), respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Statistics of Dataset</head><p>The distribution of images per emotion category is shown in Table <ref type="table" target="#tab_1">2</ref>, where the first four columns represent the number of images in each of the 8 emotions; while the last column is the number of images with binary sentiments. We can find that the number of negative emotions is relatively small. The distribution of valence, arousal (without showing dominance here) is illustrated in Figure <ref type="figure">7</ref>(a), which looks like a petal or heart, similar to the emotion space in <ref type="bibr" target="#b62">[63]</ref>. The user distribution based on the involved 4. https://en.wikipedia.org/wiki/Fleiss%27 kappa  images is shown in Figure <ref type="figure">7</ref>(b). The distribution of emotion numbers for the images with more than 20 labels each is shown in Figure <ref type="figure">7(c</ref>). Some image examples with assigned emotions in both CES and DES forms are given in Figure <ref type="figure" target="#fig_4">5</ref>. We can find that the emotion perceptions of different users are truly subjective and personalized. More image examples with dominant emotions are given in Figure <ref type="figure">6</ref>.</p><p>We also analyze the relation between the expected and personalized emotions. For each of the images with more than 20 labels, we compute the Euclidean distances between personalized emotions and expected emotion in VA space, and average all the distances. The histogram of the average VA distance is shown in Figure <ref type="figure">8</ref>(a). For CES, we count the proportion of personalized emotions that are different from expected emotion for each image. The histogram of different emotion proportions is illustrated in Figure <ref type="figure">8(b)</ref>. It is clear that there exists great inconsistency between expected and personalized emotions.</p><p>The average and standard deviation of friend numbers among the 8k users are 45.7 and 21.4, respectively. Besides, users can be correlated by joining the same groups. So there exist rich social </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Challenging Tasks</head><p>The challenging tasks that can be performed by researchers on this dataset include, but are not limited to, the followings:</p><p>1) Image-centric emotion analysis. For each image, we can predict the dominant emotion category like the traditional affective image classification. Besides, we can also predict the emotion distribution of each image, taking the normalized emotion proportion as the ground truth. 2) User-centric emotion prediction. For each user, we can predict their personalized emotion perception of some specific images. The above two tasks can be extended to regression and retrieval tasks, all of which can be done using visual, social, temporal and the combination of all features. x i2</p><p>x n3</p><p>x j2 x j2 x j1 Fig. <ref type="figure">9</ref>. The framework of the proposed method for personalized image emotion prediction.</p><p>3) Emotion related data mining and applications. This dataset contains visual and social information to support research on emotion influence mining, social advertising, affective image retrieval, emotional outbreak prediction and affective recommender systems, etc. For different tasks, the roles of expected and personalized emotions are different. For image-centric expected emotion analysis, only the expected emotions can be used. For image-centric dominant emotion analysis or emotion distribution analysis, the expected emotions can be viewed as one type of personalized emotions. For user-centric emotion prediction, the expected emotions can also be viewed as one type of personalized emotions, but only for the uploaders of the related images. In this paper, we focus on the second task, trying to combine social, temporal and other factors with traditional visual features to predict PEP for each viewer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">OVERVIEW</head><p>Our goal is to predict the emotions of a specified user after viewing an image, associated with online social networks. Different types of factors can influence the emotion perception and can be exploited for emotion prediction. The following factors are hypothesized to contribute to emotion perceptions:</p><p>Visual Content. The visual content of an image can directly influence the emotion perception of viewers. Different from traditional image emotion prediction, both the images viewed in the recent past and the current image are taken into account in our system. This point is also mentioned in the factor of temporal evolution. The visual descriptors used in this paper include low, middle, and high level features.</p><p>Social Context. One viewer's emotion may be easily and largely affected by the social environment that they live in, e.g. their friends, so social context is highly helpful to make our prediction more accurate. Specifically, we consider the following social contexts: whether two users are in common interest groups, have common contact lists and the similarity of comments to same images.</p><p>Temporal Evolution. One viewer's emotional variation within a short time is not so obvious, i.e., the current emotion is not independent on the recent past emotion, so temporal evolution gives additional information with respect to emotion prediction. Our system can take the recent past emotion into consideration.</p><p>Location Influence. Where and when a picture is taken is another factor which may contribute to emotional variation. One example is that photographs taken in entertainment venues usually lead people to feel happy. We encapsulate location information (if available) in our framework to improve the prediction performance.</p><p>We present rolling multi-task hypergraph learning (RMTHG) to jointly combine these factors. Formally, a user u i in social networks observes an image x it at time t, and their perceived emotion after viewing the image is y it . Before viewing x it , the user u i may have seen many other images. Among these images we select the recent past ones, which we believe to affect the current emotion. These selected images comprise a set S i . The emotional social network is formalized as a hybrid hypergraph G = {U, X , S}, E, W . Each vertex v = (u, x, S) in vertex set V = {U, X , S} is a compound triple (u, x, S), where u represents user, x and S are the current image and the recent past images, which are named as 'Target Image' and 'History Image Set', respectively. It should be noted that in this triple, both x and S are viewed by user u. E is the hyperedge set. Each hyperedge e of E represents a link between two or more vertices based on one component of the triple and is assigned with a weight w(e). W is the diagonal matrix of the edge weights.</p><p>Mathematically, the task of personalized emotion prediction is to find the appropriate mapping</p><formula xml:id="formula_0">f : (G, y i1 , ..., y i(t-1) ) → y it ,<label>(1)</label></formula><p>for each user u i . The framework of the proposed method is shown in Figure <ref type="figure">9</ref>. First, we generate the compound triple vertex for each viewer based on the time of related images which they uploaded or commented. Second, the hypergraphs are constructed for each component of the triple based on different factors. Finally, we obtain the personalized emotion prediction results after the rolling learning of the multi-task hypergraphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">HYPERGRAPH CONSTRUCTION</head><p>As stated in Section 5, a vertex of the proposed RMTHG is a compound one, which consists of three components. It should be emphasized that the images in such a vertex is a general concept, which not only refers to the pixel array itself, but also includes some additional information associated with the image, such as location, time, and emotional labels (if any) with respect to the specified user. For conventional emotion prediction, a vertex containing a pixel array is sufficient. In contrast, the compound vertex formulation enables our system to model all the four kinds of factors in Section 3: visual descriptors both in the target image and history image set can be extracted to represent visual content; user relationship can be exploited from the user component to take social context into consideration; past emotion can be inferred from history image set to reveal temporal evolution; location influence is embedded in the associated information with target image and history image set. Consequently, such a vertex generation mechanism is flexible and extensible.</p><p>In our hypergraph framework, hyperedges are used to indicate some types of similarity among vertices, meaning that vertices sharing a common hyperedge tend to have similar emotions. Based on the vertex components, we construct different types of hyperedges, including target image centric, history image set centric, and user centric hyperedges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Target Image Centric Hyperedges</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">Visual Content</head><p>As demonstrated in <ref type="bibr" target="#b5">[6]</ref>, the features that determine the emotions of an image are different for various kinds of images. Similar to <ref type="bibr" target="#b5">[6]</ref>, we extract commonly used visual features of different levels and generalities for each image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Low-level Features</head><p>Low-level features suffer from the difficulty of interpretation and weak link to emotions <ref type="bibr" target="#b5">[6]</ref>. However, they are useful as global descriptors of the overall image content. Here we extract two classes of low-level visual features. The first is the generic GIST feature, which is one of the most commonly used features, for its relatively powerful description ability of visual phenomena in a scene perspective <ref type="bibr" target="#b63">[64]</ref>  <ref type="bibr" target="#b64">[65]</ref>.</p><p>The second class includes the special features derived from elements of art, including color and texture <ref type="bibr" target="#b18">[19]</ref>. Here the color features include mean saturation and brightness, vector based mean hue, emotional coordinates (pleasure, arousal and dominance) based on brightness and saturation, colorfulness and color names; while the texture features include Tamura texture, Wavelet textures and gray-level co-occurrence matrix (GLCM) based texture <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mid-level Features</head><p>Mid-level features are more semantic, interpretable and have stronger link to emotions than low-level features <ref type="bibr" target="#b1">[2]</ref>. Here we extract two classes of mid-level features. The first is attribute based representation that has been widely studied in recent years for its intuitive interpretation and cross-category generalization property in visual recognition domain <ref type="bibr" target="#b64">[65]</ref>, <ref type="bibr" target="#b65">[66]</ref>, <ref type="bibr" target="#b66">[67]</ref>, <ref type="bibr" target="#b67">[68]</ref>. We extract 102 dimensional attributes which are commonly used by humans to describe scenes as mid-level generic features. As in <ref type="bibr" target="#b64">[65]</ref>, the attributes can be classified into five types: materials (mental), surface properties (dirty), functions or affordances (reading), spatial envelop attributes (cluttered) and object presence (flowers). GIST features and SVM implemented in Liblinear toolbox 5 are used to train attribute classifiers based on 14,340 images in SUN database <ref type="bibr" target="#b63">[64]</ref>.</p><p>The second class is the specific mid-level features inspired from the principles-of-art, including balance, contrast, harmony, variety, gradation, and movement <ref type="bibr" target="#b1">[2]</ref>. As the guidelines and tools, these artistic principles are used to arrange and orchestrate artistic elements in art theory for describing specific semantics and emotions; they are found to have stronger link to emotions 5. http://www.csie.ntu.edu.tw/ ∼ cjlin/liblinear/ than elements. Please refer to <ref type="bibr" target="#b1">[2]</ref> for detailed implementations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>High-level Features</head><p>High-level features are the detailed semantic contents contained in images. People can easily understand the emotions conveyed in images by recognizing the semantics. Again, we extract two classes of high-level features. The first class covers the set of concepts described by the 1,200 adjective noun pairs (ANPs) <ref type="bibr" target="#b15">[16]</ref>. The ANPs are detected by a large detector library SentiBank <ref type="bibr" target="#b15">[16]</ref>, which is trained on about 500k images downloaded from Flickr using various low-level features, including GIST, color histogram, LBP descriptor, attribute, etc. Liblinear SVM is used as classifier by early fusion. Finally, we obtain a 1,200 dimensional vector describing the probability that each ANP is detected.</p><p>Motivated by the conclusion that facial expressions may determine the emotions of the images containing faces <ref type="bibr" target="#b5">[6]</ref>, the second class of high-level features extract 8 kinds of facial expressions (anger, contempt, disgust, fear, happiness, sadness, surprise, neutral) <ref type="bibr" target="#b68">[69]</ref>. Compositional features of local Haar appearance features are built by a minimum error based optimization strategy, which are embedded into an improved AdaBoost algorithm <ref type="bibr" target="#b69">[70]</ref>. Trained on CK+ database <ref type="bibr" target="#b68">[69]</ref>, the method performs well even on low intensity expressions <ref type="bibr" target="#b69">[70]</ref>. Face detection is firstly conducted using Viola-Jones algorithm <ref type="bibr" target="#b70">[71]</ref> to decide whether an image contains faces. Finally, we obtain a 8 dimensional vector, each of which represents the proportion of related facial expressions.</p><p>The six sets of extracted visual features are abbreviated as GIST, Elements, Attributes, Principles, ANP and Expressions with dimension 512, 48, 102, 165, 1200 and 8, respectively. Given two triple vertices v it1 = (u i , x it1 , S it1 ) and v jt2 = (u j , x jt2 , S jt2 ), where x it1 and x jt2 also represent related visual features, the visual similarity based on target image (T ) is computed by</p><formula xml:id="formula_1">s T V (v it1 , v jt2 ) = exp - d(v it1 , v jt2 ) σ ,<label>(2)</label></formula><p>where d(., .) is a specified distance function, σ is set as the average distance of the distance matrix of all images. Here the Euclidean distance is used for d(., .). We can construct 6 kinds of hyperedges based on different visual features. Figure <ref type="figure" target="#fig_7">10</ref>(a) illustrates the construction procedure of visual content based hyperedges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Location Influence</head><p>Besides the image itself, there are often other metadata of social images, such as the taken time, the location, etc. We consider the location influence here, as the images taken around the similar place within a short time tend to describe similar events and express similar emotions. The geographical similarity between v it1 and v jt2 with locations l(x it1 ) = (lat it1 , lon it1 ) and l(x jt2 ) = (lat jt2 , lon jt2 ) (if available) is measured by the Harversion formula <ref type="bibr" target="#b48">[49]</ref>, which is computed by</p><formula xml:id="formula_2">s T L (v it1 , v jt2 ) = 1 -2 arctan 2 ( φ, 1 -φ), φ = sin 2 ( ∆lat 2 ) + cos(lat it1 ) cos(lat jt2 ) sin 2 ( ∆lot<label>2</label></formula><p>), ∆lat = lat jt2 -lat it1 , ∆lot = lon jt2 -lon it1 .</p><p>(</p><formula xml:id="formula_3">)<label>3</label></formula><p>Figure <ref type="figure" target="#fig_7">10</ref>(b) illustrates the construction procedure of location based hyperedges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">History Image Set Centric Hyperedges</head><p>Similar to target images, we can construct hyperedges for the history image sets based on the visual features and known locations. Besides, the emotion labels of the history image sets are known. So we can also construct hyperedges based on this information by using the normalized emotion distribution to explore the temporal influence of emotions. As each history image set is composed of different numbers of sequential images, we use dynamic time warping (DTW) <ref type="bibr" target="#b71">[72]</ref> to measure the distance between two history image sets. For pairwise images from two history image sets, the Euclidean distance is used to measure the distance and the visual similarity can be obtained using Eq. ( <ref type="formula" target="#formula_1">2</ref>). Pairwise location similarity is computed by the Harversion formula. Similar to Plutchik's wheel <ref type="bibr" target="#b72">[73]</ref>, pairwise emotion distance is defined as 1+"the number of steps required to reach one emotion from another on by Mikels' wheel (see Figure <ref type="figure">11</ref>))". Pairwise emotion similarity is defined as the reciprocal of pairwise emotion distance. Finally, we can obtain the visual similarity s H V (v it1 , v jt2 ), location similarity s H L (v it1 , v jt2 ) and emotion similarity s H E (v it1 , v jt2 ) for two history image sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">User Centric Hyperedges</head><p>In social networks, the emotions perceived by one user can be easily influenced by their friends. In our dataset, there are various kinds of social connections. The social similarity between two users (U ) v it1 and v jt2 is measured by</p><formula xml:id="formula_4">s U S (v it1 , v jt2 ) = 1, if u i = u j , 1 3 (g S + c S + f S ), otherwise,<label>(4)</label></formula><p>where g S (u i , u j ) is defined as the ratio between the common groups that both users join, and the groups that either of them joins,</p><formula xml:id="formula_5">g S (u i , u j ) = |g(u i ) g(u j )| |g(u i ) g(u j )| ,<label>(5)</label></formula><p>where g(u i ) is the group set that u i joins, |.| is the element number of a set. Similar to g S , c S (u i , u j ) is defined as the ratio between the common contact lists that both users follow, and the contact lists that either of them follows. f S (u i , u j ) is the average BoW similarity of comments to the same images</p><formula xml:id="formula_6">f S (u i , u j ) = 1 M M k=1 s(BoW i k , BoW j k ), (<label>6</label></formula><formula xml:id="formula_7">)</formula><p>where M is the number of images that both users comment on, BoW i k is the BoW feature of the kth comment from user u i , s(., .) is the cosine function that computes the similarity of two BoW features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">ROLLING MULTI-TASK HYPERGRAPH LEARNING</head><p>Given the emotional hybrid hypergraph G = {U, X , S}, E, W , we obtain the incidence matrix H by computing each entry as,</p><formula xml:id="formula_8">h(v, e) = 1, if v ∈ e, 0, if v ∈ e. (<label>7</label></formula><formula xml:id="formula_9">)</formula><p>The vertex degree of vertex v ∈ V and the edge degree of hyperedge e ∈ E are defined as d(v) = e∈E w(e)h(v, e), δ(e) = v∈V h(v, e). According to d(v) and δ(e), we define two diagonal matrices D v and D e as</p><formula xml:id="formula_10">D v (i, i) = d(v i ) and D e (i, i) = δ(e i ).</formula><p>Given N users u 1 , . . . , u N and the related images, our objective is to explore the correlation among all involved images and the user relations. Suppose the training vertices and the training labels are</p><formula xml:id="formula_11">{(u 1 , x 1j , S 1j )} m1 j=1 , • • • , {(u N , x N j , S N j )} m N j=1</formula><p>, and</p><formula xml:id="formula_12">Y 1 = [y 11 , • • • , y 1m1 ] T , . . . , Y N = [y N 1 , • • • , y N m N ] T ,</formula><p>and the to-be-estimated relevance values of all images related to the specified users are</p><formula xml:id="formula_13">R 1 = [R 11 , • • • , R 1m1 ] T , . . . R N = [R N 1 , • • • , R N m N ] T . We denote Y and R as Y = [Y T 1 , • • • , Y T N ] T , R = [R T 1 , • • • , R T N ] T .<label>(8)</label></formula><p>The proposed rolling multi-task learning can be conducted as a semi-supervised learning to minimize the empirical loss and the regularizer on the hypergraph structure simultaneously by</p><formula xml:id="formula_14">arg min R {Γ + λΨ},<label>(9)</label></formula><p>where λ is a trade-off parameter, Γ is the empirical loss defined by</p><formula xml:id="formula_15">Γ = ||R -Y|| 2 , (<label>10</label></formula><formula xml:id="formula_16">)</formula><p>and Ψ is the regularizer on the hypergraph structure defined by</p><formula xml:id="formula_17">Ψ = 1 2 e∈E µ,ν∈V w(e)h(µ, e)h(ν, e) δ(e) R(µ) D v (µ, µ) - R(ν) D v (ν, ν) 2 = R T I -D -1/2 v HWD -1 e H T D -1/2 v R.<label>(11)</label></formula><p>By setting the derivative of Eq. ( <ref type="formula" target="#formula_14">9</ref>) with respect to R to zero, we can derive the solution for the objective function Eq. ( <ref type="formula" target="#formula_14">9</ref>)</p><formula xml:id="formula_18">R = I + 1 λ ∆ -1 Y, (<label>12</label></formula><formula xml:id="formula_19">)</formula><p>where ∆ is the hypergraph Laplacian <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b55">[56]</ref>, defined as</p><formula xml:id="formula_20">∆ = I -D -1/2 v HWD -1 e H T D -1/2 v .<label>(13)</label></formula><p>By using the relevance scores in R, we can rank the related images for each user. The top results with high relevance scores are assigned with related emotion category. Suppose the predicted results of the test images are Ê = F (R) based on relevance score Algorithm 1: Learning procedure for RMTHG Input: Error threshold ε, regularization λ, max-epochs K, training labels Y Output: Predicted emotion labels</p><formula xml:id="formula_21">Ê 1 Initialization: Ê(0) ← 0; Construct hypergraph G (0) = {U, X , S}, E (0) , W (0) ; 2 for k ← 1 to K do 3 Compute H (k-1) , D (k-1) e , D (k-1) v from G (k-1) ; 4 Compute ∆ (k-1)</formula><p>by Eq.( <ref type="formula" target="#formula_20">13</ref>) using</p><formula xml:id="formula_22">H (k-1) , D (k-1) e , D (k-1) v ; 5 R (k) ← (I + 1 λ ∆ (k-1) ) -1 Y; 6 Ê(k) ← F (R (k) ); 7 if Ê(k) -Ê(k-1) 2 &lt; ε then 8 break; 9 end 10 Update the hypergraph G (k) = {U, X , S}, E (k) , W (k)</formula><p>based on the emotions of history image set s H Ê(k) ; 11 end 12 return Ê(k) . threshold function F , which is simply 'of top M results' <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b55">[56]</ref>, where M is selected by the average F1. We can then iteratively update Equ. <ref type="bibr" target="#b11">(12)</ref> until convergence, as shown in Algorithm 1. We call this method rolling multi-task hypgergraph learning, because it simultaneously classifies the emotions of a specified category for N users in a hypergraph framework and the learning process is iteratively updated until convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">EXPERIMENTS</head><p>To evaluate the effectiveness of the proposed RMTHG method for personalized image emotion prediction, we carried out emotion classification experiments on the IESN datset. We also designed one novel application using the predicted emotions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Experimental Settings</head><p>As users upload or comment on images in chronological order and the perceived emotions can be influenced temporally, we split the dataset into a training set and a test set based on the uploading time and the commenting time of related images. The first set covering 50% of images of each viewer is used for training and the rest is used for test. As there are about 8,000 users in the dataset, we randomly split them into 80 groups to facilitate fast computation and save memory. Each time we conduct experiments on one group and eventually we computed the average performance and the standard deviation of all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1.1">Baselines</head><p>For emotion classification, we used four state-of-the-art classifiers as baselines: (1) Naive Bayes (NB), (2) Support Vector Machine (SVM) with RBF kernel, which are commonly used for traditional affective image classification in <ref type="bibr" target="#b18">[19]</ref> and <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b31">[32]</ref>, respectively;</p><p>(3) Graph Model (GM), which is used for personalized emotion prediction <ref type="bibr" target="#b12">[13]</ref>; and (4) Hypergraph learning (HG), which is used for brand data gathering <ref type="bibr" target="#b22">[23]</ref>. In GM, the social factors in <ref type="bibr" target="#b12">[13]</ref> and our visual features are combined. In HG, only the "Target Image" component of RMTHG is modelled using visual and location features. The social context features makes no sense since for one user all the images are connected together with the same weight.</p><p>For emotion regression, we tested the performances of Support Vector Regression (SVR) as in <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b31">[32]</ref> and multiple linear regression (MLR). It is difficult for these methods to model features like social context, so we just used visual features. Different kernels were tested for emotion regression using SVR. How to effectively combine the different factors for emotion analysis in a regression framework remains our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1.2">Evaluation Criteria</head><p>For emotion classification, we used precision, recall and F1-Measure 6 to evaluate the performance of different methods:</p><p>• Precision (P re). Precision measures the accuracy of emotion classification and is computed by:</p><formula xml:id="formula_23">P re = # Correctly Classif ied Relevant Images # All Classif ied Relevant Images ,</formula><p>where # Correctly Classif ied Relevant Images is the number of relevant images that are correctly classified, and # All Classif ied Relevant Images is the total number of images that are classified as relevant ones.</p><p>• Recall (Rec). Recall measures the data coverage of relevant images for a specified emotion category, and is calculated by:</p><formula xml:id="formula_24">Rec = # Correctly Classif ied Relevant Images # All Relevant Images ,</formula><p>where # All Relevant Images is the number of relevant images in the dataset for a specified emotion category.</p><p>• F1-Measure (F 1). F1-Measure jointly considers Recall and Precision, which is defined as:</p><formula xml:id="formula_25">F 1 = 2 × P re × Rec P re + Rec .</formula><p>For emotion regression, we used mean squared error 7 as the evaluation criteria:</p><p>• Mean squared error (M SE). Mean squared error measures the difference of the predicted values and the real values of VAD, and is calculated by:</p><formula xml:id="formula_26">M SE = 1 n n i=1 ( E i -E i ) 2 ,</formula><p>where n is the number of test images, E i and E i are the real values and estimated values of valence, arousal or dominance, respectively.</p><p>All the four measurements above range from 0 to 1. Higher values of P re, Rec and F 1 represent better performances for emotion classification, while lower M SE indicates better performance for emotion regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Personalized Emotion Classification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.1">On Visual Features</head><p>First, we conducted experiments to compare the performance of different visual features for personalized emotion classification; and used SVM and NB as baselines. For comparison, we used a simple version of GM, HG and RMTHG that consider only visual features, abbreviated as GM(V), HG(V) and RMTHG(V). The  average performances on emotion classification using different visual features and methods are given in Figure <ref type="figure" target="#fig_0">12</ref>. The performances of F1 for every emotion category are shown in Figure <ref type="figure" target="#fig_2">13</ref>.</p><p>From the results, we can observe that: (1) generally, the fusion of different kinds of visual features outperforms the use of only single feature, possibly because it can utilize the advantages from different aspects; (2) the proposed hypergraph learning method greatly outperforms the baselines on almost all features; for the fusion of different features, the proposed RMTHG(V) method achieves 65.2%, 62.2%, 22.3% and 16.0% performance gains on F1 as compared to SVM, NB, GM(V) and HG(V), respectively;</p><p>(3) for the 8 emotion categories, the most discriminative features are different; but overall the high-level and mid-level features have stronger discriminability than low-level ones, which is consistent with the conclusions in <ref type="bibr" target="#b5">[6]</ref>; (4) the positive emotions are more accurately modelled than the negative ones by almost all the four methods; and (5) the overall precision, recall and F1 are still very low, indicating that only using visual features to classify personalized image emotions is not sufficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.2">On Different Factors</head><p>Besides, we evaluated the influence of different factors in the proposed method. We computed the performance of the proposed method without visual content (RMTHG-V), without social context (RMTHG-S), without temporal evolution (RMTHG-T), and without location influence (RMTHG-L). RMTHG-T means that the history image sets of all vertices are empty. Here all the visual features are considered. Meanwhile, we compared the performances of GM(V) and GM (fused visual features and social factors <ref type="bibr" target="#b12">[13]</ref>), HG(V) and HG (fused visual features and location   From the results, we can observe that: (1) for the GM method, the combination of the social factors in <ref type="bibr" target="#b12">[13]</ref> can improve the performance, though the improvement is not that significant; (2) for the HG method, the performance is improved with the help of location information; (3) by incorporating the various factors, the classification performance of RMTHG is greatly improved; compared to RMTHG(V), the improvement of RMTHG on precision, recall and F1 are 49.4%, 9.5% and 33.2%, respectively, which indicates that the social emotions are jointly affected by these factors; (4) the proposed RMTHG method significantly outperforms the four baselines, achieving 120.0%, 116.0%, 54.9% and 49.2% performances gains on F1 for SVM, NB, GM and HG, respectively; (5) the contributions of the various factors in the proposed RMTHG to emotion perceptions are different; on average, the discriminability order of these factors is visual content &gt; social context &gt; temporal evolution &gt; location influence; and (6) even without using the visual content, we can still get competitive results (by comparing RMTHG(V) and RMTHG-V), which demonstrates that the social features and the temporal evolution play an important role in emotion perception.</p><p>To better illustrate the help of different factors in predicting the correct emotions, we give some visual examples, as shown in Figure <ref type="figure">18</ref>. From this example, we can easily understand how these factors work in personalized emotion perception prediction.</p><formula xml:id="formula_27">u n S n2 x n2 u i S i1 x i1 u j S j1 x j1 u i S i2 x i2 u i S i3 x i3 u i S i4 x i4 u i S i5 x i5 u j S j2 x j2 u j S j3 x j3 u j S j4 x j4 u j S j5 x j5 u k S k1 x k1 u k S k3 x k3 u k S k6 x k6 u k S k4 x k4 u n S n1 x n1 u n S n6 x n6 u k S k5 x k5 u n S n3 x n3 u n S n4 x n4 u i S i6 x i6</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Time</head><p>For (u i , x i5 , S i5 ), the hyperedge based on the visual content of x is e 1 (x i5 , x i4 , x j3 , x j4 , x k2 , x k3 ), weight w 1 =0.8; the hyperedge based on u is e 2 (u i , u j ), weight w 2 =0.85. For (u n , x n6 , S n6 ), the hyperedge based on the visual content of x is e 3 (x n6 , x j4 , x k4 , x k5 ), weight w 3 =0.9; the hyperedge based on the emotions of S is e 4 (S n6 , S n5 , S n4 ), weight w 4 =1 (the emotions of history image sets S n6 , S n5 , S n4 are all</p><p>). e 3 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.3">Case Studies</head><p>Here we show some interesting cases to demonstrate the effectiveness of the proposed method. By leveraging the statistics on the performance improvement and different factor influences, we have the following discoveries.</p><p>The performance improvement in the prediction of negative emotions is higher than positive ones. Comparing the prediction results of RMTHG(V) and RMTHG in Figure <ref type="figure" target="#fig_10">17</ref>, we can observe that when taking temporal evolution, social context and location influence into account, the performance of negative emotion classification improves more significantly than position ones. This observation means that these context factors play a more important role in personalized perception of negative emotions, indicating that the influence of negative emotions is likely to be stronger than positive ones, which is consistent with <ref type="bibr" target="#b73">[74]</ref>. Similar conclusion can be obtained when comparing GM(V) and GM.</p><p>Sadness is one special category that the influence of temporal evolution is larger than social context. From Figure <ref type="figure" target="#fig_10">17</ref>, it is easy to see that the performance gain of social context is larger than temporal evolution for all the 4 positive emotions and the 3 negative emotions of anger, disgust and fear, while it is the opposite for sadness emotion. This is probably because sadness tends to be a long lasting emotion <ref type="bibr" target="#b74">[75]</ref>, <ref type="bibr" target="#b75">[76]</ref>, which may last up to 240 times longer than surprise, fear, etc., as what spurred the feeling is often of greater importance <ref type="bibr" target="#b75">[76]</ref>. So sadness requires one to make meaning of the event and cope with the new situation which takes time, while fear, disgust or awe following event offset often lacks purpose <ref type="bibr" target="#b76">[77]</ref>.</p><p>Stronger social connections tend to have more influences on performance improvement. We randomly selected 100 users from the dataset and collected the images they uploaded or commented together with various metadata. For each of the 100 users, we first obtained the average social similarity in the constructed hypergraph and then computed the performance gains of F1 with  the help of social context. The relation is depicted in Figure <ref type="figure">19</ref>. It seems that with the increase of social connections, the overall trend of performance gains is growing, which indicates that stronger social connections correlate with higher performance gains. The results accord with the findings in <ref type="bibr" target="#b77">[78]</ref>, which concluded that emotions are influenced by social context and that the stronger the connection is, the larger the influence is. This fact also demonstrates the basic hypothesis of social context's contribution to emotion perceptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">Personalized Emotion Regression</head><p>MLR performs the worst with MSE much greater than 10, which indicates that the linear regression is not appropriate for emotion regression. The results of average MSE on valence, arousal and dominance using SVR are shown in Figure <ref type="figure" target="#fig_0">20</ref>. It is clear that valence is regressed the worst, while arousal and dominance are relatively better predicted. SVR performs much better than MLR, which demonstrates its effectiveness on emotion regression. Besides, the RBF kernel works slightly better than Sigmoid kernel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.4">Application: Affective Curve Visualization</head><p>Given a specified user, we can predict the personalized emotion perceptions of the current image based on the proposed method, Figure <ref type="figure" target="#fig_16">22</ref> shows the image storyline of User A together with the turning point images and corresponding behaviors. For clarity, we just use valence to represent the pleasantness of emotions. From this figure, we can clearly see the emotional status change of a solider before, in and after war.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">CONCLUSION AND FUTURE WORK</head><p>In this paper, we proposed to predict personalized perceptions of image emotions by incorporating various factors (social context, temporal evolution, and location influence) with visual content. Rolling multi-task hypergraph learning was presented to jointly combine these factors. A large-scale personalized emotion dataset of social images was constructed and some baselines were provided. Experimental results on personalized emotion classification demonstrated that the performance of the proposed method is superior over the state-of-the-art approaches. The predicted personalized emotions can be used to develop various applications, such as affective curve visualization.</p><p>For further studies, we will try to mine group emotions on the Image-Emotion-Social-Net. Modelling social connections of users dynamically and considering interest prior by mining related personal profile may improve the performance of emotion prediction. In addition, we can model the different emotions in a multi-task learning framework to explore the emotion relations. How to extend the proposed method for personalized affective image regression is also worth studying.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Statistical results of continuous emotions in the IAPS dataset<ref type="bibr" target="#b57">[58]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Statistical results of discrete emotions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Dataset validation results. &gt;= n means at least n Yes's.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Image examples of personalized emotion categories in the Image-Emotion-Social-Net dataset. From top to bottom are: original images, VA emotion labels and the distribution of 8 emotion labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .Fig. 7 .</head><label>67</label><figDesc>Fig. 6. Image examples of different dominant emotion categories in the Image-Emotion-Social-Net dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>CES proportionFig. 8. The relation between expected vs personalized emotions of the images with more than 20 emotion labels on both DES and CES representations. connections in the dataset. The time of the collected dataset ranges from Oct. 5, 2012 to Mar. 29, 2013, lasting about 6 months.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 10 .</head><label>10</label><figDesc>Illustration of target image centric hyperedge construction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Mikels' emotion distance Fig. 11. Mikels' emotion wheel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 13 .Fig. 12 .Fig. 14 .</head><label>131214</label><figDesc>Fig.<ref type="bibr" target="#b12">13</ref>. Performance comparison on F1 of 8 emotions using all the 5 methods and 7 kinds of features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 17</head><label>17</label><figDesc>Fig. 17. The influence of different factors in the proposed method on F1 of 8 emotions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>Fig. 17. The influence of different factors in the proposed method on F1 of 8 emotions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 18 .Fig. 19 .</head><label>1819</label><figDesc>Fig. 18. Illustrate of how social context and temporal evolution help in predicting the correct emotions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>Fig. 20. Personalized emotion regression results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 21 .</head><label>21</label><figDesc>Personalized affective curve examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 22 .</head><label>22</label><figDesc>Fig. 22. Emotion based image storyline for User A.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1</head><label>1</label><figDesc>The keyword examples of each emotion category. '#' indicates the total keyword numbers.</figDesc><table><row><cell>Emotion</cell><cell>#</cell><cell>Keyword examples</cell></row><row><cell>amusement</cell><cell>24</cell><cell>amused, amusement, cheer, delight, funny, pleasing</cell></row><row><cell>anger</cell><cell>79</cell><cell>angry, annoyed, enraged, hateful, offended, provoked</cell></row><row><cell>awe</cell><cell>36</cell><cell>amazing, astonishment, awesome, impressive, wonderful</cell></row><row><cell>contentment</cell><cell>28</cell><cell>comfortable, fulfilled, gladness, happy, pleasure, satis-</cell></row><row><cell></cell><cell></cell><cell>fied</cell></row><row><cell>disgust</cell><cell>35</cell><cell>detestation, disgusted, nauseous, queasy, revolt, weary</cell></row><row><cell>excitement</cell><cell>49</cell><cell>adventure, enthusiastic, inspired, stimulation, thrilled</cell></row><row><cell>fear</cell><cell>71</cell><cell>afraid, frightened, nightmare, horror, scared, timorous</cell></row><row><cell>sadness</cell><cell>72</cell><cell>bereaved, heartbroken, pessimistic, sadness, unhappy</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 2</head><label>2</label><figDesc>Image numbers of categorical emotions.</figDesc><table><row><cell>amusement</cell><cell>awe</cell><cell cols="2">contentment excitement</cell><cell>positive</cell></row><row><cell>270,748</cell><cell>328,303</cell><cell>181,431</cell><cell>115,065</cell><cell>1,016,186</cell></row><row><cell>anger</cell><cell>disgust</cell><cell>fear</cell><cell>sadness</cell><cell>negative</cell></row><row><cell>29,844</cell><cell>20,962</cell><cell>55,802</cell><cell>57,476</cell><cell>362,400</cell></row><row><cell cols="5">them are not assigned with the emotion categories if no keyword</cell></row><row><cell>is found.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>6. http://en.wikipedia.org/wiki/Information retrieval 7. http://en.wikipedia.org/wiki/Mean squared error 1949-3045 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2016.2628787, IEEE Transactions on Affective Computing</figDesc><table><row><cell cols="3">IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. X, NO. X, OCTOBER 2016</cell><cell>10</cell></row><row><cell></cell><cell></cell><cell>GIST</cell></row><row><cell></cell><cell></cell><cell>Elements</cell></row><row><cell>0.6</cell><cell></cell><cell>Attribute Principles</cell></row><row><cell></cell><cell></cell><cell>ANP</cell></row><row><cell></cell><cell></cell><cell>Expression</cell></row><row><cell>0.4</cell><cell></cell><cell>Fusion</cell></row><row><cell>0.2</cell><cell></cell><cell></cell></row><row><cell>0.0</cell><cell>SV M</cell><cell>NB GM (V ) HG (V ) RM TH G( V)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Fig. 16. The influence of different factors in the proposed method on Recall of 8 emotions.</figDesc><table><row><cell>0.9</cell><cell></cell><cell>SVM</cell><cell>NB</cell><cell>GM(V)</cell><cell>GM</cell></row><row><cell></cell><cell></cell><cell>HG(V)</cell><cell>HG</cell><cell>RMTHG(V)</cell><cell>RMTHG-V</cell></row><row><cell>0.6</cell><cell></cell><cell>RMTHG-L</cell><cell>RMTHG-S</cell><cell>RMTHG-T</cell><cell>RMTHG</cell></row><row><cell>0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.0</cell><cell>Amusement Anger</cell><cell cols="2">Awe Contentment Disgust Excitement</cell><cell>Fear</cell><cell>Sadness</cell></row><row><cell cols="6">Fig. 15. The influence of different factors in the proposed method on</cell></row><row><cell cols="3">Precision of 8 emotions.</cell><cell></cell><cell></cell></row><row><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.0</cell><cell>Amusement Anger</cell><cell cols="2">Awe Contentment Disgust Excitement</cell><cell>Fear</cell><cell>Sadness</cell></row><row><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.0</cell><cell>Amusement Anger</cell><cell cols="2">Awe Contentment Disgust Excitement</cell><cell>Fear</cell><cell>Sadness</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. X, NO. X, OCTOBER 2016</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://sites.google.com/site/schzhao/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was supported by the National Natural Science Foundation of China (No. 61472103, 61133003, 61571269, 61271394, 61671267), and partially supported by the Singapore National Research Foundation under its International Research Centre @ Singapore Funding Initiative and administered by the IDM Programme Office. The authors would also like to thank Wenlong Xie and Xiaolei Jiang both from Harbin Institute of Technology for their valuable suggestion and help.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Object-based visual sentiment concept analysis and application</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="367" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Exploring principles-of-art features for image emotion recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="47" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A critical analysis of rational &amp; emotional approaches in car selling</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sadasivam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Business Research and Management</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="59" to="63" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Predicting elections with twitter: What 140 characters reveal about political sentiment</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tumasjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">O</forename><surname>Sprenger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Sandner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">M</forename><surname>Welpe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International AAAI Conference on Weblogs and Social Media</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="178" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Opinion mining and sentiment analysis</title>
		<author>
			<persName><forename type="first">B</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="1" to="135" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Affective image retrieval via multi-graph learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1025" to="1028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Extracting moods from pictures and sounds: Towards truly personalized tv</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hanjalic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="90" to="100" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Aesthetics and emotions in images</title>
		<author>
			<persName><forename type="first">D</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fedorovskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="94" to="115" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Predicting discrete probability distribution of image emotions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2459" to="2463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A mixed bag of emotions: Model, predict, and transfer emotion distributions</title>
		<author>
			<persName><forename type="first">K.-C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sadovnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="860" to="868" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Predicting continuous probability distribution of image emotions in valence-arousal space</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="879" to="882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Continuous probability distribution prediction of image emotions via multi-task shared sparse regression</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">User interest and social influence based emotion prediction for individuals</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="785" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Can we understand van gogh&apos;s mood? learning to infer affects from images in social networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="857" to="860" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Quantitative study of individual emotional states in social networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C M</forename><surname>Fong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="132" to="144" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Large-scale visual sentiment ontology and detectors using adjective noun pairs</title>
		<author>
			<persName><forename type="first">D</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="223" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Emotionally representative image discovery for social events</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">V</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia Retrieval</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page">177</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">How do your friends on social media disclose your emotions</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="306" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Affective image classification using features inspired by psychology and art theory</title>
		<author>
			<persName><forename type="first">J</forename><surname>Machajdik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="83" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The passions: Emotions and the meaning of life</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Solomon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>Hackett Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The emotions</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">H</forename><surname>Frijda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986">1986</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The secret of happiness: grinning on the internet</title>
		<author>
			<persName><forename type="first">J</forename><surname>Whitfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Brand data gathering from live social media streams</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia Retrieval</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page">169</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Emotional category data on images from the international affective picture system</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Mikels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Fredrickson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Larkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Lindberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Maglio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Reuter-Lorenz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="626" to="630" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Norms of valence, arousal, and dominance for 13,915 english lemmas</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Warriner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kuperman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brysbaert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1191" to="1207" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Expression, perception, and induction of musical emotions: A review and a questionnaire study of everyday listening</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Juslin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Laukka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of New Music Research</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="217" to="238" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Predicting personalized emotion perceptions of social images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1385" to="1394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Analyzing and predicting sentiment of images on the social web</title>
		<author>
			<persName><forename type="first">S</forename><surname>Siersdorfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Minack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hare</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="715" to="718" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Sentribute: image sentiment analysis from a mid-level perspective</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mcdonough</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Workshop on Issues of Sentiment Discovery and Opinion Mining</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Image retrieval by emotional semantics: A study of emotional space and feature extraction</title>
		<author>
			<persName><forename type="first">Y.-L</forename><surname>W.-N. Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Systems, Man and Cybernetics</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="3534" to="3539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Emotional valence categorization using holistic image features</title>
		<author>
			<persName><forename type="first">V</forename><surname>Yanulevskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-K</forename><surname>Herbold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Geusebroek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="101" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">On shape and the computability of emotions</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Suryanarayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Adams</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="229" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Context-aware affective images classification based on bilayer sparse representation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="721" to="724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Three dimensions of emotion</title>
		<author>
			<persName><forename type="first">H</forename><surname>Schlosberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">81</biblScope>
			<date type="published" when="1954">1954</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A connotative space for supporting movie affective recommendation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Benini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Canini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Leonardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1356" to="1370" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Emotion recognition from speech: a review</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Koolagudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Speech Technology</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="117" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning salient features for speech emotion recognition using convolutional neural networks</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2203" to="2213" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Machine recognition of music emotion: A review</title>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">40</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Clustering affective qualities of classical music: beyond the valence-arousal plane</title>
		<author>
			<persName><forename type="first">A</forename><surname>Roda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Canazza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">De</forename><surname>Poli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="364" to="376" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Emotion based image musicalization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo Workshops</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Modeling the affective content of music with a gaussian mixture model</title>
		<author>
			<persName><forename type="first">J.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-K</forename><surname>Jeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="56" to="68" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A multimodal database for affect recognition and implicit tagging</title>
		<author>
			<persName><forename type="first">M</forename><surname>Soleymani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lichtenauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="42" to="55" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Flexible presentation of videos based on affective content analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimedia Modelling</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="368" to="379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Video classification and recommendation based on affective analysis of viewers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="101" to="110" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Liris-accede: A video database for affective content analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Baveye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dellandrea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chamaret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="43" to="55" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Video affective content analysis: a survey of state of the art methods</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="410" to="430" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Filtering of brand-related microblogs using social-smooth multiview embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2115" to="2126" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Cascading outbreak prediction in networks: a data-driven approach</title>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD Conference on Konwledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="901" to="909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Scalable event-based clustering of social media via record linkage techniques</title>
		<author>
			<persName><forename type="first">T</forename><surname>Reuter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cimiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Drumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Buza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International AAAI Conference on Weblogs and Social Media</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Multimedia social event detection in microblog</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimedia Modelling</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="269" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Semantics-preserving hashing for cross-view retrieval</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3864" to="3872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Predicting viewer affective comments based on image content in social media</title>
		<author>
			<persName><forename type="first">Y.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><forename type="middle">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia Retrieval</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page">233</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Unified video annotation via multigraph learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="733" to="746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning with hypergraphs: Clustering, classification, and embedding</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1601" to="1608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Music recommendation by unified hypergraph: combining social media information and music content</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="391" to="400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">3-d object retrieval and recognition with hypergraph analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4290" to="4303" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Image retrieval via probabilistic hypergraph ranking</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="3376" to="3383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cuthbert</surname></persName>
		</author>
		<title level="m">International affective picture system (IAPS): Affective ratings of pictures and instruction manual. NIMH, Center for the Study of Emotion &amp; Attention</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Active query sensing for mobile location search</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="3" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Social influence analysis and application on multimedia sharing websites</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Multimedia Computing, Communications, and Applications</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">53</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">An argument for basic emotions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition &amp; emotion</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="169" to="200" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">What are emotions? and how can they be measured?</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social science information</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="695" to="729" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Affective video content representation and modeling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hanjalic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-Q</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="143" to="154" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Sun database: Large-scale scene recognition from abbey to zoo</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="3485" to="3492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Sun attribute database: Discovering, annotating, and recognizing scene attributes</title>
		<author>
			<persName><forename type="first">G</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2751" to="2758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Designing category-level attributes for discriminative visual recognition</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="771" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Start from scratch: Towards automatically identifying, modeling, and naming visual attributes</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="187" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Learning predictable and discriminative attributes for visual recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3783" to="3789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ambadar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="94" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Exploring facial expressions with compositional features</title>
		<author>
			<persName><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2638" to="2644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Robust real-time face detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="154" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Word image matching using dynamic time warping</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Rath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="521" to="527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Affective audio-visual words and latent topic driving model for realizing movie affective scene classification</title>
		<author>
			<persName><forename type="first">G</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Satou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kojima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="523" to="535" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Anger is more influential than joy: Sentiment correlation in weibo</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS One</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">e110184</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Predicting the duration of emotional experience: two experience sampling studies</title>
		<author>
			<persName><forename type="first">P</forename><surname>Verduyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Delvaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Van Coillie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tuerlinckx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Van Mechelen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Emotion</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="83" to="91" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Which emotions last longest and why: The role of event importance and rumination</title>
		<author>
			<persName><forename type="first">P</forename><surname>Verduyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lavrijsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Motivation and Emotion</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="119" to="127" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Recovery from severe mental illness, a gender perspective</title>
		<author>
			<persName><forename type="first">U.-K</forename><surname>Schön</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scandinavian Journal of Caring Sciences</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="557" to="564" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Social influences on the emotion process</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Manstead</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zaalberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Review of Social Psychology</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="171" to="201" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
