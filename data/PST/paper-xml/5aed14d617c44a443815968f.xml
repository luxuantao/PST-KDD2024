<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IEEE Transactions on Image Processing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">L</forename><surname>He</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">G</forename><surname>Wang</surname></persName>
							<email>ghwang@ku.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">CAS Center for Excellence in Brain Science and Intelligence Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Department of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">University of Kansas</orgName>
								<address>
									<postCode>66045</postCode>
									<settlement>Lawrence</settlement>
									<region>KS</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">IEEE Transactions on Image Processing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">635C1B7AC59439CEF4E8DA589055D7CC</idno>
					<idno type="DOI">10.1109/TIP.2018.2832296</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learning Depth from Single Images with Deep</head><p>Neural Network Embedding Focal Length Lei He, Guanghui Wang (Senior Member, IEEE) and Zhanyi Hu</p><p>Abstract-Learning depth from a single image, as an important issue in scene understanding, has attracted a lot of attention in the past decade. The accuracy of the depth estimation has been improved from conditional Markov random fields, non-parametric methods, to deep convolutional neural networks most recently. However, there exist inherent ambiguities in recovering 3D from a single 2D image. In this paper, we first prove the ambiguity between the focal length and monocular depth learning, and verify the result using experiments, showing that the focal length has a great influence on accurate depth recovery. In order to learn monocular depth by embedding the focal length, we propose a method to generate synthetic varying-focal-length dataset from fixed-focal-length datasets, and a simple and effective method is implemented to fill the holes in the newly generated images. For the sake of accurate depth recovery, we propose a novel deep neural network to infer depth through effectively fusing the middle-level information on the fixed-focal-length dataset, which outperforms the state-of-the-art methods built on pretrained VGG. Furthermore, the newly generated varying-focallength dataset is taken as input to the proposed network in both learning and inference phases. Extensive experiments on the fixed-and varying-focal-length datasets demonstrate that the learned monocular depth with embedded focal length is significantly improved compared to that without embedding the focal length information.</p><p>Index Terms-depth learning, single images, inherent ambiguity, focal length</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Scene depth inference from a single image is currently an important issue in machine learning <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. The underlying rationale of this problem is the possibility of human depth perception from single images. The task here is to assign a depth value to every single pixel in the image, which can be considered as a dense regression problem. Depth information can benefit many challenging computer vision problems, such as semantic segmentation <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, pose estimation <ref type="bibr" target="#b7">[8]</ref>, and object detection <ref type="bibr" target="#b8">[9]</ref>.</p><p>During the past decade, significant effort has been made in the research community to improve the performance of monocular depth learning, and significant accuracy has been achieved thanks to the rapid development and advances of deep neural networks. However, most available methods overlook one key problem: the ambiguity between the scene depth and the camera's focal length. Because the 3D-to-2D object imaging process must satisfy some strict projective geometric relationship, however, without prior knowledge on the camera's intrinsic parameters, it is impossible to infer the true depth from a single image.</p><p>In this paper, in order to remove the ambiguity caused by the unknown focal length, we propose a novel deep neural network to learn the monocular depth by embedding the focal length information. However, the datasets used in most machine learning methods are all of fixed-focal-length, such as the NYU dataset <ref type="bibr" target="#b9">[10]</ref>, the Make3D dataset <ref type="bibr" target="#b0">[1]</ref>, and the KITTI dataset <ref type="bibr" target="#b10">[11]</ref>. To prepare for learning monocular depth with focal length, datasets with varying focal lengths are required so that the cameras intrinsic information should be taken into account in both the learning and the inference phases. However, considering the labor in building a new varyingfocal-length dataset, it is desirable to transform the existing fixed-focal-length datasets into those of varying-focal-length. we first introduce a method to generate varying-focal-length dataset from fixed-focal-length dataset, like Make3D and NYU v2, and a simple and effective method is proposed to fill the holes produced during the image generation. The transformed datasets are demonstrated to make great contribution in depth estimation.</p><p>In order to learn fine-grained monocular depth with focal length, we propose an effective neural network to predict accurate depth, which achieves competitive performance as compared with the state-of-the-art methods, and further embedding the focal length information into the proposed model. In the literature, almost all works for pixel-wise prediction exploit an Encoder-Decoder network <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref> to infer the labels of pixels. To predict accurate labels, two general attempts have been made to address the problem. One is to integrate middle layer features <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, the other is to effectively exploit the multi-scale information and the decoder side outputs <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>. Inspired by the idea of fusing the middle-level information, we propose a novel endto-end neural network to learn fine-grained depth from single images with embedded focal length. The proposed network is composed of four parts: the first part is built on the pretrained VGG models, followed by the global transformation layer and upsampling architecture to produce depth with high resolution, the third part effectively integrates the middlelevel information to infer the structure details, converting the middle-level information to the space of the depth, and the last part embeds the focal length into the global information.</p><p>The proposed network is extensively evaluated on the Make3D, NYU v2, and KITTI datasets. We first perform the experiments without the embedded focal length, and better performance than the state-of-the-art techniques is achieved in both quantitative and qualitative terms. Then, it is further evaluated with the embedded focal length on the newly generated varying-focal-length datasets for comparison. The experimental results show that depths inferred from the model with embedded focal length significantly outperform those without the focal length in all error measures, it also demonstrates that the focal length information is very useful for the depth extraction from a single image.</p><p>In summary, the contributions of this paper are four-fold. First, we prove that the ambiguity between the focal length and the depth estimation from a single image, and further demonstrate the result using real images. Second, we propose a method to generate varying-focal-length images from fixedfocal-length images, which are visually plausible. Third, based on the classical Encoder-Decoder network, a novel neural network model is proposed to learn the fine-grained depth from single images, by virtue of effectively fusing the middlelevel information. Finally, given the newly generated varyingfocal-length datasets, we revise the fine-grained network by embedding the focal length information. The experimental evaluation shows that the depth inference with known focal length achieves significantly better performance than the one without the focal length information. The source code and the generated datasets will be available on the authors website.</p><p>The rest of this paper is organized as follows: Section II introduces the related works. The ambiguity between the focal length and monocular depth estimation is discussed in Section III. Section IV describes the generating process from fixed-focal-length dataset to varying-focal-length dataset. The proposed fine-grained network embedding focal length information is elaborated in Section V, and the experimental results on the four datasets are reported in Section VI. The paper is concluded in Section VII.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Depth extraction from single images has received a lot of attention in recent years, while it remains a very hard problem due to the inherent ambiguity. To tackle this problem, classic methods <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref> usually make strong geometric assumptions that the scene structure consists of horizontal planes, vertical walls and superpixels, employing the Markov random field (MRF) to infer the depth by leveraging the handcrafted features. One of the first work, proposed by Hoiem et al. <ref type="bibr" target="#b19">[20]</ref>, creates realistic-looking reconstructions of outdoor images by assuming planar scene composition. In <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, simple geometric assumptions have proven to be effective in estimating the layout of a room. In order to improve the accuracy of the depth-based methods, Saxena et al. <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref> utilized MRF to infer depth from both local and global features extracted from the image. In addition, superpixels <ref type="bibr" target="#b27">[28]</ref> are introduced in the MRF formulation to enforce neighboring constraints. The work has also been extended to 3D reconstruction of scenes <ref type="bibr" target="#b0">[1]</ref>.</p><p>Non-parameter algorithms <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref> are another kind of classical methods for learning the depth from a single image, relying on the assumption that the similarities between regions in the RGB images imply similar depth cues as well. After clustering the training dataset based on the global features (e.g. GIST <ref type="bibr" target="#b31">[32]</ref>, HOG <ref type="bibr" target="#b32">[33]</ref>), these methods first search the candidate RGB-D of the input RGB image in the feature space, then, the candidate pairs are warped and fused to obtain the final depth. Karsch et al. <ref type="bibr" target="#b1">[2]</ref> proposed a depth transfer method to warp the retrieved RGB-D using SIFT flow <ref type="bibr" target="#b28">[29]</ref>, followed by a global optimization framework to smooth the resulting depth. He et al. <ref type="bibr" target="#b33">[34]</ref> employed a sparse SIFT flow to speed up the depth inference based on the work <ref type="bibr" target="#b1">[2]</ref>. Konrad et al. <ref type="bibr" target="#b29">[30]</ref> computed a median over the retrieved depth maps followed by cross-bilateral filtering for smoothing. Instead of warping the retrieved candidates, Liu et al. <ref type="bibr" target="#b30">[31]</ref> explored continuous variables to represent the depth of image superpixels and discrete ones to encode relationships between neighboring superpixels, formulating the depth estimation as an optimization problem of the discretecontinuous graphical model. For learning the indoor depth, Zhuo et al. <ref type="bibr" target="#b34">[35]</ref> exploited the structure of the scene at different levels to learn depth from a single image.</p><p>Recently, convolutional neural networks have seen remarkable advances in the high-level problems of computer vision, which have also been applied with great success to depth extraction from single images <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. There exist two major approaches for the task of depth estimation in the related references: multi-scale training technique and super-pixel pooling with conditional random field (CRF) algorithm. In order to accelerate the convergence of the parameters during the training phase, most of the works are built upon winning architectures of the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) <ref type="bibr" target="#b40">[41]</ref>, often initializing their networks with Alex <ref type="bibr" target="#b41">[42]</ref>, VGG <ref type="bibr" target="#b42">[43]</ref>, or ResNet <ref type="bibr" target="#b43">[44]</ref>. Eigen et al. <ref type="bibr" target="#b35">[36]</ref> first addressed this issue by fusing the depths from the global network and refined network. Their work later was extended to use a multi-scale convolutional network to predict depth, normal and semantic label from a single image in a deeper neural network <ref type="bibr" target="#b2">[3]</ref>. Other methods to obtain the fine-grained depth leveraged the representation of the neural network and the inference of the CRFs. Liu et al. <ref type="bibr" target="#b36">[37]</ref> presented a deep convolutional neural field model based on fully convolutional networks and a novel superpixel pooling method, combining the strength of deep CNN and the continuous CRF into a unified CNNs framework. Li et al. <ref type="bibr" target="#b37">[38]</ref> and Wang et al. <ref type="bibr" target="#b38">[39]</ref> leveraged the benefit of the hierarchical CRFs to refine their patch-wise predictions from superpixel down to pixel level. Roy et al. <ref type="bibr" target="#b39">[40]</ref> combined random forests and convolutional neural networks to tackle the depth estimation. Laina et al. <ref type="bibr" target="#b3">[4]</ref> built a neural network on ResNet, followed by designed up-sampling blocks to obtain high resolution depth. However, the middle-level features are not fused into the network to obtain detailed information of the depth. Based on the multi-scale network <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b2">[3]</ref>, Dan et al. <ref type="bibr" target="#b4">[5]</ref> effectively exploited the side outputs of deep networks to infer depth by reformulating the continuous CRFs of the monocular depth estimation as sequential deep networks. For all these depth learning methods, the experimental datasets are usually created by Kinect or laser scanner, where the RGB camera has a fixed focal length. In other words, currently the available depth datasets in the literature are all of fixed-focal-length. However, there exists an inherent ambiguity between monocular depth estimation and focal length, as described in our work <ref type="bibr" target="#b44">[45]</ref>. Without knowing the camera's focal length, the depth can not be truly estimated from a single image. In order to remove the ambiguity, the camera's focal length should be considered in both depth learning and inference phases. In the following section, we will discuss the inherent ambiguity in depth recovery in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. INHERENT AMBIGUITY</head><p>Scene depth refers to the distance from the camera optical center to the object along the optical axis. In deep learning based methods for the monocular depth estimation, the depth of each pixel is inferred by fusing global and contextual information, extracted from the corresponding receptive fields in the input image, followed by affine transformations and non-linear operations, as illustrated by the following equation.</p><formula xml:id="formula_0">D i = σ n (w n (• • • σ 1 (w 1 x RF i + b 1 ) • • • ) + b n )<label>(1)</label></formula><p>where D i is the depth of the pixel i, x RF i is the receptive field corresponding to the pixel i in the depth map, σ is the activation function and w, b are the parameters of the models. In order to extract long range global information, the deep neural networks were introduced in the research community for monocular depth estimation. However, the deeper networks are very hard to train due to the vanishing gradient or exploding gradient. In addition, it may lead to another problem about the receptive fields. Note that for a specific network architecture, we can infer the theoretical receptive field associated with the output node in every layer. However, the contribution of various regions in the theoretical receptive field is not the same. To explore the role of each pixel location in the view-of-field, we propose a novel method to visualize the effective receptive field, as shown in Figure <ref type="figure" target="#fig_0">1</ref>. From the output layer to the input layer, the counts of per pixel evolved in the convolution operation is obtained layer by layer.</p><p>In current nets of depth estimation from single images, the convolution operation usually adopts the technique of sharing weights in each channel, and the weights are initialized by sampling a Gaussian with zero mean and 0.01 variance. Once the network is trained, the parameters within each channel are fixed and shared. In addition, the number of use of each pixel for the final prediction could describe the complexity of the combination of network weights at each layer, including affine transformation and non-linear operation. The higher complexity of the combination, the better ability to character the problem of the corresponding task. In a statistical sense, this number represents that the pixel information is frequently used in monocular depth estimation, regardless of the weights, which makes it able to view the contribution of each pixel. It is observed that the deeper the depth of the network, the larger the value in the middle of the receptive field, while the one along the edge is in the opposite, which reveals that the actual receptive field is smaller than the theoretical receptive field, and it also obeys the Gaussian distribution as described in Luo et al. <ref type="bibr" target="#b45">[46]</ref>. In order to enlarge the view-of-field in the specific network, a fully connected layer is a better choice when the resolution of the feature maps is very small.</p><p>The methods for monocular depth estimation are based on the assumption that the similarities between regions in the RGB images imply similar depth cues as well. There exists an inherent ambiguity between the focal length and the scene depth learned from a single image, as analyzed in the following.</p><p>Based on the imaging principle, the image of an object projected by a long-focal-length camera in the far distance could be exactly the same as the one captured by a shortfocal-length camera at a short distance. This is called the indistinguishability between the scene depth and the focal length in images. For the sake of convenience, we assume that the imaging model of the camera is the pinhole model without loss of generality. For simplicity, assume the space object is linear, as shown in Figure <ref type="figure" target="#fig_1">2</ref>. The images of the planar space object S under (f 1 , O 1 ) and (f 2 , O 2 ) are I 1 , I 2 respectively, where I 1 = I 2 . As a result, we are not able to infer the real depth without camera focal length from its projected image, since To demonstrate the ambiguity between the depth and the focal length, we collected 250 image pairs in the laboratory setting with approximate context. These images are captured by the same camera at two different focal lengths: 50 mm and 105 mm, where the actual depth difference between the two images in each group is at least 1 m. Then, we employ Liu et al. <ref type="bibr" target="#b36">[37]</ref> and Eigen et al. <ref type="bibr" target="#b2">[3]</ref> methods to infer the depth of the above dataset. Some experimental results are shown in Figure <ref type="figure" target="#fig_3">4</ref>. By human-computer interaction method, the depths of the image pairs with two focal length are measured, as shown in Figure <ref type="figure" target="#fig_2">3</ref>. The focal length of the left image is 105 mm, and the right one is 50 mm. Given the depths inferred by Liu et al. <ref type="bibr" target="#b36">[37]</ref>, the matching regions of the fixed object are selected to compute the average depth. The experiment shows that the average depth difference is 0.07506 m, while the actual depth difference between the two images is 2 m. By this measure, we take Liu et al. <ref type="bibr" target="#b36">[37]</ref> and Eigen et al. <ref type="bibr" target="#b2">[3]</ref> methods to evaluate the collected dataset, as reported in Table <ref type="table" target="#tab_1">I</ref>, the corresponding error rates are 89.76% and 87.2% respectively. The experiments demonstrate that there exists inherent ambiguity between the focal length and the scene depth learned from single images.   </p><formula xml:id="formula_1">I 1 = I 2 , D 1 ̸ = D 2 , as shown in Figure 2.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. DATASET TRANSFORMATION</head><p>In order to remove the above ambiguity, the camera's intrinsic parameter should be taken into account in the depth learning from single images, at least the focal length information should be used as input in both training and testing phases. However, all available depth datasets (like Make3D and NYU v2) in the literature are of fixed focal length. In order to remove the ambiguities caused by the focal length, we propose an approach to transform a fixed-focal-length dataset into a varying-focal-length dataset. The pipeline of the proposed approach is shown in Figure <ref type="figure" target="#fig_4">5</ref>, and the implementation details of the dataset transformation is described in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Varying-focal-length image generation</head><p>As shown in Figure <ref type="figure" target="#fig_4">5</ref>, given the camera's intrinsic parameters and the corresponding RGB-D image, the imaging process can be formulated as: </p><formula xml:id="formula_2">u v 1 3 5 = 2 4 f 0 u 0 0 f v 0 0 0 1 3 5 2 4 X Y Z 3 5 (2)</formula><p>where (u 0 , v 0 ) is the principle point, f is the focal length, Z is the corresponding depth value, and (X, Y, Z) is the 3D space point in the camera system corresponding to the image pixel (u, v).</p><p>To transform the 3D points from the original camera coordinate to a new system, a translation and a rotation are performed according to 2 4</p><formula xml:id="formula_3">X ′ Y ′ Z ′ 3 5 = R 2 4 X Y Z 3 5 -t (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>where R is the rotation matrix, and t is the translation vector. As shown in Figure <ref type="figure" target="#fig_4">5</ref>, the camera coordinate system  By specifying a new focal length, or new camera's intrinsic matrix, the transformed 3D scene points can be projected to new image points. During the process of reprojection, multiple 3D points along the ray may be projected to the same image pixel, such as the 3D points (P, Q) and pixel (u ′ , v ′ ) in Figure <ref type="figure" target="#fig_4">5</ref>. To solve this issue, we only project the 3D point with the smallest depth value, since other points are occluded by the nearest one. To obtain a real image, the new image points are quantized, and the RGB values are taken from the corresponding original image.</p><formula xml:id="formula_5">(O, X, Y, Z) is transformed to a new system (O ′ , X ′ , Y ′ , Z ′ ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Post-processing of the generated varying-focal-length datasets</head><p>After the above operations, some holes are produced in the generated RGB-D image, as shown in Figure <ref type="figure" target="#fig_5">6</ref>. By analyzing the shapes and properties of the holes, we propose a simple yet effective method to fill these holes. First, we locate the positions of the empty holes, and then design 3 × 3 binary filters to fill them. The experimental holes are filled by the corresponding binary templates, which are mainly classified into three classes, as shown in Figure <ref type="figure" target="#fig_6">7</ref>, where number 0 represents the hole pixel, and number 1 represents pixel without hole.</p><p>For class-a, a 4-neighborhood binary template is employed for mean interpolation. For class-b, we directly use the corresponding 3 × 3 templates for mean interpolation. For class-c, the template elements all equal to zero, we iteratively perform interpolation by virtue of the intermediate interpolation results as follows: Since the iteration scheme is from left to right, and top to bottom, at least one of the two pixels at m and n has been interpolated by the previous iteration, then the (RGB-D) value at pixel k is assigned to either that at m or n with a chance.</p><p>Through the above proposed filtering process, the projected holes could be filled. Some filling results are shown in Figure <ref type="figure" target="#fig_5">6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implementation details</head><p>Based on extensive experiments, we find that a reasonable range of the rotation angle should be within[-5 • , 5 • ]. Upon completion of the rotation, if the center of the new image coincide with the original one, the translation vector (C x , C y , C z ) is computed as follow.</p><p>If the rotation is around the y axis by angle β, we set 8 &gt; &gt; &gt; &lt; &gt; &gt; &gt; :</p><formula xml:id="formula_6">C y = 0 C x = 1 N (X - X + Z sin β cos β ) C z = Z - f ′ Z f cos β + (X -C x ) tan β (<label>4</label></formula><formula xml:id="formula_7">)</formula><p>where N is the number of 3D points, and f ′ is the assigned new focal length.</p><p>If the rotation is around the x axis by angle α, we set 8 &gt; &gt; &gt; &lt; &gt; &gt; &gt; :</p><formula xml:id="formula_8">C x = 0 C y = 1 N (Y - Y -Z sin α cos α ) C z = Z - f ′ Z f cos α -(Y -C y ) tan α (5)</formula><p>Using the above proposed approach, we have transformed the NYU dataset and the Make3D dataset into the new varyingfocal-length datasets (VFL). According to the equations ( <ref type="formula">2</ref>) and (3), the depth map of the transformed images are generated by strict geometric relationship. In the stage of quantization, some holes are introduced. However, the hole portion of the depth map is very small as shown in Figure <ref type="figure" target="#fig_5">6</ref>, benefiting from the completion technique in equations ( <ref type="formula" target="#formula_6">4</ref>) and <ref type="bibr" target="#b4">(5)</ref>. By making use of contextual information, the holes of the depth map are filled with the proposed filtering method, which approaches to the ground truth (f = 580) in visualization.</p><p>Figure <ref type="figure" target="#fig_7">8</ref> shows two examples of the newly generated images from the Make3D dataset and the NYU dataset. For the generated VFL datasets, the focal-length values are 460, 500, 540, 620, 660, and 700 pixels, respectively, where the value of the initial focal length is 580. From the results we can see that the generated database is geometrically reasonable by visual verification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. LEARNING MONOCULAR DEPTH WITH DEEP NEURAL NETWORK</head><p>In this section, based on the varying-focal-length datasets, we propose a new model to learn depth from a single image by embedding focal length information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Network Model</head><p>The current DNN architectures are mostly built on the network <ref type="bibr" target="#b46">[47]</ref> for digit recognition, which consists of convolution, pooling, and fully connected layers. The essential power behind the remarkable success is that the framework selects the invariant abstract features which are suitable for the high-level problem. For pixel-wise depth prediction, in order to remedy the resolution loss caused by the convolution striding or pooling operations, some techniques are proposed, such as the deconvolution or upsampling methods <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. Since these operations are usually applied on the last convolutional layer, it is very hard to accurately restore spacial structure information. In order to obtain pixel-wise fine-grained results, the classical skip connection is exploited, as described in the U-Net <ref type="bibr" target="#b15">[16]</ref> and the FCN <ref type="bibr" target="#b13">[14]</ref>. For monocular depth learning, since the distribution of the depth is different from the one of the category from pre-trained model, we propose a novel transfer network (T-net), which converts feature maps from the category cues to the depth mapping, rather than utilizing feature maps directly from previous layers.</p><p>Our proposed network can be efficiently trained in an endto-end manner, which is symmetrical on the middle network layer, as illustrated in Figure <ref type="figure">9</ref>. The first part of the network is based on the VGG network, which is initialized with the corresponding pre-trained weights. The second part of our architecture consists of the global transfer layer and upsampling architectures, which leads to the global information transformed from the category cues to the depth mapping and gain high resolution depth respectively. The third part of the network are T-nets, which effectively convert the middle-level information to meet the distribution of the monocular depth. The last part of our architecture are three fully connected layers for embedding the focal length information. Here, we first use the focal length to generate seven same digits, which Fig. <ref type="figure">9</ref>: The proposed network architecture. Our neural network is built upon the pre-trained model (VGG), followed by a fully connection layer and upsampling architectures to obtain high-resolution depth, by effectively integrating the middle-level information. In addition, focal length is embedded in the network by the encoding mode.</p><p>are then connected to 64 and 512 nodes layer by layer, and finally the 512 nodes are concatenated with the global information.</p><p>For the sake of effectively fusing the middle-level information, we divide the pre-trained VGG network into 5 blocks according to the resolutions of the feature maps, as shown in the left green blocks in Figure <ref type="figure">9</ref>. The depth of the neural networks is important for depth estimation, as described in Laina et al. <ref type="bibr" target="#b3">[4]</ref>. That means the deeper the depth of the network, the more beneficial to improving the accuracy of the depth extraction. However, very deep network may lead to a result that the actual receptive field is smaller than the theoretical receptive field, as illustrated in section III. Inspired by this observation, we propose a fully connected layer to bridge the subsampling module and the upsampling module, which is able to obtain full-resolution receptive field and convert the global information from category to depth simultaneously. To obtain the high resolution depth, we follow the work described in <ref type="bibr" target="#b47">[48]</ref> by introducing the unpooling layers, which maps each pixel into the top-left corner of a 2 × 2 (zero) kernel to double the feature map sizes, followed by a convolution implementation to fuse information, as shown in the Upconv X architecture in Figure <ref type="figure">9</ref>.</p><p>To effectively exploit the middle layer features, we propose the T-net architecture, inspired by the ResNet <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b48">[49]</ref> and Highway <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b50">[51]</ref>, to facilitate the detailed structural information propagation during both the forward and the backward stages. The identity mapping with shortcuts can facilitate the optimization of the deep network, since it iteratively generates small magnitudes of responses by passing main information layer by layer, in analogy to Taylor series. While the global information is propagated through the architecture of the first part and the second part, we utilize the T-nets to transform the detailed information in the third part. The first layer of the per T-net removes the redundant information by reducing the channels of the networks, followed by another layer to convert the feature cues. The feature maps from the T-net are concatenated with the corresponding features generated from the previous layer in the second part, followed by the unpooling and convolution operations to remedy the low resolution. As illustrated in Figure <ref type="figure">9</ref>, the feature maps in pink color are generated from the previous layer, and the feature maps in green color are the transformed middle-level information through the T-net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Loss function</head><p>The parameters of the proposed network are learned through minimizing the loss function defined on the prediction and the ground truth. In general, the mean squared error (MSE) loss is taken as the standard loss, which minimizes the squared Euclidean norm between the predictions y and the ground truth y * :</p><formula xml:id="formula_9">l M SE (y -y * ) = 1 N X yi∈|N | ∥y i -y * i ∥ 2 2 (<label>6</label></formula><formula xml:id="formula_10">)</formula><p>where N is the number of valid pixels in the batch-size training images.</p><p>Although MSE struggles to handle the uncertain inherence in recovering lost high-frequency details, minimizing MSE encourages finding pixel-wise averages of plausible solutions, leading to blurred predictions as described in <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b53">[54]</ref>. To solve this issue, L1 yields better detail than L2 norm. Based on our experimental study, we find that the error of depth at distant is larger than that at a close distance. Inspired by the observation, a weighted loss function is introduced by penalizing the pixels with large errors. We propagate large gradients in the locations of large errors during the training phase, which coincide with the gradient propagation of the L2 norm. As described in Zwald and Lambert-Lacroix <ref type="bibr" target="#b54">[55]</ref>, the BerHu loss function is appropriate for the above phenomena, which consists of L2 and L1 norms. Following Laina et al. <ref type="bibr" target="#b3">[4]</ref>, we take the BerHu loss as the error function as below by integrating the advantages of both the L2 norm and L1 norm, resulting in accelerated optimization and detailed structure.</p><formula xml:id="formula_11">B(y -y) = ¨|y -y| |y -y| &lt; c (y-y) 2 +c 2 2c</formula><p>|y -y| &gt; c <ref type="bibr" target="#b6">(7)</ref> where c = 0.05max i (|y i -y i |), and i indexes the pixels in the current batch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXPERIMENTS</head><p>To demonstrate the effectiveness of the proposed deep neural network and the embedded focal length for monocular depth estimation, we carry out comprehensive experiments on four publically available datasets and the synthetic datasets generated in this paper: NYU v2 <ref type="bibr" target="#b9">[10]</ref>, Make3D <ref type="bibr" target="#b0">[1]</ref>, KITTI <ref type="bibr" target="#b55">[56]</ref>, the varying-focal-length datasets generated from section IV and SUNRGBD <ref type="bibr" target="#b56">[57]</ref>. In the following subsections, we report the details of our implementation and the evaluation results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental setup</head><p>Datasets. The NYU Depth v2 <ref type="bibr" target="#b9">[10]</ref> consists of 464 scenes, captured using a Microsoft Kinect. Followed by the official split, the training dataset is composed of 249 scenes with the 795 pair-wise images, and the testing dataset includes 215 scenes with 654 pair-wise images. In addition, the raw dataset contains 407,024 new unlabeled frames. For data augmentation, we sample equally-spaced frames out of each raw training sequence, and further align the RGB-D pairs by virtue of the provided toolbox, resulting in approximately 4k RGB-D images.</p><p>Then, the sampled raw images and 795 pair-wise images are online augmented by Eigen et al. <ref type="bibr" target="#b35">[36]</ref>. The input images and the corresponding depths are simultaneously transformed using small scaling, color transformations and flips with a chance of 0.5, then we randomly crop the augmented images and depths down to the desired size of the network. Note that the following datasets are also online augmented by the same strategy. As a result, the magnitude of samples after data augmentation on NYU depth is about 48k, which is far less than 2M for coarse network, and 1.5M for fine network, as described in Eigen et al. <ref type="bibr" target="#b35">[36]</ref>. Due to the hardware limitation, we down-sample the original frames from size 640×480 pixels to 320 × 224 as the input to the network.</p><p>The Make3D dataset <ref type="bibr" target="#b0">[1]</ref> contains 400 training images and 134 testing images of outdoor scenes, generated from a custom 3D laser scanner. While the depth map resolution of the ground truth is only 305×55, not matching the corresponding original RGB images with 1704 × 2272 pixels, we resize all RGB-D images to 345 × 460 by preserving the aspect ratio of the original images. Due to the neural network architecture and hardware limitations, we subsample the resolution of the RGB-D images to 160 × 224.</p><p>The KITTI dataset <ref type="bibr" target="#b55">[56]</ref> contains 93k depth maps with corresponding raw LiDaR scans and RGB images. Following the suggestion in Uhrig et al. <ref type="bibr" target="#b55">[56]</ref>, the training dataset is composed of 86k pair-wise images, and the testing dataset includes 1k pair-wise images selected from the full validation split. Since the LiDAR returns no measurements to the upper part of the images, we only use the bottom two thirds of the images to produce a fixed crop size of 960 × 224. In order to reduce the load of computation, we randomly crop the images from the resolution 960 × 224 to 320 × 224 during the training stage.</p><p>The varying-focal-length (VFL) datasets contain two datasets: VFL-NYU and VFL-Make3D, which are the varyingfocal-length datasets from NYU Depth v2 and Make3D respectively, as described in section IV. For VFL-NYU, the training dataset and testing dataset of every focal length are split in the official manner. Following the above NYU data argumentation, we perform the training samples argumentation using the same manner without considering the raw unaligned frames, producing approximate 30k training pairs in total. As for VFL-Make3D dataset, we implement the same operations with the above Make3D dataset, resulting in about 17k training pairs.</p><p>The SUNRGBD dataset <ref type="bibr" target="#b56">[57]</ref> contains 10,335 RGB-D images, at a similar scale as PASCAL VOC, which is captured by four different sensors -Intel RealSense 3D Camera for tablets, Asus Xtion LIVE PRO for laptops, and Microsoft Kinect versions 1 and 2 for desktop. The dataset, although constructed of various focal lengths, it is different with the dataset generated by our VFL approach. In our approach, the varying-focal-length datasets are generated from the fixedfocal-length datasets, the images with varying focal lengths are of the same scene, while in the SUNRGBD dataset, different focal-length images correspond to different scenes. In addition, the SUNRGBD dataset contains more distortion parameters caused by the four different sensors. Following the official split, the training dataset is composed of 5285 pairwise images, and the testing dataset includes 5050 pair-wise images. In the following experiments, we sample frames out of the source dataset, resulting in 2642 pair-wise training images and 1010 pair-wise test images.</p><p>Evaluation Metrics. For quantitative evaluation, we report errors obtained with the following extensively adopted error metrics.</p><p>• Average relative error: </p><formula xml:id="formula_12">rel = 1 N P yi∈|N | |yi-y * i | y * i • Root mean squared error: rms = È 1 N P yi∈|N | |y i -y * i |</formula><formula xml:id="formula_13">y * i yi , yi y * i ) = δ &lt; t(t ∈ [1.25, 1.25 2 , 1.25 3 ])</formula><p>where y i is the estimated depth, y * i denotes the corresponding ground truth, and N is the total number of valid pixels in all 1057-7149 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.   images of the validation set. Implementation Details. We use TensorFlow <ref type="bibr" target="#b57">[58]</ref> deep learning framework to implement the proposed network, and train the network on a single NVIDIA GeForce GTX TITAN with 12GB memory. The objective function is optimized using the Adam method <ref type="bibr" target="#b58">[59]</ref>. During the initialization stage, weight layers in the first part of the architecture are initialized using the corresponding model (VGG) pre-trained on the ILSVR-C <ref type="bibr" target="#b40">[41]</ref> dataset for image classification. The weights of newly added network are assigned by sampling a Gaussian with zero mean and 0.01 variance, and the learning rate is set at 0.0001. Finally, our model is trained with a batch size of 8 for about 20 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Analysis of the different architectures and loss functions</head><p>In the first series of experiments we focus on the NYU Depth v2 <ref type="bibr" target="#b9">[10]</ref> dataset. The proposed model is evaluated and compared with other classical architectures and training loss functions. Specifically, we conduct the following experiments for comparison: (i) T-net and skip connection, (ii) BerHu loss and L1 loss, (ii) fully convolution (GIL-convolution) and fully connected (GIL-connected) as global information layer for bridging downsampling part and upsampling part. The results of experimental comparisons are reported in Table <ref type="table" target="#tab_4">II</ref>. It is evident that the model with the T-net achieves better performance than the one with standard skip connection.</p><p>The table also compares the proposed model with BerHu loss and L1 loss, respectively. As expected, the model with BerHu loss yields more accurate depth. Finally, we analyze Method Error (lower is better) Accuracy (higher is better) rel rms log 10 1.25 1.25 2 1.25 3 Karsch et al. <ref type="bibr" target="#b1">[2]</ref> 0.374 1.12 0.134 0.447 0.745 0.897 Liu et al. <ref type="bibr" target="#b30">[31]</ref> 0.335 1.06 0.127 ---Li et al. <ref type="bibr" target="#b37">[38]</ref> 0.232 0.821 0.094 ---Liu et al. <ref type="bibr" target="#b36">[37]</ref> 0.230 0.824 0.095 0.614 0.883 0.975 Wang et al. <ref type="bibr" target="#b38">[39]</ref> 0.220 0.745 0.094 0.605 0.890 0.970 Eigen et al. <ref type="bibr" target="#b35">[36]</ref> 0.215 0.907 -0.611 0.887 0.971 R. and T. <ref type="bibr" target="#b39">[40]</ref> 0.187 0.744 0.078 ---E. and F. <ref type="bibr" target="#b2">[3]</ref> 0   the impact of the GIL to the accuracy of the monocular depth, by comparing GIL-convolution and the GIL-connected. It is evident that the depth performance improves with the increase of the size of receptive field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparisons with the state-of-the-art</head><p>We also compared our method with the state-of-the-art approaches on NYU v2, Make3D and KITTI datasets. For the baselines, we reproduced the algorithms of VGG-Laina et al. <ref type="bibr" target="#b3">[4]</ref> and multi-scale Eigen, Fergus <ref type="bibr" target="#b2">[3]</ref> built on VGG, denoted as L. * <ref type="bibr" target="#b3">[4]</ref>, and E. and F. * <ref type="bibr" target="#b2">[3]</ref>, respectively, as shown in Table <ref type="table" target="#tab_6">III</ref>. For Eigen and Fergus <ref type="bibr" target="#b2">[3]</ref>, we modify the network by removing the fully connection layers in the    scale 1 and directly implement upsampling operation in the last convolution layer, finally train the model in an end-to-end manner instead of the stage-wise manner. Here, the results of other algorithms are taken from the original papers. The comparative results of the proposed approach and baselines are reported in Table <ref type="table" target="#tab_6">III</ref>. It is evident that our method is significantly better than the state-of-the-art approaches. By comparing VGG-Laina et al. <ref type="bibr" target="#b3">[4]</ref> with VGG-ours, we find that the effective integration of the middle-level information leads to a better performance. In addition, the performance of our reproductive algorithms is comparable with the corresponding methods. Figure <ref type="figure" target="#fig_8">10</ref> shows some qualitative comparisons of the depth maps recovered by our method and Laina et al. <ref type="bibr" target="#b3">[4]</ref> using the NYU v2 dataset. It can be seen that the estimated maps by our method can obtain more detailed information than Laina et al. <ref type="bibr" target="#b3">[4]</ref>, benefiting from the effective fusion of the middle-level information with the T-net.</p><p>In addition, we evaluated our proposed model on the Make3D dataset <ref type="bibr" target="#b0">[1]</ref>, generated from a custom 3D laser scanner. Following <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, the error metrics are computed   on the regions with ground truth depth maps less than 70m. We also reproduce the algorithms of VGG-Laina et al. <ref type="bibr" target="#b3">[4]</ref> and multi-scale Eigen and Fergus <ref type="bibr" target="#b2">[3]</ref> with VGG as L. *-VGG <ref type="bibr" target="#b3">[4]</ref> and E. and F. *-VGG <ref type="bibr" target="#b2">[3]</ref> in Table <ref type="table" target="#tab_8">IV</ref>. Our modified E. and F. *-VGG <ref type="bibr" target="#b2">[3]</ref> and VGG-ours outperform other methods by a significant margin, which reveals that the middle-level information is useful for the accurate depth inference, as well as multi-scale information. As expected, our proposed method yields more detailed structural information of the depth compared with Laina et al. <ref type="bibr" target="#b3">[4]</ref>, as shown in Figure <ref type="figure" target="#fig_9">11</ref>. Furthermore, considering that the Make3D <ref type="bibr" target="#b0">[1]</ref> is a very small dataset, to prove the advantage of the proposed model in the outdoor images, we further evaluate the proposed approach on the KITTI dataset <ref type="bibr" target="#b55">[56]</ref>. Due to the resolution difference of the training images and the testing images, we replace the fully connected layer of our proposed network with 1 × 1 fully convolution layer. To achieve a fair comparison with the state-of-the-art methods, we also reproduce the algorithms of L. *-VGG <ref type="bibr" target="#b3">[4]</ref> and E. and F. *-VGG <ref type="bibr" target="#b2">[3]</ref> as above. The quantitative results of each approach are reported in Table <ref type="table" target="#tab_9">V</ref>.   It is clear that the proposed approach yields lower error than both the L. *-VGG <ref type="bibr" target="#b3">[4]</ref> and the L. *-VGG <ref type="bibr" target="#b3">[4]</ref> approachs, which demonstrates the advantage of the proposed model. As shown in Figure <ref type="figure" target="#fig_10">12</ref>, compared with L. *-VGG et al. <ref type="bibr" target="#b3">[4]</ref> and E. and F. *-VGG et al. <ref type="bibr" target="#b2">[3]</ref>, two of the best methods in the literature, it is evident that our approach achieves better fine-grained depth in visualization. Note that our method and the reproduced algorithms utilize sparse point information to infer dense depth from a single image, which reveals that these methods can also be used in 3D LiDARs to address depth completion problem.</p><p>In addition, we also compared the execution time between the proposed method and the state-of-the-art algorithms. Table VI tabulates the real runtime on the NYU v2, Make3D, and KITTI datasets, corresponding to resolution of 640 × 480, 345 × 460 and 960 × 224, respectively. L. * <ref type="bibr" target="#b3">[4]</ref> is the fastest algorithm since it has less number of convolutional layers and filters. Since the proposed method exploits T-nets to fuse middle-level information, it runs a little bit slower than the L. * <ref type="bibr" target="#b3">[4]</ref> algorithm. However, the speed of our approach still performs favorably against the E. and F. * <ref type="bibr" target="#b2">[3]</ref> algorithm as the later one utilizes large convolutional kernel to integrate multiscale information. It is worth noting that it only takes about 0.1 sec in total for our method to recovery the depth map for a single image (320 × 224), which enables the possibility of inferring fine-grained monocular depth in real-time.</p><p>To evaluate the convergence process of the proposed method, the training curves of the NYU v2 dataset and the Make3D database are visualized in Figure <ref type="figure" target="#fig_11">13</ref>, and the stateof-the-art approaches are also implemented for comparison. It is notable that our algorithm exhibits lower training error, especially for the KITTI dataset, which contributes the performance gains in Table <ref type="table" target="#tab_6">III</ref> and Table <ref type="table" target="#tab_9">V</ref>. In addition, our proposed method converges faster than the L. *-VGG <ref type="bibr" target="#b3">[4]</ref> and the E. and F. *-VGG <ref type="bibr" target="#b2">[3]</ref>, which facilitates the optimization by providing faster convergence at the early stage, benefiting from the T-net architecture. These comparisons verify the effectiveness of the proposed method for learning depth from a single image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Evaluations of VFL dataset with focal length information</head><p>Given varying-focal-length datasets generated in section IV, we utilize the network of the section V to learn the depth from a single image, where the focal length is embedded in the network during the phases of training and testing. For comparison, the experiments are also implemented on L. *-VGG <ref type="bibr" target="#b3">[4]</ref> and E. and F. *-VGG <ref type="bibr" target="#b2">[3]</ref> respectively. For E. and F. *-VGG <ref type="bibr" target="#b2">[3]</ref>, the focal length information is embedded in the last convolutional layer of the scale 1, as similar with the section V. We implement the same operation on the last layer of the downsampling part in the L. *-VGG <ref type="bibr" target="#b3">[4]</ref>. In addition, the experiments without focal length are also implemented on the above models for comparison.</p><p>For VFL-NYU dataset, the experimental results are reported in Table <ref type="table" target="#tab_12">VII</ref>, where NFL denotes the model without embedded focal length, FL denotes the model with embedded focal    <ref type="figure" target="#fig_3">14</ref> shows the increase of accuracy in the form of histogram, which reveals that each model with embedded focal length obtains much better performance than that without the focal length, where L. *-VGG <ref type="bibr" target="#b3">[4]</ref> achieves a significant margin, benefiting from that the network with only one path could effectively deliver the focal length information during forward and backward phases.</p><p>We also implement our approach and the state-of-the-art methods <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b2">[3]</ref> on the VFL-Make3D dataset, as reported in Table <ref type="table" target="#tab_14">VIII</ref>, where the same trained model is also implemented on the Make3D test dataset. It is evident that, for average relative error, the three approaches with embedded focal length information also increase the accuracy by about two percentage points on average, compared with the corresponding methods without the focal length information. As shown in Figure <ref type="figure" target="#fig_14">15</ref>, all models with the embedded focal length information outperform the corresponding models without the focal length information. However, the performance gains of the VFL-Make3D dataset on root square error is not as good as that of the VFL-NYU dataset, which is caused by the accuracy range of the ground truth and the training dataset size. <ref type="table" target="#tab_14">VIII</ref>, it is notable that the models trained on the VFL-NYU dataset and VFL-Make3D dataset achieve better performance than the corresponding models without the embedded focal length information on the NYU test dataset and Make3D test dataset, which also reveals that the focal length information contributes to the performance increase in depth estimation from single images. However, compared the Table VII with Table <ref type="table" target="#tab_6">III</ref>, the experimental results of the nets on the VFL-NYU dataset show slight weakness than the corresponding ones trained on the NYU depth. This phenomena is mainly caused by the fact that the VFL-NYU dataset is much less than the NYU dataset with raw video frames. For the model trained on the NYU depth, except for the 795 pair-wise images, we also fetch 4,000 samples from the raw dataset by virtue of the provided toolbox. While the VFL-NYU dataset is only generated from 1,449 pair-wise images, which has less samples than the models in Table <ref type="table" target="#tab_6">III</ref>. In addition, The VFL-Make3D and Make3D database have approximately same number of samples, which achieve lower error difference than the VFL-NYU and the NYU datasets, as reported in Table VIII and Table IV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>From Table VII and Table</head><p>To further prove the benefits of embedding focal length, we also performed experiments on the SUNRGBD <ref type="bibr" target="#b56">[57]</ref> dataset. In order to achieve a fair comparison with the state-of-the-art methods, we also reproduce the algorithms of E. and F. *-VGG <ref type="bibr" target="#b2">[3]</ref> and L. *-VGG <ref type="bibr" target="#b3">[4]</ref> in the same way. The quantitative results of each approach are reported in Table <ref type="table" target="#tab_16">IX</ref>  imental results show that depths inferred from the model with embedded focal length significantly outperform those without the focal length information in all error measures, which demonstrates the contribution of the focal length information for depth estimation from a single image.</p><p>The above experiments demonstrate that we can boost the inference accuracy of the depth when the focal length is embedded in the network in both learning and inference phases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>In this paper, focusing on the monocular depth learning problem, we first studied the inherent ambiguity between the scene depth and the focal length in theory, and verified it using real images. In order to remove the ambiguity, we proposed an approach to generate the varying-focal-length datasets from the public fixed-focal-length datasets. Then, a novel deep neural network was proposed to infer the fine-grained monocular depth from both the fixed-and varying-focal-length datasets. We demonstrated that the proposed model, without the embedded focal length information, could achieve competitive performance on the public datasets with the state-of-the-art methods. Furthermore, by using the newly generated and publicly varying-focal-length datasets, the proposed approach and the state-of-the-art algorithms embedding focal length a significant performance increase in all error metrics, compared with the corresponding models without encoding focal length. The extensive experiments demonstrate that the embedding focal length is able to improve the depth learning accuracy from single images.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig.1:A novel method to visualize the receptive field. The number in the node represents the counts of per pixel being computed in the receptive field, which reveals that the receptive field obeys the Gaussian distribution and has a smaller size compared with the theoretical receptive field.</figDesc><graphic coords="3,48.96,53.19,251.06,76.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Indistinguishability between the scene depth and the focal length in images.</figDesc><graphic coords="3,324.53,494.55,226.02,110.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Evaluation on depth estimation accuracy via human-computer interaction.</figDesc><graphic coords="4,48.96,223.18,250.79,165.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Some results of depth estimation from single images with different focal lengths. The focal lengths from left to right are 105 mm and 50 mm, where first row are the RGB images, second row and third row are the inferred depths from Liu et al. [37] and Eigen et al. [3] respectively.</figDesc><graphic coords="4,311.97,52.97,251.27,388.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: The illustration of dataset transformation.</figDesc><graphic coords="5,64.02,53.10,220.94,166.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: An example with filling holes of RGB-D images, where each column represents original RGB-D images, transformed RGB-D images, and RGB-D images after postprocessing.</figDesc><graphic coords="5,48.96,249.19,251.27,222.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: Three classes of the 3 × 3 neighborhood patterns used to fill the projected holes</figDesc><graphic coords="5,311.97,53.19,250.79,100.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 :</head><label>8</label><figDesc>Fig. 8: The original RGB-D image (f = 580) and sixe newly generated image sets from the Make3D dataset (top two rows) and the NYU dataset (bottom two rows).</figDesc><graphic coords="6,64.38,52.94,483.27,301.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 :</head><label>10</label><figDesc>Fig. 10: Depth prediction on NYU v2 dataset. Input RGB images(first row), ground truth depth maps (second row), Laina et al. [4] depth results (third row) and our predictions (last row).</figDesc><graphic coords="9,59.00,53.08,230.96,174.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 11 :</head><label>11</label><figDesc>Fig.11: Depth prediction on Make3D. Input RGB images(first row), ground truth depth maps (second row), Laina et al.<ref type="bibr" target="#b3">[4]</ref> depth results (third row) and our predictions (last row). Pixels that corresponding to distances &gt; 70m in the ground truth are masked out.</figDesc><graphic coords="9,322.02,208.64,230.56,306.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 12 :</head><label>12</label><figDesc>Fig. 12: Depth prediction on KITTI dataset. Input RGB images(first row), ground truth (second row), L. * [4] (third row), E. and F. * [3] (fourth row), our proposed method row).</figDesc><graphic coords="10,59.24,53.00,493.19,196.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 13 :</head><label>13</label><figDesc>Fig. 13: Training error on the KITTI dataset (left) and the NYU v2 dataset (right).</figDesc><graphic coords="10,314.48,295.92,246.07,77.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>11 Fig. 14 :</head><label>1114</label><figDesc>Fig. 14: Depth reconstruction errors on the VFL-NYU test dataset and NYU test dataset.</figDesc><graphic coords="11,74.66,53.63,462.63,219.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 15 :</head><label>15</label><figDesc>Fig. 15: Depth reconstruction errors on the VFL-Make3D test dataset and the Make3D test dataset.</figDesc><graphic coords="12,74.66,53.63,462.50,217.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="7,100.37,53.39,411.41,254.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc></figDesc><table /><note><p>The statistical results of the depth estimation from 250 pairs of images.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>2 •</head><label>2</label><figDesc>Average log 10 error: log 10 = 1</figDesc><table /><note><p>N P yi∈|N | |log 10 (y i )log 10 (y * i )| • Accuracy with threshold t: percentage (%) of y i subject to max(</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE II :</head><label>II</label><figDesc>Comparisons on the different architectures and loss functions. SC, T, L1, B, G</figDesc><table /><note><p>* and G represent skip connection, T-net, L1 loss, BerHu loss, GIL-convolution and GIL-connected respectively.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE III :</head><label>III</label><figDesc>Depth reconstruction errors on the NYU depth dataset.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE IV :</head><label>IV</label><figDesc>Depth reconstruction errors on the Make3D depth dataset.</figDesc><table><row><cell>Method</cell><cell cols="2">Error (lower is better) rel rms log 10</cell><cell cols="3">Accuracy (higher is better) 1.25 1.25 2 1.25 3</cell></row><row><cell>E. and F. * [3]</cell><cell>0.095 4.131</cell><cell>0.042</cell><cell>0.893</cell><cell>0.976</cell><cell>0.993</cell></row><row><cell>L. * [4]</cell><cell>0.108 4.326</cell><cell>0.049</cell><cell>0.874</cell><cell>0.975</cell><cell>0.993</cell></row><row><cell>ours-VGG</cell><cell>0.086 4.014</cell><cell>0.040</cell><cell>0.893</cell><cell>0.975</cell><cell>0.994</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE V :</head><label>V</label><figDesc>reconstruction errors on the KITTI depth dataset.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE VI :</head><label>VI</label><figDesc>Execution time (seconds) of the proposed algorithm and the state-of-the-art approaches on the public datasets.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE VII :</head><label>VII</label><figDesc>Depth reconstruction errors on the VFL-NYU dataset and NYU dataset.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE VIII :</head><label>VIII</label><figDesc>Depth reconstruction errors on the VFL-Make3D dataset and Make3D dataset.</figDesc><table /><note><p><p><p><p><p>length. At the same time, the learned models from VFL-NYU dataset are also implemented on the NYU test dataset. As shown in the Table, for average relative error,</p><ref type="bibr" target="#b2">[3]</ref></p>-FL, ours-FL and</p><ref type="bibr" target="#b3">[4]</ref></p>-FL increase the accuracy by about two percentage points on average, compared with corresponding methods without embedded focal length information. Figure</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>. The exper-</figDesc><table><row><cell>Method</cell><cell cols="3">Error (lower is better) rel rms log 10</cell><cell cols="3">Accuracy (higher is better) 1.25 1.25 2 1.25 3</cell></row><row><cell>[3]-NFL</cell><cell>0.318</cell><cell>0.806</cell><cell>0.149</cell><cell>0.387</cell><cell>0.753</cell><cell>0.904</cell></row><row><cell>[3]-FL</cell><cell>0.278</cell><cell>0.677</cell><cell>0.117</cell><cell>0.606</cell><cell>0.853</cell><cell>0.923</cell></row><row><cell>[4]-NFL</cell><cell>0.325</cell><cell>0.834</cell><cell>0.161</cell><cell>0.419</cell><cell>0.743</cell><cell>0.874</cell></row><row><cell>[4]-FL</cell><cell>0.288</cell><cell>0.577</cell><cell>0.095</cell><cell>0.684</cell><cell>0.886</cell><cell>0.949</cell></row><row><cell>ours-NFL</cell><cell>0.294</cell><cell>0.736</cell><cell>0.139</cell><cell>0.585</cell><cell>0.822</cell><cell>0.899</cell></row><row><cell>ours-FL</cell><cell>0.274</cell><cell>0.700</cell><cell>0.120</cell><cell>0.598</cell><cell>0.859</cell><cell>0.938</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>TABLE IX :</head><label>IX</label><figDesc>Depth reconstruction errors on the SUNRGBD depth dataset.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENT</head><p>This work was supported by National Natural Science Foundation of China under the grant No (61333015, 61421004, 61772444, 61573351).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Make3d: Learning 3d scene structure from a single still image</title>
		<author>
			<persName><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Depth extraction from video using non-parametric sampling</title>
		<author>
			<persName><forename type="first">K</forename><surname>Karsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015, pp. 2650-2658. 1, 2, 3, 4, 6, 9, 10, 11</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deeper depth prediction with fully convolutional residual networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 Fourth International Conference</title>
		<meeting><address><addrLine>1, 2, 6, 7, 8, 9</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note>in 3D Vision (3DV</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Multi-scale continuous crfs as sequential deep networks for monocular depth estimation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.02157</idno>
		<imprint>
			<date type="published" when="2006">2017. 1, 2, 6</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fusenet: Incorporating depth into semantic segmentation via fusion-based cnn architecture</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Domokos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="213" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Exploiting depth from single monocular images for object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Efficient human pose estimation from single depth images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kipman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Finocchio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">2821</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep sliding shapes for amodal 3d object detection in rgb-d images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="808" to="816" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Nathan Silberman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2009">2012. 1, 8, 9</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset</title>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1231" to="1237" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.00561</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="447" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Refinenet: Multi-path refinement networks with identity mappings for high-resolution semantic segmentation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.06612</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Richer convolutional features for edge detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.02103</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1395" to="1403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Automatic photo pop-up</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM transactions on graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="577" to="584" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Efficient exact inference for 3d indoor scene understanding</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="299" to="313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Thinking inside the box: Using appearance models and context based on room geometry</title>
		<author>
			<persName><forename type="first">V</forename><surname>Hedau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="224" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">depth reconstruction from a single still image</title>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="69" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">What can we learn about the scene structure from three orthogonal vanishing points in images</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-T</forename><surname>Tsui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="192" to="202" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Single view metrology from scene constraints</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-T</forename><surname>Tsui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="831" to="840" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Camera calibration and 3d reconstruction from a single view based on scene constraints</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-T</forename><surname>Tsui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="311" to="323" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Slic superpixels compared to state-of-the-art superpixel methods</title>
		<author>
			<persName><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Süsstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2274" to="2282" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Sift flow: Dense correspondence across scenes and its applications</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="978" to="994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">2d-to-3d image conversion by learning depth from examples</title>
		<author>
			<persName><forename type="first">J</forename><surname>Konrad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ishwar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="page" from="16" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Discrete-continuous depth estimation from a single image</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Modeling the shape of the scene: A holistic representation of the spatial envelope</title>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="145" to="175" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005. 2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
	<note>CVPR 2005</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fast depth extraction from a single image</title>
		<author>
			<persName><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Advanced Robotic Systems</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">1729881416663370</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Indoor scene structure analysis for single image depth estimation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="614" to="622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep convolutional neural fields for depth estimation from a single image</title>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Depth and surface normal estimation from monocular images using regression on deep features and hierarchical crfs</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Towards unified depth and semantic prediction from a single image</title>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Monocular depth estimation using neural regression forest</title>
		<author>
			<persName><forename type="first">A</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The inherent ambiguity in scene depth learning from single images</title>
		<author>
			<persName><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SCIENTIA SINICA Informationis</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="811" to="818" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Understanding the effective receptive field in deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4898" to="4906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning to generate chairs with convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Tobias</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1538" to="1546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Highway networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00387</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Training very deep networks</title>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2377" to="2385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.04802</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Deep multi-scale video prediction beyond mean square error</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05440</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Multi-view 3d models from single images with a convolutional network</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="322" to="337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Zwald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lambert-Lacroix</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.6868</idno>
		<title level="m">The berhu penalty and the grouped effect</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Sparsity invariant cnns</title>
		<author>
			<persName><forename type="first">J</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Sun rgb-d: A rgb-d scene understanding benchmark suite</title>
		<author>
			<persName><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Tensorflow: Large-scale machine learning on heterogeneous distributed systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">He has published over 100 papers in peer-reviewed journals and conferences. His research interests include computer vision, structure from motion, object detection and tracking, artificial intelligence, and robot localization and navigation. Dr. Wang has served as associate editor and on the editorial board of two journals, as an area chair or TPC member of 20+ conferences, and as a reviewer of 20+ journals</title>
	</analytic>
	<monogr>
		<title level="m">Zhanyi Hu received his B.S. in Automation from the North China University of Technology, Beijing, China, in 1985, and the Ph.D. in Computer Vision from the University of Liege</title>
		<title level="s">Chinese Academy of Sciences. His research interests include computer vision, machine learning, and pattern recognition. Guanghui Wang</title>
		<meeting><address><addrLine>China; Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1993">2006 to 2010. 1993</date>
		</imprint>
		<respStmt>
			<orgName>Electrical and Computer Engineering, University of Windsor</orgName>
		</respStmt>
	</monogr>
	<note>Since 1993, he has been with the National Laboratory of Pattern Recognition at Institute of Automation, Chinese Academy of Sciences, where he is now a professor. His research interests are in robot vision. which include camera calibration and 3D reconstruction, vision guided robot navigation. He was the local chair of ICCV 2005, an area chair of ACCV 2009, and the PC chair of ACCV 2012</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
