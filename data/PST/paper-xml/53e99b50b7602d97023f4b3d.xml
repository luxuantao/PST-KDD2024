<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Interacting with mobile services: an evaluation of camera-phones and visual tags</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2006-02-11">11 February 2006</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Eleanor</forename><surname>Toye</surname></persName>
							<email>eleanor.toye@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Intel Research Cambridge</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Richard</surname></persName>
							<email>richard.sharp@intel.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Intel Research Cambridge</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sharp</forename><surname>Ae</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Intel Research Cambridge</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Anil</forename><surname>Madhavapeddy</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Intel Research Cambridge</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Computer Laboratory</orgName>
								<orgName type="institution">Cambridge University</orgName>
								<address>
									<addrLine>15 JJ Thomson Avenue</addrLine>
									<postCode>CB3 0FD</postCode>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Scott</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Intel Research Cambridge</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Eben</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Intel Research Cambridge</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Upton</forename><surname>Ae</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Intel Research Cambridge</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alan</forename><surname>Blackwell</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Intel Research Cambridge</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ae</forename><forename type="middle">D</forename><surname>Scott</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Computer Laboratory</orgName>
								<orgName type="institution">Cambridge University</orgName>
								<address>
									<addrLine>15 JJ Thomson Avenue</addrLine>
									<postCode>CB3 0FD</postCode>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ae</forename><forename type="middle">E</forename><surname>Upton</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Computer Laboratory</orgName>
								<orgName type="institution">Cambridge University</orgName>
								<address>
									<addrLine>15 JJ Thomson Avenue</addrLine>
									<postCode>CB3 0FD</postCode>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ae</forename><forename type="middle">A</forename><surname>Blackwell</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Computer Laboratory</orgName>
								<orgName type="institution">Cambridge University</orgName>
								<address>
									<addrLine>15 JJ Thomson Avenue</addrLine>
									<postCode>CB3 0FD</postCode>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Interacting with mobile services: an evaluation of camera-phones and visual tags</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2006-02-11">11 February 2006</date>
						</imprint>
					</monogr>
					<idno type="MD5">31073209CEEF2357372F72259E907176</idno>
					<idno type="DOI">10.1007/s00779-006-0064-9</idno>
					<note type="submission">Received: 3 June 2005 / Accepted: 20 September 2005 /</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T10:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a study of using camera-phones and visual-tags to access mobile services. Firstly, a userexperience study is described in which participants were both observed learning to interact with a prototype mobile service and interviewed about their experiences. Secondly, a pointing-device task is presented in which quantitative data was gathered regarding the speed and accuracy with which participants aimed and clicked on visual-tags using camera-phones. We found that participants' attitudes to visual-tag-based applications were broadly positive, although they had several important reservations about camera-phone technology more generally. Data from our pointing-device task demonstrated that novice users were able to aim and click on visual-tags quickly (well under 3 s per pointing-device trial on average) and accurately (almost all meeting our defined speed/accuracy tradeoff of 6% error-rate). Based on our findings, design lessons for camera-phone and visual-tag applications are presented.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Using camera-phones to recognise visual tags is currently a hot topic, receiving significant media coverage throughout 2004 <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25]</ref>. However, despite this press coverage and recent commercial tag-reader implementations <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b17">18]</ref>, there has so far been very little research into the usability of applications that exploit camera-phones and tags. Therefore it is unclear whether ordinary users are able to interact easily with cameraphone/tag-based applications and, further, what practical value (if any) ordinary users see in these types of application.</p><p>The study presented in this paper addresses these issues. More specifically we attempt to answer four key usability questions: (1) how quickly and accurately can novice users click on visual tags?; <ref type="bibr" target="#b1">(2)</ref> is it clear to novice users how to use camera-phones and tags in the context of a realistic mobile service?; <ref type="bibr" target="#b2">(3)</ref> what do users see as the advantages and disadvantages of services that employ camera-phones and tags? and <ref type="bibr" target="#b3">(4)</ref> what are users' attitudes towards adopting this technology in their daily lives?</p><p>Our study is split into two parts. Firstly, we present a user-experience study in which we both observe participants learning to interact with a prototype mobile service and interview them about their experiences. This part of the study primarily addresses questions (2), ( <ref type="formula">3</ref>) and (4) above. Secondly, we report quantitative results arising from our pointing-device task-an experiment designed to assess how quickly and accurately users can click on visual tags-providing an answer to question <ref type="bibr" target="#b0">(1)</ref> above.</p><p>The remainder of the paper is structured as follows. We start by providing background information on camera-phone/visual-tag interaction generally (Sect. 2). Following this, we describe our implementation of the particular camera-phone/visual-tag interaction system that we employ in our user studies (Sect. 3). The userexperience study (Sect. 4) and the pointing-device task (Sect. 5) are then reported. Finally, we consider related work (Sect. 6) and present conclusions and designlessons learnt from our experiments (Sect. 7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>The idea of using tags to ''bridge physical and virtual worlds'' pre-dates camera-phones <ref type="bibr" target="#b27">[28]</ref>. Several ubiquitous computing researchers have explored this idea, proposing a number of motivating application scenarios. For example, as part of the CoolTown project <ref type="bibr" target="#b13">[14]</ref>, Kindberg et al. prototyped a museum in which users can scan exhibits with their PDAs in order to read more about them and bookmark this information for future reference. More recently, Microsoft's Aura project <ref type="bibr" target="#b25">[26]</ref> successfully deployed a similar idea in the real world, using PDAs with barcode-readers to scan UPC/EAN barcodes on existing products. Other visual tag-based applications that have been proposed include an augmented calendar <ref type="bibr" target="#b21">[22]</ref>, and a scheme for assisting system administrators manage displayless rack-mounted machines <ref type="bibr" target="#b24">[25]</ref>.</p><p>The recent camera-phone boom has created a resurgence of interest in tag-based interaction techniques since, for the first time, large numbers of real-world users own a device capable of supporting the above applications and usage scenarios. The phone's embedded camera can photograph visual tags, software running on the phone can decode tags and wireless networking technologies, such as GPRS, 3G or Bluetooth, can be used to fetch data on the basis of tag sightings.</p><p>A number of commercially available products have already appeared which allow users to scan visual tags with their camera-phones and download mobile content <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b16">17]</ref>. Applications that have been built using this technology include scanning a tag to retrieve bus timetables <ref type="bibr" target="#b17">[18]</ref> and scanning a tag on a business card to record someone's contact details <ref type="bibr" target="#b20">[21]</ref>.</p><p>An important consideration is that the different applications and usage scenarios proposed for visual tags impose very different requirements on tag-reading technology. For example, in the bus timetable application <ref type="bibr" target="#b17">[18]</ref>, users typically click on a single tag in order obtain the waiting time for the next bus. In this case, the speed at which the tag must be decoded by the tag-reader is not technically challenging: after scanning the tag, users may be prepared to wait several seconds before timetable information appears on their phone's screen. In other applications users may require access to information much more quickly. For example, in the CoolTown museum <ref type="bibr" target="#b13">[14]</ref>, users may want to scan several exhibits in quick succession. Similarly, in the case of the system administration tool <ref type="bibr" target="#b24">[25]</ref>, users may quickly need to scan the tags of several rack-mounted machines in order to check their status. The speed at which users can scan tags with camera-phones thus determines the class of applications that the interaction technique can feasibly support.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Implementation</head><p>We implemented a software framework for cameraphone/tag-based interaction by extending our previous work on the Mobile Service Toolkit: a client/server architecture supporting situated services that interact with mobile phones <ref type="bibr" target="#b26">[27]</ref>. Our implementation of the Mobile Service Explorer-the phone-based client which enables users to access situated services-runs on all Nokia Series-60 camera-phones.</p><p>On activating the Mobile Service Explorer the camera-phone's display immediately becomes a viewfinder: a live video feed continually capturing frames from the phone's embedded camera. Whenever a tag becomes visible in the viewfinder, it is highlighted with a red cross-hair and tracked in real-time (see Fig. <ref type="figure" target="#fig_0">1</ref>). If multiple tags are visible on the screen at the same time, the tag nearest the centre of the viewfinder is highlighted. We adopt standard GUI terminology, saying that the highlighted tag has the focus. Users can click on a visual tag by pressing the select button on the phone's keypad when the tag has the focus.</p><p>A user interacts with a mobile service primarily by aiming and clicking on tags using their camera-phone. When a tag has the focus, a mobile service may display a small amount of information on the phone's screen in a manner analogous to augmented-reality overlays. Clicking on a tag may result in an action being performed by the service or a simple UI control appearing on the camera-phone's display. We currently allow mobile services to export numerical data-entry fields, list selection fields and multiple choice dialogues (e.g. ''Continue? Yes/No'') to a user's camera-phone. For example, clicking on a tag to join a queue in our prototype virtual queueing application (see Sect. 4.1) results in a numerical data-entry field appearing on the cameraphone's screen prompting the user to enter the number of people who want to join the queue.</p><p>When we implemented the Mobile Service Toolkit there were, to our knowledge, only three camera-phonebased tag reading packages generally available: Sema-Code <ref type="bibr" target="#b2">[3]</ref>, SpotCode <ref type="bibr" target="#b17">[18]</ref> and Rohs' tag recognizer <ref type="bibr" target="#b22">[23]</ref>. We evaluated each of these platforms in terms of decoding latency, robustness to variable lighting conditions and robustness to variability in the phone's orientation with respect to visual tags. In each of these areas, which we hypothesised were important usability factors, the SpotCode Reader performed the best. For this reason, we chose to incorporate SpotCodes in our implementation.</p><p>Initially based on the TRIP tag format <ref type="bibr" target="#b6">[7]</ref>, SpotCodes are distinctive, circular tags with 2 data rings and 21 sectors (see Fig. <ref type="figure" target="#fig_1">3</ref>). Each SpotCode encodes 42 bits of information, a data capacity more than sufficient for our purposes.</p><p>Since this paper focuses on evaluation rather than implementation, we do not discuss the technical details of the Spotcode format, nor the software architecture of the Mobile Service Toolkit here. Readers interested in these topics are referred to other publications <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b26">27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">User experience study</head><p>Our user experience study was based around a Theme Park virtual queueing system in which participants used camera-phones and visual tags both to obtain information about theme park attractions and to join and leave virtual queues. We chose this application as a technology-probe for two reasons:</p><p>1. We thought that the theme park scenario was likely to be familiar to our participants; indeed, when we interviewed our participants, we found that they had all visited theme parks. Therefore, despite the limitations imposed by presenting the application in a lab setting, we were able to gather useful data regarding participants' impressions, beliefs and opinions relevant to plausible uses of camera-phones and visual tags. 2. The virtual queueing system required participants to make heavy use of visual tags throughout the entire interaction period. Users had to aim their cameraphone at several tags in quick succession in order to compare the current queueing information for the various theme park attractions.</p><p>The approach of prototyping an application for a familiar environment in a lab-setting was inspired by Kindberg et al.'s UbiComp '04 paper <ref type="bibr" target="#b14">[15]</ref>. Note that, since our virtual queueing system is purely a technology probe designed to elicit feedback from participants, we deliberately ignore the technical details involved in extending the prototype system to a real deployment.</p><p>We continue by describing the theme park virtual queueing application in more detail (Sect. 4.1). We then describe the participants who took part in the userexperience study (Sect. 4.2), the experimental procedure (Sect. 4.3) and, finally, our findings arising from the study (Sect. 4.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Application description: theme park virtual queueing system</head><p>Theme parks often have long queues for rides. Some theme parks have addressed this problem already by devising virtual queueing systems in which, rather than waiting in line, people are allocated ''tickets'' allowing them to get on rides at specified times. Tickets may be pieces of paper or may be virtual capabilities stored on an electronic device. At present, electronic virtual queueing systems deployed in theme parks involve specialised mobile devices or smartcards lent to visitors for the duration of their time in the park. In this section we describe our prototype virtual queueing system that exploits visual tags and users' own camera-phones.</p><p>We set up ''TagLand Theme Park'' in a large student computer laboratory in our building. TagLand consists of six individual posters sited in specific locations across the lab, each representing a different attraction. A large central poster contains a map showing the locations of the six attractions. A visitor information leaflet (handed to participants at the start of each study) contains a list of all six attractions, providing some information about what each one entails.</p><p>Each attraction has an associated visual tag, which is printed on the poster where the attraction is located, at the relevant position on the central map, and on the visitor information leaflet. For each of these six attractions, holding the camera-phone over the associated tag causes a text overlay to appear at the top of the cameraphone's viewfinder displaying relevant information (see Fig. <ref type="figure">2</ref>). One of the rides is closed for maintenance and holding the camera-phone over its associated tag reveals this information; the other attractions display their current queueing times.</p><p>The posters representing attractions and the central map contain text inviting users to ''click on a tag to join a queue'', and explain that they can leave the queue at any time simply by ''clicking the tag again''. On clicking a tag, a text-entry field appears on the phone prompting the user to ''enter the party size'' (the number of people wishing to join the queue). After the party size is entered, using the keypad on the phone, the application records that the participant is now in the queue for a ride.</p><p>When a user focuses on a tag corresponding to an attraction for which they have a booking, the resulting text overlay informs them that they are in a virtual queue. The remaining queueing time in minutes and seconds is also displayed, ticking down to zero. Clicking on the tag in this state yields a confirmation dialogue asking the user if they want to leave the queue. Users can select ''Yes'' or ''No'' using their camera-phone's keypad. Shortly before the user reaches the front of a queue, the phone beeps and displays a dialogue notifying them to ''Go to AEattraction nameae in 30s''. (Clearly in a real deployment more notice would be given.) When the front of a queue is reached a fanfare sound plays and an alert dialogue appears, confirming that they are now able to take the ride.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Participants</head><p>Eighteen people (nine men and nine women) aged between 24 and 43 years (mean = 29) served as participants. They were an educated and mostly professional group. None reported difficulties with vision or motor skills. None worked in the field of Computer Science or in the mobile phone industry. All were mobile phone users and three owned camera-phones. Participants were recruited either through flyers posted around the city and University of Cambridge, or by word of mouth and were each rewarded with a book token (value 20 UKP).</p><p>Our rationale for choosing this demographic was that it allowed us to identify any general usability difficulties which affected even healthy, younger adults. We intend to carry out further studies with broader samples of the population in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Procedure</head><p>Each participant was given a Nokia 3650 camera-phone and a visitor information leaflet. They were instructed to explore the theme park and use the camera-phone to join and leave queues for the different rides and attractions. They were told that they could do this for as long as they liked and to meet the researcher back at the central map whenever they felt they had finished exploring.</p><p>Structured interviews were carried out at the end of each session. Initial questions in the interview schedule were entirely open-ended (e.g. ''Was there anything about the Theme Park that you found confusing or difficult?''). Next, users were asked to make comparisons between using camera-phones/tags and other familiar technologies, in the context of virtual queueing systems. All the comparative questions were worded to avoid drawing attention to specific features of the technology (e.g. speed or convenience). Finally, more general questions were asked in order to explore participants' overall impressions of the technology and also their attitudes to adoption.</p><p>We wanted to know whether novice users would understand how to use the camera-phone and tags in the context of the theme park application. For this reason we deliberately kept the instructions given to participants to an absolute minimum. The purpose of the instructions was primarily to direct the participants' attention to the features of the application, without giving precise information about what to do.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results and discussion</head><p>Our key result is that, within less than 15 min, each participant had succeeded in understanding and using every aspect of the queueing system without researcher intervention. This gives an unequivocally positive result for question (2) of Sect. 1: it is clear to novice users how to use the phone display in conjunction with visual tags in the context of a realistic mobile service.</p><p>Although all participants were able to interact successfully with the theme park application, two minor usability issues did arise. Two (of 18) participants commented that they had some trouble with the red cross-hair flickering on the viewfinder, so sometimes they were unsure whether they had successfully focused on a tag. Observation suggested that this was because they were holding the camera-phone just beyond the maximum distance at which the tag could be reliably decoded. We expect that this problem would disappear with further practice.</p><p>One participant also commented that ''tags below waist height were difficult to focus on'', but added that this could easily be remedied by: ''putting low tags at an angle or moving them higher.'' No other ergonomic issues were reported.</p><p>The remainder of this section addresses our third and fourth usability questions, reporting users' perceptions of advantages and disadvantages of camera-phone/tagbased services, and their attitudes surrounding the widespread adoption of this technology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Perceived advantages and disadvantages</head><p>Many participants saw advantages to using the cameraphone based queueing system. Eight made positive comments about having mobile access to current queue lengths and the ability to change their own queue status using tags. For instance: I liked the ability to get real-time updates by just looking at the tags to find out how far I was from the front of the queue. A particularly popular feature of the system was that tags were contained in the leaflet: Very portable. You've got the two things you need to make it all work (phone and leaflet) and you can do instant checks.</p><p>However, a number of participants were concerned about the use of camera-phones for this type of application. Concerns included camera-phones not working abroad and being stolen: it'd be a great place for mobile phone thieves if everyone was waving around fancy camera-phones.</p><p>One participant was also worried that a camera-phone might get broken on Theme Park rides:</p><p>Phones and water-rides don't mix easily! Another commented that they were not allowed to use camera-phones at work: I can't get a camera-phone because they aren't allowed in schools, where I often work... Recall that we asked participants to make comparisons between using camera-phones/tags and other familiar technologies in the context of virtual queueing systems. Firstly, participants were asked to compare the Theme Park application with ''entering their mobile phone number into a touchscreen'' and then ''receiving text messages telling them when to go to the ride''. Thirteen participants felt that the touchscreen/text solution would be worse than our camera-phone/tag application (two believed it would be the same, one believed it would be better and one provided uninterpretable data for this question). Six of the participants who thought it would be worse said this because they felt text messages were much less convenient to access than our tag-initiated overlays (see Fig. <ref type="figure">2</ref>). We were surprised at how negative participants were about the hypothetical text-message-based interaction technique. A characteristic comment from one participant was: With the texts, you have to wait for them to arrive, and sometimes they're late-could be minutes or days late; and they fill up your inbox, so you'd have to keep checking your messages and deleting. You have to hit more buttons to get your message than with the camera and tags system.</p><p>Secondly, participants were asked to compare the Theme Park application to a conventional ticket-based virtual queueing system. In response to this question, 16 participants said they would prefer the camera-phone system. The most common reason cited for this was that, in a ticket-based system, one has to carry round several tickets and might lose some of them-for example: You'd need a lot of tickets, you'd have too many things to carry and you might lose the tickets.</p><p>One participant, who had recently experienced a ticketbased virtual queueing system at Disneyland last year, said of his Disney queueing experience: ...[Disney's queueing system] is better than standing in line but I can't cancel my place on the ride and there'd be queues of people at the ticket machines-and I have to go to the machines, I can't just join queues from wherever I am.</p><p>To sum up, participants did see some unique benefits of using camera-phones to access mobile services. Although they also had some reservations, these focused almost entirely on the camera-phone technology itself (e.g. fragile, expensive, stealable) rather than the specifics of the interaction technique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Attitudes to adoption</head><p>At the end of the interview schedule, we asked participants to make general comments about camera-phone/ tag-based applications. Although most of the comments were positive, a few participants took this opportunity to express concerns regarding the widespread adoption of this technology. Two participants were worried about accessibility: How well would ... people with disabilities cope with this technology? The camera-tag system seems to require quite good hand-eye coordination, so it's not good for the elderly or anyone who is a bit shaky.</p><p>We recognise that further studies, with a broader sample of participants, are required to address accessibility issues.</p><p>One participant was worried that deployment of tag-based services may benefit camera-phone users to the detriment of everyone else: It could create a technology hierarchy where cameraphone owners get better services-I vaguely resent this idea. That's why I preferred the idea in the theme park of being lent a device: it seems fairer. However, on the whole, attitudes to adoption were very positive. All of our participants (not including two who already had Bluetooth camera-phones) said that, if tag-based mobile services were widely available, they would choose to buy a phone that allowed them to use tags in preference to one that did not. Nearly half (7 out of 16 participants who did not already own cameraphones) said that they would be prepared to pay more for a camera-phone if tag-based mobile services were widely available.</p><p>Of the group tested, all already used mobile phones on a regular basis. It remains an open question whether other social groups, who may not already use mobile phones regularly, would be so open to adopting cameraphones and tags.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">The pointing device task</head><p>The pointing device task is a controlled experiment that addresses the first of the usability questions presented in Sect. 1: how quickly and accurately can novice users click on visual tags?</p><p>Our key concern was to determine whether participants' clicking performance was adequate for even heavily tag-centric usage scenarios to be viable (e.g. the CoolTown Museum <ref type="bibr" target="#b13">[14]</ref> and system administration application <ref type="bibr" target="#b24">[25]</ref> that we discussed in Sect. 2). Given the nature of such applications, we chose to define acceptable performance as a clicking time of less than 3 s on average, with a clicking error rate of less than 10%.</p><p>Participants The participants for the pointing device task were the same as those who took part in the user experience study (see Sect. 4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental design</head><p>Our design of the pointing device task was heavily influenced by previous work in applying Fitts' law models to mice, tracker balls and graphics tablets <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>.</p><p>In each trial, a pair of equal-sized tags appears on a plasma screen, one highlighted with a red box around it (see Fig. <ref type="figure" target="#fig_1">3</ref>). A participant uses their camera-phone to click first on the highlighted tag, then on the other tag.</p><p>The horizontal distance between tags and the diameter of the tags varies across different trials. Every combination of eight different tag diameters (ranging from 4 to 11 cm, at increments of 0.875 cm) and six different inter-tag distances (ranging from 18 to 63 cm, at increments of 7.5 cm) are presented, resulting in 6•8=48 trials. <ref type="foot" target="#foot_0">1</ref> Each of these 48 trials is performed twice: once for each clicking order-that is, whether at the start of a trial the highlighted tag is on the right or the left. Thus, the total trials performed in a single test run = 48•2=96. Trials with the same clicking order are presented consecutively, meaning that the clicking order only changes once between the two blocks of 48 trials. The order of the trials within these two blocks of 48 are randomly permuted for each participant.</p><p>The 96 trials are divided into sets of 15, each separated by 15-s rest periods, with a short set of six trials at the end. The choice of 15 trials per rest period was not determined by the experimental design; we simply found, during pilot testing, that this was the number of trials a participant could perform consecutively before they started to feel tired.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Procedure</head><p>Participants were asked to perform ten initial practice trials, followed by the 96 clicking trials. For each trial, participants were told to click first on the highlighted tag and then, as quickly as possible, on the other tag. To ensure a standard speed/accuracy tradeoff across participants we instructed them to aim for no more than one error in each set of 15 trials (corresponding to a maximum of six errors per session). To avoid ordering effects we ensured that the same number of participants started with the left/right clicking order as started with the right/left clicking order.</p><p>For each trial, a computer recorded the clicking time:<ref type="foot" target="#foot_1">2</ref> the time between the participant clicking on the first tag and clicking on the second tag rounded to the nearest 10 ms. (This resolution was sufficient for our purposes.) An error was registered if the participant pressed the select button on their camera-phone when a tag was not in focus, or if the wrong tag was clicked on. At the bottom of the screen a status bar showed the participant both the number of trials remaining and the number of trials in the current set of 15 that had so far resulted in error.</p><p>Participants were informed that there would be a forced break every 15 trials. They were also told that they could take breaks as often as they liked and for as long as they liked between trials (although in practice no-one actually did this). Every participant performed the pointing device task twice, separated by a break of at least half an hour. This gave us 192 trials per participant (and therefore a total of 3,456 trials across our sample of 18 participants). We have made this data available for download so that other researchers can verify our results and perform their own analyses <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results and discussion</head><p>Our main result is that all participants' performance fell within the pre-defined acceptable limits for clicking speed and accuracy (see Sect. 5). Participants' mean clicking times ranged from 0.89 s to 2.63 s; number of errors per session ranged from 0 to 8 (out of a total of 96 trials per session). Table <ref type="table" target="#tab_0">1</ref> summarises our data, showing means and standard deviations of clicking time and error rate (collapsed over all participants and broken down by session). Summary data is also shown graphically in Fig. <ref type="figure" target="#fig_2">4</ref>.</p><p>This shows conclusively that even novice users can click on visual tags quickly and accurately using commercially available camera-phones and tag-reading software. It demonstrates that even heavily tag-centric interaction techniques are feasible without requiring manufacturers to add extra tag-reading hardware to their phones.</p><p>Although all participants met our acceptability criterion for clicking time in each session, participants clicked more quickly on average in the second session, indicating a significant 3 practice effect. We would expect to see continued improvement in clicking times as users become more familiar with camera-phones and tags. However, we re-iterate that even the first-time users' performance was, in all cases, already adequate for usability. (Note that, since our procedure specified a target error rate, it would be inappropriate to look for a practice effect on accuracy.)</p><p>Recall that we chose to study tag diameters between 4 and 11 cm because we felt that this was a plausible range of tag sizes for display on posters. We noticed a slight, but significant, 4 decrease in error rate as the tag-size increased. This suggests that, within this range of tag sizes, designers should aim to use larger tags where possible.</p><p>Figure <ref type="figure">5</ref> shows that all the tag sizes in the range we sampled are acceptable in terms of clicking time; for all tag diameters, mean clicking times were less than 3 s. There was no significant linear relationship 5 between clicking time and tag diameter.</p><p>Although the details are beyond the scope of this paper, we found that Fitts' law <ref type="bibr" target="#b19">[20]</ref> correlations on the data were weak (taking tag diameter as a measure of Fitts' target width). This is perhaps unsurprising when one considers that a camera can be aimed by rotation about its vertical axis; Fitts' law models typically do not account for this method of target acquisition.</p><p>When generalising these results to other cameraphones running the SpotCode reader there are two factors to consider: the optical properties of the camera lens and the shape/ergonomics of the camera-phone. Fortunately, the optical properties of different cameraphone lenses are currently very similar so we expect our results to generalise well in this respect. For example, all Nokia camera-phones we studied had the same level of zoom (3.5 mm); other camera-phones we looked at had fixed zoom of between 2.8 and 4.5 mm. <ref type="foot" target="#foot_2">6</ref> To quantify how performance might vary with radically different levels of zoom, more studies would be required.</p><p>With regard to camera-phone shape and ergonomics, we note that most camera-phones have the same basic form factor as the Nokia 3650: a display and keypad on the front and a camera lens on the back, towards the top. We expect that our results would generalise well to other camera-phones with this basic form factor. Again, for radically different camera-phone designs further studies would be required. Other tag formats/readers have very different usability properties to SpotCodes. For example, the SemaCode reader requires users to align rectangular tags within a target area appearing on the phone's viewfinder. Furthermore, there is no cross-hair to give realtime feedback while focusing on tags and, on selecting a tag, users incur a decode latency of about 3 s. We do not claim that our results generalise to SemaCode or, indeed, to any other tag reader or tag format. However, our experimental design provides a robust method for measuring the usability of tag formats and tag-reading software generally. Our work therefore forms a solid basis for future cross-platform evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related work</head><p>We have already covered much of the related work in the field of visual-tag-based interaction techniques (see Sect. 2). Here we briefly outline work in other areas that is relevant to this paper.</p><p>There are many related interaction techniques which also use cameras as an input medium. Most of these systems involve fixed cameras trained on users <ref type="bibr" target="#b8">[9]</ref>. This approach, pioneered by VideoPlace <ref type="bibr" target="#b15">[16]</ref>, has proved fruitful in areas such as gesture recognition <ref type="bibr" target="#b7">[8]</ref> and location-aware computing <ref type="bibr" target="#b6">[7]</ref>. In contrast, cameraphones are more suited to the approach often taken by the augmented reality community in which hand-held cameras are pointed at objects in the world. A variety of augmented reality systems based on optical tags have already been developed <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b12">13]</ref>. These systems influenced our design of the theme park application to a large extent. In particular, the textual overlays that appear when the user focuses on a tag were inspired by the NaviCam <ref type="bibr" target="#b21">[22]</ref> project. 3 A one-way analysis of variance (ANOVA) established that the practice effect on clicking time was statistically significant (for a=0.05): F(1,17)=6.055, p=0.025 4 Collapsing across both sessions, linear correlation between error rate and tag diameter yielded r=À0.39, with 99% confidence intervals for q of (À0.554, À0.189). Note that this effect was also observed for session 1 and session 2 independently. For session 1, r=À0.30 with 99% confidence intervals for q of (À0.48,À0.09); for session 2, r=À0.37 with 99% confidence intervals for q of (À0.53,À0.17). 5 Linear correlation between clicking time and tag diameter yielded r=0.05, with 95% confidence intervals for q of (À0.113,0.212) One of the aims of using visual tags with cameraphones is to alleviate the constraints associated with small mobile displays and keypads. Many researchers have addressed this fundamental problem in different ways including panning <ref type="bibr" target="#b11">[12]</ref>, fish-eye views <ref type="bibr" target="#b9">[10]</ref> and novel uses of audio <ref type="bibr" target="#b5">[6]</ref>. We see visual tags as complementary to these approaches. Indeed, applications based on camera-phones and tags could also usefully employ (say) panning and zooming screen displays.</p><p>7 Conclusions, design lessons and future work</p><p>We have evaluated a commonly proposed interaction technique in which users point and click on visual tags using camera-phones. Our study focused specifically on four usability questions: (1) how quickly and accurately can novice users click on visual tags?; (2) is it clear to novice users how to use camera-phones and tags in the context of a realistic mobile service?; <ref type="bibr" target="#b2">(3)</ref> what do users see as the advantages and disadvantages of services that employ camera-phones and tags? and ( <ref type="formula">4</ref>) what are users' attitudes towards adopting this technology in their daily lives?</p><p>The first of these questions was addressed by the pointing device task (Sect. 5) which provided quantitative data demonstrating conclusively that novice users could click on visual tags quickly (well under 3 s per trial on average) and accurately (inside our specified acceptable error rate of 10%).</p><p>Although the pointing device task was performed using Nokia 3650 camera-phones, we expect that the results would generalise well to other currently available camera-phones running the SpotCode reader. Furthermore, the experimental design of our pointing device task provides a robust method for measuring the usability of tag formats and tag-reading software more generally. Our method thus forms a solid basis for future cross-platform evaluation.</p><p>Questions ( <ref type="formula">2</ref>), ( <ref type="formula">3</ref>) and ( <ref type="formula">4</ref>) above were addressed by our user experience study, in which we observed our participants learning to interact with a prototype virtual queueing system and interviewed them about their experiences (Sect. 4). Our key result was that, in less than 15 min, each participant succeeded in understanding and using every aspect of the queueing system without researcher intervention. This gives an unequivocally positive result for question (2): it is clear to novice  users how to use camera-phones and tags in the context of a realistic mobile service. With reference to question (3), opinions on the usefulness of the interaction technique were broadly positive, but with some important reservations. Mobile access to up-to-date information was generally considered a very attractive feature, with several participants also commenting that the visual tags themselves were ''easy to use''. Some participants were concerned about whether camera-phones were an appropriate device for accessing mobile services because of their currently limited user-base, their fragility and their potential for being easily lost or stolen. However, despite our participants' concerns, camera-phones continue to grow in popularity <ref type="bibr" target="#b10">[11]</ref>. Tag-based services promise to add value for this growing number of camera-phone owners without imposing additional costs beyond those already incurred by camera-phone ownership.</p><p>In answer to question (4) we found that our participants' attitudes to adoption were generally positive. A few participants highlighted concerns about the social implications of providing public services which can only be accessed via individuals' camera-phones. However, all participants (who did not already own a cameraphone) considered that if this interaction technique became widely available, it would positively influence their attitude towards buying a camera-phone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Design lessons</head><p>Based on our implementation experience and the findings from our user studies, we advise developers of camera-phones/visual tags to adhere to the following guidelines:</p><p>1. Keep data-entry on the phone's keypad to a minimum, relying on tag-based interaction instead whenever possible. (In the user-experience interviews, several participants reported that they preferred tagbased interaction to entering data via the cameraphone's keypad.) 2. Place text or icons that describe the function of a tag very close to the tag itself. Then, when a cameraphone is aimed at the tag, both the tag and its surrounding context will be visible in the cameraphone's viewfinder. This prevents users from having to constantly switch their gaze between two displays; all the information they need to identify a particular tag is on the phone's own display. 3. When small amounts of dynamic information are associated with visual tags, use overlays to display this information on the phone's screen (see Fig. <ref type="figure">2</ref>). This allows the information to be seen as soon as the camera-phone is aimed at a tag, without even requiring users to click.</p><p>Beyond this specific application-design advice, our findings led to design lessons for camera-phone/tag applications more generally. Recall that, in Sect. 2, we observed that different tag-based applications impose very different requirements on tag-reading technology. For some applications (e.g. bus timetables) clicking speed may not be important. However, for other usage scenarios (e.g. the CoolTown Museum) clicking speed is more critical. The ease with which novice users were able to point and click on visual tags in the pointing device task and the success of the theme park application demonstrates that today's camera-phone technology is sufficient to support even heavily tag-centric interaction techniques. Given the findings presented in this paper we believe that all of the application scenarios proposed in Sect. 2 could feasibly be implemented on existing commodity camera-phones.</p><p>One caveat to this is that the wireless networking capabilities currently provided by mobile phone operators are often quite limited. Thus, even though our findings show that people can click on tags quickly using camera-phones, the latency incurred due to realworld networking technologies, such as GPRS for example, may prove unacceptable for some of the application scenarios proposed in Sect. 2. We expect that, over time, technical developments in mobile phone networks (e.g. 3G and 4G) will alleviate this problem. In the meantime we have demonstrated that Bluetooth can be used for low-latency short-range communication between camera-phones and tag-based situated services <ref type="bibr" target="#b26">[27]</ref>.</p><p>Despite growing adoption there are still some environments that, for reasons of security, prohibit the use of camera-phones (e.g. schools, gyms, industrial research labs). This was pointed out by one of our participants who often worked in schools. For environments in which cameras are unacceptable, other technologies such as RFID readers or even dedicated barcode scanners may provide a more suitable platform for tag-based interaction.</p><p>Our findings demonstrate that using camera-phones and visual tags to interact with mobile services offers a viable alternative to deploying shared public technology (e.g. the hypothetical theme park ticket machine that participants compared our camera-phone-based application to). By exploiting users' own camera-phones, the need to deploy shared public display/data-entry technology (and hence, the associated problems of vandalism and maintenance) can be avoided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Future work</head><p>The work presented in this paper should be considered as a first exploratory step. When interpreting our results, one must bear in mind that our evaluation was based on prototype technology presented in a laboratory setting. However, given the constraints of a laboratory environment, we believe that our hybrid approach, combining quantitative measures along with observational and interview data, was successful in answering the usability questions we posed.</p><p>We see our contribution as the beginning of a longer process of iterative user-centred design of cameraphone/tag-based services. In the future we would like to explore further the implications of using different tag formats (e.g. QR codes <ref type="bibr" target="#b0">[1]</ref>) and evaluate camera-phones and tags with a broader sample of the population. We also aim to use the findings presented in this paper to design, deploy and evaluate applications that exploit camera-phones and visual tags on a city-wide scale.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1</head><label>1</label><figDesc>Fig. 1 Using the phone-based tag reader</figDesc><graphic coords="3,51.07,49.61,238.08,114.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3</head><label>3</label><figDesc>Fig. 3 Performing the pointing device task</figDesc><graphic coords="4,51.07,49.61,238.08,150.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4</head><label>4</label><figDesc>Fig. 4 The graphs show the mean clicking time and the mean number of errors per participant, for session 1 and session 2. The error bars show standard deviations Fig. 5 Mean clicking times per participant by tag diameter (collapsed across both sessions)</figDesc><graphic coords="8,306.19,508.01,238.08,179.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>The tables show the mean (SD) clicking time per participant and the mean (SD) total number of errors per participant broken down for session 1 and session 2</figDesc><table><row><cell></cell><cell>Mean (SD)</cell></row><row><cell></cell><cell>clicking time (s)</cell></row><row><cell>Session 1</cell><cell>1.60 (0.65)</cell></row><row><cell>Session 2</cell><cell>1.51 (0.64)</cell></row><row><cell></cell><cell>Mean (SD)</cell></row><row><cell></cell><cell>total no. errors</cell></row><row><cell>Session 1</cell><cell>3.22 (2.18)</cell></row><row><cell>Session 2</cell><cell>3.33 (2.52)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We chose to study these ranges of diameters and distances because we felt they were typical of those that may be encountered in tagbased interactions with mobile services (e.g. consider the plausible tag sizes and inter-tag distances that may be designed into posters)</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p><ref type="bibr" target="#b1">2</ref> Although recorded by computer, the times were measured by the camera-phone itself. This ensured that the latency and jitter of the Bluetooth connection (used by the phone to communicate with the computer) did not affect our data</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_2"><p>As a rule of thumb, a number smaller than 3.5 mm requires a user to hold their camera-phone closer to tags than our participants did; conversely, a number greater than 3.5 mm requires users to hold their phones further away from tags</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements The authors would like to thank Rob Ennals for his valuable comments and suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m">NTT DoCoMo Unveils New mova 506i i-mode Phone Series</title>
		<imprint>
			<publisher>NTT DoCoMo press center</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Raw data from pointing-device task</title>
		<ptr target="http://chi.recoil.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<ptr target="http://semacode.org/" />
		<title level="m">Semacode website</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
	</analytic>
	<monogr>
		<title level="m">Connecting paper and online worlds by cellphone camera</title>
		<imprint>
			<date type="published" when="2004-10-07">7th October 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A survey of augmented reality</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Azuma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Presence</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="355" to="385" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Overcoming the lack of screen space on mobile computers</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Brewster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pers Ubiquitous Comput</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="188" to="205" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Trip: a low-cost vision-based location system for ubiquitous computing</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>De Ipin˜a</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mendonc¸a</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hopper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pers Ubiquitous Comput</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="206" to="219" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Finger tracking for interaction in augmented environments</title>
		<author>
			<persName><forename type="first">K</forename><surname>Dorfmueller-Ulhaas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schmalstieg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international symposium on augmented reality</title>
		<meeting>the IEEE international symposium on augmented reality<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A design tool for camera-based interaction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fails</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Olsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CHI</title>
		<meeting>CHI<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="449" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Interacting with big interfaces on small screens: a comparison of fisheye, zoom, and panning techniques</title>
		<author>
			<persName><forename type="first">C</forename><surname>Gutwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fedak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 conference on Graphics interface</title>
		<meeting>the 2004 conference on Graphics interface</meeting>
		<imprint>
			<publisher>Canadian Human-Computer Communications Society</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="145" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Moving pictures 2003: Worldwide camera phone survey, forecast, and analysis</title>
		<author>
			<persName><surname>Idc Research</surname></persName>
		</author>
		<ptr target="http://www.idcresearch.com/" />
		<imprint>
			<date type="published" when="2003">2003-2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A comparison of user interfaces for panning on a touch-controlled display</title>
		<author>
			<persName><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings ACM CHI</title>
		<meeting>ACM CHI</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="218" to="225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Marker tracking and HMD calibration for a video-based augmented reality conferencing system</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Billinghurst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International workshop on augmented reality (IWAR)</title>
		<imprint>
			<biblScope unit="page" from="85" to="94" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">People, places, things: web presence for the real world</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kindberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Barton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Caswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Debaty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gopal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Frid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schettino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Serra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Spasojevic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd IEEE workshop on mobile computing systems and applications (WMCSA&apos;00</title>
		<meeting>the 3rd IEEE workshop on mobile computing systems and applications (WMCSA&apos;00<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Security and trust in mobile interactions: a study of users perceptions and reasoning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kindberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sellen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Geelhoed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of UbiComp 2004</title>
		<meeting>UbiComp 2004<address><addrLine>Berlin Heidelberg New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">3205</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Videoplace-an artificial reality</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gionfriddo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hinrichsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI conference on human factors in computing systems</title>
		<meeting>the SIGCHI conference on human factors in computing systems<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bango spots link camera phones straight to mobile content</title>
		<author>
			<persName><forename type="first">Bango</forename><surname>Ltd</surname></persName>
		</author>
		<ptr target="http://www.bango.net/" />
	</analytic>
	<monogr>
		<title level="m">Press release</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">High Energy Magic Ltd. The spotcode framework</title>
		<ptr target="http://www.highenergymagic.com/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Extending fitts&apos; law to two-dimensional tasks</title>
		<author>
			<persName><forename type="first">Scott</forename><surname>Mackenzie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Buxton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI &apos;92: Proceedings of the SIGCHI conference on human factors in computing systems</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="219" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Fitts&apos; law as a performance model in human-computer interaction</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Mackenzie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD Thesis</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><surname>Masnik</surname></persName>
		</author>
		<ptr target="http://www.thefeature.com/article?articleid=100700&amp;ref=5960372" />
		<title level="m">Mobile barcode scanning catching on in Japan. The feature</title>
		<imprint>
			<date type="published" when="2004-07-02">2004. 2nd July 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The world through the computer: computer augmented interaction with real world environments</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rekimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nagao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM symposium on user interface software and technology</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="29" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A conceptual framework for camera phone-based interaction techniques</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rohs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zweifel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of pervasive 2005</title>
		<meeting>pervasive 2005<address><addrLine>Berlin Heidelberg New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">3468</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Using visual tags to bypass bluetooth device discovery</title>
		<author>
			<persName><forename type="first">D</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Madhavapeddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Upton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGMOBILE Mob Comput Commun Rev</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="53" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Sharp</surname></persName>
		</author>
		<ptr target="http://www.deviceforge.com/articles/AT5785815397.html" />
		<title level="m">New ways to maximize cameraphone technology. DeviceForge</title>
		<imprint>
			<date type="published" when="2004-07-01">2004. 1st July 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Aura: a mobile platform for object and location annotation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Davenport</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hwa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adjunct proceedings of Ubicomp</title>
		<imprint>
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Using smart phones to access site-specific services</title>
		<author>
			<persName><forename type="first">E</forename><surname>Toye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Madhavapeddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pervasive Computing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="60" to="66" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Bridging physical and virtual worlds with electronic tags</title>
		<author>
			<persName><forename type="first">R</forename><surname>Want</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Fishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gujar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Harrison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI &apos;99: Proceedings of the SIGCHI conference on human factors in computing systems</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="370" to="377" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
