<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Distance-Based Image Classification: Generalizing to New Classes at Near-Zero Cost</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Thomas</forename><surname>Mensink</surname></persName>
							<email>thomas.mensink@uva.nl..</email>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
							<email>jakob.verbeek@inria.fr.</email>
						</author>
						<author>
							<persName><forename type="first">Florent</forename><surname>Perronnin</surname></persName>
							<email>florent.perronnin@xrce.xerox.com</email>
						</author>
						<author>
							<persName><forename type="first">Gabriela</forename><surname>Csurka</surname></persName>
							<email>gabriela.csurka@xrce.xerox.com</email>
						</author>
						<author>
							<persName><forename type="middle">T</forename><surname>Mensink</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">ISLA Lab</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<addrLine>Science Park 904</addrLine>
									<postCode>1098 XH</postCode>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">LEAR Team</orgName>
								<orgName type="institution" key="instit2">INRIA Grenoble</orgName>
								<address>
									<addrLine>655 Avenue de l&apos;Europe</addrLine>
									<postCode>38330</postCode>
									<settlement>Montbonnot</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Xerox Research Centre Europe Grenoble</orgName>
								<address>
									<addrLine>6 chemin de Maupertuis</addrLine>
									<postCode>38240</postCode>
									<settlement>Meylan</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Distance-Based Image Classification: Generalizing to New Classes at Near-Zero Cost</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C5A90811DFE21458254ACAE7EFB09551</idno>
					<idno type="DOI">10.1109/TPAMI.2013.83</idno>
					<note type="submission">received 20 Nov. 2012; revised 28 Mar. 2013; accepted 23 Apr. 2013; published online 30 Apr. 2013. Recommended for acceptance by D. Ramanan.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Metric learning</term>
					<term>k-nearest neighbors classification</term>
					<term>nearest class mean classification</term>
					<term>large scale image classification</term>
					<term>transfer learning</term>
					<term>zero-shot learning</term>
					<term>image retrieval</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We study large-scale image classification methods that can incorporate new classes and training images continuously over time at negligible cost. To this end, we consider two distance-based classifiers, the k-nearest neighbor (k-NN) and nearest class mean (NCM) classifiers, and introduce a new metric learning approach for the latter. We also introduce an extension of the NCM classifier to allow for richer class representations. Experiments on the ImageNet 2010 challenge dataset, which contains over 10 6 training images of 1,000 classes, show that, surprisingly, the NCM classifier compares favorably to the more flexible k-NN classifier. Moreover, the NCM performance is comparable to that of linear SVMs which obtain current state-of-the-art performance. Experimentally, we study the generalization performance to classes that were not used to learn the metrics. Using a metric learned on 1,000 classes, we show results for the ImageNet-10K dataset which contains 10,000 classes, and obtain performance that is competitive with the current stateof-the-art while being orders of magnitude faster. Furthermore, we show how a zero-shot class prior based on the ImageNet hierarchy can improve performance when few training images are available.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ç 1 INTRODUCTION</head><p>I N this paper, we focus on the problem of large-scale, multiclass image classification, where the goal is to automatically assign an image to one class out of a finite set of alternatives, for example, the name of the main object appearing in the image, or a general label like the scene type of the image. To ensure scalability, often linear classifiers such as linear SVMs are used <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. Additionally, to speed-up classification, dimension reduction techniques could be used <ref type="bibr" target="#b2">[3]</ref>, or a hierarchy of classifiers could be learned <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. The introduction of the ImageNet dataset <ref type="bibr" target="#b5">[6]</ref>, which contains more than 14M manually labeled images of 22K classes, has provided an important benchmark for large-scale image classification and annotation algorithms. Recently, impressive results have been reported on 10,000 or more classes <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b6">[7]</ref>. A drawback of these methods, however, is that when images of new categories become available, new classifiers have to be trained from scratch at a relatively high computational cost.</p><p>Many real-life large-scale datasets are open-ended and dynamic: New images are continuously added to existing classes, new classes appear over time, and the semantics of existing classes might evolve too. Therefore, we are interested in distance-based classifiers which enable the addition of new classes and new images to existing classes at (near) zero cost. Such methods can be used continuously as new data becomes available and, additionally, alternated from time to time with a computationally heavier method to learn a good metric using all available training data. In particular, we consider two distance-based classifiers.</p><p>The first is the k-nearest neighbor (k-NN) classifier, which uses all examples to represent a class and is a highly nonlinear classifier that has shown competitive performance for image classification <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>. New images (of new classes) are simply added to the database, and can be used for classification without further processing.</p><p>The second is the nearest class mean classifier (NCM), which represents classes by their mean feature vector of its elements; see, for example, <ref type="bibr" target="#b9">[10]</ref>. Contrary to the k-NN classifier, this is an efficient linear classifier. To incorporate new images (of new classes), the relevant class means have to be adjusted or added to the set of class means. In Section 3, we introduce an extension that uses several prototypes per class, which allows a tradeoff between the model complexity and the computational cost of classification.</p><p>The success of these methods critically depends on the used distance functions. Therefore, we cast our classifier learning problem as one of learning a low-rank Mahalanobis distance which is shared across all classes. The dimensionality of the low-rank matrix is used as regularizer and to improve computational and storage efficiency.</p><p>In this paper, we explore several strategies for learning such a metric. For the NCM classifier, we propose a novel metric learning algorithm based on multiclass logistic discrimination (NCMML), where a sample from a class is enforced to be closer to its class mean than to any other class mean in the projected space. We show qualitatively and quantitatively the advantages of our NCMML approach over the classical Fisher discriminant analysis (FDA) <ref type="bibr" target="#b9">[10]</ref>. For k-NN classification, we rely on the large margin nearest neighbor (LMNN) framework <ref type="bibr" target="#b10">[11]</ref> and investigate two variations similar to the ideas presented in <ref type="bibr" target="#b10">[11]</ref> and <ref type="bibr" target="#b11">[12]</ref> that significantly improve classification performance.</p><p>Most of our experiments are conducted on the ImageNet Large Scale Visual Recognition Challenge 2010 (ILSVRC <ref type="bibr">'10)</ref> dataset, which consists of 1.2M training images of 1,000 classes. To apply the proposed metric learning techniques on such a large-scale dataset, we employ stochastic gradient descend (SGD) algorithms, which access only a small fraction of the training data at each iteration <ref type="bibr" target="#b12">[13]</ref>. To allow metric learning on high-dimensional image features of datasets that are too large to fit in memory, we use in addition product quantization (PQ) <ref type="bibr" target="#b13">[14]</ref>, a data compression technique that was recently used with success for large-scale image retrieval <ref type="bibr" target="#b14">[15]</ref> and classifier training <ref type="bibr" target="#b0">[1]</ref>.</p><p>As a baseline approach, we follow the winning entry of the ILSVRC '11 challenge <ref type="bibr" target="#b0">[1]</ref>: Fisher vector image representations <ref type="bibr" target="#b15">[16]</ref> are used to describe images and one-versusrest linear SVM classifiers are learned independently for each class. Surprisingly, we find that the NCM classifier outperforms the more flexible k-NN classifier. Moreover, the NCM classifier performs on par with the SVM baseline, and shows competitive performance on new classes.</p><p>This paper extends our earlier work <ref type="bibr" target="#b16">[17]</ref>, as follows: First, for the NCM classifier, in Section 3 we compare the NCMML metric learning to the classic FDA, we introduce an extension which uses multiple centroids per class, we explore a different learning objective, and we examine the critical points of the objective. Second, in Section 4, we provide more details on the SGD triplet sampling strategy used for LMNN metric learning, and we present an efficient gradient evaluation method. Third, we extend the experimental evaluation with an experiment where NCMML is used to learn a metric for instance level image retrieval.</p><p>The remainder of the paper is organized as follows: We first discuss a selection of related works which are most relevant to this paper. In Section 3, we introduce the NCM classifier and the NCMML metric learning approach. In Section 4, we review LMNN metric learning for k-NN classifiers. We present extensive experimental results in Section 5, analyzing different aspects of the proposed methods and comparing them to the current state of the art in different application settings such as large scale image annotation, transfer learning, and image retrieval. Finally, we present our conclusions in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In this section, we review related work on large-scale image classification, metric learning, and transfer learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Large-Scale Image Classification</head><p>The ImageNet dataset <ref type="bibr" target="#b5">[6]</ref> has been a catalyst for research on large-scale image annotation. The current state-of-the-art <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref> uses efficient linear SVM classifiers trained in a oneversus-rest manner in combination with high-dimensional bag-of-words <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref> or Fisher vector representations <ref type="bibr" target="#b15">[16]</ref>. Besides one-versus-rest training, large-scale ranking-based formulations have also been explored in <ref type="bibr" target="#b2">[3]</ref>. Interestingly, their WSABIE approach performs joint classifier learning and dimensionality reduction of the image features. Operating in a lower dimensional space acts as a regularization during learning, and also reduces the cost of classifier evaluation at test time. Our proposed NCM approach also learns low-dimensional projection matrices, but the weight vectors are constrained to be the projected class means. This allows for efficient addition of novel classes.</p><p>In <ref type="bibr" target="#b2">[3]</ref> and <ref type="bibr" target="#b6">[7]</ref>, k-NN classifiers were found to be competitive with linear SVM classifiers in a very largescale setting involving 10,000 or more classes. The drawback of k-NN classifiers, however, is that they are expensive in storage and computation since, in principle, all training data needs to be kept in memory and accessed to classify new images. This holds even more for Naive-Bayes nearest neighbor (NBNN) <ref type="bibr" target="#b8">[9]</ref>, which does not use descriptor quantization, but requires storage of all local descriptors of all training images. The storage issue is also encountered when SVM classifiers are trained since all training data needs to be processed in multiple passes. Product quantization was introduced in <ref type="bibr" target="#b14">[15]</ref> as a lossy compression mechanism for local SIFT descriptors in a bag-of-features image retrieval system. It has been subsequently used to compress bag-of-words and Fisher vector image representations in the context of image retrieval <ref type="bibr" target="#b19">[20]</ref> and classifier training <ref type="bibr" target="#b0">[1]</ref>. We also exploit PQ encoding in our work to compress high-dimensional image signatures when learning our metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Metric Learning</head><p>There is a large body of literature on metric learning, but here we limit ourselves to highlighting just several methods that learn metrics for (image) classification problems. Other methods aim at learning metrics for verification problems and essentially learn binary classifiers that threshold the learned distance to decide whether two images belong to the same class or not; see, for example, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>. Yet another line of work concerns metric learning for ranking problems, for example, to address text retrieval tasks as in <ref type="bibr" target="#b23">[24]</ref>.</p><p>Among those methods that learn metrics for classification, the large margin nearest neighbor approach of <ref type="bibr" target="#b10">[11]</ref> is specifically designed to support k-NN classification. It tries to ensure that for each image a predefined set of target neighbors from the same class are closer than samples from other classes. Since the cost function is defined over triplets of points-that can be sampled in an SGD training procedure-this method can scale to large datasets. The set of target neighbors is chosen and fixed using the ' 2 metric in the original space; this can be problematic as the ' 2 distance might be quite different from the optimal metric for image classification. Therefore, we explore two variants of LMNN that avoid using such a predefined set of target neighbors, similar to the ideas presented in <ref type="bibr" target="#b11">[12]</ref>.</p><p>The large margin nearest local mean classifier <ref type="bibr" target="#b24">[25]</ref> assigns a test image to a class based on the distance to the mean of its nearest neighbors in each class. This method was reported to outperform LMNN but requires computing all pairwise distances between training instances and therefore does not scale well to large datasets. Similarly, TagProp <ref type="bibr" target="#b7">[8]</ref> suffers from the same problem; it consists of assigning weights to training samples based on their distance to the test instance and in computing the class prediction by the total weight of samples of each class in a neighborhood.</p><p>Other closely related methods are metric learning by collapsing classes <ref type="bibr" target="#b25">[26]</ref> and neighborhood component analysis <ref type="bibr" target="#b26">[27]</ref>. As TagProp, for each data point these define weights to other data points proportional to the exponent of negative distance. In <ref type="bibr" target="#b25">[26]</ref>, the target is to learn a distance that makes the weights uniform for samples of the same class and close to zero for other samples, while in <ref type="bibr" target="#b26">[27]</ref> the target is only to ensure that zero weight is assigned to samples from other classes. These methods also require computing distances between all pairs of data points. Because of their poor scaling, we do not consider any of these methods below.</p><p>Closely related to our NCMML metric learning approach for the NCM classifier is the LESS model of <ref type="bibr" target="#b27">[28]</ref>. They learn a diagonal scaling matrix to modify the ' 2 distance by rescaling the data dimensions, and include an ' 1 penalty on the weights to perform feature selection. However, in their case, NCM is used to address small sample size problems in binary classification, i.e., cases where there are fewer training points (tens to hundreds) than features (thousands). Our approach differs significantly in that 1) we work in a multiclass setting and 2) we learn a low-dimensional projection which allows efficiency in large scale.</p><p>Another closely related method is the Taxonomy-embedding method of <ref type="bibr" target="#b28">[29]</ref>, where a nearest prototype classifier is used in combination with a hierarchical cost function. Documents are embedded in a lower dimensional space in which each class is represented by a single prototype. In contrast to our approach, they use a predefined embedding of the images and learn low-dimensional classifies, and therefore their method more closely resembles the WSABIE method of <ref type="bibr" target="#b2">[3]</ref>.</p><p>The Sift-bag kernel of <ref type="bibr" target="#b29">[30]</ref> is also related to our method since it uses an NCM classifier and an ' 2 distance in a subspace that is orthogonal to the subspace with maximum within-class variance. However, it involves computing the first eigenvectors of the within-class covariance matrix, which has a computational cost between OðD 2 Þ and OðD 3 Þ, undesirable for high-dimensional feature vectors. Moreover, this metric is heuristically obtained, rather than directly optimized for maximum classification performance.</p><p>Finally, the image-to-class metric learning method of <ref type="bibr" target="#b30">[31]</ref> learns per class a Mahalanobis metric, which, in contrast to our method, cannot generalize to new classes. Besides, it uses the idea of NBNN <ref type="bibr" target="#b8">[9]</ref>, and therefore requires the storage of all local descriptors of all images, which is impractical for the large-scale datasets used in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Transfer Learning</head><p>The term transfer learning is used to refer to methods that share information across classes during learning. Examples of transfer learning in computer vision include the use of part-based or attribute class representations. Part-based object recognition models <ref type="bibr" target="#b31">[32]</ref> define an object as a spatial constellation of parts, and share the part detectors across different classes. Attribute-based models <ref type="bibr" target="#b32">[33]</ref> characterize a category (e.g., a certain animal) by a combination of attributes (e.g., is yellow, has stripes, is carnivore), and share the attribute classifiers across classes. Other approaches include biasing the weight vector learned for a new class toward the weight vectors of classes that have already been trained <ref type="bibr" target="#b33">[34]</ref>. Zero-shot learning <ref type="bibr" target="#b34">[35]</ref> is an extreme case of transfer learning where for a new class no training instances are available but a description is provided in terms of parts, attributes, or other relations to already learned classes. Transfer learning is related to multitask learning, where the goal is to leverage the commonalities between several distinct but related classification problems, or classifiers learned for one type of images (e.g., ImageNet) are adapted to a new domain (e.g., imagery obtained from a robot camera); see, for example, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>.</p><p>In <ref type="bibr" target="#b37">[38]</ref>, various transfer learning methods were evaluated in a large-scale setting using the ILSVRC '10 dataset. They found transfer learning methods to have little added value when training images are available for all classes. In contrast, transfer learning was found to be effective in a zero-shot learning setting, where classifiers were trained for 800 classes and performance was tested in a 200-way classification across the held-out classes.</p><p>In this paper, we also aim at transfer learning in the sense that we allow only a trivial amount of processing on the data of new classes (storing in a database, or averaging), and rely on a metric that was trained on other classes to recognize the new ones. In contrast to most works on transfer learning, we do not use any intermediate representation in terms of parts or attributes nor do we train classifiers for the new classes. While also considering zeroshot learning, we further evaluate performance when combining a zero-shot model inspired by Rohrbach et al. <ref type="bibr" target="#b37">[38]</ref> with progressively more training images per class, from one up to thousands. We find that the zero-shot model provides an effective prior when a small amount of training data is available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">THE NEAREST CLASS MEAN CLASSIFIER</head><p>The nearest class mean classifier assigns an image to the class c Ã 2 f1; . . . ; Cg with the closest mean:</p><formula xml:id="formula_0">c Ã ¼ argmin c2f1;...;Cg dðx x x x; c Þ; ð1Þ c ¼ 1 N c X i:y i ¼c x x x x i ;<label>ð2Þ</label></formula><p>where dðx x x x; c Þ is the euclidean distance between an image x x x x and the class mean c , and y i is the ground-truth label of image i, and N c is the number of training images in class c. Next, we introduce our NCM metric learning approach and its relations to existing models. Then, we present an extension to use multiple centroids per class which transforms the NCM into a nonlinear classifier. Finally, we explore some variants of the objective which allow for smaller SGD batch sizes, and we give some insights into the critical points of the objective function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Metric Learning for the NCM Classifier</head><p>In this section, we introduce our metric learning approach, which we will refer to as "nearest class mean metric learning" (NCMML). We replace the euclidean distance in NCM by a learned (squared) Mahalanobis distance:</p><formula xml:id="formula_1">d M ðx x x x; x x x x 0 Þ ¼ ðx x x x À x x x x 0 Þ &gt; Mðx x x x À x x x x 0 Þ;<label>ð3Þ</label></formula><p>where x x x x and x x x x 0 are D-dimensional vectors and M is a positive definite matrix. We focus on low-rank metrics with M ¼ W &gt; W and W 2 IR dÂD , where d D acts as regularizer and improves efficiency for computation and storage. The Mahalanobis distance induced by W is equivalent to the squared ' 2 distance after linear projection of the feature vectors on the rows of W :</p><formula xml:id="formula_2">d W ðx x x x; x x x x 0 Þ ¼ ðx x x x À x x x x 0 Þ &gt; W &gt; W ðx x x x À x x x x 0 Þ ¼ kWx x x x À Wx x x x 0 k 2 2 :<label>ð4Þ</label></formula><p>We do not consider using the more general formulation of</p><formula xml:id="formula_3">M ¼ W &gt; W þ S,</formula><p>where S is a diagonal matrix, as in <ref type="bibr" target="#b23">[24]</ref>. While this formulation requires only D additional parameters to estimate, it still requires computing distances in the original high-dimensional space. This is costly for the dense and high-dimensional (4K-64K) Fisher vectors representations we use in our experiments; see Section 5.</p><p>We formulate the NCM classifier using a probabilistic model based on multiclass logistic regression and define the probability for a class c given an feature vector x x x x as</p><formula xml:id="formula_4">pðc j x x x xÞ ¼ exp À À 1 2 d W ðx x x x; c Þ Á P C c 0 ¼1 exp À À 1 2 d W ðx x x x; c 0 Þ Á :<label>ð5Þ</label></formula><p>This definition may also be interpreted as giving the posterior probabilities of a generative model where pðx x x x i j cÞ ¼ N ðx x x x i ; c ; AEÞ is a Gaussian with mean c , and a covariance matrix AE ¼ ðW &gt; W Þ À1 , which is shared across all classes. 1  The class probabilities pðcÞ are set to be uniform over all classes. Later, in <ref type="bibr" target="#b20">(21)</ref>, we formulate an NCM classifier with nonuniform class probabilities.</p><p>To learn the projection matrix W , we maximize the loglikelihood of the correct predictions of the training images:</p><formula xml:id="formula_5">L ¼ 1 N X N i¼1 ln pðy i j x x x x i Þ:<label>ð6Þ</label></formula><p>The gradient of the NCMML objective ( <ref type="formula" target="#formula_5">6</ref>) is</p><formula xml:id="formula_6">r W L ¼ 1 N X N i¼1 X C c¼1 ic W z z z z ic z z z z &gt; ic ;<label>ð7Þ</label></formula><formula xml:id="formula_7">where ic ¼ pðc j x x x x i Þ À ½ ½y i ¼ c , z z z z ic ¼ c À x x x</formula><p>x i , and we use the Iverson brackets ½ ½ Á to denote the indicator function that equals one if its argument is true and zero otherwise.</p><p>Although not included above for clarity, the terms in the log-likelihood in (6) could be weighted in cases where the class distributions in the training data are not representative for those when the learned model is applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Relation to Existing Linear Classifiers</head><p>First, we compare the NCMML objective with the classic Fisher discriminant analysis <ref type="bibr" target="#b9">[10]</ref>. The objective of FDA is to find a projection matrix W that maximizes the ratio of between-class variance to within-class variance:</p><formula xml:id="formula_8">L FDA ¼ tr WS B W &gt; WS W W &gt; ;<label>ð8Þ</label></formula><p>where</p><formula xml:id="formula_9">S B ¼ P C c¼1 N c N ð À c Þð À c Þ &gt;</formula><p>is the weighted covariance matrix of the class centers ( being the data center), and</p><formula xml:id="formula_10">S W ¼ P C c¼1</formula><p>Nc N AE c is the weighted sum of within class covariance matrices AE c ; see, for example, <ref type="bibr" target="#b9">[10]</ref> for details.</p><p>In the case where the within-class covariance for each class equals the identity matrix, the FDA objective seeks the direction of maximum variance in S B , i.e., it performs a PCA projection on the class means. To illustrate this, we show an example of a 2D problem with three classes in Fig. <ref type="figure">1</ref>. In contrast, our NCMML method aims at separating the classes which are nearby in the projected space so as to ensure correct predictions. The resulting projection separates the three classes reasonably well.</p><p>To relate the NCM classifier to other linear classifiers, we represent them using the class-specific score functions:</p><formula xml:id="formula_11">fðc; x x x xÞ ¼ w w w w &gt; c x x x x þ b c ;<label>ð9Þ</label></formula><p>which are used to assign samples to the class with maximum score. NCM can be recognized as a linear classifier by defining f NCM with bias and weight vectors given by <ref type="formula" target="#formula_4">5</ref>) can be written as 1. Strictly speaking the covariance matrix is not properly defined as the low-rank matrix W &gt; W is noninvertible.</p><formula xml:id="formula_12">b c ¼ À 1 2 k W c k 2 2 ;<label>ð10Þ</label></formula><formula xml:id="formula_13">w w w w c ¼ W &gt; W c :<label>ð11Þ</label></formula><formula xml:id="formula_14">This is because À 1 2 d W ðx x x x; c Þ in (</formula><p>Fig. <ref type="figure">1</ref>. Illustration to compare FDA (left) and NCMML (right); the obtained projection direction is indicated by the gray line on which the projected samples are also plotted. For FDA the result is clearly suboptimal since the blue and green classes are collapsed in the projected space. The proposed NCMML method finds a projection direction which separates the classes reasonably well.</p><formula xml:id="formula_15">À 1 2 k Wx x x x k 2 2 À 1 2 k W c k 2 2 þ x x x x &gt; W &gt; W c ;</formula><p>where the first term is independent of the class c and therefore irrelevant for classification. These definitions allow us to relate the NCM classifier to other linear methods. For example, we obtain standard multiclass logistic regression if the restrictions on b c and w w w w c are removed. Note that these are precisely the restrictions that allow us to add new classes at near-zero cost since the class specific parameters b c and w w w w c are defined by just the class means c and the class-independent projection W .</p><p>In WSABIE <ref type="bibr" target="#b2">[3]</ref> f WSABIE is defined using b c ¼ 0 and</p><formula xml:id="formula_16">w w w w c ¼ W &gt; v v v v c ;<label>ð12Þ</label></formula><p>where W 2 IR dÂD is also a low-rank projection matrix shared between all classes and v v v v c is a class-specific weight vector of dimensionality d, both learned from data. This is similar to NCM if we set v v v v c ¼ W c . As in multiclass logistic regression, however, for WSABIE the v v v v c need to be learned from scratch for new classes.</p><p>The NCM classifier can also be related to the solution of ridge-regression (RR, or regularized linear least-squares regression), where the parameters b c and w w w w c are learned by optimizing the squared loss:</p><formula xml:id="formula_17">L RR ¼ 1 N X i ðf RR ðc; x x x x i Þ À y ic Þ 2 þ k w w w w c k 2 2 ;<label>ð13Þ</label></formula><p>where acts as regularizer, and where y ic ¼ 1, if image i belongs to class c, and y ic ¼ 0 otherwise. The loss L RR can be minimized in closed form and leads to</p><formula xml:id="formula_18">b c ¼ N c N ; and w w w w c ¼ N c N &gt; c ðAE þ IÞ À1 ;<label>ð14Þ</label></formula><p>where AE is the (class-independent) data covariance matrix. Just like the NCM classifier, the RR classifier also allows us to add new classes at low cost since the class-specific parameters can be found from the class means and counts once the data covariance matrix AE has been estimated. Moreover, if N c is equal for all classes, RR is similar to NCM with W set such that</p><formula xml:id="formula_19">W &gt; W ¼ ðAE þ IÞ À1 .</formula><p>Finally, the Taxonomy-embedding <ref type="bibr" target="#b28">[29]</ref> scores a class by</p><formula xml:id="formula_20">f TAX ðc; x x x xÞ ¼ v v v v &gt; c W &gt; Wx x x x À 1 2 k v v v v c k 2 2 ;<label>ð15Þ</label></formula><p>where W 2 IR CÂD projects the data to a C-dimensional space and is set using a closed-form solution based on ridge-regression. The class-specific weight vectors v v v v c are learned from the data. Therefore, this method relates to the WSABIE method; it learns the classifier in low-dimensional space (if C &lt; D), but in this case the projection matrix W is given in closed-form. It also shares the disadvantage of the WSABIE method: It cannot generalize to novel classes without retraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Nonlinear NCM with Multiple Class Centroids</head><p>In this section, we extend the NCM classifier to allow for more flexible class representations, which result in nonlinear classification. The idea is to represent each class by a set of centroids, instead of only the class mean.</p><p>Assume that we have a set of k centroids fm m m m cj g k j¼1 for each class c. The posterior probability for class c can be defined as</p><formula xml:id="formula_21">pðc j x x x xÞ ¼ X k j¼1 pðm m m m cj j x x x xÞ;<label>ð16Þ</label></formula><formula xml:id="formula_22">pðm m m m cj j x x x xÞ ¼ 1 Z exp À 1 2 d W ðx x x x; m m m m cj Þ ;<label>ð17Þ</label></formula><p>where pðm m m m cj j x x x xÞ denotes the posterior of a centroid m m m m cj , and Z ¼ P c P j expðÀ 1 2 d W ðx x x x; m m m m cj ÞÞ is the normalizer. The value k offers a transition between NCM (k ¼ 1) and a weighted k-NN (k equals all images per class), where the weight of each neighbor is defined by the soft-min of its distance, c.f., <ref type="bibr" target="#b16">(17)</ref>. This is similar to TagProp <ref type="bibr" target="#b7">[8]</ref>, used for multilabel image annotation.</p><p>This model also corresponds to a generative model, where the probability for a feature vector x x x x, to be generated by class c, is given by a Gaussian mixture distribution:</p><formula xml:id="formula_23">pðx x x x j cÞ ¼ X k j¼1 cj N ðx x x x i ; m m m m cj ; AEÞ;<label>ð18Þ</label></formula><p>with equal mixing weights cj ¼ 1=k, and the covariance matrix AE shared among all classes. We refer to this method as the nearest class multiple centroids (NCMC) classifier. A similar model was independently developed recently for image retrieval in <ref type="bibr" target="#b38">[39]</ref>. Their objective, however, is to discriminate between different senses of a textual query, and they use a latent model to select the sense of a query.</p><p>To learn the projection matrix W , we again maximize the log-likelihood of correct classification, for which the gradient w.r.t. W in this case is given by</p><formula xml:id="formula_24">r W L ¼ 1 N X i;c;j icj W z z z z icj z z z z &gt; icj ;<label>ð19Þ</label></formula><p>where z z z z icj ¼ m m m m cj À x x x x i , and</p><formula xml:id="formula_25">icj ¼ pðm m m m cj j x x x x i Þ À ½ ½c ¼ y i pðm m m m cj j x x x x i Þ P j 0 pðm m m m cj 0 j x x x x i Þ :<label>ð20Þ</label></formula><p>To obtain the centroids of each class, we apply k-means clustering on the features x x x x belonging to that class, using the ' 2 distance. Instead of using a fixed set of class means, it could be advantageous to iterate the k-means clustering and the learning of the projection matrix W . Such a strategy allows the set of class centroids to more precisely represent the distribution of the images in the projected space, and might further improve the classification performance. However, the experimental validation of such a strategy falls beyond the scope of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Alternative Objective for Small SGD Batches</head><p>Computing the gradients for NCMML in <ref type="bibr" target="#b6">(7)</ref> and NCMC in <ref type="bibr" target="#b18">(19)</ref> is relatively expensive, regardless of the number of m samples used per SGD iteration. The cost of this computation is dominated by the computation of the squared distances d W ðx x x x; c Þ, required to compute the m Â C probabilities pðc j x x x xÞ for C classes in the SGD update. To compute these distances we have two options. First, we can compute the m Â C difference vectors ðx x x x À c Þ, project these on the d Â D matrix W , and compute the norms of the projected difference vectors, at a total cost of OðdDðmCÞ þ mCðd þ DÞÞ. Second, we can first project both the m data vectors and C class centers, and then compute distances in the low-dimensional space, at a total cost of OðdDðm þ CÞ þ mCðdÞÞ. Note that the latter option has a lower complexity, but still requires projecting all class centers at a cost OðdDCÞ, which will be the dominating cost when using small SGD batches with m ( C. Therefore, in practice, we are limited to using SGD batch sizes with m % C ¼ 1;000 samples.</p><p>To accommodate fast SGD updates based on smaller batch sizes, we replace the euclidean distance in (5) by the negative dot-product plus a class-specific bias s c . The probability for class c is now given by</p><formula xml:id="formula_26">pðc j x x x x i Þ ¼ 1 Z exp À x x x x &gt; i W &gt; W c þ s c Á ;<label>ð21Þ</label></formula><p>where Z denotes the normalizer. The objective is still to maximize the log-likelihood of ( <ref type="formula" target="#formula_5">6</ref>). The efficiency gain stems from the fact that we can avoid projecting the class centers on W by twice projecting the data vectors, x x x x i ¼ x x x x &gt; i W &gt; W , and then computing dot-products in high-dimensional space hx x x x i ; c i. For a batch of m images, the first step costs OðmDdÞ and the latter OðmCDÞ, resulting in a complexity of OðdDðmÞ þ mCðDÞÞ. This complexity scales linearly with m, and is lower for small batches with m d since in that case it is more costly to project the class vectors on W than on the double-projected data vectors x x x x i . For clarity, we summarize the complexity of the different alternatives we considered in Table <ref type="table" target="#tab_0">1</ref>.</p><p>A potential disadvantage of this approach is that we need to determine the class-specific bias s c when data of a new class becomes available, which would require more training than just computing the data mean for the new class. However, we expect a strong correlation between the learned bias s c and the bias based on the norm of the projected mean b c , as shown in Fig. <ref type="figure" target="#fig_0">2</ref>.</p><p>Similarly, as for (5), we could interpret the class probabilities in <ref type="bibr" target="#b20">(21)</ref> as being generated by a generative model where the class-conditional models pðx x x x j cÞ are Gaussian with a shared covariance matrix. In this interpretation, the class-specific biases s c define class prior probabilities given by pðcÞ / expð 1 2 kW c k 2 2 þ s c Þ. Therefore, a uniform prior is obtained by setting s c ¼ À 1  2 kW c k 2 2 ¼ b c . A uniform prior is reasonable for the ILSVRC '10 data since the classes are near uniform in the training and test data.</p><p>Experimentally, we find that using this formulation yields comparable results as obtained with the euclidean distance of (5). For example, on ILSVRC '10 with 4Kdimensional features and 128D projection matrix W , the classification error decreases from 39.2 percent when using s c to 39.0 percent when using b c at evaluation time, c.f., Table <ref type="table" target="#tab_2">4</ref>. Thus, we can use the metric learned using <ref type="bibr" target="#b20">(21)</ref> in combination with the norm of the projected mean as bias, which is easily computed for new classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Critical Points of Low Rank Metric Learning</head><p>We use a low-rank Mahalanobis distance where M ¼ W &gt; W as a way to reduce the number of parameters and to gain in computational efficiency. Learning a full Mahalanobis distance matrix M, however, has the advantage that the distance is linear in M and that the multiclass logistic regression objective of ( <ref type="formula" target="#formula_5">6</ref>) is therefore concave in M; see details in <ref type="bibr">[40, p. 74</ref>]. Using a low-rank formulation, on the other hand, yields a distance which is quadratic in the parameters W ; therefore the objective function is no longer concave. In this section, we investigate the critical-points of the low-rank formulation by analyzing W when the optimization reaches a (local) minimum and considering the gradient for the corresponding full matrix M ¼ W &gt; W .</p><p>The gradient of the objective of (6) w.r.t. to M is</p><formula xml:id="formula_27">r M L ¼ 1 N X i;c ic z z z z ic z z z z &gt; ic H;<label>ð22Þ</label></formula><p>where ic ¼ ½ ½y i ¼ c À pðc j x x x x i Þ and z z z z ic ¼ c À x x x x i . Then <ref type="bibr" target="#b6">(7)</ref> follows from the matrix chain rule, and we redefine r W L 2WH. From the gradient w.r.t. W we immediately observe that W ¼ 0 leads to a degenerate case to obtain a zero gradient, and similarly for each row of W . Below, we concentrate on the nondegenerate case.</p><p>We observe that H is a symmetric matrix containing the difference of two positive definite matrices. In the analysis, we use the eigenvalue decomposition of H ¼ V ÃV &gt; , with the columns of V being the eigenvectors, and the eigenvalues are on the diagonal of Ã.</p><p>We can now express the gradient for W as</p><formula xml:id="formula_28">r W L ¼ 2WV ÃV &gt; G:<label>ð23Þ</label></formula><p>Thus, the gradient of the ith row of W , which we denote by g g g g i , is a linear combination of the eigenvectors of H:</p><formula xml:id="formula_29">g g g g i X j j hw w w w i ; v v v v j iv v v v j ;<label>ð24Þ</label></formula><p>where w w w w i and v v v v j denote the ith row of W and the jth column of V , respectively. Thus, an SGD gradient update will drive a row of W toward the eigenvectors of H that 1) have a large positive eigenvalue, and 2) are most  aligned with that row of W . This is intuitive since we would expect the low-rank formulation to focus on the most significant directions of the full-rank metric. Moreover, the expression for the gradient in <ref type="bibr" target="#b23">(24)</ref> shows that at a critical point W Ã of the objective function, all linear combination coefficients are zero: 8 i;j : j hw w w w Ã i ; v v v v j i ¼ 0. This indicates that at the critical point, for each row w w w w Ã i and each eigenvector v v v v j , it holds that either w w w w Ã i is orthogonal to v v v v j or that v v v v j has a zero associated eigenvalue, i.e., j ¼ 0. Thus, at a critical point W Ã , the corresponding gradient for the full rank formulation at that point, with M Ã ¼ W Ã&gt; W Ã , is zero in the subspace spanned by W Ã .</p><p>Given this analysis, we believe it is unlikely to attain poor local minima using the low rank formulation. Indeed, the gradient updates for W are aligned with the most important directions of the corresponding full-rank gradient, and at convergence the full-rank gradient is zero in the subspace spanned by W . To confirm this, we have also experimentally investigated this by training several times with different random initializations of W . We observe that the classification performance differs at most AE0:1 percent on any of the error measures used in Section 5, and that the number of SGD iterations selected by the early stopping procedure are of the same order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">K-NN METRIC LEARNING</head><p>We compare the NCM classifier to the k-NN classifier, a frequently used distance-based classifier. For successful k-NN classification, the majority of the nearest neighbors should be of the same class. This is reflected in the LMNN metric learning objective <ref type="bibr" target="#b10">[11]</ref>, which is defined over triplets consisting of a query image q, an image p from the same class, and an image n from another class:</p><formula xml:id="formula_30">L qpn ¼ ½1 þ d W ðx x x x q ; x x x x p Þ À d W ðx x x x q ; x x x x n Þ þ ;<label>ð25Þ</label></formula><p>where ½z þ ¼ maxð0; zÞ. The hinge-loss for a triplet is zero if the negative image n is at least one distance unit farther from the query q than the positive image p, and the loss is positive otherwise. The final learning objective sums the losses over all triplets:</p><formula xml:id="formula_31">L LMNN ¼ X q X p2P q X n2N q L qpn ;<label>ð26Þ</label></formula><p>where P q and N q denote a predefined set of positive and negative images for each query image q. Also, in this case we could weight the terms in the loss function to account for nonrepresentative class proportions in the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Choice of Target Neighbors</head><p>In the basic version of LMNN, the set of targets P q for a query q is set to the query's k-nearest neighbors from the same class, using the ' 2 distance. The rationale is that if we ensure that these targets are closer than the instances of the other classes, then the k-NN classification will succeed. However, this implicitly assumes that the ' 2 -targets will also be the closest points from the same class using the learned metric, which in practice might not be the case. Therefore, we consider two alternatives to using a fixed set of target neighbors.</p><p>First, we consider P q to contain all images of the same class as q; hence the selection is independent on the metric. This is similar to <ref type="bibr" target="#b11">[12]</ref>, where the same type of loss was used to learn image similarity defined as the scalar product between feature vectors after a learned linear projection.</p><p>Second, we consider dynamically updating P q to contain the k images of the same class that are closest to q using the current metric W ; hence different target neighbors can be selected depending on the metric. This method corresponds to minimizing the loss function also with respect to the choice of P q . A similar approach was proposed in <ref type="bibr" target="#b10">[11]</ref>, where every T iterations, P q is redefined using target neighbors according to the current metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Triplet Sampling Strategy</head><p>Here, we describe a sampling strategy which obtains the maximal number of triplets from m images selected per SGD iteration. Using a small m is advantageous since the cost of the gradient evaluation is in large part determined by computing the projections Wx x x x of the images, and, if used, the cost of decompressing the PQ encoded signatures.</p><p>To generate triplets, we first select uniformly, at random, a class c that will provide the query and positive images. When P q is set to contain all images of the same class, we sample 2  3 m images from the class c, and 1 3 m images across other classes. In this manner, we can construct about 4  27 m 3 triplets, i.e., about 4 million triplets for m ¼ 300 used in our experiments; see our technical report <ref type="bibr" target="#b40">[41]</ref> for more details.</p><p>For other choices of P q we do the following:</p><p>. For a fixed set of target neighbors, we still sample 1 3 m negative images and take as many query images together with their target neighbors until we obtain 2 3 m images allocated for the positive class. . For a dynamic set of target neighbors, we simply select the closest neighbors among the 2 3 m sampled positive images using the current metric W . Although approximate, this avoids computing the dynamic target neighbors among all the images in the positive class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Efficient Gradient Evaluation</head><p>For either choice of the target set P q , the gradient can be computed without explicitly iterating over all triplets by sorting the distances w.r.t. query images. The subgradient of the loss of a triplet is given by</p><formula xml:id="formula_32">r W L qpn ¼ ½ ½L qpn &gt; 0 2 W À x x x x qp x x x x &gt; qp À x x x x qn x x x x &gt; qn Á ;<label>ð27Þ</label></formula><p>where x x x x qp ¼ x x x x q À x x x x p , x x x x qn ¼ x x x x q À x x x x n . We can write the gradient w.r.t. L LMNN in matrix form as</p><formula xml:id="formula_33">r W L LMNN ¼ 2 W XAX &gt; ;<label>ð28Þ</label></formula><p>where X contains the m feature vectors used in an SGD iteration, and A is a coefficient matrix. This shows that once A is available, the gradient can be computed in time Oðm 2 Þ, even if a much larger number of triplets is used.</p><p>When P q contains all images of the same class, A can be computed from the number of loss generating triplets:</p><formula xml:id="formula_34">A qn ¼ 2 X p ½ ½L qpn &gt; 0 ; A pq ¼ À2 X n ½ ½L qpn &gt; 0 :</formula><p>Once A qp and A qn are known, the coefficients A qq , A pp , and A nn are obtained from the former by summing, for example, A qq ¼ P p A qp À P n A qn ; see <ref type="bibr" target="#b40">[41]</ref> for more details. In Algorithm 1, we describe how to efficiently compute the coefficients. The same algorithm can be applied when using a small set of fixed or dynamic target neighbors. In particular, the sorted list allows us to dynamically determine the target neighbors at a negligible additional cost. In this case only the selected target neighbors obtain nonzero coefficients and we only accumulate the number of target neighbors after each position in step 3 of the algorithm.</p><p>Algorithm 1. Compute coefficients A qn and A qp .</p><p>1) Sort distances w.r.t. q in ascending order; for positive images use d W ðx x x x q ; x x x x p Þ þ 1 to account for the margin. 2) Accumulate, from start to end, the number of negative images up to each position. 3) Accumulate, from end to start, the number of positive images after each position. 4) Read-off the number of hinge-loss generating triplets of image p or n.</p><p>The cost of this algorithm is Oðm log mÞ per query, and thus Oðm<ref type="foot" target="#foot_0">2</ref> log mÞ when using OðmÞ query images per iteration. This is significantly faster than explicitly looping over all Oðm 3 Þ triplets.</p><p>Note that while this algorithm enables fast computation of the subgradient of the loss, the value of the loss itself cannot be determined using this method. However, this is not a problem when using an SGD approach as it only requires gradient evaluations, not function evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTAL EVALUATION</head><p>In this section, we experimentally validate our models described in the previous sections. We first describe the dataset and evaluation measures used in our experiments, followed by the presentation of the experimental results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup and Baseline Approach</head><p>Dataset. In most of our experiments we use the dataset of the ImageNet Large Scale Visual Recognition 2010 challenge (ILSVRC <ref type="bibr">'10)</ref>. 2 This data set contains 1.2M training images of 1,000 object classes (with between 660 to 3,047 images per class), a validation set of 50K images (50 per class), and a test set of 150K images (150 per class).</p><p>In some of the experiments, we use the ImageNet-10K dataset introduced in <ref type="bibr" target="#b6">[7]</ref>, which consists of 10,184 classes from the nodes of the ImageNet hierarchy with more than 200 images. We follow <ref type="bibr" target="#b0">[1]</ref> and use 4.5M images as the training set, 50K as the validation set, and the rest as the test set.</p><p>Image representation. We represent each image with a Fisher vector (FV) <ref type="bibr" target="#b15">[16]</ref> computed over densely extracted 128D SIFT descriptors <ref type="bibr" target="#b41">[42]</ref> and 96D local color features <ref type="bibr" target="#b42">[43]</ref>, both projected with PCA to 64 dimensions. FVs are extracted and normalized separately for both channels and then combined by concatenating the two feature vectors. We do not make use of spatial pyramids. In our experiments, we use FVs extracted using a vocabulary of either 16 or 256 Gaussians. For 16 Gaussians, this leads to a 4K-dimensional feature vector, which requires about 20 GB for the 1.2M training set (using 4-byte floating-point arithmetic). This fits into the RAM of our 32 GB servers.</p><p>For 256 Gaussians, the FVs are 16 times larger, i.e., 64Kdimensional, which would require 320 GB of memory. To fit the data in memory, we compress the feature vectors using product quantization <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. In a nutshell, it consists of splitting the high-dimensional vector into small subvectors, and vector quantizing each subvector independently. We compress the dataset to approximately 10 GB using 8D subvectors and 256 centroids per subquantizer, which allows storing each subquantizer index in a single byte, combined with a sparse encoding of the zero subvectors; see <ref type="bibr" target="#b0">[1]</ref>. In each iteration of SGD learning, we decompress the features of a limited number of images, and use these (lossy) reconstructions to compute the gradient.</p><p>Evaluation measures. We report the average top-1 and top-5 flat error used in the ILSVRC '10 challenge. The flat error is one if the ground-truth label does not correspond to the top-1 label with highest score (or any of the top-5 labels) and zero otherwise. The motivation for the top-5 error is to allow an algorithm to identify multiple objects in an image and not be penalized if one of the objects identified was in fact present but not included in the ground truth of the image which contains only a single object category per image.</p><p>Baseline approach. For our baseline, we follow the state-ofthe-art approach of <ref type="bibr" target="#b43">[44]</ref> and learn weighed one-versus-rest SVMs with SGD, where the number of negative images in each iteration is sampled proportional to the number of positive images for that class. The proportion parameter is cross validated on the validation set. The results of the baseline can be found in Tables <ref type="table" target="#tab_2">4</ref> and<ref type="table" target="#tab_5">7</ref>. We observe that the performance when using the 64K-dimensional features (28.0) is significantly better than the 4K ones (38.2), despite the lossy PQ compression.</p><p>In Table <ref type="table" target="#tab_2">4</ref>, the performance using the 64K features is slightly better than the ILSVRC '10 challenge winners <ref type="bibr" target="#b1">[2]</ref> (28.0 versus 28.2 flat top-5 error), and close to the results of <ref type="bibr" target="#b0">[1]</ref> (25.7 flat top-5 error), wherein an image representation of more than 1M dimensions was used. In Table <ref type="table" target="#tab_5">7</ref>, our baseline shows state-of-the-art performance on ImageNet-10K when using the 64K features, obtaining 78.1 versus 81.9 flat top-1 error <ref type="bibr" target="#b43">[44]</ref>. We believe that this is due to the use of the color features in addition to the SIFT features used in <ref type="bibr" target="#b43">[44]</ref>.</p><p>SGD training and early stopping. To learn the projection matrix W , we use SGD training and sample at each iteration a fixed number of m training images to estimate the gradient. Following <ref type="bibr" target="#b23">[24]</ref>, we use a fixed learning rate and do not include an explicit regularization term, but rather use the projection dimension d, as well as the number of iterations as an implicit form of regularization. For all experiments, we proceed as follows:</p><p>1. run for a large number of iterations (%750K-2M), 2. validate every 50K (k-NN) or 10K (NCM) iterations, and 3. select metric with lowest top-5 error.</p><p>In case of a tie, the metric with the lowest top-1 error is chosen. Similarly, all hyperparameters, like the value of k for k-NN, are validated in this way. Unless stated otherwise, training is performed using the ILSVRC '10 training set, and validation on the provided 50K images of the validation set.</p><p>Training and testing complexity. In Table <ref type="table" target="#tab_1">2</ref>, we give an overview of the training times and number of images seen during training for the different algorithms. While the training times are difficult to compare due to the use of different implementations (Matlab and C/C++) and different machines, it is interesting to see that the number of training images used to convergence is roughly of the same order for the different algorithms. We compare the methods in terms of 1) the number of models that is learned: SVM learns C ¼ 1;000 different classifiers, while the NCM/k-NN methods both learn a single projection matrix (C ¼ 1); 2) the number of images I per iteration: For SVM we use 64 negative images per positive image (I ¼ 65), for NCM we use I ¼ 1;000, and for k-NN we use I ¼ 300; and 3) the number of iterations T .</p><p>While it is straightforward to parallelize the learning of the SVMs (e.g., each machine learns a single classifier), it is more complex for the proposed methods where a shared projection matrix is learned for all classes. Nevertheless, the core components of these methods can be written as matrix products (e.g., projections of the means or images, the gradients of the objectives, etc.), for which we benefit from optimized multithreaded implementations.</p><p>At test time, evaluation of the classifiers is expensive for the k-NN classifiers, but cheap for the NCM and SVM classifiers. For the SVMs, the cost is OðMCDÞ, where C is the number of classes, D the dimensionality of the feature vector, and M the number of images in the test set. The NCM classifier can be evaluated at the same cost by precomputing the double projection of the means, similarly to the approach discussed in Section 3.4. If the dimensionality of the projection matrix d is smaller than C, then it may be more efficient to project the test images in OðMDdÞ, and to compute the distances in the projected space in OðMCdÞ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">k-NN Metric Learning Results</head><p>We start with an assessment of k-NN classifiers to select a baseline for comparison with the NCM classifier. Given the cost of k-NN classifiers, we focus our experiments on the 4K-dimensional features, and consider the impact of the different choices for the set of target images P q (see Section 4), and the projection dimensionality.</p><p>We initialize W as a PCA projection, and determine the number of nearest neighbors to be used for classification on the validation set; typically 100 to 250 neighbors are optimal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Target Selection for k-NN Metric Learning</head><p>In the first experiment, we compare the three different options of Section 4 to define the set of target images P q while learning projections to 128 dimensions. For LMNN and dynamic targets, we experimented with various numbers of targets on the validation set and found that using 10 to 20 targets yields the best results.</p><p>The results in Table <ref type="table">3</ref> show that all methods lead to metrics that are better than the ' 2 metric in the original space or after a PCA projection to 128 dimensions. Furthermore, we can improve over LMNN by using all within-class images as targets, or even further by using dynamic targets. The success of the dynamic target selection can be explained by the fact that among the three alternatives, the learning objective is the most closely related to the k-NN classification rule. The best performance on the flat top-5 error of 39.7 using 10 dynamic targets is, however, slightly worse than the 38.2 error rate of the SVM baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Impact of Projection Dimension on k-NN Classification</head><p>Next, we evaluate the influence of the projection dimensionality d on the performance, by varying d between 32 and 1,024. We only show results using 10 dynamic targets since this performed best among the evaluated k-NN methods.</p><p>From the results in Table <ref type="table" target="#tab_2">4</ref>, we see that a projection to 256 dimensions yields the lowest error of 39.0, which still remains somewhat inferior to the SVM baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Nearest Class Mean Classifier Results</head><p>We now consider the performance of NCM classifiers and the related methods described in Section 3. For all experiments, we use the NCM with euclidean distance according to <ref type="bibr" target="#b4">(5)</ref>. In Table <ref type="table" target="#tab_2">4</ref>, we show the results. We first consider the results for the 4K-dimensional features. As observed for the k-NN classifier, also for NCM using a learned metric outperforms using the ' 2 distance (68.0), which is worse than using ' 2 distances for the k-NN classifier (55.7, see Table <ref type="table">3</ref>). However, unexpectedly, with metric learning we observe that our NCM classifier (37.0) outperforms the more flexible k-NN classifier (39.0), as well as the SVM baseline (38.2) when For all methods, except those indicated by "Full," the data is projected to a 128D space.</p><p>projecting to 256 dimensions or more. Our implementation of WSABIE <ref type="bibr" target="#b2">[3]</ref> scores slightly worse <ref type="bibr">(38.5)</ref> than the baseline and our NCM classifier and does not generalize to new classes without retraining. We also compare our NCM classifier to several other algorithms which do allow generalization to new classes. First, we consider two other supervised metric learning approaches, NCM with FDA (which leads to 50.5) and ridge-regression (which leads to 54.6). We observe that NCMML outperforms both methods significantly. Second, we consider two unsupervised variants of the NCM classifier where we use PCA to reduce the dimensionality. In one case, we use the ' 2 metric after PCA. In the other, inspired by ridge-regression, we use NCM with the metric W generated by the inverse of the regularized covariance matrix such that W &gt; W ¼ ðAE þ IÞ À1 , see Section 3.2. We tuned the regularization parameter on the validation set, as was also done for ridge-regression. From these results we can conclude that, just like for k-NN, the ' 2 metric with or without PCA leads to poor results (68.0) as compared to a learned metric. Also, the feature whitening implemented by the inverse covariance metric leads to results <ref type="bibr">(43.8)</ref> that are better than using the ' 2 metric and are also substantially better than ridge-regression (54.6). The results are, however, significantly worse than using our learned metric, in particular when using low-rank metrics.</p><p>When we use the 64K dimensional features, the results of the NCM classifier <ref type="bibr">(30.8)</ref> are somewhat worse than the SVM baseline (28.0); again the learned metric is significantly better than using the ' 2 distance (63.2). WSABIE obtains an error of 29.2, in between the SVM and NCM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Illustration of Metric Learned by NCMML</head><p>In Fig. <ref type="figure" target="#fig_1">3</ref>, we illustrate the difference between the ' 2 and the Mahalanobis metric induced by a learned projection from 64K to 512 dimensions. For two reference classes, we show the five nearest classes, based on the distance between class means. We also show the posterior probabilities on the reference class and its five neighbor classes according to <ref type="bibr" target="#b4">(5)</ref>. The feature vector x x x x is set as the mean of the reference class, i.e., a simulated perfectly typical image of this class. For the ' 2 metric, we used our metric learning algorithm to learn a scaling of the ' 2 metric to minimize (6). This does not change the ordering of classes, but ensures that we can compare probabilities computed using both metrics. We find that, as expected, the learned metric has more visually and semantically related classes. Moreover, we see that, using the learned metric, most of the probability mass is assigned to the reference class, whereas the ' 2 metric leads to rather uncertain classifications. This suggests that, using the ' 2 metric, many classes are placed at comparable distances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Nonlinear Classification Using Multiple Class Centroids</head><p>In these experiments, we use the nonlinear NCMC classifier, introduced in Section 3.3, where each class is represented by  a set of k centroids. We obtain the k centroids per class by using the k-means algorithm in the ' 2 space. Since the cost of training these classifiers is much higher, we run two sets of experiments. In Fig. <ref type="figure" target="#fig_2">4</ref>, we show the performance of the NCMC-test classifier, where only at test time k ¼ ½2; . . . ; 30 is used, while using a metric obtained by the NCM objective (k ¼ 1). In Table <ref type="table" target="#tab_3">5</ref>, we show the performance of the NCMC classifier, trained with the NCMC objective, using the 4K features, compared to the NCM method and the best NCMC-test method.</p><p>From the results we observe that a significant performance improvement can be made by using the Nonlinear NCMC classifier, especially when using a low number of projection dimensions. When learning using the NCMC classifier we can further improve the performance of the nonlinear classification, albeit for a higher training cost. When using as little as 512 projection dimensions and k ¼ 10 centroids, we obtain a performance of 34.6 on the top-5 error. This is an improvement of about 2.4 absolute points over the NCM classifier (37.0) and 3.6 absolute points over SVM classification <ref type="bibr">(38.</ref>2), c.f., Table <ref type="table" target="#tab_2">4</ref>.</p><p>For the 64K features, the NCMC (with k ¼ 10 and d ¼ 512) yields a top-5 error 29.4, which is about 1.3 points improvement over the NCM classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Generalizing to New Classes with Few Samples</head><p>Here, we explore the ability of the distance-based classifiers to generalize to novel classes. For the NCM, we also consider its performance as a function of the number of training images available to estimate the mean of novel classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">Generalization to Classes Not Seen during Training</head><p>In this experiment, we split the ILSVRC '10 dataset into a training set consisting of approximately 1M images from 800 classes, and an evaluation set of the 200 held-out classes. The error is evaluated in a 1,000-way classification task, and computed over the 30K images in the test set of the held-out classes. Performance on the test images of the 800 train classes changes only marginally and including them would obscure the changes among the test images of the 200 held-out classes. The early stopping strategy uses the validation set of the 800 training classes.</p><p>In Table <ref type="table" target="#tab_4">6</ref>, we show the performance of NCM and k-NN classifiers, and compare it to the control setting where the metric is trained on all 1,000 classes. The results show that both classifiers generalize remarkably well to new classes. For comparison, we also include the results of the SVM baseline and the k-NN and NCM classifiers using the ' 2 distance, evaluated over the 200 held-out classes. In particular for 1,024D projections of the 4K features, the NCM classifier achieves an error of 40.0 over classes not seen during training, as compared to 36.5 when using all classes for training. For the 64K-dimensional features the drop in performance is larger, but still surprisingly good considering that training for the novel classes consists only of computing their means.</p><p>Generalization to the ImageNet-10K dataset. In this experiment, we demonstrate the generalization ability of the NCM classifier on the ImageNet-10K dataset. We use projections learned and validated on the ILSVRC '10 dataset, and only compute the means of the 10K classes. The results in Table 7   <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b43">[44]</ref> and the Deep Learning Results of <ref type="bibr" target="#b44">[45]</ref> show that, even in this extremely challenging setting, the NCM classifier performs remarkably well compared to methods which require training of 10K classifiers. We note that, to the best of our knowledge, our baseline results (78.1 top-1 error) exceed the previously known state-of-the-art (81.9 and 80.8) <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>.</p><p>Training our SVM baseline system took 9 and 280 CPU days, respectively, for the 4K and 64K features, while the computation of the means for the NCM classifier took approximately 3 and 48 CPU minutes, respectively. This represents roughly a 8,500-fold speed-up as compared to the SVMs, given a learned projection matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">Accuracy as a Function of Sample Size of Novel Classes</head><p>In this experiment, we consider the error as a function of the number of images that are used to compute the means of novel classes. Inspired by Rohrbach et al. <ref type="bibr" target="#b37">[38]</ref>, we also include a zero-shot learning experiment where we use the ImageNet hierarchy to estimate the mean of novel classes from related classes. We estimate the mean of a novel class z using the means of its ancestor nodes in the ILSVRC '10 class hierarchy:</p><formula xml:id="formula_35">z ¼ 1 jA z j X a2Az a ;<label>ð29Þ</label></formula><p>where A z denotes the set of ancestors of node z and a is the mean of ancestor a. The mean of an internal node, a , is computed as the average of the means of all its descendant training classes.</p><p>If we view the estimation of each class mean as the estimation of the mean of a Gaussian distribution, then the mean of a sample of images s corresponds to the maximum likelihood (ML) estimate, while the zero-shot estimate z can be thought of as a prior. To obtain a maximum a-posteriori (MAP) estimate p , we combine the prior and the ML estimate as follows:</p><formula xml:id="formula_36">p ¼ n s þ m z n þ m ;<label>ð30Þ</label></formula><p>where the ML estimate is weighed by n, the number of images that were used to compute it, and the prior mean obtains a weight m determined on the validation set <ref type="bibr" target="#b45">[46]</ref>.</p><p>In Fig. <ref type="figure" target="#fig_3">5</ref>, we analyze the performance of the NCM classifier trained on the images of the same 800 classes used above, with a learned projection from 4K and 64K to 512 dimensions. The metric and the parameter m are validated using the images of the 200 held-out classes of the validation set. We again report the error on the test images of the held-out classes in a 1,000-way classification as above. We repeat the experiment 10 times, and show error-bars at three times standard deviation. For the error to stabilize, we only need approximately 100 images to estimate the class means. The results show that the zero-shot prior can be effectively combined with the empirical mean to provide a smooth transition from the zero-shot setting to a setting with many training examples. Inclusion of the zero-shot prior leads to a significant error reduction in the regime where 10 images or less are available. Also, the results show that the validation on the 200 hold-out classes or on the 800 training classes yields comparable error rates (40.1 versus 39.9, using 4K and 512d, c.f. Table <ref type="table" target="#tab_4">6</ref> and Fig. <ref type="figure" target="#fig_3">5</ref>).</p><p>In <ref type="bibr" target="#b37">[38]</ref>, a zero-shot error rate of 65.2 was reported in a 200-way classification task. Using the NCM with our prior mean estimates leads to comparable error rates of 66.5 (4K) and 64.0 (64K). Note that a different set of 200 hold-out classes was used, as well as different features. However, their baseline performance of 37.6 top-5 error is comparable to our 4K features (38.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.3">Instance Level Image Retrieval</head><p>Query-by-example image retrieval can be seen as an image classification problem where only a single positive sample (the query) is given and negative examples are not explicitly provided. Recently, using classifiers to learn a metric for image retrieval was considered in <ref type="bibr" target="#b46">[47]</ref>. They found the joint subspace and classifier learning (JSCL) method to be the most effective. It consists of jointly learning a set of classifiers and a projection matrix W using WSABIE, <ref type="bibr" target="#b11">(12)</ref> on an auxiliary supervised dataset. After training, the learned projection matrix W is used to compute distances between queries and database images.</p><p>Similarly, we propose to learn a metric using our NCM classifier on the auxiliary supervised dataset and to use the learned metric to retrieve the most similar images for a given query.</p><p>For this experiment, we use the same public benchmarks as in <ref type="bibr" target="#b46">[47]</ref>. First is the INRIA Holidays dataset <ref type="bibr" target="#b47">[48]</ref>, which consists of 1,491 images of 500 scenes and objects. For evaluation, one image per scene/object is used as query to search within the remaining images, and accuracy is measured as the mean average precision (mAP) over the 500 queries. Second is the University of Kentucky Benchmark dataset (UKB) <ref type="bibr" target="#b48">[49]</ref>, which contains four images of 2,550 objects <ref type="bibr">(10,200 images)</ref>. For evaluation, each image is used as query and the performance is measured by 4 Â recall@4 averaged over all queries; hence the maximal score is 4. For both datasets, we extract the 4K image features used in our earlier experiments; these are also used in <ref type="bibr" target="#b46">[47]</ref>. To compute the distance between two images, we use the cosine-distance, i.e., the dot-product on ' 2normalized vectors.</p><p>We use NCMML to train a metric on the ILSVRC '10 dataset while using early stopping based on retrieval performance, similarly as in <ref type="bibr" target="#b46">[47]</ref>. To avoid tuning on the test data, the validation is performed on the other dataset, i.e., when testing on UKB we regularize on Holidays and vice versa. In Table <ref type="table" target="#tab_6">8</ref>, we compare the performance of the NCM-based metric with that of JSCL, with a baseline PCA method, and with the performance using the original highdimensional descriptors. Finally, for the Holidays dataset, we included the NCM metric optimized for classification performance on the ILSVRC '10 validation dataset (NCM*).</p><p>From these results, we observe that the NCM metric yields similar performance as the JSCL method on both datasets. A projection to only 128 dimensions or more yields an equal or better retrieval performance as using the original features or the PCA baseline. On the Holidays dataset the NCM metric outperforms the JSCL metric, while on the UKB dataset JSCL slightly outperforms NCM. Both the NCM and JSCL methods are effective to learn a projection metric for instance level retrieval, while employing class level labels.</p><p>Note that it is crucial to use retrieval performance for early stopping; the results of NCM* are in fact worse than using the original descriptors. Thus, the classification objective determines a good "path" through the space of projection matrices, yet to obtain good retrieval performance the number of iterations is typically an order of magnitude smaller than for classification. We explain this discrepancy by the fact that instance level retrieval does not require the suppression of the within-class variations. This suggests also that even better metrics may be learned by training NCM on a large set of queries with corresponding matches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS</head><p>In this paper, we have considered large-scale distancebased image classification, which allows integration of new data (possibly of new classes) at a negligible cost. This is not possible with the popular one-versus-rest SVM approach, but is essential when dealing with real-life open-ended datasets.</p><p>We have introduced NCMML, a metric learning method for NCM classification which maximizes the loglikelihood of correct class prediction with class probabilities using the soft-min over distances between a sample and the class means. The extended nonlinear NCMC classifier offers a tradeoff in the complexity, from the linear NCM to the nonparametric k-NN, by the number of used class-centroids.</p><p>We have experimentally validated our models and compared to a state-of-the-art baseline of one-versus-rest SVMs using Fisher vector image representations. Surprisingly, we found that the NCM outperforms the more flexible k-NN and that its performance is comparable to an SVM baseline, while projecting the data to as few as 256 dimensions.</p><p>Our experiments on the ImageNet-10K dataset show that the learned metrics generalize well to unseen classes at a negligible cost. While only computing class means, as opposed to training 10,000 SVM classifiers, we obtain competitive performance at roughly a 8,500-fold speedup.</p><p>Finally, we have also considered a zero-shot learning setting and have shown that NCM provides a unified way to treat classification and retrieval problems. NCMML is compared to a PCA baseline and the JSCL method of <ref type="bibr" target="#b46">[47]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The learned class-specific biases s c and the norm of the projected means b c are strongly correlated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The nearest classes for two reference classes using the ' 2 distance and metric learned by NCMML. Class probabilities are given for a simulated image that equals the mean of the reference class; see text for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Results of the NCMC-test classifier, which uses k ¼ 1 at train time and k &gt; 1 at test time, for the 4K (left) and 64K (right) features, for several values of k during evaluation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Results of NCM as a function of the number of images used to compute the means for test classes. Comparison of the ML (blue) and MAP (red) mean estimates, for the 4K (left) and 64K (right) features, in a 1,000-way classification task, including baseline (black) when trained on all classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1</head><label>1</label><figDesc>Comparison of Complexity of the Considered Alternatives to Compute the Class Probabilities pðc j x x x xÞ</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 2</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell>TABLE 3</cell></row><row><cell>Complexity Comparison of Classifier Training</cell><cell>Comparison of Results for Different k-NN Classification</cell></row><row><cell></cell><cell>Methods Using the 4K-Dimensional Features</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 4</head><label>4</label><figDesc>Comparison on ILSVRC '10 of the k-NN and NCM Classifiers with Related Methods, Using the 4K-and 64K-Dimensional Features and for Various Projection Dimensions</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 5</head><label>5</label><figDesc>Results of the NCMC Classifier Using the 4K Features, Compared to the NCM Classifier and the Best NCMC-Test Classifier (with k in Brackets)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 6</head><label>6</label><figDesc>Results for 1,000-Way Classification among Test Images of 200 Classes Not Used for Metric Learning, and Control Setting When Learned on All Classes</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 7</head><label>7</label><figDesc>Comparison of the Results on the ImageNet-10K DatasSet: The NCM Classifier with Metrics Learned on the ILSVRC '10 Dataset, the NCM Using ' 2 Distance, the Baseline SVM, and Previously Reported SVM Results</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 8</head><label>8</label><figDesc>Results of Instance Level Image Retrieval on the Holidays and UKB Dataset, Using 4K Features</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>See http://www.image-net.org/challenges/LSVRC/2010/index.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>. For more information on this or any other computing topic, please visit our Digital Library at www.computer.org/publications/dlib.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">High-Dimensional Signature Compression for Large-Scale Image Classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Large-Scale Image Classification: Fast Feature Extraction and SVM Training</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">WSABIE: Scaling Up to Large Vocabulary Image Annotation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 22nd Int&apos;l Joint Conf</title>
		<meeting>22nd Int&apos;l Joint Conf</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Label Embedding Trees for Large Multi-Class Tasks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Neural Information Processing Systems</title>
		<meeting>Conf. Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Discriminative Learning of Relaxed Hierarchy for Large-Scale Visual Recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;l Conf. Computer Vision</title>
		<meeting>IEEE Int&apos;l Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">What Does Classifying More than 10,000 Image Categories Tell Us?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 11th European Conf. Computer Vision</title>
		<meeting>11th European Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Tagprop: Discriminative Metric Learning in Nearest Neighbor Models for Image Auto-Annotation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE Int&apos;l Conf. Computer Vision</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">In Defense of Nearest-Neighbor Based Image Classification</title>
		<author>
			<persName><forename type="first">O</forename><surname>Boiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Webb</surname></persName>
		</author>
		<title level="m">Statistical Pattern Recognition</title>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Distance Metric Learning for Large Margin Nearest Neighbor Classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="207" to="244" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Large Scale Online Learning of Image Similarity through Ranking</title>
		<author>
			<persName><forename type="first">G</forename><surname>Checkik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Shalit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1109" to="1135" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Large-Scale Machine Learning with Stochastic Gradient Descent</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 19th Int&apos;l Conf. Computational Statistics</title>
		<meeting>19th Int&apos;l Conf. Computational Statistics</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Quantization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Neuhoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Information Theory</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2325" to="2383" />
			<date type="published" when="1998-10">Oct. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Product Quantization for Nearest Neighbor Search</title>
		<author>
			<persName><forename type="first">H</forename><surname>Je ´gou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="117" to="128" />
			<date type="published" when="2011-01">Jan. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improving the Fisher Kernel for Large-Scale Image Classification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 11th European Conf. Computer Vision</title>
		<meeting>11th European Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Metric Learning for Large Scale Image Classification: Generalizing to New Classes at Near-Zero Cost</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 12th European Conf. Computer Vision</title>
		<meeting>12th European Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Visual Categorization with Bags of Keypoints</title>
		<author>
			<persName><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dance</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Willamowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Workshop Statistical Learning in Computer Vision</title>
		<meeting>Int&apos;l Workshop Statistical Learning in Computer Vision</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Local Features and Kernels for Classification of Texture and Object Categories: A Comprehensive Study</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marszałek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int&apos;l J. Computer Vision</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="213" to="238" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Aggregating Local Image Descriptors into Compact Codes</title>
		<author>
			<persName><forename type="first">H</forename><surname>Je ´gou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sa ´nchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1704" to="1716" />
			<date type="published" when="2012-09">Sept. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning Visual Similarity Measures for Comparing Never Seen Objects</title>
		<author>
			<persName><forename type="first">E</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Is That You? Metric Learning Approaches for Face Identification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE Int&apos;l Conf. Computer Vision</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Large Scale Metric Learning from Equivalence Constraints</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ko ¨stinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning to Rank with (a Lot of) Word Features</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sadamasa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="291" to="314" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Large Margin Nearest Local Mean Classifier</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chenb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Baoa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="236" to="248" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Metric Learning by Collapsing Classes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Globerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roweis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Neural Information Processing Systems</title>
		<meeting>Conf. Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Neighbourhood Component Analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Neural Information Processing Systems</title>
		<meeting>Conf. Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">LESS: A Model-Based Classifier for Sparse Subspaces</title>
		<author>
			<persName><forename type="first">C</forename><surname>Veenman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tax</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1496" to="1500" />
			<date type="published" when="2005-09">Sept. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Large Margin Taxonomy Embedding for Document Categorization</title>
		<author>
			<persName><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Neural Information Processing Systems</title>
		<meeting>Conf. Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sift-Bag Kernel for Video Event Analysis</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. 16th ACM Int&apos;l Conf. Multimedia</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Image-to-Class Distance Metric Learning for Image Classification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-T</forename><surname>Chia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 11th European Conf. Computer Vision</title>
		<meeting>11th European Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">One-Shot Learning of Object Categories</title>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="594" to="611" />
			<date type="published" when="2006-04">Apr. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning to Detect Unseen Object Classes by Between-Class Attribute Transfer</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The More You Know, the Less You Learn: From Knowledge Transfer to One-Shot Learning of Object Categories</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tommasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. British Machine Vision Conf</title>
		<meeting>British Machine Vision Conf</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Zero-Data Learning of New Tasks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf. Artificial Intelligence</title>
		<meeting>AAAI Conf. Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Adapting Visual Category Models to New Domains</title>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 11th European Conf. Computer Vision</title>
		<meeting>11th European Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Large Margin Multi-Task Metric Learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Parameswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Neural Information Processing Systems</title>
		<meeting>Conf. Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Evaluating Knowledge Transfer and Zero-Shot Learning in a Large-Scale Setting</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Joint Image and Word Sense Discrimination for Image Retrieval</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 12th European Conf. Computer Vision</title>
		<meeting>12th European Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vandenberghe</surname></persName>
		</author>
		<title level="m">Convex Optimization</title>
		<imprint>
			<publisher>Cambridge Univ. Press</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Large Scale Metric Learning for Distance-Based Image Classification</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<idno>RR-8077</idno>
		<ptr target="http://hal.inria.fr/hal-00735908.2012" />
	</analytic>
	<monogr>
		<title level="j">INRIA</title>
		<imprint/>
	</monogr>
	<note type="report_type">Research Report</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Distinctive Image Features from Scale-Invariant Keypoints</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int&apos;l J. Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">XRCE&apos;s Participation to ImagEval</title>
		<author>
			<persName><forename type="first">S</forename><surname>Clinchant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Renders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ImageEval Workshop at CVIR</title>
		<meeting>ImageEval Workshop at CVIR</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Towards Good Practice in Large-Scale Learning for Image Classification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Building High-Level Features Using Large Scale Unsupervised Learning</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conf. Machine Learning</title>
		<meeting>Int&apos;l Conf. Machine Learning</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Maximum A Posteriori Estimation for Multivariate Gaussian Mixture Observations of Markov Chains</title>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Gauvain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Speech and Audio Processing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="291" to="298" />
			<date type="published" when="1994-04">Apr. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Leveraging Category-Level Labels for Instance-Level Image Retrieval</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rodrı ´guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Valveny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Hamming Embedding and Weak Geometric Consistency for Large Scale Image Search</title>
		<author>
			<persName><forename type="first">H</forename><surname>Je ´gou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 10th European Conf. Computer Vision</title>
		<meeting>10th European Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Scalable Recognition with a Vocabulary Tree</title>
		<author>
			<persName><forename type="first">D</forename><surname>Niste ´r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Stewe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
