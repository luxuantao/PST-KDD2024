<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MIXOUT: EFFECTIVE REGULARIZATION TO FINETUNE LARGE-SCALE PRETRAINED LANGUAGE MODELS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Cheolhyoung</forename><surname>Lee</surname></persName>
							<email>cheolhyoung.lee@kaist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Mathematical Sciences</orgName>
								<orgName type="institution">KAIST</orgName>
								<address>
									<postCode>34141</postCode>
									<settlement>Daejeon</settlement>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
							<email>kyunghyun.cho@nyu.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">New York University ‡ Facebook AI Research § CIFAR Azrieli Global Scholar</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wanmo</forename><surname>Kang</surname></persName>
							<email>wanmo.kang@kaist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Mathematical Sciences</orgName>
								<orgName type="institution">KAIST</orgName>
								<address>
									<postCode>34141</postCode>
									<settlement>Daejeon</settlement>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MIXOUT: EFFECTIVE REGULARIZATION TO FINETUNE LARGE-SCALE PRETRAINED LANGUAGE MODELS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In natural language processing, it has been observed recently that generalization could be greatly improved by finetuning a large-scale language model pretrained on a large unlabeled corpus. Despite its recent success and wide adoption, finetuning a large pretrained language model on a downstream task is prone to degenerate performance when there are only a small number of training instances available. In this paper, we introduce a new regularization technique, to which we refer as "mixout", motivated by dropout. Mixout stochastically mixes the parameters of two models. We show that our mixout technique regularizes learning to minimize the deviation from one of the two models and that the strength of regularization adapts along the optimization trajectory. We empirically evaluate the proposed mixout and its variants on finetuning a pretrained language model on downstream tasks. More specifically, we demonstrate that the stability of finetuning and the average accuracy greatly increase when we use the proposed approach to regularize finetuning of BERT on downstream tasks in GLUE.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Transfer learning has been widely used for the tasks in natural language processing (NLP) <ref type="bibr" target="#b4">(Collobert et al., 2011;</ref><ref type="bibr" target="#b6">Devlin et al., 2018;</ref><ref type="bibr" target="#b27">Yang et al., 2019;</ref><ref type="bibr" target="#b13">Liu et al., 2019;</ref><ref type="bibr" target="#b16">Phang et al., 2018)</ref>. In particular, <ref type="bibr" target="#b6">Devlin et al. (2018)</ref> recently demonstrated the effectiveness of finetuning a large-scale language model pretrained on a large, unannotated corpus on a wide range of NLP tasks including question answering and language inference. They have designed two variants of models, BERT LARGE (340M parameters) and BERT BASE (110M parameters). Although BERT LARGE outperforms BERT BASE generally, it was observed that finetuning sometimes fails when a target dataset has fewer than 10,000 training instances <ref type="bibr" target="#b6">(Devlin et al., 2018;</ref><ref type="bibr" target="#b16">Phang et al., 2018)</ref>.</p><p>When finetuning a big, pretrained language model, dropout <ref type="bibr" target="#b20">(Srivastava et al., 2014)</ref> has been used as a regularization technique to prevent co-adaptation of neurons <ref type="bibr" target="#b21">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b6">Devlin et al., 2018;</ref><ref type="bibr" target="#b27">Yang et al., 2019)</ref>. We provide a theoretical understanding of dropout and its variants, such as Gaussian dropout <ref type="bibr" target="#b24">(Wang &amp; Manning, 2013</ref><ref type="bibr">), variational dropout (Kingma et al., 2015)</ref>, and dropconnect <ref type="bibr" target="#b22">(Wan et al., 2013)</ref>, as an adaptive L 2 -penalty toward the origin (all zero parameters 0) and generalize dropout by considering a target model parameter u (instead of the origin), to which we refer as mixout(u). We illustrate mixout(u) in Figure <ref type="figure" target="#fig_0">1</ref>. To be specific, mixout(u) replaces all outgoing parameters from a randomly selected neuron to the corresponding parameters of u. mixout(u) avoids optimization from diverging away from u through an adaptive L 2 -penalty toward u. Unlike mixout(u), dropout encourages a move toward the origin which deviates away from u since dropout is equivalent to mixout(0).</p><p>We conduct experiments empirically validating the effectiveness of the proposed mixout(w pre ) where w pre denotes a pretrained model parameter. To validate our theoretical findings, we train a fully connected network on EMNIST Digits <ref type="bibr" target="#b3">(Cohen et al., 2017)</ref> and finetune it on MNIST. We observe that a finetuning solution of mixout(w pre ) deviates less from w pre in the L 2 -sense than In the dropout network, we randomly choose an input neuron to be dropped (a dotted neuron) with a probability of p. That is, all outgoing parameters from the dropped neuron are eliminated (dotted connections). (c): In the mixout(u) network, the eliminated parameters in (b) are replaced by the corresponding parameters in (a). In other words, the mixout(u) network at w is the mixture of the vanilla network at u and the dropout network at w with a probability of p.</p><p>that of dropout. In the main experiment, we finetune BERT LARGE with mixout(w pre ) on small training sets of GLUE <ref type="bibr" target="#b23">(Wang et al., 2018)</ref>. We observe that mixout(w pre ) reduces the number of unusable models that fail with the chance-level accuracy and increases the average development (dev) scores for all tasks. In the ablation studies, we perform the following three experiments for finetuning BERT LARGE with mixout(w pre ): (i) the effect of mixout(w pre ) on a sufficient number of training examples, (ii) the effect of a regularization technique for an additional output layer which is not pretrained, and (iii) the effect of probability of mixout(w pre ) compared to dropout. From these ablation studies, we observe that three characteristics of mixout(w pre ): (i) finetuning with mixout(w pre ) does not harm model performance even with a sufficient number of training examples; (ii) It is beneficial to use a variant of mixout as a regularization technique for the additional output layer; (iii) The proposed mixout(w pre ) is helpful to the average dev score and to the finetuning stability in a wider range of its hyperparameter p than dropout.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">RELATED WORK</head><p>For large-scale pretrained language models <ref type="bibr" target="#b21">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b6">Devlin et al., 2018;</ref><ref type="bibr" target="#b27">Yang et al., 2019)</ref>, dropout has been used as one of several regularization techniques. The theoretical analysis for dropout as an L 2 -regularizer toward 0 was explored by <ref type="bibr" target="#b22">Wan et al. (2013)</ref> where 0 is the origin. They provided a sharp characterization of dropout for a simplified setting (generalized linear model). <ref type="bibr" target="#b14">Mianjy &amp; Arora (2019)</ref> gave a formal and complete characterization of dropout in deep linear networks with squared loss as a nuclear norm regularization toward 0. However, neither <ref type="bibr" target="#b22">Wan et al. (2013)</ref> nor <ref type="bibr" target="#b14">Mianjy &amp; Arora (2019)</ref> gives theoretical analysis for the extension of dropout which uses a point other than 0. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES AND NOTATIONS</head><p>Norms and Loss Functions Unless explicitly stated, a norm • refers to L 2 -norm. A loss function of a neural network is written as</p><formula xml:id="formula_0">L(w) = 1 n n i=1 L i (w)</formula><p>, where w is a trainable model parameter. L i is "a per-example loss function" computed on the i-th data point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Strong Convexity</head><formula xml:id="formula_1">A differentiable function f is strongly convex if there exists m &gt; 0 such that f (y) ≥ f (x) + ∇f (x) (y − x) + m 2 y − x 2 ,<label>(1)</label></formula><p>for all x and y.</p><p>Weight Decay We refer as "wdecay(u, λ)" to minimizing</p><formula xml:id="formula_2">L(w) + λ 2 w − u 2 ,</formula><p>instead of the original loss function L(w) where λ is a regularization coefficient. Usual weight decay of λ is equivalent to wdecay(0, λ).</p><p>Probability for Dropout and Dropconnect Dropout <ref type="bibr" target="#b20">(Srivastava et al., 2014</ref>) is a regularization technique selecting a neuron to drop with a probability of p. Dropconnect <ref type="bibr" target="#b22">(Wan et al., 2013)</ref> chooses a parameter to drop with a probability of p. To emphasize their hyperparameter p, we write dropout and dropconnect with a drop probability of p as "dropout(p)" and "dropconnect(p)", respectively. dropout(p) is a special case of dropconnect(p) if we simultaneously drop the parameters outgoing from each dropped neuron.</p><p>Inverted Dropout and Dropconnect In the case of dropout(p), a neuron is retained with a probability of 1−p during training. If we denote the weight parameter of that neuron as w during training, then we use (1 − p)w for that weight parameter at test time <ref type="bibr" target="#b20">(Srivastava et al., 2014)</ref>. This ensures that the expected output of a neuron is the same as the actual output at test time. In this paper, dropout(p) refers to inverted dropout(p) which uses w/(1 − p) instead of w during training. By doing so, we do not need to compute the output separately at test time. Similarly, dropconnect(p) refers to inverted dropconnect(p).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ANALYSIS OF DROPOUT AND ITS GENERALIZATION</head><p>We start our theoretical analysis by investigating dropconnect which is a general form of dropout and then apply the result derived from dropconnect to dropout. The iterative SGD equation for dropconnect(p) with a learning rate of η is</p><formula xml:id="formula_3">w (t+1) = w (t) − ηB (t) ∇L EB (t) 1 −1 B (t) w (t) , t = 0, 1, 2, • • • ,<label>(2)</label></formula><p>where</p><formula xml:id="formula_4">B (t) = diag(B (t) 1 , B (t) 2 , • • • , B (t) d ) and B (t)</formula><p>i 's are mutually independent Bernoulli(1 − p) random variables with a drop probability of p for all i and t. We regard equation 2 as finding a solution to the minimization problem below:</p><formula xml:id="formula_5">min w EL (EB 1 ) −1 Bw ,<label>(3)</label></formula><p>where</p><formula xml:id="formula_6">B = diag(B 1 , B 2 , • • • , B d</formula><p>) and B i 's are mutually independent Bernoulli(1 − p) random variables with a drop probability of p for all i.</p><p>Gaussian dropout <ref type="bibr" target="#b24">(Wang &amp; Manning, 2013)</ref> and variational dropout <ref type="bibr" target="#b11">(Kingma et al., 2015)</ref> use other random masks to improve dropout rather than Bernoulli random masks. To explain these variants of dropout as well, we set a random mask matrix</p><formula xml:id="formula_7">M = diag(M 1 , M 2 , • • • , M d ) to satisfy EM i = µ</formula><p>and Var(M i ) = σ 2 for all i. Now we define a random mixture function with respect to w from u and M as</p><formula xml:id="formula_8">Φ(w; u, M ) = µ −1 (I − M )u + M w − (1 − µ)u ,<label>(4)</label></formula><p>and a minimization problem with "mixconnect(u, µ, σ 2 )" as min w EL Φ(w; u, M ) .</p><p>(5)</p><p>We can view dropconnect(p) equation 3 as a special case of equation 5 where u = 0 and M = B. We investigate how mixconnect(u, µ, σ 2 ) differs from the vanilla minimization problem min w EL(w).</p><p>If the loss function L is strongly convex, we can derive a lower bound of EL Φ(w; u, M ) as in Theorem 1:</p><p>Theorem 1. Assume that the loss function L is strongly convex. Suppose that a random mixture function with respect to w from u and M is given by Φ(w; u, M ) in equation 4 where</p><formula xml:id="formula_10">M is diag(M 1 , M 2 , • • • , M d ) satisfying EM i = µ and Var(M i ) = σ 2 for all i.</formula><p>Then, there exists m &gt; 0 such that</p><formula xml:id="formula_11">EL Φ(w; u, M ) ≥ L(w) + mσ 2 2µ 2 w − u 2 , (<label>7</label></formula><formula xml:id="formula_12">)</formula><p>for all w (Proof in Supplement A).</p><p>Theorem 1 shows that minimizing the l.h.s. of equation 7 minimizes the r.h.s. of equation 7 when the r.h.s. is a sharp lower limit of the l.h.s. The strong convexity of L means that L is bounded from below by a quadratic function, and the inequality of equation 7 comes from the strong convexity.</p><p>Hence, the equality holds if L is quadratic, and mixconnect(u, µ, σ 2 ) is an L 2 -regularizer with a regularization coefficient of mσ 2 /µ 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">MIXCONNECT TO MIXOUT</head><p>We propose mixout as a special case of mixconnect, which is motivated by the relationship between dropout and dropconnect. We assume that</p><formula xml:id="formula_13">w = w (N1) 1 , • • • , w (N1) d1 , w (N2) 1 , • • • , w (N2) d2 , • • • • • • , w (N k ) 1 , • • • , w (N k ) d k ,</formula><p>where w</p><formula xml:id="formula_14">(Ni) j</formula><p>is the jth parameter outgoing from the neuron N i . We set the corresponding M to</p><formula xml:id="formula_15">M = diag M (N1) , • • • , M (N1) , M (N2) , • • • , M (N2) , • • • • • • , M (N k ) , • • • , M (N k ) ,<label>(8)</label></formula><p>where EM (Ni) = µ and Var(M (Ni) ) = σ 2 for all i. In this paper, we set M (Ni) to Bernoulli(1 − p) for all i and mixout(u) hereafter refers to this correlated version of mixconnect with Bernoulli random masks. We write it as "mixout(u, p)" when we emphasize the mix probability p. Corollary 1.1. Assume that the loss function L is strongly convex. We denote the random mixture function of mixout(u, p), which is equivalent to that of mixconnect(u, 1 − p, p − p 2 ), as Φ(w; u, M ) where M is defined in equation 8. Then, there exists m &gt; 0 such that</p><formula xml:id="formula_16">EL Φ(w; u, B) ≥ L(w) + mp 2(1 − p) w − u 2 ,<label>(9)</label></formula><p>for all w.</p><p>Corollary 1.1 is a straightforward result from Theorem 1. As the mix probability p in equation 9 increases to 1, the L 2 -regularization coefficient of mp/(1 − p) increases to infinity. It means that p of mixout(u, p) can adjust the strength of L 2 -penalty toward u in optimization. mixout(u) differs from wdecay(u) since the regularization coefficient of mixout(u) depends on m determined by the current model parameter w. mixout(u, p) indeed regularizes learning to minimize the deviation from u. We validate this by performing least squares regression in Supplement D.</p><p>We often apply dropout to specific layers. For instance, <ref type="bibr" target="#b18">Simonyan &amp; Zisserman (2014)</ref> applied dropout to fully connected layers only. We generalize Theorem 1 to the case in which mixout is only applied to specific layers, and it can be done by constructing M in a particular way. We demonstrate this approach in Supplement B and show that mixout for specific layers adaptively L 2 -penalizes their parameters. <ref type="bibr" target="#b9">Hoffer et al. (2017)</ref> have empirically shown that w t − w 0 ∼ log t, (10) where w t is a model parameter after the t-th SGD step. When training from scratch, we usually sample an initial model parameter w 0 from a normal/uniform distribution with mean 0 and small variance. Since w 0 is close to the origin, w t is away from the origin only with a large t by equation 10. When finetuning, we initialize our model parameter from a pretrained model parameter w pre . Since we usually obtain w pre by training from scratch on a large pretraining dataset, w pre is often far away from the origin. By Corollary 1.1, dropout L 2 -penalizes the model parameter for deviating away from the origin rather than w pre . To explicitly prevent the deviation from w pre , we instead propose to use mixout(w pre ). <ref type="bibr" target="#b26">Wiese et al. (2017)</ref> have highlighted that wdecay(w pre ) is an effective regularization technique to avoid catastrophic forgetting during finetuning. Because mixout(w pre ) keeps the finetuned model to stay in the vicinity of the pretrained model similarly to wdecay(w pre ), we suspect that the proposed mixout(w pre ) has a similar effect of alleviating the issue of catastrophic forgetting. To empirically verify this claim, we pretrain a 784-300-100-10 fully-connected network on EMNIST Digits <ref type="bibr" target="#b3">(Cohen et al., 2017)</ref>, and finetune it on MNIST. For more detailed description of the model architecture and datasets, see Supplement C.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">MIXOUT FOR PRETRAINED MODELS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">VERIFICATION OF THEORETICAL RESULTS FOR MIXOUT ON MNIST</head><p>In the pretraining stage, we run five random experiments with a batch size of 32 for {1, 2, • • • , 20} training epochs. We use Adam (Kingma &amp; Ba, 2014) with a learning rate of 10 −4 , β 1 = 0.9, β 2 = 0.999, wdecay(0, 0.01), learning rate warm-up over the first 10% steps of the total steps, and linear decay of the learning rate after the warm-up. We use dropout(0.1) for all layers except the input and output layers. We select w pre whose validation accuracy on EMNIST Digits is best (0.992) in all experiments.</p><p>For finetuning, most of the model hyperparameters are kept same as in pretraining, with the exception of the learning rate, number of training epochs, and regularization techniques. We train with a learning rate of 5 × 10 −5 for 5 training epochs. We replace dropout(p) with mixout(w pre , p). We do not use any other regularization technique such as wdecay(0) and wdecay(w pre ). We monitor w ft − w pre<ref type="foot" target="#foot_1">2</ref> ,<ref type="foot" target="#foot_0">1</ref> validation accuracy on MNIST, and validation accuracy on EMNIST Digits to compare mixout(w pre , p) to dropout(p) across 10 random restarts. , validation accuracy on MNIST (target task), and validation accuracy on EMNIST Digits (source task), as the function of the probability p where w ft and w pre are the model parameter after finetuning and the pretrained model parameter, respectively. We report mean (curve) ± std. (shaded area) across 10 random restarts. (a): mixout(w pre , p) L 2 -penalizes the deviation from w pre , and this penalty becomes strong as p increases. However, with dropout(p), w ft becomes away from w pre as p increases. (b): After finetuning on MNIST, both mixout(w pre , p) and dropout(p) result in high validation accuracy on MNIST for p ∈ {0.1, 0.2, 0.3}. (c): Validation accuracy of dropout(p) on EMNIST Digits drops more than that of mixout(w pre , p) for all p. mixout(w pre , p) minimizes the deviation from w pre and memorizes the source task better than dropout(p) for all p.</p><p>As shown in Figure <ref type="figure" target="#fig_1">2</ref> (a), after finetuning with mixout(w pre , p), the deviation from w pre is minimized in the L 2 -sense. This result verifies Corollary 1.1. We demonstrate that the validation accuracy of mixout(w pre , p) has greater robustness to the choice of p than that of dropout(p). In Figure <ref type="figure" target="#fig_1">2</ref> (b), both dropout(p) and mixout(w pre , p) result in high validation accuracy on the target task (MNIST) for p ∈ {0.1, 0.2, 0.3}, although mixout(w pre , p) is much more robust with respect to the choice of the mix probability p. In Figure <ref type="figure" target="#fig_1">2</ref> (c), the validation accuracy of mixout(w pre , p) on the source task (EMNIST Digits) drops from the validation accuracy of the model at w pre (0.992) to approximately 0.723 regardless of p. On the other hand, the validation accuracy of dropout(p) on the source task respectively drops by 0.041, 0.074 and 0.105 which are more than those of mixout(w pre , p) for p ∈ {0.1, 0.2, 0.3}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">FINETUNING A PRETRAINED LANGUAGE MODEL WITH MIXOUT</head><p>In order to experimentally validate the effectiveness of mixout, we finetune BERT LARGE on a subset of GLUE <ref type="bibr" target="#b23">(Wang et al., 2018)</ref> tasks (RTE, MRPC, CoLA, and STS-B) with mixout(w pre ). We choose them because <ref type="bibr" target="#b16">Phang et al. (2018)</ref> have observed that it was unstable to finetune BERT LARGE on these four tasks. We use the publicly available pretrained model released by <ref type="bibr" target="#b6">Devlin et al. (2018)</ref>, ported into PyTorch by HuggingFace. <ref type="foot" target="#foot_2">3</ref> We use the learning setup and hyperparameters recommended by <ref type="bibr" target="#b6">Devlin et al. (2018)</ref>. We use Adam with a learning rate of 2 × 10 −5 , β 1 = 0.9, β 2 = 0.999, learning rate warmup over the first 10% steps of the total steps, and linear decay of the learning rate after the warmup finishes. We train with a batch size of 32 for 3 training epochs. Since the pretrained BERT LARGE is the sentence encoder, we have to create an additional output layer, which is not pretrained. We initialize each parameter of it with N (0, 0.02 2 ). We describe our experimental setup further in Supplement C.2.</p><p>The original regularization strategy used in <ref type="bibr" target="#b6">Devlin et al. (2018)</ref> for finetuning BERT LARGE is using both dropout(0.1) and wdecay(0, 0.01) for all layers except layer normalization and intermediate layers activated by GELU <ref type="bibr" target="#b8">(Hendrycks &amp; Gimpel, 2016)</ref>. We however cannot use mixout(w pre ) nor wdecay(w pre ) for the additional output layer which was not pretrained and therefore does not have w pre . We do not use any regularization for the additional output layer when finetuning BERT LARGE with mixout(w pre ) and wdecay(w pre ). For the other layers, we replace dropout(0.1) and wdecay(0, 0.01) with mixout(w pre ) and wdecay(w pre ), respectively. <ref type="bibr" target="#b16">Phang et al. (2018)</ref> have reported that large pretrained models (e.g., BERT LARGE ) are prone to degenerate performance when finetuned on a task with a small number of training examples, and that multiple random restarts<ref type="foot" target="#foot_3">4</ref> are required to obtain a usable model better than random prediction. To compare finetuning stability of the regularization techniques, we need to demonstrate the distribution of model performance. We therefore train BERT LARGE with each regularization strategy on each task with 20 random restarts. We validate each random restart on the dev set to observe the behaviour of the proposed mixout and finally evaluate it on the test set for generalization. We present the test score of our proposed regularization strategy on each task in Supplement C.3.</p><p>We finetune BERT LARGE with mixout(w pre , {0.7, 0.8, 0.9}) on RTE, MRPC, CoLA, and STS-B. For the baselines, we finetune BERT LARGE with both dropout(0.1) and wdecay(0, 0.01) as well as with wdecay(w pre , {0.01, 0.04, 0.07, 0.10}). These choices are made based on the experiments in Section 6.3 and Supplement F. In Section 6.3, we observe that finetuning BERT LARGE with mixout(w pre , p) on RTE is significantly more stable with p ∈ {0.7, 0.8, 0.9} while finetuning with dropout(p) becomes unstable as p increases. In Supplement F, we demonstrate that dropout(0.1) is almost optimal for all the tasks in terms of mean dev score although <ref type="bibr" target="#b6">Devlin et al. (2018)</ref> selected it to improve the maximum dev score.</p><p>In Figure <ref type="figure" target="#fig_2">3</ref>, we plot the distributions of the dev scores from 20 random restarts when finetuning BERT LARGE with various regularization strategies on each task. For conciseness, we only show four regularization strategies; <ref type="bibr" target="#b6">Devlin et al. (2018)</ref>'s: both dropout(0.1) and wdecay(0, 0.01), <ref type="bibr" target="#b26">Wiese et al. (2017)</ref>'s: wdecay(w pre , 0.01), ours: mixout(w pre , 0.7), and ours+Wiese et al.</p><p>(2017)'s: both mixout(w pre , 0.7) and wdecay(w pre , 0.01). As shown in Figure <ref type="figure" target="#fig_2">3 (a-c</ref>), we observe many finetuning runs that fail with the chance-level accuracy when we finetune BERT LARGE with both dropout(0.1) and wdecay(0, 0.01) on RTE, MRPC, and CoLA. We also have a bunch of degenerate model configurations when we use wdecay(w pre , 0.01) without mixout(w pre , 0.7).</p><p>Unlike existing regularization strategies, when we use mixout(w pre , 0.7) as a regularization technique with or without wdecay(w pre , 0.01) for finetuning BERT LARGE , the number of degenerate model configurations that fail with a chance-level accuracy significantly decreases. For example, in Figure <ref type="figure" target="#fig_2">3</ref>   <ref type="formula">2017</ref>)'s: both mixout(w pre , 0.7) and wdecay(w pre , 0.01). We write them as Devlin (blue), Wiese (orange), Our (green), and Our+W (red), respectively. We use the same set of 20 random initializations across all the regularization setups. Error intervals show mean±std. For all the tasks, the number of finetuning runs that fail with the chance-level accuracy is significantly reduced when we use our regularization mixout(w pre , 0.7) regardless of using wdecay(w pre , 0.01).</p><p>In Figure <ref type="figure" target="#fig_2">3</ref> (a), we further improve the stability of finetuning BERT LARGE by using both mixout(w pre , 0.7) and wdecay(w pre , 0.01).  <ref type="formula">2017</ref>)'s. In Figure <ref type="figure" target="#fig_2">3 (b, c</ref>), we observe that the number of degenerate model configurations increases when we use wdecay(w pre , 0.01) additionally to mixout(w pre , 0.7). In short, applying our proposed mixout significantly stabilizes the finetuning results of BERT LARGE on small training sets regardless of whether we use wdecay(w pre , 0.01).</p><p>In Table <ref type="table">1</ref>, we report the average and the best dev scores across 20 random restarts for each task with various regularization strategies. The average dev scores with mixout(w pre , {0.7, 0.8, 0.9}) increase for all the tasks. For instance, the mean dev score of finetuning with mixout(w pre , 0.8) on CoLA is 57.9 which is 49.2% increase over 38.8 obtained by finetuning with both dropout(p) and wdecay(0, 0.01). We observe that using wdecay(w pre , {0.01, 0.04, 0.07, 0.10}) also improves the average dev scores for most tasks compared to using both dropout(p) and wdecay(0, 0.01). We however observe that finetuning with mixout(w pre , {0.7, 0.8, 0.9}) outperforms that with wdecay(w pre , {0.01, 0.04, 0.07, 0.10}) on average. This confirms that mixout(w pre ) has a different effect for finetuning BERT LARGE compared to wdecay(w pre ) since mixout(w pre ) is an adaptive L 2 -regularizer along the optimization trajectory.</p><p>Since finetuning a large pretrained language model such as BERT LARGE on a small training set frequently fails, the final model performance has often been reported as the maximum dev score <ref type="bibr" target="#b6">(Devlin et al., 2018;</ref><ref type="bibr" target="#b16">Phang et al., 2018)</ref> among a few random restarts. We thus report the best dev score for each setting in Table <ref type="table">1</ref>. According to the best dev scores as well, mixout(w pre , {0.7, 0.8, 0.9}) improves performance for all the tasks compared to using both dropout(p) and wdecay(0, 0.01). For instance, using mixout(w pre , 0.9) improves the maximum dev score by 0.9 compared to using both dropout(p) and wdecay(0, 0.01) on MRPC. Unlike the average dev scores, the best dev scores achieved by using wdecay(w pre , {0.01, 0.04, 0.07, 0.10}) are better than those achieved by using mixout(w pre , {0.7, 0.8, 0.9}) except RTE on which it was better to use mixout(w pre , {0.7, 0.8, 0.9}) than wdecay(w pre , {0.01, 0.04, 0.07, 0.10}).</p><p>Table <ref type="table">1</ref>: Mean (max) dev scores across 20 random restarts when finetuning BERT LARGE with various regularization strategies on each task. We show the following baseline results on the first and second cells: Devlin et al. ( <ref type="formula">2018</ref>)'s regularization strategy (both dropout(p) and wdecay(0, 0.01)) and Wiese et al. ( <ref type="formula">2017</ref>)'s regularization strategy (wdecay(w pre , {0.01, 0.04, 0.07, 0.10})). In the third cell, we demonstrate finetuning results with only mixout(w pre , {0.7, 0.8, 0.9}). The results with both mixout(w pre , {0.7, 0.8, 0.9}) and wdecay(w pre , 0.01) are also presented in the fourth cell. Bold marks the best of each statistics within each column. The mean dev scores greatly increase for all the tasks when we use mixout(w pre , {0.7, 0.8, 0.9}). We investigate the effect of combining both mixout(w pre ) and wdecay(w pre ) to see whether they are complementary. We finetune BERT LARGE with both mixout(w pre , {0.7, 0.8, 0.9}) and wdecay(w pre , 0.01). This leads not only to the improvement in the average dev scores but also in the best dev scores compared to using wdecay(w pre , {0.01, 0.04, 0.07, 0.10}) and using both dropout(p) and wdecay(0, 0.01). The experiments in this section confirm that using mixout(w pre ) as one of several regularization techniques prevents finetuning instability and yields gains in dev scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TECHNIQUE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">ABLATION STUDY</head><p>In this section, we perform ablation experiments to better understand mixout(w pre ). Unless explicitly stated, all experimental setups are the same as in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">MIXOUT WITH A SUFFICIENT NUMBER OF TRAINING EXAMPLES</head><p>We showed the effectiveness of the proposed mixout finetuning with only a few training examples in Section 5. In this section, we investigate the effectiveness of the proposed mixout in the case of a larger finetuning set. Since it has been stable to finetune BERT LARGE on a sufficient number of training examples <ref type="bibr" target="#b6">(Devlin et al., 2018;</ref><ref type="bibr" target="#b16">Phang et al., 2018)</ref>, we expect to see the change in the behaviour of mixout(w pre ) when we use it to finetune BERT LARGE on a larger training set.</p><p>We train BERT LARGE by using both mixout(w pre , 0.7) and wdecay(w pre , 0.01) with 20 random restarts on SST-2. <ref type="foot" target="#foot_4">5</ref> We also train BERT LARGE by using both dropout(p) and wdecay(0, 0.01) with 20 random restarts on SST-2 as the baseline. In Table <ref type="table">2</ref>, we report the mean and maximum of SST-2 dev scores across 20 random restarts with each regularization strategy. We observe that there is little difference between their mean and maximum dev scores on a larger training set, although using both mixout(w pre , 0.7) and wdecay(w pre , 0.01) outperformed using both dropout(p) and wdecay(0, 0.01) on the small training sets in Section 5.</p><p>Table <ref type="table">2</ref>: Mean (max) SST-2 dev scores across 20 random restarts when finetuning BERT LARGE with each regularization strategy. Bold marks the best of each statistics within each column. For a large training set, both mean and maximum dev scores are similar to each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TECHNIQUE 1</head><p>TECHNIQUE 2 SST-2 dropout(0.1) wdecay(0, 0.01) 93.4 (94.0) mixout(w pre , 0.7) wdecay(w pre , 0.01) 93.5 (94.3)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">EFFECT OF A REGULARIZATION TECHNIQUE FOR AN ADDITIONAL OUTPUT LAYER</head><p>In this section, we explore the effect of a regularization technique for an additional output layer. There are two regularization techniques available for the additional output layer: dropout(p) and mixout(w 0 , p) where w 0 is its randomly initialized parameter. Either of these strategies differs from the earlier experiments in Section 5 where we did not put any regularization for the additional output layer.</p><p>Table <ref type="table">3</ref>: We present mean (max) dev scores across 20 random restarts with various regularization techniques for the additional output layers (ADDITIONAL) when finetuning BERT LARGE on each task. For all cases, we apply mixout(w pre , 0.7) to the pretrained layers (PRETRAINED). The first row corresponds to the setup in Section 5. In the second row, we apply mixout(w 0 , 0.7) to the additional output layer where w 0 is its randomly initialized parameter. The third row shows the results obtained by applying dropout(0.7) to the additional output layer. In the fourth row, we demonstrate the best of each result from all the regularization strategies shown in Table <ref type="table">1</ref>. Bold marks the best of each statistics within each column. We obtain additional gains in dev scores by varying the regularization technique for the additional output layer. We report the average and best dev scores across 20 random restarts when finetuning BERT LARGE with mixout(w pre , 0.7) while varying the regularization technique for the additional output layer in Table <ref type="table">3</ref>. <ref type="foot" target="#foot_5">6</ref> We observe that using mixout(w 0 , 0.7) for the additional output layer improves both the average and best dev score on RTE, CoLA, and STS-B. In the case of MRPC, we have the highest best-dev score by using dropout(0.7) for the additional output layer while the highest mean dev score is obtained by using mixout(w 0 , 0.7) for it. In Section 3.2, we discussed how mixout(w 0 ) does not differ from dropout when the layer is randomly initialized, since we sample w 0 from w whose mean and variance are 0 and small, respectively. Although the additional output layer is randomly initialized, we observe the significant difference between dropout and mixout(w 0 ) in this layer. We conjecture that w 0 − 0 is not sufficiently small because E w − 0 is proportional to the dimensionality of the layer (2,048). We therefore expect mixout(w 0 ) to behave differently from dropout even for the case of training from scratch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PRETRAINED</head><p>In the last row of Table <ref type="table">3</ref>, we present the best of the corresponding result from Table <ref type="table">1</ref>. We have the highest mean and best dev scores when we respectively use mixout(w pre , 0.7) and mixout(w 0 , 0.7) for the pretrained layers and the additional output layer on RTE, CoLA, and STS-B. The highest mean dev score on MRPC is obtained by using mixout(w pre , 0.8) for the pretrained layers which is one of the results in Table <ref type="table">1</ref>. We have the highest best dev score on MRPC when we use mixout(w pre , 0.7) and dropout(0.7) for the pretrained layers and the additional output layer, respectively. The experiments in this section reveal that using mixout(w 0 ) for a randomly initialized layer of a pretrained model is one of the regularization schemes to improve the average dev score and the best dev score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">EFFECT OF MIX PROBABILITY FOR MIXOUT AND DROPOUT</head><p>We explore the effect of the hyperparameter p when finetuning BERT LARGE with mixout(w pre , p) and dropout(p). We train BERT LARGE with mixout(w pre , {0.0, 0.1, • • • , 0.9}) on RTE with 20 random restarts. We also train BERT LARGE after replacing mixout(w pre , p) by dropout(p) with 20 random restarts. We do not use any regularization technique for the additional output layer. Because we use neither wdecay(0) nor wdecay(w pre ) in this section, dropout(0.0) and mixout(w pre , 0.0) are equivalent to finetuning without regularization.</p><p>0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 Error intervals show mean±std. We do not use wdecay(0) nor wdecay(w pre ). In the case of mixout(w pre , p), the number of usable models after finetuning with mixout(w pre , {0.7, 0.8, 0.9}) is significantly more than the number of usable models after finetuning with dropout(p) for all p.</p><p>It is not helpful to vary p for dropout(p) while mixout(w pre , p) helps significantly in a wide range of p. Figure <ref type="figure" target="#fig_4">4</ref> shows distributions of RTE dev scores across 20 random restarts when finetuning BERT LARGE with dropout(p) and mixout(w pre , p) for p ∈ {0.0, 0.1, • • • , 0.9}. The mean dev score of finetuning BERT LARGE with mixout(w pre , p) increases as p increases. On the other hand, the mean dev score of finetuning BERT LARGE with dropout(p) decreases as p increases. If p is less than 0.4, finetuning with mixout(w pre , p) does not improve the finetuning results of using dropout({0.0, 0.1, 0.2}). We however observe that mixout(w pre , {0.7, 0.8, 0.9}) yields better average dev scores than dropout(p) for all p, and significantly reduces the number of finetuning runs that fail with the chance-level accuracy.</p><p>We notice that the proposed mixout spends more time than dropout from the experiments in this section. It takes longer to finetune a model with the proposed mixout than with the original dropout, although this increase is not significant especially considering the waste of time from failed finetuning runs using dropout. In Supplement E, we describe more in detail the difference between mixout and dropout in terms of wall-clock time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>The special case of our approach, mixout(w pre ), is one of several regularization techniques modifying a finetuning procedure to prevent catastrophic forgetting. Unlike wdecay(w pre ) proposed earlier by <ref type="bibr" target="#b26">Wiese et al. (2017)</ref>, mixout(w pre ) is an adaptive L 2 -regularizer toward w pre in the sense that its regularization coefficient adapts along the optimization path. Due to this difference, the proposed mixout improves the stability of finetuning a big, pretrained language model even with only a few training examples of a target task. Furthermore, our experiments have revealed the proposed approach improves finetuning results in terms of the average accuracy and the best accuracy over multiple runs. We emphasize that our approach can be used with any pretrained language models such as RoBERTa <ref type="bibr" target="#b13">(Liu et al., 2019)</ref> and XLNet <ref type="bibr" target="#b27">(Yang et al., 2019)</ref>, since mixout does not depend on model architectures, and leave it as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SUPPLEMENTARY MATERIAL</head><p>A PROOFS FOR THEOREM 1</p><p>Theorem 1. Assume that the loss function L is strongly convex. Suppose that a random mixture function with respect to w from u and M is given by</p><formula xml:id="formula_17">Φ(w; u, M ) = µ −1 (I − M )u + M w − (1 − µ)u ,</formula><p>where</p><formula xml:id="formula_18">M is diag(M 1 , M 2 , • • • , M d ) satisfying EM i = µ and Var(M i ) = σ 2 for all i.</formula><p>Then, there exists m &gt; 0 such that</p><formula xml:id="formula_19">EL Φ(w; u, M ) ≥ L(w) + mσ 2 2µ 2 w − u 2 ,<label>(11)</label></formula><p>for all w.</p><p>Proof. Since L is strongly convex, there exist m &gt; 0 such that</p><formula xml:id="formula_20">EL Φ(w; u, M ) = EL w + Φ(w; u, M ) − w ≥ L(w) + ∇L(w) E[Φ(w; u, M ) − w] + m 2 E Φ(w; u, M ) − w 2 ,<label>(12)</label></formula><p>for all w by equation 1. Recall that EM i = µ and Var(M i ) = σ 2 for all i. Then, we have</p><formula xml:id="formula_21">E[Φ(w; u, M ) − w] = 0,<label>(13)</label></formula><p>and</p><formula xml:id="formula_22">E Φ(w; u, M ) − w 2 = E 1 µ (w − u)(M − µI) 2 = 1 µ 2 d i=1 (w i − u i ) 2 E(M i − µ) 2 = σ 2 µ 2 w − u 2 . (<label>14</label></formula><formula xml:id="formula_23">)</formula><p>By using equation 13 and equation 14, we can rewrite equation 12 as</p><formula xml:id="formula_24">EL Φ(w; u, M ) ≥ L(w) + mσ 2 2µ 2 w − u 2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B APPLYING TO SPECIFIC LAYERS</head><p>We often apply dropout to specific layers. For instance, <ref type="bibr" target="#b18">Simonyan &amp; Zisserman (2014)</ref> applied dropout to fully connected layers only. We generalize Theorem 1 to the case in which mixconnect is only applied to specific layers, and it can be done by constructing M in a particular way. To better characterize mixconnect applied to specific layers, we define the index set I as I = {i : M i = 1}. Furthermore, we use w and ũ to denote (w i ) i / ∈I and (u i ) i / ∈I , respectively. Then, we generalize equation 7 to</p><formula xml:id="formula_25">EL Φ(w; u, M ) ≥ L(w) + mσ 2 2µ 2 w − ũ 2 . (<label>15</label></formula><formula xml:id="formula_26">)</formula><p>From equation 15, applying mixconnect(u, µ, σ 2 ) is to use adaptive wdecay( ũ) on the weight parameter of the specific layers w. Similarly, we can regard applying mixout(u, p) to specific layers as adaptive wdecay( ũ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C EXPERIMENTAL DETAILS C.1 FROM EMNIST DIGITS TO MNIST</head><p>Model Architecture The model architecture in Section 4 is a 784-300-100-10 fully connected network with a softmax output layer. For each hidden layer, we add layer normalization <ref type="bibr" target="#b0">(Ba et al., 2016)</ref> right after the ReLU <ref type="bibr" target="#b15">(Nair &amp; Hinton, 2010)</ref> nonlinearity. We initialize each parameter with N (0, 0.02 2 ) and each bias with 0.</p><p>Regularization In the pretraining stage, we use dropout(0.1) and wdecay(0, 0.01). We apply dropout(0.1) to all hidden layers. That is, we do not drop neurons of the input and output layers. wdecay(0, 0.01) does not penalize the parameters for bias and layer normalization. When we finetune our model on MNIST, we replace dropout(p) with mixout(w pre , p). We use neither wdecay(0) nor wdecay(w pre ) for finetuning.</p><p>Dataset For pretraining, we train our model on EMNIST Digits. This dataset has 280,000 characters into 10 balanced classes. Model Architecture Because the model architecture of BERT LARGE is identical to the original <ref type="bibr" target="#b6">(Devlin et al., 2018)</ref>, we omit its exhaustive description. Briefly, BERT LARGE has 24 layers, 1024 hidden size, and 16 self-attention heads (total 340M parameters). We use the publicly available pretrained model released by <ref type="bibr" target="#b6">Devlin et al. (2018)</ref>, ported into PyTorch by HuggingFace. <ref type="foot" target="#foot_6">7</ref> We initialize each weight parameter and bias for an additional output layer with N (0, 0.02 2 ) and 0, respectively.</p><p>Regularization In the finetuning stage, <ref type="bibr" target="#b6">Devlin et al. (2018)</ref> used wdecay(0, 0.01) for all parameters except bias and layer normalization. They apply dropout(0.1) to all layers except each hidden layer activated by GELU <ref type="bibr" target="#b8">(Hendrycks &amp; Gimpel, 2016)</ref> and layer normalization. We substitute wdecay(w pre ) and mixout(w pre ) for wdecay(0, 0.01) and dropout(0.1), respectively.</p><p>Dataset We use a subset of GLUE <ref type="bibr" target="#b23">(Wang et al., 2018)</ref> tasks. The brief description for each dataset is as the following: We expect that using mixout stabilizes finetuning results of BERT LARGE on a small training set.</p><p>To show this, we demonstrated distributions of dev scores from 20 random restarts on RTE, MRPC, CoLA, and STS-B in Figure <ref type="figure" target="#fig_2">3</ref>. We further obtained the highest average/best dev score on each task in Table <ref type="table">3</ref>. To confirm the generalization of the our best model on the dev set, we demonstrate the test results scored by the evaluation server 9 in Table <ref type="table">4</ref>.</p><p>Table <ref type="table">4</ref>: We present the test score when finetuning BERT LARGE with each regularization strategy on each task. The first row shows the test scores obtained by using both dropout(p) and wdecay(0, 0.01). These results in the first row are reported by <ref type="bibr" target="#b6">Devlin et al. (2018)</ref>. They used the learning rate of {2 × 10 −5 , 3 × 10 −5 , 4 × 10 −5 , 5 × 10 −5 } and a batch size of 32 for 3 epochs with multiple random restarts. They selected the best model on each dev set. In the second row, we demonstrate the test scores obtained by using the proposed mixout in Section 6.2: using mixout(w pre , 0.7) for the pretrained layers and mixout(w 0 , 0.7) for the additional output layer where w 0 is its randomly initialized weight parameter. We used the learning rate of 2 × 10 −5 and a batch size of 32 for 3 epochs with 20 random restarts. We submitted the best model on each dev set. For all the tasks except MRPC, the test scores obtained by the proposed mixout 10 are better than those reported by <ref type="bibr" target="#b6">Devlin et al. (2018)</ref>. We explored the behaviour of finetuning BERT LARGE with mixout by using the learning rate of 2 × 10 −5 while <ref type="bibr" target="#b6">Devlin et al. (2018)</ref> obtained their results by using the learning rate of {2 × 10 −5 , 3 × 10 −5 , 4 × 10 −5 , 5 × 10 −5 }. We thus present the test scores obtained by the regularization strategy of <ref type="bibr" target="#b6">Devlin et al. (2018)</ref> when the learning rate is 2 × 10 −5 . The results in this section show that the best model on the dev set generalizes well, and all the experiments based on dev scores in this paper are proper to validate the effectiveness of the proposed mixout. For the remaining GLUE tasks such as SST-2 with a sufficient number of training instances, we observed that using mixout does not differs from using dropout in Section 6.1. We therefore omit the test results on the other tasks in GLUE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D VERIFICATION OF COROLLARY 1.1 WITH LEAST SQUARES REGRESSION</head><p>Corollary 1.1 shows that mixout(u, p) regularizes learning to minimize the deviation from the target model parameter u, and the strength of regularization increases as p increases when the loss function is strongly convex. In order to validate this, we explore the behavior of least squares regression with mixout(u, p) on a synthetic dataset. For randomly given w * 1 and w * 2 , we generated an observation y satisfying y = w * 1 x + w * 2 + where is Gaussian noise. We set the model to ŷ = w 1 x + w 2 . That is, the model parameter w is given by (w 1 , w 2 ). We randomly pick u as a target model parameter for mixout(u, p) and perform least squares regression with mixout(u, {0.0, 0.3, 0.6, 0.9}). As shown in Figure <ref type="figure" target="#fig_5">5</ref>, w converges to the target model parameter u rather than the true model parameter w * = (w * 1 , w * 2 ) as the mix probability p increases. 9 https://gluebenchmark.com/leaderboard 10 The regularization strategy in Section 6.2: using mixout(wpre, 0.7) for the pretrained layers and mixout(w0, 0.7) for the additional output where w0 is its randomly initialized weight parameter. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E TIME USAGE OF MIXOUT COMPARED TO DROPOUT</head><p>We recorded the training time of the experiment in Section 6.3 to compare the time usage of mixout and that of dropout. It took about 843 seconds to finetune BERT LARGE with mixout(w pre ). On the other hand, it took about 636 seconds to finetune BERT LARGE with dropout. mixout(w pre ) spends 32.5% more time than dropout since mixout(w pre ) needs an additional computation with the pretrained model parameter w pre . However, as shown in Figure <ref type="figure" target="#fig_4">4</ref>, at least 15 finetuning runs among 20 random restarts fail with the chance-level accuracy on RTE with dropout(p) for all p while only 4 finetuning runs out of 20 random restarts are unusable with mixout(w pre , 0.8). From this result, it is reasonable to finetune with the proposed mixout although this requires additional time usage compared to dropout.</p><p>F EXTENSIVE HYPERPARAMETER SEARCH FOR DROPOUT <ref type="bibr" target="#b6">Devlin et al. (2018)</ref> finetuned BERT LARGE with dropout(0.1) on all GLUE <ref type="bibr" target="#b23">(Wang et al., 2018)</ref> tasks. They chose it to improve the maximum dev score on each downstream task, but we have reported not only the maximum dev score but also the mean dev score to quantitatively compare various regularization techniques in our paper. In this section, we explore the effect of the hyperparameter p when finetuning BERT LARGE with dropout(p) on RTE, MRPC, CoLA, and STS-B to show dropout(0.1) is optimal in terms of mean dev score. All experimental setups for these experiments are the same as Section 6.3. As shown in Figure <ref type="figure" target="#fig_6">6</ref>, we have the highest average dev score on MRPC with dropout(0.1) as well as on STS-B. We obtain the highest average dev scores with dropout(0.0) on RTE and CoLA, but we get the second-highest average dev scores with dropout(0.1) on them. These experiments confirm that the drop probability 0.1 is almost optimal for the highest average dev score on each task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of mixout(u). Suppose that u and w are a target model parameter and a current model parameter, respectively. (a): We first memorize the parameters of the vanilla network at u. (b):In the dropout network, we randomly choose an input neuron to be dropped (a dotted neuron) with a probability of p. That is, all outgoing parameters from the dropped neuron are eliminated (dotted connections). (c): In the mixout(u) network, the eliminated parameters in (b) are replaced by the corresponding parameters in (a). In other words, the mixout(u) network at w is the mixture of the vanilla network at u and the dropout network at w with a probability of p.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: We present w ft − w pre 2 , validation accuracy on MNIST (target task), and validation accuracy on EMNIST Digits (source task), as the function of the probability p where w ft and w pre are the model parameter after finetuning and the pretrained model parameter, respectively. We report mean (curve) ± std. (shaded area) across 10 random restarts. (a): mixout(w pre , p) L 2 -penalizes the deviation from w pre , and this penalty becomes strong as p increases. However, with dropout(p), w ft becomes away from w pre as p increases. (b): After finetuning on MNIST, both mixout(w pre , p) and dropout(p) result in high validation accuracy on MNIST for p ∈ {0.1, 0.2, 0.3}. (c): Validation accuracy of dropout(p) on EMNIST Digits drops more than that of mixout(w pre , p) for all p. mixout(w pre , p) minimizes the deviation from w pre and memorizes the source task better than dropout(p) for all p.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Distribution of dev scores on each task from 20 random restarts when finetuning BERT LARGE with Devlin et al. (2018)'s: both dropout(0.1) and wdecay(0, 0.01), Wiese et al. (2017)'s: wdecay(w pre , 0.01), ours: mixout(w pre , 0.7), and ours+Wiese et al. (2017)'s: both mixout(w pre , 0.7) and wdecay(w pre , 0.01). We write them as Devlin (blue), Wiese (orange), Our (green), and Our+W (red), respectively. We use the same set of 20 random initializations across all the regularization setups. Error intervals show mean±std. For all the tasks, the number of finetuning runs that fail with the chance-level accuracy is significantly reduced when we use our regularization mixout(w pre , 0.7) regardless of using wdecay(w pre , 0.01).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Figure 3 (d) shows respectively two and one degenerate model configurations with Devlin et al. (2018)'s and Wiese et al. (2017)'s, but we do not have any degenerate resulting model with ours and ours+Wiese et al. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure4: Distribution of RTE dev scores (Accuracy) from 20 random restarts when finetuning BERT LARGE with dropout(p) (orange) or mixout(w pre , p) (blue). Error intervals show mean±std. We do not use wdecay(0) nor wdecay(w pre ). In the case of mixout(w pre , p), the number of usable models after finetuning with mixout(w pre , {0.7, 0.8, 0.9}) is significantly more than the number of usable models after finetuning with dropout(p) for all p.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure5: Behavior of mixout(u, p) for a strongly convex loss function. We plot the line obtained by least squares regression with mixout(u, {0.0, 0.3, 0.6, 0.9}) (each green line) on a synthetic dataset (blue dots) generated by the true line (each blue dotted line). As p increases, the regression line (each green line) converges to the target line generated by the target model parameter u (each orange dotted line) rather than the true line (each blue dotted line).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure6: Distribution of dev scores on each task from 20 random restarts when finetuning BERT LARGE with dropout({0.0, 0.1, • • • , 0.5}). Error intervals show mean±std. When we use dropout(0.1), we have the highest average dev scores on MRPC and STS-B and the second-highest average dev scores on RTE and CoLA. These results show that dropout(0.1) is almost optimal for all tasks in terms of mean dev score.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc><ref type="bibr" target="#b26">Wiese et al. (2017)</ref>,<ref type="bibr" target="#b12">Kirkpatrick et al. (2017)</ref>, and<ref type="bibr" target="#b17">Schwarz et al. (2018)</ref> used L 2 -penalty toward a pretrained model parameter to improve performance. They focused on preventing catastrophic forgetting to enable their models to learn multiple tasks sequentially. They however do not discuss nor demonstrate the effect of L 2 -penalty toward the pretrained model parameter on the stability of finetuning.<ref type="bibr" target="#b1">Barone et al. (2017)</ref> introduced tuneout, which is a special case of mixout. They applied various regularization techniques including dropout, tuneout, and L 2 -penalty toward a pretrained model parameter to finetune neural machine translation. They however do not demonstrate empirical significance of tuneout compared to other regularization techniques nor its theoretical justification.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>These characters are compatible with MNIST characters. EMNIST Digits provides 240,000 characters for training and 40,000 characters for test. We use 240,000 characters provide for training and split these into the training set (216,000 characters) and validation set (24,000 characters). For finetuning, we train our model on MNIST. This has 70,000 characters into 10 balance classes. MNIST provide 60,000 characters for training and 10,000 characters for test. We use 60,000 characters given for training and split these into the training set (54,000 characters) and validation set (6,000 characters).</figDesc><table><row><cell>Data Preprocessing We only use normalization after scaling pixel values into [0, 1]. We do not</cell></row><row><cell>use any data augmentation.</cell></row><row><cell>C.2 FINETUNING BERT ON PARTIAL GLUE TASKS</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>The third row shows that the test scores obtained by using both dropout(p) and wdecay(0, 0.01) with same experimental setups of the second row. Bold marks the best within each column. The proposed mixout improves the test scores except MRPC compared to the original regularization strategy proposed by<ref type="bibr" target="#b6">Devlin et al. (2018)</ref>.</figDesc><table><row><cell>STRATEGY</cell><cell cols="4">RTE MRPC CoLA STS-B</cell></row><row><cell>Devlin et al. (2018)</cell><cell>70.1</cell><cell>89.3</cell><cell>60.5</cell><cell>86.5</cell></row><row><cell cols="2">mixout(w pre , 0.7) &amp; mixout(w 0 , 0.7) 70.2</cell><cell>89.1</cell><cell>62.1</cell><cell>87.3</cell></row><row><cell>dropout(p) + wdecay(0, 0.01)</cell><cell>68.2</cell><cell>88.3</cell><cell>59.6</cell><cell>86.0</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">wft is a model parameter after finetuning.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">Using the same pretrained model parameter wpre but perform different finetuning data shuffling.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">https : / / s3 . amazonaws . com / models . huggingface . co / bert / bert-large-uncased-pytorch_model.bin</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">Using the same pretrained model parameter wpre but each random restart differs from the others by shuffling target data and initializing the additional output layer differently.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4">For the description of SST-2, see Supplement C.2.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5">In this experiment, we use neither wdecay(0) nor wdecay(wpre).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6">https : / / s3 . amazonaws . com / models . huggingface . co / bert / bert-large-uncased-pytorch_model.bin</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7">https://github.com/huggingface/pytorch-transformers</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The first and third authors' work was supported by the National Research Foundation of Korea (NRF) grants funded by the Korea government (MOE, MSIT) (NRF-2017R1A2B4011546, NRF-2019R1A5A1028324). The second author thanks support by AdeptMind, eBay, TenCent, NVIDIA and CIFAR and was partly supported by Samsung Electronics (Improving Deep Learning using Latent Structure).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lei Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Ryan Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Regularization techniques for fine-tuning in neural machine translation</title>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Valerio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miceli</forename><surname>Barone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ulrich</forename><surname>Germann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.09920</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inigo</forename><surname>Lopez-Gazpio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.00055</idno>
		<title level="m">Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Emnist: an extension of mnist to handwritten letters</title>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saeed</forename><surname>Afshar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Tapson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">André</forename><surname>Van Schaik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.05373</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011-08">Aug. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The PASCAL recognising textual entailment challenge</title>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Glickman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine learning challenges. evaluating predictive uncertainty, visual object classification, and recognising tectual entailment</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="177" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Automatically constructing a corpus of sentential paraphrases</title>
		<author>
			<persName><forename type="first">B</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><surname>Brockett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Paraphrasing</title>
				<meeting>the International Workshop on Paraphrasing</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<title level="m">Gaussian error linear units (gelus)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Train longer, generalize better: closing the generalization gap in large batch training of neural networks</title>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Itay</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Soudry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1731" to="1741" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Variational dropout and the local reparameterization trick</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Durk P Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2575" to="2583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting in neural networks</title>
		<author>
			<persName><forename type="first">James</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kieran</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiago</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agnieszka</forename><surname>Grabska-Barwinska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the national academy of sciences</title>
				<meeting>the national academy of sciences</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page" from="3521" to="3526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On dropout and nuclear norm regularization</title>
		<author>
			<persName><forename type="first">Poorya</forename><surname>Mianjy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raman</forename><surname>Arora</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v97/mianjy19a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
				<editor>
			<persName><forename type="first">Kamalika</forename><surname>Chaudhuri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</editor>
		<meeting>the 36th International Conference on Machine Learning<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-15">09-15 Jun 2019</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="4575" to="4584" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th international conference on machine learning (ICML-10)</title>
				<meeting>the 27th international conference on machine learning (ICML-10)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibault</forename><surname>Févry</surname></persName>
		</author>
		<author>
			<persName><surname>Samuel R Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.01088</idno>
		<title level="m">Sentence encoders on stilts: Supplementary training on intermediate labeled-data tasks</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Progress &amp; compress: A scalable framework for continual learning</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jelena</forename><surname>Luketina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><forename type="middle">M</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agnieszka</forename><surname>Grabska-Barwinska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.06370</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
				<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Regularization of neural networks using dropconnect</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sixin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Le Cun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1058" to="1066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><surname>Samuel R Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07461</idno>
		<title level="m">Glue: A multi-task benchmark and analysis platform for natural language understanding</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fast dropout training</title>
		<author>
			<persName><forename type="first">Sida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">international conference on machine learning</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="118" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Alex</forename><surname>Warstadt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<idno>1805.12471</idno>
		<title level="m">Neural network acceptability judgments</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Neural domain adaptation for biomedical question answering</title>
		<author>
			<persName><forename type="first">Georg</forename><surname>Wiese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariana</forename><surname>Neves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03610</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Xlnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08237</idno>
		<title level="m">Generalized autoregressive pretraining for language understanding</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
