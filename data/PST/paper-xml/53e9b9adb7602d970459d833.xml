<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Estimating Gaze Direction from Low-Resolution Faces in Video</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Neil</forename><surname>Robertson</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">QinetiQ</orgName>
								<address>
									<addrLine>St Andrews Road</addrLine>
									<postCode>WR14 3PS</postCode>
									<settlement>Malvern</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Dept. Engineering Science</orgName>
								<orgName type="institution">Oxford University</orgName>
								<address>
									<addrLine>Parks Road</addrLine>
									<postCode>OX1 3PJ</postCode>
									<settlement>Oxford</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ian</forename><surname>Reid</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Dept. Engineering Science</orgName>
								<orgName type="institution">Oxford University</orgName>
								<address>
									<addrLine>Parks Road</addrLine>
									<postCode>OX1 3PJ</postCode>
									<settlement>Oxford</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Estimating Gaze Direction from Low-Resolution Faces in Video</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">EE1ED98DFC92EC42D0BA95A12CF59AC3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T03:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we describe a new method for automatically estimating where a person is looking in images where the head is typically in the range 20 to 40 pixels high. We use a feature vector based on skin detection to estimate the orientation of the head, which is discretised into 8 different orientations, relative to the camera. A fast sampling method returns a distribution over previously-seen head-poses. The overall body pose relative to the camera frame is approximated using the velocity of the body, obtained via automatically-initiated colour-based tracking in the image sequence. We show that, by combining direction and head-pose information gaze is determined more robustly than using each feature alone. We demonstrate this technique on surveillance and sports footage.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In applications where human activity is under observation, be that CCTV surveillance or sports footage, knowledge about where a person is looking (i.e. their gaze) provides observers with important clues which enable accurate explanation of the scene activity. It is possible, for example, for a human readily to distinguish between two people walking side-by-side but who are not "together" and those who are acting as a pair. Such a distinction is possible when there is regular eye-contact or head-turning in the direction of the other person. In soccer head position is a guide to where the ball will be passed next i.e. an indicator of intention, which is essential for causal reasoning. In this paper we present a new method for automatically inferring gaze direction in images where any one person represents only a small proportion (the head ranges from 20 to 40 pixels high) of the frame.</p><p>The first component of our system is a descriptor based on skin colour. This descriptor is extracted for each head in a large training database and labelled with one of 8 distinct head poses. This labelled database can be queried to find either a nearest-neighbour match for a previously unseen descriptor or (as we discuss later) is non-parametrically sampled to provide an approximation to a distribution over possible head poses.</p><p>Recognising that general body direction plays an important rôle in determining where a person can look (due to anatomical limitations), we combine direction and head pose using Bayes' rule to obtain the joint distribution over head pose and direction, resulting in 64 possible gazes (since head pose and direction are discretised into 8 sectors each, shown in figure <ref type="figure" target="#fig_0">1</ref>). The paper is organised as follows. Firstly we highlight relevant work in this, and associated, area(s). We then describe how head-pose is estimated in section 2. In section 3 we provide motivation for a Bayesian fusion method by showing intermediate results where the best head-pose match is chosen and, by contrast, where overall body-direction alone is used. Section 3 also discusses how we fuse the relevant information we have at our disposal robustly to compute a distribution over possible gazes, rejecting non-physical gazes and reliably detecting potentially significant interactions between people. Throughout the paper we test and evaluate on a number of datasets and additionally summarise comprehensive results in section 4. We conclude in section 5 and discuss potential future work in section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Previous Work</head><p>Determining gaze in surveillance images is a challenging problem that has received little or no attention to date, though preliminary work in this specific problem domain was reported earlier by the authors <ref type="bibr" target="#b21">[22]</ref>.</p><p>Most closely related to our work is that of Efros et al. <ref type="bibr" target="#b5">[6]</ref> for recognition of human action at a distance. That work showed how to distinguish between human activities such as walking, running etc. by comparing gross properties of motion using a descriptor derived from frame-to-frame optic-flow and performing an exhaustive search over extensive exemplar data. Head pose is not discussed in <ref type="bibr" target="#b5">[6]</ref> but the use of a simple descriptor invariant to lighting and clothing is of direct relevance to head pose estimation and has directly inspired aspects of our approach.</p><p>Dee and Hogg <ref type="bibr" target="#b4">[5]</ref> developed a system for detecting unusual activity which involves inferring which regions of the scene are visible to an agent within the scene. A Markov Chain with penalties associated with state transitions is used to return a score for observed trajectories which essentially encodes how directly a person made his/her way towards predefined goals, typically scene exits. In their work, gaze inference is vital, but is inferred from trajectory information alone which can lead to significant interactions being overlooked. In fact, many systems have been created to aid urban surveillance, most based on the notion of trajectories alone. For example <ref type="bibr" target="#b8">[9]</ref> reports an entirely automated system for visual surveillance and monitoring of an urban site using agent trajectories. The same is true in the work of Buxton (who has been prominent in the use of Bayesian networks for visual surveillance) <ref type="bibr" target="#b1">[2]</ref>, Morellas et al. <ref type="bibr" target="#b16">[17]</ref> and Makris <ref type="bibr" target="#b13">[14]</ref>. Johnson and Hogg's work <ref type="bibr" target="#b11">[12]</ref> is another example where trajectory information only is considered.</p><p>In contrast, there has been considerable effort to extract gaze from relatively high-resolution faces, motivated by the press for better Human/Computer Interfaces. The technical aspects of this work have often focused on detecting the eyeball primarily. Matsumoto <ref type="bibr" target="#b14">[15]</ref> computes 3-D head pose from 2-D features and stereo tracking. Perez et al. <ref type="bibr" target="#b19">[20]</ref> focus exclusively on the tracking of the eyeball and determination of its observed radius and orientation for gaze recognition. Gee and Cipolla's <ref type="bibr" target="#b7">[8]</ref> gaze determination method based on the 3D geometric relationship between facial features was applied to paintings to determine where the subject is looking. Related work has tackled expression recognition using information measures. Shinohara and Otsu demonstrated that Fisher Weights can be used to recognise "smiling" in images.</p><p>While this approach is most useful in HCI where the head dominates the image and the eye orientation is the only cue to intention, it is too fine-grained for surveillance video where we must usually be content to assume that the gaze direction is aligned with the head-pose. In typical images of interest in our application area (low/medium resolution), locating significant features such as the eyes, irises, corners of the mouth, etc as used in much of the work above is regularly an impossible task. Furthermore, though standard head/face-detection techniques <ref type="bibr" target="#b23">[24]</ref> work well in medium reolution images, they are much less reliable for detecting, say, the back of a head, which still conveys significant gaze information.</p><p>The lowest level of our approach is based on skin detection. Because of significant interest in detecting and tracking people in images and video, skin detection has naturally received much attention in the Computer Vision community <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>. Skin detection alone, though, is error-prone when the skin region is very small as a proportion of the image. However, contextual cues such as body-direction can help to disambiguate gaze using even a very coarse headpose estimation. By combining this information in a principled (i.e. probabilistic, Bayesian) fashion, gaze estimation at a distance becomes a distinct possibility as we demonstrate in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Head Pose Detection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Head Pose Feature Vector</head><p>Although people differ in colour and length of hair, and some people may be wearing hats, beards etc. it is reasonable to assume that the amount of skin that can be seen, the position of the skin pixels within the frame and the proportion of skin to non-skin pixels is a relatively invariant cue for a person's coarse gaze in a static image. We obtain this descriptor in a robust and automatic fashion as follows. First, a mean-shift tracker <ref type="bibr" target="#b3">[4]</ref> is automatically initialised on the head by using naive background subtraction to locate people and subsequently modelling the person as distinct "blocks", the head and torso. Second, we centre the head within the tracker window at each time step which stabilises the descriptor ensuring consistent position within the frame for similar descriptors (the head images are scaled to the same size and, since the mean-shift tracker tracks in scale-space we have a stable, invariant, descriptor). Third, despite claims in the literature to the contrary, there is no specific region of colour-space which represents skin in all sequences and therefore it is necessary to define a skin histogram for each scenario by hand-selecting a region of one frame in the current sequence to compute a (normalised) skin-colour histogram in RGB-space. We then compute the weights for every pixel in the stabilised head images which the tracker automatically produces to indicate how likely it is that it was drawn from this predefined skin histogram <ref type="foot" target="#foot_0">1</ref> . Using the knowledge of the background we segment the foreground out of the tracked images. Every pixel in the segmented head image is drawn from a specific RGB bin and so is assigned the relevant weight which can be interpreted as a probability that the pixel is drawn from the skin model histograms. So for every bin i (typically we use 10 bins) in the predefined, hand-selected skin-colour histogram q the histogram of the tracked image p is a weight is computed w i = qi pi . Every foreground pixel in the tracked frame falls into one of the bins according to its RGB value and the normalised weight associated with that pixel is assigned to compute the overall weight image, as shown in figure <ref type="figure" target="#fig_0">1</ref>. The non-skin pixels are assigned a weigh that the pixel is not drawn from the skin histogram. This non-skin descriptor is necessary because it encodes the "proportion" of the head which is skin which is essential as people vary in size not only in the sense of scale within the but physically between one another. Each descriptor is scaled to a standard 20 × 20 pixel window to achieve robust comparison when the head sizes vary. Finally, in order to provide temporal context to our descriptor of head-pose we concatenate individual descriptors from 5 consecutive frames of tracker data for a particular example and this defines our instantaneous descriptor of head-pose. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Training Data</head><p>We assume that we can distinguish head pose to a resolution of 45 o . There is no obvious benefit to detecting head orientations at a higher degree of accuracy and it is unlikely that the coarse target images would be amenable in any case. This means discretising the 360 o orientation-space into 8 distinct views as shown in figure <ref type="figure" target="#fig_0">1</ref>. The training data we select is from a surveillance-style camera position and around 100 examples of each view are selected from across a number of  Here, the ML match for head pose would be incorrectly chosen as "back". The body-direction is identified as "S" which, since it is not possible to turn the head through 180 o relative to the body, this gaze has a low (predefined) prior and is rejected as the most likely at the fusion stage. The MAP gaze is identified as "Face" which is a very good approximation to the true gaze.</p><p>different sequences and under different lighting conditions (i.e. light from left, right and above). The head was automatically tracked as described above and the example sequence labelled accordingly. The weight image for 5 consecutive frames are then computed and this feature vector stored in our exemplar set. The same example set is used in all the experiments reported (e.g. there are no footballers in the training dataset used to compute the gaze estimates presented in figure <ref type="figure">9</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Matching Head Poses</head><p>The descriptors for each head pose are (20×20×5 =)2000 element vectors. With 8 possible orientations and 100 examples of each orientation, rapidly searching this dataset becomes an issue. We elect to structure the database using a binarytree in which each node in the tree divides the set of exemplars below the node into roughly equal halves. Such a structure can be searched in roughly log n time to give an approximate nearest-neighbour result. We do this for two reasons: first, even for a modest database of 800 examples such as ours it is faster by a factor of 10; second, we wish to frame the problem of gaze detection in a probabilistic way and Sidenbladh <ref type="bibr" target="#b22">[23]</ref> showed how to formulate a binary tree (based on the sign of the Principal Components of the data) search in a pseudo-probabilistic manner. This technique was later applied to probabilistic analysis of human activity in <ref type="bibr" target="#b20">[21]</ref>. We achieve recognition rates of 80% (the correct example is chosen as the ML model 8/10 queries) using this pseudo-probabilistic method based on Principal Components with 10 samples. An illustrative example of such a distribution in this context is shown in figure <ref type="figure" target="#fig_3">4</ref>. Results of sampling from this database for a number of different scenes are shown in figure <ref type="figure" target="#fig_2">3</ref>. In order to display where the person is looking in the images angles are assigned to the discretised head-poses shown in figure <ref type="figure" target="#fig_0">1</ref> according to the "compass" e.g. N : 0 o etc. The angles are then corrected for the projection of the camera at each time step (depending on the location of the person on the ground-plane in the image) as defined in figure <ref type="figure">5</ref>. When assigning angles to the matched discretised head-poses one must compensate for the camera projection since "North" (see figure <ref type="figure" target="#fig_0">1</ref>) does not in general correspond to vertical in the image plane. In order to choose the correct frame of reference we do not perform full camera calibration but compute the projective transform (H : image→ground-plane) by hand-selecting 4 points in the image. The vertical vanishing point (v, left) is computed from 2 lines normal to the ground plane and parallel in the image. The angle theta between the projection of the optic-rays through the camera centre (Hv, right) and the image centre (Hc, left) and the point at the feet of the tracked person ((Hp, right) is the angle which adjusts vertical in the image to "North" in our ground plane reference frame i. e. cos -1 [(Hc × Hv).(Hv × Hp)].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Gaze Estimation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Bayesian Fusion of Head-Pose and Direction</head><p>The naive assumption that direction of motion information is a good guide as to what a person can see has been used in figure <ref type="figure">6</ref>. However, it is clear the crucial interaction between the two people is missed. To address this issue we compute the joint posterior distribution over direction of motion and head pose. The priors on these are initially uniform for direction of motion, reflecting the fact that for these purposes there is no preference for any particular direction in the scene, and for head pose a centred, weighted function that models a strong preference for looking forwards rather than sideways. The prior on gaze is defined using a table which lists expected (i.e. physically possible) gazes and unexpected (i.e. non-physical) gazes.</p><p>We define g as the measurement of head-pose, d is the measurement of body motion direction, G is the true gaze direction and B is the true body direction, with all quantities referred to the ground centre. We compute the joint probability of true body pose and true gaze:</p><formula xml:id="formula_0">P (B, G|d, g) ∝ P (d, g|B, G)P (B, G)<label>( 1 )</label></formula><p>Now given that the measurement of direction d is independent both true and measured gaze G, g once true body B pose is known, P (d|B, G, g) = P (d|B) and similarly that the measurement of gaze g is independent of true body pose B given true gaze G, P (g|B, G) = p(g|G), then we have</p><formula xml:id="formula_1">P (B, G|d, g) ∝ P (g|G)P (d|B)P (G|B)P (B)<label>( 2 )</label></formula><p>We assume that the measurement errors in gaze and direction are unbiased and normally distributed around the respective true values</p><formula xml:id="formula_2">P (g|G) = N (G, σ 2 G ), P (d|B) = N (B, σ 2 B )<label>( 3 )</label></formula><p>(actually, since these are discrete variables we use a discrete approximation).</p><p>The joint prior, P (B, G) is factored as above into P (G|B)P (B) where the first term encodes our knowledge that people tend to look straight ahead (so the distribution P (G|B) is peaked around B, while P (B) is taken to be uniform, encoding our belief that all directions of body pose are equally likely, although this is easily changed: for example in tennis one player is expected to be predominantly facing the camera).</p><p>While for single frame estimation this formulation fuses our measurements with prior beliefs, when analysing video data we can further impose smoothness constraints to encode temporal coherence: the joint prior at time t is in this case taken to be</p><formula xml:id="formula_3">P (G t , B t |G t-1 , B t-1 ) = P (G t |B t , B t-1 , G t-1 )P (B t |B t1 )</formula><p>where we have used an assumption that the current direction is independent of previous gaze<ref type="foot" target="#foot_1">2</ref> , and current gaze depends only on current pose and previous gaze. The former term, P (G t |B t , B t-1 , G t-1 ), strikes a balance between between our belief that people tend to look where they are going, and temporal consistency of gaze via a mixture</p><formula xml:id="formula_4">G t ∼ αN (G t-1 , σ 2 G ) + (1 -α)N (B t , σ 2 B ).</formula><p>Now we compute the joint distribution for all 64 possible gazes resulting from possible combinations of 8 head poses and 8 directions. This posterior distribution allows us to maintain probabilistic estimates without committing to a defined gaze which will be advantageous for further reasoning about overall scene behaviour. Immediately though we can see that gazes which we consider very unlikely given our prior knowledge of human biomechanics (since the head cannot turn beyond 90 o relative to the torso <ref type="bibr" target="#b18">[19]</ref>) can be rejected in addition to the obvious benefit that the quality of lower-level match can be incorporated in a mathematically sound way. An illustrative example is shown in figure <ref type="figure" target="#fig_3">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>We have tested this method on various datasets (see figures 6, 7, 8, 9 and 10). The first dataset provided us with the exemplar data for use on all the test videos shown in this paper. In the first example in figure <ref type="figure">6</ref> we show significant improvement over using head-pose or direction alone to compute gaze. The</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Body-direction only</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Body-direction and Head-pose combined</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Head-pose only</head><p>Angle error Head angles Fig. <ref type="figure">6</ref>. In this video there is an interaction between the two people, and the fact they look at each other is the prime indicator that they are "together". On the first row we estimate gaze from body direction alone, on the second row using head-pose alone, which is improved but prone to some errors. We see that (third row ) fusing the headpose and body-direction estimates gives the correct result.   <ref type="figure" target="#fig_0">10</ref>. This figure shows the method tested on a standard sequence (see http:// groups.inf.ed.ac.uk/vision/CAVIAR/). The errors are exacerbated by our discretisation of gaze (accurate to 45 o ) compared to the non-discretised ground truth (computed to 10 o from a hand-drawn estimate of line-of-sight which we take to be the best-estimate a human can make from low-resolution images) and tend to be isolated (the median error is 5.5 o ). In most circumstances it is more important that the significant head-turnings are identified, which they are here, as evidenced by the expanded frames.</p><p>crucial interaction which conveys the information that the people in the scene are together is the frequent turning of the head to look at each other. We reliably detect this interaction as can be seen from the images and the estimated head angle relative to vertical. The second example is similar but in completely different scene. The skin histogram for online skin-detection in the input images is recomputed for this video. The exemplar (training) database remains the same, however. Once more the interaction implied by the head turning to look at his companions is determined. We demonstrate the method on sports video in figure <ref type="figure">9</ref> and on a standard vision sequence in figure <ref type="figure" target="#fig_0">10</ref>. It is shown in figure <ref type="figure">7</ref> how useful this technique can be in a causal-reasoning context where we identify two people looking at one another prior to meeting. Finally we discuss the failure mode in figure <ref type="figure" target="#fig_7">11</ref> which is found to be where the size of the head falls below 20 pixels and the gaze becomes ambiguous due to the small number of skin pixels. We show an example here where our method can fail. The mean body direction of the player (in the frames prior to the frame for which we estimate the gaze) is East, since he is moving backwards as his head rotates. The ML match is clearly not correct because the neck has been detected and there is no representation of gaze where the neck is visible in the training dataset. Fusing the direction and head-pose estimate results in the MAP gaze "side-LR", as expected, but incorrect. The reasons for failure are clear: body direction is not a good guide to gaze in this case and there is an unusual input which results in an incorrect match. Either of these can be compensated for on their own with the Bayesian representation we devised but a scenario which combines both is likely to fail. Additional contextual information (e.g. silhouette) could improve this result, however.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper we have demonstrated that a simple descriptor, readily computed from medium-scale video, can be used robustly to estimate head pose. In order to speed up non-parametric matching into an exemplar database and to maintain probabilistic estimates throughout we employed a fast pseudo-probabilistic binary search based on Principal Components. To resolve ambiguity, improve matching and reject known implausible gaze estimates we used a simple application of Bayes' Rule to fuse priors on direction-of-motion and head-pose, evidence from our exemplar-matching algorithm and priors on gaze (which we specified in advance). We demonstrated on a number of different datasets that this gives acceptable gaze estimation for people being tracked at a distance.</p><p>The Bayesian fusion method we have used in this work could be readily extended to include other contextual data. We used body direction in this paper but information such as the silhouette is equally interesting. Moreover the descriptor for head-pose could be extended to include information from multiple cameras. The work reported here would be most useful in a causal reasoning context where knowledge of where a person is looking can help solve interesting questions such as, "Is person A following person B?" or determine that person C looked right because a moving object entered his field-of-view. We are currently combining this advance with our reported work on human behaviour recognition <ref type="bibr" target="#b20">[21]</ref> to aid automatic reasoning in video.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The figure on the left shows the images which result from the mean-shift image patch tracker (col. 1 ) (with an additional step stabilise the descriptor by centering the head in the window), subsequent background subtraction (col. 2 ), the weight image which represents the probability that each pixel in the head is skin (col. 3 ) and nonskin (col. 4 ) (non-skin is significant as it captures proportion without the need for scaling). Thie concatenation of skin and non-skin weight vectors is our feature vector which we use to determine eight distinct head poses which are shown and labelled on the right. Varying lighting conditions are accounted for by representing the same head-pose under light from different directions in the training set. The same points on the "compass" are used as our discretisation of direction i.e. N, NE, E, etc.</figDesc><graphic coords="2,65.72,53.67,297.60,150.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Automatic location of the head is achieved by segmenting the target using simple background subtraction (top-left) and morphological operations with a kernel biased towards the scale of the target to identify objects. The head is taken as the top 1/7th of the entire body (top-right). The head is automatically centred in the bounding box at each time step to stabilise the tracking and provide an invariant descriptor for head pose, as shown in the second row.</figDesc><graphic coords="5,110.96,50.83,207.50,107.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Detecting head pose in different scenes using the same exemplar set. The main image shows the frame with the estimated gaze angle superimposed, the pair of images directly beside each frame shows the input image that the head-pose detector uses (top) and the best (ML) match in the database with corresponding label (bottom).</figDesc><graphic coords="5,78.68,458.99,104.11,82.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. (Top) The distribution over head-poses resulting from 10 queries of the database for this input frame is shown in the graph above. The leaf nodes of the database contain indices into matching frames and the matching frame images and assigned probabilities of a match are shown below the graph. (Bottom) Fusing head-pose and direction estimates improves gaze estimation.Here, the ML match for head pose would be incorrectly chosen as "back". The body-direction is identified as "S" which, since it is not possible to turn the head through 180 o relative to the body, this gaze has a low (predefined) prior and is rejected as the most likely at the fusion stage. The MAP gaze is identified as "Face" which is a very good approximation to the true gaze.</figDesc><graphic coords="6,77.24,53.79,275.04,247.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Fig.5. When assigning angles to the matched discretised head-poses one must compensate for the camera projection since "North" (see figure1) does not in general correspond to vertical in the image plane. In order to choose the correct frame of reference we do not perform full camera calibration but compute the projective transform (H : image→ground-plane) by hand-selecting 4 points in the image. The vertical vanishing point (v, left) is computed from 2 lines normal to the ground plane and parallel in the image. The angle theta between the projection of the optic-rays through the camera centre (Hv, right) and the image centre (Hc, left) and the point at the feet of the tracked person ((Hp, right) is the angle which adjusts vertical in the image to "North" in our ground plane reference frame i. e. cos -1 [(Hc × Hv).(Hv × Hp)].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .Fig. 8 .Fig. 9 .</head><label>789</label><figDesc>Fig. 7. Two people meeting could potentially be identified by each person being in the other's gaze (in addition to other cues such as proximity), as we show in this example</figDesc><graphic coords="10,41.84,53.69,55.69,55.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Fig.10. This figure shows the method tested on a standard sequence (see http:// groups.inf.ed.ac.uk/vision/CAVIAR/). The errors are exacerbated by our discretisation of gaze (accurate to 45 o ) compared to the non-discretised ground truth (computed to 10 o from a hand-drawn estimate of line-of-sight which we take to be the best-estimate a human can make from low-resolution images) and tend to be isolated (the median error is 5.5 o ). In most circumstances it is more important that the significant head-turnings are identified, which they are here, as evidenced by the expanded frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11.We show an example here where our method can fail. The mean body direction of the player (in the frames prior to the frame for which we estimate the gaze) is East, since he is moving backwards as his head rotates. The ML match is clearly not correct because the neck has been detected and there is no representation of gaze where the neck is visible in the training dataset. Fusing the direction and head-pose estimate results in the MAP gaze "side-LR", as expected, but incorrect. The reasons for failure are clear: body direction is not a good guide to gaze in this case and there is an unusual input which results in an incorrect match. Either of these can be compensated for on their own with the Bayesian representation we devised but a scenario which combines both is likely to fail. Additional contextual information (e.g. silhouette) could improve this result, however.</figDesc><graphic coords="12,59.12,50.88,311.20,189.51" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>This will be recognised as a similar approximation to the Battacharyya coefficient as implemented in the meanshift algorithm<ref type="bibr" target="#b3">[4]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Although we do recognise that, in a limited set of cases, this may in fact be a poor assumption since people may change their motion or pose in response to observing something interesting while gazing around.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Lowe Shape indexing using approximate nearest-neighbour search in high-dimensional space IEEE Conf</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Beis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>San Juan, PR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997-06">June 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Buxton</surname></persName>
		</author>
		<title level="m">Learning and Understanding Dynamic Scene Activity ECCV Generative Model Based Vision Workshop</title>
		<meeting><address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Ngan Locating facial region of a head-and-shoulders color image</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Third IEEE International Conference on Automatic Face and Gesture Recognitions</title>
		<meeting><address><addrLine>Nara, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-04">April 1998</date>
			<biblScope unit="page" from="124" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
		<title level="m">Mean Shift Analysis and Applications Proceedings of the International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="1999">September 20-25, 1999</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">1197</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Dee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hogg</surname></persName>
		</author>
		<title level="m">Detecting Inexplicable Behaviour Proceedings of the British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<title level="m">Distance Proceedings of the International Conference on Computer Vision</title>
		<meeting><address><addrLine>Nice, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-07">July 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Galata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hogg</surname></persName>
		</author>
		<title level="m">Learning Behaviour Models of Human Activities British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Determining the gaze of faces in images</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Gee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="639" to="647" />
			<date type="published" when="1994-12">December 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E L</forename><surname>Grimson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stauffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
		<title level="m">Using Adaptive Tracking to Classify and Monitor Activities in a Site Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Santa Barbara, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">June 23-25, 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Hidai</surname></persName>
		</author>
		<title level="m">Robust Face Detection against Brightness Fluctuation and Size Variation International Conference on Intelligent Robots and Systems</title>
		<meeting><address><addrLine>Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-10">October 2000</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1379" to="1384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Parametrized Structure from Motion for 3D Adaptive Feedback Tracking of Faces Proc</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Jebara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="144" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning the Distribution of Object Trajectories for Event Recognition Proc</title>
		<author>
			<persName><forename type="first">N</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hogg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="1995-09">September 1995</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="583" to="592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">An Iterative Image Registration Technique with Application to Stereo Vision DARPA Image Understanding Workshop</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Makris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ellis</surname></persName>
		</author>
		<title level="m">Spatial and Probabilistic Modelling of Pedestrian Behaviour British Machine Vision Conference</title>
		<meeting><address><addrLine>Cardiff, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-09-02">2002. September 2-5, 2002</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="557" to="566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An Algorithm for Real-time Stereo Vision Implementation of Head Pose and Gaze Direction Measurement Proceedings</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Matsumoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zelinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Fourth International Conference on Face and Gesture Recognition</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="499" to="505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">A Fast Nearest-Neighbor Algorithm Based on a Principal Axis Search Tree IEEE Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mcnames</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001-09">September 2001</date>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="964" to="976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Tsiamyrtzis DETER: Detection of Events for Threat Evaluation and Recognition Machine Vision and Applications</title>
		<author>
			<persName><forename type="first">V</forename><surname>Morellas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Pavlidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2003-10">October 2003</date>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="29" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Nene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
		<title level="m">Simple Algorithm for Nearest Neighbor Search in High Dimensions IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="1997-09">September 1997</date>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="989" to="1003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Atlantoaxial Rotatory Fixation: Part 1-Biomechanics OF Normal Rotation at the Atlantoaxial Joint in Children</title>
		<author>
			<persName><forename type="first">D</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurosurgery</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="614" to="626" />
			<date type="published" when="2004-09">September 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Cordoba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mendez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Munoz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Pedraza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
		<title level="m">Precise Eye-Gaze Detection and Tracking System Proceedings of the 11th International Conference in Central Europe on Computer Graphics, Visualization and Computer Vision</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">D</forename></persName>
		</author>
		<title level="m">Reid Behaviour understanding in video: a combined method Proceedings of the International Conference on Computer Vision</title>
		<meeting><address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-10">October 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">What are you looking at? Gaze recognition in medium-scale images Human Activity Modelling and Recognition</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Brady</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<meeting><address><addrLine>Oxford, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-09">September 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Sidenbladh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<title level="m">Implicit Probabilistic Models of Human Motion for Synthesis and Tracking European Conference on Computer Vision</title>
		<meeting><address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-06">June 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Jones Robust Real-Time Face Detection International Journal of Computer Vision</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="137" to="154" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
