<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">More Than Just a Pretty Face: Affordances of Embodiment</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">J</forename><surname>Cassell</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Gesture and Narrative Language Group MIT Media Laboratory</orgName>
								<address>
									<addrLine>E15-315 20 Ames St</addrLine>
									<postCode>+1 617, 253 4899</postCode>
									<settlement>Cambridge</settlement>
									<region>Massachusetts</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">T</forename><surname>Bickmore</surname></persName>
							<email>bickmore@media.mit.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Gesture and Narrative Language Group MIT Media Laboratory</orgName>
								<address>
									<addrLine>E15-315 20 Ames St</addrLine>
									<postCode>+1 617, 253 4899</postCode>
									<settlement>Cambridge</settlement>
									<region>Massachusetts</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">H</forename><surname>Vilhj√°lmsson</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Gesture and Narrative Language Group MIT Media Laboratory</orgName>
								<address>
									<addrLine>E15-315 20 Ames St</addrLine>
									<postCode>+1 617, 253 4899</postCode>
									<settlement>Cambridge</settlement>
									<region>Massachusetts</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">H</forename><surname>Yan</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Gesture and Narrative Language Group MIT Media Laboratory</orgName>
								<address>
									<addrLine>E15-315 20 Ames St</addrLine>
									<postCode>+1 617, 253 4899</postCode>
									<settlement>Cambridge</settlement>
									<region>Massachusetts</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">More Than Just a Pretty Face: Affordances of Embodiment</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9E21CF63A0B4EFB4EAC887A8382A5E29</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Prior research into embodied interface agents has found that users like them and find them engaging. In this paper, we argue that embodiment can serve an even stronger function if system designers use actual human conversational protocols in the design of the interface. Communicative behaviors such as salutations and farewells, conversational turn-taking with interruptions, and referring to objects using pointing gestures are examples of protocols that all native speakers of a language already know how to perform and that can thus be leveraged in an intelligent interface. We discuss how these protocols are integrated into Rea, an embodied, multi-modal conversational interface agent who acts as a real-estate salesperson, and we show why embodiment is required for their successful implementation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INTRODUCTION</head><p>There is a qualitative difference between face-to-face conversation and other forms of human-human communication <ref type="bibr" target="#b3">[4]</ref>. Businesspeople and academics routinely travel long distances to conduct certain face-toface interactions when electronic forms of communication would seemingly work just as well. When people have something really important to say, they say it in person.</p><p>The qualitative difference in these situations is not just that we enjoy looking at humans more than at computer screens but also that the human body enables the use of certain communication protocols in face-to-face conversation which provide for a more rich and robust channel of communication than is afforded by any other medium available today. The use of gaze, gesture, intonation, and body posture play an essential role in the proper execution of many conversational functions-such as conversation initiation and termination, turn-taking, interruption handling, feedback and error correction-and these kinds of behaviors enable the exchange of multiple levels of information in real time. People are extremely adept at extracting meaning from subtle variations in the performance of these behaviors; for example slight variations in pause length, feedback nod timing or gaze behavior can significantly alter the interpretation of an utterance (consider "you did a great job" vs. "you did a . . . great job").</p><p>Of particular interest to interface designers is that these communication protocols come for "free" in that users do not need to be trained in their use; all native speakers of a given language have these skills and use them daily. An embodied interface agent which exploits these protocols has the potential to provide a higher bandwidth of communication than would otherwise be possible.</p><p>Of course, depictions of human bodies are also more decorative than menus on a screen and, like any new interface design, they are also currently quite in vogue and therefore attractive to many users. Unfortunately, many embodied interface agents developed to date don't go further than their ornamental or novelty value. Aside from the use of pointing gestures and two or three facial expressions, many animated interface agents provide little more than something amusing to look at while the same old system handles the mechanics of the interaction. It is little wonder that these systems have been found to be likable and engaging, but to provide no improvement in task performance over text or speech-only interfaces.</p><p>In this paper, we first review the embodied interface agents developed to date and summarize the results of evaluations performed on them. We then discuss several human communication protocols along with their interface utility and requirements for embodiment. Finally, we present Rea, an embodied interface agent which implements these protocols and describe our ongoing research program to develop embodied interface agents that leverage knowledge of human communication skills.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RELATED WORK</head><p>Other researchers have built embodied interface agents, with varying degrees of conversational ability. The closest to our own research in this area is the work of Rickel and Johnson <ref type="bibr" target="#b19">[20]</ref>, Andre and Rist <ref type="bibr" target="#b0">[1]</ref>, and Lester et al. <ref type="bibr" target="#b16">[17]</ref> whose agents do use both verbal and nonverbal conversational behaviors, move to objects in the interface and use pointing gestures in combination with speech or text output. In these systems, however, the association between verbal and nonverbal behaviors is always additive -the affordances of the body are not exploited for the kinds of tasks that it performs better than speech.</p><p>"Animated Conversation" <ref type="bibr" target="#b7">[8]</ref> was a system that automatically generated context-appropriate gestures, facial movements and intonational patterns. In this case the domain was conversation between two artificial agents and the emphasis was on the production of non-verbal propositional behaviors that emphasized and reinforced the content of speech. However, the system was not designed to interact with a user, and did not run in real time.</p><p>The work of Thorisson provides a good first example of how an embodied interface agent inspired by studies of human psychosocial competencies might be developed <ref type="bibr" target="#b21">[22]</ref>. The agent, Gandalf, recognized and displayed interactional information such as gaze, simple gesture and canned speech events. In this way he was able to perceive and generate turn-taking and back channel behaviors that lead to a very natural conversational interaction. However, Gandalf had limited ability to recognize and generate propositional information, and was also limited in his ability to provide correct intonation for speech emphasis on speech output, or co-occurring gestures with speech. The conversational character system developed by Prevost, et al <ref type="bibr" target="#b18">[19]</ref>, uses the same architecture as the one presented in this paper (it was co-developed by our two research groups), but their application domain and many implementation details are different. In their system a conversational character assists a user with a complex A/V system by controlling equipment, answering questions or giving tutorials. To date, the conversational behaviors of their agent is limited to greeting and farewell rituals, gaze, pointing gestures and body positioning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>User Studies on Embodied Interface Agents</head><p>Koda and Maes <ref type="bibr" target="#b15">[16]</ref>, and Takeuchi and Naito <ref type="bibr" target="#b22">[23]</ref>, studied user responses to interfaces with static or animated faces, and found that users rated them to be more engaging and entertaining than functionally equivalent interfaces without a face. Kiesler and Sproull found that users were more likely to be cooperative with an interface agent when it had a human face (vs. a dog image or cartoon) <ref type="bibr" target="#b13">[14]</ref>.</p><p>Andre, Rist and Muller found that users rated their animated presentation agent ("PPP Persona") as more entertaining and helpful than an equivalent interface without the agent <ref type="bibr" target="#b0">[1]</ref>. However, there was no difference in actual performance (comprehension and recall of presented material) in interfaces with the agent vs. interfaces without it.</p><p>In a user study of the Gandalf system mentioned above <ref type="bibr" target="#b8">[9]</ref>, users rated the smoothness of the interaction and the agent's language skills significantly higher under test conditions in which Gandalf utilized limited conversational behavior (gaze, turn-taking and limited gesture) than when these behaviors were disabled.</p><p>Most of these evaluations have tried to address whether embodiment of a system is useful at all, usually by keeping the interaction the same, and then including or not including an animated figure. The studies, then, are not testing how particular uses of embodiment may improve task or learning performance. Therefore, although the previous studies inspire us by showing that the mere presence of a character wins us points, we now need to focus on the contribution of embodiment in fully functional conversational interfaces, and in order to do that, we need to start with a better understanding of what embodiment contributes to human-human interaction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HUMAN COMMUNICATION PROTOCOLS REQUIRING EMBODIMENT</head><p>Embodiment provides us with a wide range of behaviors that, when executed in tight synchronization with language, carry out a communicative function. It is important to understand that particular behaviors, such as the raising of the eyebrows, can be employed in a variety of circumstances to produce different communicative effects, and that the same communicative function may be realized through different sets of behaviors. It is therefore clear that any system dealing with conversational modeling has to handle function separately from surfaceform or run the risk of being inflexible and insensitive to the natural phases of the conversation. Here we briefly describe some of the fundamental communication protocols and their functional elements along with examples of nonverbal behavior that contribute to their successful implementation. Table <ref type="table">1</ref> shows examples of mappings from communicative function to particular behaviors and is based on previous research on typical North American nonverbal displays, mainly <ref type="bibr" target="#b10">[11]</ref> and <ref type="bibr" target="#b14">[15]</ref>. This mapping from form to function relies on a fundamental division of conversational goals: contributions to a conversation can be propositional and interactional. Propositional information corresponds to the content of the conversation. This includes meaningful speech as well as hand gestures used to complement or elaborate upon the speech content (gestures that indicate the size in the sentence "it was this big"). Interactional information consists of the cues that regulate conversational process and includes a range of nonverbal behaviors (quick head nods to indicate that one is following) as well as regulatory speech ("huh?", "Uhhuh"). This theoretical stance allows us to examine the role of embodiment not just in task-but also processrelated behaviors. From this standpoint, we note that most previous embodied interface agents do not deal with interactional and propositional information in an integrated manner, which prevents them from fully exploiting the affordances of the body.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conversation Initiation and Termination</head><p>Humans partake in an elaborate ritual when engaging and disengaging in conversation <ref type="bibr" target="#b17">[18]</ref>. For example, people will show their readiness to engage in a conversation by turning towards their potential interlocutor, gazing at the person and then exchanging signs of mutual recognition typically involving a smile, eyebrow movement and tossing the head or waving of the arm. Following this initial synchronization stage, or distance salutation, the two people approach one other, sealing their commitment to the conversation through a close salutation such as a handshake accompanied by a ritualistic verbal exchange. The greeting phase ends when the two participants reorient their bodies, moving away from a face-on orientation to stand at an angle. Terminating a conversation similarly moves through stages, starting with non-verbal cues, such as orientation shifts or glances away and cumulating in the verbal exchange of farewells and the breaking of mutual gaze.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conversational Turn-Taking and Interruption</head><p>Interlocutors do not normally talk at the same time, thus imposing a turn-taking sequence on the conversation. The protocols involved in floor management --determining whose turn it is and when the turn should be given to the listener --involve many factors including gaze and intonation <ref type="bibr" target="#b9">[10]</ref>. In addition, listeners can interrupt a speaker not only with voice, but by gesturing to indicate that they want the turn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Communicative Functions Communicative Behavior</head><p>Initiation and termination:  <ref type="table">1</ref>. Some examples of conversational functions and their behavior realization</p><formula xml:id="formula_0">Reacting</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Content Elaboration and Emphasis</head><p>Gestures can convey information about the content of the conversation in ways that the hands are uniquely suited to fulfill. For example, the two hands can better indicate simultaneity and spatial relationships than the voice or other channels.</p><p>Probably the most commonly thought of use of the body in conversation is the pointing (deictic) gesture, possibly accounting for the fact that it is also the most commonly implemented use for the bodies of animated interface agents. In fact, however, most conversations don't involve many deictic gestures <ref type="bibr" target="#b17">[18]</ref> unless the interlocutors are discussing a shared task that is currently present.Other conversational gestures also convey semantic and pragmatic information <ref type="bibr" target="#b4">[5]</ref>. Beat gestures are small, rythmic baton like movements of the hands that do not change in form with the content of the accompanying speech. They serve a pragmatic function, conveying information about what is "new" in the speaker's discourse. Iconic and metaphoric gestures convey some features of the action or event being described. They can be redundant or complementary relative to the speech channel, and thus can convey additional information or provide robustness or emphasis with respect to what is being said. Whereas iconics convey information about spatial relationships or concepts, metaphorics represent concepts which have no physical form, such as a sweeping gesture accompanying "the property title is free and clear."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feedback and Error Correction</head><p>During conversation, speakers can non-verbally request feedback from listeners through gaze and raised eyebrows and listeners can provide feedback through head nods and paraverbals ("uh-huh", "mmm", etc.) if the speaker is understood, or a confused facial expression or lack of positive feedback if not. The listener can also ask clarifying questions if they did not hear or understand something the speaker said.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REA: AN EMBODIED CONVERSATIONAL AGENT</head><p>The Rea project at the MIT Media Lab <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref> has as its goal the construction of an embodied, multi-modal real-time conversational interface agent. Rea implements the conversational protocols described above in order to make interactions almost as natural as face-to-face conversation with another person. In the current task domain, Rea acts as a real estate salesperson, answering user questions about properties in her database and showing users around the virtual houses (Figure <ref type="figure">1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1. User interacting with Rea</head><p>Rea has a fully articulated graphical body, can sense the user passively through cameras and audio input, and is capable of speech with intonation, facial display, and gestural output. The system currently consists of a large projection screen on which Rea is displayed and which the user stands in front of. Two cameras mounted on top of the projection screen track the user's head and hand positions in space. Users wear a microphone for capturing speech input. A single SGI Octane computer runs the graphics and conversation engine of Rea, while several other computers manage the speech recognition and generation and image processing Rea is able to conduct a conversation describing the features of the task domain while also responding to the users' verbal and non-verbal input. When the user makes cues typically associated with turn taking behavior such as gesturing, Rea allows herself to be interrupted, and then takes the turn again when she is able. She is able to initiate conversational error correction when she misunderstands what the user says, and can generate combined voice, facial expression and gestural output. Rea's responses are generated by an incremental natural language generation engine based on <ref type="bibr" target="#b20">[21]</ref> that has been extended to synthesize redundant and complementary gestures synchronized with speech output. A simple discourse model is used for determining which speech acts users are engaging in, and resolving and generating anaphoric references.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Architecture</head><p>Figure <ref type="figure" target="#fig_0">2</ref> shows the modules of the Rea architecture that is designed to meet the requirements of real-time face-to-face conversation <ref type="bibr" target="#b6">[7]</ref>. In this design, input is accepted from as many modalities as there are input devices. However the different modalities are integrated into a single semantic representation that is passed from module to module. This representation is a KQML frame <ref type="bibr" target="#b12">[13]</ref> which has slots for interactional and propositional information so that the regulatory and content-oriented contribution of every conversational act can be maintained throughout the system.</p><p>The categorization of behaviors in terms of their conversational functions is mirrored by the organization of the architecture which centralizes decisions made in terms of functions (in the Deliberative Module), and moves to the periphery decisions made in terms of behaviors (the Input Manager and Action Scheduler).</p><p>In addition the Input Manager and Action Scheduler can communicate through a hardwired reaction connection, to respond immediately (under 200 msec.) to user input or system commands. Tracking the user with gaze shifts as they move is an example of a reactive behavior. The other modules are more "deliberative" in nature and perform non-trivial inferencing actions that can take multiple realtime cycles to complete. Rea is implemented in C++ and CLIPS, a rule-based expert system language <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overview of Implemented Communication Protocols</head><p>Rea implements the human communication protocols previously described, as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conversation Initiation and Termination</head><p>Rea acknowledges the user's presence through posture, by turning to face the user, as detected by the vision system. She also exchanges greetings and farewells with the user using verbal and non-verbal (gestural) output, in response to the user's verbal greeting and farewell. Rea also recognizes when the user turns away during conversation (based on vision input) and suspends speech input processing until the user turns to face her again.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conversational Turn-Taking and Interruption</head><p>Rea tracks who has the speaking turn (using a conversational state model), and only speaks when she holds the turn. Currently Rea always allows verbal interruption based on audio threshold detection, and yields the turn as soon as the user begins to speak. If the user gestures (as detected by the vision system) she will interpret this as expression of a desire to speak, and halt her remarks at the nearest sentence boundary. She exhibits the "look away" behavior while she is planning her response (which serves to hold the turn until she is ready to speak), and at the end of her speaking turn she turns to face the user to indicate a turn transition point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Content Elaboration and Emphasis</head><p>Rea currently uses pointing gestures to refer to pictures of houses in her environment. Research into generation of pointing gestures to disambiguate objects during virtual house walk-throughs as well as recognition of user pointing gestures is currently being pursued.</p><p>Rea generates her natural language responses together with accompanying conversational gestures in a unified text Generation Module. This module distributes the information to be conveyed to the user across the voice and gesture channels based on semantic and pragmatic criteria, resulting in both redundant and complementary gestures. The gestures are composed as a function of hand starting and ending positions, trajectory, hand shape, and envelope size. Beat gestures are generated to mark new information when no semantic information is to be conveyed through the gesture channel. Thus, for example, when describing a new property, Rea says "It has a lovely garden" and demonstrates with her hands that the garden surrounds two sides of the house.</p><p>The development of a module to classify the user's conversational gestures, based on input from the vision system and statistical models, is currently underway.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feedback and Error Correction</head><p>Rea provides non-verbal feedback during the user's turn by nodding her head at the end of user utterances (as detected through the audio threshold device) in which the user </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sample Interaction in Detail</head><p>In order to understand better how Rea processes user input, both propositional and interactional, and produces appropriate output behavior, it is helpful to look at a segment of interaction with a user and describe the messages sent between each of Rea's internal modules. The following paragraph records an actual interaction between a user and Rea: Tim approaches Rea Rea notices and looks toward him and smiles Tim says "hello" Rea responds: "Hello, how can I help you", with a hand wave Tim says "I'm looking to buy a place near MIT" Rea glances up and away to keep the turn while "thinking" Rea says: "I have a house...", with a beat gesture to emphasis the new information "house" Tim interrupts by beginning to gesture Rea finishes the current utterance by saying "in Cambridge" and then she gives up the turn. Tim asks for details.</p><p>We will now focus on how the different modules of Rea's architecture contribute to carrying out this interaction.</p><p>All messages are packaged into a KQML tell-performative (Figure <ref type="figure" target="#fig_1">3</ref>), where the sender and recipient fields contain the names of the modules communicating. For messages that have to do with describing the interaction between the user and Rea, including all messages in this example, the content field contains a frame of type commact. The sender and recipient fields of the commact denote where the communicative action originated and who is the intended recipient of it, the value being either REA or USER, depending on whether the commact is being interpreted or generated by Rea's Decision Module (DM).</p><p>The general processing sequence is as follows: Input Manager (IM) has some new information about the user's actions and creates a new commact with sender USER and recipent REA. In the input field it places a description of the behaviors detected. The Understanding Module (UM) receives the commact, interprets the behaviors and fills in the prop and intr fields accordingly, sending the commact on to the DM. In reaction to the incoming commact, the DM may construct a new commact, this time with sender REA and recipient USER. After filling in the prop and intr fields, the DM passes the frame on to the Generation Module (GM) whose job is to translate the propositional and interactional descriptions into a series of low level behaviors to be placed in the output field. Lastly the Action Scheduler (AS) receives the new commact and using the output field, it coordinates verbal and non-verbal realization. As a user comes within a few feet of Rea, a stereoscopic vision system starts to track the user's head and hand movements <ref type="bibr" target="#b1">[2]</ref>. Upon receiving this information from the IM, the UM sends the DM an interactional message saying that the user is now present. This makes the system transition into the UserPresent state (Figure <ref type="figure" target="#fig_2">4</ref>), sending off to the GM an interactional request for generating an invitation to start a conversation. The GM maps the request to a sequence of behaviors, that includes a look towards the user and a smile, to be sent to the AS for execution. When the user responds to the invitation by saying "Hello", the IM reports the onset of voice to the UM that sends the DM an interactional message saying that the user has now taken the turn. The system transitions into the UserTurn state and stays there until the IM delivers the parsed speech content to the UM and the DM has received from the UM an interactional message saying the user has given up the turn, and a propositional message in the form of a speech act, in this case of the type SA-RITUAL-GREET.</p><p>Inside DM this speech act generates an obligation to respond to the greeting. Since a similar SA-RITUAL-GREET speech act in return would fulfil that obligation, the DM sends such an act to the GM for execution. The GM breaks the speech act into a hand wave behavior and the spoken utterance "hello, how can I help you?" to be realized by the AS. The system is momentarily in a ReaTurn state while the speech act is performed, but returns back to an OpenFloor state when done. When the user starts speaking again, the GM produces an interactional message indicating that the user has taken the turn, shifting the system's state to UserTurn. As the user finishes asking "I'm looking to buy a place near MIT" the GM gives the DM the interactional message that the user has given up the turn along with the propositional message that the user performed a SA-REQUEST-PLACE.</p><note type="other">NotPresent UserPresent OpenFloor UserTurn ReaTurn</note><p>The GM also adds "NearMIT" as an attribute that the place has to have in order to be considered . The DM determines that HOUSE1 meets the user's preferences so the SA-REQUEST-PLACE speech act generates an obligation to OFFER-HOUSE1. But since this is the first time HOUSE1 is presented to the user, another obligation DESCRIBE-HOUSE1 is also generated. Looking at the obligations one at a time, the DM sends off to the GM an SA-OFFER-HOUSE1 to fulfil the first one. Along with this propositional message, an interactional message stating that Rea also needs to take the turn is sent to the GM. The GM consults a text and gesture generator 1 for generating an appropriate verbal and gestural expression of the proposition while instructing the AS to glance up and away in an effort to take and keep the turn. The user notices that Rea is planning to speak and does not grab the floor, allowing Rea to stay in a ReaTurn state. However, as Rea is delivering the utterance generated by SPUD, "I have a house", the user realizes that "near MIT" was perhaps too weak of a constraint and wants to add more detail and therefore spontaneously raises the hands in anticipation of further elaborating on the query. The vision notices the sudden hand movement and the UM sends a message to the DM saying that the user would like the turn. Gesture is treated as a low-priority interrupt, and Rea should finish her current utterance before giving the user the turn, so the DM removes the future obligation to DESCRIBE-HOUSE1 but allows the GM and AS to continue executing the current utterance. (Had the user interrupted with speech overlapping Rea's, the DM would also have halted the GM and AS execution, causing Rea to give the user the turn immediately.) When Rea finishes her utterance ("‚Ä¶in Cambridge") she looks at the user in a UserTurn state and the user continues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CONCLUSIONS</head><p>User-testing of Gandalf, capable of some of the conversational functions also described here, showed that users relied on the interactional competency of the system to negotiate turn-taking, and that they preferred such a system to another embodied character capable of only emotional expression. In fact, users became so comfortable with Gandalf that they began to overlap their speech with his, which overtaxed his limited speech recognition 1 We are using the SPUD system developed by Matthew Stone, augmented to synthesize conversational gestures in addition to speech in real time <ref type="bibr" target="#b20">[21]</ref>.</p><p>capabilities <ref type="bibr" target="#b8">[9]</ref>. Our next step is to test Rea to see whether the implementation of a larger set of conversational functions, including error correction and gesture synthesis, allows users to engage in more efficient and fluent interaction with the system.</p><p>In this paper we have argued that embodied interface agents can provide a qualitative advantage over nonembodied interfaces, if the bodies are used in ways that leverage knowledge of human communicative behavior. We demonstrated our approach with the Rea system. Increasingly capable of making an intelligent contentoriented -or propositional -contribution to the conversation, Rea is also sensitive to the regulatory -or interactional --function of verbal and non-verbal conversational behaviors, and is capable of producing regulatory behaviors to improve the interaction by helping the user remain aware of the state of the conversation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Rea's Software Architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>(Figure 3 .</head><label>3</label><figDesc>Figure 3. A Sample Performative</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Rea's Conversational States</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>Thanks to the other members of the Rea team -Lee Campbell, David Mellis and Nina Yu -and to Jennifer Smith for their contribution to the work and comments on this paper.</p><p>Thanks to Candy Sidner and several anonymous reviewers for helpful comments that improved the paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Integrating Reactive and Scripted Behaviors in a Life-Like Presentation Agent</title>
		<author>
			<persName><forename type="first">E</forename><surname>Andre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mueller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Agents &apos;98</title>
		<meeting>Agents &apos;98<address><addrLine>Minneapolis/St. Paul</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1998-05">May 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Real-time 3-D tracking of the human body</title>
		<author>
			<persName><forename type="first">A</forename><surname>Azarbayejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IMAGE&apos;COM 96</title>
		<meeting>IMAGE&apos;COM 96<address><addrLine>Bordeaux, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996-05">May 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Lifelike computer characters: the persona project at Microsoft Research</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kurlander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pugh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Skelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stankosky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Thiel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Dantzich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wax</surname></persName>
		</author>
		<editor>Software Agents, J. M. Bradshaw</editor>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The Effects of Visibility in a Cooperative Problem Solving Task</title>
		<author>
			<persName><forename type="first">E</forename><surname>Boyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Newlands</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language and Speech</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Embodied Conversation: Integrating Face and Gesture into Automatic Spoken Dialogue Systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cassell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spoken Dialogue Systems</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
	<note>Luperfoy. to appear</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Embodiment in Conversational Interfaces: Rea</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cassell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bickmore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Billinghurst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Vilhj√°lmsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM CHI 99 Conference Proceedings</title>
		<meeting><address><addrLine>Pittsburgh, PA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Human Conversation as a System Framework: Designing Embodied Conversational Agents</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cassell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bickmore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Vilhj√°lmsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Embodied Conversational Agents</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Cassell</surname></persName>
		</editor>
		<editor>
			<persName><surname>Editor</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Animated conversation: rule-based generation of facial display, gesture and spoken intonation for multiple conversational agents</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cassell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pelachaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">I</forename><surname>Badler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Steedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Achorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Beckett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Douville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Prevost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics (SIGGRAPH &apos;94 Proceedings)</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="413" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The Power of a Nod and a Glance: Envelope vs. Emotional Feedback in Animated Conversational Agents</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cassell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Th√≥risson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Artificial Intelligence</title>
		<imprint/>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Turn taking vs. Discourse Structure: how best to model multimodal conversation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cassell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Torres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Prevost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Conversations</title>
		<meeting><address><addrLine>The Hague</addrLine></address></meeting>
		<imprint>
			<publisher>Kluwer</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
	<note>In Wilks (ed.</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Discourse-Oriented Facial Displays in Conversation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Chovil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Research on Language and Social Interaction</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="163" to="194" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m">CLIPS Reference Manual Version 6.0. Technical Report, Number JSC-25012, Software Technology Branch</title>
		<meeting><address><addrLine>Lyndon B. Johnson Space Center, Houston, TX</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">KQML as an Agent Communication Language</title>
		<author>
			<persName><forename type="first">T</forename><surname>Finin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fritzson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Conference on Information and Knowledge Management (CIKM&apos;94</title>
		<meeting>the Third International Conference on Information and Knowledge Management (CIKM&apos;94</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1994-11">November 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Human Values and the Design of Computer Technology</title>
		<author>
			<persName><forename type="first">S</forename><surname>Keisler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sproull</surname></persName>
		</author>
		<editor>B. Friedman</editor>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>CSLI Publications</publisher>
			<biblScope unit="page" from="191" to="200" />
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
	<note>Social&quot; Human-Computer Interaction</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Conducting Interaction: Patterns of behavior in focused encounters</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kendon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>Cambridge University Press</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Agents with faces: The effect of personification</title>
		<author>
			<persName><forename type="first">T</forename><surname>Koda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Maes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth IEEE International Workshop on Robot and Human Communication (RO-MAN &apos;96</title>
		<meeting>the Fifth IEEE International Workshop on Robot and Human Communication (RO-MAN &apos;96</meeting>
		<imprint>
			<biblScope unit="page" from="189" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The Persona Effect: Affective Impact of Animated Pedagogical Agents</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Converse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kahler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Barlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bhogal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Factors in Computing Systems: CHI&apos;97 Conference Proceedings</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Pemberton</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<biblScope unit="page" from="359" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">D</forename><surname>Mcneill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mind</forename><surname>Hand</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
			<publisher>University of Chicago Press</publisher>
		</imprint>
	</monogr>
	<note type="report_type">What Gestures Reveal About Thought</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Face-to-Face Interfaces</title>
		<author>
			<persName><forename type="first">S</forename><surname>Prevost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hodgson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Churchill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Factors in Computing Systems: CHI&apos;99 Extended Abstracts</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Altom</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Williams</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<biblScope unit="page" from="244" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Task-Oriented Dialogs with Animated Agents in Virtual Reality</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Embodied Conversational Characters</title>
		<meeting>the Workshop on Embodied Conversational Characters<address><addrLine>Tahoe City, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-10">October 1998</date>
			<biblScope unit="page" from="39" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Modality in Dialogue: Planning, Pragmatics, and Computation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Stone</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
		<respStmt>
			<orgName>University of Pennsylvania</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD Thesis</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Communicative Humanoids: A Computational Model of Psychosocial Dialogue Skills</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Th√≥risson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
		<respStmt>
			<orgName>MIT Media Laboratory</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD Thesis</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Situated facial displays: Towards social interaction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Takeuchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Naito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Factors in Computing Systems: CHI&apos;95 Conference Proceedings</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Katz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Mack</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Marks</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Rosson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Nielsen</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<biblScope unit="page" from="450" to="455" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
