<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Benoit</forename><surname>Jacob</surname></persName>
							<email>benoitjacob@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Skirmantas</forename><surname>Kligys</surname></persName>
							<email>skligys@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Bo</forename><surname>Chen</surname></persName>
							<email>bochen@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
							<email>menglong@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Matthew</forename><surname>Tang</surname></persName>
							<email>mttang@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Andrew</forename><forename type="middle">Howard</forename><surname>Hartwig</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Adam</forename><surname>Dmitry Kalenichenko</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Inc</forename><surname>Google</surname></persName>
						</author>
						<title level="a" type="main">Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The rising popularity of intelligent mobile devices and the daunting computational cost of deep learning-based models call for efficient and accurate on-device inference schemes. We propose a quantization scheme that allows inference to be carried out using integer-only arithmetic, which can be implemented more efficiently than floating point inference on commonly available integer-only hardware. We also co-design a training procedure to preserve end-to-end model accuracy post quantization. As a result, the proposed quantization scheme improves the tradeoff between accuracy and on-device latency. The improvements are significant even on MobileNets, a model family known for run-time efficiency, and are demonstrated in ImageNet classification and COCO detection on popular CPUs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Current state-of-the-art Convolutional Neural Networks (CNNs) are not well suited for use on mobile devices. Since the advent of AlexNet <ref type="bibr" target="#b19">[20]</ref>, modern CNNs have primarily been appraised according to classification / detection accuracy. Thus network architectures have evolved without regard to model complexity and computational efficiency. On the other hand, successful deployment of CNNs on mobile platforms such as smartphones, AR/VR devices (HoloLens, Daydream), and drones require small model sizes to accommodate limited on-device memory, and low latency to maintain user engagement. This has led to a burgeoning field of research that focuses on reducing the model size and inference time of CNNs with minimal accuracy losses.</p><p>Approaches in this field roughly fall into two categories. The first category, exemplified by MobileNet <ref type="bibr" target="#b9">[10]</ref>, SqueezeNet <ref type="bibr" target="#b15">[16]</ref>, ShuffleNet <ref type="bibr" target="#b31">[32]</ref>, and DenseNet <ref type="bibr" target="#b10">[11]</ref>, designs novel network architectures that exploit computation / memory efficient operations. The second category quan-tizes the weights and / or activations of a CNN from 32 bit floating point into lower bit-depth representations. This methodology, embraced by approaches such as Ternary weight networks (TWN <ref type="bibr" target="#b21">[22]</ref>), Binary Neural Networks (BNN <ref type="bibr" target="#b13">[14]</ref>), XNOR-net <ref type="bibr" target="#b26">[27]</ref>, and more <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35]</ref>, is the focus of our investigation. Despite their abundance, current quantization approaches are lacking in two respects when it comes to trading off latency with accuracy.</p><p>First, prior approaches have not been evaluated on a reasonable baseline architecture. The most common baseline architectures, AlexNet <ref type="bibr" target="#b19">[20]</ref>, VGG <ref type="bibr" target="#b27">[28]</ref> and GoogleNet <ref type="bibr" target="#b28">[29]</ref>, are all over-parameterized by design in order to extract marginal accuracy improvements. Therefore, it is easy to obtain sizable compression of these architectures, reducing quantization experiments on these architectures to proofof-concepts at best. Instead, a more meaningful challenge would be to quantize model architectures that are already efficient at trading off latency with accuracy, e.g. MobileNets.</p><p>Second, many quantization approaches do not deliver verifiable efficiency improvements on real hardware. Approaches that quantize only the weights ( <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b32">33]</ref>) are primarily concerned with on-device storage and less with computational efficiency. Notable exceptions are binary, ternary and bit-shift networks <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b26">27]</ref>. These latter approaches employ weights that are either 0 or powers of 2, which allow multiplication to be implemented by bit shifts. However, while bit-shifts can be efficient in custom hardware, they provide little benefit on existing hardware with multiply-add instructions that, when properly used (i.e. pipelined), are not more expensive than additions alone. Moreover, multiplications are only expensive if the operands are wide, and the need to avoid multiplications diminishes with bit depth once both weights and activations are quantized. Notably, these approaches rarely provide on-device measurements to verify the promised timing improvements. More runtime-friendly approaches quantize both the weights and the activations into 1 bit representa- Weight quantization ("wt quant") and activation quantization ("act quant") nodes are injected into the computation graph to simulate the effects of quantization of the variables (section 3). The resultant graph approximates the integer-arithmetic-only computation graph in panel a), while being trainable using conventional optimization algorithms for floating point models. c) Our quantization scheme benefits from the fast integer-arithmetic circuits in common CPUs to deliver an improved latency-vs-accuracy tradeoff (section 4). The figure compares integer quantized MobileNets <ref type="bibr" target="#b9">[10]</ref> against floating point baselines on ImageNet <ref type="bibr" target="#b2">[3]</ref> using Qualcomm Snapdragon 835 LITTLE cores.</p><p>tions <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b33">34]</ref>. With these approaches, both multiplications and additions can be implemented by efficient bit-shift and bit-count operations, which are showcased in custom GPU kernels (BNN <ref type="bibr" target="#b13">[14]</ref>). However, 1 bit quantization often leads to substantial performance degradation, and may be overly stringent on model representation.</p><p>In this paper we address the above issues by improving the latency-vs-accuracy tradeoffs of MobileNets on common mobile hardware. Our specific contributions are:</p><p>• We provide a quantization scheme (section 2.1) that quantizesh both weights and activations as 8-bit integers, and just a few parameters (bias vectors) as 32-bit integers.</p><p>• We provide a quantized inference framework that is efficiently implementable on integer-arithmetic-only hardware such as the Qualcomm Hexagon (sections 2.2, 2.3), and we describe an efficient, accurate implementation on ARM NEON (Appendix B).</p><p>• We provide a quantized training framework (section 3) co-designed with our quantized inference to minimize the loss of accuracy from quantization on real models.</p><p>• We apply our frameworks to efficient classification and detection systems based on MobileNets and provide benchmark results on popular ARM CPUs (section 4) that show significant improvements in the latency-vsaccuracy tradeoffs for state-of-the-art MobileNet architectures, demonstrated in ImageNet classification <ref type="bibr" target="#b2">[3]</ref>, COCO object detection <ref type="bibr" target="#b22">[23]</ref>, and other tasks.</p><p>Our work draws inspiration from <ref type="bibr" target="#b6">[7]</ref>, which leverages low-precision fixed-point arithmetic to accelerate the training speed of CNNs, and from <ref type="bibr" target="#b30">[31]</ref>, which uses 8-bit fixedpoint arithmetic to speed up inference on x86 CPUs. Our quantization scheme focuses instead on improving the inference speed vs accuracy tradeoff on mobile CPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Quantized Inference</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Quantization scheme</head><p>In this section, we describe our general quantization scheme 1 2 , that is, the correspondence between the bitrepresentation of values (denoted q below, for "quantized value") and their interpretation as mathematical real numbers (denoted r below, for "real value"). Our quantization scheme is implemented using integer-only arithmetic during inference and floating-point arithmetic during training, with both implementations maintaining a high degree of correspondence with each other. We achieve this by first providing a mathematically rigorous definition of our quantization scheme, and separately adopting this scheme for both integer-arithmetic inference and floating-point training.</p><p>A basic requirement of our quantization scheme is that it permits efficient implementation of all arithmetic using only integer arithmetic operations on the quantized values (we eschew implementations requiring lookup tables because these tend to perform poorly compared to pure arithmetic on SIMD hardware). This is equivalent to requiring that the quantization scheme be an affine mapping of integers q to real numbers r, i.e. of the form r = S(q − Z)</p><p>for some constants S and Z. Equation ( <ref type="formula" target="#formula_0">1</ref>) is our quantization scheme and the constants S and Z are our quantization parameters. Our quantization scheme uses a single set of quantization parameters for all values within each activations array and within each weights array; separate arrays use separate quantization parameters. For 8-bit quantization, q is quantized as an 8-bit integer (for B-bit quantization, q is quantized as an B-bit integer). Some arrays, typically bias vectors, are quantized as 32-bit integers, see section 2.4.</p><p>The constant S (for "scale") is an arbitrary positive real number. It is typically represented in software as a floatingpoint quantity, like the real values r. Section 2.2 describes methods for avoiding the representation of such floatingpoint quantities in the inference workload.</p><p>The constant Z (for "zero-point") is of the same type as quantized values q, and is in fact the quantized value q corresponding to the real value 0. This allows us to automatically meet the requirement that the real value r = 0 be exactly representable by a quantized value. The motivation for this requirement is that efficient implementation of neural network operators often requires zero-padding of arrays around boundaries.</p><p>Our discussion so far is summarized in the following quantized buffer data structure <ref type="foot" target="#foot_2">3</ref> , with one instance of such a buffer existing for each activations array and weights array in a neural network. We use C++ syntax because it allows the unambiguous conveyance of types.</p><p>template&lt;typename QType&gt; // e.g. QType=uint8 struct QuantizedBuffer { vector&lt;QType&gt; q; // the quantized values float S; // the scale QType Z; // the zero-point };</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Integer-arithmetic-only matrix multiplication</head><p>We now turn to the question of how to perform inference using only integer arithmetic, i.e. how to use Equation ( <ref type="formula" target="#formula_0">1</ref>) to translate real-numbers computation into quantized-values computation, and how the latter can be designed to involve only integer arithmetic even though the scale values S are not integers.</p><p>Consider the multiplication of two square N × N matrices of real numbers, r 1 and r 2 , with their product represented by r 3 = r 1 r 2 . We denote the entries of each of these matrices r α (α = 1, 2 or 3) as r (i,j) α for 1 i, j N , and the quantization parameters with which they are quantized as (S α , Z α ). We denote the quantized entries by q (i,j) α . Equation (1) then becomes:</p><formula xml:id="formula_1">r (i,j) α = S α (q (i,j) α − Z α ).</formula><p>(</p><formula xml:id="formula_2">)<label>2</label></formula><p>From the definition of matrix multiplication, we have</p><formula xml:id="formula_3">S 3 (q (i,k) 3 − Z 3 ) = N j=1 S 1 (q (i,j) 1 − Z 1 )S 2 (q (j,k) 2 − Z 2 ),<label>(3)</label></formula><p>which can be rewritten as</p><formula xml:id="formula_4">q (i,k) 3 = Z 3 + M N j=1 (q (i,j) 1 − Z 1 )(q (j,k) 2 − Z 2 ),<label>(4)</label></formula><p>where the multiplier M is defined as</p><formula xml:id="formula_5">M := S 1 S 2 S 3 .<label>(5)</label></formula><p>In Equation ( <ref type="formula" target="#formula_4">4</ref>), the only non-integer is the multiplier M . As a constant depending only on the quantization scales S 1 , S 2 , S 3 , it can be computed offline. We empirically find it to always be in the interval (0, 1), and can therefore express it in the normalized form</p><formula xml:id="formula_6">M = 2 −n M 0<label>(6)</label></formula><p>where M 0 is in the interval [0.5, 1) and n is a non-negative integer. The normalized multiplier M 0 now lends itself well to being expressed as a fixed-point multiplier (e.g. int16 or int32 depending on hardware capability). For example, if int32 is used, the integer representing M 0 is the int32 value nearest to 2 31 M 0 . Since M 0 0.5, this value is always at least 2 30 and will therefore always have at least 30 bits of relative accuracy. Multiplication by M 0 can thus be implemented as a fixed-point multiplication <ref type="foot" target="#foot_3">4</ref> . Meanwhile, multiplication by 2 −n can be implemented with an efficient bitshift, albeit one that needs to have correct round-to-nearest behavior, an issue that we return to in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Efficient handling of zero-points</head><p>In order to efficiently implement the evaluation of Equation (4) without having to perform 2N 3 subtractions and without having to expand the operands of the multiplication into 16-bit integers, we first notice that by distributing the multiplication in Equation ( <ref type="formula" target="#formula_4">4</ref>), we can rewrite it as</p><formula xml:id="formula_7">q (i,k) 3 = Z 3 + M   N Z 1 Z 2 − Z 1 a (k) 2 −Z 2 ā(i) 1 + N j=1 q (i,j) 1 q (j,k) 2  <label>(7)</label></formula><p>where</p><formula xml:id="formula_8">a (k) 2 := N j=1 q (j,k) 2 , ā(i) 1 := N j=1 q (i,j) 1 . (<label>8</label></formula><formula xml:id="formula_9">)</formula><p>Each</p><formula xml:id="formula_10">a (k)</formula><p>2 or ā(i) 1 takes only N additions to compute, so they collectively take only 2N 2 additions. The rest of the cost of the evaluation of ( <ref type="formula" target="#formula_7">7</ref>) is almost entirely concentrated in the core integer matrix multiplication accumulation</p><formula xml:id="formula_11">N j=1 q (i,j) 1 q (j,k) 2 (9)</formula><p>which takes 2N 3 arithmetic operations; indeed, everything else involved in <ref type="bibr" target="#b6">(7)</ref> is O(N 2 ) with a small constant in the O. Thus, the expansion into the form <ref type="bibr" target="#b6">(7)</ref> and the factored-out computation of a (k) 2 and ā(i) 1 enable low-overhead handling of arbitrary zero-points for anything but the smallest values of N , reducing the problem to the same core integer matrix multiplication accumulation (9) as we would have to compute in any other zero-points-free quantization scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Implementation of a typical fused layer</head><p>We continue the discussion of section 2.3, but now explicitly define the data types of all quantities involved, and modify the quantized matrix multiplication <ref type="bibr" target="#b6">(7)</ref> to merge the bias-addition and activation function evaluation directly into it. This fusing of whole layers into a single operation is not only an optimization. As we must reproduce in inference code the same arithmetic that is used in training, the granularity of fused operators in inference code (taking an 8-bit quantized input and producing an 8-bit quantized output) must match the placement of "fake quantization" operators in the training graph (section 3).</p><p>For our implementation on ARM and x86 CPU architectures, we use the gemmlowp library <ref type="bibr" target="#b17">[18]</ref>, whose GemmWithOutputPipeline entry point provides supports the fused operations that we now describe <ref type="foot" target="#foot_4">5</ref> .</p><p>We take the q 1 matrix to be the weights, and the q 2 matrix to be the activations. Both the weights and activations are of type uint8 (we could have equivalently chosen int8, with suitably modified zero-points). Accumulating products of uint8 values requires a 32-bit accumulator, and we choose a signed type for the accumulator for a reason that will soon become clear. The sum in ( <ref type="formula">9</ref>) is thus of the form:</p><formula xml:id="formula_12">int32 += uint8 * uint8.<label>(10)</label></formula><p>In order to have the quantized bias-addition be the addition of an int32 bias into this int32 accumulator, the bias-vector is quantized such that: it uses int32 as its quantized data type; it uses 0 as its quantization zero-point Z bias ; and its quantization scale S bias is the same as that of the accumulators, which is the product of the scales of the weights and of the input activations. In the notation of section 2.3,</p><formula xml:id="formula_13">S bias = S 1 S 2 , Z bias = 0.<label>(11)</label></formula><p>Although the bias-vectors are quantized as 32-bit values, they account for only a tiny fraction of the parameters in a neural network. Furthermore, the use of higher precision for bias vectors meets a real need: as each bias-vector entry is added to many output activations, any quantization error in the bias-vector tends to act as an overall bias (i.e. an error term with nonzero mean), which must be avoided in order to preserve good end-to-end neural network accuracy <ref type="foot" target="#foot_5">6</ref> .</p><p>With the final value of the int32 accumulator, there remain three things left to do: scale down to the final scale used by the 8-bit output activations, cast down to uint8 and apply the activation function to yield the final 8-bit output activation.</p><p>The down-scaling corresponds to multiplication by the multiplier M in equation <ref type="bibr" target="#b6">(7)</ref>. As explained in section 2.2, it is implemented as a fixed-point multiplication by a normalized multiplier M 0 and a rounding bit-shift. Afterwards, we perform a saturating cast to uint8 and to the range [0, 255].</p><p>We focus on activation functions that are mere clamps, e.g. ReLU, ReLU6. Mathematical functions are discussed in Appendix A.1 and we do not currently fuse them into such layers. Thus, the only thing that our fused activation functions need to do is to further clamp the uint8 value to some sub-interval of [0, 255] before storing the final uint8 output activation. In practice, the quantized training process (section 3) tends to learn to make use of the whole output uint8 [0, 255] interval so that the activation function no longer does anything, its effect being subsumed in the clamping to [0, 255] implied in the saturating cast to uint8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Training with simulated quantization</head><p>A common approach to training quantized networks is to train in floating point and then quantize the resulting weights (sometimes with additional post-quantization training for fine-tuning). We found that this approach works sufficiently well for large models with considerable representational capacity, but leads to significant accuracy drops for small models. Common failure modes for simple posttraining quantization include: 1) large differences (more than 100×) in ranges of weights for different output channels (section 2 mandates that all channels of the same layer be quantized to the same resolution, which causes weights in channels with smaller ranges to have much higher relative error) and 2) outlier weight values that make all remaining weights less precise after quantization.</p><p>We propose an approach that simulates quantization effects in the forward pass of training. Backpropagation still happens as usual, and all weights and biases are stored in floating point so that they can be easily nudged by small amounts. The forward propagation pass however simulates quantized inference as it will happen in the inference engine, by implementing in floating-point arithmetic the rounding behavior of the quantization scheme that we introduced in section 2:</p><p>• Weights are quantized before they are convolved with the input. If batch normalization (see <ref type="bibr" target="#b16">[17]</ref>) is used for the layer, the batch normalization parameters are "folded into" the weights before quantization, see section 3.2.</p><p>• Activations are quantized at points where they would be during inference, e.g. after the activation function is applied to a convolutional or fully connected layer's output, or after a bypass connection adds or concatenates the outputs of several layers together such as in ResNets.</p><p>For each layer, quantization is parameterized by the number of quantization levels and clamping range, and is performed by applying point-wise the quantization function q defined as follows:</p><formula xml:id="formula_14">clamp(r; a, b) := min (max(x, a), b) s(a, b, n) := b − a n − 1 (12) q(r; a, b, n) := clamp(r; a, b) − a s(a, b, n) s(a, b, n) + a,</formula><p>where r is a real-valued number to be quantized, [a; b] is the quantization range, n is the number of quantization levels, and ⌊•⌉ denotes rounding to the nearest integer. n is fixed for all layers in our experiments, e.g. n = 2 8 = 256 for 8 bit quantization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Learning quantization ranges</head><p>Quantization ranges are treated differently for weight quantization vs. activation quantization:</p><p>• For weights, the basic idea is simply to set a := min w, b := max w. We apply a minor tweak to this so that the weights, once quantized as int8 values, only range in [−127, 127] and never take the value −128, as this enables a substantial optimization opportunity (for more details, see Appendix B).</p><p>• For activations, ranges depend on the inputs to the network. To estimate the ranges, we collect [a; b] ranges seen on activations during training and then aggregate them via exponential moving averages (EMA) with the smoothing parameter being close to 1 so that observed ranges are smoothed across thousands of training steps. Given significant delay in the EMA updating activation ranges when the ranges shift rapidly, we found it useful to completely disable activation quantization at the start of training (say, for 50 thousand to 2 million steps). This allows the network to enter a more stable state where activation quantization ranges do not exclude a significant fraction of values.</p><p>In both cases, the boundaries [a; b] are nudged so that value 0.0 is exactly representable as an integer z(a, b, n) after quantization. As a result, the learned quantization parameters map to the scale S and zero-point Z in equation 1:</p><formula xml:id="formula_15">S = s(a, b, n), Z = z(a, b, n)<label>(13)</label></formula><p>Below we depict simulated quantization assuming that the computations of a neural network are captured as a Ten-sorFlow graph <ref type="bibr" target="#b0">[1]</ref>. A typical workflow is described in Algorithm 1. Optimization of the inference graph by fusing cations where tensors will be downcasted to fewer bits during inference according to equation 12. 3: Train in simulated quantized mode until convergence. 4: Create and optimize the inference graph for running in a low bit inference engine. 5: Run inference using the quantized inference graph. and removing operations is outside the scope of this paper. Source code for graph modifications (inserting fake quantization operations, creating and optimizing the inference graph) and a low bit inference engine has been opensourced with TensorFlow contributions in <ref type="bibr" target="#b18">[19]</ref>. Note that the biases are not quantized because they are represented as 32-bit integers in the inference process, with a much higher range and precision compared to the 8 bit weights and activations. Furthermore, quantization parameters used for biases are inferred from the quantization parameters of the weights and activations. See section 2.4.</p><p>Typical TensorFlow code illustrating use of <ref type="bibr" target="#b18">[19]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Batch normalization folding</head><p>For models that use batch normalization (see <ref type="bibr" target="#b16">[17]</ref>), there is additional complexity: the training graph contains batch normalization as a separate block of operations, whereas the inference graph has batch normalization parameters "folded" into the convolutional or fully connected layer's weights and biases, for efficiency. To accurately simulate quantization effects, we need to simulate this folding, and quantize weights after they have been scaled by the batch normalization parameters. We do so with the following:</p><formula xml:id="formula_16">w fold := γw σ 2 B + ε . (<label>14</label></formula><formula xml:id="formula_17">)</formula><p>Here γ is the batch normalization's scale parameter, σ 2 B is the estimate of the variance of convolution results across the batch, and ε is just a small constant for numerical stability.</p><p>After folding, the batch-normalized convolutional layer reduces to the simple convolutional layer depicted in figure 1.1a with the folded weights w fold and the corresponding folded biases. Therefore the same recipe in figure <ref type="figure" target="#fig_1">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We conducted two set of experiments, one showcasing the effectiveness of quantized training (Section. 4.1), and the other illustrating the improved latency-vs-accuracy tradeoff of quantized models on common hardware (Section. 4.2). The most performance-critical part of the inference workload on the neural networks being benchmarked is matrix multiplication (GEMM). The 8-bit and 32-bit floating-point GEMM inference code uses the gemmlowp library <ref type="bibr" target="#b17">[18]</ref> for 8-bit quantized inference, and the Eigen library <ref type="bibr" target="#b5">[6]</ref> for 32-bit floating-point inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ResNet depth 50 100 150</head><p>Floating-point accuracy 76.4% 78.0% 78.8% Integer-quantized accuracy 74.9% 76.6% 76.7%   <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b14">15]</ref>), ternary weight networks (TWN <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>), incremental network quantization (INQ <ref type="bibr" target="#b32">[33]</ref>) and fine-grained quantization (FGQ <ref type="bibr" target="#b25">[26]</ref>)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Quantized training of Large Networks</head><p>We apply quantized training to ResNets <ref type="bibr" target="#b8">[9]</ref> and Incep-tionV3 <ref type="bibr" target="#b29">[30]</ref> on the ImageNet dataset. These popular networks are too computationally intensive to be deployed on mobile devices, but are included for comparison purposes. Training protocols are discussed in Appendix D.1 and D.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">ResNets</head><p>We compare floating-point vs integer-quantized ResNets in table 4.1. Accuracies of integer-only quantized networks are within 2% of their floating-point counterparts.</p><p>We also list ResNet50 accuracies under different quantization schemes in table 4.2. As expected, integer-only quantization outperforms FGQ <ref type="bibr" target="#b25">[26]</ref>, which uses 2 bits for weight quantization. INQ <ref type="bibr" target="#b32">[33]</ref> (5-bit weight floating-point activation) achieves a similar accuracy as ours, but we provide additional run-time improvements (see section 4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Inception v3 on ImageNet</head><p>We compare the Inception v3 model quantized into 8 and 7 bits, respectively. 7-bit quantization is obtained by setting the number of quantization levels in equation 12 to n = 2 7 . We additionally probe the sensitivity of activation quantization by comparing networks with ReLU6 and ReLU. The training protocol is in Appendix D.2. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Quantization of MobileNets</head><p>MobileNets are a family of architectures that achieve a state-of-the-art tradeoff between on-device latency and Im-ageNet classification accuracy. In this section we demonstrate how integer-only quantization can further improve the tradeoff on common hardware.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">ImageNet</head><p>We benchmarked the MobileNet architecture with varying depth-multipliers (DM) and resolutions on ImageNet on three types of Qualcomm cores, which represent three different micro-architectures: 1) Snapdragon 835 LITTLE core, (figure. 1.1c), a power-efficient processor found in Google Pixel 2; 2) Snapdragon 835 big core (figure. 4.1), a high-performance core employed by Google Pixel 2; and 3) Snapdragon 821 big core (figure. 4.2), a high-performance core used in Google Pixel 1.  Integer-only quantized MobileNets achieve higher accuracies than floating-point MobileNets given the same run- time budget. The accuracy gap is quite substantial (∼ 10%) for Snapdragon 835 LITTLE cores at the 33ms latency needed for real-time (30 fps) operation. While most of the quantization literature focuses on minimizing accuracy loss for a given architecture, we advocate for a more comprehensive latency-vs-accuracy tradeoff as a better measure. Note that this tradeoff depends critically on the relative speed of floating-point vs integer-only arithmetic in hardware. Floating-point computation is better optimized in the Snapdragon 821, for example, resulting in a less noticeable reduction in latency for quantized models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">COCO</head><p>We evaluated quantization in the context of mobile real time object detection, comparing the performance of quantized 8-bit and float models of MobileNet SSD <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b24">25]</ref> on the COCO dataset <ref type="bibr" target="#b23">[24]</ref>. We replaced all the regular convolutions in the SSD prediction layers with separable convolutions (depthwise followed by 1 × 1 projection). This modification is consistent with the overall design of MobileNets and makes them more computationally efficient. We utilized the Open Source TensorFlow Object Detection API <ref type="bibr" target="#b11">[12]</ref> to train and evaluate our models. The training protocol is described in Appendix D.3. We also delayed quantization for 500 thousand steps (see section 3.1), finding that it significantly decreases the time to convergence. Table <ref type="table" target="#tab_1">4</ref>.4 shows the latency-vs-accuracy tradeoff between floating-point and integer-quantized models. Latency was measured on a single thread using Snapdragon 835 cores (big and LITTLE). Quantized training and inference results in up to a 50% reduction in running time, with a minimal loss in accuracy (−1.8% relative).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Face detection and attribute classification</head><p>To better examine quantized MobileNet SSD on a smaller scale, we benchmarked face detection on the face attribute classification dataset (a Flickr-based dataset used in <ref type="bibr" target="#b9">[10]</ref>). We contacted the authors of <ref type="bibr" target="#b9">[10]</ref> to evaluate our quantized MobileNets on detection and face attributes following the same protocols (detailed in Appendix D.4). Face detection: As indicated by tables 4.5 and Appendix D.1, quantization provides close to a 2× latency reduction with a Qualcomm Snapdragon 835 big or LIT-TLE core at the cost of a ∼ 2% drop in the average precision. Notably, quantization allows the 25% face detector to run in real-time (1K/28 ≈ 36 fps) on a single big core, whereas the floating-point model remains slower than realtime (1K/44 ≈ 23 fps).</p><p>We additionally examine the effect of multi-threading on the latency of quantized models. Table Appendix D.1 shows a 1.5 to 2.2×) speedup when using 4 cores. The speedup ratios are comparable between the two cores, and are higher for larger models where the overhead of multi-threading occupies a smaller fraction of the total computation.</p><p>Face attributes:   Ablation study To understand performance sensitivity to the quantization scheme, we further evaluate quantized training with varying weight and activation quantization bit depths. The degradation in average precision for binary attributes and age precision relative to the floating-point baseline are shown in Tables 4.6 and Appendix D.2, respectively. The tables suggest that 1) weights are more sensitive to reduced quantization bit depth than activations, 2) 8 and 7-bit quantized models perform similarly to floating point models, and 3) when the total bit-depths are equal, it is better to keep weight and activation bit depths the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>We propose a quantization scheme that relies only on integer arithmetic to approximate the floating-point computations in a neural network. Training that simulates the effect of quantization helps to restore model accuracy to near-identical levels as the original. In addition to the 4× reduction of model size, inference efficiency is improved via ARM NEON-based implementations. The improvement advances the state-of-the-art tradeoff between latency on common ARM CPUs and the accuracy of popular computer vision models. The synergy between our quantization scheme and efficient architecture design suggests that integer-arithmetic-only inference could be a key enabler that propels visual recognition technologies into the realtime and low-end phone market.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 . 1 :</head><label>11</label><figDesc>Figure 1.1: Integer-arithmetic-only quantization. a) Integer-arithmetic-only inference of a convolution layer. The input and output are represented as 8-bit integers according to equation 1. The convolution involves 8-bit integer operands and a 32-bit integer accumulator. The bias addition involves only 32-bit integers (section 2.4). The ReLU6 nonlinearity only involves 8-bit integer arithmetic. b) Training with simulated quantization of the convolution layer. All variables and computations are carried out using 32-bit floating-point arithmetic.Weight quantization ("wt quant") and activation quantization ("act quant") nodes are injected into the computation graph to simulate the effects of quantization of the variables (section 3). The resultant graph approximates the integer-arithmetic-only computation graph in panel a), while being trainable using conventional optimization algorithms for floating point models. c) Our quantization scheme benefits from the fast integer-arithmetic circuits in common CPUs to deliver an improved latency-vs-accuracy tradeoff (section 4). The figure compares integer quantized MobileNets<ref type="bibr" target="#b9">[10]</ref> against floating point baselines on ImageNet<ref type="bibr" target="#b2">[3]</ref> using Qualcomm Snapdragon 835 LITTLE cores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1</head><label>1</label><figDesc>Quantized graph training and inference 1: Create a training graph of the floating-point model. 2: Insert fake quantization TensorFlow operations in lo-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 .</head><label>1</label><figDesc>1a and b illustrate TensorFlow graphs before and after quantization for a simple convolutional layer. Illustrations of the more complex convolution with a bypass connection in figure C.3 can be found in figure C.4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>.1b applies. See Appendix for the training graph (figure C.5) for a batch-normalized convolutional layer, the corresponding inference graph (figure C.6), the training graph after batchnorm folding (figure C.7) and the training graph after both folding and quantization (figure C.8).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 . 1 :</head><label>41</label><figDesc>Figure 4.1: Latency-vs-accuracy tradeoff of float vs. integer-only MobileNets on ImageNet using Snapdragon 835 big cores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 . 2 :</head><label>42</label><figDesc>Figure 4.2: Latency-vs-accuracy tradeoff of float vs. integer-only MobileNets on ImageNet using Snapdragon 821 big core.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 .</head><label>4</label><figDesc><ref type="bibr" target="#b2">3</ref> shows the latency-vsaccuracy tradeoff of face attribute classification on the Qualcomm Snapdragon 821. Since quantized training results in little accuracy degradation, we see an improved tradeoff even though the Qualcomm Snapdragon 821 is highly optimized for floating point arithmetic (see Fig-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 . 3 :</head><label>43</label><figDesc>Figure 4.3: Latency-vs-accuracy tradeoff of float vs. integer-only MobileNets for face attribute classification on Snapdragon 821.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>follows:</figDesc><table><row><cell>from tf.contrib.quantize import quantize_graph as qg</cell></row><row><cell>g = tf.Graph()</cell></row><row><cell>with g.as_default():</cell></row><row><cell>output, total_loss, optimizer, train_tensor = ...</cell></row><row><cell>if is_training:</cell></row><row><cell>quantized_graph = qg.create_training_graph(g)</cell></row><row><cell>else:</cell></row><row><cell>quantized_graph = qg.create_eval_graph(g)</cell></row><row><cell># Train or evaluate quantized_graph.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 4 .</head><label>4</label><figDesc>1: ResNet on ImageNet: Floating-point vs quantized network accuracy for various network depths.</figDesc><table><row><cell>Scheme</cell><cell>BWN</cell><cell>TWN</cell><cell>INQ</cell><cell>FGQ</cell><cell>Ours</cell></row><row><cell>Weight bits</cell><cell>1</cell><cell>2</cell><cell>5</cell><cell>2</cell><cell>8</cell></row><row><cell cols="4">Activation bits float32 float32 float32</cell><cell>8</cell><cell>8</cell></row><row><cell>Accuracy</cell><cell cols="5">68.7% 72.5% 74.8% 70.8% 74.9%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>2: ResNet on ImageNet: Accuracy under various quantization schemes, including binary weight networks (BWN</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>.3 shows that 7-bit quantized training produces</cell></row><row><cell>model accuracies close to that of 8-bit quantized train-</cell></row><row><cell>ing, and quantized models with ReLU6 have less accuracy</cell></row><row><cell>degradation. The latter can be explained by noticing that</cell></row><row><cell>ReLU6 introduces the interval [0, 6] as a natural range for</cell></row><row><cell>activations, while ReLU allows activations to take values</cell></row><row><cell>from a possibly larger interval, with different ranges in dif-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 . 5</head><label>45</label><figDesc></figDesc><table><row><cell>DM</cell><cell cols="5">Type mAP LITTLE (ms) big (ms)</cell></row><row><cell cols="3">100% floats 22.1</cell><cell>778</cell><cell cols="2">370</cell></row><row><cell></cell><cell cols="2">8 bits 21.7</cell><cell>687</cell><cell cols="2">272</cell></row><row><cell cols="3">50% floats 16.7</cell><cell>270</cell><cell cols="2">121</cell></row><row><cell></cell><cell cols="2">8 bits 16.6</cell><cell>146</cell><cell>61</cell></row><row><cell cols="6">Table 4.4: Object detection speed and accuracy on COCO dataset</cell></row><row><cell cols="6">of floating point and integer-only quantized models. Latency (ms)</cell></row><row><cell cols="4">is measured on Qualcomm Snapdragon 835.</cell><cell></cell></row><row><cell>DM</cell><cell cols="5">Type Precision Recall LITTLE big</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(ms)</cell><cell>(ms)</cell></row><row><cell cols="2">100% floats</cell><cell>68%</cell><cell>76%</cell><cell>711</cell><cell>337</cell></row><row><cell></cell><cell>8 bits</cell><cell>66%</cell><cell>75%</cell><cell>372</cell><cell>154</cell></row><row><cell cols="2">50% floats</cell><cell>65%</cell><cell>70%</cell><cell>233</cell><cell>106</cell></row><row><cell></cell><cell>8 bits</cell><cell>62%</cell><cell>70%</cell><cell>134</cell><cell>56</cell></row><row><cell cols="2">25% floats</cell><cell>56%</cell><cell>64%</cell><cell>100</cell><cell>44</cell></row><row><cell></cell><cell>8 bits</cell><cell>54%</cell><cell>63%</cell><cell>67</cell><cell>28</cell></row></table><note>: Face detection accuracy of floating point and integeronly quantized models. The reported precision / recall is averaged over different precision / recall values where an IOU of x between the groundtruth and predicted windows is considered a correct detection, for x in {0.5, 0.55, . . . , 0.95}. Latency (ms) of floating point and quantized models are reported on Qualcomm Snapdragon 835 using a single LITTLE and big core, respectively.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">The quantization scheme described here is the one adopted in Tensor-Flow Lite<ref type="bibr" target="#b4">[5]</ref> and we will refer to specific parts of its code to illustrate aspects discussed below.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><ref type="bibr" target="#b1">2</ref> We had earlier described this quantization scheme in the documentation of gemmlowp<ref type="bibr" target="#b17">[18]</ref>. That page may still be useful as an alternate treatment of some of the topics developed in this section, and for its selfcontained example code.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">The actual data structures in the TensorFlow Lite<ref type="bibr" target="#b4">[5]</ref> Converter are QuantizationParams and Array in this header file. As we discuss in the next subsection, this data structure, which still contains a floatingpoint quantity, does not appear in the actual quantized on-device inference code.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">The computation discussed in this section is implemented in Tensor-Flow Lite<ref type="bibr" target="#b4">[5]</ref> reference code for a fully-connected layer.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4">The discussion in this section is implemented in TensorFlow Lite<ref type="bibr" target="#b4">[5]</ref> for e.g. a Convolutional operator (reference code is self-contained, optimized code calls into gemmlowp<ref type="bibr" target="#b17">[18]</ref>).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5">The quantization of bias-vectors discussed here is implemented here in the TensorFlow Lite<ref type="bibr" target="#b4">[5]</ref> Converter.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Software available from tensorflow. org</title>
				<imprint>
			<date type="published" when="2011">2015. 2015. 5, 11</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Compressing neural networks with the hashing trick</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<idno>CoRR, abs/1504.04788</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6115</idno>
		<title level="m">Compressing deep convolutional networks using vector quantization</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Google</forename><forename type="middle">Tensorflow</forename><surname>Lite</surname></persName>
		</author>
		<ptr target="https://www.tensorflow.org/mobile/tflite.2,3" />
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Eigen v3</title>
		<author>
			<persName><forename type="first">G</forename><surname>Guennebaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jacob</surname></persName>
		</author>
		<ptr target="http://eigen.tuxfamily.org.6" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep learning with limited numerical precision</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gopalakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning (ICML-15)</title>
				<meeting>the 32nd International Conference on Machine Learning (ICML-15)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1737" to="1746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding</title>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<idno>CoRR, abs/1510.00149</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno>CoRR, abs/1704.04861</idno>
		<imprint>
			<date type="published" when="2008">2017. 1, 2, 7, 8</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2001">July 2017. 1</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Tensorflow object detection api</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Speed/accuracy trade-offs for modern convolutional object detectors</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Korattikara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.10012</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Binarized neural networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Soudry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>El-Yaniv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4107" to="4115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Soudry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>El-Yaniv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.07061</idno>
		<title level="m">Quantized neural networks: Training neural networks with low precision weights and activations</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07360</idno>
		<title level="m">Squeezenet: Alexnet-level accuracy with 50x fewer parameters and¡ 1mb model size</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32Nd International Conference on International Conference on Machine Learning</title>
				<meeting>the 32Nd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>JMLR.org</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>ICML&apos;15</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">gemmlowp: a small self-contained low-precision gemm library</title>
		<author>
			<persName><forename type="first">B</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<ptr target="https://github.com/google/gemmlowp.2,4" />
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Tensorflow quantized training support</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kligys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sivakumar</surname></persName>
		</author>
		<ptr target="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/quantize.5,6" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.09870</idno>
		<title level="m">Extremely low bit neural network: Squeeze the last bit out with admm</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.04711</idno>
		<title level="m">Ternary weight networks</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.02325</idno>
		<title level="m">Ssd: Single shot multibox detector</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Mellempudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mudigere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dubey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.01462</idno>
		<title level="m">Ternary neural networks with fine-grained quantization</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Xnornet: Imagenet classification using binary convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.05279</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Improving the speed of neural networks on cpus</title>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Z</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Deep Learning and Unsupervised Feature Learning NIPS Workshop</title>
				<meeting>Deep Learning and Unsupervised Feature Learning NIPS Workshop</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>CoRR, abs/1707.01083</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Incremental network quantization: Towards lossless cnns with lowprecision weights</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.03044</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.06160</idno>
		<title level="m">Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.01064</idno>
		<title level="m">Trained ternary quantization</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
