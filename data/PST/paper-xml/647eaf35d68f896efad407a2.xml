<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Reinforcement Learning with General Utilities: Simpler Variance Reduction and Large State-Action Space</title>
				<funder ref="#_HFQ9UkX">
					<orgName type="full">ETH AI Center doctoral fellowship, ETH Foundations of Data Science</orgName>
					<orgName type="abbreviated">FDS</orgName>
				</funder>
				<funder>
					<orgName type="full">ETH Zurich Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-06-02">2 Jun 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Anas</forename><surname>Barakat</surname></persName>
							<email>&lt;anas.barakat@inf.ethz.ch&gt;.</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ilyas</forename><surname>Fatkhullin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Niao</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Reinforcement Learning with General Utilities: Simpler Variance Reduction and Large State-Action Space</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-06-02">2 Jun 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2306.01854v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We consider the reinforcement learning (RL) problem with general utilities which consists in maximizing a function of the state-action occupancy measure. Beyond the standard cumulative reward RL setting, this problem includes as particular cases constrained RL, pure exploration and learning from demonstrations among others. For this problem, we propose a simpler singleloop parameter-free normalized policy gradient algorithm. Implementing a recursive momentum variance reduction mechanism, our algorithm achieves ?(? -3 ) and ?(? -2 ) sample complexities for ?-first-order stationarity and ?-global optimality respectively, under adequate assumptions. We further address the setting of large finite state action spaces via linear function approximation of the occupancy measure and show a ?(? -4 ) sample complexity for a simple policy gradient method with a linear regression subroutine.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>While the classical Reinforcement Learning (RL) problem consists in learning a policy maximizing the expected cumulative sum of rewards through interaction with an environment, several other problems of practical interest are concerned with objectives involving more general utilities. Examples of such problems include pure exploration in RL via maximizing the entropy of the state visitation distribution (see for e.g., <ref type="bibr" target="#b22">Hazan et al. (2019)</ref>; <ref type="bibr" target="#b31">Mutti et al. (2022a)</ref>), imitation learning via minimizing an f -divergence between state-action occupancy measures of an agent and an expert <ref type="bibr" target="#b21">(Ghasemipour et al., 2020)</ref>, risk-sensitive <ref type="bibr">(Zhang et al., 2021a)</ref> or risk-averse RL maximizing, for instance, the Conditional Value-at-Risk <ref type="bibr" target="#b18">(Garc?a &amp; Fern?ndez, 2015)</ref>, constrained RL (see for e.g., <ref type="bibr" target="#b1">Altman (1999)</ref>; <ref type="bibr" target="#b6">Borkar (2005)</ref>; <ref type="bibr" target="#b5">Bhatnagar &amp; Lakshmanan (2012)</ref>; <ref type="bibr" target="#b29">Miryoosefi et al. (2019)</ref>; <ref type="bibr" target="#b12">Efroni et al. (2020)</ref>), experiment design <ref type="bibr" target="#b30">(Mutny et al., 2023)</ref> and diverse skill discovery <ref type="bibr" target="#b13">(Eysenbach et al., 2019)</ref> among others. We refer the interested reader to Table <ref type="table">1</ref> in <ref type="bibr">Zahavy et al. (2021)</ref>; <ref type="bibr">Mutti et al. (2022b)</ref>, <ref type="bibr" target="#b27">Zhang et al. (2020)</ref> and references therein for further examples and a more comprehensive description of such problems.</p><p>Recently, <ref type="bibr" target="#b27">Zhang et al. (2020;</ref><ref type="bibr">2021b)</ref> proposed a unified formulation encapsulating all the aforementioned problems as a maximization of a functional (which may not be concave) over the set of state-action occupancy measures. Interestingly, this formulation generalizes standard RL which corresponds to maximizing a linear functional of the state-action occupancy measure <ref type="bibr" target="#b35">(Puterman, 2014)</ref>. Subsuming the standard RL problem, the case where the objective functional is convex (concave for maximization) in the occupancy measure is known as Convex RL <ref type="bibr" target="#b27">(Zhang et al., 2020;</ref><ref type="bibr">Zahavy et al., 2021;</ref><ref type="bibr" target="#b20">Geist et al., 2022;</ref><ref type="bibr">Mutti et al., 2022b)</ref>.</p><p>Unlike the standard RL problem which enjoys a nice additive structure, the more general nonlinear functional alters the additive structure of the problem, invalidates the classical Bellman equations as a consequence and hence hinders the standard use of the dynamical programming machinery (see for e.g., <ref type="bibr" target="#b4">Bertsekas (2019)</ref>; <ref type="bibr" target="#b40">Sutton &amp; Barto (2018)</ref>). While value-based methods are not meaningful anymore in this general (nonlinear) utilities setting, <ref type="bibr" target="#b27">Zhang et al. (2020;</ref><ref type="bibr">2021b)</ref> proposed a direct policy search method to solve the RL problem with general utilities. This class of methods directly updates a parametrized policy along the gradient direction of the objective function. More precisely, <ref type="bibr">Zhang et al. (2021b)</ref> propose a double-loop Policy Gradient (PG) method called TSIVR-PG implementing a variance reduction mechanism requiring two large batches and checkpoints. Similarly to existing variance-reduced PG methods in the standard RL setting, the algorithm makes use of importance sampling (IS) weights to account for the distribution shift inherent to the RL setting. Interestingly, while most existing variance-reduced PG methods make an unrealistic and unverifiable assumption which guarantees that the IS weights variance is bounded at each iteration of the algorithm, <ref type="bibr">Zhang et al. (2021b)</ref> alleviate this issue by using a gradient truncation mechanism. Such a strategy consisting in performing a truncated gradient step can be formulated as solving a trustregion subproblem at each iteration, which is reminiscent of trust-region based algorithms such as TRPO <ref type="bibr" target="#b37">(Schulman et al., 2015)</ref> and PPO <ref type="bibr" target="#b38">(Schulman et al., 2017)</ref>. In particular, implementing TSIVR-PG requires tuning a gradient truncation radius depending on problem parameters while also choosing adequate large batches. Besides these algorithmic considerations, a major limitation of recent prior work <ref type="bibr">(Zhang et al., 2021b;</ref><ref type="bibr">2020;</ref><ref type="bibr" target="#b25">Kumar et al., 2022)</ref> is the need to estimate the unknown occupancy measure at each state-action pair. In several problems of practical scale, the number of states and/or actions is prohibitively large and renders tabular methods intractable. For instance, the size of a state space grows exponentially with the number of state variables. This is commonly known as the curse of dimensionality.</p><p>In this paper, we consider the RL problem with general utilities. Our contributions are as follows:</p><p>? We propose a novel single-loop normalized PG algorithm called N-VR-PG using only a single trajectory per iteration. In particular, our algorithm does not require the knowledge of problem specific parameters, large batches nor checkpoints unlike TSIVR-PG in <ref type="bibr">Zhang et al. (2021b)</ref>. Instead of gradient truncation, we propose to use a normalized update rule for which no additional gradient truncation hyperparameter is needed. At the heart of our algorithm design is a recursive double variance reduction mechanism implemented with momentum for both the stochastic policy gradient and the occupancy measure estimator (in the tabular setting), akin to STORM <ref type="bibr" target="#b8">(Cutkosky &amp; Orabona, 2019)</ref> in stochastic optimization.</p><p>? We show that using a normalized gradient update guarantees bounded IS weights for the softmax parametrization. Unlike in most prior works focusing on the particular case of the standard RL setting, variance of IS weights is automatically bounded and no further assumption is needed. We further demonstrate that IS weights can also be similarly controlled when using a gaussian policy for continuous state-action spaces under mild assumptions.</p><p>? In the general utilities setting with finite state-action spaces and softmax policy, we show that our algorithm requires ?(? -3 ) samples to reach an ?-stationary point of the objective function and ?(? -2 ) samples to reach an ?-globally optimal policy by exploiting the hidden concavity of the problem when the utility function is concave and the policy is overparametrized. In the standard RL setting, we further show that such sample complexity results also hold for continuous state-action spaces when using the gaussian policy under adequate assumptions.</p><p>? Beyond the tabular setting, we consider the case of large finite state and action spaces which has not been previously addressed in this general setting to the best of our knowledge. We consider approximating the unknown state-action occupancy measure itself by a linear combination of pre-selected basis functions via a leastmean-squares solver. This linear function approximation procedure combined with a stochastic policy gradient method results in an algorithm for solving the RL problem with general nonlinear utilities for large state and action spaces. Specifically, we show that our PG method requires ?(? -4 ) samples to guarantee an ?-first-order stationary point of the objective function up to an error floor due to function approximation.</p><p>Related works. We briefly discuss standard RL before closely related works for RL with general utility.</p><p>Variance-reduced PG for standard RL. In the last few years, there has been a vast array of work around variancereduced PG methods for solving the standard RL problem with a cumulative sum of rewards to reduce the high variance of the stochastic policy gradients (see for e.g., <ref type="bibr" target="#b33">Papini et al. (2018)</ref>; <ref type="bibr">Xu et al. (2020a)</ref>; <ref type="bibr" target="#b34">Pham et al. (2020)</ref>; <ref type="bibr" target="#b19">Gargiani et al. (2022)</ref>). <ref type="bibr">Yuan et al. (2020)</ref>; <ref type="bibr" target="#b23">Huang et al. (2020)</ref> proposed momentum-based policy gradient methods. All the aforementioned works use IS and make an unverifiable assumption stipulating that the IS weights variance is bounded. To relax this unrealistic assumption, <ref type="bibr">Zhang et al. (2021b)</ref> provide a gradient truncation mechanism complementing IS for the specific case of the softmax parameterization whereas <ref type="bibr" target="#b39">Shen et al. (2019)</ref>; <ref type="bibr" target="#b36">Salehkaleybar et al. (2022)</ref> incorporate second-order information for which IS is not needed. Even in the special case of standard cumulative reward, our algorithm differs from prior work in that it combines the following features: it is single-loop, runs with a single trajectory per iteration and uses a normalized update rule to control the IS weights without further assumption. In particular, our algorithm does not make use of second order information and thus our analysis does not require second-order smoothness conditions. Typically, variance-reduced PG methods guarantee a ?(? -3 ) sample complexity to reach a first-order stationary policy, improving over its ?(? -4 ) counterpart for vanilla PG. Subsequently to the recent work of <ref type="bibr" target="#b0">Agarwal et al. (2021)</ref> which provided global optimality guarantees for PG methods despite the non-concavity of the problem, several works <ref type="bibr" target="#b27">(Liu et al., 2020;</ref><ref type="bibr">Zhang et al., 2021b;</ref><ref type="bibr" target="#b10">Ding et al., 2021;</ref><ref type="bibr">2022;</ref><ref type="bibr">Yuan et al., 2022;</ref><ref type="bibr" target="#b28">Masiha et al., 2022;</ref><ref type="bibr">Yuan et al., 2023)</ref> established global optimality guarantees for stochastic PG methods with or without variance reduction under policy parametrization. The best known sample complexity to reach an ?-globally optimal policy is ?(? -2 ) and was achieved via policy mirror descent without parametrization <ref type="bibr" target="#b26">(Lan, 2022;</ref><ref type="bibr" target="#b45">Xiao, 2022)</ref>, with log-linear policies recently <ref type="bibr">(Yuan et al., 2023)</ref> and via variance-reduced PG for softmax parametrization by exploiting hidden convex-ity <ref type="bibr">(Zhang et al., 2021b)</ref>. Very recently, <ref type="bibr" target="#b15">Fatkhullin et al. (2023)</ref> obtained a ?(? -2 ) sample complexity for Fishernon-degenerate parametrized policies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RL with General Utility.</head><p>There is a huge literature addressing control problems with nonstandard utilities that we cannot hope to give justice to. Let us mention though some early examples in Operations Research such as inventory problems with constraints on the probability of shortage <ref type="bibr" target="#b9">(Derman &amp; Klein, 1965)</ref> and variance-penalized MDPs <ref type="bibr" target="#b16">(Filar et al., 1989;</ref><ref type="bibr" target="#b24">Kallenberg, 1994)</ref> where the problem is formulated as a nonlinear program in the space of state-action frequencies.</p><p>In the rest of this section, we briefly discuss the most relevant research to the present paper. Zhang et al. ( <ref type="formula">2020</ref>) study the policy optimization problem where the objective function is a concave function of the state-action occupancy measure to include several known problems such as constrained MDPs, exploration and learning from demonstrations. To solve this problem for which dynamic programming cannot be employed, <ref type="bibr" target="#b27">Zhang et al. (2020)</ref> investigate policy search methods and first define a variational policy gradient for RL with general utilities as the solution to a stochastic saddle point problem. Exploiting the hidden convexity structure of the problem, they further show global optimality guarantees when having access to exact policy gradients. However, the procedure to estimate even a single policy gradient via the proposed primal-dual stochastic approximation method from sample paths turns out to be complex. Leveraging the formulation of the RL problem as a stochastic composite optimization problem, <ref type="bibr">Zhang et al. (2021b)</ref> later proposed a (variance-reduced) stochastic PG approach for solving general utility RL ensuring a ?(? -3 ) sample complexity to find an ?-stationary policy under smoothness of the utility function and the policy parametrization and a ?(? </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminaries</head><p>Notations. For a given finite set X , we use the notation |X | for its cardinality and ?(X ) for the space of probability distributions over X . We equip any Euclidean space with its standard inner product denoted by We use the notation E ?,? (or often simply E instead) for the associated expectation. We define for any policy ? ? ? the state-action occupancy measure ? ? ? M(S ? A) as:</p><formula xml:id="formula_0">? ? (s, a) def = +? t=0 ? t P ?,? (s t = s, a t = a) .<label>(1)</label></formula><p>We denote by ? the set of such occupancy measures, i.e., ? def = {? ? : ? ? ?} . Then, the general utility function F assigns a real to each occupancy measure ? ? induced by a policy ? ? ? . A state-action occupancy measure ? ? will also be seen as a vector of the Euclidean space R |S|?|A| .</p><p>Policy parametrization. In this paper, we will consider the common softmax policy parametrization defined for every ? ? R d , s ? S, a ? A by:</p><formula xml:id="formula_1">? ? (a|s) = exp(?(s, a; ?)) a ? ?A exp(?(s, a ? ; ?)) ,<label>(2)</label></formula><p>where ? :</p><formula xml:id="formula_2">S ? A ? R d ? R is a smooth function.</formula><p>The softmax parametrization will be important for controlling IS weights for variance reduction. However, some of our results will not require this specific parameterization and we will explicitly indicate it when appropriate.</p><p>Problem formulation. The goal of the RL agent is to find a policy ? ? (determined by the vector ?) solving the problem:</p><formula xml:id="formula_3">max ??R d F (? ? ? ) ,<label>(3)</label></formula><p>where F is a smooth function supposed to be upper bounded and F ? is used in the remainder of this paper to denote the maximum in (3). The agent has only access to (a) trajectories of finite length H generated from the MDP under the initial distribution ? and the policy ? ? and (b) the gradient of the utility function F with respect to (w.r.t.) its variable ?. In particular, provided a time horizon H and a policy ? ? with ? ? R d , the learning agent can simulate a trajectory ? = (s 0 , a 0 , ? ? ? , s H-1 , a H-1 ) from the MDP whereas the state transition kernel P is unknown. This general utility problem was described, for instance, in Zhang et al. (2021b) (see also <ref type="bibr" target="#b25">Kumar et al. (2022)</ref>). Recall that the standard RL problem corresponds to the particular case where the general utility function is a linear function, i.e., F (? ? ? ) = ?r, ? ? ? ? for some vector r ? R S?A in which case we recover the expected return function as an objective:</p><formula xml:id="formula_4">V ? ? (r) def = E ?,? ? +? t=0 ? t r(s t , a t ) .<label>(4)</label></formula><p>In the standard RL case, we shall use the notation J(?) def = V ? ? (r) where r is the corresponding reward function.</p><p>Policy Gradient for General Utilities. Following the exposition in <ref type="bibr">(Zhang et al., 2021b)</ref> (see also more recently <ref type="bibr" target="#b25">(Kumar et al., 2022</ref>)), we derive the policy gradient for the general utility objective. For convenience, we use the notation ?(?) for ? ? ? . Since the cumulative reward can be rewritten more compactly V ? ? (r) = ?? ? ? , r?, it follows from the policy gradient theorem that:</p><formula xml:id="formula_5">[? ? ?(?)] T r = ? ? V ? ? (r) = E ?,? ? +? t=0 ? t r(s t , a t ) t t ? =0 ? log ? ? (a t ? |s t ? ) , (5)</formula><p>where ? ? ?(?) is the Jacobian matrix of the vector mapping ?(?) . Using the chain rule, we have</p><formula xml:id="formula_6">? ? F (?(?)) = [? ? ?(?)] T ? ? F (?(?)) = ? ? V ? ? (r)| r=? ? F (?(?)) .<label>(6)</label></formula><p>Stochastic Policy Gradient. In light of (6), in order to estimate the policy gradient ? ? F (?(?)) for general utilities, we can use the standard reinforce estimator suggested by Eq. ( <ref type="formula">5</ref>) but we also need to estimate the state-action occupancy measure ?(?) (when F is nonlinear) 1 . Define for every reward function r (which is also seen as a vector in R |S|?|A| ), every ? ? R d and every H-length trajectory ? simulated from the MDP with policy ? ? and initial distribution ? the (truncated) policy gradient estimate:</p><formula xml:id="formula_7">g(?, ?, r) = H-1 t=0 H-1 h=t ? h r(s h , a h ) ? log ? ? (a t |s t ) . (<label>7</label></formula><formula xml:id="formula_8">)</formula><p>1 In the cumulative reward setting, notice that the general utility function F is linear and</p><formula xml:id="formula_9">? ? F (?(?)) is independent of ?(?) .</formula><p>We also define an estimator for the state-action occupancy measure ? ? ? = ?(?) (see ( <ref type="formula" target="#formula_0">1</ref>)) truncated at the horizon H by:</p><formula xml:id="formula_10">?(? ) = H-1 h=0 ? h ? s h ,a h ,<label>(8)</label></formula><p>where for every (s, a) ? S ? A, ? s,a ? R |S|?|A| is a vector of the canonical basis of R |S|?|A| , i.e., the vector whose only non-zero entry is the (s, a)-th entry which is equal to 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Importance Sampling.</head><p>Given a trajectory ? = (s 0 , a 0 , s 1 , a 1 , ? ? ? , s H-1 , a H-1 ) of length H generated under the initial distribution ? and the policy ? ? for some ? ? R d , we define for every ? ? ? R d the IS weight:</p><formula xml:id="formula_11">w(? |? ? , ?) def = H-1 h=0 ? ? ? (a h |s h ) ? ? (a h |s h ) . (<label>9</label></formula><formula xml:id="formula_12">)</formula><p>Since the problem is nonstationary in the sense that updating the parameter ? shifts the distribution over trajectories, it follows that for any</p><formula xml:id="formula_13">r ? R |S|?|A| , E ?,? ? [g(?, ?, r) - g(?, ? ? , r)] ? = ? ? V ? ? (r) -? ? V ? ? ? (r) .</formula><p>Using the IS weights, we correct this bias to obtain</p><formula xml:id="formula_14">E ?,? ? [g(?, ?, r) -w(? |? ? , ?)g(?, ? ? , r)] = ? ? V ? ? (r) -? ? V ? ? ? (r) .</formula><p>The use of IS weights is standard in variance-reduced PG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Normalized Variance-Reduced Policy Gradient Algorithm</head><p>In this section, we present our N-VR-PG algorithm (see Algorithm 1) to solve the RL problem with general utilities. This algorithm has two main distinctive features compared to vanilla PG and existing algorithms <ref type="bibr">(Zhang et al., 2021b</ref>): (i) recursive variance reduction: instead of using the stochastic PG and occupancy measure estimators respectively reported in ( <ref type="formula" target="#formula_7">7</ref>) and ( <ref type="formula" target="#formula_10">8</ref>), we use recursive variance-reduced estimators for both the PG and the state-action occupancy measure akin to STORM in stochastic optimization <ref type="bibr" target="#b8">(Cutkosky &amp; Orabona, 2019)</ref>. This leads to a simple single-loop algorithm using a single trajectory per iteration and for which no checkpoints nor any second order information are needed;</p><p>(ii) normalized PG update rule: normalization will be crucial to control the IS weights used in the estimators. We elaborate more on the motivation for using it in Section 4.1.</p><p>Remark 3.1. In Algorithm 1, note that g(? t , ? t , r t-1 ) and g(? t , ? t-1 , r t-2 ) are used in v t instead of g(? t , ? t , r t ) and g(? t , ? t-1 , r t-1 ) respectively to address measurability and independence issues in the analysis. Remark 3.2 (Standard RL). In the cumulative reward setting, estimating the occupancy measure is not needed. Hence, Algorithm 1 simplifies (see Algorithm 4 in Appendix A).</p><p>Algorithm 1 N-VR-PG (General Utilities)</p><formula xml:id="formula_15">Input: ? 0 , T , H, {? t } t?0 , {? t } t?0 . Sample ? 0 of length H from M and ? ?0 ? 0 = ?(? 0 , ? 0 ); r 0 = ? ? F (? 0 ); r -1 = r 0 d 0 = g(? 0 , ? 0 , r 0 ) ? 1 = ? 0 + ? 0 d0 ?d0? for t = 1, . . . , T -1 do Sample ? t of length H from MDP M and ? ?t u t = ?(? t )(1 -w(? t |? t-1 , ? t )) ? t = ? t ?(? t ) + (1 -? t )(? t-1 + u t ) r t = ? ? F (? t ) v t = g(? t , ? t , r t-1 ) -w(? t |? t-1 , ? t )g(? t , ? t-1 , r t-2 ) d t = ? t g(? t , ? t , r t-1 ) + (1 -? t )(d t-1 + v t ) ? t+1 = ? t + ? t dt ?dt? end for</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Convergence Analysis of N-VR-PG</head><p>We first introduce our assumptions regarding the regularity of the policy parametrization and the utility function F . Assumption 4.1. In the softmax parametrization (2), the map ?(s, a; ?) is twice continuously differentiable and there exist l</p><formula xml:id="formula_16">? , L ? &gt; 0 s.t. (i) max s?S,a?A sup ? ???(s, a; ?)? ? l ? and (ii) max s?S,a?A sup ? ?? 2 ?(s, a; ?)? ? L ? . Assumption 4.2. There exist constants l ? , L ? , L ?,? &gt; 0 s.t. for all ?, ? ? ? ?, ?? ? F (?)? ? ? l ? and ?? ? F (?) -? ? F (? ? )? ? ? L ? ?? -? ? ? 2 , ?? ? F (?) -? ? F (? ? )? ? ? L ?,? ?? -? ? ? 1 .</formula><p>Assumptions 4.1 and 4.2 were previously considered in <ref type="bibr">Zhang et al. (2021b;</ref><ref type="bibr">2020)</ref> and guarantee together that the objective function ? ? F (? ? ? ) is smooth. Assumption 4.2 is automatically satisfied for the cumulative reward setting (i.e., F linear) if the reward function is bounded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Normalization ensures boundedness of IS weights</head><p>Most prior works suppose that the variance of the IS weights is bounded. Such assumption cannot be verified. In this section we provide an alternative algorithmic way based on the softmax policy to control the IS weights without the aforementioned assumption. Since our algorithm only uses IS weights for two consecutive iterates, our key observation is that a normalized gradient update rule automatically guarantees bounded IS weights. In particular, compared to Zhang et al. (2021b), we do not use a gradient truncation mechanism which requires an additional truncation hyperparameter depending on the problem parameters and dictates a non-standard stationarity measure (see Remark 4.6). This simple algorithmic modification requires several adjustments in the convergence analysis (see Appendix E and F). We formalize the result in the following lemma. </p><formula xml:id="formula_17">= H((8H + 2)l 2 ? + 2L ? )(W + 1) .</formula><p>In this lemma, the variance of the IS weights decreases over time at a rate controlled by ?<ref type="foot" target="#foot_0">2</ref> and this result will be crucial for our convergence analysis of N-VR-PG. We show in Lemma E.19 in the Appendix that such a result also holds for Gaussian policies for continuous state action spaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">First-order stationarity</head><p>In this section, we show that N-VR-PG requires ?(? -3 ) samples to reach an ?-first-order stationary (FOS) point of the objective function for RL with general utilities. 2</p><p>Theorem 4.4. Let Assumptions 4.1 and 4.2 hold. Let ? 0 &gt; 0 and let T ? 1 be an integer. Set</p><formula xml:id="formula_18">? t = ?0 T 2 /3 , ? t = 2 t+1 2 /3 and H = (1 -?)</formula><p>-1 log(T + 1).</p><p>Then,</p><formula xml:id="formula_19">E ? ? F (?( ?T )) ? O 1+(1-?) 3 ?? -1 0 +(1-?) -1 ?0 (1-?) 3 T 1 /3 , where ? def = F ? -E[F (?(? 1</formula><p>))] and ?T is sampled uniformly at random from {? 1 , ? ? ? , ? T } of Algorithm 1.</p><p>Remark 4.5. In terms of dependence on (1 -?) -1 , we significantly improve over the result of <ref type="bibr">Zhang et al. (2021b)</ref> which does not make it explicit. We defer a detailed comparison regarding this dependence to Appendix B. Remark 4.6. Unlike <ref type="bibr">Zhang et al. (2021b)</ref> which utilizes a gradient truncation radius, our sample complexity does not depend on the inverse of this gradient truncation hyperparameter which might be small. Indeed, to translate their guarantee from the non-standard gradient mapping dictated by gradient truncation to the standard stationarity measure (used in our result), one has to incur an additional multiplicative constant ? -1 where ? is the gradient truncation radius (see <ref type="bibr">Lemma 5.4 in (Zhang et al., 2021b)</ref>).</p><p>Recalling the notation J(?) = V ? ? (r) (see (4)) for the standard RL setting, we can state the following corollary. </p><formula xml:id="formula_20">0 = 1 -?, then E ?J( ?T ) ? O (1 -?) -2 T -1 /3 .</formula><p>The next result addresses the case of continuous state-action spaces in the standard RL setting using a Gaussian policy.</p><p>Notably, we rely on similar considerations as for the softmax policy to control the variance of IS weights. We defer a precise statement of this result to Appendix E.4. Theorem 4.8 (informal). Using the Gaussian policy under some regularity conditions, N-VR-PG (see Algorithm 4) requires ?(? -3 ) to reach an ?-first-order stationary point of the expected return J.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Global optimality</head><p>In this section, we show that N-VR-PG only requires ?(? -2 ) samples to reach an ?-globally optimal policy under a concave reparametrization of the RL problem with concave utilities and an additional overparametrization assumption. Our results and assumptions match the recent results in <ref type="bibr">Zhang et al. (2021b)</ref> for finite state-action spaces.</p><p>Assumption 4.9. The utility function F is concave.</p><p>Assumption 4.10. For the softmax policy parametrization in (2), the following three requirements hold: (i) For any ? ? R d , there exist relative neighborhoods U ? ? R d and V ?(?) ? ? respectively containing ? and ?(?) s.t. the restriction ?| U ? forms a bijection between U ? and V ?(?) ; (ii) There exists l &gt; 0 s.t. for every ? ? R d , the inverse</p><formula xml:id="formula_21">(?| U ? ) -1 is l-Lipschitz continuous; (iii) There ex- ists ? &gt; 0 s.t. for every positive real ? ? ?, (1 -?)?(?) + ??(? * ) ? V ?(?)</formula><p>where ? ? * is the optimal policy.</p><p>For the tabular softmax parametrization (i.e., ?(s, a; ?) = ? s,a , d = |S||A|), a continuous local inverse can be defined whereas computing the Lipschitz constant l is more involved as reported in <ref type="bibr">Zhang et al. (2021b)</ref>  Our global optimality convergence result is as follows.</p><p>Theorem 4.12. Let Assumptions 4.1, 4.2 and 4.9 hold. Additionally, let Assumption 4.10 be satisfied with ? ? ?0(1-?) 2? ? (T +1) a for some integer T ? 1 and reals ? 0 &gt; 0, a ? (0, 1).</p><formula xml:id="formula_22">Set ? t = ?0 (T +1) a , ? t = 2</formula><p>t+1 and H = (1 -?)</p><p>-1 log(T + 1). Then the output ? T of N-VR-PG (see Algorithm 1) satisfies</p><formula xml:id="formula_23">F ? -E [F (?(? T ))] ? O ? 2 0 (1 -?) 3 (T + 1) 2a-3 2 ,</formula><p>Thus, setting ? 0 = (1 -?) 3 /2 , the sample complexity to achieve</p><formula xml:id="formula_24">F * -E [F (?(? T ))] ? ? is O ? -2 4a-3</formula><p>.</p><p>Corollary 4.13. In the setting of Theorem 4.12, N-VR-PG</p><formula xml:id="formula_25">(see Algorithm 4) requires ? ? -2 2a-1</formula><p>samples to achieve J ? -E [J(? T )] ? ? where J ? is the optimal expected return.</p><p>Remark 4.14. We refer the reader to Appendix F.2 for a precise statement of Corollary 4.13. If we know problem parameters and choose time varying step-sizes ? t = ?0 t , then we can obtain exactly ?(? -2 ) sample complexity.</p><p>We can state a similar global optimality result to Corollary 4.13 for continuous state-action spaces (see Appendix F.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Large State-Action Space Setting</head><p>An important limitation of Algorithm 1 and the prior work <ref type="bibr">(Zhang et al., 2021b)</ref> is the need to estimate the occupancy measure for each state-action pair in the case of general nonlinear utilities. This procedure is intractable if the state and/or action spaces are prohibitively large and finite or even worse infinite/continuous. In the case of infinite or continuous state-action spaces, the occupancy measure ? ? ? induced by a policy ? ? cannot be represented by a vector in finite dimensions. Thus, the derivative of the utility function F w.r.t. its variable ? is not well defined in the chain rule in (6) for the policy gradient. Therefore, more adequate notions of derivative for optimization on the space of measures are probably needed and this would require different methodological and algorithmic tools which go beyond the scope of this work. In this paper, we propose to do a first step by considering the setting of large finite state and action spaces which is already of practical interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">PG for RL with General Utilities via linear function approximation of the occupancy measure</head><p>Similarly to the classical linear function approximation of the (action-)value function in standard RL, we propose to approximate the (truncated) state-action occupancy measure by a linear combination of pre-selected basis functions in order to break the so-called curse of dimensionality. Our exposition is similar in spirit to the compatible function approximation framework <ref type="bibr" target="#b41">(Sutton et al., 1999)</ref> which was recently extended in <ref type="bibr" target="#b0">Agarwal et al. (2021)</ref> (see also <ref type="bibr">Yuan et al. (2023)</ref> for a recent example). However, we are not concerned here by the approximation of the action-value function nor are we considering the NPG (or Q-NPG) method but we are rather interested in approximating the discounted occupancy measure. Recall that we are considering the more general problem of RL with general utilities. Beyond this connection with existing work, we shall precise that our approach mostly shares the use of standard least squares regression for estimating an unknown function which is the state-action occupancy measure in our case.</p><p>Let m be a positive integer and let ? : S ? A ? R m be a feature map. We shall approximate the truncated<ref type="foot" target="#foot_1">3</ref> stateaction occupancy measure for a given policy ? ? (? ? R d fixed) by a linear combination of feature vectors from the feature map, i.e., for every state-action pair (s, a) ? S ? A,</p><formula xml:id="formula_26">? ? ? H (s, a) ? ??(s, a), ? ? ? ,<label>(10)</label></formula><p>for some ? ? ? R m that we shall compute. Typically, the dimension m is much smaller than |S| ? |A| . The feature map summarizes the most important characteristics of state-action pairs. Typically, this map is designed based on experience and domain-specific knowledge or intuition regarding the MDP. Standard examples of basis functions for the feature map include radial basis functions, wavelet networks or polynomials. Nevertheless, designing such a feature map is an important practical question that is often problem-specific and we will not address it in this work.</p><p>In order to compute such a vector ? ? , we will use linear regression. Accordingly, we define the expected regression loss measuring the estimation quality of any parameter ? for every ? ? R d , ? ? R m by:</p><formula xml:id="formula_27">L ? (?) def = E s??,a?U (A) [(? ? ? H (s, a) -??(s, a), ??) 2 ] ,<label>(11</label></formula><p>) where ? is the initial distribution in the MDP and U(A) is the uniform distribution over the action space A . <ref type="foot" target="#foot_2">4</ref> In practice, we cannot minimize L ? exactly since this would require having access to the true state-action occupancy measure and averaging over all state-action pairs s ? ?, a ? U(A) . Therefore, we compute an approximate solution ?? ? arg min ? L ? (?) . For this procedure, we need: (i) unbiased estimates of the true truncated stateaction occupancy measure ? ? ? H (s, a) (or the non-truncated one ? ? ? (s, a)) for s ? ?, a ? U(A) and (ii) a regression solver based on samples to minimize L ? as defined in (11). As for item (i), we use a Monte-Carlo estimate ?? ? H (s, a) of the truncated occupancy measure computed from a single rollout (see Algorithm 5 for details). <ref type="foot" target="#foot_3">5</ref> An unbiased stochastic gradient of the function L ? in (11) is then given by ?? L ? (?) def = 2(??(s, a), ?? -?? ? H (s, a)) ?(s, a) . (12) We can then solve the regression problem consisting in minimizing L ? in (11) via the averaged SGD algorithm (see Algorithm 2) as proposed in <ref type="bibr" target="#b1">Bach &amp; Moulines (2013)</ref>.</p><p>Using this procedure, we propose a simple stochastic PG algorithm for solving the RL problem with general utilities Algorithm 2 (averaged) SGD for Occupancy Measure Estimation via Linear Function Approximation</p><formula xml:id="formula_28">Input: ? 0 ? R m , K ? 1, ? &gt; 0, ?, ? ? . for k = 0, . . . , K -1 do Sample s ? ?; a ? U(A) Compute an estimator ?? ? H (s, a) via Algorithm 5 ?? L ? (? k ) def = 2(??(s, a), ? k ? -?? ? H (s, a)) ?(s, a) ? k+1 = ? k -? ?? L ? (? k ) end for Return: ?? = 1 K K k=1 ? k</formula><p>for large state action spaces. Since this large-scale setting has not been priorly addressed for general utilities to the best of our knowledge, we focus on a simpler PG algorithm without the variance reduction and normalization features of our algorithm in Section 3. Incorporating variance reduction to occupancy measure estimates seems more involved with our linear regression procedure for function approximation. We leave it for future work to design a method with improved sample complexity using variance reduction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 3 Stochastic PG for RL with General Utilities via Linear Function Approximation of the Occupancy Measure</head><p>Input:</p><formula xml:id="formula_29">? 0 ? R d , T, N ? 1, ? &gt; 0, K ? 1, ? &gt; 0, H .</formula><p>Run Algorithm 2 with policy ? ?0 and define from its output ?0 (?,</p><formula xml:id="formula_30">?) = ??(?, ?), ??0 ? . r -1 = ? ? F ( ?0 )</formula><p>for t = 0, . . . , T -1 do Run Algorithm 2 with policy ? ?t and define from its output ?t (?,</p><formula xml:id="formula_31">?) = ??(?, ?), ??t ? . r t = ? ? F ( ?t ) Sample a batch of N independent trajecto- ries (? (i) t ) 1?i?N of length H from M and ? ?t ? t+1 = ? t + ? N N i=1 g(? (i)</formula><p>t , ? t , r t-1 ) end for Return: ? T Remark 5.1. When running Algorithm 3, notice that the vector ?t ? R |S|?|A| (and hence the vector r t ) does not need to be computed for all state-action pairs as this would be unrealistic and even impossible in the large stateaction setting we are considering. Indeed, at each iteration, one does only need to compute (r t (s</p><formula xml:id="formula_32">(t) h , a (t) h )) 0?h?H-1 where ? t = (s (t) h , a (t)</formula><p>h ) 0?h?H-1 to obtain the stochastic policy gradient g(? t , ? t , r t-1 ) as defined in (7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Convergence and sample complexity analysis</head><p>In this section, we provide a convergence analysis of Algorithm 3. For every integer t, let ? * (? t ) ? arg min ? L ?t (?). We decompose the regression loss into the statistical error measuring the accuracy of our approximate solution and the approximation error measuring the distance between the true occupancy measure and its best linear approximation using the feature map ?:</p><formula xml:id="formula_33">L ?t (? t ) = L ?t (? t ) -L ?t (? * (? t )) statistical error + L ?t (? * (? t )) approximation error</formula><p>, where we use the shorthand notation ?t = ??t and ??t is the output of Algorithm 2 after K iterations. We assume that both the statistical and approximation errors are uniformly bounded along the iterates of our algorithm. Such assumptions have been considered for instance in a different context in the compatible function approximation framework (see Assumptions 6.1.1 and Corollary 21 in <ref type="bibr" target="#b0">Agarwal et al. (2021)</ref>, also Assumptions 1 and 5 in Yuan et al. ( <ref type="formula">2023</ref>)).</p><p>Assumption 5.2 (Bounded statistical error). There exists ? stat &gt; 0 s.t. for all iterations t ? 0 of Algorithm 3, we have</p><formula xml:id="formula_34">E[L ?t (? ?t ) -L ?t (? * (? t ))] ? ? stat .</formula><p>We will see in the next section that we can guarantee ? stat = O(1/K) where K is the number of iterations of SGD (Algorithm 2) to find the approximate solution ?t at each iteration t of Algorithm 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Assumption 5.3 (Bounded approximation error). There exists ?</head><formula xml:id="formula_35">approx &gt; 0 s.t. for all iterations t ? 0 of Algorithm 3, we have E[L ?t (? * (? t ))] ? ? approx .</formula><p>This error is due to function approximation and depends on the expressiveness of the approximating function class. The true state-action occupancy measure to be estimated may not lie in the function approximation class under consideration.</p><p>Theorem 5.4. Let Assumptions 4.1, 4.2, 5.2 and 5.3 hold true. In addition, suppose that there exists ? min &gt; 0 s.t. ?(s) ? ? min for all s ? S . Let T ? 1 be an integer and let (? t ) be the sequence generated by Algorithm 3 with a positive step size ? = O(1) and batch size N ? 1. Then,</p><formula xml:id="formula_36">E[?? ? F (?( ?T ))? 2 ] ? O 1 T + O 1 N + O(? 2H ) + O(? stat + ? approx ) , (<label>13</label></formula><formula xml:id="formula_37">)</formula><formula xml:id="formula_38">where ?T ? {? 1 , ? ? ? , ? T } uniformly at random.</formula><p>A few comments are in order regarding Theorem 5.4 :</p><p>(1) The specific structure of the softmax parametrization is not needed for Theorem 5.4. Indeed, this softmax parametrization is only useful to control IS weights used for variance reduction in Algorithm 1. Assumption 4.1 can be replaced by any smooth policy parametrization satisfying the same standard conditions with ? log ? ? instead of ? ;</p><p>(2) If the true (truncated) occupancy measure does not lie in the class of linear functions described, a positive function approximation error ? approx is incurred due to the bias induced by the limited expressiveness of the linear function approximation. A possible natural alternative is to consider richer classes such as neural networks to approximate the state-action occupancy measure and reduce the approximation bias. In this more involved case, the expected least squares (or other metrics) regression loss would likely become nonconvex and introduce further complications in our analysis. Such an extension would require other technical tools that are beyond the scope of the present paper and we leave it for future work.</p><p>In order to establish the total sample complexity of our algorithm, we need to compute the number of samples needed in the occupancy measure estimation subroutine of Algorithm 2. To do so, we now specify the number of SGD iterations required in Algorithm 2 to approximately solve our regression problem. In particular, we will show that we can achieve ? stat = O(1/K) where K is the number of iterations of the SGD subroutine using Theorem 1 in Bach &amp; Moulines (2013). Before stating our result, we make an additional standard assumption on the feature map ? .</p><p>Assumption 5.5. The feature map ? :</p><formula xml:id="formula_39">S ? A ? R m satisfies: (i) There exists B &gt; 0 s.t. for all s ? S, a ? A, ??(s, a)? ? B and (ii) There exists ? &gt; 0 s.t. E s??,a?U (A) [?(s, a)?(s, a) T ] ? ?I m where I m ? R m?m</formula><p>is the identity matrix.</p><p>Assumption 5.5 guarantees that the covariance matrix of the feature map is invertible. Similar standard assumptions have been commonly considered for linear function approximation settings <ref type="bibr" target="#b42">(Tsitsiklis &amp; Van Roy, 1997)</ref>.</p><p>We are now ready to state a corollary of Theorem 5.4 establishing the total sample complexity of Algorithm 3 to achieve an ?-stationary point of the objective function.</p><p>Corollary 5.6. Let Assumptions 4.1, 4.2, 5.3 and 5.5 hold in the setting of Theorem 5.4 where we run the SGD subroutine of Algorithm 2 with step size ? = 1/8B 2 and ? 0 = 0 for K iterations at each timestep t of Algorithm 3. Then, for every ? &gt; 0, setting</p><formula xml:id="formula_40">T = O(? -2 ), N = O(? -2 ), K = O(? -2 ) and H = O(log( 1 ? )) guarantees that E[?? ? F (?( ?T ))?] ? O(?) + O( ? ? approx ) where ?T ? {? 1 , ? ? ? , ? T } uniformly at random. The total sample com- plexity to reach an ?-stationary point (up to the O( ? ? approx ) error floor) is given by T ? (K + M ) ? H = ?(? -4 ) .</formula><p>In terms of the target accuracy ?, this result matches the optimal sample complexity to obtain an ?-FOSP for nonconvex smooth stochastic optimization via SGD (without variance reduction) up to a log factor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Numerical Simulations</head><p>In this section, we present two simple numerical experiments to illustrate the performance of our algorithm compared to prior work and complement our theoretical contri- butions. Our implementation is based on the code provided in Zhang et al. (2021b). <ref type="foot" target="#foot_6">6</ref> Our goal is to show that our algorithm can be competitive compared to existing algorithms while gaining simplicity. We leave further experimental investigations in larger scale problems for future work. </p><formula xml:id="formula_41">F (?) def = s?S log a?A ? s,a + ? ,</formula><p>where ? is a small constant which we set to ? = 0.125. We test our algorithm in the FrozenLake8x8 benchmark environment available in OpenAI gym <ref type="bibr" target="#b7">(Brockman et al., 2016)</ref>. The result of the experiment is illustrated in Figure <ref type="figure" target="#fig_1">1</ref>  (b) Standard RL. While the focus of our work is on the general utility case beyond the standard RL setting, we also perform simulations for the particular case where the objective is a linear function of the state action occupancy measure (i.e., the standard cumulative reward setting) in the CartPole benchmark environment <ref type="bibr" target="#b7">(Brockman et al., 2016)</ref>.</p><p>Figure <ref type="figure" target="#fig_1">1</ref> (left) shows that our algorithm is competitive with TSIVR-PG (actually even slightly faster, see between 250-500 episodes and see also the shaded areas) and all other algorithms which are not designed for the general utility case (REINFORCE <ref type="bibr" target="#b44">(Williams, 1992)</ref>, SVRPG <ref type="bibr">(Xu et al., 2020b)</ref>, SRVR-PG <ref type="bibr">(Xu et al., 2020a)</ref>, HSPGA <ref type="bibr" target="#b34">(Pham et al., 2020)</ref>) while gaining simplicity compared to existing variancereduced methods. Indeed, our algorithm is single-loop and does not require two distinct batch sizes and checkpoints nor does it require bounded importance sampling weights. Hyperparameters of the algorithms are tuned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Perspectives</head><p>Compared to the standard RL setting, the general utilities setting is much less studied. A better understanding of the hidden convexity structure of the problem and its interplay with general policy parametrization would be interesting to derive global optimality guarantees under milder assumptions which would accommodate more practical and expressive policy parametrizations such as neural networks. Regarding the case of large state action spaces, future avenues of research include designing more efficient procedures and guarantees for approximating and estimating the occupancy measure to better address the curse of dimensionality as well as investigating the dual point of view for designing more efficient algorithms. Addressing the case of continuous state-action spaces is also an interesting research direction. In this section, we report the special case of Algorithm 3 for the standard cumulative sum of rewards setting.</p><formula xml:id="formula_42">Algorithm 4 N-VR-PG (Standard Cumulative Reward) Input: ? 0 , T , H, {? t } t?0 , {? t } t?0 . Sample ? 0 of length H from M d 0 = g(? 0 , ? 0 ) ? 1 = ? 0 + ? 0 d0 ?d0? for t = 1, . . . , T -1 do Sample ? t of length H from M and ? ?t v t = g(? t , ? t ) -w(? t |? t-1 , ? t )g(? t , ? t-1 ) d t = ? t g(? t , ? t ) + (1 -? t )(d t-1 + v t ) ? t+1 = ? t + ? t dt ?dt?</formula><p>end for Remark A.1. If the direction d t in Algorithms 1 and 4 is null, then we formally take ? t+1 = ? t . Note that d t ? = 0 with probability 1 in general. Observe for instance that with ? t = 1, d t = 0 means that the stochastic policy gradient is equal to zero which means we are already at a first-order stationary point in expectation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Dependence on (1 -?) -1</head><p>In this section, we discuss the dependence of our convergence guarantee in Theorem 4.4 on (1 -?) -1 where ? is the discount factor of the MDP. The dependence on 1 -? of our proposed method is ?((1 -?) -6 ? -3 ) compared to ?((1 -?) -25 ? -3 ) for TSIVR-PG of <ref type="bibr">(Zhang et al., 2021b)</ref>.</p><p>Dependence on (1 -?) -1 of our N-VR-PG algorithm. It follows from Theorem 4.4 by setting ? 0 = (1 -?) 2 that we need ?((1 -?) -6 ? -3 ) samples to reach an ?-stationary point of the utility function (i.e., E?? ? F (?(? out ))? ? ?).</p><p>Derivation of explicit dependence on (1 -?) -1 in Theorem 5.9 of <ref type="bibr">(Zhang et al., 2021b)</ref>. Although the dependence is not made explicit in the aforementioned work, we can use their intermediate results in the proofs in order to derive it. We use their notations in the following.</p><p>From the last two lines of page 26 (in the proof of Theorem 5.9), we can infer that</p><formula xml:id="formula_43">E [?G(? out )?] ? O(C 3 ?), where C 3 = O(H(1 -?) -7</formula><p>) which is defined in the statement of Lemma F.2 on page 23. Here in O we only hide the dependence on the smoothness constants and other numerical constants. The statement of Theorem 5.9 guarantees that in order to achieve this, we need T (mB + N )H = O(H? -3 ) number of samples. By setting</p><formula xml:id="formula_44">? 1 = C 3 ?, this translates to O(HC 3 3 ? -3 1 ) = O(H 4 (1 -?) -21 ? -3 1 ) = ?((1 -?) -25 ? -3 1 ) samples to achieve E [?G(? out )?] ? ? 1</formula><p>, where in the last step we used the expression of H = 2 1-? log(1/?). Moreover, if we translate this guarantee to a more standard stationarity measure E [?? ? F (?(?))?], the dependence on (1 -?) -1 may further degrade for TSIVR-PG, see Lemma 5.4 in <ref type="bibr">(Zhang et al., 2021b)</ref> where they establish</p><formula xml:id="formula_45">E[?? ? F (?(?))?] = O(? -1 E [?G(?)?]) and ? = O(H -1 ) = ?(1 -?)</formula><p>is the truncation parameter. Indeed, their convergence result is stated in terms of the gradient mapping (because their algorithm uses a truncation mechanism with hyperparameter ?) and then translated to the standard first-order stationarity measure we use in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Further discussion of Assumption 4.10</head><p>A few comments are in order regarding Assumption 4.10:</p><p>1. In Assumption 4.10, the uniformity of the Lipschitz constant l (independent of ?) and ? is important. Without this requirement, for instance, for item (iii), the existence of ?? &gt; 0 (depending on ?) with the desired property for every ? ? R d is always guaranteed since V ?(?) is an open set. </p><formula xml:id="formula_46">w(? |? ? , ?) ? exp{2Hl ? ?? -? ? ?} . (<label>14</label></formula><formula xml:id="formula_47">)</formula><p>It suffices to observe that ?? t+1 -? t ? = ? t with the normalized update rule to prove that for every integer t and any trajectory ? of length H, we have w(? |? t , ? t+1 ) ? exp{2Hl ? ? t } . This proves the first part of the result.</p><p>Combining this first part with Lemma B.1 in <ref type="bibr">Xu et al. (2020a)</ref>, we obtain the desired fine-grained control of the IS weights variance. Specifically, if ? t+1 is a trajectory of length H generated following the initial distribution ? and policy ? ?t+1 , then</p><formula xml:id="formula_48">E[w(? t+1 |? t , ? t+1 )] = 1 ,<label>(15)</label></formula><formula xml:id="formula_49">Var [w(? t+1 |? t , ? t+1 )] ? C w ? 2 , (<label>16</label></formula><formula xml:id="formula_50">)</formula><p>where</p><formula xml:id="formula_51">C w def = H((8H + 2)l 2 ? + 2L ? )(W + 1) .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Proofs for Section 4.2: First-order stationarity</head><p>From the technical point of view, our proofs for the general utility setting depart from the proof techniques of <ref type="bibr">Zhang et al. (2021b)</ref> in several ways although we share several common points. First, normalized policy gradient requires an adequate ascent-like lemma which is different from the analysis of gradient truncation mechanism. Second, our algorithm uses a different variance reduction scheme which does not require checkpoints and consists of a single loop. This requires careful changes in the proof. Compared to standard STORM variance reduction proofs in stochastic optimization <ref type="bibr" target="#b8">(Cutkosky &amp; Orabona, 2019)</ref>, our setting involves two estimators which are intertwined, namely the state-action occupancy measure estimate and the stochastic policy gradient. This makes the analysis more complex and we refer the reader to the decomposition in ( <ref type="formula" target="#formula_66">24</ref>) and the subsequent lemmas to observe this. In contrast, STORM only involves the stochastic gradient and uses a different update rule. More broadly, we believe the techniques we use here could also be useful for stochastic composite optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1. Proof sketch</head><p>For the convenience of the reader, we highlight the main steps of the proof in this subsection before diving into the full detailed proof. The main steps consist in:</p><p>(a) showing an ascent-like lemma on the general utility function (see Lemma E.1). Notice that our algorithm is not a standard policy gradient algorithm but features normalization which requires a particular treatment;</p><p>(b) controlling the variance error due to two coupled stochastic estimates: the stochastic estimates of the state-action occupancy measures (for distinct policy parameters) and the stochastic policy gradients. Controlling these coupled estimates in our single-loop batch-free algorithm constitutes one of the main challenges of the proofs. More precisely, we use the following steps:</p><p>(i) We decompose the overall stochastic policy gradients errors in estimating the true policy gradients in (24) into two errors (see ( <ref type="formula" target="#formula_68">25</ref>)): the error due to the state action occupancy measure estimation (which provides an estimate of the reward sequence) and the error due to the policy gradient given an estimate of the reward sequence. (ii) We control each one of the aforementioned errors by establishing recursions in Lemma E.5 and Lemma E.8</p><p>respectively. Then, we solve the resulting recursions in the second parts of the lemmas. (iii) We sum up each one of the expected errors over time in Lemma E.6 and Lemma E.6 and we obtain an estimation of the overall error in Lemma E.10 by combining both errors using Lemma E.4.</p><p>Further technical steps needed are described in the full proof (see for e.g. Lemma E.7).</p><p>(c) incorporating the estimates obtained in the second step to the descent lemma and telescoping the obtained inequality to derive our final convergence guarantee (see the proof of Theorem E.11 for the concluding steps).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2. Proof of Theorem 4.4 (General utilities setting)</head><p>In this section, we provide a proof for the case of general utilities. Notice that the case of cumulative rewards is a particular case. The first lemma is a an ascent-like lemma which follows from smoothness of the objective function. Before stating the lemma, we define the error sequence (e t ) for every integer t as follows:</p><formula xml:id="formula_52">e t def = d t -? ? F (? H (? t )) . (<label>17</label></formula><formula xml:id="formula_53">)</formula><p>Lemma E.1. Let Assumptions 4.1 and 4.2 hold true. Then, the sequence (? t ) generated by Algorithm 1 and the sequence (e t ) satisfy for every integer t ? 0,</p><formula xml:id="formula_54">F (?(? t+1 )) ? F (?(? t )) + ? t 3 ?? ? F (?(? t ))? -2? t ?e t ? - 4 3 D ? ? H ? t - L ? 2 ? 2 t .<label>(18)</label></formula><p>Proof. Since the objective function ? ? F (?(?)) is L ? -smooth by Lemma H.1, we obtain the following by using the update rule of the sequence (? t ):</p><formula xml:id="formula_55">F (?(? t+1 )) ? F (?(? t )) + ?? ? F (?(? t )), ? t+1 -? t ? - L ? 2 ?? t+1 -? t ? 2 = F (?(? t )) + ? t ?? ? F (?(? t )), d t ?d t ? ? - L ? 2 ? 2 t = F (?(? t )) + ? t ?? ? F (? H (? t )), d t ?d t ? ? + ? t ?? ? F (?(? t )) -? ? F (? H (? t )), d t ?d t ? ? - L ? 2 ? 2 t ? F (?(? t )) + ? t ?? ? F (? H (? t )), d t ?d t ? ? -? t ?? ? F (?(? t )) -? ? F (? H (? t ))? - L ? 2 ? 2 t (a) ? F (?(? t )) + ? t ?? ? F (? H (? t )), d t ?d t ? ? -? t D ? ? H - L ? 2 ? 2 t ,<label>(19)</label></formula><p>where (a) follows from Lemma H.2-(i).</p><p>Then we control the scalar product term. We distinguish two different cases:</p><p>Case 1: ?e t ? ? 1 2 ?? ? F (? H (? t ))? . In this case, we have</p><formula xml:id="formula_56">?? ? F (? H (? t )), d t ?d t ? ? = 1 ?d t ? (?? ? F (? H (? t ))? 2 + ?? ? F (? H (? t )), e t ?) ? 1 ?d t ? (?? ? F (? H (? t ))? 2 -?? ? F (? H (? t ))? ? ?e t ?) = 1 ?d t ? ?? ? F (? H (? t ))? ? (?? ? F (? H (? t ))? -?e t ?) ? 1 3 ?? ? F (? H (? t ))? ,</formula><p>where the last inequality follows from observing that</p><formula xml:id="formula_57">?d t ? ? ?e t ? + ?? ? F (? H (? t ))? ? 3 2 ?? ? F (? H (? t ))? . Case 2: ?e t ? ? 1 2 ?? ? F (? H (? t ))? .</formula><p>In this case, we simply have</p><formula xml:id="formula_58">?? ? F (? H (? t )), d t ?d t ? ? ? -?? ? F (? H (? t ))? ? -2?e t ? .</formula><p>Combining both cases, we obtain:</p><formula xml:id="formula_59">? t ?? ? F (? H (? t )), d t ?d t ? ? ? ? t 3 ?? ? F (? H (? t ))? -2? t ?e t ? ? ? t 3 ?? ? F (?(? t ))? - ? t 3 D ? ? H -2? t ?e t ? .<label>(20)</label></formula><p>Combining ( <ref type="formula" target="#formula_55">19</ref>) and ( <ref type="formula" target="#formula_59">20</ref>), we get</p><formula xml:id="formula_60">F (?(? t+1 )) ? F (?(? t )) + ? t 3 ?? ? F (?(? t ))? -2? t ?e t ? - 4 3 D ? ? H ? t - L ? 2 ? 2 t ,</formula><p>which completes the proof.</p><p>We now proceed with some preliminary results in order to control the error term ?e t ? in expectation.</p><p>The next lemma appeared in Prop. E.1 <ref type="bibr">(Zhang et al., 2021b)</ref>. Remark E.2. With a slight abuse of notation, ? ? ? ? means that the trajectory ? (of length H) is sampled from the MDP controlled by the policy ? ? . We adopt this notation to highlight the dependence on the parametrized policy ? ? , the MDP being fixed in the problem formulation.</p><p>Lemma E.3. For any reward vector r ? R |S|?|A| , we have</p><formula xml:id="formula_61">E ? ?? ? [?(? )] = ? H (?) ,<label>(21)</label></formula><formula xml:id="formula_62">E ? ?? ? [g(?, ?, r)] = [? ? ? H (?)] T r . (<label>22</label></formula><formula xml:id="formula_63">)</formula><p>In particular, under Assumption 4.2,</p><formula xml:id="formula_64">E ? ?p(?|? ? ) [g(?, ?, ? ? F (? H (?)))] = [? ? ? H (?)] T ? ? F (? H (?)) = ? ? F (? H (?)) . (<label>23</label></formula><formula xml:id="formula_65">)</formula><p>Proof. The proof follows from the definitions of the estimators ?(? ) and g(?, ?, r) in Eqs. ( <ref type="formula" target="#formula_10">8</ref>)-( <ref type="formula" target="#formula_7">7</ref>) and the definition of the truncated state-action occupancy measure ? H (?) (see Eq.( <ref type="formula" target="#formula_0">1</ref>)) as well as the policy gradient theorem in Eq. ( <ref type="formula">5</ref>).</p><p>In view of controlling the error sequence (e t ), we first observe the following decomposition:</p><formula xml:id="formula_66">e t = d t -? ? F (? H (? t )) = d t -[? ? ? H (? t )] T r t-1 + [? ? ? H (? t )] T (r t-1 -? ? F (? H (? t ))) = d t -[? ? ? H (? t )] T r t-1 + [? ? ? H (? t )] T (? ? F (? t-1 ) -? ? F (? H (? t ))) . (<label>24</label></formula><formula xml:id="formula_67">)</formula><p>Given the previous decomposition, we define two useful additional notations:</p><formula xml:id="formula_68">?t def = d t -[? ? ? H (? t )] T r t-1 ,<label>(25)</label></formula><formula xml:id="formula_69">?t def = ? t -? H (? t ) .<label>(26)</label></formula><p>Using these notations, we establish the following result relating the error ?e t ? 2 to the errors ?? t ? 2 and ?? t ? 2 in expectation.</p><p>Lemma E.4. Let Assumptions 4.1 and 4.2 hold true. Then we have for every integer t ? 1,</p><formula xml:id="formula_70">E[?e t ?] ? E[?? t ?] + C 1 E[?? t-1 ?] + C 2 ? t-1 ,<label>(27)</label></formula><p>where</p><formula xml:id="formula_71">C 1 def = 2L 2 ? l ? (1-?) 2 and C 2 def = 2L ? L ?,? l ? (1-?) 2 .</formula><p>Proof. It follows from the decomposition in ( <ref type="formula" target="#formula_66">24</ref>) and the definitions ( <ref type="formula" target="#formula_68">25</ref>)-( <ref type="formula" target="#formula_69">26</ref>) that</p><formula xml:id="formula_72">E[?e t ?] ? E[?? t ?] + E[?[? ? ? H (? t )] T (? ? F (? t-1 ) -? ? F (? H (? t )))?] . (<label>28</label></formula><formula xml:id="formula_73">)</formula><p>We now control the second term in the above inequality. The following step is similar to the treatment in (Zhang et al., 2021b)(Eq.( <ref type="formula" target="#formula_54">18</ref>)). Indeed, the policy gradient theorem (see ( <ref type="formula">5</ref>)) yields</p><formula xml:id="formula_74">[? ? ? H (? t )] T (? ? F (? t-1 ) -? ? F (? H (? t ))) = E ? ? H-1 t ? =0 ? t ? [? ? F (? t-1 ) -? ? F (? H (? t ))]s t ? ,a t ? ? ? ? t ? h=0 ? ? log ? ? (a h , s h ) ? ? ? ? As a consequence, ?[? ? ? H (? t )] T (? ? F (? t-1 ) -? ? F (? H (? t )))? ? E ? ? H-1 t ? =0 ? t ? ?? ? F (? t-1 ) -? ? F (? H (? t ))?? t ? h=0 ? ? log ? ? (a h , s h ) ? ? .<label>(29)</label></formula><p>Then, using Assumption 4.2, we have</p><formula xml:id="formula_75">?? ? F (? t-1 ) -? ? F (? H (? t ))? ? ? ?? ? F (? t-1 ) -? ? F (? H (? t-1 ))? ? + ?? ? F (? H (? t-1 )) -? ? F (? H (? t ))? ? ? L ? ?? t-1 -? H (? t-1 )? + L ?,? ?? H (? t-1 ) -? H (? t )? 1 ? L ? ?? t-1 ? + L ?,? ?? t -? t-1 ? ,<label>(30)</label></formula><p>where the last inequality follows from Lemma H.1-(ii). Plugging this inequality in (29) and using Lemma H.1-(i) yields</p><formula xml:id="formula_76">?[? ? ? H (? t )] T (? ? F (? t-1 ) -? ? F (? H (? t )))? ? E H-1 t ? =0 2(t ? + 1)l ? ? t ? (L ? ?? t-1 ? + L ?,? ?? t -? t-1 ?) = H-1 t ? =0 2(t ? + 1)l ? ? t ? L ? (L ? E[?? t-1 ?] + L ?,? ? t-1 ) ? 2L ? l ? (1 -?) 2 (L ? E[?? t-1 ?] + L ?,? ? t-1 ) . (<label>31</label></formula><formula xml:id="formula_77">)</formula><p>Hence, taking the total expectation, we obtain</p><formula xml:id="formula_78">E[?[? ? ? H (? t )] T (? ? F (? t-1 ) -? ? F (? H (? t )))?] ? 2L 2 ? l ? (1 -?) 2 E[?? t-1 ?] + 2L ? L ?,? l ? (1 -?) 2 ? t-1 .<label>(32)</label></formula><p>Combining ( <ref type="formula" target="#formula_72">28</ref>) and (32) yields the desired inequality.</p><p>We now control each one of the errors in the right-hand side of the previous lemma in what follows. We start with the error ?t (see ( <ref type="formula" target="#formula_69">26</ref>)) induced by the (truncated) state-action occupancy measure estimation.</p><p>Lemma E.5. Let Assumption 4.1 hold. Then, for every integer t ? 1, if ? t ? [0, 1] we have</p><formula xml:id="formula_79">E[?? t ? 2 ] ? (1 -? t )E[?? t-1 ? 2 ] + 2C w (1 -?) 2 ? 2 t-1 + 2 (1 -?) 2 ? 2 t ,<label>(33)</label></formula><p>where we recall that ?t = ? t -? H (? t ) and C w = H((8H + 2)l 2 ? + 2L ? )(W + 1) as defined in Lemma 4.3. Moreover, (i) if ? t = 2 t+1 , then for all integers t ? 1, we have</p><formula xml:id="formula_80">E[?? t ?] ? 4 (1 -?) ? t ? t 1 /2 + 2C 1 /2 w (1 -?) ? t-1 ? t 1 /2 . (<label>34</label></formula><formula xml:id="formula_81">) (ii) if ? t = 2 t+1 q and ? t = ? 2 t+1 p</formula><p>for some reals ? &gt; 0, q ? (0, 1), p ? 0 and all integers t ? 1, then we have</p><formula xml:id="formula_82">E ?? t ? 2 ? (2C + 1)? t+1 (1 -?) 2 + 2CC w (1 -?) 2 ? 2 t ? -1 t+1 ,<label>(35)</label></formula><p>where C &gt; 0 is an absolute numerical constant.</p><p>Proof. We start with the proof of (33). Using the update rule of the sequence (? t ) in Algorithm 1, we first derive a recursion on the error sequence ?t from the following decomposition:</p><formula xml:id="formula_83">?t = ? t -? H (? t ) = ? t ?(? t ) + (1 -? t )(? t-1 + u t ) -? H (? t ) = (1 -? t )? t-1 + (1 -? t )(? H (? t-1 ) + u t ) + ? t (?(? t ) -? H (? t )) -(1 -? t )? H (? t ) = (1 -? t )? t-1 + (1 -? t )z t + ? t ?t ,</formula><p>where</p><formula xml:id="formula_84">?t def = ?(? t ) -? H (? t ) ,<label>(36)</label></formula><formula xml:id="formula_85">zt def = u t -(? H (? t ) -? H (? t-1 )) = ?(? t )(1 -w(? t |? t-1 , ? t )) -(? H (? t ) -? H (? t-1 )) .<label>(37)</label></formula><p>Using these notations, we have</p><formula xml:id="formula_86">E[?? t ? 2 ] = (1 -? t ) 2 E[?? t-1 ? 2 ] + E[?(1 -? t )z t + ? t ?t ? 2 ] + E[?(1 -? t )? t-1 , (1 -? t )z t + ? t ?t ?] .<label>(38)</label></formula><p>Then, we notice that the scalar product term is equal to zero. We consider for this the filtration (F t ) of ?-algebras defined s.t. for every integer t,</p><formula xml:id="formula_87">F t def = ?(? k , ? k : k ? t)</formula><p>where ? t is a (random) trajectory of length H generated following the policy ? ?t . This ?-algebra represents the history of all the random variables until time t. As a consequence, the random variable ?t being F t-1 -measurable, it follows from the tower property of the conditional expectation that</p><formula xml:id="formula_88">E[?(1 -? t )? t-1 , (1 -? t )z t + ? t ?t ?] = E[E[?(1 -? t )? t-1 , (1 -? t )z t + ? t ?t ?|F t-1 ]] = E[?(1 -? t )? t-1 , E[(1 -? t )z t + ? t ?t ?|F t-1 ]] = 0 ,<label>(39)</label></formula><p>where the last step stems from the fact that E[z t |F t-1 ] = E[? t |F t-1 ] = 0, recall for this that ? t ? ? ?t for every t and see the definitions ( <ref type="formula" target="#formula_84">36</ref>) and (37).</p><p>It follows from ( <ref type="formula" target="#formula_86">38</ref>) and ( <ref type="formula" target="#formula_88">39</ref>) that</p><formula xml:id="formula_89">E[?? t ? 2 ] ? (1 -? t ) 2 E[?? t-1 ? 2 ] + 2(1 -? t ) 2 E[?z t ? 2 ] + 2? 2 t E[?? t ? 2 ] .<label>(40)</label></formula><p>Then, we upperbound each one of the last two terms in (40). As for the first term, since E[z t ] = 0, we have the following</p><formula xml:id="formula_90">E[?z t ? 2 ] ? E[?u t ? 2 ] = E[??(? t )(1 -w(? t |? t-1 , ? t ))? 2 ] = E[(1 -w(? t |? t-1 , ? t )) 2 ??(? t )? 2 ] .<label>(41)</label></formula><p>Given the definition of ?(? t ) in ( <ref type="formula" target="#formula_10">8</ref>), we first observe that with probability one,</p><formula xml:id="formula_91">??(? t )? ? H-1 t=0 ? t ?? st,at ? = H-1 t=0 ? t ? 1 1 -? . (<label>42</label></formula><formula xml:id="formula_92">)</formula><p>Using Lemma 4.3 together with the previous bound, we get</p><formula xml:id="formula_93">E[(1 -w(? t |? t-1 , ? t )) 2 ??(? t )? 2 ] ? 1 (1 -?) 2 E[(1 -w(? t |? t-1 , ? t )) 2 ] = 1 (1 -?) 2 Var [w(? t |? t-1 , ? t )] ? C w (1 -?) 2 ? 2 t-1 .</formula><p>(43) We deduce from ( <ref type="formula" target="#formula_90">41</ref>) and ( <ref type="formula">43</ref>) together that</p><formula xml:id="formula_94">E[?z t ? 2 ] ? C w (1 -?) 2 ? 2 t-1 . (<label>44</label></formula><formula xml:id="formula_95">)</formula><p>Regarding the last term in (40), since E[? t ] = 0, we observe that</p><formula xml:id="formula_96">E[?? t ? 2 ] ? E[??(? t )? 2 ] ? 1 (1 -?) 2 , (<label>45</label></formula><formula xml:id="formula_97">)</formula><p>where the last inequality stems from (42). Incorporating ( <ref type="formula" target="#formula_94">44</ref>) and ( <ref type="formula" target="#formula_96">45</ref>) into (40) leads to the following inequality</p><formula xml:id="formula_98">E[?? t ? 2 ] ? (1 -? t ) 2 E[?? t-1 ? 2 ] + 2C w (1 -?) 2 (1 -? t ) 2 ? 2 t-1 + 2 (1 -?) 2 ? 2 t ,<label>(46)</label></formula><p>which concludes the proof of the first since ? t ? [0, 1].</p><p>Proof of (34): In order to derive (34), we apply Lemma H.4 with</p><formula xml:id="formula_99">? t = 2 t+1 , ? t = 2 (1-?) 2 ? 2 t + 2Cw (1-?) 2 ? 2 t-1 . Using E[?? 0 ? 2 ] ? E[??(? t )? 2 ] ? 1 (1-?) 2 , we derive E[?? t ?] ? E[?? t ? 2 ] 1 2 ? 4 (1 -?) 2 (t + 1) 2 + 2 (1 -?) 2 ? 2 t ? t + 2C w (1 -?) 2 ? 2 t-1 ? t 1 /2 ? 2 (1 -?)(t + 1) + 2 (1 -?) ? t ? t 1 /2 + 2C 1 /2 w (1 -?) ? t-1 ? t 1 /2 ? 4 (1 -?) ? t ? t 1 /2 + 2C 1 /2 w (1 -?) ? t-1 ? t 1 /2 . (<label>47</label></formula><formula xml:id="formula_100">)</formula><p>Proof of (35): Let ? t = 2 t+1 q for some q ? (0, 1). In order to derive (35), we unroll the recursion (33) from t = 1 to</p><formula xml:id="formula_101">t = t ? where t ? ? T -1. Denoting ? t = 2 (1-?) 2 ? 2 t + 2Cw (1-?) 2 ? 2 t-1 , we have E ?? t ? ? 2 ? t ? ? =1 (1 -? ? )E ?? 0 ? 2 + t ? t=1 ? t t ? ? =t+1 (1 -? ? ) ? ? t ? +1 (1 -?) 2 + C? t ? +1 ? -1 t ? +1 ,<label>(48)</label></formula><p>where we used E ?? 0 ? 2 ? E ??(? t )? 2 ? 1 (1-?) 2 and the results of Lemmas H.5-H.6 with C &gt; 1 being a numerical constant.</p><p>Lemma E.6. Let Assumption 4.1 hold. Let ? 0 &gt; 0 and consider an integer T ? 1 . Set ? t = 2 t+1 2 /3 and ? t = ?0 T 2 /3 for every nonzero integer t ? T . Then, we have</p><formula xml:id="formula_102">1 T T t=1 E[?? t ?] ? C 1 + C 1 /2 w ? 0 (1 -?) 1 T 1 /3 , (<label>49</label></formula><formula xml:id="formula_103">)</formula><p>where C &gt; 0 is an absolute numerical constant.</p><p>Proof. Summing up inequality (35) from Lemma E.6 from t = 1 to t = T and choosing ? t = ? = ?0 T 2 /3 , we obtain</p><formula xml:id="formula_104">1 T T t=1 E [?? t ?] ? 1 T T t=1 E ?? t ? 2 1 /2 ? 1 T T t=1 E ?? t ? 2 1 /2 ? 1 T T t=1 ? t+1 (1 -?) 2 + C 2 (1 -?) 2 ? t+1 + 2C w (1 -?) 2 ? 2 ? -1 t+1 1 /2 (i) ? 3? T -1 (1 -?) 2 + 6C? T -1 (1 -?) 2 + 2CC w (1 -?) 2 ? 2 ? T +1 1 /2 ? 9C? T -1 (1 -?) 2 + 2CC w (1 -?) 2 ? 2 ? T +1 1 /2 ? 18C (1 -?) 2 T 2 /3 + 3CC w (1 -?) 2 ? 2 0 T 2 /3 1 /2 ? 12C 1 /2 1 + C 1 /2 w ? 0 (1 -?) 1 T 1 /3 . (<label>50</label></formula><formula xml:id="formula_105">)</formula><p>where (i) follows from observing that</p><formula xml:id="formula_106">T t=1 ? t+1 ? 2 2 3 T t=1 1 (t + 2) 2 3 ? 2 2 3 T t=1 1 (t + 1) 2 3 ? 3 ? 2 2 3 T 1 3 = 3 T 2 2 3 T 2 3 = 3 T ? T -1 .<label>(51)</label></formula><p>In view of controlling the error ?t , we first state a technical lemma that will be useful. This result controls the expected squared difference between two consecutive estimates of the (truncated) state-action occupancy measure.</p><p>Lemma E.7. Suppose Assumption 4.1 holds. Then for all integers t ? 1,</p><formula xml:id="formula_107">E[?? t-1 -? t ? 2 ] ? 3? 2 t (1 -? t ) 2 E[?? t ? 2 ] + 3? 2 t (1 -? t ) 2 (1 -?) 2 + 3C w (1 -?) 2 ? 2 t-1 ,<label>(52)</label></formula><p>where C w = H((8H + 2)l 2 ? + 2L ? )(W + 1) as defined in Lemma 4.3. Moreover, if in addition ? t = 2 t+1 q for some q ? [0, 1) then for every integer t ? 1,</p><formula xml:id="formula_108">E[?? t-1 -? t ? 2 ] ? 12C? 2 t (1 -?) 2 + 6C w (1 -?) 2 ? 2 t-1 ,<label>(53)</label></formula><p>where C &gt; 0 is a numerical constant.</p><p>Proof. Using the update rule of the truncated occupancy measure estimate sequence (? t ), we have</p><formula xml:id="formula_109">? t-1 -? t = ? t-1 -[? t ?(? t ) + (1 -? t )(? t-1 + u t )] = ? t (? t-1 -?(? t )) -(1 -? t )u t = ? t (? t-1 -? t ) + ? t (? t -?(? t )) -(1 -? t )u t .</formula><p>As a consequence, we have</p><formula xml:id="formula_110">? t-1 -? t = ? t 1 -? t (? t -?(? t )) -u t = ? t 1 -? t ?t + ? t 1 -? t (? H (? t ) -?(? t )) -u t . (<label>54</label></formula><formula xml:id="formula_111">)</formula><p>Taking expectation of the square of the previous identity, we obtain the following bound: <ref type="formula" target="#formula_90">41</ref>)-( <ref type="formula" target="#formula_94">44</ref>) and ( <ref type="formula" target="#formula_96">45</ref>) respectively. Incorporating these bounds into (55) yields:</p><formula xml:id="formula_112">E[?? t-1 -? t ? 2 ] ? 3? 2 t (1 -? t ) 2 E[?? t ? 2 ] + 3? 2 t (1 -? t ) 2 E[??(? t ) -? H (? t )? 2 ] + 3E[?u t ? 2 ] . (<label>55</label></formula><formula xml:id="formula_113">) Recall now that E[?u t ? 2 ] ? Cw (1-?) 2 ? 2 t-1 and E[??(? t ) -? H (? t )? 2 ] ? 1 (1-?) 2 from (</formula><formula xml:id="formula_114">E[?? t-1 -? t ? 2 ] ? 3? 2 t (1 -? t ) 2 E[?? t ? 2 ] + 3? 2 t (1 -? t ) 2 (1 -?) 2 + 3C w (1 -?) 2 ? 2 t-1 .</formula><p>This completes the proof of (52). We now set ? t = 2 t+1 q for some q ? (0, 1). By (76) in the proof of Lemma E.5, we have</p><formula xml:id="formula_115">E ?? t ? 2 ? ? t (1 -?) 2 + C? t ? -1 t ,<label>(56)</label></formula><p>where</p><formula xml:id="formula_116">? t = 2 (1-?) 2 ? 2 t + 2Cw (1-?) 2 ? 2 t-1</formula><p>, and C &gt; 0 is a numerical constant. Thus,</p><formula xml:id="formula_117">E[?? t-1 -? t ? 2 ] ? 3? 2 t (1 -? t ) 2 ? t (1 -?) 2 + C? t ? -1 t + 3? 2 t (1 -? t ) 2 (1 -?) 2 + 3C w (1 -?) 2 ? 2 t-1 ? 12C? 2 t (1 -?) 2 + 6C w (1 -?) 2 ? 2 t-1 .</formula><p>We are now ready to prove a recursive upper bound on the error sequence (? t ) defined in (25). Notice that this result is of the same flavor as Lemma E.5 which we already proved. In particular, the result illustrates a variance reduction effect stemming from the variance reduction updates used for both the stochastic policy gradients and the state-action occupancy measure estimates.</p><p>Lemma E.8. Suppose Assumptions 4.1 and 4.2 hold. Then, for every integer t ? 2,</p><formula xml:id="formula_118">E[?? t ? 2 ] ? (1 -? t ) 2 E[?? t-1 ? 2 ] + C 3 ? 2 t-1 + C 4 ? 2 t-2 , (<label>57</label></formula><formula xml:id="formula_119">)</formula><p>where</p><formula xml:id="formula_120">C 3 def = 288Cl 2 ? L 2 ? (1-?) 6 + 32l 2 ? l 2 ? (1-?) 4 , C 4 def = 12l 2 ? [(l 2 ? +L ? ) 2 +Cwl 2 ? ] (1-?) 4 + 144Cwl 2 ? L 2 ?</formula><p>(1-?) 6 , and C w = H((8H + 2)l 2 ? + 2L ? )(W + 1) as defined in Lemma 4.3. Moreover, (i) if ? t = 2 t+1 , then for all integers t ? 1, we have</p><formula xml:id="formula_121">E[?? t ?] ? 2 ? t + 1 + 2C 1 /2 3 ? t ? t 1 /2 + C (ii) if ? t = 2 t+1 q and ? t = ? 2 t+1 p</formula><p>for some reals ? &gt; 0, q ? (0, 1), p ? 0 and all integers t ? 1, then we have</p><formula xml:id="formula_122">E ?? t ? 2 ? ?2 ? t+1 + 2CC 3 ? t+1 + CC 4 ? 2 t-1 ? -1 t+1 ,<label>(59)</label></formula><p>where ? =</p><formula xml:id="formula_123">4l ? l ? (1-?) 2 .</formula><p>Proof. The first step of the proof consists in decomposing the error ?t in a suitable way using the update rule of the sequence (d t ) so that for every integer t ? 2,</p><formula xml:id="formula_124">?t = d t -[? ? ? H (? t )] T r t-1 = (1 -? t )(d t-1 + v t ) + ? t g(? t , ? t , r t-1 ) -[? ? ? H (? t )] T r t-1 = (1 -? t )(? t-1 + [? ? ? H (? t-1 )] T r t-2 + v t ) + ? t (g(? t , ? t , r t-1 ) -[? ? ? H (? t )] T r t-1 ) -(1 -? t )[? ? ? H (? t )] T r t-1 = (1 -? t )? t-1 + (1 -? t )? t + ? t ?t ,</formula><p>where</p><formula xml:id="formula_125">?t def = g(? t , ? t , r t-1 ) -[? ? ? H (? t )] T r t-1 ,<label>(60)</label></formula><formula xml:id="formula_126">?t def = v t -([? ? ? H (? t )] T r t-1 -[? ? ? H (? t-1 )] T r t-2 ) (61) = g(? t , ? t , r t-1 ) -[? ? ? H (? t )] T r t-1 + [? ? ? H (? t-1 )] T r t-2 -w(? t |? t-1 , ? t )g(? t , ? t-1 , r t-2 ) . (<label>62</label></formula><formula xml:id="formula_127">)</formula><p>Then we use similar derivations to ( <ref type="formula" target="#formula_86">38</ref>) and ( <ref type="formula" target="#formula_88">39</ref>). We consider again the same filtration (F t ) of ?-algebras where F t represents the randomness until time t (including time t and random trajectories ? t of length H generated by following the policy ? ?t ). Therefore, we have (see Lemma E.3)</p><formula xml:id="formula_128">E[? t |F t-1 ] = 0 ; E[? t |F t-1 ] = 0 .<label>(63)</label></formula><p>Note here as a comment that the reason why we used r t-1 instead of r t in Algorithm 1 (for the sequence (v t )) becomes clearer here in the previous identities: r t-1 is F t-1 -measurable unlike r t and this allows to obtain a null conditional expectation avoiding in particular dependency issues between r t and ? t .</p><p>Employing (63) and using the same derivations as in (39) leads to</p><formula xml:id="formula_129">E[?? t ? 2 ] = E[?(1 -? t )? t-1 + (1 -? t )? t + ? t ?t ? 2 ] = (1 -? t ) 2 E[?? t-1 ? 2 ] + E[?(1 -? t )? t + ? t ?t ? 2 ] ? (1 -? t ) 2 E[?? t-1 ? 2 ] + 2(1 -? t ) 2 E[?? t ? 2 ] + 2? 2 t E[?? t ? 2 ] .<label>(64)</label></formula><p>We now derive a bound for the term E[?? t ? 2 ] . Observe for this that</p><formula xml:id="formula_130">E[?? t ? 2 ] ? E[?g(? t , ? t , r t-1 ) -w(? t |? t-1 , ? t )g(? t , ? t-1 , r t-2 )? 2 ] = E[?g(? t , ? t , r t-1 ) -g(? t , ? t-1 , r t-1 ) + g(? t , ? t-1 , r t-1 ) -g(? t , ? t-1 , r t-2 ) + g(? t , ? t-1 , r t-2 )(1 -w(? t |? t-1 , ? t ))? 2 ] ? 3E[?g(? t , ? t , r t-1 ) -g(? t , ? t-1 , r t-1 )? 2 ] + 3E[?g(? t , ? t-1 , r t-1 ) -g(? t , ? t-1 , r t-2 )? 2 ] + 3E[(1 -w(? t |? t-1 , ? t )) 2 ?g(? t , ? t-1 , r t-2 )? 2 ] .<label>(65)</label></formula><p>Each term in this inequality is upper bounded separately in what follows.</p><formula xml:id="formula_131">Term 1: E[?g(? t , ? t , r t-1 ) -g(? t , ? t-1 , r t-1 )? 2 ]. Using Lemma H.3-(ii), we obtain ?g(? t , ? t , r t-1 ) -g(? t , ? t-1 , r t-1 )? ? 2(l 2 ? + L ? ) (1 -?) 2 ?r t-1 ? ? ? ?? t -? t-1 ? = 2(l 2 ? + L ? ) (1 -?) 2 ?? ? F (? t-1 )? ? ? ? t-1 ? 2(l 2 ? + L ? )l ? (1 -?) 2 ? t-1 ,<label>(66)</label></formula><p>where the last inequality stems from Assumption 4.2. We deduce from this the bound for the first term:</p><formula xml:id="formula_132">E[?g(? t , ? t , r t-1 ) -g(? t , ? t-1 , r t-1 )? 2 ] ? 4(l 2 ? + L ? ) 2 l 2 ? (1 -?) 4 ? 2 t-1 . (67) Term 2: E[?g(? t , ? t-1 , r t-1 ) -g(? t , ? t-1 , r t-2 )? 2 ]. Together with Assumption 4.2, Lemma H.3-(i) yields ?g(? t , ? t-1 , r t-1 ) -g(? t , ? t-1 , r t-2 )? ? 2l ? (1 -?) 2 ?r t-1 -r t-2 ? ? = 2l ? (1 -?) 2 ?? ? F (? t-1 ) -? ? F (? t-2 )? ? ? 2l ? L ? (1 -?) 2 ?? t-1 -? t-2 ? . (<label>68</label></formula><formula xml:id="formula_133">)</formula><p>Invoking Lemma E.7, we obtain from ( <ref type="formula" target="#formula_132">68</ref>)</p><formula xml:id="formula_134">E[?g(? t , ? t-1 , r t-1 ) -g(? t , ? t-1 , r t-2 )? 2 ] ? 4l 2 ? L 2 ? (1 -?) 4 E[?? t-1 -? t-2 ? 2 ] ? 4l 2 ? L 2 ? (1 -?) 4 12C? 2 t-1 (1 -?) 2 + 6C w (1 -?) 2 ? 2 t-2 ? 48l 2 ? L 2 ? (1 -?) 6 C? 2 t-1 + C w ? 2 t-2 . (<label>69</label></formula><formula xml:id="formula_135">)</formula><formula xml:id="formula_136">Term 3: E[(1 -w(? t |? t-1 , ? t )) 2 ?g(? t , ? t-1 , r t-2 )? 2 ].</formula><p>First, observe that</p><formula xml:id="formula_137">?g(? t , ? t-1 , r t-2 )? (a) = H-1 t=0 H-1 h=t ? h r t-2 (s h , a h ) ? log ? ? (a t |s t ) ? H-1 t=0 H-1 h=t ? h ?r t-2 ? ? ? ?? log ? ? (a t |s t )? (b) ? l ? H-1 t=0 H-1 h=t ? h ?? log ? ? (a t |s t )? (c) ? 2l ? l ? H-1 t=0 H-1 h=t ? h = 2l ? l ? H-1 h=0 h t=0 ? h ? 2l ? l ? H-1 h=0 (h + 1)? h ? 2l ? l ? (1 -?) 2 , (<label>70</label></formula><formula xml:id="formula_138">)</formula><p>where (a) follows from the expression of the stochastic policy gradient ( <ref type="formula" target="#formula_7">7</ref>), (b) stems from Assumption 4.2 and (c) is a consequence of Lemma H.1-(i).</p><p>Using Lemma 4.3 together with the previous bound yields</p><formula xml:id="formula_139">E[(1 -w(? t |? t-1 , ? t )) 2 ?g(? t , ? t-1 , r t-2 )? 2 ] ? 4l 2 ? l 2 ? (1 -?) 4 E[(1 -w(? t |? t-1 , ? t )) 2 ] = 4l 2 ? l 2 ? (1 -?) 4 Var [w(? t |? t-1 , ? t )] ? 4l 2 ? l 2 ? C w (1 -?) 4 ? 2 t-1 .<label>(71)</label></formula><p>Collecting ( <ref type="formula">67</ref>), ( <ref type="formula" target="#formula_134">69</ref>) and ( <ref type="formula" target="#formula_139">71</ref>) in ( <ref type="formula" target="#formula_130">65</ref>), we obtain</p><formula xml:id="formula_140">E[?? t ? 2 ] ? 12l 2 ? [(l 2 ? + L ? ) 2 + C w l 2 ? ] (1 -?) 4 ? 2 t-1 + 144l 2 ? L 2 ? (1 -?) 6 C? 2 t-1 + C w ? 2 t-2 ? 144Cl 2 ? L 2 ? (1 -?) 6 ? 2 t-1 + 12l 2 ? [(l 2 ? + L ? ) 2 + C w l 2 ? ] (1 -?) 4 + 144C w l 2 ? L 2 ? (1 -?) 6 ? 2 t-2 . (<label>72</label></formula><formula xml:id="formula_141">)</formula><p>We now bound the term E[?? t ? 2 ] in (64). First, recall from (60) that ?t = g(? t , ? t , r t-1 ) -[? ? ? H (? t )] T r t-1 . Then, with probability one,</p><formula xml:id="formula_142">?? t ? ? ?g(? t , ? t , r t-1 )? + ?[? ? ? H (? t )] T r t-1 ? (a) ? 2l ? l ? (1 -?) 2 + ?[? ? ? H (? t )] T r t-1 ? (b) ? 4l ? l ? (1 -?) 2 ,<label>(73)</label></formula><p>where (a) stems from the same bound as in ( <ref type="formula" target="#formula_137">70</ref>) and (b) also follows from a similar bound to (70). Indeed, notice using (5) that</p><formula xml:id="formula_143">?[? ? ? H (? t )] T r t-1 ? = E ? ? H-1 t ? =0 ? t ? r t-1 (s ? t , a ? t ) ? ? t ? h=0 ? log ? ? (a h |s h ) ? ? ? ? ? E ? ? H-1 t ? =0 ? t ? ?r t-1 ? ? t ? h=0 ?? log ? ? (a h |s h )? ? ? (a) ? 2l ? l ? H-1 t ? =0 (t ? + 1)? t ? ? 2l ? l ? (1 -?) 2 , (<label>74</label></formula><formula xml:id="formula_144">)</formula><p>where again (a) stems from Assumption 4.2 and Lemma H.1-(i).</p><p>We conclude from ( <ref type="formula" target="#formula_129">64</ref>), ( <ref type="formula" target="#formula_140">72</ref>) and ( <ref type="formula" target="#formula_142">73</ref>) that</p><formula xml:id="formula_145">E[?? t ? 2 ] ? (1-? t ) 2 E[?? t-1 ? 2 ]+ 288Cl 2 ? L 2 ? (1 -?) 6 + 32l 2 ? l 2 ? (1 -?) 4 ? 2 t-1 + 12l 2 ? [(l 2 ? + L ? ) 2 + C w l 2 ? ] (1 -?) 4 + 144C w l 2 ? L 2 ? (1 -?) 6 ? 2 t-2 ,</formula><p>where C w = H((8H + 2)l 2 ? + 2L ? )(W + 1) as defined in Lemma 4.3.</p><p>Proof of (58): In order to derive (58), we apply Lemma H.4 with</p><formula xml:id="formula_146">? t = 2 t+1 , ? t = C 3 ? 2 t-1 + C 4 ? 2 t-2 . Using E[?? 0 ? 2 ] ? ?2 , we derive E[?? t ?] ? E[?? t ? 2 ] 1 2 ? 4 ?2 (t + 1) 2 + 4C 3 ? 2 t ? t + C 4 ? 2 t-2 ? t 1 /2 ? 2 ? t + 1 + 2C 1 /2 3 ? t ? t 1 /2 + C 1 /2 4 ? t-2 ? t 1 /2 . (<label>75</label></formula><formula xml:id="formula_147">)</formula><p>Proof of (59): Let ? t = 2 t+1 q for some q ? (0, 1). In order to derive (59), we unroll the recursion from t = 1 to</p><formula xml:id="formula_148">t = t ? ? T . Denoting ? t = C 3 ? 2 t-1 + C 4 ? 2 t-2 , we derive E ?? t ? ? 2 ? ? ? t ? ? =1 (1 -? ? ) ? ? E ?? 0 ? 2 + t ? t=1 ? t t ? ? =t+1 (1 -? ? ) ? ?2 ? t ? +1 + C? t ? +1 ? -1 t ? +1 ? ?2 ? t ? +1 + CC 3 ? 2 t ? ? -1 t ? +1 + CC 4 ? 2 t ? -1 ? -1 t ? +1 ? ?2 ? t ? +1 + 2CC 3 ? t ? +1 + CC 4 ? 2 t ? -1 ? -1 t ? +1 . ,<label>(76)</label></formula><p>where we used the results of Lemmas H.5-H.6 and</p><formula xml:id="formula_149">E ?? 0 ? 2 ? ?2 with ? = 4l ? l ?</formula><p>(1-?) 2 , which can be derived similarly to (73).</p><p>In the next lemma, we derive an estimate of the average expected error E[?? t ?] from the recursion we have just established in Lemma E.8. Lemma E.9. Suppose Assumptions 4.1 and 4.2 hold. Let T ? 1 be an integer, let ? 0 &gt; 0 and set ? t = 2</p><formula xml:id="formula_150">t+1 2 /3 , ? t = ?0 T 2 /3</formula><p>for every integer t. Then</p><formula xml:id="formula_151">1 T T t=1 E [?? t ?] ? C ? + C 1 /2 3 + C 1 /2 4 ? 0 T 1 /3 , (<label>77</label></formula><formula xml:id="formula_152">)</formula><p>where ? =</p><formula xml:id="formula_153">4l ? l ? (1-?) 2 , C 3 = 288Cl 2 ? L 2 ? (1-?) 6 + 32l 2 ? l 2 ? (1-?) 4 , C 4 = 12l 2 ? [(l 2 ? +L ? ) 2 +Cwl 2 ? ] (1-?) 4 + 144Cwl 2 ? L 2 ? (1-?) 6 , C w = H((8H +2)l 2 ? +2L ? )(W + 1)</formula><p>as defined in Lemma 4.3, and C &gt; 1 is a numerical constant.</p><p>Proof. Summing up inequality (59) from Lemma E.8 from t = 1 to t = T and choosing ? t = ? = ?0 T 2 /3 , we obtain</p><formula xml:id="formula_154">1 T T t=1 E [?? t ?] ? 1 T T t=1 E ?? t ? 2 1 /2 ? 1 T T t=1 E ?? t ? 2 1 /2 ? 1 T T t=1 ?2 ? t+1 + 2CC 3 ? t+1 + CC 4 ? 2 ? -1 t+1 1 /2 (i) ? 3( ?2 + 2CC 3 )? T -1 + CC 3 ? 2 ? -1 T +1 1 /2 ? 12( ?2 + CC 3 ) T 2 /3 + 2CC 4 ? 2 0 T 2 /3 1 /2 ? 4C 1 /2 ? + C 1 /2 3 + C 1 /2 4 ? 0 T 1 /3</formula><p>, where (i) holds by ( <ref type="formula" target="#formula_106">51</ref>).</p><p>We are now ready to state the main lemma controlling the error sequence (e t ) in expectation as defined in (17).</p><p>Lemma E.10. Suppose Assumptions 4.1 and 4.2 hold. Let T ? 1 be an integer, let ? 0 &gt; 0 and set</p><formula xml:id="formula_155">? t = 2 t+1 2 /3 , ? t = ?0</formula><p>T 2 /3 for every integer t. Then</p><formula xml:id="formula_156">1 T T t=1 E[?e t ?] ? C ? + C 1 /2 3 + C 1 /2 4 ? 0 T 1 /3 + CC 1 1 + C 1 /2 w ? 0 (1 -?) 1 T 1 /3 + C 2 ? 0 T 2 /3 , (<label>78</label></formula><formula xml:id="formula_157">)</formula><p>where</p><formula xml:id="formula_158">C 1 = 2L 2 ? l ? (1-?) 2 and C 2 = 2L ? L ?,? l ? (1-?) 2 , C 3 = 288Cl 2 ? L 2 ? (1-?) 6 + 32l 2 ? l 2 ? (1-?) 4 , C 4 = 12l 2 ? [(l 2 ? +L ? ) 2 +Cwl 2 ? ] (1-?) 4 + 144Cwl 2 ? L 2 ? (1-?) 6 , C is a numerical constant and C w = H((8H + 2)l 2 ? + 2L ? )(W + 1) as defined in Lemma 4.3.</formula><p>Proof. By Lemma E.5 and E.9 we have the bounds</p><formula xml:id="formula_159">1 T T t=1 E [?? t ?] ? C ? + C 1 /2 3 + C 1 /2 4 ? 0 T 1 /3 , 1 T T t=1 E[?? t ?] ? C 1 + C 1 /2 w ? 0 (1 -?) 1 T 1 /3 . (<label>79</label></formula><formula xml:id="formula_160">)</formula><p>Summing up the result of Lemma E.4 from t = 1 to t = T and using the above bounds, we obtain</p><formula xml:id="formula_161">1 T T t=1 E[?e t ?] ? 1 T T t=1 E[?? t ?] + C 1 T T t=1 E[?? t-1 ?] + C 2 T T t=1 ? t-1 ? C ? + C 1 /2 3 + C 1 /2 4 ? 0 T 1 /3 + CC 1 1 + C 1 /2 w ? 0 (1 -?) 1 T 1 /3 + C 2 ? 0 T 2 /3 . (<label>80</label></formula><formula xml:id="formula_162">)</formula><p>End of the proof of Theorem 4.4. We conclude the proof of Theorem 4.4 which is first recalled in the following for the convenience of the reader.</p><p>Theorem E.11. Let Assumptions 4.1 and 4.2 hold. Let ? 0 &gt; 0 and consider an integer</p><formula xml:id="formula_163">T ? 1. Set ? t = ?0 T 2 /3 , ? t = 2 t+1 2 /3</formula><p>and H = (1 -?) -1 log(T + 1). Let ?T be sampled from the iterates {? 1 , ? ? ? , ? T } of Algorithm 1 uniformly at random.</p><p>Then, we have</p><formula xml:id="formula_164">E ? ? F (?( ?T )) ? O 1 + (1 -?) 3 ?? -1 0 + (1 -?) -1 ? 0 (1 -?) 3 T 1 /3 . (<label>81</label></formula><formula xml:id="formula_165">)</formula><formula xml:id="formula_166">If moreover ? 0 = (1 -?) 2 ? ?, then E ? ? F (?( ?T )) ? O 1+(1-?) ? ? (1-?) 3 T 1 /3</formula><p>. Proof. By Lemma E.1, we have for every integer t,</p><formula xml:id="formula_167">F (?(? t+1 )) ? F (?(? t )) + ? t 3 ?? ? F (?(? t ))? -2? t ?e t ? - 4 3 D ? ? H ? t - L ? 2 ? 2 t . (<label>82</label></formula><formula xml:id="formula_168">)</formula><p>Setting constant step-size ? t = ? = ?0 T 2 /3 , taking expectation, telescoping and rearranging, we get</p><formula xml:id="formula_169">1 T T t=1 E [?? ? F (?(? t ))?] ? 3(F ? -F (?(? 1 ))) ?T + 6 T T t=1 E [?e t ?] + 3L ? ? 2 + 4D g ? H ? 3(F ? -F (?(? 1 ))) ? 0 T 1 /3 + 6C ? + C 1 /2 3 + C 1 /2 4 ? 0 T 1 /3 + 6CC 1 1 + C 1 /2 w ? 0 (1 -?) 1 T 1 /3 + C 2 ? 0 T 2 /3 + 3L ? ? 0 2T 2 /3 + 4D g ? H = O (1 -?) -3 + ?? -1 0 + ? 0 (1 -?) -4 T 1 /3 , (<label>83</label></formula><formula xml:id="formula_170">)</formula><p>where we set H = (1 -?) -1 log(T ). Setting ? 0 = (1 -?) 2 ? ?, we obtain the desired result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3. Proof of Corollary 4.7 (Cumulative reward setting)</head><p>For this particular case, we redefine the error sequence (e t ) by overloading the notation since it plays a similar role. Define the error sequence (e t ) for every integer t as follows:</p><formula xml:id="formula_171">e t def = d t -?J H (? t ) , (<label>84</label></formula><formula xml:id="formula_172">)</formula><p>where the truncated cumulative reward J H (?) is defined as follows for any policy parameter ? ? R d :</p><formula xml:id="formula_173">J H (?) = E H-1 t=0 ? t r(s t , a t ) .<label>(85)</label></formula><p>We start by stating a complete version of Corollary 4.7 which we shall prove in this section.</p><p>Corollary E.12 (FOS convergence of N-VR-PG). Let Assumptions 4.1 and 4.2 hold. Let ? 0 &gt; 0 and let T be an integer larger than 1.</p><formula xml:id="formula_174">Set ? t = ?0 T 2 /3 , ? t = 2 t+1 2 /3</formula><p>and H = (1 -?)</p><p>-1 log(T + 1). Let ?T be sampled from the iterates {? 1 , ? ? ? , ? T } of N-VR-PG (Algorithm 4) uniformly at random. Then we have</p><formula xml:id="formula_175">E ?J( ?T ) ? O J * -J(? 1 ) ? 0 T 1 /3 + V + (L g + GC 1 /2 w )? 0 T 1 /3 , (<label>86</label></formula><formula xml:id="formula_176">)</formula><p>where V , L g , G, and C w are defined in <ref type="bibr">Lemma 4.3,</ref><ref type="bibr">H.3,</ref><ref type="bibr">and E.13</ref>. Moreover, if we set ? 0 = 1 -?, then</p><formula xml:id="formula_177">E ?J( ?T ) ? O 1 (1 -?) 2 T 1 /3 .</formula><p>The proof of this result follows the same lines as the proof of Theorem 4.4 which addresses the more general setting of general utilities. In the special case of cumulative rewards, recall that the estimation of the state-action occupancy measure is not required. In the following, we provide for clarity the intermediate results required to prove our result, mirroring the proof of the more general result of Theorem 4.4.</p><p>In order to derive the improved dependence on the (1 -?) factor in the final rate compared to Theorem 4.4, we will apply the following results from <ref type="bibr">Yuan et al. (2022, Lemma 4.2, (68) and Lemma 4.4, (19)</ref>) and <ref type="bibr">(Xu et al., 2020a, Proposition 4.2</ref> (1) and ( <ref type="formula" target="#formula_3">3</ref>)), which offer a tighter dependence on 1 -? in the standard cumulative reward setting. We use the notation g(?, ?) instead of g(?, ?, r) in this simpler standard RL setting.</p><p>Lemma E.13. Let Assumption 4.1 hold true and let ? = {s 0 , a 0 , ? ? ? , s H-1 , a H-1 } be an arbitrary trajectory of length H. Then the following statements hold:</p><formula xml:id="formula_178">(i) The objective function ? ? J(?) is L ? -smooth with L ? def = 2?r? ? (L ? +3l 2 ? ) (1-?) 2 . (ii) For all ? 1 , ? 2 ? R d , ?g(?, ? 1 ) -g(?, ? 2 )? ? L g ?? 1 -? 2 ? where L g def = 2(l 2 ? +L ? )?r?? (1-?) 2 , (iii) For all ? ? R d , E ?g(?, ?) -? ? J H (?)? 2 ? V 2 where V def = 2l ? ?r?? (1-?) 3 /2 . (iv) For all ? ? R d , ?g(?, ?)? ? G where G def = 2l ? l ? (1-?) 2 .</formula><p>We start with the next lemma which corresponds exactly to Lemma E.1.</p><p>Lemma E.14. Let Assumption 4.1 hold true. Then, the sequence (? t ) generated by Algorithm 4 and the sequence (e t ) defined in (84) satisfy for every integer t ? 0,</p><formula xml:id="formula_179">J(? t+1 ) ? J(? t ) + ? t 3 ??J(? t )? -2? t ?e t ? - 4 3 D g ? H ? t - L ? 2 ? 2 t . (<label>87</label></formula><formula xml:id="formula_180">)</formula><p>Proof. The proof is identical to the proof of Lemma E.1 upon noticing that Assumption 4.2 is not needed here since smoothness of the objective function J is a standard result in the RL literature following from Assumption 4.1 (see for e.g., Lemma 4.4 in Yuan et al. ( <ref type="formula">2022</ref>)).</p><p>Then next lemma is similar to Lemma E.8 and controls the error e t in Lemma E.14. We provide here a complete statement and proof of this result for clarity and completeness since the corresponding lemma in the more general case is more involved. Indeed, the latter result involves an additional error due to the occupancy measure estimation which is not required in our present setting.</p><p>Lemma E.15. Under Assumption 4.1, we have for every integer t ? 1</p><formula xml:id="formula_181">E[?e t ? 2 ] ? (1 -? t ) 2 E[?e t-1 ? 2 ] + 2V 2 ? 2 t + 4(L 2 g + G 2 C w )(1 -? t ) 2 ? 2 t-1 . (<label>88</label></formula><formula xml:id="formula_182">)</formula><p>where V , G, L g are constants defined in Lemma H.3 and E.13. If in addition (i) ? t = 2 t+1 , then for all t ? 1, we have</p><formula xml:id="formula_183">E[?e t ?] ? 4V ? t ? t 1 /2 + 2(L g + GC 1 /2 w )? t-1 ? t 1 /2 . (<label>89</label></formula><formula xml:id="formula_184">) (ii) ? t = 2 t+1 2 /3</formula><p>, then for all integers T ? 1, if ? t = ?0 T 2 /3 for some ? 0 &gt; 0, we have</p><formula xml:id="formula_185">T t=1 E[?e t ?] ? C V + L g + GC 1 /2 w ? 0 T 1 /3 , (<label>90</label></formula><formula xml:id="formula_186">)</formula><p>where C &gt; 0 is an absolute numerical constant.</p><p>Proof. Using the update rule of the sequence (d t ) and recalling the definition of the error e t = d t -?J H (? t ), we have</p><formula xml:id="formula_187">e t = d t -?J H (? t ) = (1 -? t )(d t-1 + v t ) + ? t g(? t , ? t ) -?J H (? t ) = (1 -? t )(e t-1 + ?J H (? t-1 ) + v t ) + ? t g(? t , ? t ) -?J H (? t ) = (1 -? t )e t-1 + ? t (g(? t , ? t ) -?J H (? t )) + (1 -? t )(v t -(?J H (? t ) -?J H (? t-1 ))) .</formula><p>Introducing additional notation for convenience:</p><formula xml:id="formula_188">y t def = g(? t , ? t ) -?J H (? t ) , (<label>91</label></formula><formula xml:id="formula_189">)</formula><formula xml:id="formula_190">z t def = v t -(?J H (? t ) -?J H (? t-1 )) ,<label>(92)</label></formula><p>we obtain the following useful decomposition:</p><formula xml:id="formula_191">e t = (1 -? t )e t-1 + ? t y t + (1 -? t )z t . (<label>93</label></formula><formula xml:id="formula_192">)</formula><p>Defining F t as the ?-algebra generated by all the random variables until time t, we observe that</p><formula xml:id="formula_193">E[y t |F t-1 ] = E[z t |F t-1 ] = 0 .</formula><p>As a consequence, we have</p><formula xml:id="formula_194">E[?e t ? 2 ] = (1 -? t ) 2 E[?e t-1 ? 2 ] + E[?? t y t + (1 -? t )z t ? 2 ] ? (1 -? t ) 2 E[?e t-1 ? 2 ] + 2? 2 t E[?y t ? 2 ] + 2(1 -? t ) 2 E[?z t ? 2 ] . (<label>94</label></formula><formula xml:id="formula_195">)</formula><p>We now control each one of the last two terms in the previous inequality. For the first term, we have by Lemma E.13-(iii)</p><formula xml:id="formula_196">E[?y t ? 2 ] ? V 2 . (<label>95</label></formula><formula xml:id="formula_197">)</formula><p>Concerning the second remaining term, using Lemma H.3-(ii) and Lemma E.13-(iv), we write</p><formula xml:id="formula_198">E[?z t ? 2 ] ? E[?v t ? 2 ] = E[?g(? t , ? t ) -w(? t |? t-1 , ? t )g(? t , ? t-1 )? 2 ] = E[?g(? t , ? t ) -g(? t , ? t-1 ) + g(? t , ? t-1 )(1 -w(? t |? t-1 , ? t ))? 2 ] ? 2L 2 g E[?? t -? t-1 ? 2 ] + 2G 2 E[(1 -w(? t |? t-1 , ? t )) 2 ] = 2L 2 g E[?? t -? t-1 ? 2 ] + 2G 2 Var(w(? t |? t-1 , ? t )) ? 2(L 2 g + G 2 C w )E[?? t -? t-1 ? 2 ] = 2(L 2 g + G 2 C w )? 2 t-1 .<label>(96)</label></formula><p>Combining ( <ref type="formula" target="#formula_194">94</ref>) with ( <ref type="formula" target="#formula_196">95</ref>) and ( <ref type="formula" target="#formula_198">96</ref>) concludes the first part of the proof.</p><p>Applying Lemma H.4 with</p><formula xml:id="formula_199">? t = 2 t+1 , ? t = 2V 2 ? 2 t + 4(L 2 g + G 2 C w )(1 -? t ) 2 ? 2 t-1 and using E[?e 0 ? 2 ] ? V 2 , we get E[?e t ?] ? E[?e t ? 2 ] 1 2 ? 4V 2 (t + 1) 2 + 2V 2 ? 2 t ? t + 4(L 2 g + G 2 C w )? 2 t-1 ? t 1 /2 ? 2V (t + 1) + 2V ? t ? t 1 /2 + 2(L g + GC 1 /2 w )? t-1 ? t 1 /2 ? 4V ? t ? t 1 /2 + 2(L g + GC 1 /2 w )? t-1 ? t 1 /2 . (<label>97</label></formula><formula xml:id="formula_200">)</formula><p>In order to derive (90), we unroll the recursion (88) from t = 1 to t = t ? , where t ? ? T . Denoting</p><formula xml:id="formula_201">? t = 2V 2 ? 2 t + 4(L 2 g + G 2 C w )(1 -? t ) 2 ? 2 t-1 , we have E ?e t ? ? 2 ? t ? ? =1 (1 -? ? )E ?e 0 ? 2 + t ? t=0 ? t t ? ? =t+1 (1 -? ? ) ? V 2 ? t ? +1 + C? t+1 ? -1 t ? +1 ,<label>(98)</label></formula><p>where we used E ?e 0 ? 2 ? V 2 and the result of Lemma H.6 with C &gt; 0 being a numerical constant. Finally, summing up the above inequality from t ? = 1 to t ? = T and choosing ? t = ? = ?0 T 2 /3 , we obtain</p><formula xml:id="formula_202">1 T T t=1 E [?e t ?] ? 1 T T t=1 E ?e t ? 2 1 /2 ? 1 T T t=1 E ?e t ? 2 1 /2 ? 1 T T t=1 V 2 ? T +1 + 2CV 2 ? t+1 + 4C(L 2 g + G 2 C w )? 2 ? -1 t+1 1 /2 (i) ? 3V 2 ? T -1 + 6CV 2 ? T -1 + 4C(L 2 g + G 2 C w ) ? 2 ? T +1 1 /2 ? 9CV 2 ? T + 6C(L 2 g + G 2 C w ) ? 2 0 ? T 1 T 4 /3 1 /2 ? 4C 1 /2 V + L g + GC 1 /2 w ? 0 T 1 /3 .</formula><p>where in (i) we used (51).</p><p>End of Proof of Corollary 4.7. The last steps of the proof are standard. Taking expectation on both sides of the result of Lemma E.14, telescoping and rearranging, we have for every integer T ? 1 and constant step-size</p><formula xml:id="formula_203">? t = ?0 T 2 /3 , 1 T T t=1 E [??J(? t )?] ? 3(J * -J(? 1 )) ? 0 T T 2 /3 + 6 T T t=1 E [?e t ?] + 3L ? ? 0 T 2 /3 + 4D g ? H ? 3(J * -J(? 1 )) ? 0 T 1 /3 + 6C V + L g + GC 1 /2 w ? 0 T 1 /3 + 3L ? ? 0 T 2 /3 + 4D g ? H<label>(99)</label></formula><p>with C &gt; 0 being a numerical constant, where we applied Lemma E.15 to bound</p><formula xml:id="formula_204">T t=1 E [?e t ?].</formula><p>Then choosing H large enough, we have after T iterations</p><formula xml:id="formula_205">1 T T t=1 E [??J(? t )?] ? O J * -J(? 1 ) ? 0 T 1 /3 + V + (L g + GC 1 /2 w )? 0 T 1 /3</formula><p>, which concludes the first part of the corollary.</p><p>As for the second part of the statement, we know from Lemma H.3 that</p><formula xml:id="formula_206">J * -J(? 1 ) = O 1 1 -? , V = O 1 (1 -?) 3 /2 , G = O 1 (1 -?) 2 , L g = O 1 (1 -?) 2 , C 1 /2 w = O 1 1 -? .</formula><p>If we set ? 0 = 1 -?, we derive the desired bound.</p><p>E.4. Proof of Theorem 4.8 (Cumulative reward setting for continuous state-action space and Gaussian policy)</p><p>In this section, we consider continuous state and action spaces where S = R p and A = R q for two positive integers p, q ? 1 .</p><p>Our focus is on the popular class of Gaussian policies which are common to handle the case of continuous state action spaces in practice.</p><p>Let ? &gt; 0 . Define for every ? ? R d a map ? ? : S ? R q . Then, we define the Gaussian policy ? ? for each parameter ? ? R d and each state-action pair (s, a) ? S ? A as follows:</p><formula xml:id="formula_207">? ? (a|s) = 1 ? ? 2? exp - ?? ? (s) -a? 2 2? 2 . (<label>100</label></formula><formula xml:id="formula_208">)</formula><p>Let us mention that ? ? is a parametrization of the Gaussian mean which can be a neural network in practice. The standard deviation ? can be fixed or parametrized as well in practice. We consider a fixed standard deviation for the purpose of our discussion.</p><p>Remark E.16. Note that one can consider even more general parametrizations such as the exponential family or symmetric ?-stable policies which include the Gaussian policy as a particular case. We refer the interested reader to the nice exposition in <ref type="bibr" target="#b2">Bedi et al. (2021)</ref> for a discussion around such heavy-tailed policy parametrizations (see also <ref type="bibr" target="#b3">Bedi et al. (2022)</ref>).</p><p>We make the following standard smoothness assumption on our Gaussian policy parametrization. Assumption E.17. In the Gaussian parametrization (100), the map ? ? ? ? (s) is continuously differentiable for every s ? S, l ? -Lipschitz continuous (uniformly in s ? S) and there exist</p><formula xml:id="formula_209">M g &gt; 0, M h &gt; 0 s.t. for every ? ? R d , (s, a) ? S ? A, ?? log ? ? (a|s)? ? M g , ?? 2 ? log ? ? (a|s)? ? M h .</formula><p>Notice that conditions on the map ? ? ? ? (s) and its higher-order derivatives can be enforced for every s ? S so that the desired regularity conditions on the policy parametrization in Assumption E.17 are satisfied upon considering a set of actions lying in a compact set. Consider for instance the simpler case where q = 1 and the mean of the policy is parametrized with a linear function, i.e., ? ? (s) = ?(s) T ? for some feature map ? : S ? R d . Then, the boundedness of ?? 2 ? log ? ? (a|s)? is automatically satisfied since ? 2 ? log ? ? (a|s) is the matrix -1 ? 2 ?(s)?(s) T which is independent from the parameter ? . As for the first condition, it is satisfied if the feature map ? as well as ? ? (s) are bounded over the state space and the policy parameter space while the action set is also bounded. Notice though that Assumption E.17 can be relaxed to hold in expectation (over state-action pairs) in order to include an even larger class of policies <ref type="bibr">(Yuan et al., 2022)</ref>. In this work, we do not pursue such relaxations and assume the standard bound for all s ? S, a ? A for simplicity <ref type="bibr">(Xu et al., 2020a;</ref><ref type="bibr" target="#b27">Liu et al., 2020)</ref>.</p><p>Similarly to the softmax parametrization setting with Lemma E.13, under Assumption E.17, one can show smoothness of the expected return function J and derive useful bounds for the norm and the variance of stochastic gradients, see <ref type="bibr">(Yuan et al., 2022, Lemma 4.2, (68)</ref> and Lemma 4.4, ( <ref type="formula" target="#formula_55">19</ref>)), and <ref type="bibr">(Xu et al., 2020a, Proposition 4.2</ref> (1) and (3)). Lemma E.18. Let Assumption E.17 hold true and let ? = {s 0 , a 0 , ? ? ? , s H-1 , a H-1 } be an arbitrary trajectory of length H. Then the following statements hold:</p><formula xml:id="formula_210">(i) The objective function ? ? J(?) is L ? -smooth with L ? def = ?r? ? (M 2 g +M h ) (1-?) 2 . (ii) For all ? 1 , ? 2 ? R d , ?g(?, ? 1 ) -g(?, ? 2 )? ? L g ?? 1 -? 2 ? with L g def = 2M 2 g ?r?? (1-?) 3 + M h ?r?? (1-?) 2 , (iii) For all ? ? R d , E ?g(?, ?) -? ? J H (?)? 2 ? V 2 with V def = Mg?r?? (1-?) 3 /2 . (iv) For all ? ? R d , ?g(?, ?)? ? G with G def = Mg?r? ? (1-?) 2 . Given a trajectory ? = (s 0 , a 0 , s 1 , a 1 , ? ? ? , s H-1 , a H-1</formula><p>) of length H generated under the initial distribution ? and the Gaussian policy ? ? as defined in (100) for some ? ? R d , recall the definition of the IS weight for every ? ? ? R d :</p><formula xml:id="formula_211">w(? |? ? , ?) def = H-1 h=0 ? ? ? (a h |s h ) ? ? (a h |s h ) . (<label>101</label></formula><formula xml:id="formula_212">)</formula><p>Lemma E.19. Let H ? 1 be an integer and let Assumption E.17 be satisfied. Suppose that the sequence (? t ) is updated via ? t+1 = ? t + ? t dt ?dt? where d t ? R d is any nonzero update direction and ? t is a positive stepsize. If ? t+1 is a (random) trajectory of length H generated following the initial distribution ? and the Gaussian policy ? ?t+1 as defined in (100), then</p><formula xml:id="formula_213">E[w(? t+1 |? t , ? t+1 )] = 1 ,<label>(102)</label></formula><formula xml:id="formula_214">Var [w(? t+1 |? t , ? t+1 )] ? C w ? 2 t ,<label>(103)</label></formula><p>where the IS weight w(? t+1 |? t , ? t+1 ) is as defined in (101) and C w def = (2H 2 M g + HM h )(W + 1) .</p><p>Proof. The first identity follows from the definitions of the expectation and the IS weight. We now prove the second identity.</p><p>For any ? ? R d , let p(?|? ? ) denote the probability distribution induced by the policy ? ? over the space of random trajectories of length H initialized with the state distribution ?. The probability density is then given by</p><formula xml:id="formula_215">p(? |? ? ) = ?(s 0 ) ? ? (a 0 |s 0 ) H-1 t=1 P(s t |s t-1 , a t-1 ) ? ? (a t |s t ) ,<label>(104)</label></formula><p>where ? = (s 0 , a 0 , ? ? ? , s H-1 , a H-1 ) .</p><p>We use the shorthand notations ? 1 = ? t , ? 2 = ? t+1 and ? = ? t+1 for the rest of this proof. Then, we have</p><formula xml:id="formula_216">E[w(? |? 1 , ? 2 ) 2 ] = p(? |? ?1 ) 2 p(? |? ?2 ) d? = ?(s 0 ) ? ? (a 0 |s 0 ) H-1 t=1 P(s t |s t-1 , a t-1 ) ? ?1 (a t |s t ) 2 ? ?2 (a t |s t ) d? .<label>(105)</label></formula><p>We bound the above integral starting from the integral of the last term of the product<ref type="foot" target="#foot_8">7</ref> which writes as follows:</p><formula xml:id="formula_217">P(s H-1 |s H-2 , a H-2 ) ? ?1 (a H-1 |s H-1 ) 2 ? ?2 (a H-1 |s H-1 ) da H-1 ds H-1 .<label>(106)</label></formula><p>We shall now compute the integral w.r.t. a H-1 . In dimension 1 for the action variable a H-1 (similar derivations hold for higher dimensions), we have for every s,</p><formula xml:id="formula_218">+? -? ? ?1 (x | s) 2 ? ?2 (x | s) dx = 1 ? ? 2? +? -? exp - 2 (x -? ?1 (s)) 2 -(x -? ?2 (s)) 2 2? 2 dx = 1 ? ? 2? exp - 2? ?1 (s) 2 -? ?2 (s) 2 2? 2 +? -? exp - x 2 -2 (2? ?1 (s) -? ?2 (s)) x 2? 2 dx = 1 ? ? 2? exp - 2? ?1 (s) 2 -? ?2 (s) 2 -(2? ?1 (s) -? ?2 (s)) 2 2? 2 +? -? exp - (x -(2? ?1 (s) -? ?2 (s))) 2 2? 2 dx = exp - 2? ?1 (s) 2 -? ?2 (s) 2 -(2? ?1 (s) -? ?2 (s)) 2 2? 2 = exp ? ? (? ?2 (s) -? ?1 (s)) 2 ? 2 ? ?<label>(107)</label></formula><p>As a consequence, we obtain</p><formula xml:id="formula_219">P(s H-1 |s H-2 , a H-2 ) ? ?1 (a H-1 |s H-1 ) 2 ? ?2 (a H-1 |s H-1 ) da H-1 ds H-1 ? P(s H-1 |s H-2 , a H-2 ) exp ?? ?1 (s H-1 ) -? ?2 (s H-1 )? 2 ? 2 ds H-1 (i) ? P(s H-1 |s H-2 , a H-2 ) exp l 2 ? ?? 1 -? 2 ? 2 ? 2 ds H-1 (ii) = exp l 2 ? ? 2 t ? 2 ,<label>(108)</label></formula><p>where (i) follows from the l ? -Lipschitzness of the parametrized mean in Assumption E.17 and (ii) utilizes the normalized update rule as well as the fact that P(?|s H-2 , a H-2 ) is a transition probability kernel. Using a similar reasoning to bound the different integrals like in (106) backward from H -1 to 0 successively, we obtain the following bound on the second moment of IS weights in ( <ref type="formula" target="#formula_216">105</ref>):</p><formula xml:id="formula_220">E[w(? |? 1 , ? 2 ) 2 ] ? exp Hl 2 ? ? 2 t ? 2 . (<label>109</label></formula><formula xml:id="formula_221">)</formula><p>Therefore, similarly to the argument in Lemma 4.3, we obtain:</p><formula xml:id="formula_222">Var(w(? |? 1 , ? 2 )) ? W ,<label>(110)</label></formula><p>where W = O (1) is a numerical constant, which can be ensured, for example, by setting the step-sizes as</p><formula xml:id="formula_223">? t = ? = T -2/3 .</formula><p>As a consequence, we can apply Lemma B.1 in <ref type="bibr">(Xu et al., 2020a)</ref> and derive the bound on the variance of IS weights:</p><formula xml:id="formula_224">Var [w(? t+1 |? t , ? t+1 )] ? C w ?? t+1 -? t ? 2 = C w ? 2 t ,</formula><p>where the last step follows by the update rule ? t+1 = ? t + ? dt ?dt? . This concludes the proof.</p><p>Given the above results, we immediately obtain convergence of Algorithm 4 for Gaussian policy parametrization.</p><p>Corollary E.20 (Stationary convergence of N-VR-PG). Let Assumption E.17 hold. Let ? 0 &gt; 0 and let T be an integer larger</p><formula xml:id="formula_225">than 1. Set ? t = ?0 T 2 /3 , ? t = 2 t+1 2 /3 and H = (1 -?) -1 log(T + 1). Let ?T be sampled from the iterates {? 1 , ? ? ? , ? T }</formula><p>of N-VR-PG (Algorithm 4) uniformly at random. Then we have</p><formula xml:id="formula_226">E ?J( ?T ) ? O J * -J(? 1 ) ? 0 T 1 /3 + V + (L g + GC 1 /2 w )? 0 T 1 /3 ,<label>(111)</label></formula><p>where V , L g , G, and C w are defined in Lemma E.18 and E.19. Moreover, if we set ? 0 = 1 -?, then</p><formula xml:id="formula_227">E ?J( ?T ) ? O 1 (1 -?) 2 T 1 /3 .</formula><p>Proof. Given the results of Lemma E.18 and E.19, the proof of this statement follows immediately from the result of Corollary E.20. We notice that in order to specify the dependence on 1 -?, we invoke Lemma E.18, which is analogous to the corresponding Lemma E.13 for softmax policy parameterization. The only difference in terms of the dependence on 1 -? is in the bound for L g . However, this fact does not affect the final dependence on 1 -? since it is dominated by other terms in (111).</p><p>F. Proofs for Section 4.3: Global optimality convergence F.1. Proof of Theorem 4.12 (General utilities setting)</p><p>In this section, to prove our global convergence result under an additional concave reparametrization assumption, we refine the result of Lemma E.1. The proof is similar to the proof of Lemma 5.12 in <ref type="bibr">Zhang et al. (2021b)</ref>. Nevertheless, we would like to mention that it deviates from the latter in that our algorithm is significantly different and its normalized nature requires a significantly different treatment. In particular, the reader can appreciate from the statement of the result that the error term ?e t ? to the gradient estimation is not squared unlike in <ref type="bibr">(Zhang et al., 2021b)</ref> and controlling its magnitude required different proof techniques given the different recursive loopless variance reduction mechanism that we consider.</p><p>Lemma F.1. Let Assumptions 4.1, 4.2 and Assumption 4.9 hold. Additionally, let Assumption 4.10 be satisfied with some positive ? . Then, the sequence (? t ) generated by Algorithm 1 and the sequence (e t ) satisfy for every positive real ? ? min ?, ?t(1-?) 2? ? and every integer t,</p><formula xml:id="formula_228">F (?(? * )) -F (?(? t+1 )) ? (1 -?)(F (?(? * )) -F (?(? t ))) + 2? t ?e t ? + 4L ? l 2 ? (1 -?) 2 ? 2 + 4 3 ? t D ? ? H + L ? 2 ? 2 t . (<label>112</label></formula><formula xml:id="formula_229">)</formula><p>Proof. Lemma E.1 provides the following inequality:</p><formula xml:id="formula_230">F (?(? t+1 )) ? F (?(? t )) + ? t 3 ?? ? F (?(? t ))? -2? t ?e t ? - 4 3 D ? ? H ? t - L ? 2 ? 2 t .<label>(113)</label></formula><p>Now, for any ? &lt; ?, the concavity reparametrization assumption implies that (1 -?)?(? t ) + ??(? * ) ? V ?(?t) and therefore we have</p><formula xml:id="formula_231">? ? def = (?| U ? t ) -1 ((1 -?)?(? t ) + ??(? * )) ? U ?t .<label>(114)</label></formula><p>It also follows from the smoothness of the objective function ? ? F (?(?)) that</p><formula xml:id="formula_232">F (?(? t )) ? F (?(? ? )) -?? ? F (?(? t )), ? ? -? t ? - L ? 2 ?? ? -? t ? 2 . (<label>115</label></formula><formula xml:id="formula_233">)</formula><p>Combining ( <ref type="formula" target="#formula_230">113</ref>) and ( <ref type="formula" target="#formula_232">115</ref>) yields</p><formula xml:id="formula_234">F (?(? t+1 )) ? F (?(? ? )) -?? ? F (?(? t )), ? ? -? t ? - L ? 2 ?? ? -? t ? 2 + ? t 3 ?? ? F (?(? t ))? -2? t ?e t ? - 4 3 D ? ? H ? t - L ? 2 ? 2 t . (116)</formula><p>Then, we notice that:</p><p>(i) By assumption, the mapping ? ? (?| U ? t ) -1 coincides with the identity mapping on the set U ?t . Hence, given the definition of ? ? in (114), we have</p><formula xml:id="formula_235">F (?(? ? )) = F ((1 -?)?(? t ) + ??(? * )) ? (1 -?)F (?(? t )) + ?F (?(? * )) ,<label>(117)</label></formula><p>where the last step follows from the concavity of the function F .</p><p>(ii) Again since the mapping ? ? (?| U ? t ) -1 coincides with the identity mapping on the set U ?t and using the (uniform) lipschitzness of the inverse mapping (?| U ? t ) -1 , we have</p><formula xml:id="formula_236">?? ? -? t ? = ?(?| U ? t ) -1 ((1 -?)?(? t ) + ??(? * )) -(?| U ? t ) -1 (?(? t ))? ? l ? ???(? t ) -?(? * )? ? 2l ? ? (1 -?) .<label>(118)</label></formula><p>(iii) Using the Cauchy-Schwarz inequality together with the inequality established in the previous item gives</p><formula xml:id="formula_237">|?? ? F (?(? t )), ? ? -? t ?| ? ?? ? F (?(? t ))? ? ?? ? -? t ? ? 2l ? ? 1 -? ?? ? F (?(? t ))? . (<label>119</label></formula><formula xml:id="formula_238">)</formula><p>Substituting the inequalities (117), ( <ref type="formula" target="#formula_236">118</ref>) and ( <ref type="formula" target="#formula_237">119</ref>) into (116) leads to</p><formula xml:id="formula_239">F (?(? t+1 )) ? (1 -?)F (?(? t )) + ?F (?(? * )) + ? t 3 - 2l ? ? 1 -? ?? ? F (?(? t ))? - 4L ? l 2 ? (1 -?) 2 ? 2 -2? t ?e t ? - 4 3 D ? ? H ? t - L ? 2 ? 2 t ? (1 -?)F (?(? t )) + ?F (?(? * )) - 4L ? l 2 ? (1 -?) 2 ? 2 -2? t ?e t ? - 4 3 D ? ? H ? t - L ? 2 ? 2 t ,<label>(120)</label></formula><p>where the last step follows from the condition ? ? ?t(1-?)</p><formula xml:id="formula_240">2? ? .</formula><p>Finally, substracting F (?(? * )) from both sides and rearranging the terms gives the desired result:</p><formula xml:id="formula_241">F (?(? * )) -F (?(? t+1 )) ? (1 -?)(F (?(? * )) -F (?(? t ))) + 2? t ?e t ? + 4L ? l 2 ? (1 -?) 2 ? 2 + 4 3 ? t D ? ? H + L ? 2 ? 2 t . (121)</formula><p>Theorem F.2 (Global convergence of N-VR-PG for general utilities). Let Assumptions 4.1 and 4.9 hold. Additionally, let Assumption 4.10 be satisfied with ? ? ?0(1-?) 2? ? (T +1) a for some integer T ? 1 and reals ? 0 &gt; 0, a ? (0, 1). Set ? t = ?0 (T +1) a , ? t = 2 t+1 for every integer t and H = (1 -?) -1 log(T + 1). Then the output ? T of N-VR-PG (see Algorithm 1) satisfies</p><formula xml:id="formula_242">F (?(? * )) -E [F (?(? T ))] ? O ? 2 0 (1 -?) 3 (T + 1) 2a-3 2 ,</formula><p>where F (?(? * )) is the optimal utility value. Therefore, the sample complexity to achieve</p><formula xml:id="formula_243">F (?(? * )) -E [F (?(? T ))] ? ? is O ? -2 4a-3 . Proof. Define ? t def = E [F (?(? * )) -F (?(? t ))].</formula><p>Applying expectation to the result of Lemma F.1, we have for ? ? min ?, ?t(1-?)</p><formula xml:id="formula_244">2? ? , ? t+1 ? (1 -?)? t + 2? t E [?e t ?] + 4L ? l 2 ? (1 -?) 2 ? 2 + 4 3 ? t D ? ? H + L ? 2 ? 2 t ? (1 -?)? t + 2? t E [?? t ?] + 2C 1 ? t E [?? t ?] + 2C 2 ? t ? t-1 + 4L ? l 2 ? (1 -?) 2 ? 2 + 4 3 ? t D ? ? H + L ? 2 ? 2 t (<label>122</label></formula><formula xml:id="formula_245">)</formula><p>where in the last step we apply Lemma E.4 with</p><formula xml:id="formula_246">C 1 def = 2L 2 ? l ? (1-?) 2 and C 2 def = 2L ? L ?,? l ? (1-?) 2</formula><p>.</p><p>By Lemma E.5 (Equation ( <ref type="formula" target="#formula_80">34</ref>)), for ? t = 2 t+1 , we have</p><formula xml:id="formula_247">E[?? t ?] ? 4 (1 -?) ? t ? t 1 /2 + 2C 1 /2 w (1 -?) ? t-1 ? t 1 /2 . (<label>123</label></formula><formula xml:id="formula_248">)</formula><p>With the same ? t as above, by Lemma E.8 (Equation ( <ref type="formula">58</ref>)), we have</p><formula xml:id="formula_249">E[?? t ?] ? 2 ? t + 1 + 2C 1 /2 3 ? t ? t 1 /2 + C 1 /2 4 ? t-2 ? t 1 /2 ,<label>(124)</label></formula><p>where</p><formula xml:id="formula_250">C 3 def = 288Cl 2 ? L 2 ? (1-?) 6 + 32l 2 ? l 2 ? (1-?) 4 , C 4 def = 12l 2 ? [(l 2 ? +L ? ) 2 +Cwl 2 ? ] (1-?) 4 + 144Cwl 2 ? L 2 ?</formula><p>(1-?) 6 , and C w = H((8H + 2)l 2 ? + 2L ? )(W + 1) . Unrolling (122) from t = T -1 to t = 0, using ( <ref type="formula" target="#formula_247">123</ref>) and ( <ref type="formula" target="#formula_249">124</ref>) and setting ? t = ?, we have</p><formula xml:id="formula_251">? T ? (1 -?) T ? 0 + 2? T -1 t=0 (E [?? t ?] + C 1 E [?? t ?]) + 2C 2 ? 2 T + 4L ? l 2 ? (1 -?) 2 ? + 4 3 ? ? D ? ? H + L ? 2 ? 2 ? ? (1 -?) T ? 0 + 2? T -1 t=0 2 ? t + 1 + 2C 1 /2 3 ? t ? t 1 /2 + C 1 /2 4 ? ? t 1 /2 + 2C 2 ? 2 T +2C 2 ? T -1 t=0 4 (1 -?) ? t ? t 1 /2 + 2C 1 /2 w (1 -?) ? ? t 1 /2 + 4L ? l 2 ? (1 -?) 2 ? + 4 3 ? ? D ? ? H + L ? 2 ? 2 ? ? (1 -?) T ? 0 + 4? ? log(T ) + 8?C 1 /2 3 (T + 1) 1 /2 + 2C 1 /2 4 ? 2 ? (T + 1) 3 /2 + 2C 2 ? 2 T + 16C 2 ? (1 -?) (T + 1) 1 /2 + 4C 2 C 1 /2 w (1 -?) ? 2 (T + 1) 3 /2 + 4L ? l 2 ? (1 -?) 2 ? + 4 3 ? ? D ? ? H + L ? 2 ? 2 ? .</formula><p>Notice that (1 -?) T ? exp (T log(1 -?)) ? exp (-?T ). Finally setting ? = ?0 (T +1) a , for 0 &lt; a &lt; 1 and ? = min ?, ?(1-?)</p><formula xml:id="formula_252">2? ? = ?(1-?)</formula><p>2? ? , we obtain</p><formula xml:id="formula_253">? T ? exp - ? 0 (1 -?) 2? ? T 1-a + 4 ? log(T )? 0 (T + 1) a + 8C 1 /2 3 + 16C 2 (1 -?) ? 0 (T + 1) a-1 /2 + 2C 2 ? 2 0 (T + 1) 2a-1 + 2C 1 2 4 + 4C 2 C 1 /2 w (1 -?) ? 2 0 (T + 1) 2a-3 2 + 4L ? l 2 ? (1 -?) 2 ? + 4 3 ? ? D ? ? H + L ? 2 ? 2 ? ? exp - ? 0 (1 -?) 2? ? T 1-a + 4 ? log(T )? 0 (T + 1) a + 8C 1 /2 3 + 16C 2 (1 -?) ? 0 (T + 1) a-1 /2 + 2C 2 ? 2 0 (T + 1) 2a-1 + 2C 1 2 4 + 4C 2 C 1 /2 w (1 -?) ? 2 0 (T + 1) 2a-3 2 + 3L ? l ? ? 0 (1 -?)(T + 1) a + 8? ? D ? 3(1 -?) ? H ? O 1 (1 -?) 3 (T + 1) 2a-3 2 ,</formula><p>where the last step follows by setting H = (1 -?) -1 log(T ) and noticing that 2a -3 2 &lt; a -1 2 for a ? (0, 1),</p><formula xml:id="formula_254">C 4 = O (1 -?) -6 , C w = O (1 -?) -2 , C 2 = O (1 -?) -2 .</formula><p>F.2. Proof of Corollary 4.13 (Cumulative reward setting)</p><p>We first recall that similarly to Section E.3, for cumulative reward setting, we redefine the error sequence (e t ) as</p><formula xml:id="formula_255">e t = d t -?J H (? t ) ,</formula><p>where the truncated cumulative reward J H (?) is defined as</p><formula xml:id="formula_256">J H (?) = E H-1 t=0 ? t r(s t , a t ) .</formula><p>Now we state a complete version of Corollary 4.13, which we shall prove in this section. Corollary F.3 (Global convergence of N-VR-PG). Let Assumptions 4.1 and 4.9 hold. Additionally, let Assumption 4.10 be satisfied with ? ? ?0(1-?) 2? ? (T +1) a for some integer T ? 1 and reals ? 0 &gt; 0, a ? (0, 1). Set ? t = ?0 (T +1) a , ? t = 2 t+2 and H = (1 -?) -1 log(T + 1). Then the output ? T of N-VR-PG (see Algorithm 4) satisfies</p><formula xml:id="formula_257">J * -E [J(? T )] ? O ? 0 V (T + 1) a-1 2 ,</formula><p>where J * is the optimal expected return and V is defined in Lemma E.13. Therefore, the sample complexity to achieve</p><formula xml:id="formula_258">J * -E [J(? T )] ? ? is O ? -2 2a-1 .</formula><p>Remark F.4. If we are allowed to select ? 0 based on the problem parameters (only the bound on (1 -?) is actually needed here), then the dependence on (1 -?) -1 in the above theorem can be made arbitrary small.</p><p>Proof. By Lemma E.15, we have the control of the variance sequence for ? t = 2 t+2 as</p><formula xml:id="formula_259">E [?e t ?] ? 4V ? t ? t 1 /2 + 2(L g + GC 1 /2 w )? t-1 ? t 1 /2 . (<label>125</label></formula><formula xml:id="formula_260">) Define ? t def = E [J(? * ) -J(? t )],</formula><p>where in the cumulative reward case F (?(?)) = J(?). Let ? t = ? for all t = 0, . . . , T -1.</p><p>Then applying full expectation to the result of Lemma F.1, we have for ? ? min ?, ?(1-?)</p><formula xml:id="formula_261">2? ? ? t+1 ? (1 -?)? t + 2?E [?e t ?] + 4L ? l 2 ? (1 -?) 2 ? 2 + 4 3 ?D ? ? H + L ? 2 ? 2 .</formula><p>Unrolling the recursion from t = 0 to t = T -1, we have</p><formula xml:id="formula_262">? T ? (1 -?) T ? 0 + 2? T -1 t=0 E [?e t ?] + 4L ? l 2 ? (1 -?) 2 ? + 4 3 ? ? D ? ? H + L ? 2 ? 2 ? ? (1 -?) T ? 0 + 8V ? T -1 t=0 ? t ? t 1 /2 + 4(L g + GC 1 /2 w )? 2 T 1 /2 + 4L ? l 2 ? (1 -?) 2 ? + 4 3 ? ? D ? ? H + L ? 2 ? 2 ? ? (1 -?) T ? 0 + 8V ?(T + 1) 1 /2 + 4(L g + GC 1 /2 w )? 2 T 1 /2 + 4L ? l 2 ? (1 -?) 2 ? + 4 3 ? ? D ? ? H + L ? 2 ? 2 ? .</formula><p>Notice that (1 -?) T ? exp (T log(1 -?)) ? exp (-?T ). Finally setting ? = ?0 (T +1) a , for 0 &lt; a &lt; 1 and ? = min ?, ?(1-?)</p><formula xml:id="formula_263">2? ? = ?(1-?)</formula><p>2? ? , we obtain</p><formula xml:id="formula_264">? T ? exp - ? 0 (1 -?) 2? 0 T 1-a + 8? 0 V (T + 1) a-1 2 + 4? 2 0 (L g + GC 1 /2 w ) T 2a-1 2 + 4L ? l 2 ? (1 -?) 2 ? + 4 3 ? ? D ? ? H + L ? 2 ? 2 ? ? exp - ? 0 (1 -?) 2? 0 T 1-a + 8? 0 V (T + 1) a-1 2 + 4? 2 0 (L g + GC 1 /2 w ) T 2a-1 2 + 2L ? l ? ? 0 (1 -?)(T + 1) a + 4 3 ? ? D ? ? H + L ? 2 ? 2 ? ? exp - ? 0 (1 -?) 2? 0 T 1-a + 8? 0 V (T + 1) a-1 2 + 4? 2 0 (L g + GC 1 /2 w ) T 2a-1 2 + 3L ? l ? ? 0 (1 -?)(T + 1) a + 8? ? D ? 3(1 -?) ? H ? O 1 (T + 1) a-1 2 ,</formula><p>where the last step follows by setting H = (1 -?) -1 log(T ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.3. Global optimality in the cumulative reward setting for continuous state-action space and Gaussian policy</head><p>We first present our set of assumptions to derive global convergence results under the Gaussian policy parameterization. We start by assuming that our Gaussian policy parametrization is Fisher-non-degenerate, meaning that the Fisher information matrix induced by the policy parametrization is (uniformly) positive definite. This assumption is standard in the literature <ref type="bibr" target="#b27">(Liu et al., 2020;</ref><ref type="bibr" target="#b11">Ding et al., 2022;</ref><ref type="bibr">Yuan et al., 2022;</ref><ref type="bibr" target="#b28">Masiha et al., 2022;</ref><ref type="bibr" target="#b14">Fatkhullin et al., 2022)</ref>. We remark that <ref type="bibr" target="#b15">Fatkhullin et al. (2023)</ref> recently obtained a O(? -2 ) sample complexity under similar assumptions using a similar proof technique. The key difference between our N-VR-PG method and their (N)-HARPG algorithm is that our algorithm does not require the use of second-order information. The bound of IS weights is automatically ensured by the normalization step of the algorithm and the specific structure of the Gaussian policy parametrization (Lemma E.19).</p><p>Assumption F.5. There exists ? F &gt; 0 such that for every ? ? R d , the Fisher information matrix satisfies</p><formula xml:id="formula_265">F ? (?) def = E s?d ? ? ? , a?? ? (?|s) [? log ? ? (a|s)? log ? ? (a|s) ? ] ? ? F I d , where d ? ? ? (?) def = (1 -?) ? t=0 ? t P ?,? ? (s t ? ?) is the discounted state visitation measure.</formula><p>For Gaussian policies with fixed covariance matrix and linear mean parametrization ? ? (s) = ?(s) ? ?, the Fisher information matrix can be written explicitly. Namely, we have F ? (?) = ? -2 ?(s)?(s) ? for every s ? S. Therefore, the above assumption is satisfied if we assume that the feature map ?(s) has full-row-rank. Now we introduce an assumption which characterizes the expressivity of our policy parameterization class via the framework of compatible function approximation <ref type="bibr" target="#b41">(Sutton et al., 1999;</ref><ref type="bibr" target="#b0">Agarwal et al., 2021)</ref>. In order to state this assumption, we first define the advantage function. Define for every policy ? the state-action value function Q ? : S ? A ? R for every s ? S, a ? A as:</p><formula xml:id="formula_266">Q ? (s, a) def = E ? ? t=0 ? t r(s t , a t )|s 0 = a, a 0 = a .</formula><p>Under the same policy ?, the state-value function V ? : S ? R and the advantage function A ? : S ? A ? R are defined for every s ? S, a ? A as follows:</p><formula xml:id="formula_267">V ? (s) def = E a??(?|s) [Q ? (s, a)] , A ? (s, a) def = Q ? (s, a) -V ? (s) .</formula><p>Now we are ready to state the compatible function approximation error assumption.</p><p>Assumption F.6. There exists ? bias ? 0 s.t. for every ? ? R d , the transfer error satisfies:</p><formula xml:id="formula_268">E[(A ? ? (s, a) -(1 -?)w * (?) ? ? log ? ? (a|s)) 2 ] ? ? bias ,</formula><p>where A ? ? is the advantage function, w * (?)</p><formula xml:id="formula_269">def = F ? (?) ? ?J(?)</formula><p>where F ? (?) ? is the pseudo-inverse of the matrix F ? (?) and expectation is taken over s ? d ? * ? , a ? ? * (?|s) where ? * is an optimal policy (maximizing J(?)).</p><p>The above assumption requires that the policy parametrization ? ? should be able to approximate the advantage function A ? ? by the score function ? log ? ? . Naturally ? bias is necessarily positive for a parameterization ? ? that does not cover the set of all stochastic policies and ? bias is small for a rich neural policy <ref type="bibr" target="#b43">(Wang et al., 2020)</ref>. We note that this is a common assumption which was used for instance in <ref type="bibr" target="#b0">(Agarwal et al., 2021;</ref><ref type="bibr" target="#b27">Liu et al., 2020;</ref><ref type="bibr" target="#b11">Ding et al., 2022;</ref><ref type="bibr">Yuan et al., 2022)</ref>.</p><p>Equipped with Assumptions E.17, F.5, F.6, and following the derivations of <ref type="bibr" target="#b11">Ding et al. (2022)</ref>, we obtain a relaxed weak gradient dominance inequality.</p><p>Lemma F.7 (Relaxed weak gradient domination, <ref type="bibr" target="#b11">(Ding et al., 2022)</ref>). Let Assumptions E.17, F.5 and F.6 hold. Then</p><formula xml:id="formula_270">? ? ? R d , ? ? + ??J(?)? ? 2? (J * -J(?)) ,<label>(126)</label></formula><p>where J * is the optimal expected return,</p><formula xml:id="formula_271">? ? = ? F ? ? bias Mg(1-?) and ? = ? 2 F 2M 2 g .</formula><p>Corollary F.8 (Global convergence of N-VR-PG). Let Assumptions E.17, F.5 and F.6 hold. Set ? t = 3 ? 2?(T +1) a , for some 0 &lt; a &lt; 1 , ? t = 2 t+1 and H = (1 -?) -1 log(T + 1). Then the output ? T of N-VR-PG (see Algorithm 4) satisfies</p><formula xml:id="formula_272">J * -E [J(? T )] ? O 1 (1 -?) 3 /2 (T + 1) a-1 2 + ? ? bias 1 -? ,</formula><p>where J * is the optimal expected return. Therefore, the sample complexity to achieve J</p><formula xml:id="formula_273">* -E [J(? T )] ? ? + ? ? bias 1-? is O ? -2 2a-1 .</formula><p>Proof. As in the case of softmax parametrization, given the result of Lemma E.18, and following the steps in the proof of Lemma E.15, we can derive the control of the variance sequence for ? t = 2 t+1 as</p><formula xml:id="formula_274">E [?e t ?] ? 4V ? t ? t 1 /2 + 2(L g + GC 1 /2 w )? t-1 ? t 1 /2 . (<label>127</label></formula><formula xml:id="formula_275">)</formula><p>Similarly to Lemma E.1, we can obtain</p><formula xml:id="formula_276">J(? t+1 ) ? J(? t ) + ? t 3 ??J(? t )? -2? t ?e t ? - 4 3 D g ? H ? t - L ? 2 ? 2 t ? J(? t ) + ? t ? 2? 3 (J * -J(? t )) -2? t ?e t ? - 4 3 D g ? H ? t - L ? 2 ? 2 t - ? ? ? t 3 ,<label>(128)</label></formula><p>where in the last step we applied the relaxed weak gradient dominance condition (Lemma F.7). Now we define ? t def = E [J(? * ) -J(? t )]. Let ? t = ? for all t = 0, . . . , T . Then applying full expectation to the result of Lemma F.1, we have</p><formula xml:id="formula_277">? t+1 ? 1 - ? ? 2? 3 ? t + 2?E [?e t ?] + 4 3 D g ? H ? + L ? 2 ? 2 + ? ? ? 3 .</formula><p>Unrolling the recursion from t = 0 to t = T -1, we have</p><formula xml:id="formula_278">? T ? 1 - ? ? 2? 3 T ? 0 + 2? T -1 t=0 E [?e t ?] + 4 ? 2? D g ? H + 3 ? 2? L ? 2 ? + ? ? ? 2? ? 1 - ? ? 2? 3 T ? 0 + 8V ? T -1 t=0 ? t ? t 1 /2 + 4(L g + GC 1 /2 w )? 2 T 1 /2 + 4 ? 2? D g ? H + 3 ? 2? L ? 2 ? + ? ? ? 2? ? 1 - ? ? 2? 3 T ? 0 + 8V ?(T + 1) 1 /2 + 4(L g + GC 1 /2 w )? 2 T 1 /2 + 4 ? 2? D g ? H + 3 ? 2? L ? 2 ? + ? ? ? 2? .</formula><p>Finally, setting ? = 3 ? 2?(T +1) a , for 0 &lt; a &lt; 1 and noticing that (1</p><formula xml:id="formula_279">-(T + 1) -a ) T ? exp (T log(1 -(T + 1) -a )) ? exp -T 1-a , we obtain ? T ? exp -T 1-a ? 0 + 24V ? 2?(T + 1) a-1 2 + 18(L g + GC 1 /2 w ) ? ? T 2a-1 2 + 4 ? 2? D g ? H + 9L ? 8? 1 (T + 1) a + ? ? ? 2? ? O V ? ?(T + 1) a-1 2 + ? ? ? 2? ,</formula><p>where the last step follows by setting H = (1 -?) -1 log(T ). It only remains to notice from Lemma E.18 and F.7 that</p><formula xml:id="formula_280">? ? = ? F ? ? bias M g (1 -?) = O 1 1 -? , V = M g ?r? ? (1 -?) 3 /2 = O 1 (1 -?) 3 /2 .</formula><p>G. Proofs for Section 5: Large state-action space setting G.1. Unbiased estimates of the occupancy measure at state-action pairs Notation. For a given set A, the indicator function 1 A is equal to one on the set A and zero otherwise.</p><p>In this section, we provide two different estimators: the first one is a Monte-Carlo estimate of the truncated occupancy measure whereas the second one is an unbiased estimate of the true occupancy measure. Notice that we can also slightly modify the second estimator to a obtain a minibatch estimator via sampling (independently) similarly N different state-action pairs (s</p><formula xml:id="formula_281">(i) H , a<label>(i)</label></formula><p>H ) 0?i?N via the same sampling procedure as in Algorithm 6 and averaging out the outputs, i.e., considering the following estimator:</p><formula xml:id="formula_282">?? ? (s, a) = 1 N N i=1 1 {s (i) H =s, a<label>(i)</label></formula><p>H =a} .</p><p>(129) Theorem G.1. Let Assumptions 4.1, 4.2, 5.2 and 5.3 hold true. In addition, suppose that there exists ? min &gt; 0 s.t. the initial distribution ? satisfies ?(s) ? ? min for all s ? S . Let T ? 1 be an integer and let (? t ) be the sequence generated by Algorithm 3 with a positive step size ? ? min(1/ 5 C1 , 1/2L ? ) (see C1 below) and batch size N ? 1. Then, we have</p><formula xml:id="formula_283">E[?? ? F (?( ?T ))? 2 ] ? 16(F * -E[F (?(? 1 ))]) + ? C4 ?T + C3 N + 2D 2 ? ? 2H + C2 (? stat + ? approx ) ,<label>(130)</label></formula><p>where ?T be a random iterate drawn uniformly at random from {?</p><formula xml:id="formula_284">1 , ? ? ? , ? T }, C1 def = 48l 3 ? L 2 ?,? (1-?) 6 , C2 def = 48l 2 ? L 2 ? (1-?) 4 |A| ?min , C3 def = 24l 2 ? l 2 ? (1-?) 4 , C4 def = 8l 2 ? l 2 ? (1-?) 4 and D ? is defined in Lemma H.2.</formula><p>Proof. We introduce the shorthand notation u</p><formula xml:id="formula_285">t def = 1 N N i=1 g(?<label>(i)</label></formula><p>t , ? t , r t-1 ) for this proof. The smoothness of the objective function ? ? F (?(?)) (see Lemma H.1) together with the update rule of the sequence (? t ) yields</p><formula xml:id="formula_286">F (?(? t+1 )) ? F (?(? t )) + ?? ? F (?(? t )), ? t+1 -? t ? - L ? 2 ?? t+1 -? t ? 2 = F (?(? t )) + ??? ? F (?(? t )), u t ? - L ? ? 2 2 ?u t ? 2 = F (?(? t )) + ??? ? F (?(? t )) -u t , u t ? + ? 1 - L ? ? 2 ?u t ? 2 ? F (?(? t )) - ? 2 ?? ? F (?(? t )) -u t ? 2 - ? 2 ?u t ? 2 + ? 1 - L ? ? 2 ?u t ? 2 = F (?(? t )) - ? 2 ?? ? F (?(? t )) -u t ? 2 + ? 2 (1 -L ? ?)?u t ? 2 (i) ? F (?(? t )) - ? 2 ?? ? F (?(? t )) -u t ? 2 + ? 4 ?u t ? 2 = F (?(? t )) - ? 2 ?? ? F (?(? t )) -u t ? 2 + ? 8 ?u t ? 2 + ? 8 ?u t ? 2 (ii) ? F (?(? t )) + ? 16 ?? ? F (?(? t ))? 2 - 5 8 ??? ? F (?(? t )) -u t ? 2 + ? 8 ?u t ? 2 ,<label>(131)</label></formula><p>where (i) follows from the condition ? ? 1/2L ? and (ii) from 1 2 ?? ? F (?(? t ))? 2 ? ?u t ? 2 + ?? ? F (?(? t )) -u t ? 2 . We now control the last error term in the above inequality in expectation. Observe first that</p><formula xml:id="formula_287">E[?? ? F (?(? t )) -u t ? 2 ] ? 2 E[?? ? F (?(? t )) -? ? F (? H (? t ))? 2 ] + 2E[?? ? F (? H (? t )) -u t ? 2 ] ? 2D 2 ? ? 2H + 2E[?? ? F (? H (? t )) -u t ? 2 ] ,<label>(132)</label></formula><p>where the last inequality stems from Lemma H.2. Now, it remains to control</p><formula xml:id="formula_288">E[?? ? F (? H (? t )) -u t ? 2 ] . Using the notation r t def = ? ? F (? H (? t</formula><p>)), we have the following decomposition:</p><formula xml:id="formula_289">? ? F (? H (? t )) -u t = ? ? F (? H (? t )) -[? ? ?(? t )] T r * t-1 + [? ? ?(? t )] T r * t-1 -[? ? ?(? t )] T r t-1 + [? ? ?(? t )] T r t-1 -u t .<label>(133)</label></formula><p>Then it follows that</p><formula xml:id="formula_290">E[?? ? F (? H (? t )) -u t ? 2 ] ? 3E[?? ? F (? H (? t )) -[? ? ?(? t )] T r * t-1 ? 2 ] + 3E[?[? ? ?(? t )] T (r * t-1 -r t-1 )? 2 ] + 3E[?[? ? ?(? t )] T r t-1 -u t ? 2 ] . (134)</formula><p>We control each term in the above decomposition separately in what follows.</p><p>Term 1 in (134): For this term, we have the following series of inequalities</p><formula xml:id="formula_291">?? ? F (? H (? t )) -[? ? ?(? t )] T r * t-1 ? 2 = ?[? ? ?(? t )] T (r * t -r * t-1 )? 2 (a) ? 4l 2 ? (1 -?) 4 ?r * t-1 -r * t ? 2 ? (b) ? 4l 2 ? L 2 ?,? (1 -?) 4 ?? H (? t-1 ) -? H (? t )? 2 1 (c) ? 8l 3 ? L 2 ?,? (1 -?) 6 ?? t -? t-1 ? 2 (d) = 8l 3 ? L 2 ?,? (1 -?) 6 ?u t-1 ? 2 ? ? 2 ,<label>(135)</label></formula><p>where (a) follows from similar derivations to ( <ref type="formula" target="#formula_74">29</ref>)-( <ref type="formula" target="#formula_76">31</ref>) using ( <ref type="formula">5</ref>), (b) stems from Assumption 4.2, (c) is an immediate consequence of Lemma H.1-(ii) and (d) uses the update rule of Algorithm 3.</p><p>Term 2 in (134): For this term, we start with the following inequalities:</p><formula xml:id="formula_292">E[?[? ? ?(? t )] T (r * t-1 -r t-1 )? 2 ] (i) ? 4l 2 ? (1 -?) 4 E[?r t-1 -r * t-1 ? 2 ? ] (ii) ? 4l 2 ? L 2 ? (1 -?) 4 E[? ?t-1 -? H (? t-1 )? 2 ] ,<label>(136)</label></formula><p>where (i) follows from similar derivations to (29)-( <ref type="formula" target="#formula_76">31</ref>) using ( <ref type="formula">5</ref>) and (ii) follows from Assumption 4.2. Then we decompose and upper bound the above error as follows:</p><formula xml:id="formula_293">E[? ?t-1 -? H (? t-1 )? 2 ] = E[???(?, ?), ??t-1 ? -? H (? t-1 )? 2 ] = E[???(?, ?), ??t-1 -? * (? t-1 )? + ??(?, ?), ? * (? t-1 )? -? H (? t-1 )? 2 ] ? 2 E[???(?, ?), ??t-1 -? * (? t-1 )?? 2 ] + 2 E[???(?, ?), ? * (? t-1 )? -? H (? t-1 )? 2 ] .<label>(137)</label></formula><p>Our task now is to upper bound each one of the above errors, the first one being related to the statistical error whereas the second one relates to the approximation error. Recall the definition of the regression loss function for every</p><formula xml:id="formula_294">? ? R d , ? ? R m , L ? (?) = E s??,a?U (A) [(? ? ? H (s, a) -??(s, a), ??) 2 ] ,<label>(138)</label></formula><p>where U(A) is the uniform distribution over the action space A . </p><p>Then, we have for all ? ? R m , L ?t-1 (?) -L ?t-1 (? * (? t-1 ))</p><p>= E s??,a?U (A) <ref type="bibr">[(??(s, a)</ref>, ?? -? ? ? t-1 (s, a)) 2 ] -L ?t-1 (? * (? t-1 ))</p><p>= E s??,a?U (A) <ref type="bibr">[(??(s, a)</ref>, ? -? * (? t-1 )? + ??(s, a), ? * (? t-1 )? -? ? ? t-1 (s, a)) 2 ] -L ?t-1 (? * (? t-1 ))</p><p>= E s??,a?U (A) <ref type="bibr">[??(s, a)</ref>, ? -? * (? t-1 )? 2 ] + 2?? -? * (? t-1 ), E s??,a?U (A) <ref type="bibr">[(??(s, a)</ref>, ? * (? t-1 )? -? ? ? t-1 (s, a))?(s, a)]? = E s??,a?U (A) <ref type="bibr">[??(s, a)</ref>, ? -? * (? t-1 )? 2 ] + ?? -? * (? t-1 ), ? ? L ?t-1 (? * (? t-1 ))? ? E s??,a?U (A) <ref type="bibr">[??(s, a)</ref>, ? -? * (? t-1 )? 2 ] ,</p><p>where the last inequality stems from the first-order optimality condition for ? * (? t-1 ) ? arg min ? L ?t-1 (?) , which gives the inequality ?? -? * (? t-1 ), ? ? L ?t-1 (? * (? t-1 ))? ? 0 for every ? ? R m .</p><p>Combining ( <ref type="formula" target="#formula_295">139</ref>) with (140) and using Assumption 5.2, we obtain </p><p>Combining ( <ref type="formula" target="#formula_292">136</ref>), ( <ref type="formula" target="#formula_293">137</ref>), ( <ref type="formula">141</ref>) and ( <ref type="formula" target="#formula_297">142</ref>) yields</p><formula xml:id="formula_298">E[?[? ? ?(? t )] T (r * t-1 -r t-1 )? 2 ] ? 8l 2 ? L 2 ? (1 -?) 4 |A| ? min (? stat + ? approx ) . (<label>143</label></formula><formula xml:id="formula_299">)</formula><p>Term 3 in (134): For this last term, we have</p><formula xml:id="formula_300">E[?[? ? ?(? t )] T r t-1 -u t ? 2 ] (a) = E ? ? 1 N N i=1 ([? ? ?(? t )] T r t-1 -g(? (i) t , ? t , r t-1 )) 2 ? ? (b) = 1 N E[?g(? (i) t , ? t , r t-1 ) -[? ? ?(? t )] T r t-1 ? 2 ] (c) ? 1 N E[?g(? (i) t , ? t , r t-1 )? 2 ] (d) ? 4l 2 ? l 2 ? N (1 -?) 4 ,<label>(144)</label></formula><p>where (a) stems from the definition of u t , (b) follows from using Lemma E.3 and recalling that the trajectories (? (i) t ) 1?i?N are independently drawn in Algorithm 3, (c) is due to the inequality Var(X) ? E[?X? 2 ] for any random vector X ? R d and (d) uses a similar bound to (70).</p><p>Combining ( <ref type="formula">134</ref>), ( <ref type="formula" target="#formula_291">135</ref>), ( <ref type="formula" target="#formula_298">143</ref>) and (144) together with (132) gives</p><formula xml:id="formula_301">E[?? ? F (?(? t )) -u t ? 2 ] ? C1 ? 2 ?u t-1 ? 2 + C2 (? stat + ? approx ) + C3 N + 2D 2 ? ? 2H ,<label>(145)</label></formula><p>where C1 =</p><formula xml:id="formula_302">48l 3 ? L 2 ?,?</formula><p>(1-?) 6 , C2 = Then, we upper bound the remaining sum in the right-hand side of (147) as follows:</p><formula xml:id="formula_303">1 T T t=1 10 C1 ? 2 E[?u t-1 ? 2 ] -2E[?u t ? 2 ] = 1 T T t=1 (10 C1 ? 2 -2)E[?u t-1 ? 2 ] + 2 T T t=1 E[?u t-1 ? 2 -?u t ? 2 ] (a) ? 2 T T t=1 E[?u t-1 ? 2 -?u t ? 2 ] (b) ? 2E[?u 0 ? 2 ] T (c) ? C4 T ,<label>(148)</label></formula><p>where C4 = </p><p>where ?T be a random iterate drawn uniformly at random from {? 1 , ? ? ? , ? T }. This concludes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.3. Proof of Corollary 5.6: Sample complexity analysis</head><p>In order to establish the total sample complexity of our algorithm, we shall use Theorem 1 in Bach &amp; Moulines (2013) for the least-mean-square algorithm corresponding to SGD for least-squares regression to explicit the number of samples needed in the occupancy measure estimation subroutine of Algorithm 2. In other words, our objective here is to precise the number of iterations of SGD needed to approximately solve our regression problem. In particular, we will show that we can achieve ? stat = O(1/K) where K is the number of iterations of the SGD subroutine. We first report Theorem 1 from Bach &amp; Moulines (2013) before applying it to our specific case.</p><p>Theorem G.2 (Theorem 1, <ref type="bibr" target="#b1">(Bach &amp; Moulines, 2013)</ref>). Let H be an m-dimensional Euclidean space with m ? 1 . Let (x n , z n ) ? H ? H be independent and identically distributed observations. Assume the following: Consider the Stochastic Gradient Descent (SGD) recursion started at ? 0 ? H and defined for every integer n ? 1 as</p><formula xml:id="formula_305">? n = ? n-1 -? ? (?? n-1 , x n ?x n -z n ) ,<label>(150)</label></formula><p>where ? ? &gt; 0 . Then for a constant step size ? ? = 1 4R 2 the averaged iterate ?n </p><p>Note here that we consider the unbiased estimator ?? ? H (s, a) of the truncated state-action occupancy measure as computed in Algorithm 5. Remark G.3. One could also consider the unbiased estimator ?? ? H (s, a) of the true state-action occupancy measure (without truncation) using Algorithm 6 and slightly modify the definition of the expected loss with ?? ? (s, a) instead of ?? ? H (s, a) . The latter procedure would lead to the same result since the truncation error can be made as small as desired via setting the horizon large enough, the error being of the order of ? H . Take x n = ?(s, a) ? R m , z n = ?? ? H (s, a)?(s, a) ? R m . The observations (x n , z n ) are indeed independent and identically distributed (for each state-action pair (s, a) sample). </p><formula xml:id="formula_307">E[?] = E 1 2 ?? L ? (? * ) = 1 2 ? ? L ? (? * ) = 0 ,<label>(154)</label></formula><p>where the last identity stems from the definition of the optimal solution ? * . </p><p>Proof. Notice that 1 -? t = t+? -2 t+? . Then for all t ? 1 r t ? t + ? -2 t + ? r t-1 + ? t .</p><p>Multiplying both sides by (t + ? ) 2 , we get (t + ? ) 2 r t ? (t + ? -2)(t + ? )r t-1 + ? t (t + ? ) 2 ? (t + ? -1) 2 r t-1 + ? t (t + ? ) 2 .</p><p>By summing this inequality from t = 1 to T , we obtain</p><formula xml:id="formula_309">(T + ? ) 2 r T ? ? 2 r 0 + T t=1 ? t (t + ? ) 2 .</formula><p>H.3. Technical lemma for decreasing stepsizes Lemma H.5. Let q ? [0, 1] and let ? t = 2 t+2 q for every integer t. Then for every integer t and any integer T ? 1 we have</p><formula xml:id="formula_310">? t (1 -? t+1 ) ? ? t+1 ,<label>(162)</label></formula><p>T -1 t=0</p><p>(1 -? t+1 ) ? ? T .</p><p>Proof. For every integer t we have</p><formula xml:id="formula_311">1 -? t+1 = 1 - 2 t + 3 q ? 1 - 1 t + 3 = t + 2 t + 3 ? ? t+1 ? t .</formula><p>Using the above result, we can write</p><formula xml:id="formula_312">T -1 t=0 (1 -? t+1 ) ? T -1 t=0 ? t+1 ? t = ? T ? 0 = ? T .</formula><p>Lemma H.6. Let q ? [0, 1), p ? 0, ? 0 &gt; 0 and let ? t = 2 t+2 q , ? t = ? 0 2 t+2 p for every integer t. Then for any integers t and T ? 1, it holds</p><formula xml:id="formula_313">T -1 t=0 ? t T -1 ? =t+1 (1 -? ? ) ? C? T ? -1 T ,</formula><p>where C &gt; 1 is an absolute constant depending on p and q.</p><p>Proof. See, for instance, <ref type="bibr">(Gadat et al., 2018, Proposition B.1)</ref> or <ref type="bibr">(Fatkhullin et al., 2023, Lemma 15)</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Corollary 4. 7 .</head><label>7</label><figDesc>Under the setting of Theorem 4.4, if we set ?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. (right) Nonlinear objective maximization in the FrozenLake environment and (left) Standard RL in the CartPole environment. In both cases, the performance curves represent the median return over 20 runs of the algorithms (with 20 seeds) and the shaded colored areas are computed with the 1/4 and 3/4 quantiles of the outcomes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>( a )</head><label>a</label><figDesc>Nonlinear objective function maximization. We consider a general utility RL problem where the objective function F : R |S|?|A| + ? R is a nonlinear function of the occupancy measure defined for every ? ? R |S|?|A| + by:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(right). The performance curves show that our NVR-PG algorithm shows a relatively faster convergence compared to the TSIVR-PG algorithm (Zhang et al., 2021b) and the MaxEnt algorithm which is specific to the maximum entropy exploration problem (by<ref type="bibr" target="#b22">Hazan et al. (2019)</ref>) while the final performances are comparable (see also the overlapping shaded areas). We refer the reader to Section 6.3 in Zhang et al. (2021b) for further details regarding our setting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>( a )</head><label>a</label><figDesc>Bounding term 1 in (137) by the statistical error. First, observe for this term thatE[???(?, ?), ??t-1 -? * (? t-1 )?? 2 ] = E |A| ??(s, a), ??t-1 -? * (? t-1 )? 2 ? ? = |A| ? min E E s??,a?U (A) [??(s, a), ??t-1 -? * (? t-1 )? 2 ] .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>E</head><label></label><figDesc>[???(?, ?), ??t-1 -? * (? t-1 )?? 2 ] ? |A| ? min E[L ?t-1 (? ?t-1 ) -L ?t-1 (? * (? t-1 ))] ? |A| ? min ? stat .(141)(b) Bounding term 2 in (137) by the approximation error. Similar derivations as for the previous term yieldE[???(?, ?), ? * (? t-1 )? -? H (? t-1 )? 2 ] = E ? ? s?S,a?A (??(s, a), ? * (? t-1 )? -? ,a?U (A)[(??(s, a), ? * (? t-1 )? -?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>?) 4 . Rearranging (131), dividing by ? 16 and taking full expectation yieldsE[?? ? F (?(? t ))? 2 ] ? 16 ? (F (?(? t+1 )) -F (?(? t ))) -2E[?u t ? 2 ] + 10E[?? ? F (?(? t )) -u t ? 2 ] .(146)Plugging (145) into (146), summing the resulting inequality for t = 1, ? ? ? , T and dividing byT gives ? F (?(? t ))? 2 ] ? 16 ?T E[F (?(? T +1 )) -F (?(? 1 )2 E[?u t-1 ? 2 ] -2E[?u t ? 2 ] + C2 (? stat + ? approx ) + C3 N + 2D 2 ? ? 2H (147)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>?) 4 , (a) stems from the condition ? ? 1 ? 5 C1 , (b) follows telescoping the sum and upper bounding the remaining resulting negative term by zero and (c) is a consequence of a similar bound to (70).Finally, we obtainE[?? ? F (?( ?T ))? 2 ] ? 16(F * -E[F (?(? 1 ))]) + ? C4 ?T + C3 N + 2D 2 ? ? 2H + C2 (? stat + ? approx ) ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>(i) The expectations E[?x n ? 2 ] and E[?z n ? 2 ] are finite; the covariance matrix E[x n x T n ] is invertible. (ii) The global minimum of f (?) = 1 2 E[??, x n ? 2 -2??, z n ?] is attained at a certain ? * ? H. Denoting by ? n def = z n -?? * , x n ?x n the residual, assume that E[? n ] = 0 . (iii) There exist R &gt; 0, ? &gt; 0 s.t. E[? n ? T n ] ? ? 2 E[x n x T n ] and E[?x n ? 2 x n x T n ] ? R 2 E[x n x T n ], where for two matrices A, B ? R m?m , A ? B if and only if B -A is positive semi-definite.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>R?? 0 -? * ?) 2 . (151)Proof of Corollary 5.6. It follows from Theorem 5.4 thatE[?? ? F (?( ?T ))? 2 ] ? 16(F * -E[F (?(? 1 ))]) + C4 ?T + C3 N + 2D 2 ? ? 2H + C2 (? stat + ? approx ) , (152)where ?T is a random iterate drawn uniformly at random from {? 1 , ? ? ? , ? T }. We now upper bound the statistical error ? stat as a function of the number K of SGD iterations (see Algorithm 2) by applying Theorem G.2. Let ? * ? arg min ? L ? (?) where ? ? R d is fixed (at each iteration of Algorithm 3). To do so, we successively verify each assumption of the latter theorem in the Euclidean space R m . Recall from (12) that the stochastic gradient of the loss function L ? (?) is given for every ? ? R d , ? ? R m by ?? L ? (?) def = 2(??(s, a), ?? -?? ? H (s, a)) ?(s, a) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>(i) Given Assumption 5.5, we haveE[?x n ? 2 ] = E[??(s, a)? 2 ] ? B 2 . Similarly we haveE[?z n ? 2 ] ? B 2 /(1 -?) 2 .Moreover, the covariance matrix E[?(s, a)?(s, a) T ] has full rank by Assumption 5.5.(ii) Take f = L ? . Define the residual ? def = ( ?? ? H (s, a) -?? * , ?(s, a)?)?(s, a) . Then we conclude the verification of the second item by observing that</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>(</head><label></label><figDesc>iii) As for this last item, recall again that ??(s, a)? ? B which immediately implies thatE[?x n ? 2 x n x T n ] ? R 2 E[x n x T n ] with R = B . It remains to show that the covariance matrix of ? satisfies E[?? T ] ? ? 2 E[?(s, a)?(s, a) T ]for some positive constant ? that we will now determine. First, we writeE[?? T ] = E[( ?? ? H (s,a) -?? * , ?(s, a)?) 2 ?(s, a)?(s, a) T ] = E E[( ?? ? H (s, a) -?? * , ?(s, a)?) 2 |s, a]?(s, a)?(s, a) T ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>??, ?? . The notation ??? refers to both the standard 2-norm and the spectral norm for vectors and matrices respectively. ? is the initial state distribution and ? ? (0, 1) is the discount factor. A stationary policy ? : S ? ?(A) maps each state s ? S to a distribution ?(?|s) over the action space A. The set of all stationary policies is denoted by ? . At each time step t ? N in a state s t ? S, the RL agent chooses an action a</figDesc><table><row><cell>Markov Decision Process with General Utility. Con-</cell></row><row><cell>sider a discrete-time discounted Markov Decision Process</cell></row><row><cell>(MDP) with a general utility function M(S, A, P, F, ?, ?),</cell></row><row><cell>where S and A are finite state and action spaces respec-</cell></row><row><cell>tively, P : S ? A ? ?(S) is the state transition probability</cell></row><row><cell>kernel, F : M(S ? A) ? R is a general utility function de-</cell></row><row><cell>fined over the space of measures M(S ? A) on the product</cell></row><row><cell>space S ? A,</cell></row></table><note><p>t ? A with probability ?(a t |s t ) and the environment transitions to a state s t+1 with probability P(s t+1 |s t , a t ) . We denote by P ?,? the probability distribution of the Markov chain (s t , a t ) t?N induced by the policy ? with initial state distribution ?.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Lemma 4.3. Let Assumption 4.1 hold true. Suppose that the sequence (? t ) is updated via ? t+1 = ? t +? t dt ?dt? where d t ? R d is any non-zero update direction and ? t is a positive stepsize. Then, for every integer t and any trajectory ? of length H, we have w(? |? t , ? t+1 ) ? exp{2Hl ? ? t } . If, in addition, H = O( log T 1-? ) and ? t = ? = T -2 3 , then there exists a constant W &gt; 0 s.t. w(? |? t , ? t+1 ) ? W . Moreover, we have Var [w(? t+1 |? t , ? t+1 )] ? C w ? 2 where ? t+1 is a trajectory of length H sampled from ? ?t+1 and C w</figDesc><table><row><cell>def</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Xu, P., Gao, F., and Gu, Q. An improved convergence analysis of stochastic variance-reduced policy gradient. In Uncertainty in Artificial Intelligence, pp. 541-551. PMLR, 2020b. Yuan, H., Lian, X., Liu, J., and Zhou, Y. Stochastic Recursive Momentum for Policy Gradient Methods, 2020. Normalization ensures boundedness of IS weights . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2 First-order stationarity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.3 Global optimality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Proof of Theorem 4.12 (General utilities setting) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.2 Proof of Corollary 4.13 (Cumulative reward setting) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Proof of Theorem 5.4: Convergence analysis under bounded statistical and approximation errors . . . . . 41 G.3 Proof of Corollary 5.6: Sample complexity analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 Smoothness, Lipschitzness and truncation error technical lemmas . . . . . . . . . . . . . . . . . . . . . . 47 H.2 Technical lemma for solving a recursion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 H.3 Technical lemma for decreasing stepsizes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48 Appendix A. N-VR-PG algorithm for standard RL setting with cumulative reward</figDesc><table><row><cell>Contents</cell><cell></cell></row><row><cell>1 Introduction</cell><cell></cell></row><row><cell>Yuan, R., Gower, R. M., and Lazaric, A. A general sam-ple complexity analysis of vanilla policy gradient. In Proceedings of The 25th International Conference on Artificial Intelligence and Statistics, volume 151 of Pro-ceedings of Machine Learning Research, pp. 3332-3380. PMLR, 2022. Yuan, R., Du, S. S., Gower, R. M., Lazaric, A., and Xiao, L. Linear convergence of natural policy gradient methods with log-linear policies. In The Eleventh International Conference on Learning Representations, 2023. Zhang, J., Bedi, A. S., Wang, M., and Koppel, A. Multi-agent reinforcement learning with general utilities via decentralized shadow reward actor-critic. Proceedings of the AAAI Conference on Artificial Intelligence, 36(8): 9031-9039, Jun. 2022. 2 Preliminaries 3 Normalized Variance-Reduced Policy Gradient Algorithm 4 Convergence Analysis of N-VR-PG 4.1 6 Numerical Simulations 7 Perspectives A F Proofs for Section 4.3: Global optimality convergence F.1 H Useful technical lemma H.1</cell><cell>47</cell></row></table><note><p>Zahavy, T., O'Donoghue, B., Desjardins, G., and Singh, S. Reward is enough for convex mdps. Advances in Neural Information Processing Systems, 34:25746-25759, 2021. Zhang, J., Koppel, A., Bedi, A. S., Szepesvari, C., and Wang, M. Variational policy gradient method for reinforcement learning with general utilities. Advances in Neural Information Processing Systems, 33:4572-4583, 2020. Zhang, J., Bedi, A. S., Wang, M., and Koppel, A. Cautious reinforcement learning via distributional risk in the dual domain. IEEE Journal on Selected Areas in Information Theory, 2(2):611-626, 2021a. doi: 10.1109/JSAIT.2021. 3081108. Zhang, J., Ni, C., Szepesvari, C., Wang, M., et al. On the convergence and sample efficiency of variance-reduced policy gradient method. Advances in Neural Information Processing Systems, 34:2228-2240, 2021b. 5 Large State-Action Space Setting 5.1 PG for RL with General Utilities via linear function approximation of the occupancy measure . . . . . . 5.2 Convergence and sample complexity analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . N-VR-PG algorithm for standard RL setting with cumulative reward B Dependence on (1 -?) -1 C Further discussion of Assumption 4.10 D Proof of Lemma 4.3 in Section 4.1 E Proofs for Section 4.2: First-order stationarity E.1 Proof sketch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.2 Proof of Theorem 4.4 (General utilities setting) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.3 Proof of Corollary 4.7 (Cumulative reward setting) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.4 Proof of Theorem 4.8 (Cumulative reward setting for continuous state-action space and Gaussian policy) . F.3 Global optimality in the cumulative reward setting for continuous state-action space and Gaussian policy . G Proofs for Section 5: Large state-action space setting G.1 Unbiased estimates of the occupancy measure at state-action pairs . . . . . . . . . . . . . . . . . . . . . G.2</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>2. As it was recently reported in<ref type="bibr" target="#b27">Zhang et al. (2020)</ref>, the direct parametrization satisfies the bijection Assumption 4.10. We refer the reader to Appendix H in (Zhang et al., 2020) for a complete proof of this fact. Notice also that Assumption 4.10 which was first introduced in (Zhang et al., 2021b) is a local version of Assumption 1 in (Zhang et al., 2020) and is hence less restrictive. 3. As for the softmax parametrization, verifying Assumption 4.10 is more challenging as we explain in the main part of the present paper. It is a delicate and interesting question to investigate whether Assumption 4.10 accommodates complex policy parametrizations such as practical neural networks or if it can be relaxed to do so.</figDesc><table /><note><p><p><p><p><p><p><p>D. Proof of Lemma 4.3 in Section 4.1</p>The proof of this lemma is simple and combines an elementary technical lemma from Zhang et al. (2021b) (Lemma 5.6) upper bounding the IS weights for the special case of the softmax parametrization with Lemma B.1 in</p>Xu et al. (2020a)   </p>which provides a bound on the variance of IS weights which are bounded as a function of the squared euclidean distance between two policy parameters.</p>Proof. Using the softmax parametrization (2) with Assumption 4.1 satisfied, Lemma 5.6 in</p>Zhang et al. (2021b)  </p>stipulates that for every ?, ? ? ? R d and every truncated trajectory ? = (s t , a t ) 0?t?H-1 of length H, the IS weights defined in (9) satisfy:</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Algorithm 5 Monte-Carlo estimate of the truncated state-action occupancy measure for (s, a): ? ? ? H (s, a) Input: Initial state distribution ?, state-action pair (s, a) ? S ? A, policy ? ? , discount factor ? ? [0, 1), truncation horizon H. Sample a trajectory ? = (s t , a t ) 0?t?H-1 from the MDP controlled by policy ? ? ?? ? Unbiased estimator of the state-action occupancy measure for (s, a): ? ? ? (s, a) Input: Initial state distribution ?, state-action pair (s, a) ? S ? A, policy ? ? , discount factor ? ? [0, 1), h = 0.s 0 ? ?, a 0 ? ? ? (?|s 0 ) Draw H from the geometric distribution Geom(1 -?) for h = 0, . . . , H -1 do s h+1 ? P (?|s h , a h ); a h+1 ? ? ? (?|s h ) end for ?? ? (s, a) = 1 {s H =s, a H =a}Return: ?? ? (s, a) .G.2. Proof of Theorem 5.4: Convergence analysis under bounded statistical and approximation errorsWe first state a more detailed version of Theorem 5.4.</figDesc><table><row><cell>Algorithm 6</cell></row></table><note><p>H (s, a) = H t=0 ? t 1 {st=s, at=a} Return: ?? ? H (s, a) .</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>All the proofs of our results are provided in the Appendix.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>We could use the non-truncated occupancy measure (see Appendix G). For simplicity of exposition, we use the truncated version, the difference between both quantities is of the order of ? H .</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>  4  Other exploratory sampling distributions for s and a can be considered, we choose ? and U(A) for</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>simplicity.5  We can also compute an unbiased estimator of the true occupancy measure ? ? ? (s, a) via a standard procedure with a random horizon H following a geometric</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_4"><p>distribution (see Algorithm 6).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_5"><p>Reinforcement Learning with General Utilities: Simpler Variance Reduction and Large State-Action Space</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_6"><p>Available in OpenReview (https://openreview.net/ forum?id=Re_VXFOyyO).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_7"><p>/2 4 ? t-2 ? t 1 /2 . (58)</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_8"><p>Notice that the integrand is nonnegative and we can integrate in any order by Tonelli's theorem.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work was supported by <rs type="funder">ETH AI Center doctoral fellowship, ETH Foundations of Data Science (ETH-FDS)</rs>, and <rs type="grantName">ETH Research Grant</rs> funded via <rs type="funder">ETH Zurich Foundation</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_HFQ9UkX">
					<orgName type="grant-name">ETH Research Grant</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>where the conditional expectation E[?|s, a] is w.r.t. randomness induced by sampling the state-action pair (s, a) . Then, we have for every s ? S, a ? A , </p><p>We know that | ?? ? H (s, a)| ? 1 1-? . It remains to bound ?? * ? to be able to upper bound the quantity of (156). Recall for this that ? ? L ? (? * ) = 0 , i.e., E <ref type="bibr">[(??(s, a)</ref>, ? * ? -? ? ? H (s, a))?(s, a)] = 0 , which can be rewritten as follows:</p><p>Therefore, we obtain by invoking Assumption 5.5 that</p><p>Using this inequality, it follows from (156) that:</p><p>Hence, the missing part of item (iii) is satisfied with</p><p>We conclude the proof by using the result of Theorem G.2 with ? ? = 2? = 1 4B 2 and ? 0 = 0 to obtain after K iterations of the SGD subroutine (see Algorithm 2)</p><p>where ?K is the output of Algorithm 2. As a consequence, we have</p><p>Plugging this inequality into (152) leads to</p><p>We set the number of iterations T , the batch size N , the number of iterations K in the subroutine of Algorithm 2 and the horizon H to guarantee that</p><p>where the expectation is taken over both the randomness inherent to the sequence produced by the algorithm together with the uniform sampling defining ?T .</p><p>) and H = O(log( 1 ? )) concludes the proof. In particular, the total sample complexity to solve the RL problem with general utilities with occupancy measure approximation in order to achieve an ?-approximate stationary point of the objective function (up to the O( ? ? approx ) error floor) is given by T ? (K + N ) ? H = ?(? -4 ) , where ? hides a logarithmic factor in ? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Useful technical lemma</head><p>In this section, we gather a few technical results that are useful throughout the proofs of our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.1. Smoothness, Lipschitzness and truncation error technical lemmas</head><p>The following result from (Zhang et al., 2021b)(Lemma 5.3) ensures in particular that the objective function ? ? F (? ? ? ) is smooth which is used to derive an ascent-like lemma in our convergence analysis.</p><p>Lemma H.1. Let Assumptions 4.1 and 4.2 hold. Then, the following statements hold:</p><p>Proof. See Lemma 5.3 in <ref type="bibr">(Zhang et al., 2021b)</ref>. The second part of item (ii) was not reported in the aforementioned reference but the proof follows the same lines upon replacing the infinite horizon but the finite one H for the truncated state-action occupancy measures.</p><p>The next lemma controls the truncation error due to truncating simulated trajectories to the horizon H in our infinite horizon setting. Notably, this error vanishes geometrically fast with the horizon H . Lemma H.2. Let Assumptions 4.1 and 4.2 be satisfied. Then, we have for any H ? 1 and every ? ? R d :</p><p>1-? + H and r is the fixed reward function in the cumulative reward setting .</p><p>Proof. See <ref type="bibr">Lemma E.3 in (Zhang et al., 2021b)</ref> for the first item. The second item is standard and follows directly from using the policy gradient expression together with Lemma H.1-(i).</p><p>The following result whose proof follows immediately from (7) and Assumption 4.1 (see <ref type="bibr">Lemma E.2, (Zhang et al., 2021b)</ref>) establishes the Lipschitz continuity of the policy gradient estimator w.r.t. the policy parameter and the reward variable.</p><p>Lemma H.3. Let Assumption 4.1 hold true and let ? = {s 0 , a 0 , ? ? ? , s H-1 , a H-1 } be an arbitrary trajectory of length H. Then the following statements hold:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.2. Technical lemma for solving a recursion</head><p>The next lemma is useful for solving recursions appearing in our analysis to derive convergence rates.</p><p>Lemma H.4. Let ? be a positive integer and let {r t } t?1 be a non-negative sequence satisfying for every integer t ? 1</p><p>where {? t } t?1 is a non-negative sequence. Then for ? t = 2 t+? we have for every integer T ? 1 r T ? ? 2 r 0 (T + ? ) 2 + T t=1 ? t (t + ? ) 2 (T + ? ) 2 .</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On the theory of policy gradient methods: Optimality, approximation, and distribution shift</title>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mahajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">98</biblScope>
			<biblScope unit="page" from="1" to="76" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Non-strongly-convex smooth stochastic approximation with convergence rate o(1/n)</title>
		<author>
			<persName><forename type="first">E</forename><surname>Altman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Moulines</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1999">1999. 2013</date>
			<biblScope unit="volume">26</biblScope>
		</imprint>
	</monogr>
	<note>Constrained markov decision processes</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">On the sample complexity and metastability of heavytailed policy search in continuous control</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Bedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Parayil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Koppel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08414</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On the hidden biases of policy mirror ascent in continuous action spaces</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Bedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Parayil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Sadler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tokekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Koppel</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th International Conference on Machine Learning</title>
		<meeting>the 39th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2022-07">Jul 2022</date>
			<biblScope unit="volume">162</biblScope>
			<biblScope unit="page" from="17" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Reinforcement learning and optimal control</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bertsekas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Athena Scientific</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An online actor-critic algorithm with function approximation for constrained markov decision processes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bhatnagar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lakshmanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Optimization Theory and Applications</title>
		<imprint>
			<biblScope unit="volume">153</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="688" to="708" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An actor-critic algorithm for constrained markov decision processes</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Borkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Systems &amp; control letters</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="207" to="213" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">G</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pettersson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01540</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">Openai gym. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Momentum-based variance reduction in non-convex sgd</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cutkosky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Orabona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Some remarks on finite horizon markovian decision models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Derman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="272" to="278" />
			<date type="published" when="1965">1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Beyond exact gradients: Convergence of stochastic soft-max policy gradient methods with entropy regularization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lavaei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.10117</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On the global optimum convergence of momentum-based policy gradient</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lavaei</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1910" to="1934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Explorationexploitation in constrained mdps</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Efroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pirotta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.02189</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Diversity is all you need: Learning skills without a reward function</title>
		<author>
			<persName><forename type="first">B</forename><surname>Eysenbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ibarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sharp analysis of stochastic optimization under global Kurdyka?ojasiewicz inequality</title>
		<author>
			<persName><forename type="first">I</forename><surname>Fatkhullin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Etesami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kiyavash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Stochastic policy gradient methods: Improved sample complexity for fisher-non-degenerate policies</title>
		<author>
			<persName><forename type="first">I</forename><surname>Fatkhullin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Barakat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kireeva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.01734</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Variancepenalized markov decision processes</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Filar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Kallenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of Operations Research</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="147" to="161" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Stochastic Heavy ball</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gadat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Panloup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Saadane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronic Journal of Statistics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="461" to="529" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A comprehensive survey on safe reinforcement learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Garc?a</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fern?ndez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1437" to="1480" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">PAGE-PG: A simple and loopless variancereduced policy gradient method with probabilistic gradient estimation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gargiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zanelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Martinelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Summers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lygeros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th International Conference on Machine Learning</title>
		<meeting>the 39th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">162</biblScope>
			<biblScope unit="page" from="7223" to="7240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Concave utility reinforcement learning: The mean-field game viewpoint</title>
		<author>
			<persName><forename type="first">M</forename><surname>Geist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>P?rolat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lauri?re</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Elie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Perrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bachem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Pietquin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems, AAMAS &apos;22</title>
		<meeting>the 21st International Conference on Autonomous Agents and Multiagent Systems, AAMAS &apos;22</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="489" to="497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A divergence minimization perspective on imitation learning methods</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K S</forename><surname>Ghasemipour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1259" to="1277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Provably efficient maximum entropy exploration</title>
		<author>
			<persName><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Soest</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2681" to="2691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Momentum-based policy gradient methods</title>
		<author>
			<persName><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4422" to="4433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Survey of linear programming for standard and nonstandard markovian control problems. part i: Theory</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Kallenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Zeitschrift f?r Operations Research</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="42" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Policy gradient for reinforcement learning with general utilities</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.00991</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Policy mirror descent for reinforcement learning: Linear convergence, new sampling complexity, and generalized problem classes</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1" to="48" />
		</imprint>
	</monogr>
	<note type="report_type">Mathematical programming</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">An improved analysis of (variance-reduced) policy gradient and natural policy gradient methods</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Basar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7624" to="7636" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Stochastic second-order methods improve best-known sample complexity of SGD for gradientdominated functions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Masiha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Salehkaleybar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kiyavash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Thiran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Reinforcement learning with convex constraints</title>
		<author>
			<persName><forename type="first">S</forename><surname>Miryoosefi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Brantley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename><surname>Daume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dudik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schapire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Active exploration via experiment design in markov chains</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mutny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Janik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 26th International Conference on Artificial Intelligence and Statistics</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Ruiz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Dy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-W</forename><surname>Van De Meent</surname></persName>
		</editor>
		<meeting>The 26th International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2023-04">Apr 2023</date>
			<biblScope unit="volume">206</biblScope>
			<biblScope unit="page" from="25" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The importance of non-markovianity in maximum state entropy exploration</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mutti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>De Santi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Restelli</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th International Conference on Machine Learning</title>
		<meeting>the 39th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2022-07">Jul 2022a</date>
			<biblScope unit="volume">162</biblScope>
			<biblScope unit="page" from="17" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Challenging common assumptions in convex reinforcement learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mutti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Santi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Bartolomeis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Restelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Stochastic variance-reduced policy gradient</title>
		<author>
			<persName><forename type="first">M</forename><surname>Papini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Binaghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Canonaco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pirotta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Restelli</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4026" to="4035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A hybrid stochastic policy gradient algorithm for reinforcement learning</title>
		<author>
			<persName><forename type="first">N</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dijk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tran-Dinh</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="374" to="385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Markov decision processes: discrete stochastic dynamic programming</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Puterman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Adaptive momentum-based policy gradient with second-order information</title>
		<author>
			<persName><forename type="first">S</forename><surname>Salehkaleybar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khorasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kiyavash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Thiran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.08253</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Trust region policy optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Moritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1889" to="1897" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Klimov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06347</idno>
		<title level="m">Proximal policy optimization algorithms</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Hessian aided policy gradient</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mi</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5729" to="5738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Reinforcement learning: An introduction</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Policy gradient methods for reinforcement learning with function approximation</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mansour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">An analysis of temporaldifference learning with function approximation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tsitsiklis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Roy</surname></persName>
		</author>
		<idno type="DOI">10.1109/9.580874</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automatic Control</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="674" to="690" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Neural policy gradient methods: Global optimality and rates of convergence</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">On the convergence rates of policy gradient methods</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">282</biblScope>
			<biblScope unit="page" from="1" to="36" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Sample efficient policy gradient methods with recursive variance reduction</title>
		<author>
			<persName><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
