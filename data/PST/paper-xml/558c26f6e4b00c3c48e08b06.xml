<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Capturing Time-of-Flight Data with Confidence</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Malcolm</forename><surname>Reynolds</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jozef</forename><surname>Dobo≈°</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Leto</forename><surname>Peel</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Advanced Technology Centre</orgName>
								<orgName type="institution">BAE Systems</orgName>
								<address>
									<settlement>Bristol</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tim</forename><surname>Weyrich</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Gabriel</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
						</author>
						<title level="a" type="main">Capturing Time-of-Flight Data with Confidence</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">47F25B31EC7365665F68A07F32CE002F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T07:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Time-of-Flight cameras provide high-frame-rate depth measurements within a limited range of distances. These readings can be extremely noisy and display unique errors, for instance, where scenes contain depth discontinuities or materials with low infrared reflectivity. Previous works have treated the amplitude of each Time-of-Flight sample as a measure of confidence. In this paper, we demonstrate the shortcomings of this common lone heuristic, and propose an improved per-pixel confidence measure using a Random Forest regressor trained with real-world data. Using an industrial laser scanner for ground truth acquisition, we evaluate our technique on data from two different Time-of-Flight cameras 1 . We argue that an improved confidence measure leads to superior reconstructions in subsequent steps of traditional scan processing pipelines. At the same time, data with confidence reduces the need for point cloud smoothing and median filtering.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Time-of-Flight (ToF) cameras have been successfully used for a variety of applications such as Simultaneous Localisation and Mapping (SLAM) <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b24">25]</ref>, 3D reconstruction of static scenes <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b33">34]</ref>, and object tracking and scene analysis <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b32">33]</ref>. In contrast to stereo vision and triangulation-based scanners, the ToF camera operates from a single viewpoint and does not rely on matching of corresponding features, which greatly increases its robustness in the presence of traditionally difficult scene materials and internal occlusions. Compared to ToF laser scanners, ToF cameras deliver a lower-resolution 2D depth image at much higher framerates and at competitive costs of deployment. With costs expected to dramatically decrease in the near future, ToF cameras are likely to become a commodity item.</p><p>Despite the advantages of this new technology, it suffers from problems such as low signal-to-noise ratio, multiple light reflections and scattering. All these affect the recorded Figure <ref type="figure">1</ref>. Flying pixels. (a) Color-coded ToF depth map (red indicates close geometry, blue indicates distant geometry). Erroneous depth readings are caused by materials with unsuitable reflectivity and large depth discontinuities (see bottom ellipse). These pixels collect modulated light from both the foreground and the background and give a resulting distance which is somewhere between the two. The top ellipse shows pixels flying towards the camera, but this is a different phenomenon, caused by highly specular reflections from the top of the monitor. (b) intensity and (c) amplitude images.</p><p>depth values and hence produce erroneous 3D points, whose reliability depends on various scene parameters. Previous works <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b33">34]</ref> rely on the amplitude values (Fig. <ref type="figure">1(c</ref>)) as an indicator of confidence for point removal and filtering. In agreement with Kolb et al. <ref type="bibr" target="#b15">[16]</ref>, our experiments reveal that simply thresholding low-amplitude values is insufficient to remove inaccurate pixels-valid points could be lost before flying pixels and other anomalies as depicted in Fig. <ref type="figure">1</ref>(a) are removed.</p><p>We observe that basing a confidence measure on the image amplitude alone disregards valuable additional information. Explicitly incorporating additional cues, such as depth and local orientation, however, may be challenging, due to the difficulty of developing a descriptive error model that covers their interplay. Instead, we propose to use Random Forests <ref type="bibr" target="#b4">[5]</ref> to train a regressor to infer the reliability of a depth sample from these input quantities. Random Forests have proven useful as a supervised learning technique that performs feature selection, and are even being used for realtime 3D classification tasks <ref type="bibr" target="#b31">[32]</ref>.</p><p>Analogously to Mac Aodha et al. <ref type="bibr" target="#b18">[19]</ref> and Carreira and Sminchisescu <ref type="bibr" target="#b5">[6]</ref>, we obtain a per-point confidence assignment, which could be exploited in other data filtering and 3D reconstruction systems <ref type="bibr" target="#b22">[23]</ref>.</p><p>We qualitatively and quantitatively evaluate the robustness of our approach with two different ToF cameras, comparing it against baseline approaches of depth sample filtering. We show that our machine-learning-based approach outperforms previous methods, leading to a confidence measure of high discriminative power.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>Optical range cameras <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b29">30]</ref> measure the time of flight of near-infrared (NIR) light emitted by the camera's active illumination and reflected back from the scene. The illumination is modulated either by a sinusoidal or pseudonoise signal, depending on the camera model. The phase shift of the received demodulated signals conveys the time between emission and detection, indicating how far the light has traveled and thus indirectly measuring the depth of the scene. Due to the periodicity in the modulation signal, these devices have a limited working range, usually only up to 7.5 m. Distances beyond this point are recorded modulo of the maximum depth, known as wraparound error <ref type="bibr" target="#b23">[24]</ref>. ToF cameras output images of distance and grayscale intensity (Fig. <ref type="figure">1(b)</ref>). With knowledge of the camera intrinsics, the distance readings can be converted to a 3D point cloud (Fig. <ref type="figure">1(a)</ref>). Some ToF sensors also record an amplitude value at each pixel, which indicates the amount of modulated light reaching the sensor and disregards external illumination and incoherent stray light (Fig. <ref type="figure">1(c)</ref>). Similarly to intensity, this amplitude is influenced by scene depth, surface materials and orientation, and lens vignetting <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b21">22]</ref>. Low amplitude suggests that a pixel has not received enough light to produce an accurate depth measurement, although saturation of highintensity responses may also lead to invalid readings, despite a high amplitude <ref type="bibr" target="#b12">[13]</ref>. If the amplitude values are not provided by the camera, intensity images have been used instead <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Depth Corrections and Calibration</head><p>When dealing with measurement devices, it is important to properly understand the causes of potential errors and calibrate for them. Given the optical properties of ToF cameras, a standard intrinsic camera calibration sufficiently describes the focal length, principal focal point and lens distortion <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b17">18]</ref>. To further improve the unreliable depth readings caused by systematic errors, Lindner and Kolb <ref type="bibr" target="#b17">[18]</ref> estimate a higher-order polynomial function which takes into account global as well as per-pixel inaccuracies. Assuming such initial depth calibration, Beder et al. <ref type="bibr" target="#b1">[2]</ref> further estimate the camera's 3D pose using a single image of a checkerboard. Unfortunately, as Boehm and Pattinson <ref type="bibr" target="#b2">[3]</ref> recently showed, even a careful calibration is not sufficient for real-life scenarios. Despite calibration, they report errors in pose estimates that are an order of magnitude larger than the error modeled by the calibration. These errors are scene-dependent and cannot be overcome by calibration <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b19">20]</ref>. Fuchs and May <ref type="bibr" target="#b11">[12]</ref> and May et al. <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>, on the other hand, find known camera positions using a robotic arm to achieve high-precision results for both the initial calibration and final 3D reconstruction, using variations of the Iterative Closest Point (ICP) <ref type="bibr" target="#b25">[26]</ref> algorithm. They demonstrate an overall error accumulation which has to be globally relaxed when closing a loop.</p><p>In contrast, Schiller et al. <ref type="bibr" target="#b27">[28]</ref> use up to 80 images from each camera from their rigidly mounted multi-recording setup, improving on previous results of single ToF camera calibration methods. Similarly, Kim et al. <ref type="bibr" target="#b14">[15]</ref> combine several video and ToF cameras, while taking into account the scene-dependent effects. They introduce ratios of normalized amplitude values, which are median and Gaussian filtered. Random noise, however, is assumed to be negligible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">3D Reconstruction and Filtering</head><p>Feulner et al. <ref type="bibr" target="#b9">[10]</ref> register consecutive ToF frames by detecting binary edge presence in the intensity images and aligning their corresponding 3D coordinates by maximizing the uncentered correlation coefficient. In contrast, Fuchs and May <ref type="bibr" target="#b11">[12]</ref> filter points at depth discontinuities, as these exhibit the largest distance error. Swadzba et al. <ref type="bibr" target="#b33">[34]</ref> present a full acquisition pipeline that relies on several preprocessing steps to improve depth accuracy: a varying distance-adaptive median filter is applied to the intensity, amplitude, and depth images, and points with low amplitude are thresholded. Subsequently, a custom-made neighborhood consistency filter detects and removes flying pixels at edge locations in the distance image. Nevertheless, all of the above produce only binary classification of erroneous depth readings.</p><p>Fusing high resolution color images with ToF data provides enhanced range maps, as shown by Yang et al. <ref type="bibr" target="#b35">[36]</ref>.</p><p>In their work, a bilateral filter aggregates probabilities of depth estimates based on color affinity, iteratively reducing the level of up-sampled blur that occurs due to interpolation in discontinuous areas. Our main inspiration, however, comes from the work of Schuon et al. <ref type="bibr" target="#b28">[29]</ref>. Their Lidar-Boost method combines several noisy ToF frames into a single, high-resolution depth image. By assigning zero confi- dence to low amplitude pixels (i.e. thresholding), the results are further improved. This technique was later built upon by Cui et al. <ref type="bibr" target="#b7">[8]</ref> to achieve state-of-the-art reconstruction results. In their work, one point cloud is randomly selected as a reference model, and remaining frames from the sequence are aligned with it. At each point, a multi-variate Gaussian with fixed uniform covariance is centered, and the maximum likelihood estimate is found through Expectation Maximization (EM). Unfortunately, all sources of systematic error (such as surface orientation and reflectance, camera's integration time, or depth) are neglected, and the only assumption is an increasing bias growing from the image center.</p><p>In contrast, our approach derives confidence measures across both systematic and random errors. As a singleframe method operating directly on the camera output, our method is further amenable to combination with many of the approaches mentioned above, providing an improved prior for depth sample confidences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Learning ToF Confidence</head><p>Each model of ToF camera has its own set of characteristics and inaccuracies. To make meaningful predictions about the confidence of a camera's output, we propose that these characteristics can be learned by comparing data from the camera with ground truth distance. The objective is to assign a confidence value in the range [0, 1] to each point produced by a ToF camera, where 1 signifies a distance reading believed to be completely accurate while 0 signifies a reading that should be ignored. This assignment of confi- dence is performed with a trained Random Forest operating in regression mode. Regression is preferable here to classification because confidence is continuous, and different applications will stipulate their own expectations of accuracy. Other supervised learning systems such as SVMs <ref type="bibr" target="#b6">[7]</ref> could also be used. Random Forests were chosen because they automatically perform feature selection and they require no cross-validation due to the use of out-of-bag estimates during training <ref type="bibr" target="#b4">[5]</ref>. We acquire training data of the form {(x i , y i )} N i=1 . Each feature vector x i is encoded from a single point recorded by the ToF camera, and the corresponding y i is the target value, or confidence. These confidences are computed from the difference between the ToF's reported distance and the ground truth distance for each reading. When the disparity between these two values is low, the correct label is close to 1, whereas for pixels where the camera's reported distance is significantly in error, the label value is close to 0. Once the Random Forest has been trained, we can compute a prediction y * for an unseen x * , allowing a confidence map to be created for each new image (see Fig. <ref type="figure">2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Data Capture</head><p>To compute ground truth label values for each pixel, a true distance image D must be generated. This image is the same resolution as the output from the ToF camera, and each pixel contains the true distance, i.e. the reading that the ToF would have produced were it completely accurate. The viewpoint of this perfect depth image must match the actual location of the ToF camera to be meaningful. In practice, we obtained ground truth by fixing a ToF camera close to a high-end, Time-of-Flight laser scanner, so that the view frustra overlap as much as possible (see Fig. <ref type="figure" target="#fig_1">4</ref>). In principle, its measurements may be subject to similar errors as the ToF camera. The scanner's precision and resolution are significantly higher than the camera's <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b10">11]</ref>, which makes it sufficient for ground-truth in our case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Calibration</head><p>The ToF camera intrinsics were computed using images of a checkerboard and the Bouguet toolbox <ref type="bibr" target="#b3">[4]</ref>. The intensity images I have properties similar to a standard grayscale camera, so no modifications to the calibration code are required.</p><p>The intrinsic parameters included focal lengths in x and y, the principal point, and a 5-parameter lens distortion model. The extrinsics of the scanner relative to the ToF camera were computed by placing targets in the scene. Paper photogrammetry targets (see Fig. <ref type="figure" target="#fig_0">3</ref>) with printed circles were attached to available flat surfaces throughout the scene, spanning the range of depths and spread across the joint field of view of both systems. The 2D location of ellipse centers was determined with sub-pixel accuracy in I, and in an equivalent intensity image from the laser scanner. The 2D target centers in I and the 3D locations of the target centers determined by the laser scanner were used to compute an estimate of the extrinsics which minimized the squared reprojection error.</p><p>Due to the low resolution of current ToF cameras, the extrinsics calibration based on target detection may only be accurate enough to initialise registration. Experiments with targets at the back and sides of the capture volume showed that background geometry reprojected accurately but foreground objects were offset sideways in D compared to the ToF recorded D. Using unweighted rigid ICP, the initial registration was refined by aligning the laser point cloud with the ToF point cloud. A streamlined one-shot registration technique could be useful if large quantities of training data were acquired in the future, possibly leading to even better confidence estimates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Computing Ground Truth Depth</head><p>After calibration, a projection of the laser geometry into the ToF camera is computed for comparison against the ToF's own depth image. The laser scan has much higher resolution than the ToF, (a typical scan was 1251 √ó 1055 compared to 200 √ó 200 for the ToF, while covering roughly the same field of view), but still consists of points not surfaces. As a result, many laser scanned points at various depths fall within the point spread function of a ToF pixel, as shown in Fig. <ref type="figure" target="#fig_1">4</ref>. One could choose the closest, i.e. frontmost of the points for Di,j , but flying pixels occasionally occur in laser data as well and could provide erroneous depths. Neither the minimum nor the mean of these points' depths would be appropriate (Fig. <ref type="figure" target="#fig_1">4</ref>). All laser points which project into the sensor region of pixel (i, j) are used to compute Di,j . We fit a Mixture of 3 Gaussians using EM to the depth values of these points. Di,j is set to the minimum of the means of the fitted Gaussians. Finally, due to parallax and occlusion, the ToF camera may be sampling parts of the surface geometry that were unseen by the laser scanner. By placing The area visible to the ToF between D and E is not covered by the laser scanner and so no training data is acquired here.</p><p>the ToF camera very close to the laser scanner, we minimize the areas of the ToF image for which no ground truth exists. Each depth value in D must be converted to a training label y ‚àà [0, 1]. Intuitively, the difference between D and the ToF's D is normalized by D to find the relative depth error. Relative depth error is used because we assume that it is unrealistic to expect ToF cameras to have uniform absolute accuracy at all depths <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b14">15]</ref>. The relative depth error is passed through the arctan sigmoidal function, reflected, and rescaled so that a relative distance error of 0 becomes 1, and a hypothetical infinite distance error becomes 0. The final confidence label y is</p><formula xml:id="formula_0">y ij = 1 - 2 œÄ arctan Œ± abs( Di,j -D i,j ) Di,j ,<label>(1)</label></formula><p>where the parameter Œ± controls how quickly y tends to 0 as relative distance error tends to infinity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Feature Extraction</head><p>A feature vector x i,j is computed for each pixel (i, j) in the ToF image. To allow the Random Forest to generalize the properties of pixels with high and low confidence, three types of data are used to construct the feature vector. Note that Random Forests calculate the importance of each entry in the feature vector as part of the training process. This feature selection property makes it possible to use a large number of features that may be correlated with confidence, and have the training process automatically determine which ones were in fact useful. The categories of features are: local features from a single point (i, j), spatial features calculated from a neighbourhood surrounding (i, j), and global features calculated from the entire frame.</p><p>Local Features. The primary feature values at each ToF pixel (i, j) consist of the intensity I i,j , signal amplitude A i,j if available, and distance D i,j . These elements are included to allow learning of different physical phenomena, e.g. pixels not receiving enough light to compute an accurate distance where the scene has low reflectivity in NIR light spectrum, or receiving too much light so that pixels saturate. Distance is included because, in accordance with the inverse square law, surfaces a larger distance away are likely to have a higher error magnitude. The radial distance of each pixel is also included in the feature vector to allow the possibility of learning different error characteristics near to the edge of an image. Spatial Features. Filters are used to incorporate local spatial information about the scene. Neglecting perspective projection of the camera, we find approximations to the normal angle in x and y. A Laplacian filter is convolved with both the distance and intensity images. This filter is commonly used for edge detection, so it is included in the feature vector to allow the forest to learn the relationship between error and depth discontinuities. Our initial observations indicated that flying pixels at depth discontinuities were among the most noticeable artifacts in ToF data, but that not all sharp depth changes were incorrect. As well as the 3 √ó 3 Laplacian kernel, 5 √ó 5 and 7 √ó 7 versions are used to incorporate information at different scales.</p><p>Gabor filters <ref type="bibr" target="#b8">[9]</ref> describe a family of kernels, which differ from the Laplacian in that they can be set to a particular orientation. Whereas a large response from the Laplacian indicates the presence of an edge, Gabor filters at different orientations Œ∏ produce a response which can determine the direction of the edge. Filters at 0 ‚Ä¢ , 45 ‚Ä¢ , 90 ‚Ä¢ and 135 ‚Ä¢ were computed over the image. Due to the more complex form of the Gabor wavelet, a 13 √ó 13 filter was used over the whole image.</p><p>Global Features. Inspired by Kim et al. <ref type="bibr" target="#b14">[15]</ref> we allow the forest to learn the relationship between some feature at a particular point and the distribution of feature values across the whole image. For every feature listed above except radial distance, the minimum, maximum, mean, and median values across the whole frame were included. Because of this, each feature already mentioned contributes 5 elements to the feature vector, giving a final dimensionality of 91 (18 features √ó 5, plus radial distortion). Our final feature vector takes advantage of many of the cues used deterministically in previous works to probabilistically assess the quality of ToF data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>To evaluate the confidence assignment method, scenes were captured with a PMD CamCube 2.0 ToF camera. This camera has a resolution of 200 √ó 200 px, and provides both amplitude and intensity images. One scene of a staircase (stairs Fig. <ref type="figure" target="#fig_0">3(b)</ref>) and one scene of an office (room Fig. <ref type="figure" target="#fig_0">3(a)</ref>) were captured, both with a number of clutter items placed at different depths in the scene. The objects were chosen to include a wide variety of surface reflectance properties, including plastic, wood, textiles, polystyrene, and brushed metal. The object placement was intended to create multiple depth discontinuities of varying magnitudes, and to span a variety of different errors on which to train the model. For these scenes, a laser scanner was used to acquire ground truth, so the scenes could be used for training and quantitative experiments. Additional scenes were captured without ground truth (see supplementary material for full details) to be used for qualitative evaluation, so that our algorithm was tested on a total of 4 different scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Quantitative Results</head><p>To generate quantitative results, a forest was trained on the stairs scene and tested on the room scene. 125 trees were generated with a maximum depth of 25, forest accuracy of 0.0001 and the labels were generated with Œ± = 7.</p><p>It is standard practice when dealing with noisy point cloud data to delete points which are deemed to have the highest error, so that further processing must cope only with more reliable data. Previous work such as Schuon et al. <ref type="bibr" target="#b28">[29]</ref> used amplitude as a per-point quality indicator, deleting points with low values. The probabilistic confidence output from our algorithm can be thresholded in the same way to produce an alternative binary classifier for points.</p><p>During the review process a "geometric" baseline algorithm was suggested, which classified each point by computing the distance to the closest other point in each scan, so a small distance would indicate a reliable point. We include comparisons of our method to both this method and the amplitude baseline.</p><p>To quantify the relative improvement of our method over simply using the amplitude or examining geometric properties of the data, we present ROC curves on room in Fig. <ref type="figure">5</ref>. After discarding points for which no laser geometry or filter output was available, 31k points were left for classification. The ground truth in this binary classification task was computed by thresholding relative distance error from (1). Fig. <ref type="figure">5</ref>(a) shows the classifiers' accuracy at correctly identifying which points are within 4% of the true depth, and Fig. <ref type="figure">5</ref>(b) shows the same for 25%. ROC curves are generated by sweeping each metric from minimum to maximum value, where the metrics are amplitude, confidence posterior probability, and inverse distance to sample point.</p><p>The average ROC Area Under Curve (AUC) calculated across the error tolerance range {1 . . . 25}% (see Table <ref type="table">1</ref>) shows our technique outperforming both the amplitude and geometric baselines on average, and in fact in each of these 25 cases, the AUC for confidence thresholding is largest. Further graphs for these settings can be found in the supplementary material. Our technique no longer has a clear advantage over the baselines for tolerances above 25%, but very few bad points remain. Over 93% of the data already falls within this tolerance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Qualitative Results</head><p>It is informative to examine which points are deleted by each ToF assessment algorithm. We compare amplitude thresholding, the thresholding technique of Swadzba et al. <ref type="bibr" target="#b33">[34]</ref>, and confidence thresholding, referred to as methods A, B, and C (Fig. <ref type="figure" target="#fig_2">6</ref>). Method B performs amplitude thresholding after smoothing the amplitude image, and additionally discards points based on edge detection. It is unclear how to vary both the amplitude and edge parameters to draw a fair ROC. As our technique is only being used to threshold points and not actually change their positions, we omit their distance smoothing step.</p><p>Using the recommended parameters, method B removes 27% of the 35, 344 points in room. Thresholds for methods A and C were adjusted to remove the same number of points for direct comparison, although we would use a lower threshold on method C to improve results. As the Fig. <ref type="figure" target="#fig_2">6</ref> insets show, methods A and B are not as comprehensive in their removal of flying pixels at depth discontinuities. Please see the supplementary material to better inspect the 3D data. The Random Forest training process calculates the relevance of each entry of the feature vector, which allows us to analyze which features carry the most information as shown in Fig. <ref type="figure">7</ref>. A full graph is included in the supplementary material. Distance is the most important factor, followed by intensity, the Laplacian of the distance image at two different scales, and finally the amplitude. For this analysis, the forhas been trained on both stairs and room, which allows the variation in the global features to contribute to the confidence model. It is shown that the global features based on amplitude contain the most discriminative content. Note that when training on only one scene, all the global features are constant across the whole training set and so have the same level of importance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Application to Different Model ToF</head><p>To evaluate the effectiveness of our technique on a different model of ToF camera, we also collected training data for the Mesa SwissRanger SR-3100. This camera does not provide an amplitude image, only intensity and distance, so the feature vector was correspondingly smaller. Fig. <ref type="figure" target="#fig_3">8</ref> shows a photo of a test scene which included large depth discontinuities and many flying pixels. The thresholded point cloud once more shows successful elimination of flying pixels and other forms of outliers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion and Future Work</head><p>We have presented a method to assign per-point confidences to ToF scans using a supervised learning approach. Both qualitative and quantitative results show marked improvement on contemporary methods for the removal of inaccurate points. The algorithm is particularly good at de-  tecting flying pixels. Demonstrations using different camera models suggest that our confidence assignment method is hardware agnostic. We anticipate methods such as Schuon et al. <ref type="bibr" target="#b28">[29]</ref> could be improved with our confidences. ICP is a common algorithm for merging point clouds, and could be augmented by applying confidence weighting to each matching pair of points. Our confidence measure could potentially be adapated to support the semantic cues used for SfM in Bao and Savarese <ref type="bibr" target="#b0">[1]</ref>. Similarly, the initial mesh models reconstructed using multi-view stereo and then refined based on shading in Wu et al. <ref type="bibr" target="#b34">[35]</ref> could instead start from our ToF data, and be more malleable where the confidence was low.</p><p>As with all supervised learning methods, for a high level of performance, a representative training set is required. Despite the limited size and variety in our training set, the technique has proven successful on unseen test data. Some artifacts remain, such as the incorrectly deleted points in the green square in Fig. <ref type="figure" target="#fig_2">6(c</ref>). We surmise that adding data from a wider range of scenes, including bright nearby objects, would further improve generalisation ability and performance. The registration problems detailed in ¬ß3.2 could be solved either with higher-resolution ToF cameras or a better distribution of targets within the scene. Applying a non-linear calibration for the systematic depth <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b10">11]</ref> could improve the raw distance readings which would then be used to compute feature vectors.</p><p>An area we have not yet explored is the possibility of using a variant of the standard Random Forest or indeed a different Machine Learning algorithm altogether. Recent developments such as Adaptive Random Forests <ref type="bibr" target="#b30">[31]</ref> have the potential to increase the speed of the confidence assignment, which is currently around 5 seconds for a 200 √ó 200 frame.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Intensity images of training scenes from the laser scanner.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Overhead view of a scene acquired by the laser scanner (red) and ToF camera (blue). We project the laser geometry into the ToF camera (green) without considering occlusions. The inset depth histogram shows how laser points from distinct locations can project into a single ToF pixel, but only points from A are valid. The area visible to the ToF between D and E is not covered by the laser scanner and so no training data is acquired here.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Points removed by a method are shown in red. The green box in (c) shows an artifact of our method, see ¬ß5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Cluttered desk environment captured by Mesa SwissRanger 3100. (a) photo of the scene, (b) and (c) are respective frontal and side views of the obtained point cloud where points with confidence below 0.6 are shown in red.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Figure 5. ROC curves. Comparison of our method against amplitude and geometric filtering on: (a) less than 4% error with 21, 290 actual positives and 9, 931 actual negatives, (b) less than 25% error with 29, 102 actual positives and 2, 119 actual positives. 4% error corresponds to an error margin of 16cm on a surface 4 meters away.</figDesc><table><row><cell></cell><cell></cell><cell>Threshold</cell><cell>Amp</cell><cell></cell><cell>Geom</cell><cell>Conf</cell></row><row><cell></cell><cell cols="2">(relative error, %)</cell><cell>AUC</cell><cell></cell><cell>AUC</cell><cell>AUC</cell></row><row><cell></cell><cell></cell><cell>4</cell><cell>0.639</cell><cell></cell><cell>0.599</cell><cell>0.729</cell></row><row><cell></cell><cell></cell><cell>25</cell><cell>0.685</cell><cell></cell><cell>0.722</cell><cell>0.757</cell></row><row><cell></cell><cell></cell><cell>{1 . . . 25} (average)</cell><cell>0.654</cell><cell></cell><cell>0.661</cell><cell>0.736</cell></row><row><cell cols="7">Table 1. AUC of ROC curves for Amplitude (Amp), Geometric</cell></row><row><cell cols="6">(Geom) and Confidence (Conf) algorithms.</cell></row><row><cell></cell><cell>1</cell><cell></cell><cell></cell><cell>1</cell><cell></cell></row><row><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell></row><row><cell>True Positive Rate</cell><cell>0.4 0.6</cell><cell>Confidence</cell><cell>True Positive Rate</cell><cell>0.4 0.6</cell><cell></cell><cell>Confidence</cell></row><row><cell></cell><cell>0.2</cell><cell>Amplitude</cell><cell></cell><cell>0.2</cell><cell></cell><cell>Amplitude</cell></row><row><cell></cell><cell></cell><cell>Geometric</cell><cell></cell><cell></cell><cell></cell><cell>Geometric</cell></row><row><cell></cell><cell>0</cell><cell cols="2">0 0.2 0.4 0.6 0.8 1</cell><cell>0</cell><cell cols="2">0 0.2 0.4 0.6 0.8 1</cell></row><row><cell></cell><cell></cell><cell>False Positive Rate</cell><cell></cell><cell></cell><cell cols="2">False Positive Rate</cell></row><row><cell></cell><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell>(b)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>For code, data, further results and other supplementary material please see http://visual.cs.ucl.ac.uk/pubs/tofconfidence/ (a) 3D Point Cloud (b) Intensities (c) Amplitudes</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We are grateful to Jan Boehm and Stuart Robson, for sharing their laser scanner and expertise, and to Marc Pollefeys for lending us the SR-3100. Thanks to Maciej Gryka, Oisin Mac Aodha, and Fr√©d√©ric Besse who helped during data collection, and to Jim Rehg for valuable discussions. We would also like to thank the reviewers for their feedback and suggestions. The student authors were supported by the UK EPSRC-funded Eng. Doctorate Centre in Virtual Environments, Imaging and Visualisation (EP/G037159/1).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semantic structure from motion</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Calibration of focal length and 3D pose based on the reflectance and depth image of a planar object</title>
		<author>
			<persName><forename type="first">C</forename><surname>Beder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Intell. Syst. Technol. Appl</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="285" to="294" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Accuracy of exterior orientation for a range camera</title>
		<author>
			<persName><forename type="first">J</forename><surname>Boehm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pattinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISPRS Commission V Mid-Term Symposium &apos;Close Range Image Measurement Techniques</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="103" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Visual methods for three-dimensional modeling</title>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Bouguet</surname></persName>
		</author>
		<idno>AAI9941097</idno>
		<imprint>
			<date type="published" when="1999">1999</date>
			<pubPlace>Pasadena, CA, USA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Random forests</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="5" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Constrained parametric min-cuts for automatic object segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="3241" to="3248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">An Introduction to Support Vector Machines and Other Kernel-based Learning Methods</title>
		<author>
			<persName><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
	<note>first edition</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">3D shape scanning with a time-of-flight camera</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schuon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Derek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page" from="1173" to="1180" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Two-dimensional spectral analysis of cortical receptive field profiles</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Daugman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Research</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="847" to="856" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Robust real-time 3D modeling of static scenes using solely a timeof-flight sensor</title>
		<author>
			<persName><forename type="first">J</forename><surname>Feulner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Penne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kollorz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th IEEE Workshop on Object Tracking and Classification Beyond and in the Visible Spectrum</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Extrinsic and depth calibration of ToF-cameras</title>
		<author>
			<persName><forename type="first">S</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hirzinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Calibration and registration for precise surface reconstruction with time-of-flight cameras</title>
		<author>
			<persName><forename type="first">S</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>May</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Intell. Syst. Technol. Appl</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="274" to="284" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A time-of-flight depth sensor -system description, issues and solutions</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Gokturk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yalcin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bamji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">35</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Calibration for increased accuracy of the range imaging camera swissranger</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kahlmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Remondino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ingensand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISPRS</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">XXXVI</biblScope>
			<biblScope unit="page" from="136" to="141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Design and calibration of a multi-view TOF sensor fusion system</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Time-of-flight sensors in computer graphics</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kolb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Barth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Larsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eurographics State of the Art Reports</title>
		<imprint>
			<biblScope unit="page" from="119" to="134" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Solid-state time-of-flight range camera</title>
		<author>
			<persName><forename type="first">R</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Quantum Electronics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="390" to="397" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Lateral and depth calibration of PMD-distance sensors</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lindner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolb</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="524" to="533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Segmenting video into classes of algorithm-suitability</title>
		<author>
			<persName><forename type="first">O</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Three-dimensional mapping with time-of-flight cameras</title>
		<author>
			<persName><forename type="first">S</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Droeschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Holz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Malis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>N√ºchter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hertzberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Field Robot</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">11-12</biblScope>
			<biblScope unit="page" from="934" to="965" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">3D Pose Estimation and Mapping with Time-of-Flight Cameras</title>
		<author>
			<persName><forename type="first">S</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Droeschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Holz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wiesen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fuchs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS Workshop on 3D Mapping</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Robust 3D-Mapping with Time-of-Flight Cameras</title>
		<author>
			<persName><forename type="first">S</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Droeschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Holz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>N√ºchter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1673" to="1678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Example-based 3D scan completion</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pauly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Giesen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SGP</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">A novel approach to efficient error correction for the SwissRanger time-of-flight 3D camera. RoboCup</title>
		<author>
			<persName><forename type="first">J</forename><surname>Poppinga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Birk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008. 2009</date>
			<biblScope unit="page" from="247" to="258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pose estimation and map building with a time-of-flightcamera for robot navigation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Prusak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Melnychuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Schiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Intell. Syst. Technol. Appl</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3/4</biblScope>
			<biblScope unit="page" from="355" to="364" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Efficient variants of the ICP algorithm</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rusinkiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DIM</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="145" to="152" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Obstacle detection with a photonic mixing devicecamera in autonomous vehicles</title>
		<author>
			<persName><forename type="first">T</forename><surname>Schamm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Zollner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vacek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schroder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dillmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Intell. Syst. Technol. Appl</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3/4</biblScope>
			<biblScope unit="page" from="315" to="324" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Calibration of a pmd camera using a planar calibration object together with a multicamera setup</title>
		<author>
			<persName><forename type="first">I</forename><surname>Schiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Beder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Part B</title>
		<imprint>
			<biblScope unit="volume">XXXVII</biblScope>
			<biblScope unit="page" from="297" to="302" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>ISPRS</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Lidarboost: Depth superresolution for ToF 3D shape scanning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Schuon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page" from="343" to="350" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">New electro-optical mixing and correlating sensor: facilities and applications of the photonic mixer device (PMD)</title>
		<author>
			<persName><forename type="first">R</forename><surname>Schwarte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-G</forename><surname>Heinol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Olk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Buxbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schulte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPIE</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Adaptive random forest -how many &quot;experts&quot; to ask before making a decision?</title>
		<author>
			<persName><forename type="first">A</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Real-time human pose recognition in parts from single depth images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Finocchio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kipman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Tracking objects in 6D for reconstructing static scenes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Swadzba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Beuter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sagerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop: Time of Flight Camera based Computer Vision</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A comprehensive system for 3D modeling from range images acquired from a 3D ToF sensor</title>
		<author>
			<persName><forename type="first">A</forename><surname>Swadzba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Penne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Jesorsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kompe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICVS</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Highquality shape from multi-view stereo and shading under general illumination</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wilburn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Matsushita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Spatial-depth super resolution for range images</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nist√©r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
