<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Binary Reconstruction for Cross-modal Hashing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xuelong</forename><surname>Li</surname></persName>
							<email>li@opt.ac.cn</email>
						</author>
						<author>
							<persName><forename type="first">Di</forename><surname>Hu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Feiping</forename><surname>Nie</surname></persName>
							<email>feipingnie@gmail.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">OPTIMAL in Northwestern Polytechnical University</orgName>
								<address>
									<addrLine>127 West Youyi Road Xi&apos;an</addrLine>
									<settlement>Shaanxi</settlement>
									<country>China xuelong</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">OPTIMAL in Northwestern Polytechnical University</orgName>
								<address>
									<addrLine>127 West Youyi Road Xi&apos;an</addrLine>
									<settlement>Shaanxi</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">OPTIMAL in Northwestern Polytechnical University</orgName>
								<address>
									<addrLine>127 West Youyi Road Xi&apos;an</addrLine>
									<settlement>Shaanxi</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Binary Reconstruction for Cross-modal Hashing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">608743702C29F2426F9E6EDF5FDFF3BB</idno>
					<idno type="DOI">10.1145/3123266.3123355</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T05:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Retrieval</term>
					<term>Cross-modal hashing</term>
					<term>Binary reconstruction</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With the increasing demand of massive multimodal data storage and organization, cross-modal retrieval based on hashing technique has drawn much attention nowadays. It takes the binary codes of one modality as the query to retrieve the relevant hashing codes of another modality. However, the existing binary constraint makes it difficult to find the optimal cross-modal hashing function. Most approaches choose to relax the constraint and perform thresholding strategy on the real-value representation instead of directly solving the original objective. In this paper, we first provide a concrete analysis about the effectiveness of multimodal networks in preserving the inter-and intra-modal consistency. Based on the analysis, we provide a so-called Deep Binary Reconstruction (DBRC) network that can directly learn the binary hashing codes in an unsupervised fashion. The superiority comes from a proposed simple but efficient activation function, named as Adaptive Tanh (ATanh). The ATanh function can adaptively learn the binary codes and be trained via back-propagation. Extensive experiments on three benchmark datasets demonstrate that DBRC outperforms several state-of-the-art methods in both image2text and text2image retrieval task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The same contents or topics can be expressed in multiple kinds of modalities in practice. For example, the usual speech can be expressed by audio signal or lip movements <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>, the common content can be described by not only the textual data but also the images <ref type="bibr" target="#b15">[16]</ref>, and the environment perception could utilize both image and 3D depth information <ref type="bibr" target="#b24">[25]</ref>.</p><p>As these modalities jointly describe the same contents, it becomes possible to make up for each other's limitations and provide more valuable information than single modality. Hence, there have been many attempts over the years to make use of multimodal data for specific areas, such as audiovisual speech recognition <ref type="bibr" target="#b8">[9]</ref>, image-text classification <ref type="bibr" target="#b21">[22]</ref>. And the shared contents across modalities also provide possibilities for retrieving relevant data by giving the query of another modality, which has drawn much attention recently <ref type="bibr" target="#b26">[27]</ref>. A typical scenario of such task is to retrieve relevant images by a text query. However, faced with the increasing requirements of massive data organization, storage, and retrieval, traditional cross-modal retrieval shows obvious disadvantages in terms of efficiency.</p><p>Recently, hashing method shows its efficiency in approximated nearest neighbor search, which employs the short, binary codes for retrieval instead of the original high dimensional, real-value data. The binary codes learned from the original database can vastly reduce the storage space and retrieval time. Hence, hashing technique has been widely used in various machine learning and computer vision problems <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b28">29]</ref>, especially in unimodal retrieval <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13]</ref>. For crossmodal retrieval, hashing has also attracted considerable research attention due to its efficiency. Cross-modal hashing aims to discover the correlations across modalities to enable the cross-modal similarity search <ref type="bibr" target="#b26">[27]</ref>. Hence, different from the unimodal hashing, it should preserve not only the intramodality consistency, but also the inter-modality consistency. In this paper, we focus on unsupervised cross-modal hashing technique.</p><p>Unsupervised cross-modal hashing problem has been just proposed in recent years. Most of existing hashing methods employ a two-stage framework for learning hashing codes, which first generates the real-value codes in a learned shared semantic space across different modalities, then binarizes the real-value codes via thresholding <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b29">30]</ref>. But such methods are usually based on the shallow model, where linear projection is a common selection for semantic space learning. Hence, the nonlinear correlation across modalities could not be effectively learned. Recently, as the effectiveness of deep networks in producing useful representation has been confirmed <ref type="bibr" target="#b18">[19]</ref>, some works choose to learn the common semantic space via a shared layer across the multi-layer nonlinear projection of different modalities <ref type="bibr" target="#b26">[27]</ref>. However, these works based on multimodal networks just provide an empirical analysis in preserving the intra-and inter-modality consistency, which could be unreliable for learning efficient codes. More importantly, the above works do not directly learn the hashing codes, but are just a simple combination of conventional cross-modal network and binarization. Such frameworks actually relax the binary constraint, and the extra binarization may destroy the learned semantic space and result in a sub-optimal solution. Although Courbariaux et al. <ref type="bibr" target="#b2">[3]</ref> aim to make the weights and activations binary, such model still suffers from difficult optimization.</p><p>In this paper, to figure out the effectiveness of multimodal deep network in cross-modal hashing, we provide a theoretical analysis about the usually employed Multimodal Restricted Boltzmann Machine (MRBM) with Maximum Likelihood Learning (MLL). We show that such deep networks with a shared layer across modalities can simultaneously preserve the intra-and inter-modality consistency. Then, based on the above conclusion, we propose to directly learn the binary hashing codes via a multimodal deep reconstruction network, which is called as Deep Binary Reconstruction (D-BRC). In the proposed DBRC, we introduce a novel hashing activation function, named as Adaptive Tanh (ATanh). The hashing layer with ATanh function can adaptively map the activations of previous layers into approximated binary codes. Then, based on the projected hamming semantic space, the original multimodal data is reconstructed in an unsupervised fashion. The proposed hashing layer makes it possible to simultaneously learn the hashing codes and optimize the deep networks via back-propagation, which could learn more efficient binary codes than the two-stage methods. We conduct extensive experiments on three benchmark datasets, and DBRC shows better codes over state-of-the-art methods on various metrics.</p><p>In the following sections, we first revisit the related crossmodal hashing methods in Section 2. Then we give a concrete analysis about the MRBM with MLL objective in preserving modal consistency in Section 3. In Section 4, we introduce the hashing activation function ATanh, and corresponding optimization method. We then propose DBRC cross-modal hashing framework. Experiments are conducted for evaluation in Section 5. Section 6 concludes this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Existing unsupervised cross-modal hashing methods share similar framework for fast retrieval: the data of different modalities are projected into a common low-dimensional space, then binarization operation is performed over the projected real-value vector to obtain binary codes. And these methods can be grouped into two categories with respect to the projection manner: linear modeling and nonlinear modeling methods <ref type="bibr" target="#b26">[27]</ref>. Linear modeling. Linear modeling methods aim to utilize linear projection function to learn the common subspace. Cross View Hashing (CVH) <ref type="bibr" target="#b11">[12]</ref> and Inter-Media Hashing (IMH) <ref type="bibr" target="#b20">[21]</ref> extend the unimodal spectral hashing to multimodal scenario and aim to retain the inter-and intra-modal consistency in the common subspace. Note that Canonical Correlation Analysis (CCA) is actually a special case of CVH, which targets to find effective linear projections of different modalities that are maximally correlated. Rastegari et al. <ref type="bibr" target="#b17">[18]</ref> propose Predictable Dual-view Hashing (PDH) to refine the CCA projection via ignoring the orthogonal bases and learning the hashing codes in a self-taught manner. Apart from the CCA-like methods, Zhou et al. <ref type="bibr" target="#b29">[30]</ref> propose another novel Latent Semantic Sparse Hashing (LSSH) for cross-modal hashing. It aims to maximally correlate the learned latent semantic features of different modalities, which are obtained from sparse coding and matrix factorization. Similarly with LSSH, Collective Matrix Factorization Hashing (CMFH) <ref type="bibr" target="#b3">[4]</ref> assumes that different modalities can be factorized into modality-specific matrices and latent semantic matrix, meanwhile the original modality can be linearly reconstructed from the semantic matrix. And the hashing codes are obtained via a thresholding operation over the common semantic matrix. These methods are all based on linear modeling that limits their effectiveness in the common subspace modeling. Nonlinear modeling. To overcome the limits of linear modeling, nonlinear modeling based on deep networks has attracted much attention recently. To our best knowledge, Multimodal Deep Autoencoder (MDAE) <ref type="bibr" target="#b14">[15]</ref> is the first one employing deep networks in multimodal learning. Concretely, MDAE focuses on audiovisual speech recognition task. It learns the joint representation across modalities via a shared layer of different modality-specific networks, and the whole network is trained by minimizing the reconstruction error of both modalities. Similar frameworks have also been served to multimodal retrieval <ref type="bibr" target="#b21">[22]</ref>. Recently, there have been some attempts to employ such frameworks for cross-modal hashing problem. Wang et al. <ref type="bibr" target="#b25">[26]</ref> directly employ MDAE network for cross-modal hashing, but impose the orthogonal regularizer on the weights of MDAE to make the codes more efficient. And the hashing codes in <ref type="bibr" target="#b25">[26]</ref> are obtained by performing an indicator function over the joint representation. Differently, Feng et al. <ref type="bibr" target="#b5">[6]</ref> and Wang et al. <ref type="bibr" target="#b27">[28]</ref> propose to employ stacked specific-networks to encode each modality, then learn the latent representation by maximizing the semantic similarity of different modalities. Although these methods make use of the advantages of deep networks in nonlinear modeling, they fail to take consideration of the binary constraint of hashing codes when training the multimodal networks. In other words, they just perform a thresholding operation over the learned joint representation across modalities, which could destroy the original representation and make the codes inefficient. Unlike the two-stage strategy in these methods, our model can directly generate the hashing codes and perform nonlinear modeling over the modalities. Moreover, the multimodal networks in these works are designed to preserve the inter-and intra-modal consistency under heuristic consideration, which is not convincing to some extent. Hence, we provide a concrete analysis about such models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MULTIMODAL MAXIMUM LIKELIHOOD LEARNING</head><p>Different from unimodal hashing, cross-modal hashing need to preserve both the inter-and intra-modal correlation. To capture such correlations, multimodal deep networks propose to fuse multiple modality-specific networks with the help of one shared layer <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b25">26]</ref>. And the fusion scheme is almost always based on Multimodal Restricted Boltzmann Machine (MRBM), which is the core of cross-modal retrieval networks, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. MRBM is a special case of RBM that is an energy-based network. To be specific, RBM is an undirected graphical model which defines a probability distribution of visible units using hidden units. Under the case of multimodal input, MRBM defines a joint distribution over modality x, modality y, and shared hidden units h <ref type="bibr" target="#b21">[22]</ref>, which is written as,</p><formula xml:id="formula_0">P (x, y, h) = 1 Z exp (-E (x, y, h)) , (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where Z is the partition function and E is an energy term given by</p><formula xml:id="formula_2">E (x, y, h) = -x T W x h -y T W y h -x T b x -y T b y -h T b h ,<label>(2)</label></formula><p>where x and y are the visible units of modality x and y, and h is the shared hidden units. W x is a matrix of pairwise weights between elements of x and h, and similar for W y . b x , b y , and b h are bias vectors for x, y, and h, respectively. To obtain the joint likelihood P (x, y), h is marginalized out from the distribution,</p><formula xml:id="formula_3">P (x, y) = ∑ h exp(-E(x, y, h))/Z.<label>(3)</label></formula><p>For the MRBM model, similar to the standard RBM, Contrastive Divergence (CD) <ref type="bibr" target="#b7">[8]</ref> or Persistent CD (PCD) <ref type="bibr" target="#b22">[23]</ref> can be used to approximate the gradient to maximize the joint likelihood, i.e., P (x, y). This is the typical maximum likelihood learning for MRBM. as follows,</p><formula xml:id="formula_4">M LL = -E P D (x,y) [log P θ (x, y)] = -E P D (x) [ E P D (y|x) log P θ (x) ] -E P D (x) [ E P D (y|x) log P θ (y|x) ] = E P D (x) [ E P D (y|x) log PD (x) P θ (x)</formula><p>]</p><formula xml:id="formula_5">+E P D (x) [ E P D (y|x) log PD (y|x) P θ (y|x) ] + C = E P D (x) [ E P D (y|x) log PD (y|x) P θ (y|x)</formula><p>]</p><formula xml:id="formula_6">+E P D (x) [ log PD (x) P θ (x) ] + C = E P D (x) [KL(PD (y|x) ||P θ (y|x))] cross modalities + KL(PD (x) ||P θ (x))</formula><p>single modality</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>+C</head><p>where C is a constant that is irrelevant to θ. Note that, the above formula can also be re-written with respect to y. It is easy to find that the MLL objective of MRBM consists of two terms: one is related to the distribution of single modality x and the other one is about the conditional probability of cross-modalities. In other words, maximizing the joint distribution P θ (x, y) is equal to simultaneously learning the unimodal and cross-modal data distribution, which actually preserves both the intra-and inter-modal consistency. Hence, the deep networks based on MRBM have the ability to meet the requirements of cross-modal hashing, and the experimental results of previous works also confirm this.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DEEP BINARY RECONSTRUCTION</head><p>In this section, based on the above concrete analysis, we first propose ATanh activation function for generating binary codes, then show the optimization w.r.t. the function. Finally, a multimodal deep binary reconstruction network is proposed for cross-modal hashing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Adaptive Tanh</head><p>The hashing technique requires the generated codes to be binary, i.e., {-1, 1}. As the binary constraint makes it hard to optimize the networks, conventional two-stage methods perform the sign function over the activations of tanh or sigmoid <ref type="foot" target="#foot_0">1</ref> after training the deep networks, as shown in Fig. <ref type="figure">2</ref>.</p><formula xml:id="formula_7">1 -1 j ( ) f j ( ) f j ( ) f j 1 -1 … … 1 -1 j j</formula><p>In this paper, we propose an adaptive tanh function that is similar to the tanh function but controlled by a learnable scaling parameter α &gt; 0, which is defined as,</p><formula xml:id="formula_8">f (s) = tanh(αs), (<label>4</label></formula><formula xml:id="formula_9">)</formula><p>where s is the activation of previous layers. In Eq. 4, it is easy to find that when α is small, especially α = 1, ATanh becomes tanh, where the activations change gently between -1 and 1. Conversely, when α is large enough, the proposed ATanh approaches the sign function, which means the activations of ATanh fall into the binary value of hashing codes. In particular, ATanh is differentiable everywhere, which is totally different from the sign function. Hence, the deep network using ATanh as the activation function can be optimized via back-propagation.</p><p>Although the parameter α makes ATanh learnable compared with other functions, it is hard to guarantee that the activations of ATanh fall into the binary codes after training the deep networks. Actually, α should gradually increase so that the final ATanh has the ability to generate the binary hashing codes. Hence, the activation function becomes</p><formula xml:id="formula_10">f (s) = tanh(αs) + λ α -1 2 2 , (<label>5</label></formula><formula xml:id="formula_11">)</formula><p>where λ is a regularization constant. The regularization term, α -1 2 2 , is a penalty to the original ATanh function, which provides a convenient way to control the magnitude of α. Hence, the new function Eq. 5 becomes more reliable for binary code learning.</p><p>Note that, the proposed ATanh function is an elementwise function, so that the introduced α is different for different bits. In other words, we could simultaneously learn 32 ATanh functions for 32 bits hashing codes, which makes the codes more adaptable compared with the consistent sign function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Optimization</head><p>As the proposed ATanh is derivable, it can be jointly trained with other layers via back-propagation. Eq. 5 can be rewritten into element-wise,</p><formula xml:id="formula_12">f (si) = tanh(αisi) + λ|αi| -2 , i = 1, 2, ..., bits<label>(6)</label></formula><p>where bits is the code length. For each ATanh function f (si), the update of αi can be simply derived by chain rule. Let ε denote the objective function (e.g., reconstruction error), the partial derivative w.r.t. αi becomes</p><formula xml:id="formula_13">∂ε ∂αi = ∂ε ∂f (si) ∂f (si) ∂αi . (<label>7</label></formula><formula xml:id="formula_14">)</formula><p>Here, the first term is the gradient to current hashing layer, which is propagated from previous layers. And the second term is the derivative of ATanh function,</p><formula xml:id="formula_15">∂f (si) ∂αi = ( 1 -tanh 2 (αisi) ) si -2λα -3 i . (<label>8</label></formula><formula xml:id="formula_16">)</formula><p>The update of αi can be performed by employing stochastic gradient descent with RMSprop <ref type="bibr" target="#b23">[24]</ref> based on the derivative (Eq. 7). RMSprop adaptively rescales the step size for updating trainable weights according to the corresponding gradient history. Note that, when αi becomes larger, it is more easily influenced by the vanishing gradient. Hence, in the experiments, we follow the empirical parameter setting and αi = 1 is considered as the initialization. And compared with other trainable parameters of the network, the time complexity of ATanh in both forward and backward computation is negligible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Binary Reconstruction Network</head><p>Based on the concrete analysis of the effectiveness of M-RBM in preserving the inter-and intra-modal consistency, we propose a novel multimodal Deep Binary Reconstruction(DBRC) network that can directly generate cross-modal hashing codes, as shown in Fig. <ref type="figure">3</ref>. Specifically, the highdimensional data of each modality is first encoded into the low-dimensional representation via modality-specific network, which could capture the data manifold based on the nonlinear modeling of multi-layers <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b18">19]</ref>. Then the joint representation across modalities is learned via MRBM model (i.e., Fig. <ref type="figure" target="#fig_0">1</ref>). To generate the binary codes within the network, the real-value representation of MRBM is binarized into the hashing codes by taking advantage of the proposed ATanh function, which become the shared hashing layer. Finally, we can directly reconstruct the original data of each modality based on the binary representation. As the ATanh function is derivable, the whole network can be trained via back-propagation, and directly generate the embedded binary codes<ref type="foot" target="#foot_1">2</ref> .</p><p>As the proposed DBRC model learns the binary representation from both modalities, these modalities share the identical hashing codes. But it is not suitable for the testing data, as only one modality is available in the retrieval scenario. Hence, we propose to employ different models for these different scenarios. Hashing codes for training data. As all the modalities are available in the training phase, the hashing codes can be learned by simply training the proposed DBRC model. Hashing codes for testing data. Inspired by Ngiam et al. <ref type="bibr" target="#b14">[15]</ref> and Hu et al. <ref type="bibr" target="#b9">[10]</ref>, we propose to learn the modalityspecific hashing codes by retaining the original value of one modality and setting the other modality to zero when faced with single modality input. In other words, we require the model to reconstruct both modalities with only one modality, just like the video-only deep autoencoder in <ref type="bibr" target="#b14">[15]</ref>. In this case, the model can still preserve the inter-and intra-modal consistency, as the MRBM model is retained in the reconstruction phase. And the original joint representation of MRBM is replaced with the hashing layer. In practice, we find that such reconstruction model performs better when initialized from complete DBRC model then trained with the unimodal input data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In this section, we show the results of DBRC compared with other models on three datasets, including image2text and text2image retrieval task. Different code lengths are considered for evaluating the performance. In addition, we also provide an analysis about the sensitivity of hyper-parameter λ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Setup</head><p>Dataset. Three benchmark datasets are chosen for evaluation, including Wiki<ref type="foot" target="#foot_2">3</ref>  <ref type="bibr" target="#b16">[17]</ref>, FLICKR-25K<ref type="foot" target="#foot_3">4</ref>  <ref type="bibr" target="#b10">[11]</ref>, and NUS-WIDE<ref type="foot" target="#foot_4">5</ref>  <ref type="bibr" target="#b1">[2]</ref>. Wiki is an image-text dataset, which is collected from Wikipedia's "featured article". There are 2,866 pairs in the dataset. For each pair, the image modality is represented as 128dimensional SIFT descriptor histograms, and text is expressed as 10-dimensional semantic vector via latent Dirichlet allocation model. These pairs are annotated with one of 10 topic labels. In this paper, we choose 25% of the dataset as the query set and the rest for retrieval set. FLICKR-25K is an image collection from Flickr, where 25,000 images are associated with multiple textual tags (text). The average number of tags for each image is about 5.1 <ref type="bibr" target="#b21">[22]</ref>. And these image-tag pairs are annotated by 24 provided labels. Following the setting in <ref type="bibr" target="#b13">[14]</ref>, we select the textual tags which appear more than 20 times and keep the valid pairs. The left images are represented with 150dimensional edge histogram and the texts are expressed as 500-dimensional tagging vector. Here we take 5% of the dataset as the query set and the rest for training set. NUS-WIDE dataset consists of 269,648 multi-label images. Each image is also associated with multiple tags (6 in average). The image-tag pairs are annotated with 81 concept labels. Among these concepts, the common 10 ones are considered in our experiments. The images are represented into 500-dimensional bag-of-words based on SIFT descriptor. The textual tags are expressed with 1000-dimensional tag occurrence vector. 4000 image-tag pairs are uniformly sampled as the query set, and the rest ones are served as the training set. Evaluation. In this paper, we focus on two cross-modal retrieval task, i.e., image2text (I2T) and text2image (T2I). Hamming ranking and hash lookup are both employed for evaluation. Specifically, Mean Average Precision (MAP) is computed based on the Hamming distance to a query for Hamming ranking, and the hash lookup performance is according to a Hamming ball of radius 2 to a query. And the ground-truth of relevant items for a query are defined as whether they share at least one common label. Baselines. The proposed method is compared with several unsupervised cross-modal hashing methods, including IMH <ref type="bibr" target="#b20">[21]</ref>, CVH <ref type="bibr" target="#b11">[12]</ref>, CMFH (UCMFH) <ref type="bibr" target="#b3">[4]</ref>, LSSH <ref type="bibr" target="#b29">[30]</ref>, Corr-Full-AE <ref type="bibr" target="#b5">[6]</ref>, and DMHOR <ref type="bibr" target="#b25">[26]</ref>. Note that, the first four are based on linear modeling, while the last two are based on nonlinear modeling (deep networks). Source codes of IMH, CVH, CMFH, and LSSH are provided by the corresponding authors. While the rest two are not available, so we implement them carefully. For DMHOR, we follow the network architecture and hyper-parameter settings introduced in the original paper. While for Corr-Full-AE, the detailed network settings are not provided, so we choose the similar network as DBRC for fairness. Note that, the initialization and optimization method are also not provided, so we try different strategies for training Corr-Full-AE. However, it still tends to map all the original data into similar codes. Hence, we only compare it with DBRC in Hamming ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Model architecture</head><p>The proposed DBRC model consists of two pathways for the two modalities (i.e., image and text). The image pathway consists of an encoder and a decoder, where the encoder is a 3-layers networks (n-128-512, n is the unit number of input feature), while the decoder takes 512-128-n settings. And we take the same networks for the text modality. Note that, due to the efficient gradient propagation of ReLu, it is chosen as the activation function of DBRC except the hashing layer and the joint layer of MRBM. The hyper-parameter of λ is set to 0.001 for all the datasets, and we also provide a discussion about it in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experimental Results</head><p>Results on Wiki. Table <ref type="table" target="#tab_0">1</ref> shows the comparison results on MNIST dataset in MAP (for Hamming ranking). We can easily find that our proposed DBRC model shows remarkable performance compared with other methods, especially the ones based on deep network. And we also find that the nonlinear modeling methods (i.e., Corr-Full-AE and DMHOR) do not perform better than conventional linear modeling, especially the state-of-the-art method of CMFH. This is because these methods based on deep network simply perform the thresholding operation over the hidden units, while the hidden units suffer from the unbalanced activations <ref type="bibr" target="#b18">[19]</ref>, which results in inefficient codes. However, DBRC can overcome such weakness by directly learning the binary codes from reconstruction. The hash lookup results in precision and f-measure are shown in Fig. <ref type="figure" target="#fig_2">4</ref>. Different from the comparison results in MAP, the nonlinear modeling methods significantly improve the performance over linear modeling ones. The reason is that hash lookup just focuses on the top retrieved items, i.e., the ones within specific Hamming ball, while hashing ranking relies on the whole retrieved items. In other words, the nonlinear modeling ones can provide more exact results. Hence, the results in Fig. <ref type="figure" target="#fig_2">4</ref> demonstrate that our method can generate more efficient hashing code for cross-modal retrieval, especially when compared with other nonlinear modeling methods.</p><p>Results on FLICKR-25K. We show the Hamming ranking performance in Table <ref type="table" target="#tab_0">1</ref>. It is obvious that DBRC takes the best performance among all the methods. Corr-Full-AE and DMHOR still suffer from the same problem in Wiki dataset. And Fig. <ref type="figure" target="#fig_2">4</ref> shows the hash lookup results. We can find that although most methods decrease sharply with the increasing code length, which results from more sparse hamming space, DBRC still remains substantial superiority over them. Moreover, CMFH outperforms our method in precision, but DBRC takes better balance between precision and recall and shows superiority over all the other methods.</p><p>Results on NUS-WIDE. In Table <ref type="table" target="#tab_0">1</ref>, DBRC shows the best MAP scores in both image2text and text2image retrieval task. Here, we report the MAP score of IMH in <ref type="bibr" target="#b26">[27]</ref> due to the limited memory of our desktop PC. As the hash  lookup performance is not provided in the corresponding paper, we do not compare with IMH on NUS-WIDE in Fig. <ref type="figure" target="#fig_2">4</ref>. In Fig. <ref type="figure" target="#fig_2">4</ref>, the linear modeling methods enjoy better performance in precision at short codes, but decrease rapidly with the increasing code length. While DBRC shows stable performance when faced with longer code length, which shows its effectiveness in learning binary codes in more sparse hamming space. More importantly, DBRC shows remarkable superiority in F-measure, which also confirms its ability in recalling similar items of another modality. Parameter sensitivity. We provide an empirical analysis about the affects of the hyper-parameter of λ. Fig. <ref type="figure">5</ref> shows the results on Wiki dataset with 32 and 64 bits codes (The other two datasets have similar performance). We can find that DBRC is very insensitive to the choices of λ in all the cases, which controls the tradeoff between the adaptive activation and the regularization. Actually, when λ becomes larger, the units in hashing layer tend to be closer to binary function. Hence, DBRC with larger λ need more times to converge. In this paper, we choose λ = 0.001 for all the experiments, which shows remarkable performance over other methods. Activation comparison. Here, we make a comparison among different variants of the proposed ATanh activation function. To address the complex optimization of sign function, DBRC-C <ref type="bibr" target="#b0">[1]</ref> makes use of a fixed sequence of α for training the networks one by one. We also consider the activation function without the regularization term, i.e., Eq. 4, which is named as DBRC-N. As shown in Table . 2, the learnable activation functions (i.e., DBRC-N and DBRC) show better performance than the fixed one in the two tasks. This is because they can adaptively learn the binarization function based on the projected low-dimensional subspace, especially for each bit. While DBRC-C just performs the hardthreshold function over the real-value representation without consideration of the data structure. On the other hand, D-BRC is better than DBRC-N in different code lengths. The reason is that the extra binarization can almostly be ignored in DBRC, when compared with DBRC-N. Hence, DBRC can learn more effective binary projection in the hashing layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we propose to directly learn the cross-modal hashing codes by reconstructing the original data from embedded shared binary representation, which is distinctly different from previous works. Our model fuses the original twostage methods and can generate more efficient codes. And the proposed ATanh activation function gives rise to such superiority, which can adaptively learn the binary codes within networks and be trained via back-propagation. More importantly, we provide a concrete analysis about the effectiveness of multimodal networks in preserving inter-and intraconsistency for cross-modal retrieval. Extensive experimental results on three benchmark datasets demonstrate that our proposed method can generate better compact codes in both image2text and text2image retrieval task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An illustration of MRBM model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: The comparison among sign, tanh, and ATanh function. The ATanh is initialized by a small value of α (in pale yellow), then adaptively approaching the sign function (in dark yellow).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The hash lookup performance (in Precision and F-measure) of different cross-modal hashing methods on Wiki, FLICKR-25K, and NUS-WIDE dataset with varying code lengths.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 : Hamming ranking performance (in MAP) on Wiki, FLICKR-25K, and NUS-WIDE dataset with varying code lengths.</head><label>1</label><figDesc>bits 64 bits 128 bits 16 bits 32 bits 64 bits 128 bits 16 bits 32 bits 64 bits 128 bits</figDesc><table><row><cell cols="3">Dataset Code Length IMH [21] CVH [12] CMFH [4] 16 bits 32 I2T Task 0.1593 0.1477 0.1420 0.1291 0.5621 0.5643 0.5649 0.5642 0.4187 0.3975 0.3778 0.3668 Wiki FLICKR-25K NUS-WIDE 0.1993 0.1889 0.1803 0.1782 0.5815 0.5756 0.5710 0.5677 0.3888 0.3744 0.3621 0.3537 0.2126 0.2208 0.2322 0.2337 0.5721 0.5740 0.5739 0.5736 0.3443 0.3438 0.3454 0.3461 LSSH [30] 0.2122 0.2260 0.2155 0.2297 0.5779 0.5795 0.5848 0.5878 0.3891 0.3910 0.3977 0.3949</cell></row><row><cell></cell><cell cols="2">Corr-Full-AE [6] 0.1802 0.1937 0.1911 0.2014 0.5557 0.5551 0.5583 0.5553 0.3468 0.3468 0.3470 0.3410</cell></row><row><cell></cell><cell>DMHOR [26]</cell><cell>0.1919 0.1841 0.1847 0.1877 0.5848 0.5810 0.5842 0.5851 0.3657 0.3620 0.3678 0.3590</cell></row><row><cell></cell><cell>DBRC</cell><cell>0.2534 0.2648 0.2686 0.2878 0.5873 0.5898 0.5902 0.5907 0.3939 0.4087 0.4166 0.4165</cell></row><row><cell></cell><cell>IMH [21]</cell><cell>0.1417 0.1297 0.1243 0.1105 0.5624 0.5643 0.5651 0.5648 0.4053 0.3892 0.3758 0.3627</cell></row><row><cell></cell><cell>CVH [12]</cell><cell>0.1652 0.1582 0.1512 0.1469 0.5817 0.5761 0.5715 0.5681 0.3822 0.3697 0.3592 0.3519</cell></row><row><cell></cell><cell>CMFH [4]</cell><cell>0.4830 0.5147 0.5338 0.5370 0.5673 0.5693 0.5681 0.5682 0.3506 0.3509 0.3524 0.3547</cell></row><row><cell>T2I</cell><cell>LSSH [30]</cell><cell>0.4992 0.5245 0.5326 0.5395 0.5874 0.5926 0.5957 0.5964 0.4115 0.4162 0.4229 0.4198</cell></row><row><cell></cell><cell cols="2">Corr-Full-AE [6] 0.1410 0.1262 0.1366 0.1483 0.5576 0.5545 0.5576 0.5567 0.3385 0.3438 0.3390 0.3382</cell></row><row><cell></cell><cell>DMHOR [26]</cell><cell>0.4272 0.4874 0.4916 0.4818 0.5664 0.5622 0.5540 0.5653 0.3724 0.3613 0.3498 0.3401</cell></row><row><cell></cell><cell>DBRC</cell><cell>0.5439 0.5377 0.5476 0.5488 0.5883 0.5963 0.5962 0.5975 0.4249 0.4294 0.4381 0.4427</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Parameter sensitivity on Wiki with differ- ent code lengths. The results with various settings of λ are in MAP scores. value</head><label></label><figDesc>in the initial training stages, which may damage the abstract representation across modalities, just like the sign</figDesc><table><row><cell>0.0001 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 0.55 0.6 Figure 5: Task 0.0005 MAP Method 8 bits 16 bits 48 bits 64 bits 96 bits I2T DBRC-C 0.2219 0.2199 0.2234 0.2379 0.2483 DBRC-N 0.2308 0.2500 0.2616 0.2565 0.2632 DBRC 0.2327 0.2534 0.2674 0.2686 0.2736 I2T DBRC-C 0.4342 0.5165 0.5419 0.5351 0.5424 DBRC-N 0.4650 0.5382 0.5508 0.5336 0.5469 DBRC 0.4868 0.5439 0.5538 0.5476 0.5520</cell><cell>0.001 λ on Wiki @ 32 bits 0.005 0.01 λ</cell><cell>0.05</cell><cell>0.1 I2T T2I</cell><cell>MAP</cell><cell>0.0001 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 0.55 0.6</cell><cell>0.0005</cell><cell>0.001 λ on Wiki @ 64 bits 0.005 0.01 λ</cell><cell>0.05</cell><cell>0.1 I2T T2I</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 : Hamming ranking performance (in MAP) with varying code length. Different variants of ATan- h are compared on Wiki dataset.</head><label>2</label><figDesc></figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The sign function has an offset of -0.5 for the activation values of sigmoid function.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Although the proposed ATanh function is very close to the sign function after training, there are still very few activations falling into the interval (-1, 1). Hence, we simply perform binarization over these activations.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>http://www.svcl.ucsd.edu/projects/crossmodal/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>http://press.liacs.nl/mirflickr/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>http://lms.comp.nus.edu.sg/research/NUS-WIDE.htm</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Zhangjie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.00758</idno>
		<title level="m">Deep Learning to Hash by Continuation</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">NUS-WIDE: a real-world web image database from National University of Singapore</title>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haojie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yantao</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM international conference on image and video retrieval</title>
		<meeting>the ACM international conference on image and video retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page">48</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Ran El-Yaniv, and Yoshua Bengio</title>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Itay</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Soudry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.02830</idno>
	</analytic>
	<monogr>
		<title level="m">Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Large-Scale Cross-Modality Search via Collective Matrix Factorization Hashing</title>
		<author>
			<persName><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jile</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="5427" to="5440" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Thanh-Toan</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anh-Zung</forename><surname>Doan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ngai-Man</forename><surname>Cheung</surname></persName>
		</author>
		<idno>arX- iv:1508.07148</idno>
		<title level="m">Discrete hashing with deep neural network</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cross-modal retrieval with correspondence autoencoder</title>
		<author>
			<persName><forename type="first">Fangxiang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruifan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Multimedia</title>
		<meeting>the 22nd ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="7" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Iterative quantization: A procrustean approach to learning binary codes for large-scale image retrieval</title>
		<author>
			<persName><forename type="first">Yunchao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florent</forename><surname>Perronnin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2916" to="2929" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Temporal multimodal learning in audiovisual speech recognition</title>
		<author>
			<persName><forename type="first">Di</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuelong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqiang</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3574" to="3582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multimodal Learning via Exploring Deep Semantic Similarity</title>
		<author>
			<persName><forename type="first">Di</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqiang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuelong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM on Multimedia Conference</title>
		<meeting>the 2016 ACM on Multimedia Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="342" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The MIR flickr retrieval evaluation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Huiskes</surname></persName>
		</author>
		<author>
			<persName><surname>Lew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st ACM international conference on Multimedia information retrieval</title>
		<meeting>the 1st ACM international conference on Multimedia information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="39" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning hash functions for cross-view similarity search</title>
		<author>
			<persName><forename type="first">Shaishav</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raghavendra</forename><surname>Udupa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI proceedingsinternational joint conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="1360">2011. 1360</date>
			<biblScope unit="volume">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Large graph hashing with spectral rotation</title>
		<author>
			<persName><forename type="first">Xuelong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feiping</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2203" to="2209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semantics-preserving hashing for cross-view retrieval</title>
		<author>
			<persName><forename type="first">Zijia</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3864" to="3872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multimodal deep learning</title>
		<author>
			<persName><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juhan</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th international conference on machine learning</title>
		<meeting>the 28th international conference on machine learning</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="689" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Multimodal learning for multi-label image classification</title>
		<author>
			<persName><forename type="first">Yanwei</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuelong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kongqiao</forename><surname>Wang</surname></persName>
		</author>
		<idno>ICIP. 1797-1800</idno>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A new approach to cross-modal multimedia retrieval</title>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Rasiwasia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><forename type="middle">Costa</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emanuele</forename><surname>Coviello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Doyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gert</forename><forename type="middle">Rg</forename><surname>Lanckriet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM international conference on Multimedia</title>
		<meeting>the 18th ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="251" to="260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Predictable Dual-View Hashing</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonghyun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shobeir</forename><surname>Fakhraei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1328" to="1336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semantic hashing</title>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Approximate Reasoning</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="969" to="978" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improved multimodal deep learning with variation of information</title>
		<author>
			<persName><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenling</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2141" to="2149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Inter-media hashing for large-scale retrieval from heterogeneous data sources</title>
		<author>
			<persName><forename type="first">Jingkuan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng Tao</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data</title>
		<meeting>the 2013 ACM SIGMOD International Conference on Management of Data</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="785" to="796" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multimodal learning with deep boltzmann machines</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2222" to="2230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Training restricted Boltzmann machines using approximations to the likelihood gradient</title>
		<author>
			<persName><forename type="first">Tijmen</forename><surname>Tieleman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1064" to="1071" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude</title>
		<author>
			<persName><forename type="first">Tijmen</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
	<note>COURSERA: Neural networks for machine learn</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multi-modal unsupervised feature learning for RGB-D scene labeling</title>
		<author>
			<persName><forename type="first">Anran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Jen</forename><surname>Cham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2014</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="453" to="467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Deep Multimodal Hashing with Orthogonal Units</title>
		<author>
			<persName><forename type="first">Daixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingdong</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="2291" to="2297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Kaiye</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiyue</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06215</idno>
		<title level="m">A Comprehensive Survey on Cross-modal Retrieval</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Effective multi-modal retrieval based on stacked auto-encoders</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beng</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Chin</forename><surname>Ooi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="649" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Feature hashing for large scale multitask learning</title>
		<author>
			<persName><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirban</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Attenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning</title>
		<meeting>the 26th Annual International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1113" to="1120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Latent semantic sparse hashing for cross-modal similarity search</title>
		<author>
			<persName><forename type="first">Jile</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th international ACM SIGIR conference on Research &amp; development in information retrieval</title>
		<meeting>the 37th international ACM SIGIR conference on Research &amp; development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="415" to="424" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
