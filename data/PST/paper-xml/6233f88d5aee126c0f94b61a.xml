<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">When Chosen Wisely, More Data Is What You Need: A Universal Sample-Efficient Strategy For Data Augmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ehsan</forename><surname>Kamalloo</surname></persName>
							<email>kamalloo@ualberta.ca</email>
							<affiliation key="aff3">
								<orgName type="department">Equal Contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mehdi</forename><surname>Rezagholizadeh</surname></persName>
							<email>mehdi.rezagholizadeh@huawei.com</email>
							<affiliation key="aff3">
								<orgName type="department">Equal Contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ali</forename><surname>Ghodsi</surname></persName>
							<email>ali.ghodsi@uwaterloo.ca</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Univeristy of Alberta</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">When Chosen Wisely, More Data Is What You Need: A Universal Sample-Efficient Strategy For Data Augmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Data Augmentation (DA) is known to improve the generalizability of deep neural networks. Most existing DA techniques naively add a certain number of augmented samples without considering the quality and the added computational cost of these samples. To tackle this problem, a common strategy, adopted by several state-of-the-art DA methods, is to adaptively generate or re-weight augmented samples with respect to the task objective during training. However, these adaptive DA methods: (1) are computationally expensive and not sample-efficient, and (2) are designed merely for a specific setting. In this work, we present a universal DA technique, called Glitter, to overcome both issues. Glitter can be plugged into any DA method, making training sample-efficient without sacrificing performance. From a pre-generated pool of augmented samples, Glitter adaptively selects a subset of worst-case samples with maximal loss, analogous to adversarial DA. Without altering the training strategy, the task objective can be optimized on the selected subset. Our thorough experiments on the GLUE benchmark, SQuAD, and HellaSwag in three widely used training setups including consistency training, self-distillation and knowledge distillation reveal that Glitter is substantially faster to train and achieves a competitive performance, compared to strong baselines. 1  </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The undeniable importance of data in deep learning <ref type="bibr" target="#b40">(Sambasivan et al., 2021;</ref><ref type="bibr" target="#b39">Rogers, 2021)</ref> and the costly process of data annotation has propelled researchers into leveraging Data Augmentation (DA) in a broad range of applications from computer vision <ref type="bibr" target="#b4">(Cubuk et al., 2019;</ref><ref type="bibr" target="#b47">Wang et al., 2020)</ref> to natural language processing (NLP) including machine translation <ref type="bibr" target="#b42">(Sennrich et al., 2016;</ref><ref type="bibr" target="#b44">Shen et al., 2020)</ref>, language understanding <ref type="bibr" target="#b44">(Shen et al., 2020;</ref><ref type="bibr" target="#b36">Qu et al., 2021;</ref><ref type="bibr" target="#b7">Du et al., 2021;</ref><ref type="bibr" target="#b19">Kamalloo et al., 2021)</ref>, and question answering <ref type="bibr" target="#b0">(Alberti et al., 2019;</ref><ref type="bibr" target="#b26">Longpre et al., 2019;</ref><ref type="bibr" target="#b43">Shakeri et al., 2020)</ref>. DA is shown to be effective in improving generalization of deep neural networks <ref type="bibr" target="#b6">(DeVries and Taylor, 2017;</ref><ref type="bibr" target="#b52">Xie et al., 2020)</ref> and in increasing the number of training samples especially in low resource data regimes <ref type="bibr" target="#b42">(Sennrich et al., 2016;</ref><ref type="bibr" target="#b55">Zhang et al., 2018)</ref>. Nonetheless, in NLP, the discrete nature of text poses additional complexity to DA as generating semantically viable text from another text is challenging <ref type="bibr" target="#b10">(Feng et al., 2021)</ref>.</p><p>DA methods can be broadly categorized into task-aware and task-agnostic methods. Taskagnostic DA methods essentially generate augmented text regardless of the task at hand and often do not warrant additional training or fine-tuning. They can be based on some hand-crafted heuristics <ref type="bibr" target="#b56">(Zhang et al., 2015;</ref><ref type="bibr" target="#b48">Wei and Zou, 2019)</ref>, backtranslation <ref type="bibr" target="#b42">(Sennrich et al., 2016;</ref><ref type="bibr" target="#b8">Edunov et al., 2018)</ref>, or token replacement from a pre-trained language model <ref type="bibr" target="#b22">(Kobayashi, 2018;</ref><ref type="bibr" target="#b51">Wu et al., 2019;</ref><ref type="bibr" target="#b33">Ng et al., 2020)</ref>. Even though deploying task-agnostic methods is straightforward, these methods do not take into account any task-specific information, and thus, their performance is usually limited. On the other hand, task-aware DA methods are capable of generating augmented samples, conditioned on the downstream task objective <ref type="bibr" target="#b16">(Hu et al., 2019;</ref><ref type="bibr" target="#b52">Xie et al., 2020;</ref><ref type="bibr" target="#b38">Rashid et al., 2021)</ref>. These methods adapt augmented examples specifically for a task in that they construct augmented examples, sometimes partly, during training. Despite their advantages, they often incur additional training costs, resulting in a prohibitively slow and a computationally expensive training.</p><p>In general, the central problems surrounding DA techniques in NLP can be summarized as follows:</p><p>First, DA methods are mostly not sample-efficient in that they add arbitrary number of augmented samples to the training data and naively incorporate all of them into training without investigating how many of augmented samples are actually needed. Second, although more effective, taskaware methods are notoriously time-consuming to train. This is especially problematic in large-scale datasets such as SQuAD <ref type="bibr" target="#b37">(Rajpurkar et al., 2016)</ref> and MNLI <ref type="bibr" target="#b49">(Williams et al., 2018)</ref>. Third, most DA methods are not universal as they work solely with a particular setup-e.g., training a singlenetwork <ref type="bibr" target="#b52">(Xie et al., 2020)</ref>, or training in teacherstudent settings <ref type="bibr" target="#b38">(Rashid et al., 2021)</ref>. Overall, the importance of both sample efficiency and training efficiency for DA has been often overlooked.</p><p>Motivated by the above problems, in this work, we introduce a universal DA method, Glitter 2 , which can be plugged into any DA method to make them sample-efficient, and task-aware without sacrificing performance. Specifically, given a pool of augmented samples that are generated offline, our proposed method follows a minimax approach <ref type="bibr" target="#b9">(Farnia and Tse, 2016)</ref> to select a small subset with maximal expected loss (maximization step) during training. Without any further adjustments to the training algorithm, the task objective can be optimized for this selected subset (minimization step).</p><p>Our key contributions in this paper can be summarized as follows:</p><p>1. Glitter is a universal method which can be effortlessly applied to any DA method to enforce sample efficiency while maintaining (or even boosting) their performance.</p><p>2. We devise strategies to adapt Glitter for a variety of widely used training setups including single-network, consistency training, selfdistillation and knowledge distillation.</p><p>3. Through our empirical evaluations, we show that Glitter achieves superior performance over state-of-the-art DA methods on GLUE, SQuAD, and HellaSwag, while significantly speeding up the training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>2.1 Task-agnostic DA in NLP Contextual augmentation techniques <ref type="bibr" target="#b22">(Kobayashi, 2018;</ref><ref type="bibr" target="#b51">Wu et al., 2019)</ref> use pre-trained language 2 Inspired by "All that is gold does not glitter" -J.R.R. Tolkien, The Fellowship of the Ring. models for DA. <ref type="bibr" target="#b22">Kobayashi (2018)</ref> propose bidirectional LSTM language models for word substitution conditioned on the label of their input text. SSMBA <ref type="bibr" target="#b33">(Ng et al., 2020)</ref> and TinyBERT <ref type="bibr" target="#b18">(Jiao et al., 2020)</ref> perturb the input by masking some of the tokens, and then, sample tokens from a BERT model to replace the masked tokens and generate augmented samples. Back-Translation <ref type="bibr" target="#b42">(Sennrich et al., 2016)</ref> augments data using two consecutive translation models: the first model to translate the input into an arbitrary target language; then, a second model to translate the result back into its original language. Mixed-up <ref type="bibr" target="#b13">(Guo et al., 2019)</ref> generates augmented samples based on interpolating word embedding and sentence embedding vectors. <ref type="bibr" target="#b44">Shen et al. (2020)</ref> introduce a set of cut-off techniques that zero out contiguous spans of the embedding matrix at token level, feature level and span level. EDA <ref type="bibr" target="#b48">(Wei and Zou, 2019)</ref> consists of simple word-level operations including synonym replacement, random deleting, random insertion and random swapping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Task-aware DA in NLP</head><p>One approach to leverage task-specific information is to assign different weights to augmented samples based on their individual impacts on the model <ref type="bibr" target="#b53">(Yi et al., 2021)</ref>. Although effective, the re-weighting mechanism largely ignores sample efficiency. <ref type="bibr" target="#b51">Wu et al. (2019)</ref> introduce a mask-andreconstruct approach, namely c-BERT, that finetune a pre-trained BERT model to predict labelcompatible tokens. CoDA <ref type="bibr" target="#b36">(Qu et al., 2021)</ref> combines various label-preserving transformations with adversarial training jointly with a contrastive regularization objective. Unsupervised DA (UDA; <ref type="bibr" target="#b52">Xie et al. 2020</ref>) uses off-the-shelf DA methods and adds an auxiliary consistency loss to the training objective. However, UDA is not sample-efficient and it is designed only for a single-network setup; how to deploy it in other training scenarios such as knowledge distillation is not clear. <ref type="bibr" target="#b16">Hu et al. (2019)</ref> propose a reinforcement learning-based technique where the reward function is defined based on whether generated augmented samples are labelpreserving or not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">DA for KD</head><p>KD <ref type="bibr" target="#b1">(Buciluǎ et al., 2006;</ref><ref type="bibr" target="#b14">Hinton et al., 2015)</ref>, initially proposed as a model compression technique, aims at transferring the knowledge of an already trained model, called teacher, to a smaller or a same-size student model. Several studies found that DA can significantly boost KD's performance in NLP. TinyBERT <ref type="bibr" target="#b18">(Jiao et al., 2020)</ref> uses a taskagnostic DA technique for its task-specific finetuning. <ref type="bibr" target="#b19">Kamalloo et al. (2021)</ref> and <ref type="bibr" target="#b38">Rashid et al. (2021)</ref> showed that DA can also be tailored for KD. In particular, MATE-KD <ref type="bibr" target="#b38">(Rashid et al., 2021)</ref> tunes a separate masked language model in order to generate augmented samples with maximum divergence. <ref type="bibr" target="#b19">Kamalloo et al. (2021)</ref> and <ref type="bibr" target="#b7">Du et al. (2021)</ref> employ kNN retrieval to fetch augmented samples from a massive sentence bank.</p><p>Glitter differs from previous work in that it simultaneously focuses on sample efficiency, and universality such that it can be freely used in any training setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>In this section, we introduce our task-aware DA method, Glitter , that aims at using an efficient number of augmented samples without sacrificing performance. Our proposed strategy is agnostic to DA methods; it can be seamlessly plugged into any DA method with any training setting to enforce sample efficiency.</p><p>Existing learning-based DA methods train a separate DA model and adapt its output for a particular objective function that is entirely task-dependent:</p><formula xml:id="formula_0">φ * ← min φ DA (M (Ω(x; φ); θ)) x * = Ω(x; φ * ) (1)</formula><p>where DA () is a loss function, geared towards the objective of the task, Ω(; φ) is the DA model with trainable parameters φ, and M (; θ) refers to the original model, parameterized by θ.</p><p>In contrast to learning-based DA, we propose to generate many augmented candidates using any arbitrary DA method prior training, and adaptively select most suitable candidates during training. This procedure does not introduce additional trainable parameters into training, and more importantly, is capable of automatically ignoring unnecessary augmented examples. Let (x i , y i ) N i=1 ∈ {(X , Y)} represent training data such that a pair x i ∈ X and y i ∈ Y are an input example and its corresponding label. Suppose a pool of K augmented examples,</p><formula xml:id="formula_1">X (i) = {x k (i)} K</formula><p>k=1 , are sampled from some DA model for each training example (x i , y i ) ∈ (X , Y). Note that Glitter imposes no restrictions on how to augment training data; augmented samples can be generated via a single or even multiple DA models.</p><p>Sample Selection. Given a pool of augmented samples, our approach is to adaptively select the best candidates according to particular defined criteria. Inspired by the minimax approach <ref type="bibr" target="#b9">(Farnia and Tse, 2016;</ref><ref type="bibr" target="#b45">Volpi et al., 2018)</ref>, our selection mechanism is based on finding top-k 1 (out of K) worst-case augmented samples from the X set. Minimizing the main model loss function on these worst-case augmented samples will help improving generalization of the model <ref type="bibr" target="#b45">(Volpi et al., 2018)</ref>. In order to rank augmented samples, we evaluate X (i) based on a distance function with respect to the corresponding original training sample, x i , within the model's latent space:</p><formula xml:id="formula_2">X * (i) ← top k 1 eval M (x i ; θ), M (X (i); θ) X * (i) = {x * j (i)} k 1 j=1 ⊂ X (i) (2)</formula><p>where top k 1 () denotes returns top-k 1 indices based on the scores returned by eval , X * (i) is the set of k 1 selected augmented samples for x i ; eval () is the evaluation loss which is determined via the task objective.</p><p>Updating the Model Parameters. After obtaining the top-k 1 augmented samples, we group them with the original training samples, {x i } ∪ X * (i), and subsequently, update the model parameters only based on this selected set of augmented samples on the original loss:</p><formula xml:id="formula_3">L(θ) = N i=1 task M (x i ; θ), M (X * (i); θ), y i θ t ← θ t−1 − λ∇ θ (L(θ))| θ t−1 (3)</formula><p>where N is the number of training samples, λ is the learning rate, and task () is the final task losse.g., cross entropy (ce) for classification-that is computed over both original data and selected augmented data. In the remainder of this section, we discuss how Glitter can be applied to popular training settings including general DA for single networks, and DA for teacher-student (KD) setups. Note that Glitter is not restricted to these settings and may be adapted for other settings such as DAIR <ref type="bibr" target="#b17">(Huang et al., 2022)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">General DA for Single Networks</head><p>We consider three potential setups for the single network scenario: (1) General single network, (2) General Single Network. In this setup, augmented samples are exploited in a semi-supervised manner where we can evaluate them based on the divergence of their predicted output M (x k (i); θ) = p(y|x k (i); θ) from the ground-truth label or the prediction of the original corresponding training sample M (x i ; θ) = p(y|x i ; θ) using the cross entropy loss, ce :</p><formula xml:id="formula_4">eval = ce y i , M (x k (i); θ) or eval = ce M (x i ; θ), M (x k (i); θ) . (4)</formula><p>The cross entropy criterion is not the only option here. Other choices for eval include (but not limited to) focal loss <ref type="bibr" target="#b24">(Lin et al., 2017)</ref>, and tilted loss <ref type="bibr" target="#b23">(Li et al., 2021)</ref>.</p><p>For the final task loss, task we can deploy a standard cross entropy loss over both training samples and their corresponding selected augmented samples:</p><formula xml:id="formula_5">task = ce y i , M (x i ; θ) + 1 k 1 x∈X * (i) ce y i , M (x; θ) . (<label>5</label></formula><formula xml:id="formula_6">)</formula><p>Consistency Training (CT; <ref type="bibr" target="#b52">Xie et al. 2020)</ref>. In this configuration, we can employ the same eval introduced in Eq. ( <ref type="formula">4</ref>). As a result, our method naturally selects top-k 1 most inconsistent augmented samples for each training sample. Then, the network is optimized to make predictions for input augmented samples that are consistent with predictions of their corresponding original training samples:</p><formula xml:id="formula_7">CT task = ce y i , M (x i ; θ t ) + 1 k 1 x∈X * (i) ce M (x i ; θ t−1 ), M (x; θ t ) . (6)</formula><p>As stated by <ref type="bibr" target="#b52">Xie et al. (2020)</ref>, the second term in Eq. ( <ref type="formula">6</ref>) leverages the previous prediction of the network for each training example.</p><p>Self-Distillation (Self-KD). In Self-KD, we first train a model, and then, use it (M (; θ * )) as a teacher to train an identical model but initialized from scratch using KD <ref type="bibr" target="#b11">(Furlanello et al., 2018)</ref>. How to adjust eval and task is detailed in §3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">DA for Teacher-Student (KD)</head><p>In this setup, we have a teacher model, T (; ψ * ) with parameters ψ that is already trained on the training data, along with a student model, M (; θ), which we aim to train. The selection criterion for augmented samples is to maximize divergence between the teacher and the student:</p><formula xml:id="formula_8">KD eval = KL T x k (i); ψ * , M x k (i); θ (7)</formula><p>where KL refers to the KL divergence. After selecting the maximum divergence augmented samples, then we calculate the KD loss as following:</p><formula xml:id="formula_9">KD task = α ce y i , M (x i ; θ) + (1 − α)× 1 k 1 + 1 x∈{x i }∪X * (i) KL T (x; ψ * ), M (x; θ) (8)</formula><p>where α is a hyperparameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments 4.1 Setup</head><p>To incorporate unlabelled augmented data into training, we adopt CT <ref type="bibr" target="#b52">(Xie et al., 2020)</ref> and KD <ref type="bibr" target="#b14">(Hinton et al., 2015)</ref>. To this end, we conduct experiments under two settings:</p><p>Standalone where we train a single model on the augmented data. In this setting, we seek to answer two questions: (1) How much is DA capable of improving the model generalization? (2) Does sample efficiency of Glitter hurt performance? For this purpose, we fine-tune RoBERTa base <ref type="bibr" target="#b25">(Liu et al., 2019)</ref> using CT and Self-KD on augmented data.</p><p>Distilled where we distill DistilRoBERTa <ref type="bibr" target="#b41">(Sanh et al., 2019)</ref> (student) from RoBERTa Large <ref type="bibr" target="#b25">(Liu et al., 2019</ref>) (teacher) using the augmented data. Note that the teacher is already trained on the original data and DA comes into play only during distilling the student model. Our goal here is to investigate whether DA is an effective means in knowledge transfer to curb the capacity gap <ref type="bibr" target="#b3">(Cho and Hariharan, 2019</ref>) between a large model and a small one.</p><p>In both settings, we take the best performing model on the development set and evaluate it on the test set (depicted by Test). Additionally, for the standalone model setting, we also report results on the development set when models are trained only for 5 epochs (depicted by Dev), similar to CoDA <ref type="bibr" target="#b36">(Qu et al., 2021)</ref>, to make a comparison with baselines. Our Dev results are an average of 10 runs with different seeds. The implementation details and hyperparameters are provided in §A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">DA Methods</head><p>We leverage three widely used textual augmentation methods:</p><p>1. EDA (Wei and Zou, 2019)<ref type="foot" target="#foot_1">3</ref> : We randomly replace 5% of the tokens with their synonyms and randomly delete up to 10%.</p><p>For each augmentation method, we generate 12 augmented examples per training instance for all datasets, except for large datasets-i.e., MNLI, QQP, and SQuAD-where the number of augmented examples are 8 per train example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Baselines</head><p>Because the two environments-i.e., standalone and distilled-are different in nature, we compare Glitter with different baselines for each environment. For both, Vanilla-DA that takes all augmented data into account without reservation is the first baseline.</p><p>The baselines for the standalone setting are: CoDA <ref type="bibr" target="#b36">(Qu et al., 2021)</ref>, MMEL <ref type="bibr" target="#b53">(Yi et al., 2021)</ref>, and HiddenCut <ref type="bibr" target="#b2">(Chen et al., 2021)</ref>. And for distilled, we consider MATE-KD <ref type="bibr" target="#b38">(Rashid et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">GLUE</head><p>The GLUE benchmark <ref type="bibr" target="#b46">(Wang et al., 2019</ref>) is a well-known suite of nine<ref type="foot" target="#foot_2">4</ref> tasks that aim at evaluating natural language understanding models. We present test results in the distilled mode in Table <ref type="table" target="#tab_1">1</ref>. Glitter consistently outperforms Vanilla-DA, while it is faster to train. Specifically, Glitter achieves parity with Vanilla-DA for EDA in terms of the overall average score, while scoring +0.2% and +0.4% higher for BT and MR, respectively. We observe that only in few cases Vanilla-DA negligibly outperforms Glitter-e.g., on MRPC, and STS-B for BT. Nonetheless, Glitter 8x/1x trains 50% faster than Vanilla-DA 8x on average, and 30% faster for 8x/2x.   data for all GLUE datasets except for SST in the remainder of our experiments.</p><p>For the standalone mode, Tables <ref type="table" target="#tab_2">2 and 3</ref> present the results on test and dev, respectively. Similar to distilled, Glitter outperforms Vanilla-DA by +0.5% for both self-KD and CT. Self-KD yields better results than CT on all GLUE tasks except CoLA. CT falls short on most GLUE tasks, compared to no DA results-i.e., top-2 rows in Table <ref type="table" target="#tab_2">2</ref>. This is why, we only evaluated Glitter with self-KD on the dev data. Glitter achieves superior performance gains, compared to all three baselines on all datasets except QNLI. The key advantage of Glitter is that the training procedure remains intact.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Out-of-Domain Generalization</head><p>We also evaluate Glitter on OOD datasets. To this end, we test our models, already trained on GLUE tasks, on OOD datasets whose data distribution differs from the original data. In particular, here are our selected OOD datasets:</p><p>• SST: IMDb <ref type="bibr" target="#b27">(Maas et al., 2011)</ref>, IMDb-Cont. <ref type="bibr" target="#b12">(Gardner et al., 2020)</ref>, and IMDb-CAD <ref type="bibr" target="#b20">(Kaushik et al., 2020)</ref>, as done in <ref type="bibr" target="#b2">Chen et al. (2021)</ref>. Although both SST and IMDb datasets are collected on movie reviews, IMDb reviews tend to be substantially longer than SST sentences.</p><p>• STS-B: SICK <ref type="bibr" target="#b28">(Marelli et al., 2014)</ref>, a semantic relatedness dataset, created from image and video captions. SICK and STS-B are collected on roughly identical domains, but from different sources.</p><p>• QQP: PAWS QQP <ref type="bibr" target="#b57">(Zhang et al., 2019)</ref> 94.3 ± 0.1 91.6 ± 0.5 87.7 ± 0.1 92.8 ± 0.2 84.5 ± 0.8 90.0 ± 0.4 30.8 ± 0.9 73.6 ± 0.7 Self-KD 94.3 ± 0.2 91.5 ± 0.3 87.9 ± 0.1 92.9 ± 0.2 84.0 ± 0.6 90.3 ± 0.5 30.9 ± 0.4 73.5 ± 0.7 + Vanilla-DA 95.4 ± 0.5 92.0 ± 0.3 88.2 ± 0.1 93.4 ± 0.1 84.4 ± 0.7 90.2 ± 0.4 31.3 ± 0.5 73.9 ± 0.4 + Glitter 95.7 ± 0.2 92.2 ± 0.5 88.2 ± 0.1 93.4 ± 0.1 85.6 ± 0.7 90.6 ± 0.2 31.8 ± 0.4 74.6 ± 0.3</p><p>Table <ref type="table">3</ref>: Dev results of the standalone experiment on GLUE using RoBERTa base . ( ♠ ) denotes results are taken verbatim from: RoB and CoDA <ref type="bibr" target="#b36">(Qu et al., 2021)</ref>, and HiddenCut <ref type="bibr" target="#b2">(Chen et al., 2021)</ref>. ( † ) indicates the results are obtained from our implementation of MMEL <ref type="bibr" target="#b53">(Yi et al., 2021)</ref>.</p><p>• MNLI: SciTail <ref type="bibr" target="#b21">(Khot et al., 2018)</ref>, collected from school-level science questions, and similar to Chen et al. ( <ref type="formula">2021</ref>), A-NLI <ref type="bibr" target="#b34">(Nie et al., 2020), and</ref><ref type="bibr">HANS (McCoy et al., 2019)</ref>.</p><p>• RTE: HANS <ref type="bibr" target="#b29">(McCoy et al., 2019)</ref>.</p><p>Table <ref type="table" target="#tab_10">10</ref> in §B.1 showcases the OOD results for the distilled mode. Glitter outperforms Vanilla-DA in most cases, and is on par with it for nearly the rest. The only exceptions are IMDb-Cont., MQP, and PAWS QQP where Vanilla-DA outperforms Glitter by almost 1% on average. Also, all models do not generalize well to PAWS QQP and A-NLI because their performance is below a majorityclass performance. Moreover, a fine-tuned Distil-RoBERTa achieves the best OOD performance on HANS, highlighting that DA is not actually helpful for OOD accuracy on HANS.</p><p>Table <ref type="table">3</ref> (the right side) reports the OOD results for standalone models. The complete results are presented in §B.2-i.e., Table <ref type="table" target="#tab_1">11</ref> on test and Table 12 on dev. Glitter overwhelmingly outperforms all the baselines with a few exceptions. In the dev results, the fine-tuned model with no DA achieves the best OOD generalization on IMDb, and SciTail, while HiddenCut scores the highest on A-NLI with a 1% margin. Similarly, in the test results, Glitter trails Self-KD with no DA on IMDb, IMDb-CAD, and SciTail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">HellaSwag</head><p>HellaSwag <ref type="bibr" target="#b54">(Zellers et al., 2019)</ref> is a dataset for situated commonsense reasoning that involves picking the best ending given a context. We augment contexts in HellaSwag using only BT to ensure that the choices remain meaningful for the augmented contexts. Because our standalone results have been consistent with the distilled results, we report our results only in the distilled mode. According to our</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>SQuAD HellaSwag results demonstrated in Table <ref type="table" target="#tab_4">4</ref>, Glitter comfortably surpasses Vanilla-DA by a +2.3% margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">SQuAD</head><p>SQuAD <ref type="bibr" target="#b37">(Rajpurkar et al., 2016</ref>) is a crowd-sourced reading comprehension benchmark that consists of more than 100K questions, derived from Wikipedia passages. The task objective is to extract an answer span from a given question/passage pair. We augment questions in SQuAD v1.1 using only BT to ensure that the answer can still be found in the given passage for the augmented questions. Analogous to HellaSwag, we report our results only in the distilled mode. As shown in Table <ref type="table" target="#tab_4">4</ref>, Glitter outperformas Vanilla-DA by +1.8% in exact-match accuracy on the development set. We also evaluate our trained models under distribution shift by testing them on QA datasets from four different domains: Wikipedia, New York Times, Reddit, and Amazon product reviews <ref type="bibr" target="#b31">(Miller et al., 2020)</ref>. The OOD results are presented in Table <ref type="table">5</ref>. Glitter is consistently superior to Vanilla-DA in all four domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Ablation Study and Discussion</head><p>In this section, we aim to answer the following questions: Table <ref type="table">5</ref>: OOD results for models trained on SQuAD and tested on QA datasets from four different domains <ref type="bibr" target="#b31">(Miller et al., 2020)</ref>.</p><p>• How does training time of Glitter compare against Vanilla-DA?</p><p>• Instead of adaptively selecting augmented data during training, can we pre-process them to dispense with unnecessary examples prior to training?</p><p>• How many augmented examples are required for Glitter to work?</p><p>• Is our selection strategy based on sorting of eval in Glitter important?</p><p>For this purpose, we conduct a detailed analysis on 4 GLUE tasks-i.e., SST, MRPC, QNLI, and RTE. We trained models based on Vanilla-DA and Glitter using Self-KD and tested them on the development set (the dev setting).</p><p>Runtime Analysis. Throughout our experiments in §4, we compare Glitter with Vanilla-DA when number of augmentations are similar for both methods-i.e., 8x. A natural question is: how would both DA methods behave with fewer augmented data? To this end, we vary augmentation size from 1x to 8x and train different Vanilla-DA models on each augmented dataset. We measure average the training time per epoch for all models. Figure <ref type="figure" target="#fig_1">2</ref> illustrates the dev accuracy as the training time increases. The training speed of Glitter 8x/2x is slightly faster than Vanilla-DA 6x on SST, MRPC, and QNLI and for Glitter 8x/1x, is faster than Vanilla-DA 4x on RTE. Glitter is superior of the two on all datasets.</p><p>Effect of Pre-processing Augmented Data. We conjecture that Glitter does not need any data engineering on augmented examples to obtain preferable performance gains. However, Vanilla-DA may require some pre-processing by weeding out potentially noisy data to become more effective. To investigate this, we exploit two pre-processing Effect of Augmentation Size in Glitter. We explore how augmentation size affects the performance of Glitter. Throughout our experiments, we fix the augmentation size to 8x, but now, we reduce augmentation size K to 6x and 4x, while retaining selection size k 1 as before-i.e., 1 for RTE, and 2 for the rest. Our results, shown in Table <ref type="table" target="#tab_7">7</ref>, reveal that when K becomes close to k 1 , Glitter's performance declines. Nonetheless, for a sufficiently large augmentation, Glitter starts to shine. For SST, and MRPC, the magic number is 8x, whereas for QNLI, and RTE, Glitter performs best on 6x. Another parameter in Glitter is the selection size k 1 . We find that for all tasks, the best value can be chosen from {1, 2} (2 by default). Using this method, tuning k 1 is straightforward and does not impose additional complexity to our method.</p><p>Effect of Selection Strategy in Glitter. In this section, our objective is to assess whether our proposed selection algorithm is crucial in Glitter. To  this end, we sample random augmented examples at each iteration, namely Glitter-Rnd, instead of selecting worst-case examples. As illustrated in Table 7 (the bottom two rows), the performance drops on all datasets-i.e., 0.2% on QNLI, and more than 1% on the rest, confirming the effectiveness of our selection algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we proposed a universal DA technique, namely Glitter, that can be freely applied to any DA technique to enforce sample efficiency without introducing additional parameters or changing the training procedure. We extensively evaluated Glitter on a broad range of NLU tasks and in various widely used settings including consistency training, self-distillation and knowledge distillation and demonstrated substantial efficiency gains without compromising effectiveness. Extending Glitter to auto-regressive models for machine translation and abstractive summarization is an interesting direction for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Implementation Details</head><p>A.1 Fine-tuning details We adopted the publicly available pre-trained RoBERTa <ref type="bibr" target="#b25">(Liu et al., 2019)</ref> and DistilRoBERTa <ref type="bibr" target="#b41">(Sanh et al., 2019)</ref>-using the Huggingface Transformers library <ref type="bibr" target="#b50">(Wolf et al., 2020)</ref> and the Pytorch Lightning library<ref type="foot" target="#foot_3">5</ref> . For the test settings, the model is evaluated on the development data once per epoch for small datasets and twice per epoch for large ones-i.e., SST-2, MNLI, QNLI, SQuAD, and HellaSwag. The best performing model is chosen for testing. Our learning rate schedule follows a linear decay scheduler with a warm-up, specified as a ratio of the total number of training steps. Maximum number of epochs is set to 20 for all tasks except SQuAD, following <ref type="bibr" target="#b32">(Mosbach et al., 2021)</ref>. For large datasets, we early stop with a patience of 10. The learning rate, and the batch size are tuned for each task separately. The details of hyperparameters are summarized in Table <ref type="table" target="#tab_9">9</ref>. We ran RoBERTa base experiments with the similar hyperparameters, but with these exceptions: On QNLI, learning rate, batch size, and weight decay are set to 3e-5, 64, and 0.1; warmup ratio is set to 0.06 on QQP.</p><p>For dev experiments, we follow CoDA <ref type="bibr" target="#b36">(Qu et al., 2021)</ref> on the GLUE tasks. Specifically, we train the model for 5 epochs with a batch size of 32, learning rate 1e-5, warmup ratio 0.06, weight decay 0.1, and linear learning rate decay. For SQuAD, and HellaSwag, the hyperparameters are detailed in Table <ref type="table" target="#tab_8">8</ref>.</p><p>All experiments were conducted on two Nvidia Tesla V100 GPUs.</p><p>Hyperparam. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SQuAD HellaSwag</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Knowledge distillation details</head><p>We implemented knowledge distillation by caching the teacher's logits prior to training. We performed grid search to find the best softmax temperature τ from {5.0, 10.0, 12.0, 20.0, 30.0}. The value of τ used in our experiments are reported in Tables <ref type="table" target="#tab_9">8  and 9</ref> for DistilRoBERTa and RoBERTa base ; with the exception τ = 20.0 on MRPC for RoBERTa base .</p><p>Loss weight α, in Eq. ( <ref type="formula">8</ref>), is set to 0.5 for all tasks except CoLA in which α = 0.75.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B OOD results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Distilled Mode</head><p>OOD results for models trained in the distilled mode are presented in Table <ref type="table" target="#tab_10">10</ref>.  Table <ref type="table" target="#tab_2">12</ref>: OOD results of models with dev settings in the standalone mode, same models whose results are reported in Table <ref type="table">3</ref>. ( ♠ ) denotes results are taken verbatim from: HiddenCut <ref type="bibr" target="#b2">(Chen et al., 2021)</ref>. ( † ) indicates the results are obtained from our implementation of MMEL <ref type="bibr" target="#b53">(Yi et al., 2021)</ref>. Bold numbers indicate the best result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Standalone Mode</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of Glitter (from left to right): first, generating augmented samples from different DA techniques; second, forming a pool of samples X (i); third, evaluating the augmented samples using the eval () loss; fourth, filtering the top-k 1 samples based on their corresponding eval (); fifth, updating the parameters of the model by minimizing the task loss task (: θ).</figDesc><graphic url="image-3.png" coords="4,116.21,70.85,362.86,142.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Runtime Analysis of DA when training RoBERTa base using self-KD. The red point signifies Glitter.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>± 0.3 90.0 ± 0.4 94.1 ± 0.4 80.1 ± 0.4 31.0 ± 0.6 73.7 ± 0.7 78.3 ± 0± 0.4 79.7 ± 0.3 31.4 ± 0.6 74.5 ± 0.6 78.3 ± 0.3 Self-KD 91.9 ± 0.3 90.3 ± 0.5 94.4 ± 0.4 79.9 ± 0.3 30.9 ± 0.4 73.5 ± 0.7 78.2 ± 0.4 + Vanilla-DA 91.6 ± 0.4 90.2 ± 0.4 94.3 ± 0.3 79.3 ± 0.4 31.3 ± 0.5 73.9 ± 0.4 77.8 ± 0± 0.1 31.8 ± 0.4 74.6 ± 0.3 78.4 ± 0.2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Also, Glitter surpasses MATE-KD by +0.2% in the overall score. Unlike Glitter, MATE-KD introduces additional parameters to the model during training and it trains drastically slower because it generates augmented examples on-the-fly. Moreover, Table 1 illustrates that MR yields the best test results across the three DA methods except for SST where BT leads to better results. Based on this observation, we report results on MR augmented</figDesc><table><row><cell>Method</cell><cell>CoLA Mcc</cell><cell>SST Acc</cell><cell cols="2">MRPC STS-B Acc/F1 P/S</cell><cell>QQP Acc/F1</cell><cell cols="4">MNLI-m/mm QNLI RTE Avg. Acc Acc Acc</cell></row><row><cell>RoBLarge (teacher)</cell><cell>63.8</cell><cell>96.8</cell><cell>90.6</cell><cell>92.4</cell><cell>81.5</cell><cell>90.3/89.8</cell><cell>94.8</cell><cell>88.3</cell><cell>87.3</cell></row><row><cell>BERTLarge ♣</cell><cell>60.5</cell><cell>94.9</cell><cell>87.4</cell><cell>87.1</cell><cell>80.7</cell><cell>86.7/85.9</cell><cell>92.7</cell><cell>70.1</cell><cell>82.5</cell></row><row><cell>DistilRoB</cell><cell>55.2</cell><cell>93.9</cell><cell>85.9</cell><cell>86.0</cell><cell>80.3</cell><cell>84.0/83.1</cell><cell>90.6</cell><cell>73.6</cell><cell>81.1</cell></row><row><cell>KD</cell><cell>54.9</cell><cell>94.0</cell><cell>86.8</cell><cell>87.3</cell><cell>80.5</cell><cell>85.1/83.7</cell><cell>91.9</cell><cell>73.5</cell><cell>81.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Task-Aware DA</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MATE-KD ♣</cell><cell>56.0</cell><cell>94.9</cell><cell>90.2</cell><cell>88.0</cell><cell>81.2</cell><cell>85.5/84.8</cell><cell>92.1</cell><cell>75.0</cell><cell>82.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">EDA (Wei and Zou, 2019)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Vanilla-DA (8x)</cell><cell>55.5</cell><cell>94.8</cell><cell>87.6</cell><cell>86.1</cell><cell>80.7</cell><cell>85.3/84.7</cell><cell>92.0</cell><cell>72.8</cell><cell>81.8</cell></row><row><cell>Glitter</cell><cell>54.5</cell><cell>95.1</cell><cell>87.5</cell><cell>86.5</cell><cell>80.4</cell><cell>85.4/84.8</cell><cell>92.1</cell><cell>73.2</cell><cell>81.8</cell></row><row><cell></cell><cell>8x/2x</cell><cell>8x/1x</cell><cell>8x/2x</cell><cell>8x/2x</cell><cell>8x/2x</cell><cell>8x/2x</cell><cell>8x/2x</cell><cell>8x/1x</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Back-Translation</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Vanilla-DA (8x)</cell><cell>53.4</cell><cell>95.1</cell><cell>88.5</cell><cell>87.5</cell><cell>80.9</cell><cell>85.9/85.9</cell><cell>92.2</cell><cell>73.5</cell><cell>82.1</cell></row><row><cell>Glitter</cell><cell>54.9</cell><cell>95.1</cell><cell>88.4</cell><cell>87.3</cell><cell>80.9</cell><cell>86.2/85.3</cell><cell>92.2</cell><cell>73.7</cell><cell>82.3</cell></row><row><cell></cell><cell>8x/2x</cell><cell>8x/1x</cell><cell>8x/1x</cell><cell>8x/2x</cell><cell>8x/2x</cell><cell>8x/2x</cell><cell>8x/2x</cell><cell>8x/2x</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Mask-and-reconstruct</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Vanilla-DA (8x)</cell><cell>58.8</cell><cell>94.5</cell><cell>88.7</cell><cell>87.0</cell><cell>80.9</cell><cell>85.8/84.9</cell><cell>91.8</cell><cell>74.0</cell><cell>82.6</cell></row><row><cell>Glitter</cell><cell>59.2</cell><cell>95.1</cell><cell>89.2</cell><cell>87.6</cell><cell>81.0</cell><cell>86.6/84.8</cell><cell>92.4</cell><cell>74.1</cell><cell>83.0</cell></row><row><cell></cell><cell>8x/1x</cell><cell>8x/1x</cell><cell>8x/2x</cell><cell>8x/1x</cell><cell>8x/2x</cell><cell>8x/2x</cell><cell>8x/2x</cell><cell>8x/2x</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Test results of the distilled experiment on GLUE. (</figDesc><table><row><cell>Method</cell><cell>CoLA Mcc</cell><cell>SST Acc</cell><cell cols="2">MRPC STS-B Acc/F1 P/S</cell><cell>QQP Acc/F1</cell><cell cols="2">MNLI-m QNLI Acc Acc</cell><cell>RTE Acc</cell><cell>Avg.</cell></row><row><cell>RoBERTa</cell><cell>61.9</cell><cell>95.4</cell><cell>88.6</cell><cell>89.3</cell><cell>80.4</cell><cell>87.6</cell><cell>93.0</cell><cell>81.6</cell><cell>84.7</cell></row><row><cell>Self-KD</cell><cell>61.7</cell><cell>95.7</cell><cell>89.0</cell><cell>89.0</cell><cell>80.8</cell><cell>88.3</cell><cell>93.0</cell><cell>81.7</cell><cell>84.9</cell></row><row><cell>+ Vanilla-DA</cell><cell>61.5</cell><cell>96.1</cell><cell>88.9</cell><cell>89.7</cell><cell>81.0</cell><cell>88.0</cell><cell>92.9</cell><cell>81.1</cell><cell>84.9</cell></row><row><cell></cell><cell>8x</cell><cell>8x</cell><cell>8x</cell><cell>8x</cell><cell>8x</cell><cell>8x</cell><cell>8x</cell><cell>12x</cell><cell></cell></row><row><cell>+ Glitter</cell><cell>62.5</cell><cell>96.0</cell><cell>89.8</cell><cell>89.5</cell><cell>81.1</cell><cell>88.1</cell><cell>93.5</cell><cell>82.3</cell><cell>85.4</cell></row><row><cell></cell><cell>8x/1x</cell><cell>8x/2x</cell><cell>8x/2x</cell><cell>8x/2x</cell><cell>8x/2x</cell><cell>8x/2x</cell><cell>8x/2x</cell><cell>12x/1x</cell><cell></cell></row><row><cell>CT + Vanilla-DA</cell><cell>59.4</cell><cell>95.6</cell><cell>89.0</cell><cell>85.8</cell><cell>80.3</cell><cell>82.5</cell><cell>92.0</cell><cell>80.2</cell><cell>83.1</cell></row><row><cell></cell><cell>8x</cell><cell>8x</cell><cell>8x</cell><cell>10x</cell><cell>8x</cell><cell>8x</cell><cell>8x</cell><cell>10x</cell><cell></cell></row><row><cell>CT + Glitter</cell><cell>62.7</cell><cell>95.8</cell><cell>89.2</cell><cell>87.9</cell><cell>80.9</cell><cell>84.1</cell><cell>92.9</cell><cell>81.8</cell><cell>84.4</cell></row><row><cell></cell><cell>8x/1x</cell><cell>8x/1x</cell><cell>8x/1x</cell><cell>10x/1x</cell><cell>8x/2x</cell><cell>8x/2x</cell><cell>8x/2x</cell><cell>10x/1x</cell><cell></cell></row></table><note>♣ ) denotes results are taken verbatim from: BERT Large<ref type="bibr" target="#b5">(Devlin et al., 2019)</ref>, and MATE-KD<ref type="bibr" target="#b38">(Rashid et al., 2021)</ref>. Bold and underlined numbers indicate the best and the second best results across the DA methods.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Test result of the standalone experiments on GLUE using RoBERTa base .</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>± 0.8 91.9 ± 0.4 88.1 ± 0.1 93.2 ± 0.1 85.3 ± 1.0 90.5 ± 0.7 31.4 ± 0.6 74.5 ± 0.6 RoB †</figDesc><table><row><cell>Method</cell><cell>SST Acc</cell><cell>MRPC F1</cell><cell>MNLI-m Acc</cell><cell>QNLI Acc</cell><cell>RTE Acc</cell><cell>IMDb-Con. Acc</cell><cell>A-NLI Acc</cell><cell>HANS Acc</cell></row><row><cell>RoB ♠</cell><cell>94.8</cell><cell>90.2</cell><cell>87.6</cell><cell>92.8</cell><cell>78.7</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CoDA ♠</cell><cell>95.3</cell><cell>91.7</cell><cell>88.1</cell><cell>93.6</cell><cell>82.0</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>HiddenCut ♠</cell><cell>95.8</cell><cell>92.0</cell><cell>88.2</cell><cell>93.7</cell><cell>83.4</cell><cell>87.8</cell><cell>32.8</cell><cell>71.2</cell></row><row><cell>MMEL  †</cell><cell>94.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>, analogous to Chen et al. (2021), and MQP (Mc-Creery et al., 2020), a medical question similarity dataset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Dev results of the distilled experiment on two downstream tasks.</figDesc><table><row><cell></cell><cell>EM/F1</cell><cell>Acc</cell></row><row><cell>RoB Large</cell><cell>88.9/94.6</cell><cell>85.2</cell></row><row><cell>DistilRoB</cell><cell>80.9/87.9</cell><cell>42.9</cell></row><row><cell>KD</cell><cell>81.1/88.2</cell><cell>42.5</cell></row><row><cell cols="2">+ Vanilla-DA (8x) 81.8/89.1</cell><cell>41.8</cell></row><row><cell cols="2">+ Glitter (8x/2x) 83.6/90.3</cell><cell>44.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Dev results of self-KD exhibiting the effectiveness of different pre-processing techniques to filter augmented examples on 4 GLUE tasks. β and LP depict a minimum confidence threshold, and label preserving, respectively.</figDesc><table><row><cell>Method</cell><cell cols="4">SST MRPC QNLI RTE Acc F1 Acc Acc</cell></row><row><cell cols="2">Vanilla-DA 95.1</cell><cell>92.2</cell><cell>93.3</cell><cell>84.8</cell></row><row><cell>β = 0.7</cell><cell>95.1</cell><cell>92.5</cell><cell>93.4</cell><cell>84.8</cell></row><row><cell>β = 0.9</cell><cell>95.0</cell><cell>92.2</cell><cell>93.3</cell><cell>83.8</cell></row><row><cell>LP</cell><cell>94.8</cell><cell>92.4</cell><cell>93.3</cell><cell>84.8</cell></row><row><cell>Glitter</cell><cell>95.8</cell><cell>92.8</cell><cell>93.4</cell><cell>85.9</cell></row><row><cell>β = 0.7</cell><cell>95.0</cell><cell>91.5</cell><cell>93.5</cell><cell>85.2</cell></row><row><cell>β = 0.9</cell><cell>95.0</cell><cell>92.5</cell><cell>93.3</cell><cell>84.1</cell></row><row><cell>LP</cell><cell>95.1</cell><cell>92.2</cell><cell>93.5</cell><cell>85.9</cell></row><row><cell cols="5">techniques: (1) Confidence-based filtering: Aug-</cell></row><row><cell cols="5">mented examples for which the model's confidence</cell></row><row><cell cols="5">is below a minimum threshold β are discarded,</cell></row><row><cell cols="5">(2) Label-preserving augmentation (LP): Aug-</cell></row><row><cell cols="5">mented examples for which the model predicts a</cell></row><row><cell cols="5">different label than the original example are dis-</cell></row><row><cell cols="5">carded. The results, reported in Table 6, show</cell></row><row><cell cols="5">no meaningful performance gains by these pre-</cell></row><row><cell cols="5">processing techniques. For Vanilla-DA, minimum</cell></row><row><cell cols="5">confidence threshold of 0.7 performs slightly better</cell></row><row><cell cols="5">as it brings minor improvements on MRPC (+0.3%)</cell></row><row><cell cols="5">and QNLI (+0.1%), but is still lower than Glit-</cell></row><row><cell cols="5">ter. On the other hand, applying these techniques</cell></row><row><cell cols="5">slightly deteriorates the performance of Glitter in</cell></row><row><cell cols="5">almost all cases. The only improvements are +0.1%</cell></row><row><cell cols="3">on QNLI for LP and β=0.7.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Dev results of self-KD for studying the effect of augmentation size and the selection algorithm for 4 GLUE tasks.</figDesc><table><row><cell>Method</cell><cell></cell><cell cols="4">SST MRPC QNLI RTE Acc F1 Acc Acc</cell></row><row><cell>Glitter</cell><cell>(8x)</cell><cell>95.8</cell><cell>92.8</cell><cell>93.4</cell><cell>85.9</cell></row><row><cell>Glitter</cell><cell>(6x)</cell><cell>94.7</cell><cell>92.7</cell><cell>93.7</cell><cell>86.3</cell></row><row><cell>Glitter</cell><cell>(4x)</cell><cell>95.0</cell><cell>92.1</cell><cell>93.3</cell><cell>85.7</cell></row><row><cell cols="3">Glitter-Rnd (8x/2x) 94.3</cell><cell>91.4</cell><cell>93.2</cell><cell>85.2</cell></row><row><cell cols="3">Glitter-Rnd (8x/1x) 94.3</cell><cell>91.8</cell><cell>93.2</cell><cell>84.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Hyperparameters of DistilRoBERTa on two downstream tasks.</figDesc><table><row><cell>Learning rate</cell><cell>1.5e-5</cell><cell>1.5e-5</cell></row><row><cell>Batch size</cell><cell>16</cell><cell>32</cell></row><row><cell>Max length</cell><cell>512</cell><cell>512</cell></row><row><cell>Max epochs</cell><cell>3</cell><cell>20</cell></row><row><cell>Warmup ratio</cell><cell>0.06</cell><cell>0.06</cell></row><row><cell>Grad. acc. steps</cell><cell>4</cell><cell>1</cell></row><row><cell>Weight Decay</cell><cell>0.01</cell><cell>0.01</cell></row><row><cell>temp. τ (for KD)</cell><cell>5.0</cell><cell>10.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Table 11 presents OOD results for models trained using test settings, and Table 12 (complementary to Table 3 in §4.2.1) presents OOD results for dev experiments.Hyperparameters of DistilRoBERTa on the GLUE benchmark. We used the same configuration for RoBERTa base albeit with a few exceptions marked by ( * ). EDA<ref type="bibr" target="#b48">(Wei and Zou, 2019)</ref> </figDesc><table><row><cell>Hyperparam.</cell><cell></cell><cell>CoLA</cell><cell>SST</cell><cell cols="7">MRPC STS-B QQP MNLI-m/mm QNLI RTE</cell></row><row><cell cols="2">Learning rate</cell><cell cols="3">1e-5 1e-5 1e-5</cell><cell cols="2">1e-5 1e-5</cell><cell cols="2">3e-5/1e-5</cell><cell cols="2">5e-5  *  1e-5</cell></row><row><cell>Batch size</cell><cell></cell><cell>32</cell><cell>64</cell><cell>16</cell><cell>32</cell><cell>64</cell><cell cols="2">64</cell><cell>128  *</cell><cell>32</cell></row><row><cell>Max length</cell><cell></cell><cell>128</cell><cell>256</cell><cell>128</cell><cell>128</cell><cell>256</cell><cell cols="2">256</cell><cell>256</cell><cell>256</cell></row><row><cell cols="2">Warmup ratio</cell><cell>0.1</cell><cell cols="2">0.06 0.06</cell><cell cols="2">0.06 0.1  *</cell><cell cols="2">0.08/0.06</cell><cell cols="2">0.08 0.06</cell></row><row><cell cols="2">Gradient acc. steps</cell><cell>1</cell><cell>4</cell><cell>1</cell><cell>1</cell><cell>4</cell><cell>4</cell><cell></cell><cell>4</cell><cell>1</cell></row><row><cell cols="2">Weight Decay</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell><cell cols="2">0.0/0.1</cell><cell>0.0  *</cell><cell>0.1</cell></row><row><cell cols="5">Softmax temp. τ (for KD) 30.0 20.0 12.0  *</cell><cell cols="2">12.0 20.0</cell><cell cols="2">12.0</cell><cell cols="2">12.0 12.0</cell></row><row><cell>Trained On →</cell><cell>SST</cell><cell>SST</cell><cell>SST</cell><cell>STS</cell><cell>QQP</cell><cell cols="2">QQP</cell><cell>MNLI</cell><cell>MNLI</cell><cell>RTE</cell></row><row><cell>Method</cell><cell cols="4">IMDb IMDb-Con. IMDb-CAD SICK Acc Acc Acc P/S</cell><cell>MQP Acc/F1</cell><cell cols="5">PAWS QQP SciTail A-NLI HANS Acc Acc Acc Acc</cell></row><row><cell>RoBLarge</cell><cell>93.7</cell><cell>92.0</cell><cell>94.0</cell><cell>84.3</cell><cell>71.6</cell><cell cols="2">43.6</cell><cell>82.0</cell><cell>45.9</cell><cell>81.8</cell></row><row><cell>DistilRoB</cell><cell>90.2</cell><cell>87.6</cell><cell>92.5</cell><cell>79.6</cell><cell>67.3</cell><cell cols="2">36.3</cell><cell>74.8</cell><cell>27.8</cell><cell>71.3</cell></row><row><cell>KD</cell><cell>90.6</cell><cell>87.4</cell><cell>93.2</cell><cell>79.9</cell><cell>65.6</cell><cell cols="2">33.1</cell><cell>77.3</cell><cell>28.9</cell><cell>70.6</cell></row><row><cell>Vanilla-DA</cell><cell>91.8</cell><cell>87.2</cell><cell>92.9</cell><cell>80.0</cell><cell>59.9</cell><cell cols="2">38.0</cell><cell>75.8</cell><cell>27.3</cell><cell>66.6</cell></row><row><cell>Glitter</cell><cell>91.2</cell><cell>87.1</cell><cell>94.0</cell><cell>80.0</cell><cell>64.0</cell><cell cols="2">36.6</cell><cell>75.6</cell><cell>28.8</cell><cell>65.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Back-Translation</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Vanilla-DA</cell><cell>92.2</cell><cell>87.9</cell><cell>92.1</cell><cell>80.3</cell><cell>69.6</cell><cell cols="2">35.0</cell><cell>76.5</cell><cell>27.9</cell><cell>68.0</cell></row><row><cell>Glitter</cell><cell>92.4</cell><cell>87.9</cell><cell>92.8</cell><cell>81.2</cell><cell>68.7</cell><cell cols="2">35.2</cell><cell>77.6</cell><cell>30.4</cell><cell>70.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Masked-and-reconstruct</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Vanilla-DA</cell><cell>91.8</cell><cell>88.8</cell><cell>92.9</cell><cell>80.4</cell><cell>68.5</cell><cell cols="2">33.7</cell><cell>77.4</cell><cell>28.5</cell><cell>69.3</cell></row><row><cell>Glitter</cell><cell>92.0</cell><cell>88.0</cell><cell>92.5</cell><cell>80.7</cell><cell>68.8</cell><cell cols="2">35.3</cell><cell>78.2</cell><cell>29.9</cell><cell>70.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>OOD results of models whose in-domain test results are reported in Table1for the distilled mode. Bold numbers indicate the best result across DistilRoB models.</figDesc><table><row><cell>Trained On →</cell><cell>SST</cell><cell>SST</cell><cell>SST</cell><cell>STS</cell><cell>QQP</cell><cell>QQP</cell><cell>MNLI</cell><cell>MNLI</cell><cell>RTE</cell></row><row><cell>Method</cell><cell cols="4">IMDb IMDb-Con. IMDb-CAD SICK Acc Acc Acc P/S</cell><cell>MQP Acc/F1</cell><cell cols="4">PAWS QQP SciTail A-NLI HANS Acc Acc Acc Acc</cell></row><row><cell>RoBBase</cell><cell>92.2</cell><cell>89.1</cell><cell>94.3</cell><cell>80.6</cell><cell>70.7</cell><cell>38.6</cell><cell>78.5</cell><cell>31.4</cell><cell>78.5</cell></row><row><cell>Self-KD</cell><cell>92.6</cell><cell>89.1</cell><cell>95.0</cell><cell>80.2</cell><cell>70.9</cell><cell>37.6</cell><cell>79.4</cell><cell>32.1</cell><cell>79.5</cell></row><row><cell>+ Vanilla-DA</cell><cell>91.8</cell><cell>88.8</cell><cell>94.8</cell><cell>81.5</cell><cell>71.4</cell><cell>38.8</cell><cell>78.4</cell><cell>31.5</cell><cell>79.3</cell></row><row><cell>+ Glitter</cell><cell>92.0</cell><cell>89.6</cell><cell>94.8</cell><cell>81.7</cell><cell>72.1</cell><cell>39.4</cell><cell>79.1</cell><cell>32.7</cell><cell>80.1</cell></row><row><cell>CT + Vanilla-DA</cell><cell>90.6</cell><cell>88.1</cell><cell>92.1</cell><cell>76.6</cell><cell>70.6</cell><cell>38.3</cell><cell>76.6</cell><cell>30.3</cell><cell>78.4</cell></row><row><cell>CT + Glitter</cell><cell>92.2</cell><cell>88.6</cell><cell>93.7</cell><cell>79.4</cell><cell>70.7</cell><cell>38.8</cell><cell>77.0</cell><cell>31.6</cell><cell>80.2</cell></row></table><note>Table 11: OOD results of models whose in-domain test results are reported in Table 2 for the standalone experiment. Bold numbers indicate the best result.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">. Back-Translation (BT;<ref type="bibr" target="#b42">Sennrich et al. 2016</ref>): We use fairseq<ref type="bibr" target="#b35">(Ott et al., 2019)</ref> to translate sentences into German and then back into English. We do nucleus sampling<ref type="bibr" target="#b15">(Holtzman et al., 2020)</ref> with p = 0.9 for both translations. We find that p = 0.6 works better on sentiment classification</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">.3 https://github.com/makcedward/nlpaug 3. Mask-and-Reconstruct (MR;<ref type="bibr" target="#b33">Ng et al. 2020</ref>): We randomly mask 15% of the tokens and construct a new sentence by sampling from a pre-trained BERT Large for masked tokens. We adopt top-k sampling with k = 20 to select new tokens. For MNLI, we obtain better results with top-10 sampling.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2">We excluded WNLI since our DA methods are not designed for this task.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3">https://github.com/PyTorchLightning/ pytorch-lightning</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Synthetic QA corpora generation with roundtrip consistency</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Andor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Pitler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1620</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6168" to="6173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Model compression</title>
		<author>
			<persName><forename type="first">Cristian</forename><surname>Buciluǎ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandru</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="535" to="541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">HiddenCut: Simple data augmentation for natural language understanding with better generalizability</title>
		<author>
			<persName><forename type="first">Jiaao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.338</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<title level="s">Long Papers</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4380" to="4390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On the efficacy of knowledge distillation</title>
		<author>
			<persName><forename type="first">Hyun</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
				<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4794" to="4802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">AutoAugment: Learning augmentation policies from data</title>
		<author>
			<persName><forename type="first">Barret</forename><surname>Ekin D Cubuk</surname></persName>
		</author>
		<author>
			<persName><surname>Zoph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Dandelion Mane, Vijay Vasudevan, and Quoc V Le</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Self-training improves pre-training for natural language understanding</title>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beliz</forename><surname>Gunel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onur</forename><surname>Celebi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.426</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5408" to="5418" />
		</imprint>
	</monogr>
	<note>Veselin Stoyanov, and Alexis Conneau</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Understanding back-translation at scale</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1045</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="489" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A minimax approach to supervised learning</title>
		<author>
			<persName><forename type="first">Farzan</forename><surname>Farnia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Tse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="4240" to="4248" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A survey of data augmentation approaches for NLP</title>
		<author>
			<persName><forename type="first">Steven</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varun</forename><surname>Gangal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarath</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soroush</forename><surname>Vosoughi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teruko</forename><surname>Mitamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.84</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
				<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="968" to="988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Born again neural networks</title>
		<author>
			<persName><forename type="first">Tommaso</forename><surname>Furlanello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
				<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1607" to="1616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Evaluating models&apos; local decision boundaries via contrast sets</title>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victoria</forename><surname>Basmov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Bogin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sihao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pradeep</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dheeru</forename><surname>Dua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanai</forename><surname>Elazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananth</forename><surname>Gottumukkala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Ilharco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nelson</forename><forename type="middle">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phoebe</forename><surname>Mulcaire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reut</forename><surname>Tsarfaty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ally</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.117</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
				<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1307" to="1323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Augmenting data with mixup for sentence classification: An empirical study</title>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongyi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richong</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.08941</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The curious case of neural text degeneration</title>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.12795</idno>
		<title level="m">Learning data manipulation for augmentation and weighting</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">DAIR: Data augmented invariant regularization</title>
		<author>
			<persName><forename type="first">Tianjian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaunak</forename><surname>Halbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chinnadhurai</forename><surname>Sankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pooyan</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alborz</forename><surname>Geramifard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meisam</forename><surname>Razaviyayn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmad</forename><surname>Beirami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">TinyBERT: Distilling BERT for natural language understanding</title>
		<author>
			<persName><forename type="first">Xiaoqi</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linlin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.372</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
				<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4163" to="4174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Not far away, not so close: Sample efficient nearest neighbour data augmentation via MiniMax</title>
		<author>
			<persName><forename type="first">Ehsan</forename><surname>Kamalloo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Rezagholizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peyman</forename><surname>Passban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Ghodsi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.309</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
				<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3522" to="3533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning the difference that makes a difference with counterfactually-augmented data</title>
		<author>
			<persName><forename type="first">Divyansh</forename><surname>Kaushik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Lipton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">SciTail: A textual entailment dataset from science question answering</title>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Contextual augmentation: Data augmentation by words with paradigmatic relations</title>
		<author>
			<persName><forename type="first">Sosuke</forename><surname>Kobayashi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-2072</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Short Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="452" to="457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Tilted empirical risk minimization</title>
		<author>
			<persName><forename type="first">Tian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmad</forename><surname>Beirami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maziar</forename><surname>Sanjabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Virginia</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
				<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
	<note>Kaiming He, and Piotr Dollár</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">RoBERTa: A robustly optimized BERT pretraining approach</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">An exploration of data augmentation and sampling techniques for domain-agnostic question answering</title>
		<author>
			<persName><forename type="first">Shayne</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhucheng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dubois</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.02145</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Oregon, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A SICK cure for the evaluation of compositional distributional semantic models</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Marelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Menini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Zamparelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC&apos;14)</title>
				<meeting>the Ninth International Conference on Language Resources and Evaluation (LREC&apos;14)<address><addrLine>Reykjavik, Iceland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="216" to="223" />
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Mccoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tal</forename><surname>Linzen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1334</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3428" to="3448" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Effective transfer learning for identifying similar questions: Matching user questions to COVID-19 FAQs</title>
		<author>
			<persName><forename type="first">Clara</forename><forename type="middle">H</forename><surname>Mccreery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Namit</forename><surname>Katariya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anitha</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manish</forename><surname>Chablani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Amatriain</surname></persName>
		</author>
		<idno type="DOI">10.1145/3394486.3412861</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3458" to="3465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The effect of natural distribution shift on question answering models</title>
		<author>
			<persName><forename type="first">John</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Krauth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
				<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="6905" to="6916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">On the stability of fine-tuning BERT: Misconceptions, explanations, and strong baselines</title>
		<author>
			<persName><forename type="first">Marius</forename><surname>Mosbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maksym</forename><surname>Andriushchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dietrich</forename><surname>Klakow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">SSMBA: Self-supervised manifold based data augmentation for improving out-of-domain robustness</title>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marzyeh</forename><surname>Ghassemi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.97</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1268" to="1283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Adversarial NLI: A new benchmark for natural language understanding</title>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.441</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4885" to="4901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">fairseq: A fast, extensible toolkit for sequence modeling</title>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-4009</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)</title>
				<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)<address><addrLine>Minneapolis</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="48" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">CoDA: Contrast-enhanced and diversity-promoting data augmentation for natural language understanding</title>
		<author>
			<persName><forename type="first">Yanru</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandra</forename><surname>Sajeev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1264</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">MATE-KD: Masked adversarial TExt, a companion to knowledge distillation</title>
		<author>
			<persName><forename type="first">Ahmad</forename><surname>Rashid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vasileios</forename><surname>Lioutas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Rezagholizadeh</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.86</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<title level="s">Long Papers</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1062" to="1071" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Changing the world by changing the data</title>
		<author>
			<persName><forename type="first">Anna</forename><surname>Rogers</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.170</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<title level="s">Long Papers</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2182" to="2194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">everyone wants to do the model work, not the data work&quot;: Data cascades in high-stakes ai</title>
		<author>
			<persName><forename type="first">Nithya</forename><surname>Sambasivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivani</forename><surname>Kapania</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannah</forename><surname>Highfill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana</forename><surname>Akrong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lora</forename><forename type="middle">M</forename><surname>Aroyo</surname></persName>
		</author>
		<idno type="DOI">10.1145/3411764.3445518</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</title>
				<meeting>the 2021 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.01108</idno>
		<title level="m">DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Improving neural machine translation models with monolingual data</title>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1009</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="86" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">End-to-end synthetic data generation for domain adaptation of question answering systems</title>
		<author>
			<persName><forename type="first">Siamak</forename><surname>Shakeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cicero</forename><surname>Nogueira Dos Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henghui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Nan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.439</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5445" to="5460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">A simple but toughto-beat data augmentation approach for natural language understanding and generation</title>
		<author>
			<persName><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingzhi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanru</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.13818</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Generalizing to unseen domains via adversarial data augmentation</title>
		<author>
			<persName><forename type="first">Riccardo</forename><surname>Volpi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongseok</forename><surname>Namkoong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ozan</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vittorio</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">GLUE: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Neural networks are more productive teachers than human raters: Active mixup for data-efficient knowledge distillation from a blackbox model</title>
		<author>
			<persName><forename type="first">Dongdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yandong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1498" to="1507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">EDA: Easy data augmentation techniques for boosting performance on text classification tasks</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1670</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6382" to="6388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1101</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1112" to="1122" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.6</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Conditional BERT contextual augmentation</title>
		<author>
			<persName><forename type="first">Xing</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shangwen</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangjun</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jizhong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songlin</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Science</title>
				<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="84" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Unsupervised data augmentation for consistency training</title>
		<author>
			<persName><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6256" to="6268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Reweighting augmented samples by minimizing the maximal expected loss</title>
		<author>
			<persName><forename type="first">Mingyang</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi-Ming</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">HellaSwag: Can a machine really finish your sentence?</title>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1472</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4791" to="4800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">PAWS: Paraphrase adversaries from word scrambling</title>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1131</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1298" to="1308" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
