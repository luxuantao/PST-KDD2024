<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ProSelfLC: Progressive Self Label Correction for Training Robust Deep Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xinshao</forename><surname>Wang</surname></persName>
							<email>xinshao@zenithai.co.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">Zenith Ai</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Biomedical Engineering</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yang</forename><surname>Hua</surname></persName>
							<email>y.hua@qub.ac.uk</email>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Institute of Electronics</orgName>
								<orgName type="department" key="dep2">Communications and Information Technology</orgName>
								<orgName type="institution">Queen&apos;s University</orgName>
								<address>
									<settlement>Belfast</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Elyor</forename><surname>Kodirov</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zenith Ai</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Clifton</surname></persName>
							<email>david.clifton@eng.ox.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="department">Institute of Biomedical Engineering</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Neil</forename><forename type="middle">M</forename><surname>Robertson</surname></persName>
							<email>n.robertson@qub.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">Zenith Ai</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Institute of Electronics</orgName>
								<orgName type="department" key="dep2">Communications and Information Technology</orgName>
								<orgName type="institution">Queen&apos;s University</orgName>
								<address>
									<settlement>Belfast</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ProSelfLC: Progressive Self Label Correction for Training Robust Deep Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>To train robust deep neural networks (DNNs), we systematically study several target modification approaches, which include output regularisation, self and non-self label correction (LC). Two key issues are discovered: (1) Self LC is the most appealing as it exploits its own knowledge and requires no extra models. However, how to automatically decide the trust degree of a learner as training goes is not well answered in the literature? (2) Some methods penalise while the others reward low-entropy predictions, prompting us to ask which one is better?</p><p>To resolve the first issue, taking two well-accepted propositions-deep neural networks learn meaningful patterns before fitting noise <ref type="bibr" target="#b2">[3]</ref> and minimum entropy regularisation principle [10]-we propose a novel end-to-end method named ProSelfLC, which is designed according to learning time and entropy. Specifically, given a data point, we progressively increase trust in its predicted label distribution versus its annotated one if a model has been trained for enough time and the prediction is of low entropy (high confidence). For the second issue, according to ProSelfLC, we empirically prove that it is better to redefine a meaningful low-entropy status and optimise the learner toward it. This serves as a defence of entropy minimisation.</p><p>We demonstrate the effectiveness of ProSelfLC through extensive experiments in both clean and noisy settings. The source code is available at https://github.com/ XinshaoAmosWang/ProSelfLC-CVPR2021.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>There exist many target (label) modification approaches. They can be roughly divided into two groups: (1) Output regularisation (OR), which is proposed to penalise overconfident predictions for regularising deep neural networks. It includes label smoothing (LS) <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b28">29]</ref> and confidence penalty (CP) <ref type="bibr" target="#b32">[33]</ref>; <ref type="bibr" target="#b1">(2)</ref> Label correction (LC). On the one hand, LC regularises neural networks by adding the similarity structure information over training classes into onehot label distributions so that the learning targets become structured and soft. On the other hand, it can correct the semantic classes of noisy label distributions. LC can be further divided into two subgroups: Non-self LC and Self LC. The former requires extra learners, while the latter relies on the model itself. A typical approach of Non-self LC is knowledge distillation (KD), which exploits the predictions of other model(s), usually termed teacher(s) <ref type="bibr" target="#b16">[17]</ref>. Self LC methods include Pseudo-Label <ref type="bibr" target="#b22">[23]</ref>, bootstrapping (Boot-soft and Boot-hard) <ref type="bibr" target="#b34">[35]</ref>, Joint Optimisation (Jointsoft and Joint-hard) <ref type="bibr" target="#b42">[43]</ref>, and Tf-KD self <ref type="bibr" target="#b54">[55]</ref>. According to an overview in Figure <ref type="figure" target="#fig_1">1</ref> (detailed derivation is in Section 3 and Table <ref type="table" target="#tab_0">1</ref>), in label modification, the output target of a data point is defined by combining a one-hot label distribution and its corresponding prediction or a predefined label distribution.</p><p>Firstly, we present the drawbacks of existing approaches: (1) OR methods naively penalise confident outputs without leveraging easily accessible knowledge from other learners or itself (Figure <ref type="figure" target="#fig_1">1a</ref>); <ref type="bibr" target="#b1">(2)</ref> Non-self LC relies on accurate auxiliary models to generate predictions (Figure <ref type="figure" target="#fig_1">1b</ref>). (3) Self LC is the most appealing because it exploits its own knowledge and requires no extra learners. However, there is a core question that is not well answered:</p><p>In Self LC, how much should we trust a learner to leverage its knowledge?</p><p>As shown in Figure <ref type="figure" target="#fig_1">1b</ref>, in Self LC, for a data point, we have two labels: a predefined one-hot q and a predicted structured p. Its learning target is (1 − ǫ)q + ǫp, i.e., a trade-off between q and p, where ǫ defines the trust score of a learner. In existing methods, ǫ is fixed without considering that a model's knowledge grows as the training progresses. For example, in bootstrapping, ǫ is fixed throughout the training process. Joint Optimisation stage-wisely trains a model. It fully trusts predicted labels and uses them to replace old ones when a stage ends, i.e., ǫ = 1. Tf-KD self trains a model by two stages: ǫ = 0 in the first one while ǫ is tuned for the second stage. Note that p is gen-</p><formula xml:id="formula_0">LS CP 1/3 1/3 1/3 1 0 0 1/2 1/3 1/6 1 0 0 0 0 1 1</formula><p>(a) OR includes LS <ref type="bibr" target="#b41">[42]</ref> and CP <ref type="bibr" target="#b32">[33]</ref>. LS softens a target by adding a uniform label distribution. CP changes the probability 1 to a smaller value 1 − ǫ in the one-hot target. The double-ended arrow means factual equivalence, because an output is definitely non-negative after a softmax layer. (b) LC contains Self LC <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b54">55]</ref> and Non-self LC <ref type="bibr" target="#b16">[17]</ref>. The parameter ǫ defines how much a predicted label distribution is trusted. To improve Self LC, we propose a novel method named Progressive Self Label Correction (ProSelfLC), which is end-to-end trainable and needs negligible extra cost. Most importantly, ProSelfLC modifies the target progressively and adaptively as training goes. Two design principles of ProSelfLC are: (1) When a model learns from scratch, human annotations are more reliable than its own predictions in the early phase, during which the model is learning simple meaningful patterns before fitting noise, even when severe label noise exists in human annotations <ref type="bibr" target="#b2">[3]</ref>. (2) As a learner attains confident knowledge as time progresses, we leverage it to revise annotated labels. This is surrounded by minimum entropy regularisation, which is widely evaluated in unsupervised and semi-supervised scenarios <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>.</p><p>Secondly, note that OR methods penalise low entropy while LC rewards it, intuitively leading to a second vital question:</p><p>Should we penalise a low-entropy status or reward it?</p><p>Entropy minimisation is the most widely used principle in machine learning <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b21">22]</ref>. In standard classification, minimising categorical cross entropy (CCE) optimises a model towards a low-entropy status defined by human annotations, which contain noise in very large-scale machine learning. As a result, confidence penalty becomes popular for reducing noisy fitting. In contrast, we prove that it is better to reward a meaningful low-entropy status redefined by our ProSelfLC. Therefore, our work offers a defence of entropy minimisation against the recent confidence penalty practice <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b5">6]</ref>. Finally, we summarise our main contributions:</p><p>• We provide a theoretical study on popular target modification methods through entropy and KL divergence <ref type="bibr" target="#b20">[21]</ref>. Accordingly, we reveal their drawbacks and propose ProSelfLC as a solution. ProSelfLC can: (1) enhance the similarity structure information over training classes;</p><p>(2) correct the semantic classes of noisy label distributions. ProSelfLC is the first method to trust self knowledge progressively and adaptively.</p><p>• Our extensive experiments: (1) defend the entropy minimisation principle; (2) demonstrate the effectiveness of ProSelfLC in both clean and noisy settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Label noise and semi-supervised learning. We test target modification approaches in the setting of label noise because it is generic and connected with semi-supervised learning, where only a subset of training examples are annotated, leading to missing labels. Then the key to semisupervised training is to reliably fill them. When these missing labels are incorrectly filled, the challenge of semisupervised learning changes to noisy labels. For a further comparison, in semi-supervised learning, the annotated set is clean and reliable, because the label noise only exists in the unannotated set. While in our experimental setting, we are not given information on whether an example is trusted or not, thus being even more challenging. We summarise existing approaches for solving label noise: (1) Loss correction, in which we are given or we need to estimate a noisetransition matrix, which defines the distribution of noise labels <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b47">48]</ref>. A noise-transition matrix is difficult and complex to estimate in practice; (2) Exploiting an auxiliary trusted training set to differentiate examples <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b15">16]</ref>. This requires extra annotation cost;</p><p>(3) Co-training strategies, which train two or more learners <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b24">25]</ref> and exploit their 'disagreement' information to differentiate data points; (4) Label engineering methods <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b24">25]</ref>, which relate to our focus in this work. Their strategy is to annotate unlabelled samples or correct noisy labels.</p><p>LC and knowledge distillation (KD) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b16">17]</ref>. Mathematically, we derive that some KD methods also modify labels. We use the term label correction instead of KD for two reasons: (1) label correction is more descriptive; (2) the scope of KD is not limited to label modification. For example, multiple networks are trained for KD <ref type="bibr" target="#b6">[7]</ref>. When two models are trained, the consistency between their predictions of a data point is promoted in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b57">58]</ref>, while the distance between their feature maps is reduced in <ref type="bibr" target="#b36">[37]</ref>. Regarding self KD, two examples of the same class are constrained to have consistent output distributions <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b55">56]</ref>. In another self KD <ref type="bibr" target="#b56">[57]</ref>, the deepest classifier provides knowledge for shallower classifiers. In a recent self KD method <ref type="bibr" target="#b54">[55]</ref>, Tf-KD self applies two-stage training. In the second stage, a model is trained by exploiting its knowledge learned in the first stage. Our focus is to improve the end-to-end self LC. First, self KD methods <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b56">57]</ref> maximise the consistency of intraclass images' predictions or the consistency of different classifiers. In our view, they do not modify labels, thus being less relevant for comparison. Second, the two-stage self KD method <ref type="bibr" target="#b54">[55]</ref> can be an add-on (i.e., an enhancement plugin) other than a competitor. E.g., in realworld practice, the first stage can be ProSelfLC instead of CCE with early stopping. Finally, we acknowledge that exploiting ProSelfLC to improve non-self KD and stage-wise approaches is an important area for future work, e.g., a better teacher model can be trained using ProSelfLC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Mathematical Analysis and Theory</head><formula xml:id="formula_1">Let X = {(x i , y i )} N i=1 represent N training examples, where (x i , y i ) denotes i−th sample with input x i ∈ R D and label y i ∈ {1, 2, ..., C}. C is the number of classes. A deep neural network z consists of an embedding network f (•) : R D → R K and a linear classifier g(•) : R K → R C , i.e., z i = z(x i ) = g(f (x i )) : R D → R C .</formula><p>For the brevity of analysis, we take a data point and omit its subscript so that it is denoted by (x, y). The linear classifier is usually the last fully-connected layer. Its output is named logit vector z ∈ R C . We produce its classification probabilities p by normalising the logits using a softmax function:</p><formula xml:id="formula_2">p(j|x) = exp(z j )/ C m=1 exp(z m ),<label>(1)</label></formula><p>where p(j|x) is the probability of x belonging to class j. Its corresponding ground-truth is usually denoted by a onehot representation q: q(j|x) = 1 if j = y, q(j|x) = 0 otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Semantic class and similarity structure in a label distribution</head><p>A probability vector p ∈ R C can also be interpreted as an instance-to-classes similarity vector, i.e., p(j|x) measures how much a data point x is similar with (analogously, likely to be) j-th class. Consequently, p should not be exactly one-hot, and is proposed to be corrected at training, so that it can define a more informative and structured learning target. For better clarity, we first present two definitions:</p><p>Definition 1 (Semantic Class). Given a target label distribution q(x) ∈ R C , the semantic class is defined by arg max j q(j|x), i.e., the class whose probability is the largest.</p><p>Definition 2 (Similarity Structure). In q(x), x has C probabilities of being predicted to C classes. The similarity structure of x versus C classes is defined by these probabilities and their differences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Revisit of CCE, LS, CP and LC</head><p>Standard CCE. For any input (x, y), the minimisation objective of standard CCE is:</p><formula xml:id="formula_3">L CCE (q, p) = H(q, p) = E q (− log p),<label>(2)</label></formula><p>where H(•, •) represents the cross entropy. E q (− log p) denotes the expectation of negative log-likelihood, and q serves as the probability mass function.</p><p>Label smoothing. In LS <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b16">17]</ref>, we soften one-hot targets by adding a uniform distribution: qLS = (1 − ǫ)q + ǫu, u ∈ R C , and ∀j, u j = 1 C . Consequently:</p><formula xml:id="formula_4">L CCE+LS (q, p; ǫ) = H(q LS , p) = E qLS (− log p) = (1 − ǫ)H(q, p)+ǫH(u, p).<label>(3)</label></formula><p>Confidence penalty. CP <ref type="bibr" target="#b32">[33]</ref> penalises highly confident predictions:</p><formula xml:id="formula_5">L CCE+CP (q, p; ǫ) = (1 − ǫ)H(q, p)−ǫH(p, p).<label>(4</label></formula><p>) Label correction. As illustrated in Figure <ref type="figure" target="#fig_1">1</ref>, LC is a family of algorithms, where a one-hot label distribution is modified to a convex combination of itself and a predicted distribution:</p><formula xml:id="formula_6">qLC = (1 − ǫ)q + ǫp ⇒ L CCE+LC (q, p; ǫ) = H(q LC , p) = (1 − ǫ)H(q, p)+ǫH(p, p).</formula><p>(5) We remark: (1) p provides meaningful information about an example's relative probabilities of being different training classes; (2) If ǫ is large, and p is confident in predicting a different class, i.e., arg max j p(j|x) = arg max j q(j|x), qLC defines a different semantic class from q.  </p><formula xml:id="formula_7">q qLS = (1 − ǫ)q +ǫu qCP = (1 − ǫ)q −ǫp qLC = (1 − ǫ)q +ǫp Cross Entropy Eq(− log p) E qLS (− log p) E qCP (− log p) E qLC (− log p) KL Divergence KL(q||p) (1 − ǫ)KL(q||p) +ǫKL(u||p) (1 − ǫ)KL(q||p) +ǫKL(p||u) (1 − ǫ)KL(q||p) −ǫKL(p||u)</formula><formula xml:id="formula_8">(q, p; ǫ) = (1 − ǫ)H(q, p)−ǫH(p, p) = E (1−ǫ)q−ǫp (− log p). Therefore, qCP = (1 − ǫ)q−ǫp. Additionally, qLS = (1 − ǫ)q+ǫu, qLC = (1 − ǫ)q+ǫp.</formula><p>Proposition 2. Some KD methods, which aim to minimise the KL divergence between predictions of a teacher and a student, belong to the family of label correction. Proof. In general, a loss function of such methods can be defined to be L KD (q, p t , p) = (1 − ǫ)H(q, p) + ǫKL(p t ||p) <ref type="bibr" target="#b54">[55]</ref>. KL(•||•) denotes the KL divergence. As KL(p t ||p) = H(p t , p) − H(p t , p t ), p t is from a teacher and fixed when training a student. We can omit H(p t , p t ):</p><formula xml:id="formula_9">L KD (q, p t , p) = (1 − ǫ)H(q, p) + ǫH(p t , p) = E (1−ǫ)q+ǫpt (− log p) ⇒ qKD = (1 − ǫ)q + ǫp t .<label>(6)</label></formula><p>Consistent with LC in Eq (5), L KD (q, p t , p) revises a label using p t .</p><p>Proposition 3. Compared with CCE, LS and CP penalise entropy minimisation while LC reward it.</p><p>Proposition 4. In CCE, LS and CP, a data point x has the same semantic class. In addition, x has an identical probability of belonging to other classes except for its semantic class.</p><p>The proof of propositions 3 and 4 is presented in the Appendix A. Only LC exploits informative information and has the ability to correct labels, while LS and CP only relax the hard targets. We summarise CCE, LS, CP and LC in Table <ref type="table" target="#tab_0">1</ref>. Constant terms are ignored for concision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">ProSelfLC: Progressive and Adaptive Label Correction</head><p>In standard CCE, a semantic class is considered while the similarity structure is ignored. It is mainly due to the difficulty of annotating the similarity structure for every data point, especially when C is large <ref type="bibr" target="#b49">[50]</ref>. Fortunately, recent progress demonstrates that there are some effective approaches to define the similarity structure of data points without annotation: <ref type="bibr" target="#b0">(1)</ref> In KD, an auxiliary teacher model can provide a student model the similarity structure information <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b28">29]</ref>; (2) In Self LC, e.g., Boot-soft, a model helps itself by exploiting the knowledge it has learned so far. We focus on studying the end-to-end Self LC.</p><p>In Self LC, ǫ indicates how much a predicted label distribution is trusted. In ProSelfLC, we propose to set it automatically according to learning time t and prediction entropy H(p), i.e., ProSelfLC trusts self knowledge according to training time and confidence. For any x, we summarise:</p><formula xml:id="formula_10">                     Loss: L ( qProSelfLC , p; ǫ ProSelfLC ) = H(q ProSelfLC , p) = E qProSelfLC (− log p). Label: qProSelfLC = (1 − ǫ ProSelfLC )q + ǫ ProSelfLC p. ǫ ProSelfLC = g(t) × l(p)    g(t) = h(t/Γ − 0.5, B) ∈ (0, 1), l(p) = 1 − H(p)/H(u) ∈ (0, 1).</formula><p>(7) t and Γ are the iteration counter and the number of total iterations, respectively. h(η, B) = 1/(1 + exp(−η × B)).</p><p>Here, η = t/Γ − 0.5. B, Γ are task-dependent and searched on a validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Self trust scores</head><p>Global trust score g(t) denotes how much we trust a learner. It is independent of data points, thus being global. g(t) grows as t rises. B adjusts the exponentiation's base and growth speed of g(t).</p><p>Local trust score l(p) indicates how much we trust an output distribution p, which is data-dependent. l(p) rises as H(p) becomes lower, rewarding a confident distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Design reasons</head><p>Regarding g(t), in the earlier learning phase, i.e., t &lt; Γ/2, g(t) &lt; 0.5 ⇒ ǫ ProSelfLC &lt; 0.5, ∀p, so that the human annotations dominate and ProSelfLC only modifies the similarity structure. When a learner has not seen the training data for enough time at the earlier stage, its knowledge is less reliable and a wrong confident prediction may occur. Our design assuages the bad impact of such unexpected cases. When it comes to the later training phase, i.e., t &gt; Γ/2, we have g(t) &gt; 0.5 as it has been trained for more than half of entire iterations.</p><p>Table <ref type="table">2</ref>: The values of g(t), l(p) and ǫ ProSelfLC = g(t) × l(p) under different cases. We use concrete values for concise interpretation. We bold the special case when the semantic class is changed. l(p): Consistency is defined by whether p and q share the semantic class or not. 0.1(non-confident) 0.9(confidently consistent) 0.9(confidently inconsistent) Earlier phase g(t) = 0.1 0.01 0.09 0.09 Later phase g(t) = 0.9 0.09 0.81 0.81  Regarding l(p), it affects the later learning phase. If p is less confident, l(p) will be smaller, then ǫ ProSelfLC will be smaller, hence we trust p less when it is of higher uncertainty. If p is highly confident, we trust its confident knowledge. Ablation study of our design is in Figure <ref type="figure" target="#fig_3">2</ref>, where three variants of ǫ are presented. In our experiments, note that when ǫ is fixed, we try three values (0.125, 0.25, 0.50) and display the best instantiation, i.e., ǫ = 0.50.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Case analysis</head><p>Due to the potential memorisation in the earlier phase (which is more rare), we may get unexpected confident wrong predictions for noisy labels, but their trust scores are small as g(t) is small. We conduct the case analysis of Pro-SelfLC in Table <ref type="table">2</ref> and summarise its core tactics as follows:</p><p>(1) Correct the similarity structure for every data point in all cases, thanks to exploiting the self knowledge of a learner, i.e., p.</p><p>(2) Revise the semantic class when t is large enough and p is confidently inconsistent. As highlighted in Table <ref type="table">2</ref>, when two conditions are met, we have ǫ ProSelfLC &gt; 0.5 and arg max j p(j|x) = arg max j q(j|x), then p redefines the semantic class. For example, if p = [0.95, 0.01, 0.04], q = [0, 0, 1], ǫ ProSelfLC = 0.8 ⇒ qProSelfLC = (1 − ǫ ProSelfLC )q + ǫ ProSelfLC p = [0.76, 0.008, 0.232]. Note that ProSelfLC also becomes robust against lengthy exposure to the training data, as demonstrated in Figures <ref type="figure" target="#fig_5">2, 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In deep learning, small differences (e.g., random accelerators like cudnn and different frameworks like Caffe <ref type="bibr" target="#b17">[18]</ref>,</p><p>Tensorflow <ref type="bibr" target="#b0">[1]</ref> and PyTorch <ref type="bibr" target="#b30">[31]</ref>) may lead to a large gap of final performance. Therefore, to compare more properly, we re-implement CCE, LS and CP. Regarding Self LC methods, we re-implement Boot-soft <ref type="bibr" target="#b34">[35]</ref>, where ǫ is fixed throughout training. We do not re-implement stagewise Self LC and KD methods, e.g., Joint Optimisation and Tf-KD self respectively, because time-consuming tuning is required. We fix the random seed and do not use any random accelerator. In standard and synthetic cases, we train on 80% training data (corrupted in synthetic cases) and use 20% trusted training data as a validation set to search all hyperparameters, e.g., ǫ, Γ, B and settings of an optimiser. Note that Γ and an optimiser's settings are searched first and then fixed for all methods. Finally, we retrain a model on the entire training data (corrupted in synthetic cases) and report its accuracy on the test data to fairly compare with prior results. In real-world label noise, the used dataset has a separate clean validation set. Here, a clean dataset is used only for validation, which is generally necessary for any method and differs from the methods <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b59">60]</ref> which use a clean dataset to train a network's learning parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Standard image classification</head><p>Datasets and training details. (1) CIFAR-100 <ref type="bibr" target="#b19">[20]</ref> has 20 coarse classes, each containing 5 fine classes. There are 500 and 100 images per class in the training and testing sets, respectively. The image size is 32 × 32. We apply simple data augmentation <ref type="bibr" target="#b14">[15]</ref>, i.e., we pad 4 pixels on every side of the image, and then randomly crop it with a size of 32 × 32. Finally, this crop is horizontally flipped with a   (2) We train ResNet-50 <ref type="bibr" target="#b14">[15]</ref> on ImageNet 2012 classification dataset, which has 1k classes and 50k images in the test set <ref type="bibr" target="#b38">[39]</ref>. We use SGD with a start learning rate of 2e − 3. A polynomial learning rate decay with a power of 2 is used. We set the momentum to 0.95 and the weight decay to 1e − 4. We train on a single V100 GPU and the batch size is 64. We report the final test accuracy when the training ends at 500k iterations. We use the standard data augmentation: an original image is warped to 256 × 256, followed by a random crop of 224 × 224. This crop is randomly flipped. We fix common settings to fairly compare CCE, LS, CP, Boot-soft and ProSelfLC.</p><p>Result analysis. In Table <ref type="table" target="#tab_2">3</ref>, we observe the superiority of ProSelfLC in standard setting without considering label noise. Being probably surprising, LS and CP reduce the performance consistently as ǫ increases on ImageNet. Instead, Boot-soft and ProSelfLC improve versus CCE. We remark that both test sets are large so that their differences are noticeable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Synthetic label noise</head><p>Noise generation. (1) Symmetric label noise: the original label of an image is uniformly changed to one of the other classes with a probability of r; (2) Asymmetric label noise: we follow <ref type="bibr" target="#b45">[46]</ref> to generate asymmetric label noise to fairly compare with their reported results. Within each coarse class, we randomly select two fine classes A and B. Then we flip r × 100% labels of A to B, and r × 100% labels of B to A. We remark that the overall label noise rate is smaller than r. Baselines. <ref type="foot" target="#foot_0">1</ref> We compare with the results reported recently in SL <ref type="bibr" target="#b45">[46]</ref>. Forward is a loss correction approach that uses a noise-transition matrix <ref type="bibr" target="#b31">[32]</ref>. D2L monitors the subspace dimensionality change at training <ref type="bibr" target="#b26">[27]</ref>. GCE denotes generalised cross entropy <ref type="bibr" target="#b58">[59]</ref> and SL is symmetric cross entropy <ref type="bibr" target="#b45">[46]</ref>. They are robust losses designed for solving label noise. Training details are the same as Section 5.1.</p><p>Result analysis. For all methods, we directly report their final results when training terminates. Therefore, we test the robustness of a model against not only label noise, but also a long time being exposed to the data. In Table <ref type="table" target="#tab_3">4</ref>, we observe that: (1) ProSelfLC outperforms all baselines, which is significant in most cases; (2) In both implementation, Boot-hard and Boot-soft perform worse than the others. However, our ProSelfLC makes Self LC the best solution. Furthermore, learning are visualised in Figure <ref type="figure" target="#fig_5">3</ref>, which helps to understand why ProSelfLC works bet-ter. Figure <ref type="figure" target="#fig_7">4</ref> shows the learning dynamics when r changes. Results of different B, ǫ are in Table <ref type="table" target="#tab_4">5</ref>. We note that when the noise rate is higher, a smaller B performs better.</p><p>We further discuss their differences on model calibration <ref type="bibr" target="#b10">[11]</ref> in Appendix B, the changes of entropy and ǫ ProSelfLC during training in Appendix C.</p><p>Revising the semantic class and similarity structure. In Figures <ref type="figure" target="#fig_5">3b and 3c</ref>, we show dynamic statistics of different approaches on fitting wrong labels and correcting them. ProSelfLC is much better than its counterparts. Semantic class correction reflects the change of similarity structure.</p><p>To redefine and reward a low-entropy status. On the one hand, we observe that LS and CP work well, being consistent with prior claims. In Figures <ref type="figure" target="#fig_5">3d and 3e</ref>, the entropies of both clean and noisy subsets are much higher in LS and CP, correspondingly their generalisation is the best except for ProSelfLC in Figure <ref type="figure" target="#fig_5">3f</ref>. On the other hand, ProSelfLC has the lowest entropy while performs the best, which proves that a learner's confidence does not necessarily weaken its generalisation performance. Instead, a model needs to be careful with what to be confident in. As shown   In the others, we show the second half iterations, which are of higher interest. As the noise rate increases, the superiority of ProSelfLC becomes more significant, i.e., avoiding fitting noise in the 2nd row and better generalisation in the 1st row.</p><p>by Figures <ref type="figure" target="#fig_5">3b and 3c</ref>, ProSelfLC has the least wrong fitting and most semantic class correction, which indicates that a meaningful low-entropy status is redefined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Real-world label noise</head><p>Clothing 1M <ref type="bibr" target="#b47">[48]</ref> has around 38.46% label noise in the training data and about 1 million images of 14 classes from shopping websites. Its internal noise structure is agnostic.</p><p>Baselines. For loss correction and estimating the noisetransition matrix, S-adaption <ref type="bibr" target="#b7">[8]</ref> uses an extra softmax layer, while Masking <ref type="bibr" target="#b11">[12]</ref> exploits human cognition. MD-DYR-SH <ref type="bibr" target="#b1">[2]</ref> is a combination of three techniques: dynamic mixup (MD), dynamic bootstrapping together with label regularisation (DYR) and soft to hard (SH). The other baselines have been introduced heretofore.</p><p>Training details. We follow <ref type="bibr" target="#b42">[43]</ref> to train ResNet-50 and initialise it by a trained model on ImageNet. We follow Section 5.1 with small changes: the initial learning rate is 0.01 and we train 10k iterations. They are searched on the separate clean validation set.</p><p>Result analysis. In Table <ref type="table" target="#tab_5">6</ref>, analogously to CIFAR-100, we report our trained results of CCE, LS, CP, Boot-soft and ProSelfLC for an entirely fair comparison. ProSelfLC has the highest accuracy, which demonstrates its effectiveness again.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We present a thorough mathematical study on several target modification techniques. Through analysis of entropy and KL divergence, we reveal their relationships and limitations. To improve and endorse self label correction, we propose ProSelfLC. Extensive experiments prove its superiority over existing methods under standard and noisy settings. ProSelfLC enhances the similarity structure information over classes, and rectifies the semantic classes of noisy label distributions. ProSelfLC is the first approach to trust self knowledge progressively and adaptively.</p><p>ProSelfLC redirects and promotes entropy minimisation, which is in marked contrast to recent practices of confidence penalty <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b5">6]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Target modification includes OR (LS and CP), and LC (Self LC and Non-self LC). Assume there are three training classes. q is the one-hot target. u is a uniform label distribution. p denotes a predicted label distribution. The target combination parameter is ǫ ∈ [0, 1]. erated by a preceding-stage model in stage-wise training, which requires significant human intervention and is timeconsuming in practice.To improve Self LC, we propose a novel method named Progressive Self Label Correction (ProSelfLC), which is end-to-end trainable and needs negligible extra cost. Most importantly, ProSelfLC modifies the target progressively and adaptively as training goes. Two design principles of ProSelfLC are: (1) When a model learns from scratch, human annotations are more reliable than its own predictions in the early phase, during which the model is learning simple meaningful patterns before fitting noise, even when severe label noise exists in human annotations<ref type="bibr" target="#b2">[3]</ref>. (2) As a learner attains confident knowledge as time progresses, we leverage it to revise annotated labels. This is surrounded by minimum entropy regularisation, which is widely evaluated in unsupervised and semi-supervised scenarios<ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>.Secondly, note that OR methods penalise low entropy while LC rewards it, intuitively leading to a second vital question:Should we penalise a low-entropy status or reward it?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Study of setting ǫ using three schemes: global trust and local trust, merely global trust, and fixed ǫ. Experiments are done on CIFAR-100 with 40% asymmetric label noise. Vertical axes denote evaluation metrics. Mean results are displayed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: Comprehensive learning dynamics on CIFAR-100 with 40% asymmetric label noise. Vertical axes denote evaluation metrics. Their mean results are displayed. At training, a learner is NOT GIVEN whether a label is trusted or not. We store intermediate models and analyse them when the training ends. probability of 0.5. We choose SGD with its settings as: (a) a learning rate of 0.1; (b) a momentum of 0.9; (c) a weight decay of 5e − 4; (d) the batch size is 256 and the number of training iterations is 30k. We divide the learning rate by 10 at 15k and 22k iterations, respectively. (2) We train ResNet-50<ref type="bibr" target="#b14">[15]</ref> on ImageNet 2012 classification dataset, which has 1k classes and 50k images in the test set<ref type="bibr" target="#b38">[39]</ref>. We use SGD with a start learning rate of 2e − 3. A polynomial learning rate decay with a power of 2 is used. We set the momentum to 0.95 and the weight decay to 1e − 4. We train on a single V100 GPU and the batch size is 64. We report the final test accuracy when the training ends at 500k iterations. We use the standard data augmentation: an original image is warped to 256 × 256, followed by a random crop of 224 × 224. This crop is randomly flipped. We fix common settings to fairly compare CCE, LS, CP, Boot-soft and ProSelfLC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(a) r = 20%. (b) r = 30%. (c) r = 40%. (d) r = 20%. (e) r = 30%. (f) r = 40%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>Figure4: Learning dynamics on CIFAR-100 under asymmetric noisy labels. We show all iterations only in (a) and (d). In the others, we show the second half iterations, which are of higher interest. As the noise rate increases, the superiority of ProSelfLC becomes more significant, i.e., avoiding fitting noise in the 2nd row and better generalisation in the 1st row.</figDesc><graphic url="image-45.png" coords="8,52.59,277.75,163.35,128.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Summary of CCE, LS, CP and LC.</figDesc><table><row><cell>CCE</cell><cell>LS</cell><cell>CP</cell><cell>LC</cell></row><row><cell>Learning Target</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Test accuracy (%) in the standard setting. We report three settings of hyperparameters.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell>CCE</cell><cell>LS (ǫ)</cell><cell>CP (ǫ)</cell><cell>Boot-soft (ǫ)</cell><cell>ProSelfLC (B)</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">0.125 0.25 0.50 0.125 0.25 0.50 0.125 0.25 0.50</cell><cell>8</cell><cell>10</cell><cell>12</cell></row><row><cell cols="2">CIFAR-100</cell><cell>69.0</cell><cell cols="4">69.9 69.6 68.4 69.5 69.3 68.7 68.9 69.1 69.1 70.1 70.3 69.8</cell></row><row><cell cols="3">ImageNet 2012 75.5</cell><cell cols="4">75.3 75.2 74.9 75.2 74.8 74.6 75.7 75.8 75.8 76.0 76.0 75.9</cell></row><row><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>10 4</cell><cell></cell><cell></cell></row><row><cell cols="3">(a) Correct fitting.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Accuracy (%) on the CIFAR-100 clean test set. All compared methods use ResNet-44.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Asymmetric Noisy Labels</cell><cell cols="3">Symmetric Noisy Labels</cell></row><row><cell></cell><cell>Method</cell><cell>r=0.2</cell><cell>r=0.3</cell><cell>r=0.4</cell><cell>r=0.2</cell><cell>r=0.4</cell><cell>r=0.6</cell></row><row><cell></cell><cell>Boot-hard</cell><cell>63.4</cell><cell>63.2</cell><cell>62.1</cell><cell>57.9</cell><cell>48.2</cell><cell>12.3</cell></row><row><cell>Results From SL [46]</cell><cell>Forward D2L GCE</cell><cell>64.1 62.4 63.0</cell><cell>64.0 63.2 63.2</cell><cell>60.9 61.4 61.7</cell><cell>59.8 59.2 59.1</cell><cell>53.1 52.0 53.3</cell><cell>24.7 35.3 36.2</cell></row><row><cell></cell><cell>SL</cell><cell>65.6</cell><cell>65.1</cell><cell>63.1</cell><cell>60.0</cell><cell>53.7</cell><cell>41.5</cell></row><row><cell></cell><cell>CCE</cell><cell>66.6</cell><cell>63.4</cell><cell>59.5</cell><cell>58.0</cell><cell>50.1</cell><cell>37.9</cell></row><row><cell>Our Trained Results</cell><cell>LS CP Boot-soft</cell><cell>67.9 67.7 66.9</cell><cell>66.4 66.0 65.3</cell><cell>65.0 64.4 61.0</cell><cell>63.8 64.0 63.2</cell><cell>57.2 56.8 59.0</cell><cell>46.5 44.1 44.8</cell></row><row><cell></cell><cell>ProSelfLC</cell><cell>68.7</cell><cell>68.5</cell><cell>67.9</cell><cell>64.8</cell><cell>59.3</cell><cell>47.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>The results of different hyperparameters on CIFAR-100 using ResNet-44. Under different noise rates, the best instantiation of each approach is bolded except for CCE.</figDesc><table><row><cell>Method (hyperparameter)</cell><cell>Value of hyperparameter</cell><cell cols="7">Asymmetric label noise Symmetric label noise Clean 20% 30% 40% 20% 40% 60%</cell></row><row><cell>CCE</cell><cell>None or ǫ = 0</cell><cell>66.6</cell><cell>63.4</cell><cell>59.5</cell><cell>58.0</cell><cell>50.1</cell><cell>37.9</cell><cell>69.0</cell></row><row><cell></cell><cell>0.125</cell><cell>66.4</cell><cell>65.6</cell><cell>63.1</cell><cell>61.7</cell><cell>52.5</cell><cell>39.1</cell><cell>69.9</cell></row><row><cell>LS (ǫ)</cell><cell>0.25</cell><cell>67.9</cell><cell>66.4</cell><cell>65.0</cell><cell>62.8</cell><cell>55.9</cell><cell>40.9</cell><cell>69.6</cell></row><row><cell></cell><cell>0.50</cell><cell>66.8</cell><cell>65.8</cell><cell>64.6</cell><cell>63.8</cell><cell>57.2</cell><cell>46.5</cell><cell>68.4</cell></row><row><cell></cell><cell>0.125</cell><cell>65.7</cell><cell>64.2</cell><cell>60.3</cell><cell>59.8</cell><cell>52.3</cell><cell>39.6</cell><cell>69.5</cell></row><row><cell>CP (ǫ)</cell><cell>0.25</cell><cell>66.8</cell><cell>65.1</cell><cell>61.6</cell><cell>61.0</cell><cell>53.3</cell><cell>40.9</cell><cell>69.3</cell></row><row><cell></cell><cell>0.50</cell><cell>67.7</cell><cell>66.0</cell><cell>64.4</cell><cell>64.0</cell><cell>56.8</cell><cell>44.1</cell><cell>68.7</cell></row><row><cell></cell><cell>0.125</cell><cell>65.8</cell><cell>64.1</cell><cell>60.7</cell><cell>59.7</cell><cell>51.2</cell><cell>40.6</cell><cell>68.9</cell></row><row><cell>Boot-soft (ǫ)</cell><cell>0.25</cell><cell>66.2</cell><cell>64.1</cell><cell>60.3</cell><cell>61.1</cell><cell>54.4</cell><cell>43.3</cell><cell>69.1</cell></row><row><cell></cell><cell>0.50</cell><cell>66.9</cell><cell>65.3</cell><cell>61.0</cell><cell>63.2</cell><cell>59.0</cell><cell>44.8</cell><cell>69.1</cell></row><row><cell></cell><cell>8</cell><cell>67.8</cell><cell>67.4</cell><cell>67.9</cell><cell>64.7</cell><cell>57.7</cell><cell>47.7</cell><cell>70.1</cell></row><row><cell></cell><cell>10</cell><cell>68.5</cell><cell>68.5</cell><cell>66.8</cell><cell>63.9</cell><cell>59.0</cell><cell>47.5</cell><cell>70.3</cell></row><row><cell>ProSelfLC (B)</cell><cell>12</cell><cell>68.6</cell><cell>67.9</cell><cell>67.4</cell><cell>64.0</cell><cell>59.3</cell><cell>47.5</cell><cell>69.8</cell></row><row><cell></cell><cell>14</cell><cell>68.7</cell><cell>68.0</cell><cell>67.8</cell><cell>64.8</cell><cell>59.0</cell><cell>47.4</cell><cell>69.6</cell></row><row><cell></cell><cell>16</cell><cell>68.4</cell><cell>67.2</cell><cell>67.3</cell><cell>63.7</cell><cell>59.0</cell><cell>32.3</cell><cell>69.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Test accuracy (%) on the real-world noisy dataset Clothing 1M.</figDesc><table><row><cell>Boot-hard</cell><cell cols="2">Forward D2L GCE SL</cell><cell>S-adaptation</cell><cell>Masking</cell><cell>MD-DYR-SH</cell><cell>Joint-soft</cell><cell cols="3">Our Trained Results CCE LS CP Boot-soft ProSelfLC</cell></row><row><cell>68.9</cell><cell>69.8</cell><cell>69.5 69.8 71.0</cell><cell>70.3</cell><cell>71.1</cell><cell>71.0</cell><cell cols="2">72.2 71.8 72.6 72.4</cell><cell>72.3</cell><cell>73.4</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">We do not consider DisturbLabel<ref type="bibr" target="#b48">[49]</ref>, which flips labels randomly and is counter-intuitive. It weakens the generalisation because generally the accuracy drops as the uniform label noise increases.</note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>* Prof. David A. Clifton was supported by the National Institute for Health Research (NIHR) Oxford Biomedical Research Centre (BRC).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th USENIX Symposium on Operating Systems Design and Implementation</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised label noise modeling and loss correction</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Arazo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Ortego</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O'</forename><surname>Noel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName><surname>Mcguinness</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A closer look at memorization in deep networks</title>
		<author>
			<persName><forename type="first">Devansh</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanisław</forename><surname>Jastrzębski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxinder</forename><forename type="middle">S</forename><surname>Kanwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tegan</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asja</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Lacoste-Julien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Do deep nets really need to be deep</title>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rich Caruana, and Alexandru Niculescu-Mizil. Model compression</title>
		<author>
			<persName><forename type="first">Cristian</forename><surname>Bucila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDDM</title>
				<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Ramesh Raskar, and Nikhil Naik. Maximum-entropy fine grained classification</title>
		<author>
			<persName><forename type="first">Abhimanyu</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Otkrist</forename><surname>Gupta</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Born again neural networks</title>
		<author>
			<persName><forename type="first">Tommaso</forename><surname>Furlanello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Training deep neural-networks using a noise adaptation layer</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ehud</forename><surname>Ben-Reuven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Semi-supervised learning by entropy minimization</title>
		<author>
			<persName><forename type="first">Yves</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Entropy regularization. Semi-supervised learning</title>
		<author>
			<persName><forename type="first">Yves</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="151" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On calibration of modern neural networks</title>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoff</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Masking: A new perspective of noisy supervision</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangchao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivor</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Coteaching: Robust training of deep neural networks with extremely noisy labels</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingrui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivor</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Algorithm as 136: A k-means clustering algorithm</title>
		<author>
			<persName><forename type="first">A</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manchek</forename><forename type="middle">A</forename><surname>Hartigan</surname></persName>
		</author>
		<author>
			<persName><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series C (Applied Statistics)</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="100" to="108" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Using trusted data to train deep networks on labels corrupted by severe noise</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duncan</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS Deep Learning and Representation Learning Workshop</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMMM</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels</title>
		<author>
			<persName><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">On information and sufficiency. The annals of mathematical statistics</title>
		<author>
			<persName><forename type="first">Solomon</forename><surname>Kullback</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">A</forename><surname>Leibler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1951">1951</date>
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="page">436</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName><forename type="first">Dong-Hyun</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Cleannet: Transfer learning for scalable image classifier training with label noise</title>
		<author>
			<persName><forename type="first">Kuang-Huei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjun</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Dividemix: Learning with noisy labels as semi-supervised learning</title>
		<author>
			<persName><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">Ch</forename><surname>Hoi</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning from noisy labels with distillation</title>
		<author>
			<persName><forename type="first">Yuncheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yale</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangliang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dimensionality-driven learning with noisy labels</title>
		<author>
			<persName><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">E</forename><surname>Houle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><forename type="middle">M</forename><surname>Erfani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu-Tao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sudanthi</forename><surname>Wijewickrema</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Decoupling &quot;when to update&quot; from &quot;how to update</title>
		<author>
			<persName><forename type="first">Eran</forename><surname>Malach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shai</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">When does label smoothing help?</title>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Obtaining well calibrated probabilities using bayesian binning</title>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Mahdi Pakdaman Naeini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milos</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><surname>Hauskrecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Making deep neural networks robust to label noise: A loss correction approach</title>
		<author>
			<persName><forename type="first">Giorgio</forename><surname>Patrini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Rozza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizhen</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Regularizing neural networks by penalizing confident output distributions</title>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Pereyra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep co-training for semi-supervised image recognition</title>
		<author>
			<persName><forename type="first">Siyuan</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhishuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Training deep neural networks on noisy labels with bootstrapping</title>
		<author>
			<persName><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Bin Yang, and Raquel Urtasun. Learning to reweight examples for robust deep learning</title>
		<author>
			<persName><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyuan</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Fitnets: Hints for thin deep nets</title>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlo</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>David E Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="page" from="533" to="536" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Selfie: Refurbishing unclean samples for robust deep learning</title>
		<author>
			<persName><forename type="first">Hwanjun</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minseok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jae-Gil</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Learning from noisy labels with deep neural networks</title>
		<author>
			<persName><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2080</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Joint optimization framework for learning with noisy labels</title>
		<author>
			<persName><forename type="first">Daiki</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daiki</forename><surname>Ikami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toshihiko</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiyoharu</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Toward robustness against label noise in training deep discriminative neural networks</title>
		<author>
			<persName><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning from noisy largescale datasets with minimal supervision</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gal</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Symmetric cross entropy for robust learning with noisy labels</title>
		<author>
			<persName><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaiyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Combating noisy labels by agreement: A joint training method with co-regularization</title>
		<author>
			<persName><forename type="first">Hongxin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>An</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning from massive noisy labeled data for image classification</title>
		<author>
			<persName><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Disturblabel: Regularizing cnn on the loss layer</title>
		<author>
			<persName><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Variational label enhancement</title>
		<author>
			<persName><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun-Peng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Geng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Data-distortion guided self-distillation for deep neural networks</title>
		<author>
			<persName><forename type="first">Ting-</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng-Lin</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Safeguarded dynamic label regression for noisy supervision</title>
		<author>
			<persName><forename type="first">Jiangchao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivor</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning to learn from noisy web videos</title>
		<author>
			<persName><forename type="first">Serena</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liyue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">How does disagreement help generalization against label corruption</title>
		<author>
			<persName><forename type="first">Xingrui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangchao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivor</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Revisiting knowledge distillation via label smoothing regularization</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><forename type="middle">Eh</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guilin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Regularizing class-wise predictions via self-knowledge distillation</title>
		<author>
			<persName><forename type="first">Sukmin</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jongjin</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kimin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Be your own teacher: Improve the performance of convolutional neural networks via self distillation</title>
		<author>
			<persName><forename type="first">Linfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiebo</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anni</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingwei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenglong</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaisheng</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Deep mutual learning</title>
		<author>
			<persName><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Generalized cross entropy loss for training deep neural networks with noisy labels</title>
		<author>
			<persName><forename type="first">Zhilu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mert</forename><forename type="middle">R</forename><surname>Sabuncu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Distilling effective supervision from severe label noise</title>
		<author>
			<persName><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sercan</forename><forename type="middle">O</forename><surname>Arik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
