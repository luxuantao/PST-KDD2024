<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Network Flow Algorithms for Structured Sparsity</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Julien</forename><surname>Mairal</surname></persName>
							<email>julien.mairal@inria.fr</email>
						</author>
						<author>
							<persName><forename type="first">Rodolphe</forename><surname>Jenatton</surname></persName>
							<email>rodolphe.jenatton@inria.fr</email>
						</author>
						<author>
							<persName><forename type="first">Guillaume</forename><surname>Obozinski</surname></persName>
							<email>guillaume.obozinski@inria.fr</email>
						</author>
						<author>
							<persName><forename type="first">Francis</forename><surname>Bach</surname></persName>
							<email>francis.bach@inria.fr</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">INRIA -Willow Project-Team</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">INRIA -Willow Project-Team</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory" key="lab1">Laboratoire d&apos;Informatique de l&apos;</orgName>
								<orgName type="laboratory" key="lab2">UMR 8548</orgName>
								<orgName type="institution" key="instit1">Ecole Normale Supérieure (INRIA/ENS</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Network Flow Algorithms for Structured Sparsity</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5E172B18981163715662CE1C35F281A8</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We consider a class of learning problems that involve a structured sparsityinducing norm defined as the sum of ℓ ∞ -norms over groups of variables. Whereas a lot of effort has been put in developing fast optimization methods when the groups are disjoint or embedded in a specific hierarchical structure, we address here the case of general overlapping groups. To this end, we show that the corresponding optimization problem is related to network flow optimization. More precisely, the proximal problem associated with the norm we consider is dual to a quadratic min-cost flow problem. We propose an efficient procedure which computes its solution exactly in polynomial time. Our algorithm scales up to millions of variables, and opens up a whole new range of applications for structured sparse models. We present several experiments on image and video data, demonstrating the applicability and scalability of our approach for various problems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sparse linear models have become a popular framework for dealing with various unsupervised and supervised tasks in machine learning and signal processing. In such models, linear combinations of small sets of variables are selected to describe the data. Regularization by the ℓ 1 -norm has emerged as a powerful tool for addressing this combinatorial variable selection problem, relying on both a well-developed theory (see <ref type="bibr" target="#b0">[1]</ref> and references therein) and efficient algorithms <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>.</p><p>The ℓ 1 -norm primarily encourages sparse solutions, regardless of the potential structural relationships (e.g., spatial, temporal or hierarchical) existing between the variables. Much effort has recently been devoted to designing sparsity-inducing regularizations capable of encoding higher-order information about allowed patterns of non-zero coefficients <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>, with successful applications in bioinformatics <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11]</ref>, topic modeling <ref type="bibr" target="#b11">[12]</ref> and computer vision <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>.</p><p>By considering sums of norms of appropriate subsets, or groups, of variables, these regularizations control the sparsity patterns of the solutions. The underlying optimization problem is usually difficult, in part because it involves nonsmooth components. Proximal methods have proven to be effective in this context, essentially because of their fast convergence rates and their scalability <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. While the settings where the penalized groups of variables do not overlap or are embedded in a treeshaped hierarchy <ref type="bibr" target="#b11">[12]</ref> have already been studied, regularizations with general overlapping groups have, to the best of our knowledge, never been addressed with proximal methods. This paper makes the following contributions:</p><p>-It shows that the proximal operator associated with the structured norm we consider can be computed with a fast and scalable procedure by solving a quadratic min-cost flow problem.</p><p>-It shows that the dual norm of the sparsity-inducing norm we consider can also be evaluated efficiently, which enables us to compute duality gaps for the corresponding optimization problems.</p><p>-It demonstrates that our method is relevant for various applications, from video background subtraction to estimation of hierarchical structures for dictionary learning of natural image patches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Structured Sparse Models</head><p>We consider in this paper convex optimization problems of the form min w∈R p f (w) + λΩ(w),</p><p>where f : R p → R is a convex differentiable function and Ω : R p → R is a convex, nonsmooth, sparsity-inducing regularization function. When one knows a priori that the solutions of this learning problem have only a few non-zero coefficients, Ω is often chosen to be the ℓ 1 -norm (see <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>). When these coefficients are organized in groups, a penalty encoding explicitly this prior knowledge can improve the prediction performance and/or interpretability of the learned models <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>. Denoting by G a set of groups of indices, such a penalty might for example take the form:</p><formula xml:id="formula_1">Ω(w) g∈G η g max j∈g |w j | = g∈G η g w g ∞ ,<label>(2)</label></formula><p>where w j is the j-th entry of w for j in [1; p] {1, . . . , p}, the vector w g in R |g| records the coefficients of w indexed by g in G, and the scalars η g are positive weights. A sum of ℓ 2 -norms is also used in the literature <ref type="bibr" target="#b6">[7]</ref>, but the ℓ ∞ -norm is piecewise linear, a property that we take advantage of in this paper. Note that when G is the set of singletons of [1; p], we get back the ℓ 1 -norm.</p><p>If G is a more general partition of [1; p], variables are selected in groups rather than individually. When the groups overlap, Ω is still a norm and sets groups of variables to zero together <ref type="bibr" target="#b4">[5]</ref>. The latter setting has first been considered for hierarchies <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b14">15]</ref>, and then extended to general group structures <ref type="bibr" target="#b4">[5]</ref>. <ref type="foot" target="#foot_0">1</ref> Solving Eq. (1) in this context becomes challenging and is the topic of this paper. Following Jenatton et al. <ref type="bibr" target="#b11">[12]</ref> who tackled the case of hierarchical groups, we propose to approach this problem with proximal methods, which we now introduce.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Proximal Methods</head><p>In a nutshell, proximal methods can be seen as a natural extension of gradient-based techniques, and they are well suited to minimizing the sum f + λΩ of two convex terms, a smooth function f -continuously differentiable with Lipschitz-continuous gradient-and a potentially non-smooth function λΩ (see <ref type="bibr" target="#b15">[16]</ref> and references therein). At each iteration, the function f is linearized at the current estimate w 0 and the so-called proximal problem has to be solved:</p><formula xml:id="formula_2">min w∈R p f (w 0 ) + (w -w 0 ) ⊤ ∇f (w 0 ) + λΩ(w) + L 2 w -w 0 2 2 .</formula><p>The quadratic term keeps the solution in a neighborhood where the current linear approximation holds, and L &gt; 0 is an upper bound on the Lipschitz constant of ∇f . This problem can be rewritten as</p><formula xml:id="formula_3">min w∈R p 1 2 u -w 2 2 + λ ′ Ω(w),<label>(3)</label></formula><p>with λ ′ λ/L, and u w 0 -1 L ∇f (w 0 ). We call proximal operator associated with the regularization λ ′ Ω the function that maps a vector u in R p onto the (unique, by strong convexity) solution w ⋆ of Eq. (3). Simple proximal methods use w ⋆ as the next iterate, but accelerated variants <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> are also based on the proximal operator and require to solve problem (3) exactly and efficiently to enjoy their fast convergence rates. Note that when Ω is the ℓ 1 -norm, the solution of Eq. ( <ref type="formula" target="#formula_3">3</ref>) is obtained by soft-thresholding <ref type="bibr" target="#b15">[16]</ref>. The approach we develop in the rest of this paper extends <ref type="bibr" target="#b11">[12]</ref> to the case of general overlapping groups when Ω is a weighted sum of ℓ ∞ -norms, broadening the application of these regularizations to a wider spectrum of problems. <ref type="foot" target="#foot_1">2</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">A Quadratic Min-Cost Flow Formulation</head><p>In this section, we show that a convex dual of problem (3) for general overlapping groups G can be reformulated as a quadratic min-cost flow problem. We present an efficient algorithm to solve it exactly, as well as a related algorithm to compute the dual norm of Ω. We start by considering the dual formulation to problem (3) introduced in <ref type="bibr" target="#b11">[12]</ref>, for the case where Ω is a sum of ℓ ∞ -norms: Lemma 1 (Dual of the proximal problem <ref type="bibr" target="#b11">[12]</ref>) Given u in R p , consider the problem</p><formula xml:id="formula_4">min ξ∈R p×|G| 1 2 u - g∈G ξ g 2 2 s.t. ∀g ∈ G, ξ g 1 ≤ λη g and ξ g j = 0 if j / ∈ g,<label>(4)</label></formula><p>where ξ = (ξ g ) g∈G is in R p×|G| , and ξ g j denotes the j-th coordinate of the vector ξ g . Then, every solution ξ ⋆ = (ξ ⋆g ) g∈G of Eq. ( <ref type="formula" target="#formula_4">4</ref>) satisfies w ⋆ = u-g∈G ξ ⋆g , where w ⋆ is the solution of Eq. ( <ref type="formula" target="#formula_3">3</ref>).</p><p>Without loss of generality, <ref type="foot" target="#foot_2">3</ref> we assume from now on that the scalars u j are all non-negative, and we constrain the entries of ξ to be non-negative. We now introduce a graph modeling of problem (4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Graph Model</head><p>Let G be a directed graph G = (V, E, s, t), where V is a set of vertices, E ⊆ V × V a set of arcs, s a source, and t a sink. Let c and c ′ be two functions on the arcs, c : E → R and c ′ : E → R + , where c is a cost function and c ′ is a non-negative capacity function. A flow is a non-negative function on arcs that satisfies capacity constraints on all arcs (the value of the flow on an arc is less than or equal to the arc capacity) and conservation constraints on all vertices (the sum of incoming flows at a vertex is equal to the sum of outgoing flows) except for the source and the sink.</p><p>We introduce a canonical graph G associated with our optimization problem, and uniquely characterized by the following construction: (i) V is the union of two sets of vertices V u and V gr , where V u contains exactly one vertex for each index j in [1; p], and V gr contains exactly one vertex for each group g in G. We thus have |V | = |G| + p. For simplicity, we identify groups and indices with the vertices of the graph. (ii) For every group g in G, E contains an arc (s, g). These arcs have capacity λη g and zero cost. (iii) For every group g in G, and every index j in g, E contains an arc (g, j) with zero cost and infinite capacity. We denote by ξ g j the flow on this arc. (iv) For every index j in [1; p], E contains an arc (j, t) with infinite capacity and a cost c j 1 2 (u j -ξj ) 2 , where ξj is the flow on (j, t). Note that by flow conservation, we necessarily have ξj = g∈G ξ g j . Examples of canonical graphs are given in Figures <ref type="figure" target="#fig_0">1(a)-(c</ref>). The flows ξ g j associated with G can now be identified with the variables of problem (4): indeed, the sum of the costs on the edges leading to the sink is equal to the objective function of (4), while the capacities of the arcs (s, g) match the constraints on each group. This shows that finding a flow minimizing the sum of the costs on such a graph is equivalent to solving problem (4).</p><p>When some groups are included in others, the canonical graph can be simplified to yield a graph with a smaller number of edges. Specifically, if h and g are groups with h ⊂ g, the edges (g, j) for j ∈ h carrying a flow ξ g j can be removed and replaced by a single edge (g, h) of infinite capacity and zero cost, carrying the flow j∈h ξ g j . This simplification is illustrated in Figure <ref type="figure" target="#fig_0">1</ref>(d), with a graph equivalent to the one of Figure <ref type="figure" target="#fig_0">1(c</ref>). This does not change the optimal value of ξ⋆ , which is the quantity of interest for computing the optimal primal variable w ⋆ (a proof and a formal definition of these equivalent graphs are available in a longer technical report <ref type="bibr" target="#b16">[17]</ref>). These simplifications are useful in practice, since they reduce the number of edges in the graph and improve the speed of the algorithms we are now going to present. The source is linked to every group g, h with respective maximum capacity λη g , λη h and zero cost. Each variable u j is linked to the sink t, with an infinite capacity, and with a cost c j 1 2 (u j -ξj ) 2 . All other arcs in the graph have zero cost and infinite capacity. They represent inclusion relationships in-between groups, and between groups and variables. The graphs (c) and (d) correspond to a special case of tree-structured hierarchy in the sense of <ref type="bibr" target="#b11">[12]</ref>. Their min-cost flow problems are equivalent.</p><formula xml:id="formula_5">s g ξ g 1 +ξ g 2 +ξ g 3 ≤ λη g u 2 ξ g 2 u 1 ξ g 1 u 3 ξ g 3 t ξ1 , c 1 ξ2 , c 2 ξ3 , c 3 (a) G = {g = {1, 2, 3}}. s g ξ g 1 +ξ g 2 ≤ λη g h ξ h 2 +ξ h 3 ≤ λη h u 2 ξ h 2 ξ g 2 u 1 ξ g 1 u 3 ξ h 3 t ξ1 , c 1 ξ2 , c 2 ξ3 , c 3 (b) G = {g = {1, 2}, h = {2, 3}}. s g ξ g 1 +ξ g 2 +ξ g 3 ≤ λη g h ξ h 2 +ξ h 3 ≤ λη h u 2 ξ h 2 ξ g 2 u 1 ξ g 1 u 3 ξ g 3 ξ h 3 t ξ1 , c 1 ξ2 , c 2 ξ3 , c 3 (c) G = {g = {1, 2, 3}, h = {2, 3}}. s g ξ g 1 +ξ g 2 +ξ g 3 ≤ λη g h ξ h 2 +ξ h 3 ≤ λη h ξ g 2 +ξ g 3 u 2 ξ g 2 +ξ h 2 u 1 ξ g 1 u 3 ξ g 3 +ξ h 3 t ξ1 , c 1 ξ2 , c 2 ξ3 , c 3 (d) G = {g = {1} ∪ h, h = {2, 3}}.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Computation of the Proximal Operator</head><p>Quadratic min-cost flow problems have been well studied in the operations research literature <ref type="bibr" target="#b17">[18]</ref>. One of the simplest cases, where G contains a single group g (Ω is the ℓ ∞ -norm) as in Figure <ref type="figure" target="#fig_0">1(a)</ref>, can be solved by an orthogonal projection on the ℓ 1 -ball of radius λη g . It has been shown that such a projection can be done in O(p) operations <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>. When the group structure is a tree as in Figure <ref type="figure" target="#fig_0">1(d)</ref>, the problem can be solved in O(pd) operations, where d is the depth of the tree <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b17">18]</ref>. <ref type="foot" target="#foot_3">4</ref>The general case of overlapping groups is more difficult. Hochbaum and Hong have shown in <ref type="bibr" target="#b17">[18]</ref> that quadratic min-cost flow problems can be reduced to a specific parametric max-flow problem, for which an efficient algorithm exists <ref type="bibr" target="#b19">[20]</ref>. <ref type="foot" target="#foot_4">5</ref> While this generic approach could be used to solve Eq. ( <ref type="formula" target="#formula_4">4</ref>), we propose to use Algorithm 1 that also exploits the fact that our graphs have non-zero costs only on edges leading to the sink. As shown in the technical report <ref type="bibr" target="#b16">[17]</ref>, it has a significantly better performance in practice. This algorithm clearly shares some similarities with existing approaches in network flow optimization such as the simplified version of <ref type="bibr" target="#b19">[20]</ref> presented in <ref type="bibr" target="#b20">[21]</ref> that uses a divide and conquer strategy. Moreover, we have discovered after that this paper was accepted for publication that an equivalent algorithm exists for minimizing convex functions over polymatroid sets <ref type="bibr" target="#b21">[22]</ref>. This equivalence, however, requires a non-trivial representation of structured sparsityinducing norms with submodular functions, as recently pointed out by <ref type="bibr" target="#b22">[23]</ref>.</p><p>Algorithm 1 Computation of the proximal operator for overlapping groups.</p><p>1: Inputs: u ∈ R p , a set of groups G, positive weights (η g ) g∈G , and λ (regularization parameter).</p><p>2: Build the initial graph G 0 = (V 0 , E 0 , s, t) as explained in Section 3.2.</p><p>3: Compute the optimal flow: ξ ← computeFlow(V 0 , E 0 ). 4: Return: w = u -ξ (optimal solution of the proximal problem).</p><formula xml:id="formula_6">Function computeFlow(V = V u ∪ V gr , E) 1: Projection step: γ ← arg min γ j∈Vu 1 2 (u j -γ j ) 2 s.t. j∈Vu γ j ≤ λ g∈Vgr η g . 2:</formula><p>For all nodes j in V u , set γ j to be the capacity of the arc (j, t). 3: Max-flow step: Update ( ξj ) j∈Vu by computing a max-flow on the graph (V, E, s, t).</p><formula xml:id="formula_7">4: if ∃ j ∈ V u s.t. ξj = γ j then 5:</formula><p>Denote by (s, V + ) and (V -, t) the two disjoint subsets of (V, s, t) separated by the minimum (s, t)-cut of the graph, and remove the arcs between V + and V -. Call E + and E -the two remaining disjoint subsets of E corresponding to V + and V -.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>( ξj ) j∈V + u ← computeFlow(V + , E + ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>( ξj</p><formula xml:id="formula_8">) j∈V - u ← computeFlow(V -, E -). 8: end if 9: Return: ( ξj ) j∈Vu .</formula><p>The intuition behind this algorithm is the following: The first step looks for a candidate value for ξ = g∈G ξ g by solving a relaxed version of problem Eq. ( <ref type="formula" target="#formula_4">4</ref>), where the constraints ξ g 1 ≤ λη g are dropped and replaced by a single one ξ 1 ≤ λ g∈G η g . The relaxed problem only depends on ξ and can be solved in linear time. By calling its solution γ, it provides a lower bound uγ 2 2 /2 on the optimal cost. Then, the second step tries to find a feasible flow of the original problem (4) such that the resulting vector ξ matches γ, which is in fact a max-flow problem <ref type="bibr" target="#b23">[24]</ref>. If ξ = γ, then the cost of the flow reaches the lower bound, and the flow is optimal. If ξ = γ, the lower bound is not achievable, and we construct a minimum (s, t)-cut of the graph <ref type="bibr" target="#b24">[25]</ref> that defines two disjoints sets of nodes V + and V -; V + is the part of the graph that could potentially have received more flow from the source (the arcs between s and V + are not saturated), whereas all arcs linking s to V -are saturated. At this point, it is possible to show that the value of the optimal min-cost flow on all arcs between V + and V -is necessary zero. Thus, removing them yields an equivalent optimization problem, which can be decomposed into two independent problems of smaller sizes and solved recursively by the calls to computeFlow(V + , E + ) and computeFlow(V -, E -). A formal proof of correctness of Algorithm 1 and further details are relegated to <ref type="bibr" target="#b16">[17]</ref>.</p><p>The approach of <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20]</ref> is guaranteed to have the same worst-case complexity as a single max-flow algorithm. However, we have experimentally observed a significant discrepancy between the worst case and empirical complexities for these flow problems, essentially because the empirical cost of each max-flow is significantly smaller than its theoretical cost. Despite the fact that the worst-case guarantee of our algorithm is weaker than their (up to a factor |V |), it is more adapted to the structure of our graphs and has proven to be much faster in our experiments (see technical report <ref type="bibr" target="#b16">[17]</ref>). Some implementation details are crucial to the efficiency of the algorithm:</p><p>• Exploiting connected components: When there exists no arc between two subsets of V , it is possible to process them independently in order to solve the global min-cost flow problem.</p><p>• Efficient max-flow algorithm: We have implemented the "push-relabel" algorithm of <ref type="bibr" target="#b23">[24]</ref> for solving our max-flow problems, using classical heuristics that significantly speed it up in practice (see <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b25">26]</ref>). This algorithm leverages the concept of pre-flow that relaxes the definition of flow and allows vertices to have a positive excess. It can be initialized with any valid pre-flow, enabling warm-restarts when the max-flow is called several times as in our algorithm.</p><p>• Improved projection step: The first line of the function computeFlow can be replaced by γ ← arg min γ j∈Vu 1 2 (u jγ j ) 2 s.t.</p><p>j∈Vu γ j ≤ λ g∈Vgr η g and |γ j | ≤ λ g∋j η g . The idea is that the structure of the graph will not allow ξj to be greater than λ g∋j η g after the maxflow step. Adding these additional constraints leads to better performance when the graph is not well balanced. This modified projection step can still be computed in linear time <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Computation of the Dual Norm</head><p>The dual norm Ω * of Ω, defined for any vector κ in R p by Ω * (κ) max Ω(z)≤1 z ⊤ κ, is a key quantity to study sparsity-inducing regularizations <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b26">27]</ref>. We use it here to monitor the convergence of the proximal method through a duality gap, and define a proper optimality criterion for problem <ref type="bibr" target="#b0">(1)</ref>. We denote by f * the Fenchel conjugate of f <ref type="bibr" target="#b27">[28]</ref>, defined by</p><formula xml:id="formula_9">f * (κ) sup z [z ⊤ κ -f (z)].</formula><p>The duality gap for problem (1) can be derived from standard Fenchel duality arguments <ref type="bibr" target="#b27">[28]</ref> and it is equal to f (w) + λΩ(w) + f * (-κ) for w, κ in R p with Ω * (κ) ≤ λ. Therefore, evaluating the duality gap requires to compute efficiently Ω * in order to find a feasible dual variable κ. This is equivalent to solving another network flow problem, based on the following variational formulation:</p><formula xml:id="formula_10">Ω * (κ) = min ξ∈R p×|G| τ s.t. g∈G ξ g = κ, and ∀g ∈ G, ξ g 1 ≤ τ η g with ξ g j = 0 if j / ∈ g. (<label>5</label></formula><formula xml:id="formula_11">)</formula><p>In the network problem associated with ( <ref type="formula" target="#formula_10">5</ref>), the capacities on the arcs (s, g), g ∈ G, are set to τ η g , and the capacities on the arcs (j, t), j in [1; p], are fixed to κ j . Solving problem (5) amounts to finding the smallest value of τ , such that there exists a flow saturating the capacities κ j on the arcs leading to the sink t (i.e., ξ = κ). The algorithm below is proven to be correct in <ref type="bibr" target="#b16">[17]</ref>.</p><p>Algorithm 2 Computation of the dual norm. 1: Inputs: κ ∈ R p , a set of groups G, positive weights (η g ) g∈G .</p><p>2: Build the initial graph G 0 = (V 0 , E 0 , s, t) as explained in Section 3.3.</p><p>3: τ ← dualNorm(V 0 , E 0 ). 4: Return: τ (value of the dual norm).</p><p>Function dualNorm(V = V u ∪ V gr , E) 1: τ ← ( j∈Vu κ j )/( g∈Vgr η g ) and set the capacities of arcs (s, g) to τ η g for all g in V gr . 2: Max-flow step: Update ( ξj ) j∈Vu by computing a max-flow on the graph (V, E, s, t).</p><formula xml:id="formula_12">3: if ∃ j ∈ V u s.t. ξj = κ j then 4:</formula><p>Define (V + , E + ) and (V -, E -) as in Algorithm 1, and set τ ← dualNorm(V -, E -). 5: end if 6: Return: τ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Applications and Experiments</head><p>Our experiments use the algorithm of <ref type="bibr" target="#b3">[4]</ref> based on our proximal operator, with weights η g set to 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Speed Comparison</head><p>We compare our method (ProxFlow) and two generic optimization techniques, namely a subgradient descent (SG) and an interior point method, <ref type="foot" target="#foot_5">6</ref> on a regularized linear regression problem. Both SG and ProxFlow are implemented in C++. Experiments are run on a single-core 2.8 GHz CPU. We consider a design matrix X in R n×p built from overcomplete dictionaries of discrete cosine transforms (DCT), which are naturally organized on one-or two-dimensional grids and display local correlations. The following families of groups G using this spatial information are thus considered: (1) every contiguous sequence of length 3 for the one-dimensional case, and (2) every 3×3-square in the two-dimensional setting. We generate vectors y in R n according to the linear model y = Xw 0 + ε, where ε ∼ N (0, 0.01 Xw 0 2 2 ). The vector w 0 has about 20% percent nonzero components, randomly selected, while respecting the structure of G, and uniformly generated between [-1, 1].</p><p>In our experiments, the regularization parameter λ is chosen to achieve the same sparsity as w 0 . For SG, we take the step size to be equal to a/(k + b), where k is the iteration number, and (a, b) are the best parameters selected in {10 -3 , . . . , 10}×{10 2 , 10 3 , 10 4 }. For the interior point methods, since problem (1) can be cast either as a quadratic (QP) or as a conic program (CP), we show in Figure <ref type="figure" target="#fig_2">2</ref> the results for both formulations. Our approach compares favorably with the other methods, on three problems of different sizes, (n, p) ∈ {(100, 10 3 ), (1024, 10 4 ), (1024, 10 5 )}, see Figure <ref type="figure" target="#fig_2">2</ref>. In addition, note that QP, CP and SG do not obtain sparse solutions, whereas ProxFlow does. We have also run ProxFlow and SG on a larger dataset with (n, p) = (100, 10 6 ): after 12 hours, ProxFlow and SG have reached a relative duality gap of 0.0006 and 0.02 respectively.<ref type="foot" target="#foot_6">7</ref>    For the top row, the percentage of pixels matching the ground truth is 98.8% with Ω, 87.0% without. As for the bottom row, the result is 93.8% with Ω, 90.4% without (best seen in color).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Background Subtraction</head><p>Following <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>, we consider a background subtraction task. Given a sequence of frames from a fixed camera, we try to segment out foreground objects in a new image. If we denote by y ∈ R n a test image, we model y as a sparse linear combination of p other images X ∈ R n×p , plus an error term e in R n , i.e., y ≈ Xw + e for some sparse vector w in R p . This approach is reminiscent of <ref type="bibr" target="#b28">[29]</ref> in the context of face recognition, where e is further made sparse to deal with occlusions. The term Xw accounts for background parts present in both y and X, while e contains specific, or foreground, objects in y. The resulting optimization problem is min w,e 1 2 y -Xwe 2 2 + λ 1 w 1 + λ 2 e 1 , with λ 1 , λ 2 ≥ 0. In this formulation, the ℓ 1 -norm penalty on e does not take into account the fact that neighboring pixels in y are likely to share the same label (background or foreground), which may lead to scattered pieces of foreground and background regions (Figure <ref type="figure" target="#fig_3">3</ref>). We therefore put an additional structured regularization term Ω on e, where the groups in G are all the overlapping 3×3-squares on the image. A dataset with hand-segmented evaluation images is used to illustrate the effect of Ω. <ref type="foot" target="#foot_7">8</ref> For simplicity, we use a single regularization parameter, i.e., λ 1 = λ 2 , chosen to maximize the number of pixels matching the ground truth. We consider p = 200 images with n = 57600 pixels (i.e., a resolution of 120×160, times 3 for the RGB channels). As shown in Figure <ref type="figure" target="#fig_3">3</ref>, adding Ω improves the background subtraction results for the two tested videos, by encoding, unlike the ℓ 1 -norm, both spatial and color consistency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Multi-Task Learning of Hierarchical Structures</head><p>In <ref type="bibr" target="#b11">[12]</ref>, Jenatton et al. have recently proposed to use a hierarchical structured norm to learn dictionaries of natural image patches. Following this work, we seek to represent n signals {y 1 , . . . , y n } of dimension m as sparse linear combinations of elements from a dictionary X = [x 1 , . . . , x p ] in R m×p . This can be expressed for all i in [1; n] as y i ≈ Xw i , for some sparse vector w i in R p . In <ref type="bibr" target="#b11">[12]</ref>, the dictionary elements are embedded in a predefined tree T , via a particular instance of the structured norm Ω; we refer to it as Ω tree , and call G the underlying set of groups. In this case, each signal y i admits a sparse decomposition in the form of a subtree of dictionary elements.</p><p>Inspired by ideas from multi-task learning <ref type="bibr" target="#b13">[14]</ref>, we propose to learn the tree structure T by pruning irrelevant parts of a larger initial tree T 0 . We achieve this by using an additional regularization term Ω joint across the different decompositions, so that subtrees of T 0 will simultaneously be removed for all signals y i . In other words, the approach of <ref type="bibr" target="#b11">[12]</ref> is extended by the following formulation:</p><formula xml:id="formula_13">min X,W 1 n n i=1 1 2 y i -Xw i 2 2 + λ 1 Ω tree (w i ) +λ 2 Ω joint (W), s.t. x j 2 ≤ 1, for all j in [1; p],<label>(6)</label></formula><p>where W [w 1 , . . . , w n ] is the matrix of decomposition coefficients in R p×n . The new regularization term operates on the rows of W and is defined as Ω joint (W) g∈G max i∈[1;n] |w i g |. <ref type="foot" target="#foot_8">9</ref> The overall penalty on W, which results from the combination of Ω tree and Ω joint , is itself an instance of Ω with general overlapping groups, as defined in Eq (2).</p><p>To address problem (6), we use the same optimization scheme as <ref type="bibr" target="#b11">[12]</ref>, i.e., alternating between X and W, fixing one variable while optimizing with respect to the other. The task we consider is the denoising of natural image patches, with the same dataset and protocol as <ref type="bibr" target="#b11">[12]</ref>. We study whether learning the hierarchy of the dictionary elements improves the denoising performance, compared to standard sparse coding (i.e., when Ω tree is the ℓ 1 -norm and λ 2 = 0) and the hierarchical dictionary learning of <ref type="bibr" target="#b11">[12]</ref> based on predefined trees (i.e., λ 2 = 0). The dimensions of the training set -50 000 patches of size 8×8 for dictionaries with up to p = 400 elements -impose to handle large graphs, with |E| ≈ |V | ≈ 4.10 7 . Since problem ( <ref type="formula" target="#formula_13">6</ref>) is too large to be solved many times to select the regularization parameters (λ 1 , λ 2 ) rigorously, we use the following heuristics: we optimize mostly with the currently pruned tree held fixed (i.e., λ 2 = 0), and only prune the tree (i.e., λ 2 &gt; 0) every few steps on a random subset of 10 000 patches. We consider the same hierarchies as in <ref type="bibr" target="#b11">[12]</ref>, involving between 30 and 400 dictionary elements. The regularization parameter λ 1 is selected on the validation set of 25 000 patches, for both sparse coding (Flat) and hierarchical dictionary learning (Tree). Starting from the tree giving the best performance (in this case the largest one, see Figure <ref type="figure" target="#fig_5">4</ref>), we solve problem (6) following our heuristics, for increasing values of λ 2 . As shown in Figure <ref type="figure" target="#fig_5">4</ref>, there is a regime where our approach performs significantly better than the two other compared methods. The standard deviation of the noise is 0.2 (the pixels have values in [0, 1]); no significant improvements were observed for lower levels of noise.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have presented a new optimization framework for solving sparse structured problems involving sums of ℓ ∞ -norms of any (overlapping) groups of variables. Interestingly, this sheds new light on connections between sparse methods and the literature of network flow optimization. In particular, the proximal operator for the formulation we consider can be cast as a quadratic min-cost flow problem, for which we propose an efficient and simple algorithm. This allows the use of accelerated gradient methods. Several experiments demonstrate that our algorithm can be applied to a wide class of learning problems, which have not been addressed before within sparse methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Graph representation of simple proximal problems with different group structures G. The three indices 1, 2, 3 are represented as grey squares, and the groups g, h in G as red discs. The source is linked to every group g, h with respective maximum capacity λη g , λη h and zero cost. Each variable u j is linked to the sink t, with an infinite capacity, and with a cost c j</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>, one-dimensional DCT log(CPU time) in seconds log(relative distance to optimum) , one-dimensional DCT log(CPU time) in seconds log(relative distance to optimum) ProxFlow SG</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: Speed comparisons: distance to the optimal primal value versus CPU time (log-log scale).6   </figDesc><graphic coords="7,107.05,215.97,74.48,52.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: From left to right: original image y; estimated background Xw; foreground (the sparsity pattern of e used as mask on y) estimated with ℓ 1 ; foreground estimated with ℓ 1 + Ω; another foreground obtained with Ω, on a different image, with the same values of λ 1 , λ 2 as for the previous image.For the top row, the percentage of pixels matching the ground truth is 98.8% with Ω, 87.0% without. As for the bottom row, the result is 93.8% with Ω, 90.4% without (best seen in color).</figDesc><graphic coords="7,107.05,272.71,74.48,52.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Left: Hierarchy obtained by pruning a larger tree of 76 elements. Right: Mean square error versus dictionary size. The error bars represent two standard deviations, based on three runs.</figDesc><graphic coords="8,108.00,416.62,153.99,117.81" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Note that other types of structured sparse models have also been introduced, either through a different norm<ref type="bibr" target="#b5">[6]</ref>, or through non-convex criteria<ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>For hierarchies, the approach of<ref type="bibr" target="#b11">[12]</ref> applies also to the case of where Ω is a weighted sum of ℓ2-norms.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Let ξ ⋆ denote a solution of Eq. (4). Optimality conditions of Eq. (4) derived in<ref type="bibr" target="#b11">[12]</ref> show that for all j in [1; p], the signs of the non-zero coefficients ξ ⋆g j for g in G are the same as the signs of the entries uj. To solve Eq. (4), one can therefore flip the signs of the negative variables uj, then solve the modified dual formulation (with non-negative variables), which gives the magnitude of the entries ξ ⋆g j (the signs of these being known).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>When restricted to the case where Ω is a sum of ℓ∞-norms, the approach of<ref type="bibr" target="#b11">[12]</ref> is in fact similar to<ref type="bibr" target="#b17">[18]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>By definition, a parametric max-flow problem consists in solving, for every value of a parameter, a maxflow problem on a graph whose arc capacities depend on this parameter.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>In our simulations, we use the commercial software Mosek, http://www.mosek.com/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>Due to the computational burden, QP and CP could not be run on every problem.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7"><p>http://research.microsoft.com/en-us/um/people/jckrumm/wallflower/testimages.htm</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8"><p>simplified case where Ωtree and Ωjoint are the ℓ1and mixed ℓ1/ℓ2-norms<ref type="bibr" target="#b12">[13]</ref> corresponds to<ref type="bibr" target="#b30">[30]</ref>.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This paper was partially supported by the European Research Council (SIERRA Project). The authors would like to thank Jean Ponce for interesting discussions and suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Simultaneous analysis of Lasso and Dantzig selector</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ritov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tsybakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Stat</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1705" to="1732" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Least angle regression</title>
		<author>
			<persName><forename type="first">B</forename><surname>Efron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Johnstone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Stat</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="407" to="499" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Gradient methods for minimizing composite objective function</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Nesterov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
		<respStmt>
			<orgName>Center for Operations Research and Econometrics (CORE), Catholic University of Louvain</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A fast iterative shrinkage-thresholding algorithm for linear inverse problems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Teboulle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Imag. Sci</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="183" to="202" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Structured variable selection with sparsity-inducing norms</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J-Y</forename><surname>Audibert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:0904.3523v1</idno>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Group Lasso with overlap and graph Lasso</title>
		<author>
			<persName><forename type="first">L</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Obozinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-P</forename><surname>Vert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The composite absolute penalties family for grouped and hierarchical variable selection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rocha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Stat</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6A</biblScope>
			<biblScope unit="page" from="3468" to="3497" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Model-based compressive sensing</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Baraniuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Cevher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Duarte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hegde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T. Inform. Theory</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">signal recovery using markov random fields</title>
		<author>
			<persName><forename type="first">V</forename><surname>Cehver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Duarte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hedge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Baraniuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adv. NIPS</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning with structured sparsity</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Tree-guided group lasso for multi-task regression with structured sparsity</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Proximal methods for sparse hierarchical dictionary learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Obozinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Model selection and estimation in regression with grouped variables</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Roy. Stat. Soc. B</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="49" to="67" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Joint covariate selection and joint subspace selection for multiple classification problems</title>
		<author>
			<persName><forename type="first">G</forename><surname>Obozinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Comput</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="231" to="252" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Exploring large feature spaces with hierarchical multiple kernel learning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adv. NIPS</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Proximal splitting methods in signal processing</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Combettes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-C</forename><surname>Pesquet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fixed-Point for Inverse Problems in Science and Engineering</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Network flow algorithms for structured sparsity</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Obozinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1008.5209v1</idno>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">About strongly polynomial time algorithms for quadratic optimization over submodular constraints</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Hochbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Program</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="269" to="309" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An O(n) algorithm for quadratic knapsack problems</title>
		<author>
			<persName><forename type="first">P</forename><surname>Brucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Oper. Res. Lett</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="163" to="166" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A fast parametric maximum flow algorithm and applications</title>
		<author>
			<persName><forename type="first">G</forename><surname>Gallo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Grigoriadis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Tarjan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Comput</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="30" to="55" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Experimental evaluation of a parametric flow algorithm</title>
		<author>
			<persName><forename type="first">M</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Goldberg</surname></persName>
		</author>
		<idno>MSR-TR-2006-77</idno>
	</analytic>
	<monogr>
		<title level="j">Technical Microsoft Research</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Two algorithms for maximizing a separable concave function over a polymatroid feasible region</title>
		<author>
			<persName><forename type="first">H</forename><surname>Groenevelt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. J. Oper. Res</title>
		<imprint>
			<biblScope unit="page" from="227" to="236" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Structured sparsity-inducing norms through submodular functions</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adv. NIPS</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A new approach to the maximum flow problem</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Tarjan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM Symposium on Theory of Computing</title>
		<meeting>of ACM Symposium on Theory of Computing</meeting>
		<imprint>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Maximal flow through a network</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Ford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Fulkerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Canadian J. Math</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1956">1956</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On implementing the pushrelabel method for the maximum flow problem</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">V</forename><surname>Cherkassky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Algorithmica</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="390" to="410" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">A unified framework for high-dimensional analysis of M-estimators with decomposable regularizers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Negahban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Convex analysis and nonlinear optimization</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Borwein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Lewis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Robust face recognition via sparse representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">T</forename><surname>Ieee</surname></persName>
		</author>
		<author>
			<persName><surname>Pattern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Anal</title>
		<imprint>
			<biblScope unit="page" from="210" to="227" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Collaborative hierarchical sparse modeling</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sprechmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ramirez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Eldar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1003.0400v1</idno>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
