<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Perceptron-Based Prefetch Filtering</title>
				<funder ref="#_6YKsxyX">
					<orgName type="full">Texas A&amp;M High Performance Research Computing</orgName>
				</funder>
				<funder ref="#_CZJ4MuQ">
					<orgName type="full">National Science Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Eshan</forename><surname>Bhatia</surname></persName>
							<email>eshanbhatia22@tamu.edu</email>
						</author>
						<author>
							<persName><forename type="first">Gino</forename><surname>Chacon</surname></persName>
							<email>ginochacon@tamu.edu</email>
						</author>
						<author>
							<persName><forename type="first">Elvira</forename><surname>Teran</surname></persName>
							<email>elvira.teran@tamiu.edu</email>
						</author>
						<author>
							<persName><forename type="first">Seth</forename><surname>Pugsley</surname></persName>
							<email>seth.h.pugsley@intel.com</email>
						</author>
						<author>
							<persName><forename type="first">Paul</forename><forename type="middle">V</forename><surname>Gratz</surname></persName>
							<email>pgratz@gratz1.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Texas A&amp;M University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Texas A&amp;M University Seth Pugsley</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Intel Labs</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Texas A&amp;M International University Paul V. Gratz</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Texas A&amp;M University Daniel A. Jim?nez</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Texas A&amp;M University</orgName>
								<address>
									<settlement>Barcelona Supercomputing Center</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="department">ISCA &apos;19</orgName>
								<address>
									<addrLine>June 22-26</addrLine>
									<postCode>2019</postCode>
									<settlement>Phoenix</settlement>
									<region>AZ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Perceptron-Based Prefetch Filtering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3307650.3322207</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Hardware prefetching is an effective technique for hiding cache miss latencies in modern processor designs. Prefetcher performance can be characterized by two main metrics that are generally at odds with one another: coverage, the fraction of baseline cache misses which the prefetcher brings into the cache; and accuracy, the fraction of prefetches which are ultimately used. An overly aggressive prefetcher may improve coverage at the cost of reduced accuracy. Thus, performance may be harmed by this over-aggressiveness because many resources are wasted, including cache capacity and bandwidth. An ideal prefetcher would have both high coverage and accuracy.</p><p>In this paper, we introduce Perceptron-based Prefetch Filtering (PPF) as a way to increase the coverage of the prefetches generated by an underlying prefetcher without negatively impacting accuracy. PPF enables more aggressive tuning of the underlying prefetcher, leading to increased coverage by filtering out the growing numbers of inaccurate prefetches such an aggressive tuning implies. We also explore a range of features to use to train PPF's perceptron layer to identify inaccurate prefetches. PPF improves performance on a memory-intensive subset of the SPEC CPU 2017 benchmarks by 3.78% for a single-core configuration, and by 11.4% for a 4-core configuration, compared to the underlying prefetcher alone.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The impact of aggressive prefetching on performance for 603.bwaves_s. The number of useful prefetches increases with aggressiveness slower than total prefetches, which wastes bandwidth and harms performance.</p><p>Memory Wall <ref type="bibr" target="#b0">[1]</ref> -the increasing gap between processor and memory performance. Data prefetching is one important technique that has been developed to minimize the effects of this trend.</p><p>An ideal prefetching scheme would perfectly capture a program's memory access pattern, and then predict and pre-load the needed data into the processor's caches in a timely manner. Memory access patterns may be simple, such as accessing every item in an array with a for-loop, or very complex, such as chasing pointers through dynamically-allocated memory. All prefetchers are designed around a fundamental trade-off between two important metrics: coverage and accuracy. Prefetcher coverage refers to the fraction of baseline cache misses that the prefetcher pulls into the cache prior to their reference. For example, if an application experiences 1,000 cache misses without a prefetcher, while 800 of those cache misses become hits with a prefetcher, then the prefetcher has 80% coverage for that application. Prefetcher accuracy refers to the fraction of prefetched cache lines that end up being used by the application. So if a prefetcher prefetches 1,200 cache lines, but only 800 of them are used by the application, then that prefetcher's accuracy is 66.7%.</p><p>Coverage and accuracy are generally at odds with one another, and as one metric improves, the other usually gets worse. For example, when an application accesses a new region of memory for the first time, a na?ve prefetcher may predict that all data in that region will be used by the application. This will clearly result in 100% coverage for that region, but with possibly a very low accuracy. In fact, so much cache capacity and bandwidth may be wasted prefetching unused data that performance can ultimately be harmed by this strategy. At the other extreme, another prefetcher may be overly conservative and never prefetch anything, wasting no capacity or bandwidth, and achieving 0% prefetch coverage.</p><p>Figure <ref type="figure">1</ref> illustrates the above scenario. Here we consider a stateof-the-art lookahead prefetcher -SPP <ref type="bibr" target="#b1">[2]</ref>. Lookahead prefetchers such as SPP provide a mechanism to speculate an arbitrary number of references ahead of the initial triggering access. In SPP, a throttling confidence threshold is then used to ensure that the lookahead stops when confidence falls too low to ensure that prefetches are accurate. In the figure, we iteratively re-tuned this threshold to allow the prefetcher to lookahead a fixed depth from 7 to 15. The figure depicts the behavior of the 603.bwaves_s SPEC CPU 2017 benchmark. The IPC, the total number of prefetches issued by the prefetcher (TOTAL_PF), and the actual useful predictions (GOOD_PF), all have been normalized to lookahead depth 7. As the lookahead depth increases, so do useful prefetches, and hence coverage. This coverage, however, comes at the cost of total prefetches increasing at an even higher rate. This leads to cache pollution and bandwidth contention, and leads to a reduction in IPC.</p><p>Therefore, a delicate balance between coverage and accuracy is required for a prefetcher to maximize its performance impact. Prefetchers are generally designed with internal mechanisms to monitor their accuracy, and throttling mechanisms that can be tuned for either coverage or accuracy. The more irregular an application's memory access pattern is, the more difficult it is to accurately predict every access, so a prefetcher will have to be tuned more toward coverage (and away from accuracy) in order to gain any benefit. This may be especially dangerous to do in the context of a multicore processor, where overly aggressive prefetching in one core can waste shared resources, such as last-level cache (LLC) capacity, and off-chip bandwidth, impacting the performance of other cores <ref type="bibr" target="#b2">[3]</ref>.</p><p>Here, we propose Perceptron-based Prefetch Filtering (PPF) as an enhancement to existing state-of-the-art prefetchers, allowing them to speculate deeply to achieve high coverage while filtering out the inaccurate prefetches this deep speculation implies. PPF works by observing the stream of candidate prefetches generated by a prefetcher, and then rejects those that are predicted by the online-trained neural model to be inaccurate. The state-of-the-art prefetcher that we use to evaluate PPF in this paper is the Signature Path Prefetcher (SPP) <ref type="bibr" target="#b1">[2]</ref>, however as we describe, PPF can be designed to benefit any prefetcher. In this design, PPF replaces SPP's existing confidence-based throttling mechanism, which itself was a highly tuned feature of that prefetcher. Because PPF is so much more effective at rejecting inaccurate prefetches than SPP's internal mechanism, we are free to re-tune the rest of SPP's design around maximizing coverage. The result is an increase in both accuracy and coverage, and a notable increase in performance. This paper describes PPF, explains its merits, offers analysis, and outlines the scope for future research. Its contributions are:</p><p>? An on-line neural model used for hardware data prefetching.</p><p>Previous work in this area either relied on program semantics <ref type="bibr" target="#b3">[4]</ref> or were application specific <ref type="bibr" target="#b4">[5]</ref>. ? Implementing PPF filtering a state-of-the-art prefetcher, giving a significant performance improvement compared to previous work. PPF learns to adapt itself to shared resource constraints, leading to further increased performance in multicore and bandwidth-constrained environments. ? A methodology for determining an appropriate set of features for prediction, regardless of the underlying prefetcher used. More details are explained in Section 5.5. In a single core configuration, PPF increases performance by 3.78% compared to the underlying prefetcher, SPP. In a multi-core system running a mixes of memory intensive SPEC CPU 2017 traces, PPF saw an improvement of 11.4% over SPP for a 4-core system, and 9.65% for an 8-core system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">MOTIVATION</head><p>In this section we discuss the most closely related work to our proposed technique. The idea of prefetching begins with Jouppi's Instruction Stream Buffers <ref type="bibr" target="#b5">[6]</ref>. Early prefetchers detected stride access patterns in order to predict future memory requests <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref>. Modern prefetching mechanisms are more sophisticated as they look into past memory behavior <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>, locality <ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref>, controlflow speculation <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>, and other other aspects to detect complex memory access patterns. See Section 7 for other relevant work.  <ref type="bibr" target="#b1">[2]</ref>, a confidencebased lookahead prefetcher. SPP creates a signature associated with a page address by compressing the history of accesses. By correlating the signature with future likely delta patterns, SPP learns both simple and complicated memory access patterns quickly. While the basic idea of perceptron based prefetch filtering is applicable to any lookahead prefetcher, we develop a practical implementation of our proposed prefetch filter using SPP as our underlying mechanism. Here we describe the basic architecture of SPP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Underlying Prefetcher: SPP</head><p>Signature Table : As shown on the left side of Figure <ref type="figure" target="#fig_2">2</ref>, the Signature Table (ST) keeps track of 256 most recently accessed pages. It is meant to capture memory access patterns within a page boundary. SPP indexes into an entry of the Signature Table using the page number. For each entry corresponding to a page, the table stores a 'last block offset' and an 'old signature'. Last block offset is the block offset of the last memory access of that given page. The block offset is calculated with respect to the page boundary. The signature is a 12-bit compressed representation of the past few memory accesses for that page. The signature is calculated as: NewSignature = OldSignature &lt;&lt; 3bits XOR Delta Delta is the numerical difference between the block offset of the current and the previous memory access. In case a matching page entry is found, the stored signature retrieved and used to index into the Pattern Table. This process is illustrated in Figure <ref type="figure" target="#fig_2">2</ref>.   : The Pattern Table (PT), shown on the right side in Figure <ref type="figure" target="#fig_2">2</ref> is indexed by the signature generated from the Signature Table . Pattern Table <ref type="table">holds</ref> predicted delta patterns and their confidence estimates. Each entry indexed by the signature holds up to 4 unique delta predictions.</p><p>Lookahead Prefetching: On each trigger, SPP goes down the program speculation path using its own prefetch suggestion. Using the current prefetch as a starting point, it re-accesses the Pattern Table to generate further prefetches. As illustrated in Figure <ref type="figure" target="#fig_4">3</ref>, it repeats the cycle of accessing the PT and updating the signature based on highest confidence prefetch from the last iteration. The iteration counter on which SPP manages to predict prefetch entries in the lookahead manner is characterized as its 'depth'. While doing so, SPP also keeps compounding the confidence in each depth. Thus as depth increases, overall confidence keeps decreasing.</p><p>Confidence Tracking: As shown in Figure <ref type="figure" target="#fig_4">3</ref>, the Pattern Table keeps a count of hits to each signature through a counter C sig . The number of hits for a given delta per signature are tracked using a counter C delta . The confidence for a given delta is approximated through C d = C delta / C sig . When SPP enters into a lookahead mode, the path confidence P d is:</p><formula xml:id="formula_0">P d = ? . C d . P d-1</formula><p>Here ? represents the global accuracy, calculated as the ratio of the number of prefetches which led to a demand hit to the number of prefetches recommended in total. The range of ? is [0,1]. The lookahead depth is represented by d. For d = 1, when SPP is in non-speculative mode, P 0 can be thought of as 1. The final P d is thresholded against prefetch threshold (T p ) to reject the low confidence suggestions and then against a numerically bigger fill threshold (T f ) to decide whether to send the prefetch to L2 Cache (high confidence prefetch) or Last Level Cache (low confidence prefetch). The two thresholds were empirically set to 25 and 90 respectively, on the scale of 0 to 100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Case for an On-line Filter</head><p>As was noted in Figure <ref type="figure">1</ref>, aggressive lookahead prefetching, if done without any accuracy check, can harm the performance of the system. As the figure shows, aggressive lookahead and its accompanied loss of accuracy degrades performance by almost 9%. This is despite a growing number of useful prefetches generated by the prefetcher. Thus, we need a mechanism that is orthogonal to the  underlying prefetching scheme and can be used to prune out the useful prefetches from the useless ones. Moreover, the on-line confidence mechanism used by most prefetchers is very rudimentary. For example, SPP's internal confidence mechanism is based on taking the ratio C d = C delta / C sig . This confidence was used to make the decision of whether to prefetch or not to prefetch; and which level to prefetch. While this approximation was shown to work in the original implementation, we believe that a better form of generalized on-line decision making is possible. Hence, it was necessary to build a robust and adaptable learning mechanism to accept / reject the prefetch suggestions, and to decide the fill level (L2 Cache vs Last Level Cache). Thus, we introduce an independent on-line perceptron based filtering mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Perceptron Learning</head><p>Perceptron learning for microarchitectural prediction was introduced for branch prediction <ref type="bibr" target="#b19">[20]</ref>. Our predictor uses a version of microarchitectural perceptron prediction known as the "hashed perceptron" organization <ref type="bibr" target="#b20">[21]</ref>. As an abstract idea, a hashed perceptron predictor hashes several different features into values that index several distinct tables. Small integer weights are read out from the tables and summed. If the sum exceeds some threshold, a positive prediction is made, e.g. "predict branch taken" or "allow the prefetch." Otherwise, a negative prediction is made. Once the ground truth is known, the weights corresponding to the prediction are incremented if the outcome was positive, or decremented if it was negative. This update only occurs if the prediction was incorrect or if the magnitude of the sum failed to exceed a threshold. Beyond branch prediction, perceptron learning has been applied to last-level cache reuse prediction <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>. In this paper, we apply it for the first time to do prefetch filtering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PPF DESIGN AND ARCHITECTURE</head><p>It can be beneficial to allow a prefetcher to speculate as deeply as possible. Often, some useful prefetches are generated long after the confidence of the prefetcher has fallen below the point at which performance degrades due to the increase of inaccurate prefetches. In order to allow deep speculation in the prefetcher, however, inaccurate prefetches must be filtered out. We propose to leverage perceptronbased learning as a mechanism to differentiate between potentially useful deeply speculated prefetches and likely not-useful ones. The Perceptron Prefetch Filter (PPF) is placed between the prefetcher and the prefetch insertion queue, as illustrated in Figure <ref type="figure" target="#fig_5">4</ref>, to prevent  not-useful prefetches from polluting the higher levels of the memory hierarchy.</p><p>Perceptron learning is a light-weight mechanism to pull together disparate forms of information and synthesize a decision from them. Our work considers a number of features corresponding to a prefetch, such as speculation depth, page address and offset, and uses this information as the inputs to our perceptron-based filter in order to predict the usefulness of a prefetch. Here, we discuss our the design of our proposed perceptron prefetch filter (PPF). PPF enhances an underlying prefetcher by filtering out predicted unused prefetches. PPF is a generalized prefetch filtering mechanism that may be adapted to any prefetcher with appropriate feature selection and modifications which we describe below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Perceptron Filter</head><p>Figure <ref type="figure" target="#fig_6">5</ref> shows the microarchitecture of PPF, as well as the steps required to filter out not-useful prefetches. The perceptron filter is organized as a set of tables, where each entry in the tables holds a weight. For a configuration of PPF using N number of features, N different tables of weights are needed. Each feature is used to index a distinct table. The number of entries of each table varies according to the corresponding feature, hence, different number of bits are needed to index different tables. Each weight is a 5-bit saturating counter ranging from -16 to +15. We found that having 5-bit weights provides a good trade-off between accuracy and area. A detailed explanation of the storage overhead of PPF can be found in Section 5.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inferencing</head><p>The prefetcher is triggered on every demand access to the L2 Cache, as seen in Figure <ref type="figure" target="#fig_5">4</ref>. At this point, it has the opportunity to trigger a prefetch. If it does so, it will also need to decide how many cache blocks to prefetch. These blocks can be either placed in the L2 or L3 cache according to the confidence of the prefetching mechanism. Once the underlying prefetcher is triggered, the suggested prefetch candidates are fed to the perceptron filter to determine the usefulness of these prefetches. The filter ultimately decides whether to issue the prefetch suggestions of the underlying prefetcher. As shown in step 1 Reject Table <ref type="table">Index</ref>  FEAT 2</p><formula xml:id="formula_1">W 12 W 22 W J2 FEAT N W 1N W 2N W 3N W KN</formula><p>Re-access same weights of Figure <ref type="figure" target="#fig_6">5</ref>(a), to make the decision, each feature corresponding to a suggested prefetch is used to index a table and all the corresponding weights are summed. The sum denotes the confidence value for the suggested prefetch, and is thresholded against two different values: ? hi and ? lo . Prefetches whose sum exceeds ? hi are placed into the L2 cache. The higher confidence value hints the prefetch would be useful and should be prioritized. A prefetch for which the features result in a confidence value between ? lo and ? hi is allocated in the larger LLC, as the filter is moderately confident of the future reuse of the cache block, but not enough to possible pollute a significantly smaller L2. Suggested prefetches for which the features lead to a confidence value lower than ? lo are not prefetched, as the low confidence value represents that the perceptron learned that a similar set of features are associated with non-useful prefetches.</p><formula xml:id="formula_2">(a) Prediction (b) Update Reject Table Index 1 ?.. Index N Prefetch Table Index 1 ?.. Index N</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recording</head><p>As shown in step 2 of Figure <ref type="figure" target="#fig_6">5</ref>(a) the prefetches that make it through the inference stage are recorded in the "Prefetch Table ". The prefetch table is a 1,024-entry, direct mapped structure that contains all metadata required to re-index the perceptron entries for training. Ten bits of the address are used to index into the tables, and another six bits are stored to perform tag matching.</p><p>In addition to the prefetch table mentioned above, PPF also maintains a 1,024-entry direct-mapped "Reject Table ." If a prefetch suggestion is rejected by the perceptron layer, it is logged into the reject table. The table is used to train the perceptron to avoid false negatives i.e., cases where the prediction suggested to reject the prefetch but the prefetch was proven to be useful based on the observed demand accesses to the L2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feedback and Data Retrieval</head><p>As depicted in step 3, when there is an eviction or a demand access to the L2, training for both the underlying prefetcher and our filter mechanism is triggered. The address of the cache block that triggered the training is used to index both the Prefetch and Reject tables. If it is a match, the corresponding features are retrieved to index into the tables of perceptron weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training</head><p>As can be seen in step 4 of, the address from the demand request triggering the training is looked up in both tables. If the address is in the prefetch table and marked as valid, this hints the previous prediction was correct and this is a useful prefetch. We compute the sum of the corresponding weights. If the sum falls below a specific threshold, training occurs and the corresponding weights are adjusted accordingly. These thresholds are introduced in order to avoid over-training, helping the filter adapt quickly to changes in memory behavior. These thresholds are referred to as ? p and ? n , respectively for the positive and negative values of training saturation.</p><p>On a cache block eviction, we look up the corresponding address in the prefetch table. If there is a valid entry with this address, the filter made a misprediction. The block was allocated in the L2 with a prefetch request that the filter should have categorized as a useless prefetch. Thus, the corresponding features of the prefetch request are used to re-index the tables of weights, and those weights are adjusted accordingly.</p><p>Parallel to accessing the prefetch table, on a demand access, the reject table is accessed. Before the demand access triggers the next set of prefetches, the reject table is checked for a valid entry. A hit means that the corresponding cache block was initially suggested by the underlying prefetcher, but wrongly rejected by the perceptron filter. The perceptron filter learns from this and makes use of the corresponding features associated to the original prefetch request, which are stored in the reject table, to index the weights tables and adjust the weights accordingly. The implementation of the reject table, allows us to capture the information of prefetches that were rejected, and that can be used to further optimize our prefetching mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Optimizing PPF for a Given Prefetcher</head><p>The above discussion of PPF shows that it is highly modular and can be adapted to be used over any underlying prefetcher for increased prefetch accuracy. As a first step, all the prefetch candidates of the prefetcher have to pass through the perceptron filter. If qualified, the metadata for perceptron indexing has to be stored. Next, when the feedback of a prior prefetch is available in form of a subsequent demand hit or cache eviction, the stored metadata needs to be retrieved to update the state of the perceptrons.</p><p>In general, PPF can be adapted to a new prefetcher with only a few modifications:</p><p>Making the Underlying Prefetcher More Aggressive: By Tuning down any internal thresholds or throttling mechanisms to increase its aggressiveness.</p><p>Inferencing and Storing: All prefetch recommendations are tested using the perceptron inferencing algorithm. The perceptron's output, true or false, should be saved appropriately, along with all metadata required for perceptron indexing.</p><p>Retrieving and Training: When feedback for a prefetch becomes available, the previously stored metadata can be used to re-index into the perceptron entries and increment or decrement the weights.</p><p>Feature Selection: Perceptrons essentially integrate contributions from different features to get a single sum representing the final confidence. Thus, perceptron learning can only be as good as the set of features chosen. Interestingly, this is what makes perceptron learning scalable, as it can easily learn to incorporate newer information in the form of new features. Some of the features we developed use information derived directly from program execution, agnostic to the underlying prefetcher. Beyond that, the feature set can be expanded to convey any useful information or metadata available in the underlying prefetcher itself.</p><p>Using Metadata from the Prefetcher: Some of the internal counters specific to the underlying prefetcher can be suitable candidates for the perceptron features. To make sure that the perceptron layer sees that, the relevant metadata must be exported from the prefetcher to PPF. This way, PPF can be optimized to work tightly-knit with the underlying prefetcher.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">PPF IMPLEMENTATION USING SPP</head><p>This section describes a case study implementation of PPF and the range of features that are used to determine the usefulness of prefetches. Here, we have selected SPP as the underlying prefetcher.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Changes Made to SPP</head><p>To modify the SPP design to suit our scheme, the following changes were made:</p><p>Exporting Features from SPP: PPF uses the metadata specific to SPP, to build some of the perceptron features. These include the lookahead depth, signature and the confidence counter. These features were made visible to PPF.</p><p>Original Thresholds Discarded: In PPF, the perceptron sum is used to decide whether to prefetch or not, and the fill-level in case of prefetch. Thus, the confidence thresholds used by SPP -T f and T p are no longer needed to throttle the prefetcher and can be discarded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Features used by Perceptron</head><p>Here we discuss the various features that correlate the prefetching decision with the program behavior. All the features we used can be derived from the information available in the L2 Cache access stream or are taken as metadata derived from the underlying prefetcher. Our feature selection involved searching over a large space of relevant perceptron features. Note that part of the process of tuning PPF to a specific prefetcher involves examining the available metadata in the prefetcher itself, and thus PPF is attuned to the underlying prefetcher's design. Using the statistical methodology outlined in Section 5.5, we pruned the feature set to a minimal yet relevant set of features.</p><p>Physical Address: Here we use the lower bits of the physical address of the demand access that triggers the prefetch. This address corresponds to a stream of accesses that SPP and the PPF have seen before. Therefore, PPF will correlate the past behavior of this address to the prefetch outcome.</p><p>Cache Line and Page Address: These two separate features are derived from shifting the base address that triggered the prefetch by the size of the cache blocks or by the size of a page. The idea behind using three different shifted versions of the same feature is that it allows the filter to focus its examination in more detail on different aspects of the address than with a single version. It also helps give more importance to the overlapping bits and lesser importance to most and least significance bits. This approach can also eliminate destructive interference that can be caused by directly folding the address bits into half.</p><p>Program Counter XOR Depth: The PC is for the instruction that triggered the prefetch chain. Depth refers to the iteration count of the lookahead stages. By itself, we find the PC to not be a good basis for filtering a lookahead prefetcher, as all the prefetches with depth &gt;= 1 are aliased into the same PC, which will not be the PC of the eventual actual demand access. This feature resolves a PC into a different value for each lookahead depth of prefetch speculation, giving a more accurate correlation in lookahead cases. This is akin to the concept of Virtual Program Counters <ref type="bibr" target="#b23">[24]</ref> introduced by Kim et. al. for indirect branch prediction. PC 1 XOR PC 2 ?1 XOR PC 3 ?2: Here PC i refers to the last i th PC before the instruction that triggered the current prefetch. Hashing together the last three PCs informs PPF about the path that led to the current demand access and helps capture and branching information of the current basic block. PCs are shifted in the increasing order of history before being hashed together. This is done to avoid the resultant value of zero when 2 or more PCs are the same. Additionally, blurring the information as it gets older allows us to get a wider and yet more approximate look into the program's history.</p><p>Program Counter XOR Delta: This feature tells us if a given PC favors particular value(s) of delta. As noted earlier, while the PC alone does not convey useful information, this hash resolves the PC into different values based on the tendency of that PC to favor a certain delta. Thus, the dynamic nature of different instances of the same memory access instruction is captured here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Confidence:</head><p>The confidence, on a scale of 0 to 100, used to throttle lookahead depth in the original SPP design. While the original confidence does not directly make the decision to prefetch, PPF  <ref type="table">1</ref>: Simulation Parameters correlates it to the correctness of a proposed prefetch. While the original SPP may have dismissed a prefetch due to running further into speculation, PPF can use the original confidence as indicator of not only when prefetches become less confident, but also how likely a low confident speculation is correct in the context of other features.</p><p>Page Address XOR Confidence: This feature scores the tendency of each page to be prefetch friendly or prefetch averse. It helps resolve a page into different entries depending on its confidence for prefetching, which can vary during phases of a program execution.</p><p>Current Signature XOR Delta: Recall from the discussion of SPP in Section 2.1 that the new signature is generated using the old signature and the current block delta. The result of this feature is the next signature that is predicted to be accessed based on the delta predicted by SPP. While "Current Signature XOR Delta" is not the exact formula for generating the future signature, it gives an approximate idea of the path that the combination of these two values can lead to.</p><p>As can be noted above, some composite features are derived from simple hashing (XOR) of two primary features. There is always a question of usefulness of such composite features and the new information added. We justify the choice of each feature by quantifying the contribution made towards predicting prefetch behavior, in Section 5.5. Finally, as noted above, each feature indexes into its independent entry of perceptron weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">METHODOLOGY 5.1 Performance Model</head><p>We use the ChampSim <ref type="bibr" target="#b24">[25]</ref> simulator for the evaluation of PPF against prior work techniques. ChampSim is an enhanced version of the framework that was used for the 2nd Data Prefetch Championship (DPC-2) <ref type="bibr" target="#b25">[26]</ref>, also used in the 2nd Cache Replacement Competition (CRC2) <ref type="bibr" target="#b26">[27]</ref>. We model 1-core, 4-core, and 8-core out-of-order machines. The details of the configuration parameters are summarized in Table <ref type="table">1</ref>.</p><p>The block size is fixed at 64 bytes. Prefetching is only triggered upon L2 cache demand accesses but could be directed to the L2 or last-level cache. No L1 data level prefetching is done. The LRU replacement policy is used on all levels of cache hierarchies. Branch prediction is done using the perceptron branch predictor <ref type="bibr" target="#b19">[20]</ref>. The page size is 4KB. ChampSim operates all the prefetchers strictly in the physical address space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Testing Under Additional Memory Constraints</head><p>The default single-core configuration simulates a 2MB LLC and a single channel DRAM with 12.8GB/s bandwidth. We extend the simulations to include memory constraints introduced in DPC-2.</p><p>Specifically we look at the following two variations: Low Bandwidth DRAM, where DRAM bandwidth is limited to 3.2 GB/s, and small LLC, where the LLC size is reduced to 512 KB. All the multi-core simulations are only done in the default configuration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Workloads</head><p>We use all the 20 workloads available in the SPEC CPU 2017 suite <ref type="bibr" target="#b27">[28]</ref>. Using the SimPoint <ref type="bibr" target="#b28">[29]</ref> methodology, we identified 95 different program segments of 1 Billion instructions each.</p><p>Single-core performance: For single-core simulations, we use the first 200 million instructions to warm-up the microarchitectural structures and the next one billion instructions to do detailed simulations and collect run-time statistics. We report the IPC speedup over the baseline of no prefetching. The final numbers reported are the geometric mean of the weighted mean speedup achieved per application using the SimPoint methodology.</p><p>Multi-core performance: For multi-application workloads, we generate 100 random mixes and another 100 mixes from the memory intensive subset of SPEC CPU 2017. For 4-core workloads, 200 Million instructions are used for warm-up and additional 1 Billion instruction simulated for collecting statistics. Each CPU keeps executing its workload till the last CPU completes one billion instructions after warm-up. For collecting IPC and other data, only the first billion instructions are considered as the region of interest.</p><p>Here we report the weighted speedup normalized to baseline i.e., no prefetching. For each of the workloads running on a particular core of the 4-core 8 MB LLC system, we compute IPC i . We then find the IPC_isolated i of the same workload running in isolated 1-core 8 MB LLC environment. Then we calculate the total weighted-IPC for a given workload mix as ? (IPC i / IPC_isolated i ). For each of the 100 workload-mix, the sum obtained is normalized to the weighted-IPC calculated similarly for baseline case i.e., no prefetching, to get the weighted-IPC-speedup. Finally the geometric mean of these 100 weighted-IPC-speedup is reported as the effective speedup obtained by the prefetching scheme.</p><p>We repeat the same process for 8-core workloads, correspondingly with 16MB LLC. The only difference is that 20 million warm-up instructions and 100 million full instructions are executed. This is done so as to keep the simulation run-time within reasonable limits as a single 8-core mix takes up to 3 days to simulate one billion instructions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Validation:</head><p>We cross-validated our PPF model using SPEC CPU 2006 <ref type="bibr" target="#b29">[30]</ref> and CloudSuite <ref type="bibr" target="#b30">[31]</ref> benchmarks. For single-core SPEC CPU 2006, we developed 94 simpoints spread across all the 29 applications. For multi-core, we followed the same methodology as SPEC CPU 2017. For CloudSuite, we used the traces made available for the 2nd Cache Replacement Competition (CRC-2) <ref type="bibr" target="#b26">[27]</ref>. The traces include four 4-core applications with six distinct phases per application.</p><p>In total, we used 285 traces representing workloads across 53 applications. Throughout the paper, we consider memory intensive subset as the applications with SimPoint weighted LLC MPKI &gt; 1. This includes 11 out of 20 SPEC CPU 2017 applications. For SPEC CPU 2006, this includes 16 out of 29 applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Prefetchers Simulated</head><p>We compared PPF against three of the latest, state of the art hardwareonly prefetchers: Best Offset Prefetcher (BOP), DRAM Aware -Access Map Pattern Matching (DA-AMPM) <ref type="bibr" target="#b31">[32]</ref> and Signature Path Prefetcher (SPP). BOP was the winner of 2nd Data Prefetching Championship. DA-AMPM is the enhanced version of AMPM, modified to account for DRAM row buffer locality. SPP has been shown to outperform BOP on SPEC CPU 2006 traces <ref type="bibr" target="#b1">[2]</ref>. For each of these, we compare their speedups taking the no prefetching case as the baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Developing Features for PPF</head><p>This section describes the intuition and analysis that went behind developing the perceptron features. As noted earlier, we developed a set of nine features that allow the perceptron layer to correlate prefetching decision with the program behavior. To study the correlation across each feature, we statistically examine the perceptron weights and try to interpret their distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Global Pearson's Correlation:</head><p>Here we examine the perceptron weights at the end of all trace execution by which time the weights have settled to steady values. The weights obtained from running all the SPEC CPU 2017 traces are concatenated. Features with a bulk of their perceptron weights concentrated around 0 or small magnitude numbers show a weak correlation with the prefetching outcome. On the other hand, features with most of the weights saturated around highest value (+15) show a high positive correlation and the features with weights close to the lowest value (-16) show a strong negative correlation.</p><p>We plot a histogram for each feature depicting weights distribution from -16 to +15 and generate the Pearson's correlation factor for that feature. Pearson's factor is a numerical measure ranging from -1 to 1 of the degree of linear correlation between two variables. The magnitude of Pearson's factor gives the extent of correlation and the sign indicates whether it is a positive correlation or a negative correlation. Values close to 0 suggest a low correlation while a value of +1/-1 suggests a perfectly linear positive / negative correlation respectively.</p><p>As a part of our perceptron feature selection methodology, we explored a wide variety of features to begin with. Features with a low Pearson's coefficient were rejected as they didn't provide much useful correlation. Figure <ref type="figure">6</ref> depicts the histogram distribution of trained weights for two features. The first feature, Confidence XOR Page address has the highest observed P-value, hence was retained. On the other hand, the second feature, Last Signature did not provide any meaningful correlation and hence was rejected. This is visible from the bulk of its trained weights settling to near zero values.</p><p>Figure <ref type="figure">7</ref> shows all the features which are finally used, arranged in the increasing order of their Pearson's factor. As can be seen 5 out of the 9 features provide a moderate to high correlation, with the magnitude of P-value &gt; 0.6. The single most important feature, Confidence XOR Page address helps provide a correlation to prefetch outcome with a factor of 0.90.</p><p>Per Trace Correlation: Another important way to look at the perceptron features is to see how much their contribution varies across the traces. Here we give special attention to features with low Pvalues in. Figure <ref type="figure" target="#fig_9">8</ref> shows the variation of P-values for three features : PC XOR Delta, Signature XOR Delta and PC XOR Depth; across all the SPEC CPU 2017 traces. For simplicity, the traces are arranged in an increasing order of contribution made by the feature. It can be seen that even features with a low overall correlation provide useful correlation (magnitude &gt; 0.5) for a significant number of traces. This study motivated us to choose PC XOR DELTA over Last Signature as it provided useful correlation in at least some of the traces.  As we examined correlation of each feature with the final outcome, we also studied correlation between the features. We used the above methodology to eliminate features providing little information that has already been captured in other features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Trimming Features Using</head><p>We initially came up with a set of 23 features. By studying cross correlation of each of these features against others in a 23?23 matrix, we identified pairs of features with correlation factor &gt; 0.9 in magnitude and eliminated redundant features, using guidance from Global and per-trace Pearson's factor of those features. By doing this, we reduced the feature count to 9. Thus, in the final implementation of PPF, no two features have a high correlation between them. This way we can be sure that each feature makes a contribution that cannot be captured using other features.</p><p>Secondly, studying the relative importance of each feature enabled us to vary the number of entries dedicated for each feature. Features with higher correlation, cache line and page address were given most importance and allowed full 12-bits of indexing. Features like PC XOR delta and PC XOR depth with a low overall P-value were allocated fewer entries in the feature table.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Overhead for PPF</head><p>In this section, we analyze the hardware overhead required to implement PPF. The Prefetch  In terms of computations, the perceptron mechanism only introduces an extra adder tree. The hash perceptron mechanism makes sure than there is no actual vector multiplication happening in the hardware. Obtaining the perceptron sum requires addition of nine 5-bit numbers. Using an adder tree of four 5-bit adders, this can be done in ceil(log 2 9) = 4 steps. Perceptron update only requires weight update by +1 or -1. Thus, all the operations required for perceptron inferencing or updating the states of the perceptrons can be easily done in the time constraints of L2 Cache Accesses. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RESULTS</head><p>This section discusses the results obtained from running PPF in terms of speedup and prefetch cache, for the SPEC CPU 2017 benchmarks. First, we present the results for single-threaded workloads then for multi-core workloads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Single-core Results</head><p>Figure <ref type="figure">9</ref> shows the single core speedup obtained by BOP, DA-AMPM, SPP and PPF for each of the individual SPEC CPU 2017 applications, followed by the geomean of the memory intensive subset and finally the geomean across the full suite. All the results have been normalized to the baseline of no prefetching. PPF yields a geometric mean speedup of 26.95% over the baseline. This is equivalent to 4.63% over DA-AMPM, 4.61% over BOP and 3.78% over SPP. Out of the 20 SPEC CPU 2017 applications, PPF nearly matches or outperforms all the other prefetchers on 19 applications. Benchmarks 603.bwaves_s, 605.mcf_s, 623.xalancbmk_s and 649. fotonik3d_s benefit the most from PPF, with the speedup over SPP ranging from 10% to 25%.</p><p>One interesting case here is 623.xalancbmk. Despite SPP under performing on that application, PPF manages to considerably outperform all prefetchers. Since this benchmark has varying prefetch deltas, SPP's conservative throttling mechanism catches that and quickly halts prefetching at an average depth of 2.1. On the other hand, PPF's more efficient accuracy check enables it to prefetch up to a lookahead depth of 3.3. Doing this, PPF suggests 1.61 times more total prefetches and 2.53 times more useful prefetches than SPP.</p><p>The only benchmark where PPF fails to match the improvement offered by any other prefetcher is 607.cactuBSSN_s. Based on our observation of prefetching behavior, we gather that BOP's aggressive and localized nature fits this workload very well; as opposed to SPPs lookahead nature. As a result, SPP, and hence PPF, underperform on this benchmark.</p><p>On the full SPEC CPU 2017 suite, PPF improves the geometric mean IPC of the baseline by 15.24%, which is 2.27% better than the next best prefetcher -SPP. For PPF, the average lookahead depth over the full benchmark is 3.97, while it is 3.28 for just SPP. It is evident that on average for SPP, our scheme allows the prefetcher to speculate 21% deeper.  Coverage: Prefetcher coverage is defined as the ratio of the number of misses avoided through prefetching over the number of misses with no prefetching. Figure <ref type="figure" target="#fig_11">10</ref> shows the fraction of misses in the L2 and LLC avoided by the various prefetchers. PPF has the highest coverage of all the prefetchers simulated. On the SPEC CPU 2017 benchmarks, PPF reduces misses by 75.5% and 86.9% in the L2 and LLC respectively. For the same benchmarks, the next best prefetcher, DA-AMPM, covers 54.3% and 78.5% of the misses respectively.</p><p>This superior coverage of PPF can be attributed to aggressive re-tuning of the underlying SPP, enabled by the Perceptron Filter making sure the high coverage does not lead to increased cache pollution. In this section, we demonstrate the improvement achieved by PPF for a mix of multi-programmed workloads.</p><p>4-Core: Figure <ref type="figure" target="#fig_12">11</ref> shows a comparison of speedups obtained on 4-core mixes of a memory intensive subset of SPEC CPU 2017. We plot all 4 prefetchers, normalized to the baseline. The workloads have been sorted in increasing order of the speedup. PPF offers a speedup of 51.2% on these traces, an improvement of 11.4% over the underlying SPP, 9.7% over the next DA-AMPM, and 16.9% over BOP. On a different set of fully random SPEC CPU 2017 4core mixes (not illustrated for space reasons), PPF provides an IPC speedup of 26.07% over the baseline, which is an improvement of 5.6% over SPP. The sorted comparison of speedups on the memory intensive 8-core mixes is shown in Figure <ref type="figure" target="#fig_13">12</ref>. PPF improves baseline performance by 37.6%, an improvement of 9.65% over SPP. For a random set of SPEC CPU 2017 mixes (not illustrated for space reasons), PPF improves performance by 23.4% over the baseline, corresponding to 4.6% over SPP. This increased improvement achieved by PPF over the underlying prefetcher, SPP, in a multi-core environment is expected as PPF is a very accurate filter. Thus, it eliminates useless prefetches before they can cause pollution in the shared LLC. BOP offers a better improvement than SPP for the memory intensive mixes. This superiority can be attributed to BOP's inherent aggressive nature. DA-AMPM is also ahead of SPP in both the mixes. Interestingly, in all these cases, PPF consistently outperforms the best performing prefetcher.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Additional Memory Constraints</head><p>We also model PPF with reduced LLC and with low bandwidth constraints, respectively (not illustrated for space reasons). Benchmark 605.mcf_s in low bandwidth conditions is prefetch averse. In general, any prefetcher yields a negative speedup on that trace. On 654.roms_s and 607.cactuBSSN_s, PPF is unable to match the performance achieved by the best prefetcher. On the other hand, PPF outperforms all the other prefetchers on 623.xalancbmk_s and 638.imagick_s benchmarks. Overall, PPF provides a greater improvement under small LLC condition and matches the best prefetcher, BOP, under low DRAM bandwidth conditions.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Cross Validation</head><p>Figure <ref type="figure" target="#fig_14">13</ref>(a) shows the performance benefit comparison of all the prefetch schemes on 4 different applications in the benchmark. In general, these applications are prefetch agnostic. Even so, PPF manages a 3.78% improvement over no prefetching, putting it ahead of the next best prefetcher, SPP, which provides a 3.08% speedup.</p><p>Figure <ref type="figure" target="#fig_14">13</ref>(b) shows the speed-up achieved on the memory intensive subset and the full SPEC CPU 2006 suite for a single-processor machine. PPF provides a speedup of 36.3% over the baseline on the memory intensive subset of SPEC CPU 2006 benchmark, giving an improvement of 6.1% over SPP and 8.44% over DA-AMPM and 9.93% over BOP. On the whole of the SPEC CPU 2006 suite, the speedup is 19.6%, an improvement of 3.33% over SPP.</p><p>For 4-core memory intensive mixes, PPF improves the baseline by 59.1%, 8.6% ahead of SPP. For 8-core memory intensive mixes, the speedup over the baseline is 47.8%, 11.3% ahead of SPP.</p><p>We developed PPF to yield good performance on the SPEC CPU 2017 benchmarks. Nevertheless, the performance is consistently good on other benchmark suites. We attribute this fact to the inherent adaptability of the perceptron model. In general, perceptron weights are able to adjust in real-time so as to find the best possible correlation between the output and the given set of features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">RELATED WORK 7.1 Spatial Prefetchers</head><p>Spatial prefetchers include such well-understood examples as the next-line (or next-n-line) prefetcher, and the stream prefetcher, and are distinguished by prefetching data without regard for the order in which the data will be accessed. In addition to these simpler examples, Somogyi et al. propose Spatial Memory Streaming (SMS) <ref type="bibr" target="#b12">[13]</ref>.</p><p>SMS works by learning the spatial footprint of all data used by a program within a region of memory around a given missing load, and when the load that causes an new miss elsewhere, the same spatial footprint is prefetched. Ishii et al. propose the Access Map Pattern Matching prefetcher (AMPM) <ref type="bibr" target="#b10">[11]</ref>, which creates a map of all accessed lines within a region of memory, and then analyzes this map on every access to see if any fixed-stride pattern can be identified and prefetched that is centered on the current access. DRAM-Aware AMPM (DA-AMPM) <ref type="bibr" target="#b31">[32]</ref> is a variant of AMPM that delays some prefetches so they can be issued together with others in the same DRAM row, increasing bandwidth utilization. Pugsley et al. propose the Sandbox Prefetcher <ref type="bibr" target="#b32">[33]</ref>, which analyzes candidate fixedoffset prefetchers in a sandboxed environment to determine which is most suitable for the current program phase. Michaud proposes the Best-Offset Prefetcher <ref type="bibr" target="#b33">[34]</ref>, which determines the optimal offset to prefetch while considering memory latency and prefetch timeliness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Lookahead Prefetchers</head><p>Unlike spatial prefetchers, lookahead prefetchers take program order into account when they make predictions. Shevgoor et al. propose the Variable Length Delta Prefetcher (VLDP) <ref type="bibr" target="#b34">[35]</ref>, which correlates histories of deltas between cache line accesses within memory pages with the next delta within that page. SPP <ref type="bibr" target="#b1">[2]</ref> and KPC's prefetching component <ref type="bibr" target="#b35">[36]</ref> are more recent examples of lookahead prefetchers. They try to predict not only what data will be used in the future, but also the precise order in which the data will be used, within a given page. Predictions made by lookahead prefetchers can be fed back into their prediction mechanisms to predict even further down a speculative path of memory accesses. These prefetchers can also generalize their learned patterns from one page, and use those patterns to make predictions in other pages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Managing Prefetched Data</head><p>A low-accuracy aggressive prefetcher can significantly harm performance. To minimize interference from prefetching, Wu et al. propose PACMan <ref type="bibr" target="#b36">[37]</ref>, a prefetch-aware cache management policy. PACMan dedicates some LLC sets to each of three competing policies that treat demand and prefetch requests differently, using the policy in the rest of the cache that shows the fewest misses. Seshadri et al. propose ICP <ref type="bibr" target="#b37">[38]</ref>, which demotes a prefetch to the lowest reuse priority on a demand hit, based on the observation that most prefetches are dead after their first hit. To address prefetcher-caused cache pollution, it also uses a variation of EAF <ref type="bibr" target="#b38">[39]</ref> to track prefetching accuracy, and inserts only accurate prefetches to the higher priority position in the LRU stack. Jain et al. propose Harmony <ref type="bibr" target="#b39">[40]</ref> to accommodate prefetches in their MIN algorithm-inspired Hawkeye cache management system. Ebrahimi et al. introduce HPAC <ref type="bibr" target="#b40">[41]</ref> which provides a coordinated control between multiple prefetchers present in a CMP by looking at the prefetcher-induced inter-core interference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Machine Learning for Prefetching</head><p>Peled et al. introduce interesting ideas for on-line Reinforcement Learning and dynamically scaling the magnitude of feedback given to the baseline prefetcher <ref type="bibr" target="#b3">[4]</ref>. The prefetcher relies on compiler support to receive features and build the context. Liao et al. focus on prefetching for data center applications <ref type="bibr" target="#b4">[5]</ref>. They use offline machine learning algorithms such as SVMs and logistic regression to do a parametric search for an optimal prefetcher configuration. Hasheni et al. <ref type="bibr" target="#b41">[42]</ref> categorize prefetching as a regression problem and use LSTM based Deep Learning approach.</p><p>Wang and Lou propose a similar work where perceptrons filter useless prefetches <ref type="bibr" target="#b42">[43]</ref>. In their design's primary focus was on improving the accuracy of an unmodified baseline prefetcher. Unlike the scheme presented here, they implement a basic Rosenblatt perceptron, with general error-correction learning. While they are able to increase accuracy, their design results in lower coverage, and hence has low impact on overall performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">Perceptrons in Cache Management</head><p>In addition to branch prediction <ref type="bibr" target="#b19">[20]</ref>, perceptron-based learning has been applied to the area of cache management. Teran et al. propose using perceptrons to predict cache line reuse, bypass, and replacement <ref type="bibr" target="#b21">[22]</ref>. Perceptron Learning trains weights selected by hashes of multiple features, including the PC of the memory access instruction, some other recent PCs, and two different shifts of the tag of the referenced block. These features are used to index into weight tables, and the weights are then thresholded to generate a prediction. When a block from one of a few sampled sets <ref type="bibr" target="#b43">[44]</ref> is reused or evicted, the corresponding weights are decremented or incremented, according to the perceptron learning rule. Multiperspective Reuse Prediction <ref type="bibr" target="#b22">[23]</ref> improves on Perceptron Learning by contributing many new features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>In this paper, we introduce the Perceptron-Based Prefetch Filtering (PPF). PPF acts as an independent check on the quality of predictions made by the underlying prefetch engine. We also created a case study implementation of PPF using SPP as the underlying prefetcher, while in principle other prefetchers could be used. We show that PPF effectively filters bad prefetches, such that the given underlying prefetcher can be highly aggressively tuned to achieve increasing coverage. PPF improves performance over the underlying prefetcher by up to 11.4%. PPF is a robust and adaptable technique that can be used to enhance any existing prefetcher and can be a valuable tool in the design of future memory latency constrained systems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure1: The impact of aggressive prefetching on performance for 603.bwaves_s. The number of useful prefetches increases with aggressiveness slower than total prefetches, which wastes bandwidth and harms performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: SPP Data-path Flow</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: SPP Architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: PPF Architecture in the Memory Hierarchy</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: PPF Data Path and Operation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Figure 6: Distribution of Trained Weights</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Cross-correlation: Beside providing</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: P-value Variation across Traces for selected Features</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Fraction of Cache Misses Covered</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Speedup for 4-core SPEC CPU 2017</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Speedup for 8-core SPEC CPU 2017</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: IPC Speedup for Unseen Workloads</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>PPF Reject Table Prefetch Table Perceptron Weight Tables Prefetch Suggestion Training and feature data from L2 demand access Prefetch (insert into either L2 or L3) L1D L1I Prefetches Metadata Prefetcher specific metadata</head><label></label><figDesc></figDesc><table><row><cell>Core</cell><cell></cell></row><row><cell>Cache L2</cell><cell>Baseline Prefetcher</cell></row><row><cell>L3 Cache</cell><cell></cell></row><row><cell>Off-Chip DRAM</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>1 ?..</figDesc><table><row><cell cols="3">Prefetch cache line suggested</cell><cell></cell><cell></cell></row><row><cell cols="3">by the base prefetcher</cell><cell cols="2">2. Recording</cell></row><row><cell></cell><cell></cell><cell>1. Inferencing</cell><cell></cell><cell></cell></row><row><cell>FEAT 1</cell><cell>FEAT 2</cell><cell>FEAT N</cell><cell cols="2">Index N Prefetch Table</cell></row><row><cell>W 11 W 21</cell><cell>W 12 W 22</cell><cell>W 1N W 2N</cell><cell>Index 1 ?..</cell><cell>Index N</cell></row><row><cell>W 31</cell><cell></cell><cell>W 3N</cell><cell></cell><cell></cell></row><row><cell>W 41</cell><cell>W J2</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>W KN</cell><cell></cell><cell></cell></row><row><cell>W I1</cell><cell></cell><cell></cell><cell>Perceptron Filter</cell><cell></cell></row><row><cell></cell><cell>Sum</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Thresholding</cell><cell cols="2">Sent to L2C / LLC for Prefetch</cell></row><row><cell></cell><cell>4. Training</cell><cell></cell><cell></cell><cell></cell></row><row><cell>FEAT 1 W 11</cell><cell></cell><cell></cell><cell>3. Retrieving</cell><cell></cell></row><row><cell>W 21</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>W 31 W 41</cell><cell></cell><cell></cell><cell cols="2">Demand Hit / Eviction</cell></row><row><cell>W I1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Previous L2C Prefetch</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Table was enhanced to accommodate storing of metadata for perceptron training. Table 2 depicts the metadata stored for each entry in the Prefetch Table. Table 3 shows the total storage overhead of PPF implementation. The hardware budget for 2nd Data Prefetching championship was 32 KB. Keeping that in mind the considerable speedup PPF obtained over the winner, the extra hardware budget can be accounted for. The extra hardware also Metadata Stored in Prefetch Table makes the overall scheme more scalable than SPP. In the original SPP paper, it was demonstrated that adding extra hardware brings little advantage in terms of performance gain. The newly added perceptron tables can be scaled to increase / decrease features depending on the permitted budget.</figDesc><table><row><cell>Field</cell><cell>Bits</cell><cell></cell><cell>Comment</cell></row><row><cell>Valid</cell><cell>1</cell><cell cols="3">Indicates a valid entry in the table</cell></row><row><cell>Tag</cell><cell>6</cell><cell cols="3">Identifier for the entry in the table</cell></row><row><cell>Useful</cell><cell>1</cell><cell cols="3">To show if the given entry led to a useful demand fetch</cell></row><row><cell>Perc Decision</cell><cell>1</cell><cell></cell><cell cols="2">Prefetched vs Not-prefetched</cell></row><row><cell>PC</cell><cell>12</cell><cell></cell><cell></cell></row><row><cell>Address</cell><cell>24</cell><cell></cell><cell></cell></row><row><cell cols="5">Curr Signature 10 Metadata required for perceptron PC i Hash 12 training Delta 7</cell></row><row><cell>Confidence</cell><cell>7</cell><cell></cell><cell></cell></row><row><cell>Depth</cell><cell>4</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Total 85 bits</cell></row><row><cell>Structure</cell><cell cols="2">Entry</cell><cell>Components</cell><cell>Total</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Valid (1 bit)</cell></row><row><cell cols="2">Signature Table</cell><cell>256</cell><cell>Tag (16 bits) Last Offset (6 bits) Signature (12 bits)</cell><cell>11008 bits</cell></row><row><cell></cell><cell></cell><cell></cell><cell>LRU (6 bits)</cell></row><row><cell>Pattern Table</cell><cell></cell><cell>512</cell><cell>C sig (4bits) C delta (4*4 bits) Delta (4*7 bits)</cell><cell>24576 bits</cell></row><row><cell></cell><cell cols="2">4096*4</cell><cell></cell></row><row><cell>Perceptron Weights</cell><cell cols="2">2048*2 1024*2</cell><cell>5 bits</cell><cell>113280 bits</cell></row><row><cell></cell><cell cols="2">128*1</cell><cell></cell></row><row><cell>Prefetch Table 1</cell><cell cols="2">1024</cell><cell>85 bits</cell><cell>87040 bits</cell></row><row><cell>Reject Table 2</cell><cell cols="2">1024</cell><cell>84 bits</cell><cell>86016 bits</cell></row><row><cell>Global History Register</cell><cell></cell><cell>8</cell><cell>Signature (12 bits) Confidence (8 bits) Last Offset (6 bits) Delta (7 bits)</cell><cell>264 bits</cell></row><row><cell>Accuracy</cell><cell></cell><cell>1</cell><cell>C total</cell><cell>10 bits</cell></row><row><cell>Counters</cell><cell></cell><cell>1</cell><cell>C use f ul</cell><cell>10 bits</cell></row><row><cell>Global PC Trackers</cell><cell></cell><cell>3</cell><cell>PC 1 (12 bits) PC 2 (12 bits) PC 3 (12 bits)</cell><cell>36 bits</cell></row><row><cell cols="4">Total: 322,240 bits = 39.34 KB</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>SPP-Perc Storage Overhead</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Components of Prefetch Table can be found in Table</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p><ref type="bibr" target="#b1">2</ref> The Reject Table does not need to maintain the useful bit as that only applies for prefetches that ultimately made through. 9</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="9">ACKNOWLEDGMENTS</head><p>We thank the anonymous reviewers for their valuable comments and feedback to improve the content and quality of this paper. We also thank the <rs type="funder">National Science Foundation</rs>, which supports this work through grants <rs type="grantNumber">FoMR-1823403, I/UCRC-1439722</rs> and <rs type="grantNumber">CCF-1649242</rs>. We thank <rs type="institution">Intel Corp</rs>. for their generous support. We thank <rs type="institution">Samsung</rs> for supporting this work through their <rs type="programName">Global Outreach Program</rs>. Portions of this research were conducted with the advanced computing resources provided by <rs type="funder">Texas A&amp;M High Performance Research Computing</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_CZJ4MuQ">
					<idno type="grant-number">FoMR-1823403, I/UCRC-1439722</idno>
				</org>
				<org type="funding" xml:id="_6YKsxyX">
					<idno type="grant-number">CCF-1649242</idno>
					<orgName type="program" subtype="full">Global Outreach Program</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Hitting the memory wall: Implications of the obvious</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Wulf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Mckee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGARCH Comput. Archit. News</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="20" to="24" />
			<date type="published" when="1995-03">Mar. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Path confidence based lookahead prefetching</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Pugsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V</forename><surname>Gratz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L N</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wilkerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chishti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 49th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="2016-10">Oct 2016</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Friendly fire: understanding the effects of multiprocessor prefetches</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D E</forename><surname>Jerger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Lipasti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2006 IEEE International Symposium on Performance Analysis of Systems and Software</title>
		<imprint>
			<date type="published" when="2006-03">March 2006</date>
			<biblScope unit="page" from="177" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semantic locality and contextbased prefetching using reinforcement learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Peled</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Weiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Etsion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 ACM/IEEE 42nd Annual International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2015-06">June 2015</date>
			<biblScope unit="page" from="285" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Machine learningbased prefetch optimization for data center applications</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on High Performance Computing Networking, Storage and Analysis</title>
		<meeting>the Conference on High Performance Computing Networking, Storage and Analysis</meeting>
		<imprint>
			<date type="published" when="2009-11">Nov 2009</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improving direct-mapped cache performance by the addition of a small fully-associative cache and prefetch buffers</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th Annual International Symposium on Computer Architecture, ISCA &apos;90</title>
		<meeting>the 17th Annual International Symposium on Computer Architecture, ISCA &apos;90<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="364" to="373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sequential program prefetching in memory hierarchies</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="7" to="21" />
			<date type="published" when="1978-12">Dec. 1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An effective on-chip preloading scheme to reduce data access penalty</title>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Baer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-F</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1991 ACM/IEEE Conference on Supercomputing, Supercomputing &apos;91</title>
		<meeting>the 1991 ACM/IEEE Conference on Supercomputing, Supercomputing &apos;91<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="176" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Effective hardware-based data prefetching for highperformance processors</title>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Baer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-F</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="609" to="623" />
			<date type="published" when="1995-05">May 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Making address-correlated prefetching practical</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moshovos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="50" to="59" />
			<date type="published" when="2010-01">Jan 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Access map pattern matching for data cache prefetch</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Inaba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hiraki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Supercomputing, ICS &apos;09</title>
		<meeting>the 23rd International Conference on Supercomputing, ICS &apos;09<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="499" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Accurate and complexityeffective spatial pattern prediction</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">.</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moshovos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th International Symposium on High Performance Computer Architecture (HPCA&apos;04)</title>
		<imprint>
			<date type="published" when="2004-02">Feb 2004</date>
			<biblScope unit="page" from="276" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Spatial memory streaming</title>
		<author>
			<persName><forename type="first">S</forename><surname>Somogyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moshovos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd Annual International Symposium on Computer Architecture, ISCA &apos;06</title>
		<meeting>the 33rd Annual International Symposium on Computer Architecture, ISCA &apos;06<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="252" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Temporal instruction fetch streaming</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moshovos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 41st Annual IEEE/ACM International Symposium on Microarchitecture<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Practical off-chip meta-data for temporal memory streaming</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moshovos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE 15th International Symposium on High Performance Computer Architecture</title>
		<imprint>
			<date type="published" when="2009-02">Feb 2009</date>
			<biblScope unit="page" from="79" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Spatio-temporal memory streaming</title>
		<author>
			<persName><forename type="first">S</forename><surname>Somogyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th Annual International Symposium on Computer Architecture, ISCA &apos;09</title>
		<meeting>the 36th Annual International Symposium on Computer Architecture, ISCA &apos;09<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="69" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Spatial memory streaming</title>
		<author>
			<persName><forename type="first">S</forename><surname>Somogyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Instruction-Level Parallelism</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">B-fetch: Branch prediction directed prefetching for chip-multiprocessors</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kadjo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gratz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jim?nez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 47th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="2014-12">Dec 2014</date>
			<biblScope unit="page" from="623" to="634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mtb-fetch: Multithreading aware hardware prefetching for chip multiprocessors</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Albarakat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V</forename><surname>Gratz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jim?nez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Architecture Letters</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="175" to="178" />
			<date type="published" when="2018-07">July 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dynamic branch prediction with perceptrons</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jim?nez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Symposium on High Performance Computer Architecture (HPCA-7)</title>
		<meeting>the 7th International Symposium on High Performance Computer Architecture (HPCA-7)</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="197" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Merging path and gshare indexing in perceptron branch prediction</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tarjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Skadron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Archit. Code Optim</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="280" to="300" />
			<date type="published" when="2005-09">Sept. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Perceptron learning for reuse prediction</title>
		<author>
			<persName><forename type="first">E</forename><surname>Teran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jim?nez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 49th Annual IEEE/ACM International Symposium on Microarchitecture, MICRO-49</title>
		<meeting><address><addrLine>Piscataway, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="2" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multiperspective reuse prediction</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jim?nez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Teran</surname></persName>
		</author>
		<idno>MICRO-50 &apos;17</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 50th Annual IEEE/ACM International Symposium on Microarchitecture<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="436" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Virtual program counter (vpc) prediction: Very low cost indirect branch prediction using conditional branch prediction hardware</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Joao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="1153" to="1170" />
			<date type="published" when="2008">12 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">The champsim simulator</title>
		<ptr target="https://github.com/ChampSim/ChampSim" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">The 2nd data prefetching championship (dpc-2)</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Pugsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Alameldeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wilkerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<ptr target="http://comparch-conf.gatech.edu/dpc2/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">The 2nd cache replacement championship (crc-2)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Standard performance evaluation corporation cpu2017 benchmark suite</title>
		<ptr target="http://www.spec.org/cpu" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Using simpoint for accurate and efficient simulation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Perelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hamerly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Biesbrouck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sherwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Calder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems, SIGMETRICS &apos;03</title>
		<meeting>the 2003 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems, SIGMETRICS &apos;03<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="318" to="319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Standard performance evaluation corporation cpu2006 benchmark suite</title>
		<ptr target="http://www.spec.org/cpu2006/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Clearing the clouds: A study of emerging scale-out workloads on modern hardware</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Adileh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kocberber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Volos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alisafaee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jevdjic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kaynak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the Seventeenth International Conference on Architectural Support for Programming Languages and Operating Systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">XVII</biblScope>
			<biblScope unit="page" from="37" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unified memory optimizing architecture: Memory subsystem control with a unified predictor</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Inaba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hiraki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM International Conference on Supercomputing, ICS &apos;12</title>
		<meeting>the 26th ACM International Conference on Supercomputing, ICS &apos;12<address><addrLine>NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="267" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sandbox prefetching: Safe run-time evaluation of aggressive prefetchers</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Pugsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chishti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wilkerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Balasubramonian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE 20th International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<date type="published" when="2014-02">Feb 2014</date>
			<biblScope unit="page" from="626" to="637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Best-offset hardware prefetching</title>
		<author>
			<persName><forename type="first">P</forename><surname>Michaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<date type="published" when="2016-03">March 2016</date>
			<biblScope unit="page" from="469" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Efficiently prefetching complex address patterns</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shevgoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Koladiya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Balasubramonian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wilkerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Pugsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chishti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 48th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="2015-12">Dec 2015</date>
			<biblScope unit="page" from="141" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Kill the program counter: Reconstructing program behavior in the processor cache hierarchy</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Teran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V</forename><surname>Gratz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jim?nez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Pugsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wilkerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Second International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS &apos;17</title>
		<meeting>the Twenty-Second International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS &apos;17<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="737" to="749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pacman: Prefetchaware cache management for high performance caching</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Martonosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Steely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 44th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="2011-12">Dec 2011</date>
			<biblScope unit="page" from="442" to="453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Mitigating prefetcher-caused pollution using informed caching policies for prefetched blocks</title>
		<author>
			<persName><forename type="first">V</forename><surname>Seshadri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yedkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>Gibbons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Kozuch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Mowry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Archit. Code Optim</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1" to="51" />
			<date type="published" when="2015-01">Jan. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The evicted-address filter: A unified mechanism to address both cache pollution and thrashing</title>
		<author>
			<persName><forename type="first">V</forename><surname>Seshadri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Kozuch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Mowry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Parallel Architectures and Compilation Techniques, PACT &apos;12</title>
		<meeting>the 21st International Conference on Parallel Architectures and Compilation Techniques, PACT &apos;12<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="355" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Rethinking belady&apos;s algorithm to accommodate prefetching</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2018-06">June 2018</date>
			<biblScope unit="page" from="110" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Coordinated control of multiple prefetchers in multi-core systems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42Nd Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 42Nd Annual IEEE/ACM International Symposium on Microarchitecture<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="316" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning memory access patterns</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hashemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ayers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Litz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
		<idno>abs/1803.02329</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Data cache prefetching with perceptron learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<idno>abs/1712.00905</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Sampling dead block prediction for lastlevel caches</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jim?nez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 43rd Annual IEEE/ACM International Symposium on Microarchitecture, MICRO &apos;43</title>
		<meeting>the 2010 43rd Annual IEEE/ACM International Symposium on Microarchitecture, MICRO &apos;43<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="175" to="186" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
