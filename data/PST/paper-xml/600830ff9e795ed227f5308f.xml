<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Under review as a conference paper at ICLR 2021 INTRINSIC-EXTRINSIC CONVOLUTION AND POOLING FOR LEARNING ON 3D PROTEIN STRUCTURES</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<title level="a" type="main">Under review as a conference paper at ICLR 2021 INTRINSIC-EXTRINSIC CONVOLUTION AND POOLING FOR LEARNING ON 3D PROTEIN STRUCTURES</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Proteins perform a large variety of functions in living organisms and thus play a key role in biology. However, commonly used algorithms in protein learning were not specifically designed for protein data, and are therefore not able to capture all relevant structural levels of a protein during learning. To fill this gap, we propose two new learning operators, specifically designed to process protein structures. First, we introduce a novel convolution operator that considers the primary, secondary, and tertiary structure of a protein by using n-D convolutions defined on both the Euclidean distance, as well as multiple geodesic distances between the atoms in a multi-graph. Second, we introduce a set of hierarchical pooling operators that enable multi-scale protein analysis. We further evaluate the accuracy of our algorithms on common downstream tasks, where we outperform state-of-the-art protein learning algorithms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Geometry similar Topology different</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Geometry different</head><p>Topology similar</p><p>Figure <ref type="figure">1</ref>: Invariances present in protein structures.</p><p>Proteins perform specific biological functions essential for all living organisms and hence play a key role when investigating the most fundamental questions in the life sciences. These biomolecules are composed of one or several chains of amino acids, which fold into specific conformations to enable various biological functionalities. Proteins can be defined using a multi-level structure:: The primary structure is given by the sequence of amino acids that are connected through covalent bonds and form the protein backbone. Hydrogen bonds between distant amino acids in the chain form the secondary structure, which defines substructures such as α-helices and β-sheets. The tertiary structure results from protein folding and expresses the 3D spatial arrangement of the secondary structures. Lastly, the quarternary structure is given by the interaction of multiple amino acid chains.</p><p>Considering only one subset of these levels can lead to misinterpretations due to ambiguities. As shown by <ref type="bibr" target="#b1">Alexander et al. (2009)</ref>, proteins with almost identical primary structure, i.e., only containing a few different amino acids, can fold into entirely different conformations. Conversely, proteins from SH3 and OB folds have similar tertiary structures, but their primary and secondary structures differ significantly <ref type="bibr" target="#b0">(Agrawal &amp; Kishan, 2001</ref>) (Fig. <ref type="figure">1</ref>). To avoid misinterpretations arising from these observations, capturing the invariances with respect to primary, secondary, and tertiary structures is of key importance when studying proteins and their functions.</p><p>Previously, the SOTA was dominated by methods based on hand-crafted features, usually extracted from multi-sequence alignment tools <ref type="bibr" target="#b3">(Altschul et al., 1990)</ref> or annotated databases <ref type="bibr" target="#b18">(El-Gebali et al., 2019)</ref>. In recent years, these have been outperformed by protein learning algorithms in different protein modeling tasks such as protein fold classification <ref type="bibr" target="#b27">(Hou et al., 2018;</ref><ref type="bibr" target="#b45">Rao et al., 2019;</ref><ref type="bibr" target="#b7">Bepler &amp; Berger, 2019;</ref><ref type="bibr" target="#b2">Alley et al., 2019;</ref><ref type="bibr" target="#b38">Min et al., 2020)</ref> or protein function prediction <ref type="bibr" target="#b48">(Strodthoff et al., 2020;</ref><ref type="bibr" target="#b22">Gligorijevic et al., 2019;</ref><ref type="bibr" target="#b36">Kulmanov et al., 2017;</ref><ref type="bibr" target="#b35">Kulmanov &amp; Hoehndorf, 2019;</ref><ref type="bibr" target="#b4">Amidi et al., 2017)</ref>. This can be attributed to the ability of machine learning algorithms to learn meaningful representations of proteins directly from the raw data. However, most of these techniques only consider a subset of the relevant structural levels of proteins and thus can only create a representation from partial information. For instance, due to the high amount of available protein sequence data, most techniques solely rely on protein sequence data as input, and apply learning algorithms from the field of natural language processing <ref type="bibr" target="#b45">(Rao et al., 2019;</ref><ref type="bibr" target="#b2">Alley et al., 2019;</ref><ref type="bibr" target="#b38">Min et al., 2020;</ref><ref type="bibr" target="#b48">Strodthoff et al., 2020)</ref>, 1D convolutional neural networks <ref type="bibr" target="#b36">(Kulmanov et al., 2017;</ref><ref type="bibr" target="#b35">Kulmanov &amp; Hoehndorf, 2019)</ref>, or use structural information during training <ref type="bibr" target="#b7">(Bepler &amp; Berger, 2019)</ref>. Other methods have solely used 3D atomic coordinates as an input, and applied 3D convolutional neural networks (3DCNN) <ref type="bibr" target="#b4">(Amidi et al., 2017;</ref><ref type="bibr" target="#b14">Derevyanko et al., 2018)</ref> or graph convolutional neural networks (GCNN) <ref type="bibr" target="#b32">(Kipf &amp; Welling, 2017)</ref>. While few attempts have been made to consider more than one structural level of proteins in the network architecture <ref type="bibr" target="#b22">(Gligorijevic et al., 2019)</ref>, none of these hybrid methods incorporate all structural levels of proteins simultaneously. In contrast, a common approach is to process one structural level with the network architecture and the others indirectly as input features <ref type="bibr" target="#b6">(Baldassarre et al. (2020)</ref> or <ref type="bibr" target="#b27">Hou et al. (2018)</ref>).</p><p>In this paper, we introduce a novel end-to-end protein learning algorithm, that is able to explicitly incorporate the multi-level structure of proteins and captures the resulting different invariances. We show how a multi-graph data structure can represent the primary and secondary structures effectively by considering covalent and hydrogen bonds, while the tertiary structure can be represented by the spatial 3D coordinates of the atoms (Sec. 3). By borrowing terminology from differential geometry of surfaces, we define a new convolution operator that uses both intrinsic (primary and secondary structures) and extrinsic (tertiary and quaternary structures) distances (Sec. 4). Moreover, since protein sizes range from less than one hundred to tens of thousands of amino acids <ref type="bibr" target="#b9">(Brocchieri &amp; Karlin, 2005)</ref>, we propose protein-specific pooling operations that allow hierarchical grouping of such a wide range of sizes, enabling the detection of features at different scales (Sec. 5). Lastly, we demonstrate, that by considering all mentioned protein structure levels, we can significantly outperform recent SOTA methods on protein tasks, such as protein fold and enzyme classification.</p><p>Code and data of our approach is available at https://github.com/x/y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Early works on learning protein representations <ref type="bibr" target="#b5">(Asgari &amp; Mofrad, 2015;</ref><ref type="bibr" target="#b56">Yang et al., 2018)</ref> used word embedding algorithms <ref type="bibr" target="#b37">(Mikolov et al., 2013)</ref>, as employed in Natural Language Processing (NLP).</p><p>Other approaches have used 1D convolutional neural networks (CNN) to learn protein representations directly from an amino acid sequence, for tasks such as protein function prediction <ref type="bibr" target="#b36">(Kulmanov et al., 2017;</ref><ref type="bibr" target="#b35">Kulmanov &amp; Hoehndorf, 2019)</ref>, protein-compound interaction <ref type="bibr" target="#b52">(Tsubaki et al., 2018)</ref>, or protein fold classification <ref type="bibr" target="#b27">(Hou et al., 2018)</ref>. Recently, researchers have applied complex NLP models trained unsupervised on millions of unlabeled protein sequences and fine-tune them for different downstream tasks <ref type="bibr" target="#b45">(Rao et al., 2019;</ref><ref type="bibr" target="#b2">Alley et al., 2019;</ref><ref type="bibr" target="#b38">Min et al., 2020;</ref><ref type="bibr" target="#b48">Strodthoff et al., 2020;</ref><ref type="bibr" target="#b7">Bepler &amp; Berger, 2019)</ref>. While representing proteins as amino acid sequences during learning, is helpful when only sequence data is available, it does not leverage the full potential of spatial protein representations that become more and more available with modern imaging and reconstruction techniques.</p><p>To learn beyond sequences, approaches have been developed, that consider the 3D structure of proteins. A range of methods has sampled protein structures to regular volumetric 3D representations and assessed the quality of the structure <ref type="bibr" target="#b14">(Derevyanko et al., 2018)</ref>, classified proteins in enzymes classes <ref type="bibr" target="#b4">(Amidi et al., 2017)</ref>, predicted the protein-ligand binding affinity <ref type="bibr" target="#b44">(Ragoza et al., 2017)</ref> and the binding site <ref type="bibr" target="#b29">(Jiménez et al., 2017)</ref>, as well as the contact region between two proteins <ref type="bibr" target="#b51">(Townshend et al., 2019)</ref>. While this is attractive, as 3D grids allow for unleashing the benefits of all approaches developed for 2D images, such as pooling and multi-resolution techniques, unfortunately, grids do not scale well to fine structures or many atoms, and even more importantly, they do not consider the primary and secondary structure of proteins.</p><p>Another approach that makes use of a protein's 3D structure, is representing proteins as graphs and applying GCNNs <ref type="bibr" target="#b32">(Kipf &amp; Welling, 2017;</ref><ref type="bibr" target="#b24">Hamilton et al., 2017)</ref>. Works based on this technique represent each amino acid as a node in the graph, while edges between them are created if they are at a certain Euclidean distance. This approach has been successfully applied to different problems. Classification of protein graphs into enzymes, for example, have become part of the standard data sets used to compare GCNN architectures <ref type="bibr" target="#b21">(Gao &amp; Ji, 2019;</ref><ref type="bibr" target="#b57">Ying et al., 2018)</ref>. Moreover, other works with similar architectures have predicted protein interfaces <ref type="bibr" target="#b20">(Fout et al., 2017)</ref>, or protein structure quality <ref type="bibr" target="#b6">(Baldassarre et al., 2020)</ref>. However, GCNN approaches suffer from over-smoothing, i. e., indistinguishable node representations after stacking several layers, which limits the maximum depth usable for such architectures <ref type="bibr" target="#b10">(Cai &amp; Wang, 2020)</ref>.</p><p>It is also worth noticing that some of the aforementioned works on GCNN have considered different levels of protein structures indirectly by providing secondary structure type or distance along the sequence as initial node or edge features. However, these are not part of the network architecture and can be blended out due to the over-smoothing problem of GCNN. On the other hand, a recent protein function prediction method proposed by <ref type="bibr" target="#b22">Gligorijevic et al. (2019)</ref> uses Long-Short Term Memory cells (LSTM) to encode the primary structure and then apply GCNNs to capture the tertiary structure. Also, the recent work from <ref type="bibr" target="#b28">Ingraham et al. (2019)</ref> proposes an amino acid encoder that can capture primary and tertiary structures in the context of protein generative models. Unfortunately, none of these previous methods can incorporate all structural protein levels within the network architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MULTI-GRAPH PROTEIN REPRESENTATION</head><p>To simultaneously take into account the primary, secondary, and tertiary protein structure during learning, we propose to represent proteins as a multi-graph G = (N , F, A, B). In this graph, atoms are represented as nodes associated with their 3D coordinates, N ∈ R n×3 , and associated features, F ∈ R n×t , n being the number of atoms, and t the number of features. Moreover, A ∈ R n×n and B ∈ R n×n are two different adjacency matrices representing the connectivity of the graph. Elements of matrix A are defined as A ij = 1 if there is a covalent bond between atom i and atom j, and A ij = 0 otherwise. Similarly, the elements of matrix B are defined as B ij = 1 if there is a covalent or hydrogen bond between atom i and atom j, and B ij = 0 otherwise.  Differential geometry of surfaces <ref type="bibr" target="#b41">(Pogorelov, 1973)</ref> defines intrinsic geometric properties as those, that are invariant under isometric mappings, i.e., under deformations preserving the length of curves on a surface. On the other hand, extrinsic geometric properties are dependent on the embedding of the surfaces into the Euclidean space. Analogously, in our protein multi-graph, we define intrinsic geometric properties as those that are invariant under deformations preserving the length of paths along the graph, i.e., deformations that preserve the connectivity of the protein. Additionally, we define extrinsic geometric properties as those, that depend on the embedding of the protein into the Euclidean space, i.e., on the 3D protein conformation. Using this terminology, we define three distances in our multi-graph, one extrinsic and two intrinsic (see Fig. <ref type="figure" target="#fig_0">2</ref>). The extrinsic distance τ e is defined by the protein conformation in Euclidean space, therefore we use the Euclidean distance between atoms, which enables us to capture the tertiary and quaternary structures of the protein. The intrinsic distances are inherent of the protein and independent of the actual 3D conformation. For the first intrinsic distance τ i1 we use the shortest path between two atoms along the adjacency matrix A of the graph, capturing the primary structure. The second intrinsic distance τ i2 is defined as the shortest path between two atoms along the adjacency matrix B, capturing thus the secondary structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">INTRINSIC-EXTRINSIC DISTANCES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">INTRINSIC-EXTRINSIC PROTEIN CONVOLUTION</head><p>The key idea of our work is to take into account the multiple invariances described in Sec. 1 during learning. Therefore, based on the success of convolutional neural networks for images <ref type="bibr" target="#b34">(Krizhevsky et al., 2012)</ref> and point clouds <ref type="bibr" target="#b42">(Qi et al., 2017a)</ref>, in this paper we define a convolution operator for proteins which is able to capture these invariances effectively. To this end, we propose a convolution on 3D protein structures, which is inspired by conventional convolutions as used to learn on structured images. First, we define the neighborhood of an atom as all atoms at a Euclidean distance smaller than m e . Moreover, we define our convolution kernels as a single Multi Layer Perceptron (MLP) which takes as input the three distances defined in Sec. 3.1, one extrinsic and two intrinsic, and outputs the <ref type="figure">Figure 3</ref>: Intrinsic-extrinsic convolution on our multi-graph for atom A. First, we detect the neighboring atoms involved in the convolution using a ball query (a function returning all nodes closer than r to a point x). For each atom, the extrinsic (pink) and two intrinsic (blue and green) distances are input into the kernel and the result is multiplied by the atom's features. Lastly, all contributions from neighboring atoms are summed up.</p><formula xml:id="formula_0">4 3 2 1 A Conv(x A ) = F 1 k Ext. Int. 1 Int. 2 ( ( MAX F 2 k + ( ( F 3 k + ( ( F 4 k + ( ( F 5 k + ( (</formula><p>values of all kernels. This enables the convolution to learn kernels based on one or multiple structural levels of the protein. Thus, the proposed convolution operator is defined as:</p><formula xml:id="formula_1">(κ * F ) k (x) = i∈N (x) t j=1 F i,j • κ j,k τ e (x, x i ) m e , τ i1 (x, x i ) m 1 , τ i2 (x, x i ) m 2 (1)</formula><p>where N (x) are the atoms at Euclidean distance d &lt; m e from x, F i,j is the input feature j of atom x i , κ j,k is the aforementioned kernel that maps R 3 → R, τ e (x, x i ) is the Euclidean distance between atom x and atom x i , τ i1 and τ i2 are the two intrinsic distances, and m e , m 1 , and m 2 are the maximum distances allowed (m 1 = m 2 = 6 hops in all experiments while m e is layer-dependent).</p><p>All normalized distances are clamped to [0, 1]. The convolution, performed independently for every atom, is illustrated on a model protein in Fig. <ref type="figure">3</ref>, where the distances between neighboring atoms are shown. This operation has all properties of the standard convolution, locality and translational invariance, and at the same time rotational invariant.</p><p>While we state our operation as an unstructured convolution as done in 3D point cloud learning <ref type="bibr" target="#b26">(Hermosilla et al., 2018;</ref><ref type="bibr" target="#b23">Groh et al., 2018;</ref><ref type="bibr" target="#b54">Wu et al., 2019;</ref><ref type="bibr" target="#b55">Xiong et al., 2019;</ref><ref type="bibr" target="#b49">Thomas et al., 2019)</ref>, it can also be understood in a message passing framework <ref type="bibr" target="#b32">(Kipf &amp; Welling, 2017)</ref> where hidden states are atoms, edges are formed based on nearby atoms and messages between atoms. The key difference to standard GCNNs is that we have edges for multiple kinds of bonds, use a hierarchy of graphs, and that the message passing function is learned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">HIERARCHICAL PROTEIN POOLING</head><p>We consider proteins at atomic resolutions. This allows us to identify the spatial configuration of the amino acid side chains, something of key importance for active site detection. However, proteins can have a large number of atoms, preventing the usage of large receptive fields in our convolutions (computational restriction) and limiting the number of learned features per atom (memory restriction).</p><p>Pooling is a technique commonly used in convolutional neural networks, as it hierarchically reduces the dimensionality of the data by aggregating local information <ref type="bibr" target="#b34">(Krizhevsky et al., 2012)</ref>. While it is able to overcome computation and memory restrictions when learning on images, unfortunately, it cannot be directly applied to proteins, as conventional pooling methods rely on discrete sampling. While, on the other hand, some of the techniques developed for unstructured point cloud data <ref type="bibr" target="#b43">(Qi et al., 2017b;</ref><ref type="bibr" target="#b26">Hermosilla et al., 2018)</ref> could be applied to learn on the 3D coordinates of atoms, it is unclear what the edges of that new graph representation would be.</p><p>Therefore, we define a set of operations that successively reduce the number of nodes in our protein graph. First, we iteratively reduce the number of atoms per amino acid to its alpha carbon. Afterward, we reduce the number of nodes along the backbone. Fig. <ref type="figure">4</ref> illustrates this process, which we describe in detail in the following paragraphs.</p><p>Amino acid pooling Simplified representations of amino acids have been previously used in Molecular Dynamics (MD) simulations, in order to get the number of computations down to a manageable</p><formula xml:id="formula_2">Global features (a) (b) (c) (d) (e)</formula><p>Figure <ref type="figure">4</ref>: Hierarchical protein pooling: We segment the protein into amino acids (blue, orange, green). First, (a), we apply spectral clustering on each independent amino acid graph. Then, (b), each resulting amino acid is pooled to its α-carbon. After that, we apply two backbone pooling operations, (c) and (d). Lastly, we apply the symmetric operation average, (e), to obtain the final feature vector. level. Nevertheless, in contrast to our approach, these techniques do not use a uniform pooling pattern. Rather, while the atoms belonging to the backbone are not simplified, most of these approaches <ref type="bibr" target="#b47">(Simons et al., 1997)</ref> simplify all atoms of the side chains to a single node. Other more conservative approaches, such as PRIMO <ref type="bibr" target="#b31">(Kar et al., 2013)</ref> or Klein's models <ref type="bibr" target="#b15">(DeVane et al., 2009)</ref>, represent side chains with a variable number of nodes using a lookup table. However, they do not perform a uniform simplification, resulting in a high variance in the number of atoms per cluster. Moreover, these methods need to manually update the lookup table to incorporate rare amino acids.</p><p>In this work, we decided instead to follow the common practice in CNN for images where a uniform pooling is used over the whole image. We propose a method that is able to reduce the number of nodes in the protein graph by half. To this end, we generate an independent graph for each amino acid using the covalent bonds as edges and apply spectral clustering <ref type="bibr" target="#b53">(von Luxburg, 2007)</ref> to reduce by half the number of nodes. Since the number of amino acids is finite (20 appearing in the genetic code), these pooling matrices are reused among all proteins (see Fig. <ref type="figure">4 (a)</ref>). However, since the method only requires a graph as input, it can be directly applied to synthetic or rare amino acids.</p><p>From the amino acid pooling matrices, we create a protein pooling matrix P ∈ R n×m , where n is the number of input atoms in the protein and m is the number of resulting clusters in the simplified protein. This matrix is defined as P ij = 1 if the atom i collapses into cluster j, and P ij = 0 otherwise. Using P we can create a simplified protein graph, G = (N , F , A , B ), with the following equations:</p><formula xml:id="formula_3">N = D −1 PN F = D −1 PF A = PAP T B = PBP T (2)</formula><p>where D ∈ R m×m is a diagonal matrix with D jj = n i=0 P ij . Note, that the resulting matrices A and B might not be binary adjacency matrices, i. e., non-zero diagonal values or values greater than one. Therefore, we assign zeros to the diagonals and clamp edge values to one. These matrices are computed using sparse representations in order to reduce the memory footprint.</p><p>Alpha carbon pooling In a second pooling step, we simplify the protein graph to a backbone representation, whereby we cluster all nodes from the same amino acid to a single node. We define P accordingly, and the number of clusters m is equal to the number of amino acids, while P ij = 1 if node i belongs to amino acid j, and P ij = 0 otherwise. We use Eq. 2 to compute F , A , and B . However, N is defined as the alpha carbon positions of each amino acid since they better represent the backbone and the secondary structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Backbone pooling</head><p>The last pooling steps are simplifications of the backbone chain. In each pooling step, every two consecutive amino acids in the chain are clustered together, effectively reducing by half the number of amino acids. Therefore, to compute N , F , A , and B , we define P accordingly and use Equation <ref type="formula">2</ref>. For single-chain proteins, for example, P ij = 1 if i/2 = j, or 0 otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EVALUATION</head><p>To evaluate our proposed protein learning approach, we specify a deep architecture (Sec. 6.1) which we compare to SOTA methods (Sec. 6.2) as well as different ablations of our approach (Sec. 6.3). During the entire evaluation, we focus on two downstream tasks in Sec. 6.4 and Sec. 6.5. 6.1 OUR ARCHITECTURE By facilitating protein convolution and pooling, we are able to realize a deep architecture that enables learning on complex structures. In particular, the proposed architecture encodes an input protein into a latent representation which is later used in different downstream tasks.</p><p>The input of our network is a protein graph at atom level with a 6D input feature vector per each atom. The atom features comprise 1) covalent radius, 2) van der Waals radius, 3) atom mass, and 4-6) are the features of an atom type embedding learned together with the network. The input is then processed by 5 layers which iteratively increases the number of features, each composed of two ResNet <ref type="bibr" target="#b25">(He et al., 2016)</ref> bottleneck blocks followed by a pooling operation (see Fig. <ref type="figure">5</ref>). The respective receptive fields are <ref type="bibr">[3,</ref><ref type="bibr">6,</ref><ref type="bibr">8,</ref><ref type="bibr">12,</ref><ref type="bibr">16</ref>] angstroms (Å) using <ref type="bibr">[64,</ref><ref type="bibr">128,</ref><ref type="bibr">256,</ref><ref type="bibr">512,</ref><ref type="bibr">1024]</ref> features.</p><p>The size of the receptive fields has been chosen to include a reduced number of nodes in it. As several proteins come without hydrogen bonds, we compute them using DSSP <ref type="bibr" target="#b30">(Kabsch &amp; Sander, 1983)</ref>.</p><p>As the architecture is fully-convolutional, it can process proteins of arbitrary size, but after finitely many steps, it obtains an intermediate result of varying size. Hence, to reduce this to one final result vector, a symmetric aggregation operation, average, is used. Lastly, a single layer MLP with 1024 hidden neurons is used to predict the final probabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">OTHER ARCHITECTURES</head><p>We compare our architecture, as described above, to SOTA learners designed for similar downstream tasks. First, we compare to the latest sequence-based methods pre-trained unsupervised on millions of sequences:  <ref type="formula">2020</ref>), who indirectly process primary and secondary structures by using as input distances along the backbone as edge features and secondary structure type as node features in a GCNN setup. When available, we provide the results reported in the original papers, while more details of the training procedures are provided in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">ABLATIONS OF OUR METHOD</head><p>We study four axes of ablation: convolution, neighborhood, pooling, and representation. When moving along one axis, all other axes are fixed to Ours (•).</p><p>Convolution ablation. We consider four different ablations of convolutions: GCNN (•) (Kipf &amp; Welling, 2017); ExConv (•), kernels defined in 3D space only <ref type="bibr" target="#b26">(Hermosilla et al., 2018)</ref> intrinsic distances plus distances along the three spatial dimensions; and lastly, Ours (•), that refers to our proposed convolution which uses both geodesics plus Euclidean distance.</p><p>Neighborhood ablation. We compare several methods to define our receptive field: CovNeigh (•), which uses the intrinsic distance τ i1 on the graph; HyNeigh (•); which uses the intrinsic distance τ i2 , as well as Ours (•) which uses the Euclidean distance.</p><p>Pooling ablation. We consider six options: NoPool (•), which does not use any pooling operation; GridPool (•) overlays the protein with increasingly coarse grids and pools all atoms into one cell <ref type="bibr" target="#b49">(Thomas et al., 2019)</ref>; TopKPool (•) learns a per-node importance score which is used to drop nodes <ref type="bibr" target="#b21">(Gao &amp; Ji, 2019)</ref>; EdgePool (•), which learns a per-edge importance score to collapse edges <ref type="bibr" target="#b16">(Diehl, 2019)</ref>; RosettaCEN (•), which uses the centroid simplification method used by the Rosetta software in Molecular Dynamics simulation <ref type="bibr" target="#b47">(Simons et al., 1997)</ref>; and our pooling, Ours (•).</p><p>Representation ablation. Lastly, we evaluate the granularity of the input, considering the protein at the amino acid level, AminoGraph (•), or at atomic level, Ours (•).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">TASK 1: FOLD CLASSIFICATION (FOLD)</head><p>Task. Protein fold classification is of key importance in the study of the relationship between protein structure and function, and protein evolution. The different fold classes group proteins with similar secondary structure compositions, orientations, and connection orders. In this task, given a protein, we predict the fold class, whereby the performance is measured as mean accuracy.</p><p>Data set. We use the training/validation/test splits of the SCOPe 1.75 data set of <ref type="bibr" target="#b27">Hou et al. (2018)</ref>. This data set consolidated 16, 712 proteins from 1, 195 folds. We obtained the 3D structures of the proteins from the SCOPe 1.75 database <ref type="bibr" target="#b39">(Murzin et al., 1955)</ref>. The data set provides three different test sets: Fold, in which proteins from the same superfamily are not present during training; Superfamily, in which proteins from the same family are not seen during training; and Family, in which proteins of the same family are present during training. Results. Results, as compared to other methods, are reported in Tbl. 1. We find our method to perform better by a large margin without using additional features as input or pre-trained on additional data. Tbl. 2 shows how the tested ablations perform on this task. Overall we see that Ours (•) performs better than any ablation, indicating that our protein-specific representation, convolution, and pooling are all important ingredients. Lastly, we compare in the top of Tbl. 1 to the state of the art results reported in the paper <ref type="bibr" target="#b27">(Hou et al., 2018)</ref> as to the results obtained using multiple sequence alignment, where we outperform all.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">TASK 2: ENZYME-CATALYZED REACTION CLASSIFICATION (REACT)</head><p>Task. For this task, we classify proteins based on the enzyme-catalyzed reaction according to all four levels of the Enzyme Commission (EC) number <ref type="bibr">(Webb, 1992)</ref>. The performance is again evaluated as mean accuracy.</p><p>Data set. We collected a total of 37, 428 proteins from 384 EC numbers. The data was then split into 29, 215 instances for training, 2, 562 instances for validation, and 5, 651 for testing. Note that all proteins have less than 50% sequence-similarity in-between splits. A full description of the data set is provided in Appendix C.</p><p>Results. Again, our method outperforms previous works on this task (Tbl. 1). Ablations of our method (Tbl. 2) give further indication that all our components are relevant also for other tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">TASK 3: ONE-SHOT FOLD CLASSIFICATION (ONESHOT)</head><p>Task. The objective of this task is to give the binary answer to the question if a pair of proteins, which are both not observed during training, are in the same fold or not. This is also known as one-shot learning and we adopt the siamese architecture from <ref type="bibr" target="#b33">Koch et al. (2015)</ref> using our protein encoder.</p><p>Data. We used the same dataset as for classification (Sec. 6.4), but withheld a random subset of 50 folds out of the original 1195 folds during training. During testing, we compare all proteins contained in the test set to all proteins contained in a reference set that comprises of all proteins of the original training set, and thus represents all 1195 folds. From each test protein, we select the pair with higher probability to predict the fold. To analyze how accuracy is affected by folds unseen during training, we divided the Fold test set from Task 1 in two different test sets. Our seen test set contains 419 proteins from 86 folds seen during training, while our unseen test set contains 299 proteins from the 50 folds not seen during training.</p><p>Results. When analyzing how many proteins could be predicted correctly, we obtained an accuracy of 39.0% for seen and an accuracy of 31.9% for unseen. This indicates that our method is able to generalize to folds that have not been seen during training. It also shows that our method is flexible enough to take concepts like the one from <ref type="bibr" target="#b33">Koch et al. (2015)</ref>, which uses hierarchical image convolutions, and generalize them to proteins. To view the resulting accuracies in the context of the other values reported in our paper, we also have trained the architecture for standard classification of Sec. 6.4 with the training set of this task composed of 1145 folds. We obtained an accuracy of 46.5% when testing against the 86 folds compared to the 39 % of the one-shot training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSIONS</head><p>Based on the multi-level structure of proteins, we have proposed a neural network architecture to process protein structures. The presented architecture takes advantage of primary, secondary, and tertiary protein structures to learn a convolution operator that works with intrinsic and extrinsic distances as input. Moreover, we have presented a set of pooling operations that enable the dimensionality reduction of the input mimicking the designs used by convolutional neural networks on images. Lastly, our evaluation has shown that by incorporating all the structural levels of a protein in our designs we are able to outperform existing SOTA protein learning algorithms on two downstream tasks, protein fold and enzyme classification.</p><p>Despite the reported success achieved in protein learning, some limitations apply to our idea. The main one being, that it requires the 3D structure of each protein, whilst other methods only make use of the protein sequence. This, however, may be alleviated by the advances in protein structure determination <ref type="bibr" target="#b11">(Callaway, 2020)</ref> and prediction <ref type="bibr" target="#b46">(Senior et al., 2020)</ref>. Moreover, like other commonly used approaches such as GCNN, our convolution operator is invariant to rotations but also to chirality changes. This could be solved by incorporating directional information into the kernel's input. We leave this improvement for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A ADDITIONAL EXPERIMENTS</head><p>A.1 ENZYME-NO-ENZYME CLASSIFICATION Table <ref type="table">3</ref>: Results for the D&amp;D data set <ref type="bibr" target="#b17">(Dobson &amp; Doig, 2003)</ref>. <ref type="bibr" target="#b21">Gao &amp; Ji (2019)</ref> 82.4 % <ref type="bibr" target="#b57">Ying et al. (2018)</ref> 82.1 % <ref type="bibr" target="#b60">Zhao &amp; Wang (2019)</ref> 82.0 % <ref type="bibr" target="#b40">Nguyen et al. (2020)</ref> 81.2 % <ref type="bibr" target="#b59">Zhang et al. (2019)</ref> 81.0 % <ref type="bibr" target="#b50">Togninalli et al. (2019)</ref> 79.7 % <ref type="bibr" target="#b58">Zhang et al. (2018)</ref> 79.4 % Ours 85.5 % Task. This is a binary classification of a protein into either being an enzyme or not. Even though this task is rather simple as compared to our other tasks, we have included it, as it has been extensively used as a benchmark for graph classification methods (Code), where ir is referred as D&amp;D, and allows a direct comparison with several methods.</p><p>Data set. We downloaded the 1,178 proteins of the data set defined by <ref type="bibr" target="#b17">Dobson &amp; Doig (2003)</ref> from the Protein Data Bank (PDB) <ref type="bibr" target="#b8">(Berman et al., 2000)</ref>. Due to the low number of proteins, the performance is measure using k = 10-fold cross-validation.</p><p>Results. We compared to the results reported for methods that did not use extra training data and they were trained supervised as our method. We found our method to outperform all recent algorithms. In order to further validate that the network learned meaningful protein representations, we used t-SNE (van der <ref type="bibr" target="#b52">Maaten &amp; Hinton, 2008)</ref> to visualize the high-dimensional space of the representation learned for each protein on the different test sets of the SCOPe 1.75 data set <ref type="bibr" target="#b27">(Hou et al., 2018)</ref>, see Fig. <ref type="figure" target="#fig_1">6</ref>. We use as protein representation the global feature vector before is fed to the final MLP.</p><p>We color-coded each data point with the label of the higher level in the SCOPe hierarchy, protein class. We can see that, despite the network was not trained using this label, the network learned a representation in which folds are clustered by their upper class in the SCOPe hierarchy. In particular, the four major classes (a, b, c, and d) and the class of small proteins can be easily differentiate in the latent space. However, classes e and f are more difficult to identify. This could be due to the hyper-parameter tuning of t-SNE, or, in the case of class e, to the fact that proteins from this class are composed of more than one protein domain, each belonging to one of the other classes.</p><p>B AMINO ACID CLUSTERING MATRICES Fig. <ref type="figure" target="#fig_2">7</ref> illustrates the clustering matrices generated by the spectral clustering algorithm <ref type="bibr" target="#b53">(von Luxburg, 2007)</ref> for the 20 amino acids appearing in the genetic code. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C ENZYME REACTION DATA SET</head><p>Here we describe how the Enzyme Reaction data set was obtained and processed for the task described in Sec. 6.5. We downloaded EC annotations from the SIFTS database <ref type="bibr" target="#b13">(Dana et al., 2018)</ref>. This database contains EC annotations for entries on the Protein Data Bank (PDB) <ref type="bibr" target="#b8">(Berman et al., 2000)</ref>. From the annotated enzymes, we cluster their protein chains using a 50 % sequence similarity threshold, as provided by PDB <ref type="bibr" target="#b8">(Berman et al., 2000)</ref>. We selected the unique complete EC numbers (e.g. <ref type="bibr">EC 3.4.11.4)</ref> for what five or more unique clusters were annotated with it. For each cluster, we selected five different proteins (less than 100 % sequence similarity among them) as representatives.</p><p>Note that even if the proteins are similar, selecting several chains per cluster can be understood as a type of data augmentation. In total, we collected 37, 428 annotated protein chains. Due to the low number of annotations for some of the enzyme types, the resulting data set contains a highly unbalanced number of protein chains per EC number.</p><p>Lastly, we split the data set into three sets, training, validation, and testing, ensuring that every EC number was represented in each set and all protein chains belonging to the same cluster are in the same set. This resulted in 29, 215 protein chains for training, 2, 562 protein chains for validation, and 5, 651 for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D TRAINING AND HYPERPARAMETERS D.1 OUR ARCHITECTURE</head><p>We trained our model with Momentum optimizer with momentum equal to 0.98 for 600 epochs for the FOLD task and 300 epochs for the REACTION. We used an initial learning rate of 0.001, which was multiplied by 0.5 every 50 epochs with the allowed minimum learning rate of 1e − 6. Moreover, we used L2 regularization scaled by 0.001 and we clipped the norm of the gradients to 10.0. We used a batch size equal to eight for both tasks. We represented the convolution kernel with a single layer MLP with 16 hidden neurons for the FOLD task and 32 for the REACTION task.</p><p>To further regularize our model, we applied dropout with a probability of 0.2 before each 1 × 1 convolution, and 0.5 in the final MLP. Moreover, we set to zero all features of an atom before the Intrinsic / Extrinsic convolution with a probability of 0.05 for the FOLD task and 0.0 for the REACTION task. Lastly, we added Gaussian noise to the features before each convolution with a standard deviation of 0.025.</p><p>In the FOLD task, we increased the data synthetically by applying data augmentation techniques. Before feeding the protein to the network, we applied random Gaussian noise to the atom coordinates with a standard deviation of 0.1, randomly rotated the protein, and randomly scaled each axis in the range [0.9, Model Batch <ref type="bibr" target="#b45">(Rao et al., 2019)</ref> ResNet 153 LSTM 221 Transf. 90 <ref type="bibr" target="#b7">(Bepler &amp; Berger, 2019)</ref> 240 <ref type="bibr" target="#b2">(Alley et al., 2019)</ref> 221</p><p>For the FOLD task, we used the reported numbers in the paper <ref type="bibr" target="#b45">(Rao et al., 2019)</ref> for the same data set. For the REACTION task, we downloaded the code and pretrained models from the official repository of <ref type="bibr" target="#b45">Rao et al. (2019)</ref> and pre-processed our data with their pipeline. We used the same training procedure as the one used by the authors: The weights of the network were initialized with the values of a pre-trained model on unsupervised tasks, and they were fine-tuned for the task. We used a learning rate of 1e − 4 with a warm-up of 1000 steps.</p><p>In order to find the optimal batch size for each model, we trained each model increasing/decreasing the suggested optimal batch size by multiplying/dividing by 2 until no improvement was achieved. Tbl. 4 presents the optimal selected batch sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 BEPLER &amp; BERGER 2019 MULTITASK</head><p>We download the code and pre-trained models on the multitask method described in the paper of <ref type="bibr" target="#b7">Bepler &amp; Berger (2019)</ref> from the official repository. Since this model was designed to predict feature embeddings for amino acids, we added an attention-based pooling layer which computes a weighted aggregation of the amino acid embeddings as a protein descriptor. This is then process by a 2 layer MLP to finally classify the protein. This model was then trained by selecting the best hyper-parameters from different runs. We increased the batch size by multiplying it by 2 and the best performance was achieved with a batch size of 64. The learning rate was selected by multiplying it by 0.1, and the best performance was achieved by a learning rate of 0.001 for the classification head and attention-based pooling layers and 0.00001 for the pre-trained model. During the first epoch only the classification head and pooling layers where updated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4 STRODTHOFF ET AL. 2020</head><p>We downloaded the code and pre-trained models from the official repository <ref type="bibr" target="#b48">(Strodthoff et al., 2020)</ref> and pre-processed the data sets with their pre-processing pipeline. We used the same training procedure followed by the authors, an initial learning rate of 0.001, which was decreased every time a new layer of the model was unfreezed for fine-tuning. To evaluate the model, we averaged the performance of the forward and backward pre-trained models. We experimented with different batch sizes and last layer sizes since in the original work the last layers were composed of only 50 neurons. The best performance was obtained for a batch size of 32, and a hidden layer of 1024 for the FOLD task and 512 for the REACTION task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.5 ELNAGGAR ET AL. 2020</head><p>We downloaded the code and pre-trained model from the official repository <ref type="bibr" target="#b19">(Elnaggar et al., 2020)</ref>. We use the default parameters for classification tasks and optimized the batch size by increasing and decreasing it multiplying by 2. The best performance was achieved by a batch size of 32.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.6 KIPF &amp; WELLING 2017</head><p>We implemented a GCNN architecture using the message passing defined by <ref type="bibr" target="#b32">Kipf &amp; Welling (2017)</ref>. We used the standard architecture of three GCNN which outputs are concatenated and give as input to a global pooling ReadOut layer. The global features were processed by a single layer MLP with 1024 hidden layers. The protein graph was defined using the contact map of the amino acid sequence with a threshold of 8 Å, as this is common practice in GCNN literature <ref type="bibr" target="#b21">(Gao &amp; Ji, 2019)</ref>. The initial amino acid features were learned with a 16D embedding layer that was trained together with the model. We trained the model until convergence with momentum optimizer, a batch size of 8, and an initial learning rate of 0.001 which was multiplied by 0.5 every 50 epochs with an allowed minimum learning rate of 1e − 6. We also use dropout with probability equal to 0.5 in the final MLP and L2 regularization scaled by 5e − 4. All parameters were selected for optimal performance using the same procedure as in our ablations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.7 DIEHL 2019</head><p>The <ref type="bibr" target="#b16">Diehl (2019)</ref> model was implemented by applying the pooling algorithm after each GCNN layer in the model of <ref type="bibr" target="#b32">Kipf &amp; Welling (2017)</ref>. The same hyperparameters were used to train this model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.8 DEREVYANKO ET AL. 2018</head><p>We implemented the 3DCNN model of the paper <ref type="bibr" target="#b14">Derevyanko et al. (2018)</ref> following the same number of layers, features, and input resolution, 11 density volumes with a resolution of 120 × 120 × 120. We trained the model with momentum optimizer, a batch size of 16, and an initial learning rate of 0.005 which was multiplied by 0.5 every 50 epochs with an allowed minimum learning rate of 1e − 6. The model was trained until convergence. For regularization, we used dropout layers with a probability of 0.5 in the final MLP and L2 regularization scaled by 5e − 4. All parameters were selected for optimal performance using the same procedure as in our ablations.</p><p>D.9 GLIGORIJEVIC ET AL. 2019</p><p>We downloaded the code from the official repository and pre-processed both data sets using their pipeline. We downloaded the pre-trained language model and run several experiments to select the best hyperparameters. We used a dropout rate of 0.5, a learning rate of 2e − 4, and a batch size of 16.</p><p>The model was trained until convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.10 BALDASSARRE ET AL. 2020</head><p>We downloaded the code from the official repository and pre-processed both data sets using their pipeline. Since this method was designed to perform predictions at protein and amino acid level, we adapted the architecture by only using the global feature vector for the predictions and searching for the best architecture for our tasks. The best model had three GCNN layers, 128 out node features, 32 out edge features, and 512 out global features. We used a learning rate of 0.005 scaled by 0.8 every 50 epochs, weight decay of 1e − 5, and batch size of 16. The model was trained until convergence.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Distances between atoms in our protein graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>AFigure 6 :</head><label>6</label><figDesc>Figure 6: Visualization of the learned representations for the proteins of the different test sets of the SCOPe 1.75 data set. The data points are colored based on the higher level in the SCOPe hierarchy, showing that the network learned a latent space in which folds from the same class are clustered together.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Visualization of the amino acid clustering matrices obtained with the spectral clustering algorithm (von Luxburg, 2007).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>The architecture of our model. The input is composed of atom features and an atom embedding learned together with the network. Each layer is composed of two ResNet<ref type="bibr" target="#b25">(He et al., 2016)</ref> bottleneck blocks, for which the radius in angstrom and the number of features are indicated in parentheses, followed by a pooling operation. An illustration of a single ResNet bottleneck block is presented in the right for D input features (before each convolution we use batch normalization and a Leaky ReLU). The global protein features are processed by an MLP which computes the final probabilities. Protein graphs used in each level are indicated at the bottom.</figDesc><table><row><cell>Atom embedding Atom features</cell><cell>ResNet x2</cell><cell>(3, 64)</cell><cell>Pooling</cell><cell>ResNet x2</cell><cell>(6, 128)</cell><cell>Pooling</cell><cell>ResNet x2</cell><cell>(8, 256)</cell><cell>Pooling</cell><cell>ResNet x2</cell><cell>(12, 512)</cell><cell>Pooling</cell><cell>ResNet x2</cell><cell>(16, 1024)</cell><cell>Pooling</cell><cell>Global MLP features</cell><cell>ResNet Int. / Ext. Conv Conv 1x1 D/4 D</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>D/4</cell></row><row><cell>Hierarchy Level</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Probabilities</cell><cell>Conv 1x1 + D</cell></row><row><cell>Figure 5:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Comparison of our network to other methods on the two tasks (protein fold and enzyme catalytic reaction classification) measured as mean accuracy, where we outperform all methods.</figDesc><table><row><cell></cell><cell cols="2">Architecture # params</cell><cell>FOLD</cell><cell>REACT</cell></row><row><cell></cell><cell></cell><cell>Fold</cell><cell>Super.</cell><cell>Fam.</cell></row><row><cell>HHSuite</cell><cell></cell><cell cols="2">17.5 % 69.2 % 98.6 %</cell></row><row><cell>Hou et al. (2018)</cell><cell>1D CNN</cell><cell cols="2">1.0 M 40.9 % 50.7 % 76.2 %</cell></row><row><cell></cell><cell>1D ResNet</cell><cell cols="2">41.7 M 17.0 % 31.0 % 77.0 % 70.9 %</cell></row><row><cell>Rao et al. (2019)  *</cell><cell>LSTM</cell><cell cols="2">43.0 M 26.0 % 43.0 % 92.0 % 79.9 %</cell></row><row><cell></cell><cell>Transformer</cell><cell cols="2">38.4 M 21.0 % 34.0 % 88.0 % 69.8 %</cell></row><row><cell>Bepler &amp; Berger (2019)  *</cell><cell>LSTM</cell><cell cols="2">31.7 M 17.0 % 20.0 % 79.0 % 74.3 %</cell></row><row><cell>Bepler &amp; Berger (2019)  †</cell><cell>LSTM</cell><cell cols="2">31.7 M 36.6 % 62.7 % 95.2 % 66.7 %</cell></row><row><cell>Alley et al. (2019)  *</cell><cell>mLSTM</cell><cell cols="2">18.2 M 23.0 % 38.0 % 87.0 % 72.9 %</cell></row><row><cell>Strodthoff et al. (2020)  *</cell><cell>LSTM</cell><cell cols="2">22.7 M 14.9 % 21.5 % 83.6 % 73.9 %</cell></row><row><cell>Elnaggar et al. (2020)  *</cell><cell>Transformer</cell><cell cols="2">420.0 M 26.6 % 55.8 % 97.6 % 72.2 %</cell></row><row><cell>Kipf &amp; Welling (2017)</cell><cell>GCNN</cell><cell cols="2">1.0 M 16.8 % 21.3 % 82.8 % 67.3 %</cell></row><row><cell>Diehl (2019)</cell><cell>GCNN</cell><cell cols="2">1.0 M 12.9 % 16.3 % 72.5 % 57.9 %</cell></row><row><cell>Derevyanko et al. (2018)</cell><cell>3D CNN</cell><cell cols="2">6.0 M 31.6 % 45.4 % 92.5 % 78.8 %</cell></row><row><cell cols="2">Gligorijevic et al. (2019)  *  LSTM+GCNN</cell><cell cols="2">6.2 M 15.3 % 20.6 % 73.2 % 63.3 %</cell></row><row><cell>Baldassarre et al. (2020)</cell><cell>GCNN</cell><cell cols="2">1.3 M 23.7 % 32.5 % 84.4 % 60.8 %</cell></row><row><cell>Ours</cell><cell></cell><cell cols="2">9.8 M 45.0 % 69.7 % 98.9 % 87.2 %</cell></row><row><cell></cell><cell></cell><cell></cell></row></table><note>; InConvC (•) uses only intrinsic distance τ i1 ; InConvH (•) uses only intrinsic distance τ i2 ; InConvCH (•) makes use of both intrinsic distances at the same time; Ours3DCH (•) uses both * Pre-trained unsupervised on 10-31 million protein sequences.† Pre-trained on several supervised tasks with structural information.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Study of ablations (rows) for the FOLD and REACTION tasks (columns).</figDesc><table><row><cell></cell><cell></cell><cell>FOLD</cell><cell>REACT</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Fold Super. Fam.</cell><cell></cell><cell>50</cell><cell cols="2">Fold task</cell></row><row><cell></cell><cell>•GCNN</cell><cell cols="2">25.7 % 46.5 % 95.9 % 84.9 %</cell><cell></cell><cell></cell></row><row><cell>Conv.</cell><cell>•ExConv •InConvC •InConvH</cell><cell cols="2">30.1 % 46.3 % 92.0 % 85.0 % 37.6 % 65.1 % 98.7 % 85.4 % 40.8 % 62.0 % 98.4 % 85.5 %</cell><cell>20 88</cell><cell cols="2">Reac�on task</cell></row><row><cell></cell><cell>•InConvCH</cell><cell cols="2">43.5 % 66.7 % 98.7 % 85.2 %</cell><cell></cell><cell></cell></row><row><cell></cell><cell>•Ours3DCH •Ours</cell><cell cols="2">40.7 % 62.2 % 98.1 % 85.8 % 45.0 % 69.7 % 98.9 % 87.2 %</cell><cell>84</cell><cell></cell></row><row><cell></cell><cell>•CovNeigh</cell><cell cols="2">27.2 % 41.5 % 92.3 % 41.6 %</cell><cell>47</cell><cell>Fold</cell><cell>90</cell><cell>Reac�on</cell></row><row><cell>Neighbors</cell><cell>•HyNeigh •Ours •NoPool</cell><cell cols="2">33.3 % 50.6 % 96.9 % 56.9 % 45.0 % 69.7 % 98.9 % 87.2 % 37.1 % 59.8 % 97.6 % 84.7 %</cell><cell>25 47</cell><cell></cell><cell>40 Fold Task</cell></row><row><cell>Pool</cell><cell>•GridPool •TopKPool •EdgePool</cell><cell cols="2">28.6 % 41.8 % 91.8 % 86.1 % 40.7 % 65.4 % 98.4 % 84.5 % 44.4 % 69.6 % 99.0 % 86.9 %</cell><cell>25 88</cell><cell cols="2">Reac�on Task</cell></row><row><cell></cell><cell cols="3">•RosettaCEN 41.7 % 66.5 % 98.9 % 86.5 %</cell><cell></cell><cell></cell></row><row><cell></cell><cell>•Ours</cell><cell cols="2">45.0 % 69.7 % 98.9 % 87.2 %</cell><cell>84</cell><cell></cell></row><row><cell>Repr.</cell><cell cols="3">•AminoGraph 39.6 % 64.7 % 99.1 % 85.3 % •Ours 45.0 % 69.7 % 98.9 % 87.2 %</cell><cell>88 85</cell><cell>Fold</cell><cell>88 85</cell><cell>Reac�on</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>1.1].</figDesc><table><row><cell>D.2 RAO ET AL. 2019, BEPLER &amp; BERGER 2019, ALLEY ET AL. 2019</cell></row><row><cell>Table 4: Optimal batch size for the models</cell></row><row><cell>from Rao et al. (2019), Bepler &amp; Berger</cell></row><row><cell>(2019), and Alley et al. (2019).</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Functional evolution of two subtly different (similar) folds</title>
		<author>
			<persName><forename type="first">Vishal</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><surname>Radha Kv Kishan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Structural Biology</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A minimal sequence code for switching protein structure and function</title>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">A</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yihong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Orban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">N</forename><surname>Bryan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Proceedings of the National Academy of Sciences</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unified rational protein engineering with sequence-based deep representation learning</title>
		<author>
			<persName><forename type="first">Ethan</forename><forename type="middle">C</forename><surname>Alley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grigory</forename><surname>Khimulya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surojit</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Alquraishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">M</forename><surname>Church</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Methods</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Basic local alignment search tool</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Altschul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">W</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Molecular Biology</title>
		<imprint>
			<biblScope unit="volume">215</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="403" to="410" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">En-zyNet: enzyme classification using 3D convolutional neural networks on spatial representation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Amidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Amidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vlachakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Megalooikonomou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Paragios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zacharaki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06017</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Continuous distributed representation of biological sequences for deep proteomics and genomics</title>
		<author>
			<persName><forename type="first">E</forename><surname>Asgari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R K</forename><surname>Mofrad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<author>
			<persName><forename type="first">F</forename><surname>Baldassarre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Hurtado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Elofsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GraphQA: Protein Model Quality Assessment using Graph Convolutional Networks</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning protein sequence embeddings using information from structure</title>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Bepler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bonnie</forename><surname>Berger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on learning representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The Protein Data Bank</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Westbrook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gilliland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Weissig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">N</forename><surname>Shindyalov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Bourne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Protein length in eukaryotic and prokaryotic proteomes</title>
		<author>
			<persName><forename type="first">Luciano</forename><surname>Brocchieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Karlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3390" to="3400" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A note on over-smoothing for graph neural networks. ICML 2020 Graph Representation Learning workshop</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusu</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Revolutionary cryo-em is taking over structural biology</title>
		<author>
			<persName><forename type="first">E</forename><surname>Callaway</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature News</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">URL paperswithcode.com/sota/ graph-classification-on-dd</title>
		<imprint>
			<date type="published" when="2020-10-01">1.10.2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">SIFTS: updated Structure Integration with Function, Taxonomy and Sequences resource allows 40-fold increase in coverage of structure-based annotations for proteins</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gutmanas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>O'donovan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Velankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Research</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep convolutional networks for quality assessment of protein folds</title>
		<author>
			<persName><forename type="first">Georgy</forename><surname>Derevyanko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergei</forename><surname>Grudinin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lamoureux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page" from="4046" to="4053" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Transferable coarse grain nonbonded interaction model for amino acids</title>
		<author>
			<persName><forename type="first">R</forename><surname>Devane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shinoda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Chemical Theory and Computation</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Edge contraction pooling for graph neural networks</title>
		<author>
			<persName><forename type="first">Frederik</forename><surname>Diehl</surname></persName>
		</author>
		<idno>arxiv:1905.10990</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distinguishing enzyme structures from non-enzymes without alignments</title>
		<author>
			<persName><forename type="first">D</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">J</forename><surname>Dobson</surname></persName>
		</author>
		<author>
			<persName><surname>Doig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Molecular Biology</title>
		<imprint>
			<biblScope unit="volume">330</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="771" to="783" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The Pfam protein families database in 2019</title>
		<author>
			<persName><forename type="first">Sara</forename><surname>El-Gebali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaina</forename><surname>Mistry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Bateman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><forename type="middle">R</forename><surname>Eddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurélien</forename><surname>Luciani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><forename type="middle">C</forename><surname>Potter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matloob</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorna</forename><forename type="middle">J</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gustavo</forename><forename type="middle">A</forename><surname>Salazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alfredo</forename><surname>Smart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L L</forename><surname>Erik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Layla</forename><surname>Sonnhammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisanna</forename><surname>Hirsh</surname></persName>
		</author>
		<author>
			<persName><surname>Paladin</surname></persName>
		</author>
		<author>
			<persName><surname>Damiano Piovesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C E</forename><surname>Silvio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">D</forename><surname>Tosatto</surname></persName>
		</author>
		<author>
			<persName><surname>Finn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Research</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Prottrans: Towards cracking the language of life&apos;s code through self-supervised deep learning and high performance computing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Elnaggar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Heinzinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dallago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rihawi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gibbs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Feher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Angerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Steinegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bhowmik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rost</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Protein interface prediction using graph convolutional networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shariat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ben-Hur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Graph U-nets</title>
		<author>
			<persName><forename type="first">Hongyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2083" to="2092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Structure-based function prediction using graph convolutional networks</title>
		<author>
			<persName><forename type="first">V</forename><surname>Gligorijevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Renfrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kosciolek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Leman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vatanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Berenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">M</forename><surname>Fisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Xavier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bonneau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Flex-convolution (million-scale point-cloud learning beyond grid-worlds)</title>
		<author>
			<persName><forename type="first">Fabian</forename><surname>Groh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Wieschollek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hendrik</forename><surname>Lensch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07289</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Monte carlo convolution for learning on non-uniformly sampled point clouds</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hermosilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ritschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P-P</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vinacua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ropinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGGRAPH Asia)</title>
				<meeting>SIGGRAPH Asia)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">37</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deepsf: Deep convolutional neural network for mapping protein sequences to folds</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Adhikari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics</title>
				<meeting>the 2018 ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Generative models for graphbased protein design</title>
		<author>
			<persName><forename type="first">John</forename><surname>Ingraham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="15820" to="15831" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">DeepSite: protein-binding site predictor using 3D-convolutional neural networks</title>
		<author>
			<persName><surname>Jiménez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Doerr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A S</forename><surname>Martínez-Rosell</surname></persName>
		</author>
		<author>
			<persName><surname>Rose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">19</biblScope>
			<biblScope unit="page" from="3036" to="3042" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dictionary of protein secondary structure: pattern recognition of hydrogen-bonded and geometrical features</title>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Kabsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Sander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biopolymers: Original Research on Biomolecules</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2577" to="2614" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Primo: A transferable coarse-grained force field for proteins</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Gopal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Predeus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Feig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Chemical Theory and Computation</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Deep Learning Workshop</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>In NIP</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">DeepGOPlus: improved protein function prediction from sequence</title>
		<author>
			<persName><forename type="first">Maxat</forename><surname>Kulmanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Hoehndorf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="422" to="429" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">DeepGO: predicting protein functions from sequence and interactions using a deep ontology-aware classifier</title>
		<author>
			<persName><forename type="first">Maxat</forename><surname>Kulmanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Asif Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Hoehndorf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="660" to="668" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Efficient estimation of word representations in vector space</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Pre-training of deep bidirectional protein sequence representations with structural information</title>
		<author>
			<persName><forename type="first">Seonwoo</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seunghyun</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyun-Soo</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungroh</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">SCOP: a structural classification of proteins database for the investigation of sequences and structures</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Murzin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Brenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chothia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Molecular Biology</title>
		<imprint>
			<date type="published" when="1955">1955</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Universal self-attention network for graph classification</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Phung</surname></persName>
		</author>
		<idno>arXiv</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Extrinsic geometry of convex surfaces</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Pogorelov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1973">1973</date>
			<publisher>American Mathematical Soc</publisher>
			<biblScope unit="volume">35</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">PointNet: Deep learning on point sets for 3D classification and segmentation</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><surname>Guibas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017a</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">PointNet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2017">2017b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Protein-ligand scoring with convolutional neural networks</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Ragoza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Hochuli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elisa</forename><surname>Idrobo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jocelyn</forename><surname>Sunseri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koes</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Chemical Information and Modeling</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="942" to="957" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Evaluating protein transfer learning with tape</title>
		<author>
			<persName><forename type="first">Roshan</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Canny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><forename type="middle">S</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Improved protein structure prediction using potentials from deep learning</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jumper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Židek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W R</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bridgland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Penedones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Crossan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">T</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Assembly of protein tertiary structures from fragments with similar local sequences using simulated annealing and bayesian scoring functions</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">T</forename><surname>Simons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kooperberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Molecular Biology</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Udsmprot: universal deep sequence models for protein classification</title>
		<author>
			<persName><forename type="first">Nils</forename><surname>Strodthoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Wenzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Kpconv: Flexible and deformable convolution for point clouds</title>
		<author>
			<persName><forename type="first">Hugues</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Emmanuel</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beatriz</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">François</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6411" to="6420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Wasserstein weisfeiler-lehman graph kernels</title>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Togninalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elisabetta</forename><surname>Ghisu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felipe</forename><surname>Llinares-López</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bastian</forename><surname>Rieck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">End-to-end learning on 3D protein structure for interface prediction</title>
		<author>
			<persName><forename type="first">Raphael</forename><surname>Townshend</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishi</forename><surname>Bedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patricia</forename><surname>Suriana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><surname>Dror</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="15642" to="15651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Compound-protein interaction prediction with end-to-end learning of neural networks for graphs and sequences</title>
		<author>
			<persName><forename type="first">Masashi</forename><surname>Tsubaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kentaro</forename><surname>Tomii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Sese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><forename type="middle">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="309" to="318" />
			<date type="published" when="2008">2018. 2008</date>
		</imprint>
	</monogr>
	<note>Bioinformatics</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A tutorial on spectral clustering</title>
		<author>
			<persName><forename type="first">Ulrike</forename><surname>Von</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luxburg</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and computing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="395" to="416" />
			<date type="published" when="1992">2007. 1992. 1992</date>
			<publisher>Academic Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Pointconv: Deep convolutional networks on 3d point clouds</title>
		<author>
			<persName><forename type="first">Wenxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fuxin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9621" to="9630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Deformable filter convolution for point cloud reasoning</title>
		<author>
			<persName><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.13079</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Learned protein embeddings for machine learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bedbrook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Arnold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4800" to="4810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">An end-to-end deep learning architecture for graph classification</title>
		<author>
			<persName><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marion</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conference on Artificial Intelligence</title>
				<meeting>AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4438" to="4445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengwei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Can</forename><surname>Wang</surname></persName>
		</author>
		<title level="m">Hierarchical graph pooling with structure learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Learning metrics for persistence-based summaries and applications for graph classification</title>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusu</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
