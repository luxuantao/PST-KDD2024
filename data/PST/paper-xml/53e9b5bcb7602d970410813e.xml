<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SUIF Explorer: An Interactive and Interprocedural Parallelizer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shih-Wei</forename><surname>Liao</surname></persName>
							<email>sliao@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Systems Laboratory</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Amer</forename><surname>Diwan</surname></persName>
							<email>diwan@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Systems Laboratory</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Robert</forename><forename type="middle">P</forename><surname>Bosch</surname><genName>Jr</genName></persName>
							<email>bosch@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Systems Laboratory</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Anwar</forename><surname>Ghuloum</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Systems Laboratory</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Monica</forename><forename type="middle">S</forename><surname>Lam</surname></persName>
							<email>lam@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Systems Laboratory</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SUIF Explorer: An Interactive and Interprocedural Parallelizer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">927E75943CFE9749CF93FE234490F9C4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T03:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The SUIF Explorer is an interactive parallelization tool that is more effective than previous systems in minimizing the number of lines of code that require programmer assistance. First, the interprocedural analyses in the SUIF system is successful in parallelizing many coarse-grain loops, thus minimizing the number of spurious dependences requiring attention. Second, the system uses dynamic execution analyzers to identify those important loops that are likely to be parallelizable. Third, the SUIF Explorer is the first to apply program slicing to aid programmers in interactive parallelization. The system guides the programmer in the parallelization process using a set of sophisticated visualization techniques. This paper demonstrates the effectiveness of the SUIF Explorer with three case studies. The programmer was able to speed up all three programs by examining only a small fraction of the program and privatizing a few variables.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>parallelizable, as it is often necessary to modify the data structures of the program, such as changing global arrays into private copies on each processor. Manual transformation errors can cause race conditions which are hard to track down.</p><p>A solution that combines the advantages of both automatic and manual parallelization is to incorporate users' input into the parallelization process <ref type="bibr" target="#b1">[1]</ref>[2] <ref type="bibr" target="#b11">[11]</ref> <ref type="bibr" target="#b12">[12]</ref>. A common model is for the user to provide information in an a priori fashion, in the form of explicit directives to the compiler to perform or ignore some compiler analysis or optimization <ref type="bibr" target="#b12">[12]</ref>. This requires that the application programmer have relatively deep knowledge of the compiler. Another model is for the compiler to make the analysis results available to the programmer in an interactive manner <ref type="bibr" target="#b11">[11]</ref>. The user can then modify the source code and/or add directives as he or she examines the compilation result. Experience with earlier systems suggests the importance of coupling compiler analyses with dynamic program profilers. This approach has been adopted in more recent systems <ref type="bibr" target="#b1">[1]</ref> <ref type="bibr" target="#b2">[2]</ref>. In addition, it is found that more powerful compiler analyses are necessary to help the programmer find coarse-grain parallelism, and that the programmer needs guidance in choosing the proper program transformations.</p><p>The SUIF Explorer is a new interactive parallelizer designed with the goal of enabling programmers without compiler expertise to parallelize a program with minimum effort. We achieve this through three means:</p><p>1. Deep program analysis. Deep program knowledge is a prerequisite to an effective interactive parallelizer as it minimizes the mundane work that the programmer must perform. The SUIF Explorer is based on the SUIF parallelizing compiler <ref type="bibr" target="#b10">[10]</ref> <ref type="bibr" target="#b24">[24]</ref>, which is a state-of-the-art interprocedural parallelizer. Its repertoire of analyses includes array privatization, which is one of the techniques critical to eliminating spurious dependences. The SUIF Explorer also includes a set of dynamic execution analyzers that can provide valuable run-time information to the programmer. It can pinpoint the loops that dominate the execution and thus deserve attention. It can also locate loops that are potentially parallelizable by detecting if they have loop-carried dependences in sample runs of the program.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Exploiting coarse-grain parallelism is critical to getting good performance on multiprocessors. It has been demonstrated that interprocedural analysis can automatically locate large outer parallel loops that span hundreds of lines of code. However, interprocedural parallelization is fragile, as a single dependence in a large, otherwise parallel, loop can ruin the program's parallel performance. Even a compiler that included every single conceivable parallelization technique would be inadequate, because compilers are fundamentally limited by the sequential semantics originally coded into a program. It often requires application-specific knowledge to modify the algorithm to make it parallelizable.</p><p>Finding coarse-grain parallelism in legacy codes manually, on the other hand, is difficult. It requires the programmer to fully understand many lines of code. It is generally not a matter of simply declaring a particular loop to be dynamic information derived from the compiler and the execution analyzers, focuses the users on the important loops, and asks pertinent questions about the program that are critical to parallelization. The intention is that the programmer only needs to know about how the program works rather than the details of effective parallelization. The Guru communicates with the programmer using the Rivet visualization system which displays the information at different levels of detail using different visual metaphors.</p><p>3. Assistance with user inputs via interprocedural slicing.</p><p>Our experience with the system is that the user often makes costly mistakes when attempting to parallelize the code. The SUIF Explorer uses the concept of program slicing <ref type="bibr" target="#b23">[23]</ref> to make the user's parallelization process less error prone. A program slice of an expression is defined as a subset of statements that may potentially affect the value of the expression. By presenting the programmer with the program slice that affects the accesses in a data dependence relationship found by the compiler, the system focuses the programmer's attention on fewer lines of codes and thus reduces the likelihood of human error. The SUIF Explorer also checks the user assertions to ensure that they do not contradict the program analysis results.</p><p>We have applied the SUIF Explorer system to several applications: MDG (a molecular dynamics model from the Perfect Club benchmarks), Arc3d (a 3-D Euler equations solver using an implicit method from NASA), and Hydro (2-D Lagrangian hydrodynamics program from Los Alamos National Laboratory). Our experience with these applications suggests that the concepts in the SUIF Explorer are effective in assisting a programmer in locating coarsegrain parallelism in a program.</p><p>The rest of the paper is organized as follows. Section 2 presents an overview of the SUIF Explorer architecture and the components in the system. Section 3 describes our slicing analysis and its use in interactive parallelization. Section 4 illustrates the SUIF Explorer parallelization process, using MDG as an example. Section 5 presents the experimental results of applying the Explorer to three applications. Section 6 discusses related work and Section 7 concludes and summarizes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">The SUIF Explorer System</head><p>The SUIF Explorer System is designed to lead the user down the same path a compiler expert might take in improving a parallel program. The main components of the system are shown in Figure <ref type="figure" target="#fig_0">1</ref>. After parallelizing the code with the SUIF compiler, the Explorer instruments the code using the dynamic tools and gathers profile data of an execution. The Parallelization Guru module analyzes the static and dynamic information to identify target loops that may be parallelizable and have the greatest potential of improving program performance once it is parallelized. It interacts with the programmer via the visualization module and focuses the programmer's attention on the target loops and the slices on the critical data accessed in those loops.</p><p>In the following, we first present a brief overview of the compiler and the execution analyzers, before presenting the details of the Parallelization Guru.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Automatic Parallelization</head><p>The SUIF compiler consists of a large number of parallelization analyses designed to find coarse-grain parallelism in a program <ref type="bibr" target="#b10">[10]</ref>. The analysis of array accesses is based on the polyhedral theory of integer programs. Array regions are represented as sets of systems of linear inequalities and general mathematical algorithms are used to precisely capture the data accesses in a program. All of the parallelization analyses can be applied across whole programs, thus allowing information to be gathered across procedural boundaries. The list of analyses implemented is as follows:</p><p>• symbolic analysis on scalar variables (loop invariants, constant propagation, induction variables and affine relationships between variables)</p><p>• dependence analysis on scalar and array variables • detection of privatizable scalar and array variables • detection of reduction operations to both scalars and regions of array variables The parallelizer uses results of these analyses to parallelize the outermost loops in the program whenever possible. The programmer only has to concentrate on the sequential loops. Furthermore, even when a compiler fails to parallelize a loop, it can often determine that many of the data structures used in the loop are either parallelizable or privatizable. With interprocedural analysis, the compiler is able to eliminate many of the spurious data dependences that limited the usability of previous systems. The programmer can thus concentrate on the remaining difficult dependences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Execution Analyzers</head><p>It is well known that most of a program's execution time is spent on a small percentage of the code. The Loop Profile Analyzer is a simple tool that helps identify the important loops that need attention. It runs a program sequentially, and determines for each loop its total execution time and its average computation per invocation. This information indicates which loops dominate the execution time and whether the computation time is spread over many different invocations of the loop. The Loop Profile Analyzer uses the compiler to insert instrumentation code into the program to record timing information at the beginning and end of each loop execution. The execution overhead of this code is rather small, and the analysis is very fast.</p><p>Our second dynamic tool, the Dynamic Dependence Analyzer <ref type="bibr" target="#b18">[18]</ref>, computes the dependences that arise during an execution of the program and determines what loops may be parallelizable. The dynamic dependence analyzer works by instrumenting the read and write accesses of the program and keeping track of the most recent write operations for each memory location in the program. It is aware of the induction variables and reduction operations found by the compiler, and will ignore dependences on these variables. It also ignores anti-dependences and can detect parallelism that requires data to be privatized. The analysis is slow, but can be very useful in locating potential parallelism in a program.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">The Parallelization Guru</head><p>The Parallelization Guru uses two quantitative metrics to guide the parallelization process:</p><p>• Parallelism coverage is defined as the percentage of total execution time spent in the parallel regions. According to Amdahl's law, the total speedup from parallelization is fundamentally limited by the fraction of time spent in sequential regions of the code: for example, if only half of the program execution is parallelized, the limit on the speedup factor is two. As a result, having a high parallelism coverage is critical.</p><p>• Parallelism granularity is defined as the average length of computation between synchronizations in the parallel regions. Due to overheads of synchronization and data communication, parallelizing fine-grain parallel loops on multiprocessors can actually result in a loss of performance. Thus, a high parallelism coverage alone does not automatically lead to better parallel performance. If most of the computation is spent in fine-grain parallel loops, we try to parallelize the enclosing loops to increase the parallelism granularity. The goal of the Guru is to increase both the coverage and granularity of the application. It presents to the programmer the coverage and granularity of the automatically parallelized code, and updates the information as new loops are parallelized. It also presents to the programmer a list of loops to parallelize. The list contains all the sequential loops that have no I/O and are not nested under some parallel loops; the loops are sorted in decreasing order of their execution time as measured by the Loop Profile Analyzer. Note that a sequential loop may be nested in another sequential loop, so parallelizing a loop may eliminate the need to parallelize an inner sequential loop. Attached to each loop is the information on whether they contain any loop-carried dynamic dependences and the number of static data dependences found by the parallelizing compiler.</p><p>The Guru then interacts with the programmer, starting with the loop at the top of the list. If the loop has many dynamic and static dependences, the user may choose not to attempt that loop. The Guru looks up each static dependence in a loop and presents the program slice that needs to be examined to the programmer. The programmer then determines if the static dependence can be ignored or if an array can be privatized. He may also choose to rewrite the code to eliminate the dependences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Visualization</head><p>The Rivet visualization environment <ref type="bibr" target="#b4">[4]</ref> is a flexible tool for the efficient creation and use of visualizations for complex systems. Rivet enables the development of visualizations using well-known tools: visual metaphors are implemented in C++/OpenGL and are configured using Tcl/Tk. Tcl is also used to combine visual metaphors, enabling sophisticated visualizations to be built from simpler components.</p><p>We provide three metaphors in Rivet to show key aspects of the code being parallelized to the user and let users control the information being displayed. This enables the Explorer to present information on large programs at multiple levels of detail. These metaphors include:</p><p>• Hyperbolic graph browser. A "focus-plus-context" graph drawing algorithm such as the hyperbolic graph viewer <ref type="bibr" target="#b17">[17]</ref> is able to display much larger graphs than traditional layout techniques. Nodes that are the focus of the current view are large and the nodes get smaller as they get further from the focus. This browser can be used to display structures such as the function call graphs of large programs. This is especially useful for very large systems with hundreds of thousands of lines of code, providing a high-level understanding of the program structure and a roadmap for navigating through the system. Because the Rivet codeview described below is effective only up to tens of thousands lines of code, the hyperbolic viewer is used to help guide navigation for larger programs by distilling the program text down to the call graph.</p><p>• Line-oriented program statistics. Inspired by the SeeSoft system <ref type="bibr" target="#b7">[7]</ref>, the Codeview metaphor provides a "bird's-eye" view of the source code. Each line of the source is displayed as a single line segment whose length is proportional to the textual length of the line. This view can be used to display attributes of the source code on a line-by-line basis, allowing the user to see information about thousands of lines of code at once. While the SeeSoft system shows mainly static information, Codeview shows both static information, such as loop nesting depth, and dynamic data, such as the amount of execution time spent on each line.</p><p>• Source code viewer. Rivet also includes an enhanced source code viewer which allows the code to be annotated with additional information through the use of color, font selection, and text. This traditional view is limited to displaying tens of lines of code at a time, but is very useful when linked with the visual metaphors above, enabling the user to see the source code associated with a region of the Codeview or a node of the call graph.</p><p>We utilized the extensibility and configurability of Rivet in developing these metaphors and in devising the interface between Rivet and the Explorer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Assertion Checkers</head><p>Our experience is that the programmer is often mistaken when making assertions on the program, and these mistakes can result in long and difficult debugging sessions. To help make this process more robust, the Explorer includes an assertion checker which uses the static and dynamic information available to try to disprove the programmer's assertion. If the user asserts that two references are independent, the Explorer checks the information against the Dynamic Dependence Analyzer to determine if any true dependence has been observed for the user-supplied input set. If the user asserts that a global array needs to be privatized in a procedure, the Explorer checks if a similar assertion is provided for all other called procedures that access the same array. If it is not, it issues a warning and privatizes the array for the programmer automatically.</p><p>After checking for consistency, the Explorer inserts the annotations in the program, which are then used by the compiler to re-parallelize the code. The programmer need not change the source directly, because the compiler will automatically change global arrays into private arrays where needed and invoke the necessary run-time routines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Context-Sensitive Interprocedural Slicing</head><p>Even after the Guru narrows the parallelization question down to whether a pair of memory references in a loop is dependent, the programmer is still presented with a difficult problem. To answer the question, the programmer may have to examine the entire loop, and sometimes even code outside the loop, to identify all the statements that may affect the dependence. This examination is both labor intensive and error prone, especially if the loop bodies contain procedure calls. We have developed an interprocedural program slicing analysis that automatically reduces the code to only a small number of relevant statements. In the following, we first motivate the need for slicing with an example, then we describe how slicing can be used for interactive parallelization, and finally we describe the slicing algorithm itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Motivation with a Real-Life Example</head><p>The use of slicing was motivated by a real-life experience a user had when he tried to parallelize an application that computes asset allocations in a portfolio. Figure <ref type="figure" target="#fig_1">2</ref> shows an excerpt from a 96-line loop that includes 4 procedure calls. The compiler could not parallelize this loop because of the dependences of array XPS. In his eagerness to speed up the application, the programmer declared that the XPS array is privatizable. The reasoning was that since line 2349 writes XPS[1:NLS] before it is read by line 2356, there is no loop-carried dependence if every processor gets its private copy of the array. This information enabled the compiler to parallelize the loop, but unfortunately the parallelized code did not run correctly. This was a costly mistake and it took the programmer many hours to realize that the array is actually not privatizable. He did not notice that statement 2320 can cause the control flow to bypass the statement 2349. As a result the data read by line 2356 in some iterations were written by a previous iteration of the loop, and therefore the array cannot be privatized. This example illustrates how easy it is to make a mistake when trying to determine if a data dependence exists in a code. The concept of a program slice helps alleviate this problem by distilling out the code that must be analyzed. As explained below, the program slicer will highlight exactly those lines shown in the excerpt. The programmer can then easily make the right inference when presented with these ten lines of code, rather than the 96-line loop in its entirety.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Slices and Their Use in Interactive Parallelization</head><p>The program slice of a reference is the set of operations that contribute to the value of the reference. A common technique for computing program slices is to transitively follow all the control and data dependence edges originating from the reference being sliced. The data slice of a reference is a subset of the program slice in which only the data dependence edges, and not the control dependence edges, are followed. The control slice, also a subset of the program slice, is all the operations that affect the conditions under which a reference is executed. It is computed as the immediate control dependences of the reference plus the program slices of the expressions upon which the reference is control dependent. As illustrated by the example above, proving data independence between a pair of array accesses requires us to know the locations that are read and written, and the condition under which the accesses are performed. The program slices of the array index expressions specify the locations accessed, and the control slices of the accesses specify when the accesses are performed. When presented with these slices, the programmer can use the following procedure to determine if there is a dependence:</p><p>1. Analyze the program slices of the array indices to determine the ranges of the array indices read and written. If they do not intersect, or if the location read in an iteration is written only in that iteration, there is no loop-carried dependence that prevents parallelization.</p><p>2. Analyze the program and control slices of the accesses to see if the data read in an iteration have been written earlier in that same iteration. If so, the array is privatizable. In the example above, the write operation depends on the condition in line 2320 whereas the read operation does not. The read is not always preceded by a write in the same iteration, and the array is therefore not privatizable.</p><p>We found that program and control slices of a reference can get quite large and have thus created the notion of slice pruning to help programmers cope with large slices. The idea is to prune the slice computation at those nodes that are unlikely to yield useful information for proving data independence. We call these terminal nodes and highlight them in the display to remind the programmer not to assume anything about the contents of those nodes. Slice pruning keeps the size of the displayed slice small, and the programmer can expand any of these terminal nodes if they so desire. We have found two useful forms of pruning:</p><p>• Array-restricted slices: Array contents are seldom useful for proving data independence, and slices of array accesses are usually large and imprecise. Thus, it is useful to prune the slice computation at array accesses.</p><p>• Code-region-restricted slices: The part of a program slice outside of a loop is often irrelevant to the parallelization of the loop. It is useful to prune the slice computation upon reaching nodes outside the loop. Our slicing algorithm described below can be easily parameterized to include these restrictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">The Slicing Algorithm</head><p>We have developed a demand-driven, context-sensitive interprocedural slicing algorithm. It first builds a sparse interprocedural SSA representation <ref type="bibr" target="#b5">[5]</ref> across the entire program, then on demand, computes the requested contextsensitive slice. To make the slice computation fast, we have developed the concept of slice summaries to exploit redundancy in the calculation of slices and an efficient representation to make set union, the most common operation in the slice computation, fast.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Interprocedural SSA Representation</head><p>Our slicing algorithm operates on an interprocedural SSA form (ISSA), which differs from traditional SSA form in two ways. First, it incorporates pointer alias information in the representation. Second, it contains additional nodes (similar in function to parameter-in and parameter-out nodes in Horwitz et al <ref type="bibr" target="#b13">[13]</ref>) to connect the intraprocedural form of procedures together into a global program graph. The rest of this section first describes how we incorporate aliasing information into our SSA form for C and Fortran programs, then describes how we connect the intraprocedural graphs together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1.1">Alias Information for C Programs</head><p>The pointer alias analysis we use is Steensgaard's flow and context-insensitive alias analysis that executes in almost linear time <ref type="bibr" target="#b21">[21]</ref>. The analysis partitions all the references into alias equivalence classes. To incorporate the alias information in our SSA form, we assign a new alias variable to each alias equivalence class. We then substitute every reference in the program with the alias variable representing the equivalence class to which the reference belongs. Assignments to alias variables representing non-singleton equivalence classes are treated as weak updates. That is, we introduce a node that combines the original content of the variable with the new value. This is similar to Cytron's approach <ref type="bibr" target="#b6">[6]</ref>, except that nodes are used instead of Cytron's IsAlias function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Consider the following example:</head><p>In this example, the pointer alias analysis finds variables a and b to be aliased. Variable av1 represents the alias equivalence class consisting of a and b, and av2 represents p. Since av1 represents two variables (a and b), all the right-hand-side expressions that assign to av1 are functions, indicating that the variables represented may either retain their old values or be assigned new values.</p><p>Once the code has been rewritten as shown above, alias variables behave just like regular program variables. For the rest of the paper, we will refer to alias variables as simply variables.</p><p>We have extended Steensgaard's technique to improve its ability to locate strong updates in the program. We further partition each alias equivalence class so that direct reads and writes to individual scalar variables are placed in their own subclasses, and the rest are placed in the "alias" subclass. As we create the SSA form, direct writes to a scalar variable are treated as strong updates on that variable and weak updates for the alias subclass, whereas writes to the alias subclass are treated as weak updates for all the variables in the same equivalence class. This improved accuracy is helpful in keeping slices small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1.2">Alias Information for FORTRAN</head><p>Although Fortran 77 does not have pointers, aliases are possible due to the use of common blocks and passing parameters by reference. The former is handled separately by a simple pass that identifies all the overlapping common blocks. The Fortran standard says that even if parameters or global variables may be aliased, the value of a variable is undefined if it has been modified by an assignment to an alias of that variable. Thus, we can simply model Fortran's parameter passing convention by assigning the actual parameters to the formal parameters before a call and assigning the formals to the actuals after the call (known as copy-in/copy-out).</p><p>Our algorithm does not distinguish between different elements in an array. We assume that any reference to an array element accesses the entire array and any store to an array element potentially modifies the entire array. We handle assignments to array elements in the same way we handle weak assignments in C. We have not found the need for more detailed array information so far in our experience with our system, and slicing using array data dependence information is outside the scope of this paper. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1.3">Building an Interprocedural SSA Graph</head><p>We build the interprocedural SSA graph by introducing new functions to capture the semantics of parameter passing. Once these functions are inserted, a standard SSA technique can be used to find the interprocedural SSA graph.</p><p>Our first step is to apply an interprocedural analysis to find, for each procedure, all the variables that are modified or referenced by the procedure and its callees. They may be either global variables or variables accessed via pointer dereferences in the case of C programs. We handle these variables as if they were parameters of the procedure. All variables that are potentially modified in the procedure are treated as return values.</p><p>To capture the semantics of passing a parameter into a procedure, we introduce an explicit assignment statement that initializes the parameter with a function that combines all the corresponding actual parameters passed to that procedure, one from each potential caller. Similarly, to capture the semantics of return values, we introduce a statement that assigns the formal return variable to the actual return variable. The right hand side of this assignment will be a node if the call site has multiple callees. Once these assignments are introduced, we compute the minimal SSA form for the whole program using the concept of iterated dominance frontiers <ref type="bibr" target="#b4">[4]</ref>. In SSA form, there is exactly one assignment to each variable. SSA form accomplishes this by creating a new version of a variable (SSA variable) at each assignment to the variable and adding additional assignments to ensure that only one version of a variable reaches any use of the variable.</p><p>The example in Figure <ref type="figure" target="#fig_3">3</ref> shows a Fortran code before and after the ISSA transformation. Procedure R has one formal parameter f. The global variables G and H used in procedures P and Q, respectively, are also treated as parameters. A pair of assignments, one at the beginning of each procedure and one after each call site, are introduced to model the "copy-in-copy-out" parameter passing semantics.</p><p>In addition, we augment the ISSA graph with control dependence edges, which are necessary for calculating program and control slices.  </p><formula xml:id="formula_0">φ φ φ G 2 =G 0 G 3 =1 call R(G 3 ) G 4 =f 1 ...=G 4 H 2 =H 0 H 3 =2 call R(H 3 ) H 4 =f 1 G 0 =0 H 0 =0 call P() G 1 =G 4 call Q() H 1 =H 4 f 0 = (G 3 ,H 3 ) f 1 =f 0 +1 φ Main P() R(f) Q()<label>(b)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2.1">Slice Summaries</head><p>We observe that even context-sensitive slices involving calls to the same procedure share many common statements. Consider two program slices of a return value from the same procedure p at two different call sites. Each of the slices has two portions: the set of statements executed in an invocation of p that contributes to the return value, and the slices of the actual parameters passed into p that contribute to the return value. It is the latter part that is context sensitive. The former set, on the other hand, is identical in both cases and is independent of the calling context, and it includes statements in p as well as statements in the callees of p. Thus, we only need to compute this set once and reuse it for different invocations to the same procedure. This optimization is just another example of path compression used in interval analysis.</p><p>We define the notion of a slice summary to capture this concept. A slice summary, SS r , of a reference r, in a procedure p is a tuple of two elements &lt;T, F&gt;: T is the call subslice, which is the set of statements in p and its callees that contribute to the value of r, and F is the upwardsexposed uses with respect to r, which is the set of formal parameters of the function p that r depends on. That is, the slice of r is the union of its call subslice and the slices of all the actual parameters passed to the upwards-exposed uses in p with respect to r.</p><p>The data slice summary of a reference can be calculated using (EQ 1). We define the union of two slice summaries to be the component-wise union of the tuples. The function GetActual returns the actual parameter passed to a formal variable at a call site. If the reference is neither a formal nor a return value at a call site, its slice summary is the union of the slice summaries of all the operands in its definition with the addition of the definition of the reference in the call subslice. If the reference is a formal parameter, its slice summary simply consists of an empty call subslice and a singleton set containing itself as the upwards-exposed use component. If the reference represents a return value at a call site, its definition is a node whose operands are formal return variables, one for each potential callee. We first find the slice summaries of all the formal return variables in the callees. The slice summary of the reference consists of the union of the slice summaries of all the actual parameters passed to the upwards-exposed formal parameters of the callees, plus all the call subslices in the callees' slice summaries.</p><p>We can compute the control and program slice summaries in a similar manner by adding control dependence edges to the above equations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2.2">Algorithm to Find Slices</head><p>We are now ready to define slices formally in terms of slice summaries. Let SS r = &lt;T, F&gt; be the slice summary of a reference r, and C r be the set of call sites that invoke the procedure to which r belongs, That is, the slice of r in procedure p is recursively defined as the call subslice of r plus the union of all the slices of the actual parameters supplied to an upwards-exposed formal variable of r in any potential site that calls procedure p.</p><p>Here a slice is defined to include all the relevant statements in all the possible paths that lead to the reference of interest. It is sometimes useful to find the slices with respect to some constraints on the execution path of interest. For example, in a debugging session, we may wish to ask for the slice given a particular call stack value. To capture this notion, we define a slice of a reference r with respect to a calling context C, denoted as Cslice(r, C), as follows. Let C=[c 1 , ..., c n ] be the call sites currently on the stack, with c n being the one on the top of the stack. Let SS r = &lt;T, F&gt; be the slice summary of a reference r, That is, the computation of context-specific slices only tracks the slices up the chain of calls on the stack.</p><p>Our demand-driven slice computation accepts a request for a slice, with or without a specific calling context, and uses a primarily recursive descent algorithm based on the equations above. The results of all slice summaries for every program point are memoized as they are computed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Note that the equations above form a recurrence if recursive</head><formula xml:id="formula_1">φ Slice r ( ) T Slice GetActual f c , ( ) ( ) ċ C r ∈ f , F ∈ ∪ ∪ = Cslice r c 1 …c n [ ] , (<label>)</label></formula><formula xml:id="formula_2">T Cslice GetActual f c n , ( ) c 1 …c n 1 - [ ] , ( ) f F ∈ ∪ ∪ =</formula><p>procedure calls or loops are present, and we need to find the fixed point solution to the equations. Our algorithm locates the recurrences by using a stack to keep track of the slice summaries being constructed. When a recurrence is detected-if a slice summary to be computed is already on the stack-the algorithm simply uses the approximate slice summary found so far. If an approximate summary is used in the computation of another slice summary, we say that the latter depends on the former. All such dependence relationships are recorded. When the approximate slice summary is finalized, its dependent summaries are placed on a worklist. The algorithm finds the fixed point solution by iteratively removing a summary from the worklist, recomputing it, and adding new dependents on the worklist if the result changes. The fixed point computation terminates when the worklist is empty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2.3">Hierarchical Slice Representation</head><p>Not only is there redundancy in the calculation of slices at the procedural level, there is also redundancy at the statement level. Namely, a slice of a reference includes the slices of all its definition's operands. We use a hierarchical set representation to capitalize on the redundancy between sets of statements in slices and slice summaries.</p><p>We represent a set of statements by a collection of subsets of statements plus additional individual statements. Graphically, a set is represented as a node, labeled by the additional statements, and whose directed edges point to its subsets. Thus a union operator between two nodes can be performed by simply creating a new node that points to the operands. The graph created in our slicing algorithm can be cyclic; two variables can depend on each other mutually causing the slices to include each other. All elements in a strongly connected component have the same value. We remove the redundancy and simplify the graph by reducing all the strongly connected components to a single node. Edges that used to point to nodes in the strongly connected components now point to the new node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">A Case Study</head><p>In this section, we illustrate the experience of using the SUIF Explorer with the MDG benchmark from the PERFECT Club. The MDG program implements a molecular dynamics model for 343 water molecules in the liquid state at room temperature and pressure. The code uses the Matsuoka-Clementi-Yoshimine configuration interaction potential for rigid water-water interactions and extends it to include the effects of intra-molecular vibration <ref type="bibr" target="#b19">[19]</ref>.</p><formula xml:id="formula_3">∅ r , 〈 〉 if r is a formal parameter T ∅ , 〈 〉 SS GetActual f D , ( ) f F ∈ ∪ ∪ T F , 〈 〉 SS v v op r ( ) ∈ ∪ = D { } ∅ , 〈 〉 SS v v op r ( ) ∈ ∪ ∪ otherwise.</formula><p>(EQ 1)</p><p>Let D be the definition of r, and op(r) be its operands, SS r = if r is a return value, and where</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Applying the SUIF Parallelizer and Execution Analyzers</head><p>SUIF Explorer starts by invoking the SUIF compiler to parallelize MDG and then runs the parallelized code on a 4processor Digital TurboLaser. The parallelized application shows no speedup over a sequential execution of the program; in fact, it actually takes slightly longer to complete. The execution analyzers are then used to characterize the parallel behavior of the application. The SUIF compiler succeeds in parallelizing 73% of the computation, but, by Amdahl's Law, the parallel code is fundamentally limited to less than a 2.2-times speedup. In addition, the parallel loops are very small, with each loop taking less than 0.002 millisecond on average if executed sequentially.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Finding Targets of Parallelization</head><p>The Explorer then evaluates the compiler results and profile data to find the important sequential loops that are potentially parallelizable. By far the best candidate found is the loop labeled 900 in the interf procedure (interf/ 900), a loop consisting of 109 lines and three procedure calls. This loop accounts for 90% of the total execution time of the program and each instance executes for a relatively long time, unlike its fine-grained inner loops. This means that parallelizing interf/900 will greatly improve the parallel performance of the application. In addition, the single loop-carried dependence (on array RL) reported by the compiler is not observed by the Dynamic Dependence Analyzer for the user-supplied input set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Presenting the Relevant Program Slice to User</head><p>This analysis is presented to the programmer using the codeview and the source code viewer, as shown in Figure <ref type="figure">4</ref>.</p><p>The Codeview provides a "bird's-eye" view of the MDG source, where each character in the code is represented by a pixel. The user can move the scroll bar in either viewer to control the code displayed in the source code viewer. The Explorer or the user can use a set of sliders to determine if loops should be filtered from the code view according to their loop depth, granularity and execution time. Filtered loops are shown in gray; unfiltered sequential loops are shown in black; unfiltered parallel loops are shown in white.</p><p>A white focus bar in the Codeview indicates that the interf/900 loop was selected as a good candidate for hand parallelization. The compiler cannot parallelize the loop labeled 900 because of a static dependence between the accesses of RL. Since K defines the region of the array RL accessed, the Explorer computes the array-restricted and code-region-restricted control slices of the references to K in statements 1125 and 1135 as shown in Figure <ref type="figure">5</ref>. Had the slice not been array-restricted, there would be many more nodes in the slice that contribute to the value of the array RS referenced in statement 1109.</p><p>It is easy to see that each iteration potentially reads and writes RL <ref type="bibr">[6:9]</ref>. If the read condition implies the write condition, then the array is privatizable. The control slice of the read operation on line 1135 includes all the statements shown except for the DO 1130 loop. From the slice, the programmer sees that the array elements RL <ref type="bibr">[6:9]</ref> are read only if KC equals 0, but KC equals 0 only if elements RS <ref type="bibr">[1:9]</ref> are all less than or equal to CUT2. The control slice of the write operation on line 1125 includes all the It is clear that if elements RS <ref type="bibr">[1:9]</ref> are all less than or equal to CUT2, then RL[6:9] will first be written before they are read. A programmer can easily make this inference once presented with the slices. While the Polaris compiler can privatize RL using special-purpose symbolic analysis and pattern matching <ref type="bibr" target="#b3">[3]</ref>, no automatic tool can handle every such case. Thus we feel this is a fair demonstration of slicing as a general and effective mechanism for interactive parallelization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Parallelization with User's Input</head><p>Once the programmer asserts that the array RL is privatizable, the Assertion Checker confirms that the assertion is consistent with its data and adds the annotation to the code. The annotation enables the compiler to successfully parallelize this main loop in this program, as shown in Figure <ref type="figure" target="#fig_5">6</ref>. As a result, the application achieves a 4times speedup on 4 processors and a 6-times speedup on 8 processors in a Digital TurboLaser machine.</p><p>This example illustrates that SUIF Explorer is effective in assisting the programmer in his parallelization task. By examining just a small fraction of the program, the user is able to supply a few assertions that enable the compiler to produce effective parallel code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Results</head><p>We now present more empirical data gathered from three case studies: the MDG program described above, NASA's Arc3d program and LANL's Hydro program. Table <ref type="table" target="#tab_2">1</ref> presents some high-level information about the programs and shows the performance data of automatic parallelization using the SUIF compiler. The largest program in this case is Hydro, consisting of about 13000 lines of code. The compiler is able to parallelize 73 to 89 percent of the computation, which is a respectable result; however, it only obtains speedups between 1 to 2.4 on four processors. But getting good performance would require parallelizing nearly all of the computation in coarse-grain loops. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Size of Code Requiring Intervention</head><p>The SUIF Explorer is effective if it presents only a small number of lines of code for the programmer to analyze, assuming it includes every line needed for intervention. The Explorer uses three concepts to filter out code not requiring attention: automatically parallelized loops require no attention, only important sequential loops require attention, and only the slices that contribute to static dependences require attention. We evaluate each of these aspects in turn below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Automatic Parallelization</head><p>The statistics on the number of loops parallelized automatically and manually are shown in Table <ref type="table" target="#tab_1">2</ref>. The first row shows the total number of loops that executed at least once for the given input data. The measurements are separated according to whether the loop calls other procedures: "inter" for loops that do and "intra" otherwise.</p><p>The second row shows the number of loops that have not been parallelized by the compiler. We see that the compiler manages to handle about 75% of the loops, with a higher rate of success for loops that do not call other procedures.</p><p>Altogether there are about 100 loops found to be sequential. Note that a sequential loop may be nested in another sequential loop and that it may contain parallelized inner loops.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Using Dynamic Information</head><p>The Explorer next identifies the important sequential loops that have sufficient coverage and granularity. As shown in procedure calls are found to be unimportant. The average code size of important loops with procedure calls is over 100 lines, including called procedure(s) and excluding comment lines. There are only 22 loops found to be important, and only two of these are found to carry true dependences when the code was executed. Using the Explorer, the programmer found ten loops to be parallelizable. Because parallelizing an outer loop will also execute the inner loops in parallel, parallelizing these ten loops reduces the number of important sequential loops to four.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Program Slicing</head><p>Finally, we show how slicing reduces the number of lines of codes that need to be examined by the programmer. Of the 20 important loops found to have no dynamic dependences, ten were parallelized by the programmer, two were attempted without success, and the rest were nested within the ten parallelized. For each dependence, the Explorer needs to show to the programmer two sets of slices, one for each reference sharing the data dependence relationship.</p><p>The average size of the combined slices for each dependence, measured as a percentage of the loop size, is reported for each loop in Table <ref type="table" target="#tab_4">3</ref>.</p><p>The first column in the The columns labeled "CR" report the sizes of the coderegion-restricted slices. They are much smaller than those under the "loop" column, as they are computed by pruning the slice computation at the first nodes that leave the loop. In contrast, tracking nodes outside the loop in a full slice computation may lead to more nodes inside the loop. They represent dependences between multiple invocations of the same loop, which should not prevent the parallelization of iterations within the loop. The last columns, labeled "AR", represent slices with both the code-region and array restrictions.</p><p>Our results show that full program slices and control slices can be large. Code-region restriction is successful in reducing the slices to about 11% of the loop size on average. Applying the array restriction to the code-region-restricted slices reduces them from 33% to 9% in the interf-1000 loop and has no effect on the others.</p><p>In summary, we have shown that the Explorer is successful in minimizing the size of the code that requires intervention. It focuses the programmer's attention on 20 of the 429 executed loops in the three programs, based on static and dynamic analyses of the programs. Through the use of slicing, it requires the programmer to read only about 10% of the code in each loop to resolve a data dependence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Cooperation Between the SUIF Explorer and the Programmer</head><p>Even for those loops that require user intervention, the SUIF parallelizer still plays an important role in reducing the user's effort. Table <ref type="table" target="#tab_5">4</ref> shows the number of data structures analyzed automatically and manually in the ten loops parallelized with the programmer's help. All of these loops contain procedure calls.</p><p>Arrays whose accesses in a loop do not create a loop-carried dependence are classified as parallel arrays in this table; arrays and scalar variables whose updates are commutative are classified as reduction arrays and reduction scalars, respectively.</p><p>We observe that the compiler successfully finds all 129 parallel arrays in these applications but finds only 56 of the 91 privatizable arrays. Since array privatization requires the analysis of the flow of values, whereas finding parallel arrays requires only the analysis of locations, it is not surprising that the compiler is not as successful with array privatization. Although the programmer needs to privatize 25 arrays in Hydro, the arrays are accessed in a similar manner and are subject to the same analysis. The compiler can identify almost all privatizable scalar variables and all of the reduction variables in these loops.</p><p>In summary, the compiler can parallelize the references to many data structures within the sequential loop automatically, leaving only a small amount of work to the programmer.</p><p>Finally, we show how having the user analyze a relatively small number of lines of codes pays off tremendously. As shown in Table <ref type="table" target="#tab_6">5</ref>, both the parallel coverage and granularity improve significantly for each of the applications. The improvement in parallel performance is substantial for Finally, there has been much work on program slicing <ref type="bibr" target="#b22">[22]</ref>. Slicing has been used mostly for software engineering such as software maintenance, testing, debugging. We believe we are the first to integrate demand-driven slicing into an interactive parallelizer. Prior context-sensitive approaches to slicing <ref type="bibr">[13][20]</ref> proceed in three steps. The first step builds a program dependence graph <ref type="bibr" target="#b9">[9]</ref> which makes data and control dependences explicit. The second step makes one or more passes over the dependence graph to compute edges that summarize the effects of all the calls on the dependences. To be more concrete, it links return values from a call to the actual parameters of the call. This step may take multiple iterations if the program being sliced is recursive. The final step computes the slices from the modified graph in a demand-driven manner. Others have modified this basic approach to work on value dependence graphs <ref type="bibr">[8]</ref> instead of program dependence graphs and to apply to object-oriented programs <ref type="bibr" target="#b16">[16]</ref>.</p><p>Our algorithm improves on past approaches in four ways. First, we compute both slices and slice summaries in a demand-driven fashion. Horwitz et al. <ref type="bibr" target="#b14">[14]</ref> describe how to compute summary edges in a demand-driven fashion for a class of data-flow problems. However, their approach requires constructing an exploded supergraph which has D nodes for every node in a program-wide control flow graph, where D is the number of possible data flow facts. This graph would be prohibitively big for computing summary edges for slicing. Second, our slicing algorithm can compute slices with respect to a particular calling context. This is useful in debugging and in stepping through contextsensitive slices one level at a time. Third, we compute and memoize slice summaries for all nodes and not just return values. This avoids redundant work especially when we are computing many slices. Finally, we use a hierarchical representation of slices which contributes to making our algorithm efficient in execution time and memory usage.</p><p>Prior algorithms for slicing do not specify the slice representation they use.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>This paper presents the design of the SUIF Explorer and shows that the system is effective in assisting a programmer in finding coarse-grain parallelism in sequential programs. The Explorer minimizes the lines of code requiring manual examination using three techniques: advanced interprocedural parallelization, sophisticated dynamic execution analyzers and program slicing.</p><p>The key to the Explorer's success lies in having sufficiently powerful analyses that can restrict the need for user assistance to a small number of lines of code. It is critical that the SUIF compiler parallelizes many of the loops automatically and leaves only a few unresolved dependences in the remaining sequential loops.</p><p>We show that the concept of program slicing can be applied to interactive parallelization effectively. Our contextsensitive slicing algorithm is successful in reducing the number of lines that need to be analyzed and minimizing the likelihood of human error.</p><p>MDG and Arc3D, and is notable for Hydro. The performance of Arc3D degrades as the processors increase from 4 to 8 due to poor memory performance. We show that applying memory optimizations to the user-assisted parallel code yields over an 11-times speedup on 8 processors, but this subject is beyond the scope of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Related Work</head><p>Four prior systems, the Parascope Editor (PED) <ref type="bibr" target="#b11">[11]</ref>, the dPablo browser <ref type="bibr" target="#b1">[1]</ref>, ForgeExplorer <ref type="bibr" target="#b2">[2]</ref>, and the KAP/Pro Toolset <ref type="bibr" target="#b15">[15]</ref>, support various degrees of interactive parallelization. PED displays data-dependence information to the user and provides a variety of source-to-source transformations that the user can pick to improve the code. Users have found the data dependence information to be too low-level, and they need guidance with program transforms. dPablo extends the functionality of PED by coupling runtime measurements to the source code and providing visualizations of data-access patterns. ForgeExplorer is a commercial system and it too, like PED and dPablo, displays data-dependence information. ForgeExplorer also provides visualizations of UD-chains, DU-chains and control flow in the program and helps users locate all the references to a variable and its aliases. Finally, the KAP/Pro Toolset also provides users with dynamic dependence information which is similar to that generated by our dynamic dependence analyzer. They also have tools to check the validity of users' OpenMP directives. The SUIF Explorer provides users with higher quality information than these systems because of its interprocedural analysis and higher-level tools such as program slicing. The effectiveness of the system has been demonstrated on three applications. The programmers need only examine 12 of the 429 loops in the programs; and for these 12 loops, the compiler automatically parallelizes 305 of the 343 variables used. The slicing algorithm requires the programmer to read only about 10% of the code in the loop to resolve a dependence. And finally, the MDG benchmark improves from no speedup at all to a 6-times speedup on 8 processors, Arc3d improves from 1.6 to 4.9 and Hydro improves from 2.7 to 4.3.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Components of the SUIF Explorer</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: A code excerpt illustrating the usefulness of slicing XPS(...) = ... DO 2365 S=1,N 2320 IF ((S.NE.1).AND.(S.NE.5).AND.REE) GO TO 2355 ... DO 2350 H=1,NLS ... 2349 XPS(H) = Y(H+1) 2350 CONTINUE 2355 DO 2360 JJ=1, NLS 2356 XP(S+(JJ-1)*N)=XPS(JJ) ... 2360 CONTINUE 2365 CONTINUE</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3</head><label>3</label><figDesc>Figure 3: (a) Code before and (b) code after the transformation to ISSA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 Figure 5 :</head><label>45</label><figDesc>Figure 4: (a) Rivet Codeview and (b) source code viewer displaying MDG parallelization information</figDesc><graphic coords="8,353.41,225.27,170.79,170.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Codeview of the optimized version of MDG,showing that interf/900 has been successfully parallelized.</figDesc><graphic coords="9,90.90,172.32,171.19,161.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>3.3.2 Demand-driven Slice Computation Based on ISSA</head><label></label><figDesc>After building the ISSA graph for the entire program, our program slicer accepts requests for slices and computes them on demand. The slices computed are context-sensitive, and thus do not suffer from inaccuracy due to unrealizable paths. Consider again the example from Figure3. Suppose we are interested in finding the program slice of the read operation of variable G in function P. A context-sensitive analysis will determine that the slice includes the procedure R and the assignment to G in procedure P. A contextinsensitive analysis, on the other hand, will follow the control flow backwards from procedure P into procedure R via the return edge, and from procedure R reach both procedures P and Q, picking up the assignment to W in procedure Q as part of the slice. Context sensitivity is very important to keeping the slices small.</figDesc><table><row><cell>In Section 3.3.2.1 we first introduce the concept of slice</cell></row><row><cell>summaries, which are designed to exploit redundancy of</cell></row><row><cell>slice computations at the procedural level. In Section</cell></row><row><cell>3.3.2.2 we describe our slicing algorithm. Finally, in Section</cell></row><row><cell>3.3.2.3, we describe the hierarchical slice representation we</cell></row><row><cell>use in the algorithm to exploit redundancy of slice</cell></row><row><cell>computations at the statement level.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 ,</head><label>2</label><figDesc>many of the sequential loops that have no</figDesc><table><row><cell></cell><cell>MDG</cell><cell>Arc3d</cell><cell>Hydro</cell></row><row><cell>Program</cell><cell>Molecular</cell><cell>3-D Euler</cell><cell>2-D Lagrangian</cell></row><row><cell>description</cell><cell>dynamics</cell><cell>equations</cell><cell>hydrodynamics</cell></row><row><cell></cell><cell>model</cell><cell>solver</cell><cell></cell></row><row><cell>Data set size</cell><cell cols="2">1029x1029 64x64x64</cell><cell>450x450</cell></row><row><cell>No. of lines</cell><cell>1246</cell><cell>4053</cell><cell>12942</cell></row><row><cell>Coverage</cell><cell>73%</cell><cell>89%</cell><cell>86%</cell></row><row><cell>Granularity</cell><cell cols="2">.002 msec .3 msec</cell><cell>.3 msec</cell></row><row><cell>Speedup on 4 proc.</cell><cell>1.0</cell><cell>2.1</cell><cell>2.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 : Program information and results of automatic parallelization</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell>MDG</cell><cell></cell><cell cols="2">Arc3d</cell><cell cols="2">Hydro</cell><cell>Total</cell></row><row><cell>No. of loops</cell><cell cols="6">inter intra inter intra inter intra</cell><cell></cell></row><row><cell>Executed</cell><cell>4</cell><cell>39</cell><cell cols="2">14 269</cell><cell>11</cell><cell cols="2">92 429</cell></row><row><cell>Sequential</cell><cell>2</cell><cell>8</cell><cell>6</cell><cell>36</cell><cell>11</cell><cell cols="2">46 109</cell></row><row><cell>Important</cell><cell>2</cell><cell>0</cell><cell>6</cell><cell>5</cell><cell>9</cell><cell>0</cell><cell>22</cell></row><row><cell>Important &amp; no</cell><cell>2</cell><cell>0</cell><cell>6</cell><cell>5</cell><cell>7</cell><cell>0</cell><cell>20</cell></row><row><cell>dynamic dep.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>User-parallelized</cell><cell>1</cell><cell>0</cell><cell>3</cell><cell>0</cell><cell>6</cell><cell>0</cell><cell>10</cell></row><row><cell>Remaining</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>3</cell><cell>0</cell><cell>4</cell></row><row><cell>important</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 : Number of loops requiring user intervention</head><label>2</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 : Size of slices requiring intervention, as a percentage of loop size.</head><label>3</label><figDesc>table gives the subroutine name and the label for each loop. The second column reports the number of lines in a loop, including those in the callees but excluding comment lines. Statistics are reported both for the program slices and control slices. Columns labeled "full" are the unrestricted slice sizes. Note the full program slice may include statements outside the loop and can be much larger than the number of lines in the loop. Programmers would mainly concentrate on statements inside the loop of interest, and hence, the columns labeled "loop" count only those statements in the full slice that are inside the loop.</figDesc><table><row><cell></cell><cell>No.</cell><cell cols="2">Program slice (%)</cell><cell cols="2">Control slice (%)</cell></row><row><cell></cell><cell>lines</cell><cell cols="4">full loop CR AR full loop CR AR</cell></row><row><cell></cell><cell></cell><cell></cell><cell>MDG</cell><cell></cell><cell></cell></row><row><cell>interf-1000</cell><cell cols="2">109 342</cell><cell cols="3">86 31 9 342 86 20 9</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Arc3d</cell><cell></cell><cell></cell></row><row><cell>filter3d-701</cell><cell>67</cell><cell>33</cell><cell>10 10 10</cell><cell cols="2">33 10 10 10</cell></row><row><cell>stepf3d-701</cell><cell>261</cell><cell>8</cell><cell>2 2 2</cell><cell>7</cell><cell>1 1 1</cell></row><row><cell>stepf3d-702</cell><cell>242</cell><cell>10</cell><cell>3 3 3</cell><cell>7</cell><cell>1 1 1</cell></row><row><cell>stepf3d-801</cell><cell>260</cell><cell>13</cell><cell>2 2 2</cell><cell>7</cell><cell>1 1 1</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Hydro</cell><cell></cell><cell></cell></row><row><cell cols="3">update-1000 268 220</cell><cell cols="3">41 3 3 220 41 3 3</cell></row><row><cell>vh2200-1000</cell><cell cols="2">45 1309</cell><cell cols="3">27 11 11 1309 27 11 11</cell></row><row><cell>vqterm-85</cell><cell cols="2">51 1155</cell><cell cols="3">29 22 22 1155 29 22 22</cell></row><row><cell>vsetgc-200</cell><cell cols="2">47 1253</cell><cell cols="3">36 23 23 1253 36 23 23</cell></row><row><cell>vsetuv-85</cell><cell cols="2">79 746</cell><cell cols="3">37 11 11 746 37 11 11</cell></row><row><cell>vsetuv-105</cell><cell cols="2">49 1202</cell><cell cols="3">29 10 10 1202 29 10 10</cell></row><row><cell>vsetuv-155</cell><cell cols="2">130 453</cell><cell cols="3">46 6 6 453 46 6 6</cell></row><row><cell>Average</cell><cell cols="2">134 562</cell><cell cols="3">29 11 9 561 29 10 9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 : User-assisted parallelization of 10 loops in 3 applications</head><label>4</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="4">MDG Arc3d Hydro Total</cell></row><row><cell></cell><cell>Parallel arrays</cell><cell>6</cell><cell>27</cell><cell>96</cell><cell>129</cell></row><row><cell>Auto</cell><cell>Privatizable arrays</cell><cell>8</cell><cell>19</cell><cell>29</cell><cell>56</cell></row><row><cell>matic</cell><cell>Privatizable scalars</cell><cell>26</cell><cell>48</cell><cell>42</cell><cell>116</cell></row><row><cell></cell><cell>Reduction arrays</cell><cell>3</cell><cell>0</cell><cell>0</cell><cell>3</cell></row><row><cell></cell><cell>Reduction scalars</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>1</cell></row><row><cell>User</cell><cell>Privatizable arrays</cell><cell>1</cell><cell>9</cell><cell>25</cell><cell>35</cell></row><row><cell>Input</cell><cell>Privatizable scalars</cell><cell>0</cell><cell>3</cell><cell>0</cell><cell>3</cell></row><row><cell></cell><cell></cell><cell>MDG</cell><cell>Arc3d</cell><cell cols="2">Hydro</cell></row><row><cell></cell><cell>Coverage</cell><cell>73%</cell><cell>90%</cell><cell cols="2">86%</cell></row><row><cell>Auto-matic</cell><cell>Granularity</cell><cell cols="4">0.002 msec 0.3 msec 0.3 msec</cell></row><row><cell></cell><cell>Speedup (4 proc)</cell><cell>1.0</cell><cell>2.1</cell><cell>2.4</cell><cell></cell></row><row><cell></cell><cell>Speedup (8 proc)</cell><cell>1.0</cell><cell>1.6</cell><cell>2.7</cell><cell></cell></row><row><cell></cell><cell>Coverage</cell><cell>98%</cell><cell>98%</cell><cell cols="2">94%</cell></row><row><cell>With User</cell><cell>Granularity</cell><cell cols="4">0.08 msec 50.9 msec 0.6 msec</cell></row><row><cell>Input</cell><cell>Speedup (4 proc)</cell><cell>4.0</cell><cell>5.4</cell><cell>3.2</cell><cell></cell></row><row><cell></cell><cell>Speedup (8 proc)</cell><cell>6.0</cell><cell>4.9</cell><cell>4.3</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 : Results of parallelization with and without user intervention</head><label>5</label><figDesc></figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>Guidance to improving program performance. Instead of simply presenting all the raw data to the programmer, our system uniquely includes an active agent, known as the Parallelization Guru, which guides a novice user through the parallelization process. It integrates the static and</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This research is supported by the ARPA and Air Force Materiel Command Contract F30602-95-C-0098, ARPA grant DABT63-94-C-0054, and the Department of Energy Contract B341491.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An integrated compilation and performance analysis environment for data parallel programs</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Adve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mellor-Crummey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Reed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Supercomputing &apos;95</title>
		<meeting>Supercomputing &apos;95<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995-11">November 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<ptr target="http://www.apri.com" />
		<title level="m">Applied Parallel Research, Documentation for Forge-Explorer Programming Tool</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Parallel programming with Polaris</title>
		<author>
			<persName><forename type="first">W</forename><surname>Blume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Doallo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Eigenmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Grout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoeflinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Padua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Paek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pottenger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rauchwerger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996-12">December 1996</date>
			<publisher>IEEE Computers</publisher>
			<biblScope unit="page" from="78" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Bosch</surname></persName>
		</author>
		<ptr target="http://www-graphics.stanford.edu/projects/rivet" />
		<title level="m">Visualization of computer systems in Rivet</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Efficiently computing static single assignment form and the control dependence graph</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cytron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ferrante</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K</forename><surname>Rosen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Wegman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">K</forename><surname>Zadeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Programming Languages and Systems</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="451" to="490" />
			<date type="published" when="1991-10">October 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Efficient accommodation of may-alias information in SSA form</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cytron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gershbein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGPLAN &apos;93 conference on programming language design and implementation</title>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="36" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">See-Soft: a tool for visualizing line oriented software statistics</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Eick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Steffen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">E</forename><surname>Sumner</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Software Engineering</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="957" to="968" />
			<date type="published" when="1992-11">November 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Practical fine-grain slicing of optimized code</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ernst</surname></persName>
		</author>
		<idno>MSR-TR-94-14</idno>
		<imprint>
			<date type="published" when="1994-07">July 1994</date>
			<publisher>Microsoft Research</publisher>
			<pubPlace>Redmond, WA</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The program dependence graph and its use in optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ferrante</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ottenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Warren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Programming Languages and Systems</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="319" to="349" />
			<date type="published" when="1988-07">July 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Detecting coarse-grain parallelism using an interprocedural parallelizing compiler</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Amarasinghe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-W</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Supercomputing &apos;95</title>
		<meeting>Supercomputing &apos;95<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995-11">November 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Experiences using the ParaScope Editor: an interactive parallel programming tool</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Harvey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mcintosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Mckinley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Oldham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Paleczny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Principles and Practices of Parallel Programming &apos;93</title>
		<meeting>the Principles and Practices of Parallel Programming &apos;93</meeting>
		<imprint>
			<date type="published" when="1993-05">May 1993</date>
			<biblScope unit="page" from="33" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">High Performance Fortran Language Specification, Version 2.0</title>
		<ptr target="ftp://softlib.rice.edu/pub/HPF" />
		<imprint>
			<date type="published" when="1997-01">January 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Interprocedural slicing using dependence graphs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Horwitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Reps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Binkley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Programming Languages and Systems</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="26" to="50" />
			<date type="published" when="1990-01">January 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Demand interprocedural dataflow analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Horwitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Reps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sagiv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the third ACM SIGSOFT Symposium on the Foundations of Software Engineering</title>
		<meeting>the third ACM SIGSOFT Symposium on the Foundations of Software Engineering</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="104" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<ptr target="http://www.kai.com" />
		<title level="m">Documentation for the KAP/ Pro Toolset</title>
		<imprint>
			<publisher>Kuck &amp; Associates Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Slicing object-oriented software</title>
		<author>
			<persName><forename type="first">L</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Harrold</surname></persName>
		</author>
		<idno>95-103</idno>
		<imprint>
			<date type="published" when="1995-03">March 1995</date>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, Clemson University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Exploring large graphs in 3D hyperbolic space</title>
		<author>
			<persName><forename type="first">T</forename><surname>Munzner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1998-08">July/August 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Software and hardware for exploiting speculative parallelism with a multiprocessor</title>
		<author>
			<persName><forename type="first">J</forename><surname>Oplinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Heine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-W</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nayfeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Olukotun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lam</surname></persName>
		</author>
		<idno>CSL-97-715</idno>
		<imprint/>
	</monogr>
	<note type="report_type">Stanford Technical Report</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Perfect: Performance evaluation for cost effective transformations report 2</title>
		<author>
			<persName><forename type="first">L</forename><surname>Pointer</surname></persName>
		</author>
		<idno>964</idno>
		<imprint>
			<date type="published" when="1990-03">March 1990</date>
			<pubPlace>Urbana-Champaign</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Illinois</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Speeding up slicing</title>
		<author>
			<persName><forename type="first">T</forename><surname>Reps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Horwitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sagiv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rosay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the second ACM SIGSOFT Symposium on Foundations of Software Engineering</title>
		<meeting>the second ACM SIGSOFT Symposium on Foundations of Software Engineering</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Points-to analysis in almost lineartime</title>
		<author>
			<persName><forename type="first">B</forename><surname>Steensgaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Symposium on Principles of Programming Languages</title>
		<imprint>
			<date type="published" when="1996-01">January 1996</date>
			<biblScope unit="page" from="32" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">A survey of program slicing techniques</title>
		<author>
			<persName><forename type="first">F</forename><surname>Tip</surname></persName>
		</author>
		<idno>CS- R9438</idno>
		<imprint>
			<date type="published" when="1994-07">July 1994</date>
		</imprint>
	</monogr>
	<note type="report_type">Report</note>
	<note>Centrum voor Wiskunde en Informatica</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Program slicing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Weiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Software Engineering</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="352" to="357" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">SUIF: An infrastructure for research on parallelizing and optimizing compilers</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Amarasinghe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tjiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-W</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hennessy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGPLAN Notices</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="31" to="37" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
