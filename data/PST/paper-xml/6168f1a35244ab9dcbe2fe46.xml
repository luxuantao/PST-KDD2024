<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RPT: Toward Transferable Model on Heterogeneous Researcher Data via Pre-Training</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-03-01">1 Mar 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ziyue</forename><surname>Qiao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yanjie</forename><surname>Fu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Pengyang</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Meng</forename><surname>Xiao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhiyuan</forename><surname>Ning</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Denghui</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yi</forename><surname>Du</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yuanchun</forename><surname>Zhou</surname></persName>
						</author>
						<title level="a" type="main">RPT: Toward Transferable Model on Heterogeneous Researcher Data via Pre-Training</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-03-01">1 Mar 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2110.07336v2[cs.IR]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Pre-Training</term>
					<term>Contrastive Learning</term>
					<term>Transformer</term>
					<term>Graph Representation Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With the growth of the academic engines, the mining and analysis acquisition of massive researcher data, such as collaborator recommendation and researcher retrieval, has become indispensable for improving the quality and intelligence of services. However, most of the existing studies for researcher data mining focus on a single task for a particular application scenario and learning a task-specific model, which is usually unable to transfer to out-of-scope tasks. In this paper, we propose a multi-task self-supervised learning-based researcher data pre-training model named RPT, which is efficient to accomplish multiple researcher data mining tasks. Specifically, we divide the researchers' data into semantic document sets and community graph. We design the hierarchical Transformer and the local community encoder to capture information from the two categories of data, respectively. Then, we propose three self-supervised learning objectives to train the whole model. For RPT's main task, we leverage contrastive learning to discriminate whether these captured two kinds of information belong to the same researcher. In addition, two auxiliary tasks, named hierarchical masked language model and community relation prediction for extracting semantic and community information, are integrated to improve pre-training. Finally, we also propose two transfer modes of RPT for fine-tuning in different scenarios. We conduct extensive experiments to evaluate RPT, results on three downstream tasks verify the effectiveness of pre-training for researcher data mining.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>W ITH the pervasiveness of digital bibliographic search engines, e.g., Google Scholar, Aminer, Microsoft Academic Search, and DBLP, efforts have been dedicated to mining scientific data, one of the main focuses is researcher data mining, which aims to mine researchers' semantic attributes and community relationships for important applications, including: collaborator recommendation <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, academic network analysis <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, and expert finding <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>.</p><p>Millions of researchers and relevant data have been added to digital bibliographic datasets every year. The scientific research achievements and academic cooperation of researchers saw a continuation in the upward trend. However, different researcher data mining tasks usually choose particular features and design unique models, and minimal works can be transferred to out-of-domain tasks. For example, previous studies <ref type="bibr" target="#b7">[8]</ref> are more inclined to use graph representation learning-based models to explore the researcher community graphs in the collaborator recommendation task. In researchers' research field classification task, their publications are more valuable features, and the semantic models are more often used. Hence, when multiple mining tasks on a tremendous amount of researcher data, it would be laborious for feature selection and computationally expensive to train different task-specific models, especially when these models need to be trained from scratch. Also, most researcher data mining tasks need labeled data, which is usually quite expensive and time-consuming, especially involving manual effort. The deficiency of labeled data makes supervised models are easily over-fitting. And the unlabelled graph data is usually easily and cheaply collected. Motivated by these, we aim to exploit the intrinsic data information disclosed by the unlabeled researcher data to train a generalized model for researcher data mining, which is transferable to various downstream tasks.</p><p>The pre-training technology is a proper solution, which has drawn increasing attention recently in many domains, such as natural language processing (NLP) <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, computer vision (CV) <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, and graph data mining <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. The idea is to first pre-train a general model to capture useful information on a large unlabeled dataset via self-supervised learning, and then, the pre-trained model is treated as a good initialization of the downstream tasks, further trained along with the downstream models for different application tasks with a few fine-tuning steps. The pre-training model is a transferable model and convenient to train because the unlabeled data is easily available. In finetuning, the downstream models can be very lightweight. Thus, this sharing mechanism of pre-training models is more efficient than independently training various taskspecific models. Early attempts to pre-training mainly focus on learning word or node representation, which optimizes the embedding vectors by preserving some similarity measure, such as the word co-occurrence frequency in texts and the network proximity in graphs, and directly uses the learned embeddings for downstream tasks. However, the embeddings are limited in preserving extensive information in large datasets. Thus, recent studies consider a model transfer setting that greatly improve the model ability of knowledge transferring <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b13">[14]</ref>. The schema is first pre-training a deep neural network-based encoder to incorporate the unsupervised information into the parameters, then fine-tuning it along with the the downstream model head (e.g, a classifier or a decoder) in an end-to-end manner.</p><p>Inspired by these improvements, we aim to conduct pretraining on researcher data for mining tasks. The study in <ref type="bibr" target="#b15">[16]</ref> has proved the effectiveness of domain data for pre-training, and some work has leveraged pre-training models on academic domain data. For example, SciBERT <ref type="bibr" target="#b16">[17]</ref> leverages BERT on scientific publications to improve the performance on downstream scientific NLP tasks. SPECTER <ref type="bibr" target="#b17">[18]</ref> introduces the citation information into the pre-training to learn document-level embeddings of scientific papers. However, existing pre-training models can not be directly applied to researcher data pre-training. Because the researcher data is more heterogeneous, including textual attributes (e.g., profiles, publications, patents, etc.) and graph-structured community relationships (e.g., collaborating, advisor-advisee, etc.). In contrast, most present models can only deal with a specific type of data. Also, to extract information from the heterogeneous researcher data, the pre-training model should be more complex and heavyweight than traditional ones, which brings new challenges to the pre-training of models on large-scale data.</p><p>In this paper, we leverage the idea of multi-task learning and self-supervised learning to design the Researcher data Pre-Training(RPT) model. Specifically, for each researcher, we propose a hierarchical Transformer as the textual en-coder to capture semantic information in researcher textual attributes. We also propose a linear local community sampling strategy and an efficient graph neural network (GNN) based local community encoder to capture the local community information of researchers. To leverage optimization on these two encoders, the main task of RPT is a contrastive learning model to discriminate whether these two captured information belongs to the same researcher. We also design two auxiliary tasks, Hierarchical Masked Language Model (HMLM) and Community Relation Prediction (CRP), respectively, to extract token-level semantic information and link-level relation information to improve the fine-grained performance of pre-training. Finally, we pretrain RPT on big unlabeled researcher dataset extracted from DBLP, ACM, and MAG digital library. We set two transfer modes for RPT and fine-tune RPT on three researcher data mining tasks including researcher classification, collaborator prediction, and top-k researcher retrieval to evaluate the effectiveness and transferability of RPT. We also conduct ablation studies and hyper-parameters sensitivity experiments to analyze the underlying mechanism of RPT. The contributions of our paper are summarised as follows:</p><p>1) We introduce the pre-training idea to handle the multiple mining tasks on abundant and heterogeneous researcher data. We propose the RPT framework to extract and transfer useful information from big unlabeled scientific data to benefit researcher data mining tasks. 2) To perform the RPT framework in considering heterogeneity, generalization, scalability, and transferability, we propose Transformer, GNN based information extractor and a multi-task self-supervised learning objective to capture both the semantic features and local community information of researchers. 3) Experimental results on real-world researcher data RPT can significantly benefit various downstream tasks. We also conduct model analysis experiments to evaluate different components and hyper-parameters of RPT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Researcher Data Mining</head><p>The increasing availability of digital scholarly data offers unprecedented opportunities to explore the structure and evolution of science <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>. Multiple data sources such as Google Scholar, Microsoft Academic, Arnet-Miner, Scopus, and PubMed cover millions of data points pertaining to researchers(also known as scholars and scientists) community and their output. Analysis and mining based on big data technology have been implemented on these data and the analyses of researchers have been a hot topic. The researcher data analysis tasks including collaborator prediction <ref type="bibr" target="#b22">[23]</ref> and recommendation <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, collaboration sustainability prediction <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b26">[27]</ref>, reviewer recommendation <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, expert finding <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, advisoradvisee discovery <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>, academic influence prediction <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, author identification <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref> etc. Mainstream works focus on mining the various academic characteristics and community graph properties of researchers, then learn task-specific researcher representations for various tasks. For example, here are some recent representative works, <ref type="bibr" target="#b0">[1]</ref> recommends context-aware collaborator for researchers by exploring the semantic similarity between researchers' published literature and restricted research topics. <ref type="bibr" target="#b1">[2]</ref> use researchers' personal properties and network properties as input features to predict the collaboration sustainability. <ref type="bibr" target="#b5">[6]</ref> propose an expert finding model for reviewer recommendation, which learn hierarchical representations to express the semantic information of researchers. <ref type="bibr" target="#b37">[38]</ref> propose a network representation learning method on scientific collaboration networks to discover advisor-advisee relationships. <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref> study the problem of citation recommendation for researchers and use the generative adversarial network to integrates network structure and the vertex content into researcher representations. <ref type="bibr" target="#b41">[42]</ref> incorporate both network structures and researcher features into convolutional neural and attention networks and learn representations for social influence prediction. <ref type="bibr" target="#b42">[43]</ref> study the problem of top-k similarity search of researchers on the academic network, the content and meta-path based structure information is embedded into researcher representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Self-Supervised Learning</head><p>Self-supervised learning is a form of unsupervised learning which aims to train a pretext task where the supervised signals are obtained by data itself automatically, it can guide the learning model to capture the underlying patterns of the data. The key of self-supervised learning is to design the pretext tasks. In the area of computer vision (CV), various self-supervised learning pretext tasks have been widely exploited, such as predicting image rotations <ref type="bibr" target="#b43">[44]</ref>, solving jigsaw puzzles <ref type="bibr" target="#b44">[45]</ref>, and predicting relative patch locations <ref type="bibr" target="#b45">[46]</ref>. In natural language processing (NLP), many works propose pretext tasks based on language models, including the context-word prediction <ref type="bibr" target="#b46">[47]</ref> the Cloze task, the next sentence prediction <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref> and so on <ref type="bibr" target="#b47">[48]</ref>. For graph data, the pretext task are usually designed to predict the central nodes given node context <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b48">[49]</ref> or sub-graph context <ref type="bibr" target="#b49">[50]</ref>, or maximize mutual information between local and global graph <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b51">[52]</ref>. Recently, many works <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b55">[56]</ref> on different domains has integrated self-supervised learning with multi-task learning, i.e., joint training multiple self-supervised tasks on the underlying models, which can introduce useful information of different facets and improve the generalization performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Pre-Training Model</head><p>With the idea of self-supervised learning, pre-training models can be applied on big unlabeled data to build more universal representations that work across a wider variety of tasks and datasets <ref type="bibr" target="#b56">[57]</ref>. The pre-training models can be classified as feature-based models and end-to-end models.</p><p>Early pre-training studies are mainly feature-based, which directly parameterizes the entity embeddings and optimizes them by preserving some similarity measure. The learned embeddings are used as input features, in combination with downstream models to accomplish different tasks. For example, Word2vec <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b57">[58]</ref>, Glove <ref type="bibr" target="#b58">[59]</ref> and Doc2vec <ref type="bibr" target="#b59">[60]</ref> in NLP, which are optimized via textual context information.</p><p>Early graph pre-training models are similar to NLP, like Deepwalk <ref type="bibr" target="#b60">[61]</ref>, LINE <ref type="bibr" target="#b61">[62]</ref>, node2vec <ref type="bibr" target="#b62">[63]</ref> and metapath2vec <ref type="bibr" target="#b3">[4]</ref>, which aim to learn node embeddings to preserve network proximity or graph-based context. Differently, recent pre-training models pre-train deep neural network-based encoders and fine-tune them along with downstream models in end-to-end manner. Typical examples includes: MoCo <ref type="bibr" target="#b10">[11]</ref> and SimCLR <ref type="bibr" target="#b11">[12]</ref> for unlabeled image dataset; BERT <ref type="bibr" target="#b8">[9]</ref>, RoBERTa <ref type="bibr" target="#b63">[64]</ref> and XLNet <ref type="bibr" target="#b9">[10]</ref> for unlabeled text dataset; GCC <ref type="bibr" target="#b13">[14]</ref> and GPT-GNN <ref type="bibr" target="#b14">[15]</ref> for unlabeled graph dataset. Our proposed RPT is a meaningful attempt to apply a pre-training model on domain-specific and heterogeneous data as the researcher data is scientific data and contains researcher textual attributes and researcher community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROPOSED METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Statement and Framework Overview</head><p>To leverage pre-training models on big unlabeled researcher data, we need to design proper self-supervised tasks to capture the underlying patterns of the data. The key idea of self-supervised learning is to automatically generate supervisory signals or pseudo labels based on the data itself. Given the raw data of N researchers, denoted by A = {a 1 , a 2 , ..., a N }, we first explore the researcher data and extract two categories of researcher features: (1) semantic document set and (2) community graph, for pre-training. Semantic Document Set. A researcher may have multiple textual features, including published literature, patents, profiles, curriculum vitae(CV), and personal homepage, and these features may contain rich semantic information with various lengths and different properties. We collect the text information of these features as documents and compose these documents of each researcher together to a organized semantic document set. Formally, the semantic document set of researcher a i ∈ A is expressed as D i = {d 1 , d 2 , ..., d |Di| }, where every document is formed as a token sequence d j = {t 1 , t 2 , .., t |dj | }. Noted that each semantic document set D i and each document d j may have arbitrary lengths.</p><p>Community Graph. Besides the semantic features, the social communications and relationships are also significant for researcher and have been widely utilized to analysis in previous studies, which can be expressed as graphstructured data. As the relations between researcher may have different types, We construct the researcher community graph in a heterogeneous graph manner, expressed as G = {(a i , r i,j , a j )} ⊆ A × R × A, where (a i , r i,j , a j ) represent one link between researchers, R is the relation set, and r i,j ∈ R. Multiple types of relations between researchers are automatically extracted based on the original data to construct G. For example, we can make the rules that if two researchers coauthor a same paper, there is a relation named Collaboraing between them, if two researchers work for the same organization, there is a relation named Colleague between them. Noted that two researchers can have multiple relations of the same type in G. For instance, if they collaborate on n papers, there would be n Collaboraing relations between them.</p><p>Researcher Data Pre-Training. Formally, the problem of researcher data pre-training is defined as: given N researchers A = {a 1 , a 2 , ..., a N }, their semantic document sets D = {D 1 , D 2 , ..., D N }, and their community graph G = {(a i , r i,j , a j )} ⊆ A × R × A, we aim to pre-train a generalized model via self-supervised learning, which is expected to capture both the semantic information and the community information of researchers into low-dimensional representation space, where we hope the researchers with similar researcher topics and academic community are close with each other. Then, in fine-tuning, the learned model is treated as a generic initialized model for benefiting various downstream researcher mining tasks and is optimized by the task objectives via a few gradient descent steps. Formally, in the pre-training stages, the pre-training model is expressed as f θ = Ψ(θ; D, G) and the output is researcher representations Z = {z 1 , z 2 , ..., z N }. Let L pre be the self-supervised loss functions, which extract samples and pseudo-labels in researcher data D and G for pretraining. Thus, the objective of pre-training is to optimize the following:</p><formula xml:id="formula_0">θ pre = arg min θ L pre (f θ ; D, G)<label>(1)</label></formula><p>Based on the motivation described above, the pretraining model f θ optimized by objective L pre should have the following properties:</p><p>• Heterogeneity. Ability of encoding semantic information from multi-type textual document data and community information from heterogeneous graph data. • Generalization. Under no supervised information, the pre-training should integrate heterogeneous information from massive unlabeled data into generalized model parameters and researcher representations. • Scalability. As the researcher data is usually massive and contains rich information, the model should be heavy-weight enough to extract the information on the one hand. On the other hand, it needs to be friendly to mini-batch training and parallel computing. • Transferability. For fine-tuning on multiple tasks, the model should be compatible with researcher document features and community graph in the downstream tasks.</p><p>As such, the pre-trained model f θpre can be adopted on multiple researcher mining tasks. Noted that the focus of this work is on the practicability of the pre-training framework on the researcher data. The goal is to make the model satisfy the above properties, making it different from the common text embedding model or graph embedding model research. Figure <ref type="figure" target="#fig_2">2</ref> shows the architecture of the proposed RPT framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Hierarchical Transformer</head><p>The Transformer model is the state-of-the-art text encoder, which has demonstrated very competitive ability in combination with language pre-training models <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b63">[64]</ref>. Comparing with RNN-like models as text encoder, Transformer is more efficient to learn the long-range dependencies between words. Also, RNN-like models extract text information via sequentially propagation on sequence, which need serial operation of the time complexity of sequence length. While self-attention layers in Transformer connect all positions with constant number of sequentially executed operation. Therefore, Transformer encode text in parallel.</p><p>A Transformer usually has multiple layers. A layer of Transformer encoder (i.e, a Transformer block) consists of a Multi-Head Self-Attention Layer, a Residual Connections and Normalization Layer, a Feed Forward Layer, and another Residual Connections and Normalization Layer. The detailed explanation of the Transformer architecture can be found in <ref type="bibr" target="#b64">[65]</ref>. Given the input sequence embeddings</p><formula xml:id="formula_1">X (0) = [x (0) 1 , x (0) 2 , ..., x<label>(0)</label></formula><p>l ], after the propagation on a L-layers Transformer, formulized as X (L) = T ransf ormer(X (0) ), we can obtain the final output embeddings of this sequence</p><formula xml:id="formula_2">X (L) = [x (L) 1 , x (L) 2 , ..., x (L) l ],</formula><p>where each embedding has contained the context information in this sequence. The main hyper-parameters of a Transformer are the number of layers (i.e., Transformer blocks), the number of self-attention heads, and the maximum length of inputs.</p><p>For the semantic document set  [SOD] , then we concatenate it with the embedding sequence of d j into the Document Transformer, which is a multi-layer bidirectional Transformer, and take the final output of [SOD] as the document representation:</p><formula xml:id="formula_3">D i = {d 1 , d 2 , ...,</formula><formula xml:id="formula_4">d (0) j = [t (L D ) [SOD] , t (L D ) 1 , t (L D ) 2 , .., t (L D ) |dj | ] [SOD] = T ransf ormer D (t (0) [SOD] , t (0) 1 , t (0) 2 , .., t (0) |dj | ) [SOD]<label>(2)</label></formula><p>where d (0) j is the d j 's representation, T ransf ormer D () is the forward function of Document Transformer, L D is the number of layers, and [t</p><formula xml:id="formula_5">(L D ) [SOD] , t (L D ) 1 , t (L D ) 2 , .., t (L D ) |dj |</formula><p>] is the output. The input token embeddings are initialized by Word2vec <ref type="bibr" target="#b57">[58]</ref>, we collect all the texts in the dataset to train the Word2vec model. Also, for each token, its input representation is constructed by summing its embedding with its corresponding position embeddings in documents.</p><p>Researcher Transformer. Then, given a i 's semantic document set</p><formula xml:id="formula_6">D i = {d 1 , d 2 , ..., d |Di| }, we can obtain the documents' representations {d (0) 1 , d (0) 2 , ..., d (0)</formula><p>|Di| } from the Document Transformer and input them into the Researcher Transformer, which is yet another multi-layer bidirectional Transformer but applied on document level, followed with a mean-pooling layer: </p><formula xml:id="formula_7">u i = P ooling([d (L R ) 1 , d (L R ) 2 , .., d (L R ) |Di| ]) = P ooling(T ransf ormer R (d (0) 1 , d (0) 2 , , .., d (0) |Di| )) (3)</formula><formula xml:id="formula_8">d |D i | Semantic Document Set D i d 1 (0) t 1 [MASK] t |d 1 | [SOD] t 2 d 1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GNN Encoder</head><p>Researcher Community G where u i is the semantic representation of researcher a i . P ooling() represent the average of all documents' final outputs. T ransf ormer R () is the forward function of Researcher Transformer, L R is the number of layers, and [d</p><formula xml:id="formula_9">Linear Random Sampling c i G i a i</formula><formula xml:id="formula_10">(L R ) 1 , d (L R ) 2 , d (L R ) 3 , .., d (L R )</formula><p>|Di| ] is the output. Thus, in this Researcher Transformer, for each researcher's documents set, each document in the set can collect information from other documents with different attention weights contributed by the self-attention mechanism of the Transformer. So that we can obtain context-aware and researcher-specific document outputs for different researchers, rather than assuming a document (e.g., a paper) has the same contribution to different owners. Figure <ref type="figure" target="#fig_2">2</ref> shows the architecture of the proposed hierarchical Transformer, where the Document Transformer is shared by different document inputs and the Researcher Transformer is shared by different researcher inputs.</p><p>Complexity Analysis. Instead of splicing each researcher's documents directly into one long text and encoding them using a flat Transformer, we organize the researcher's text documents in a hierarchical structure and propose the hierarchical Transformer architecture, which significantly improves the efficiency. Specifically, suppose the number of documents is n, the length of each document is l, and the hidden dimension is h, according to <ref type="bibr" target="#b64">[65]</ref>, the self-attention layer is the most time-consuming component of Transformer and computing the self-attention in the flat Transformer has a quadratic complexity O((nl) 2 h) on the total text of researcher's documents. In our approach, the complexity in the document Transformer is O(l 2 h) and that in the researcher Transformer is O(n 2 h). The total complexity in the hierarchical text encoder is O(nl 2 h+n 2 h). Thus, the radio of the computing time of the self-attention in the flat and hierarchical Transformer is nl 2 n+l 2 . As usually n &lt;&lt; l 2 , nl 2 n+l 2 ≈ n and the hierarchical Transformer can be approximately n times faster than the flat Transformer, while their spatial complexity is comparable as the parameter size is independent of the sample length and the number of layers of both would not differ greatly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Local Community Encoder</head><p>Given the researcher community graph G = {(a i , r, a j )} ⊆ A × R × A, first we aim to extract community information of researchers from this graph. Recently, Graph Neural Networks (GNNs) have achieved state-of-the-art performance on handling graph data <ref type="bibr" target="#b65">[66]</ref>, <ref type="bibr" target="#b66">[67]</ref>. Typically, GNNs output node representations via a message-passing mechanism, i.e., they stack K layers to encode node features into lowdimensional representations by aggregating K-hop local neighbors' information of nodes. However, taking the whole graph as the input for GNN models can hardly be applied on large-scale graph data due to memory limitations. Also, the inter-connected graph structure prevents parallel computing on complete graph topology, making the GNNs propagation on large graph data extremely time-consuming <ref type="bibr" target="#b49">[50]</ref>. One prominent direction for improving the scalability of GNNs is using sub-graph sampling strategies, For example, instead of using full neighborhood sets, GraphSAGE <ref type="bibr" target="#b67">[68]</ref> uniformly sample a fixed-size set of neighbors to keep the computational footprint of each batch fixed. SEAL <ref type="bibr" target="#b68">[69]</ref> extracts k-hop enclosing sub-graphs to perform link prediction. GraphSAINT <ref type="bibr" target="#b69">[70]</ref> propose random walk samplers to construct mini-batches during training. Thus, we propose to sample a sub-graph of G to represent the local community graph of researcher a i , denoted by G i , which can preserve the interactions and relations of a i with other researchers. An intuitive way to sample G i is to directly sample a i 's h-hops neighborhoods, e.g., to sample all a i 's neighbors within h-hops as well as corresponding relations to compose G i . However, the community graph of researchers usually is denser than other kinds of graphs, each researcher may have dozens of neighbors. In this sampling way, the size of G i might grow geometrically and it would become expensive to process it in training when h increases.</p><p>Linear Random Sampling. In our paper, we propose a linear random sampling strategy, which can make the number of sampled neighbors increase linearly with the number of sampling hops h. The procedure of linear random sampling is presented as follow:</p><p>Algorithm 1: Procedure of linear random sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input:</head><p>The researcher a i , the community graph G, the number of sampling hops h, and the sampling size n.</p><formula xml:id="formula_11">Output: The local community graph G i of a i . 1 G (1) i = SampleOnehopN eighborhood(a i , n, G); 2 for s = 1, .., h − 1 do 3 Random sample a s-hop neighbor of a i in G (s) i , denoted by a j ; 4 G (1) j = SampleOnehopN eighborhood(a j , n, G); 5 G (s+1) i = G (s) i ∪ G (1) j ; 6 end 7 return G (h) i ;</formula><p>where the SampleOnehopN eighborhood(a i , n, G) represents the process that randomly sampling n numbers of a i 's 1-hop links, expressed as (a i , r, a j ) ∈ G, to compose</p><formula xml:id="formula_12">G (1) i , G (s) i</formula><p>is the sampled local community within s-hop. In this procedure, we obtain a sub-graph of a i 's h-hop neighborhood with n neighbors in each hop.</p><p>This sampling strategy has the following advantages: (1) The size of local community graphs are linearly correlated to the number of sampling hops, so it would not increase the time complexity of computing community embeddings below when h increases. <ref type="bibr" target="#b1">(2)</ref> The sampled neighbor size of each researcher is fixed as h × n and neighbors with more links are more likely to be sampled. (3) Noted that we re-sample the local community graphs in each training step, so that all the links in the local community may be sampled after multiple training steps. (4) The sampling strategy can be seen as a data augmentation operation by masking partial neighbors, which is widely used in graph embedding models <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b67">[68]</ref>. It can help to improve the generalization of models like the mechanism of Dropout <ref type="bibr" target="#b70">[71]</ref>.</p><p>GNN Encoder. Obtained the local community graph G i of a i , we use GNN model to encode G i into a community embedding. Traditional GNNs can only learn the representations of nodes by aggregating the features of their neighborhood nodes, we refer to these node representations as patch representations. Then, we utilize a Readout function to summarize all the obtained patch representations into a fixed length graph-level representation. Formally, the propagation of L C -layer GNN encoder and the Readout operation is represent as:</p><formula xml:id="formula_13">h (l) i = Aggregation (l) (h (l−1) i , {h (l−1) j } : (a i , r i,j , a j ) ∈ G i ) (4) c i = Readout(h (L C ) j : a j ∈ N (G i ))<label>(5)</label></formula><p>where</p><formula xml:id="formula_14">0 ≤ l ≤ L C , h<label>(l)</label></formula><p>j is the output hidden vector of node a j at the l-th layer of GNN and h</p><formula xml:id="formula_15">(0) j = u j , h (l) i</formula><p>represents the neighborhood message of a i passing from all its neighbors in G i at the l-th layer, Aggregation (l) (•) and Combine (l) (•) are component functions of the l-th GNN layer, and N (G i ) is the node set of G i . After L C -layer propagation, the output community embedding of G i is summarized on node representation vectors through the Readout(•) function, which can be a simple permutation invariant function such as averaging or more sophisticated graph-level pooling function <ref type="bibr" target="#b71">[72]</ref>. As the relation between researchers may have multiple types, in practice,, we choose the classic and widely used RGCN <ref type="bibr" target="#b72">[73]</ref>, which can be applied on heterogeneous graphs, as the encoder of subgraphs, we set the layer number L C same as the sampling hops h. Also, we use averaging function for Readout(•) in consider of efficiency.</p><p>Complexity Analysis. For simplicity, we omit the relations and discuss the time complexity of applying GCN as the encoder on the researcher community, which can be approximately considered as the time complexity of applying RGCN. Suppose m is the number of nodes, d is the average node degree in the graph, and f is the hidden dimension. According to <ref type="bibr" target="#b73">[74]</ref>, the time complexity of a l-layer GCN on the complete graph can be bounded by O(lmdf + lmf 2 ), where O(lmdf ) is the cost of feature propagation, i.e, the sparse-dense matrix multiplication on the normalized adjacency matrix and the node hidden embedding matrix, and O(lmf 2 ) is the cost of the feature transformation by applying weight matrix. Notably, the feature propagation is the dominating complexity term of GCN, and performing propagation on the full neighborhood is the main bottleneck for achieving scalability <ref type="bibr" target="#b73">[74]</ref>. In our local community encoder, the cost of feature transformation is the same. We sample sub-graphs with reduced node degree n &lt; d, the cost of feature aggregation to the central nodes is O(lnf ), then the per-epoch cost is bounded by O(lmnf ) &lt; O(lmdf ), which is more efficient than propagation on the complete graph. In addition, sampling and encoding sub-graphs can be easily parallelized, which can further improve efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Substitutability of Local Community Encoder.</head><p>It is worth mentioning that our method places no constraints on the choices of the local community's sampling strategy and GNN encoder. Our framework is flexible with other neighborhood sampling methods, such as random walk with restart <ref type="bibr" target="#b74">[75]</ref>, hierarchical tree schema <ref type="bibr" target="#b75">[76]</ref>. Also, traditional GNN such as GCN <ref type="bibr" target="#b76">[77]</ref>, GraphSAGE <ref type="bibr" target="#b67">[68]</ref>, and other GNN models that can encode graph-level representations are available for local community graph encoding, such as graph isomorphism network <ref type="bibr" target="#b77">[78]</ref>, DiffPol <ref type="bibr" target="#b71">[72]</ref> can work in our framework. The design of our framework mainly consider the heterogeneity of researcher community graph and the efficiency as the pre-training dataset is very large, the comparison of different sampling strategies and encoders is not the focus of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Multi-Task Self-Supervised Learning</head><p>In this section, we propose the multi-task self-supervised objective for pre-training, which consists of the main task: global contrastive learning, and two auxiliary selfsupervised tasks: hierarchical mask language model and community relation prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Global Contrastive Learning</head><p>We consider the strong correlation between a researcher's semantic attributes and the local community to design a self-supervised contrastive learning task. The assumption is that given a researcher's local community, we can use the community embedding to infer the semantic information of this researcher, based on the fact that we can usually infer a researcher's research topics according to the community he/she belongs to, and vice versa. Obtained a researcher embedding u i and the embedding c i of one sampled local community, this task aim to discriminate whether they belong to the same researcher. We define the similarity between them as the dot-product of their embeddings and adopt the infoNCE <ref type="bibr" target="#b78">[79]</ref> loss as our learning objective:</p><formula xml:id="formula_16">L M ain = −log exp(c T i u i /τ ) exp(c T i u i /τ ) + aj ∈Nneg(ai)</formula><p>exp(c T i u j /τ ) (6) where N neg (a i ) is the random sampled negative researchers set for a i , its size is fixed as k. τ is the temperature hyper-parameter. By minimize L main , we can simultaneously optimize the semantic encoder and local community encoder for researchers. Thus, the purpose of the contrastive learning is to preserve the semantic and community information into the model parameters and help the model to integrade these two kinds of information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Hierarchical Masked Language Model</head><p>In the main task, the researcher-level representations are trained via the contrastive learning with their community embeddings. While the document-level and token-level representations are not directly trained. Inspired by the Mask Language Model(MLM) task in Bert, we propose the Hierarchical Masked Language Model(HMLM) on the hierarchical Transformer to train the hidden outputs of documents and tokens, which can further improve the researcher representations. As shown in Figure <ref type="figure" target="#fig_3">3</ref>, the MLM task masks a few tokens in each document and use the Transformer outputs corresponding to these tokens, which have captured the context token information, to predict the original tokens. As our semantic encoder is a two-level hierarchical Transformer, besides the token-level context captured in the Document Transformer, the Researcher Transformer can capture document-level context (i.e., other documents belong to the same researchers) information, which we assume is also helpful to predict the masked token. Thus, Given a researcher a i 's semantic document set D i = {d 1 , d 2 , ..., d |Di| }, where d j ∈ D i is one document of a i and the textual sequence of d j is expressed as d j = {t 1 , t 2 , ..., t |dj | }. We first mask 15% of the tokens in each document. Suppose t k ∈ d j is one masked token and it is replaced with [MASK], the HMLM task aims to predict the original token t k based on the sequence context in d j and document context in D i . First, we obtain the output of the Document Transformer in the position of t k :</p><formula xml:id="formula_17">t (L D ) k = [t (L D ) [SOD] , t (L D ) 1 , t (L D ) 2 , .., t (L D ) |dj | ] t k = T ransf ormer D (t (0) [SOD] , t (0) 1 , t (0) 2 , .., t (0) |dj | ) t k (7) Researcher Transformer Document Transformer t 1 [MASK] t |d 1 | [SOD] … … … … [MASK] t |d 2 | [SOD] … t 2 … … [SOD] … t 2 t 1 [MASK] … Document #1 Document #2 Document #|D i | Semantic document set</formula><p>Predicting the masked token. where the input embedding t </p><formula xml:id="formula_18">d (L R ) j = [d (L R ) 1 , d (L R ) 2 , d (L R ) 3 , .., d (L R ) |Di| ] dj = T ransf ormer R (d (0) 1 , d (0) 2 , d (0) 3 , .., d (0) |Di| ) dj (8)</formula><p>After that, We sum up these two outputs and fed it into a linear transformation and a softmax function:</p><formula xml:id="formula_19">ŷt k = Sof tmax(W • (t (L D ) k + d (L R ) j ) + b)<label>(9)</label></formula><p>where ŷt k ∈ R V is the probability of t k over all tokens, V is the vocabulary size. Finally, the HMLM loss can be formulazed as a cross-entropy loss for predicting all masked tokens in a i 's documents:</p><formula xml:id="formula_20">L HM LM = − dj ∈Di tj ∈M d j V c=1 y t k ln ŷt k (<label>10</label></formula><formula xml:id="formula_21">)</formula><p>where y t k is the one-hot encoding of t k , M dj is the set of masked tokens in document d j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3">Community Relation Prediction</head><p>Also, in the main task, we represent the relation information between researchers as the graph-level local community embeddings via linear random sampling and local community encoder. However, the link-level relatedness between researchers is not directly learned. In this auxiliary task, we propose another self-supervised learning task named Community Relation Prediction(CRP), which utilize the links in sampled local communities to construct supervisory signals, as shown in Figure <ref type="figure">4</ref>. The CRP task has two objectives, the first is to predict the relation type between two researchers, the second is to predict the hop counts between two researchers. Specifically, given a researcher a i 's one sampled local community graph G i , we random select 15% links in G i as the inputs of CRP task, expressed as L s i = {(a i , l i,j , a j )}, l i,j is the link type between researcher a i and a j , which can be a atomic relation or composite relations. For example, if a i and a j are linked with the relation r ∈ R, l i,j = r; if they are linked by a path (a i , l i,j , a j ). l i,j has the properties of relation type and hop count, which is what the CRP task aim to predict given the researcher a i and a j . Thus, we first input the element-wise multiplication of a i and a j 's output representations into two linear transformations and softmax functions:</p><formula xml:id="formula_22">a i r1 − → a k r2 − → ... rm − − → a j , r 1 , r 2 , ..., r m ∈ R, l i,j is the compo- sition from r 1 to r m , i.e., l i,j = r 1 • r 2 •, ..., •r m .. In each link</formula><formula xml:id="formula_23">ŷt li,j = Sof tmax(W t • (h (L C ) i • h (L C ) j ) + b t ) (<label>11</label></formula><formula xml:id="formula_24">)</formula><formula xml:id="formula_25">ŷh li,j = Sof tmax(W h • (h (L C ) i • h (L C ) j ) + b h )<label>(12)</label></formula><p>where ŷt li,j ∈ R T is the probabilities of l i,j 's type over all link types and T is the number of link types, ŷh li,j ∈ R H is the probabilities of l i,j 's hop count range from 1 to H and H is the maximum hop count from a i to its neighbors in G i . Next we input these two probabilities into respective crossentropy losses to compose the loss function of CRP task:</p><formula xml:id="formula_26">L CRP = − (ai,li,j ,aj )∈L s i T c=1 y t li,j ln ŷt li,j + H c=1 y h li,j ln ŷh li,j<label>(13)</label></formula><p>where y t li,j is the one-hot encoding of l i,j 's link type index, y h li,j is the one-hot encoding of l i,j 's hop count. With the guidance of this task, the model can learn fine-grained interaction between researchers such that the GNN encoder is capable of finely capture community information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Pre-training and Fine-tuning</head><p>Pre-training. We leverage a multi-task learning objective by combining the main task and two auxiliary tasks for pretraining. The final loss function of RPT can be written as:</p><formula xml:id="formula_27">L pre (f θ ; D, G) = L M ain + λ 1 L HM LM + λ 2 L CRP (<label>14</label></formula><formula xml:id="formula_28">)</formula><p>where the loss weights λ 1 and λ 2 are hyper-parameters. We sample mini-batches of researchers' semantic document sets and local community graphs as the inputs to train the whole model, and The parameters is optimized by back propagation consistently.</p><p>Fine-tuning. After pre-training, we obtain the pretrained RPT model f θpre , which is able to extract valuable information from the researcher semantic document features and community network. Thus, in the fine-tuning stage, given the researcher data D f t and G f t for a specific finetuning task, we can obtains the semantic representation u i and the community representation c i of each researcher a i . Suppose U f t and X f t is the semantic and community representation matrix of the fine-tuning task, respectively, the final researcher representation matrix Z f t is obtained from the following:</p><formula xml:id="formula_29">U f t , X f t = f θpre = Ψ(θ pre ; D f t , G f t ) (<label>15</label></formula><formula xml:id="formula_30">)</formula><formula xml:id="formula_31">Z f t = M erge(U f t , X f t , axis = −1)<label>(16)</label></formula><p>where M erge(•) is a merging function to fuse the semantic representation and community representation of researchers, in practice, we directly use the concatenating operation as these two representations have been integrated with each other via contrastive learning in Eq. 6. In our framework, we propose two transfer modes of RPT for finetuning:</p><p>• The first is the feature-based mode, expressed as RPT (fb). We treat the RPT as an pre-trained representation generator that first extracts researchers' original features into a low dimensional representation vectors Z f t . Then, the encoded representations are used as input initial features of researchers for the downstream tasks. • The second is the end-to-end mode, expressed as RPT (e2e). The pre-trained model f θpre with hierarchical Transformer and local community encoder is trained together with each fine-tuning downstream task. Suppose the models of fine-tuning task is g(•) with the parameters φ, the objective of RPT (e2e) can be written as:</p><formula xml:id="formula_32">θ f t , φ f t = arg min θ,φ L f t (g(f θpre , φ); D f t , G f t )<label>(17)</label></formula><p>where θ f t and φ f t is the optimized parameters, and L f t is the loss function of the fine-tuning task. All parameters are optimized end-to-end. RPT (e2e) can further extract semantic and community information useful for the downstream researcher data mining tasks. While RPT (fb) without saving and training the pre-trained model is more efficient than RPT (e2e) in the fine-tuning stage. But compared with traditional solutions training different task-specific models for different tasks from scratch, both these two transfer modes are relatively inexpensive, as the downstream model can be very lightweight and converge quickly with the help of pretraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we first introduce the experimental settings including dataset, baselines, pre-training and fine-tuning parameter setting and implemental hardware and software. Then we fine-tune the pre-trained model on three tasks: researcher classification, collaborator prediction, and top-k researcher retrieval to evaluate the effectiveness and transferability of RPT. Lastly, we perform the ablation studies and hyper-parameters sensitivity experiments to analyze the underlying mechanism of RPT. The code of RPT is publicly available on https://github.com/joe817/RPT. The data in this study is openly available in Science Data Bank at https://datapid.cn/31253.11.sciencedb.01504 and http://www.doi.org/10.11922/sciencedb.01504.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head><p>Dataset. We perform RPT on the public scientific dataset: Aminer citation dataset 1 <ref type="bibr" target="#b2">[3]</ref>, which is extracted from DBLP, ACM, MAG (Microsoft Academic Graph) and contains mullions of publication records. The available features of researchers contained in the dataset are the published literature, published venues, organizations, etc. To prepare for the experiments, we select 40281 researchers from Aminer, who have published at least ten papers range from the year of 2013 to 2018. The information of publication records from 2013 to 2015 are extracted to create the semantic document sets and the community graph of researchers for pre-training. Specifically, we collect each researcher's papers as his/her semantic documents, the textual sequence of each document is composed by the paper's fields of study. We extract three kinds of relations, Collaborating (if they collaborated a paper), Colleague (if they are in the same organization), and CoVenue (if they published on same venue) between researchers, to construct the researcher community graph (Noted that we random sample 100 neighbors of relation CoVenue per researcher). The statistics of semantic document set of researchers and researcher community graph is presented in Table <ref type="table" target="#tab_3">1</ref>. Baselines. We choose several pre-training models that can capture the information in semantic document sets and researcher community to researcher representations as baselines. Based on if they can be trained end-to-end with the downstream models, we divide these models into featurebased models, including Doc2vec <ref type="bibr" target="#b59">[60]</ref>, Metapath2vec <ref type="bibr" target="#b3">[4]</ref>, and ASNE <ref type="bibr" target="#b79">[80]</ref>, and end-to-end models, including BERT <ref type="bibr" target="#b8">[9]</ref>, GraphSAGE <ref type="bibr" target="#b67">[68]</ref>, and RGCN <ref type="bibr" target="#b72">[73]</ref>. We also perform our model in feature-based mode and end-to-end mode in finetuning. The detailed descriptions and implementations of baselines are presented as follows:</p><p>• Doc2vec: Doc2vec is a document embedding model, we collect all the researcher documents from whole dataset to train the model, and use the average embeddings of each researcher's document as the his/her pre-trained embeddings. We use the python gensim library to conduct Doc2vec, we set training algorithm as PV-DM, We also design two graph pre-training model based on two state-of-the-art GNN models: GraphSAGE and RGCN, GraphSAGE can be applied on homogeneous graph and RGCN can be applied on heterogeneous graphs, and they both can aggregate the local neighborhood information and node attributes into researcher embeddings. we conduct GraphSAGE and RGCN on the researcher community graph and use the self-supervised graph context based loss function introduced in GraphSAGE for pre-training, then in finetuning, the pre-trained GraphSAGE and RGCN is trained together with downstream models.</p><p>• GraphSAGE: We use Deep Graph Library tools to build GraphSAGE model. The node attributes in researcher community is initialized by the researcher semantic representations learned by Doc2vec. We use the mean aggregator of GraphSAGE and set the aggregation layer number as 2. https://github.com/dmlc/dgl. • RGCN: RGCN also used the tools from Deep Graph Library. The node attributes initialization and number of aggregation layers is same with GraphSAGE, we choose the same graph-based loss function from GraphSAGE paper which encourages nearby nodes to have similar representations. https://github.com/dmlc/dgl.</p><p>Although some of the above models use both the document information and community information of researchers for pre-training, they may be inclined to extract one kind of information. Thus, we further design two baselines by model combinations that take advantage of semantic models in extracting textual information and graph-based models in extracting community information.</p><p>• D&amp;M2vec:A combination of feature-based models-Doc2vec and Metapath2vec. The Doc2vec and Metapathevec is pre-trained based on the above settings to obtain the researcher representations, which are concatenated as the input for fine-tuning tasks. • BERT&amp;RGCN: A combination of end-to-end models-BERT and RGCN. The BERT and RGCN is first pretrained based on the above setting. Then, we combine the pre-trained BERT model and RGCN model by concatenating the output embeddings. Both the pre-trained models are co-trained along with downstream models in the fine-tuning tasks.</p><p>Pre-Training Parameter Setting. For the hierarchical Transformer, we set the number of layers of hierarchical Transformer as 6, including three layers of Document Transformer and three layers of Researcher Transformer. The number of self-attention heads for each Transformer layer is set as 8. According to the average number of tokens per document and that of documents per researcher, we set the maximum length of inputs as 20 and 10 for Document Transformer and Researcher Transformer, respectively. For the local community encoder, we set the neighbor sampling hops as 2, the size of sampled neighbor set as 8 for efficiency, and the number of layers of GNN model as 2. For global contrastive learning, We set the temperature τ as 1, and the size of negative samples as 3. In training, we use Adam optimization with learning rate of 0.01, the exponential decay rates β 1 = 0.9 and β 2 = 0.999, weight decay of 1e-7. we train for 64000 steps with the batch size of 64, which is about 100 training epochs on the pre-training data. We set the researchers representation dimension and all the hidden layer dimensions as 64. We evaluate different loss weights of two auxiliary tasks and set the optimal values 0.1 for both λ 1 and λ 2 . The code and data is on https://github.com/joe817/RPT. 0.9 0.9 0.9 adam parameter(β 2 ) 0.999 0.999 0.999</p><p>Hardware &amp; Software All experiments are conducted with the following setting:</p><p>• Operating system: CentOS Linux release <ref type="bibr">7.7.1908</ref> • Intel(R) Xeon(R) Gold 6130 CPU @ 2.10GHz.  <ref type="table" target="#tab_5">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Task #1: Researcher Classification</head><p>The classification task is to predict the researcher categories. Researchers are labeled by four areas: Data Mining (DM), Database (DB), Natural Language Processing (NLP) and Computer Vision (CV). For each area, we choose three top venues 2 , then we label researchers by the area with the majority of their publish records on these venues(i.e., their representative areas). The learned researcher representations by each model are feed into a multi-layer perceptron(MLP) classifier. The experimental results are shown in Table <ref type="table" target="#tab_6">3</ref>. The number of extracted labeled authors is 4943, and they are randomly split into the training set, validation set, and testing set with different proportions. We use both Micro-F1 and Macro-F1 as the multi-class classification evaluation metrics.</p><p>According to Table <ref type="table" target="#tab_6">3</ref>, we can observe that (1) the proposed RPT outperform all baselines by a substantial margin in terms of two metrics, the RPT (e2e) obtains 0.6%-10.5% Micro-F1 and 0.7%-12.4% Macro-F1 improvement over baselines. (2) Noted that the pre-trained model in RPT (fb) is not trained in fine-tuning, comparing four feature-based baselines, RPT (fb) obtains at least 4.5% improvement, proving that our designed multi-task self-supervised learning objectives can better capture the semantic and community information. (3) Also, both RPT (fb) and RPT (e2e) outperform D&amp;M2vec and BERT&amp;RGCN that combines models by embedding concatenation, proving the superior of the global contrastive learning and two auxiliary self-supervised tasks in improving the researcher representations. ( <ref type="formula">4</ref>) RPT (e2e) consistently outperforms RPT (fb), indicating the learned parameters in the pre-training model can further contribute to the downstream task by end-to-end fine-tuning. from 2017 to 2018 for testing, noted that duplicated collaborators are removed from evaluation. We use the elementwise multiplication of two candidate researchers' representations as the representation of their collaborating link, then we input the link representation into a binary MLP classifier to predict whether this link exists. Also, negative links (two researchers who did not collaborate in the dataset) with 3 times the number of true links are randomly sampled. We sample various numbers of collaborating links and use accuracy and F1 as evaluation metrics.</p><p>The experimental results of different models are reported in Table <ref type="table" target="#tab_7">4</ref>. According to the table, RPT still performs best in all cases. The following insights can be drawn: (1) Graph representation learning models and GNNs generally achieve better performance than semantic representation learning models, showing that the community information of researchers maybe more important to collaborating prediction. (2) RPT and RGCN outperform GraphSAGE, indicating the benefit of incorporating heterogeneity information of relations to researcher representations. (3) Incorporating the semantic models with the graph model can further improve the performance. RGCN just uses embeddings of the researcher's documents to initialize node features. BERT&amp;RGCN further extracts the document information via multi-layer Transformers, thus achieving better performance. (4) Our methods can achieve 82.9% accuracy even when the size of the training set is far less than the testing set, indicating the effectiveness of the pre-training model in preserving useful information from unlabeled data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Task #3: Top-K Researcher Retrieval</head><p>The problem of top-K researcher retrieval is a typical information retrieval problem, which is defined as given a researcher, we aim to retrieve several most relevant researchers of him/her. For each input researcher, we use the dot-product of his and the candidate researcher's representations as their scores and input the scores into a softmax over all researchers to predict top-K relevant researchers, we also use negative sampling <ref type="bibr" target="#b80">[81]</ref> in training for efficiency. We randomly select 2000 researchers for training, 1000 researchers for validation, and 7000 researchers for testing. The ground truth for each researcher is defined as its coauthor list ordered by the times of collaboration. Noted that we do not introduce any extra parameters except pretrained models to fine-tuning in this task, and for a fair comparison, the researcher representations are regarded as trainable parameters for feature-based models. Finally, we use Precision@K and Recall@K in the top-K retrieval list as the evaluation metric and we set K as 1, 5, 10, 15, 20 respectively. The results are shown in Table <ref type="table" target="#tab_8">5</ref>. We can observe that: (1) The performance of feature-based models in this task is far less than end-to-end models in general, comparing their performance in previous tasks. That is because, without downstream parameters, they are inadequate to fit the objective function well. But still, RPT(fb) achieves better performance. (2) While the end-to-end models can adaptively optimize the parameters in pre-trained models by the downstream objectives, so they can achieve better performance. (3) The proposed RPT in end-to-end mode still achieves the best performance, showing the designed framework is robust in transferring to different downstream tasks. (4) Experiments on all three tasks show that models fusing both document and community information achieve better results than those inclined to extract one of them, indicating that the researcher's documents and community contain differential information, and both can contribute to the researcher data mining tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Model Analysis</head><p>In this section, we analyze the underlying mechanism of RPT, we conduct several ablation studies and parameter analysis to investigate the effect of different components, stages, and parameters.</p><p>Ablation Study of Multi-tasks. As the proposed RPT is a multi-task learning model with the main task and two auxiliary tasks. How different tasks impact the model Metric Pre@K Rec@K Pre@K Rec@K Pre@K Rec@K Pre@K Rec@K Pre@K Rec@K Doc2vec We perform these variant models on the researcher classification task and collaborating prediction task, the pretraining setting is same with the complete version, and the fine-tuning is set as the end-to-end mode. Figure <ref type="figure">5</ref> show the performance of these variants comparing with the original RPT. We can observe that: (1) The results of RPT are consistently better than all the other variants, it is evident that using the three objectives together achieves better performance. (2) Both the RPT M +H and RPT M +C achieve better performance than RPT M , indicating the usefulness of both these two auxiliary tasks. (3) RPT M +C is better than RPT M +H on two downstream tasks, which implies the L CRP plays a more important role than L HM LM in this framework. (4) Comparing the performance of RPT M with the baselines in Table <ref type="table" target="#tab_7">3 and 4</ref>, we can find that RPT M still achieves very competitive performance, demonstrating that our framework has outstanding ability in learning researcher representations.</p><p>Effect of Pre-training. To verify if RPT's good performance is due to the pre-training and fine-tuning framework, or only because our designed neural network is powerful in encoding researcher representations. In this experiment, we do not pre-train the designed framework and fully fine-tune it with all the parameters randomly initialized. In Figure <ref type="figure">6</ref>, we present the training and testing curves of RPT with pretraining and without pre-training as the epoch increasing in two downstream tasks. We can observe that the pretrained model achieves orders-of-magnitude faster training and validation convergence than the non-pre-trained model. For example in the classification task, it took 10 epoch for the non-pre-trained model to get the 77.9% Micro-F1, while it took only 1 epoch for the pre-trained model to get 83.6% Micro-F1, showing pre-training can improve the training efficiency of downstream tasks. On the other hand, we can observe that the non-pre-trained model is inferior to pre-trained models in the final performance. It proves that our designed pre-training objective can preserve rich information in parameters and provides a better start point for fine-tuning than random initialization. Hyper-parameters sensitivity. We also conduct experiments to evaluate the effect of two key hyper-parameters in our model, i.e, the number of self-attention heads in hierarchical Transformers and the sampling hops in the local community encoder. We investigate the sensitivity of these two parameters on the researcher classification task and report the results in Figure <ref type="figure" target="#fig_8">7</ref>. According to these figures, we can observe that (1) the more number of attention heads will generally improve the performance of RPT, while with the further increase of attention heads, the improvement becomes slightly. Meanwhile, we also find that more attention heads can make the pre-training more stable. (2) When the number of sampling hops varies from 1 to 5, the performance of RPT increases at first as a suitable amount of neighbors are considered. Then the performance decrease slowly when the hops further increase as more noises (uncorrelated neighbors) are involved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we propose a researcher data pre-training framework named RPT to solve researcher data mining problems. RPT jointly consider the semantic information and community information of researchers. In the pretraining stage, a multi-task self-supervised learning objective is employed on big unlabeled researcher data for pretraining, while in fine-tuning, we transfer the pre-trained model to multiple downstream tasks with two modes. Experimental results show RPT is robust and can significantly benefit various downstream tasks. In the future, we plan to perform RPT on more meaningful researcher data mining tasks to verify the extensibility of the framework. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. An illustration of our proposed pre-training and fine-tuning framework for researcher data mining. (a) We divide the researchers' data into semantic document sets (containing researchers' textual attributes) and community graphs (preserving researchers' community relationships) as inputs for the RPT model and (b) pre-train the RPT model via a multitask self-supervised learning objective to preserve the researcher data. (c) Then, we fine-tune the pre-trained RPT model via the objectives of different downstream tasks for researcher data mining.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>|dj | }. we define a new token named [SOD] (Start Of Document) and random initialize its embedding t (0)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. An brief illustration of our proposed researcher data pre-training model. The model contains two encoders, (1) the hierarchical Transformer that encodes the researcher semantic attributes and (2) the local community encoder that encodes the researcher community-noted that in the researcher community, neighbors with different link types to the central researcher are colored differently. Three self-supervised tasks, including global contrastive learning, hierarchical masked language model, and community relation prediction, are designed to optimize the whole model.</figDesc><graphic url="image-33.png" coords="5,165.51,154.33,93.66,64.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Hierarchical masked language model. 15% of the tokens in each document are masked for prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>t k is replaced as the embedding of [MASK]. Then, we obtain the output of Researcher Transformer in the position of document d j , which is the document t k is in:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>5 a 1 a 4 a 1 a 4 a 1 a 3 Fig. 4 .</head><label>54434</label><figDesc>Fig. 4. An illustration of preparing self-supervised signals on the sampled local community of researchers. 15% links in the community graph is selected for relation prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>• GPU: 4 GeForce GTX 1080 Ti • Software versions: Python 3.7; Pytorch 1.7.1; Numpy 1.20.0; SciPy 1.6.0; Gensim 3.8.3; scikit-learn 0.24.0 Fine-tuning Parameter Setting. The hyper-parameter settings of RPT on three fine-tuning tasks are presented on Table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 5 .Fig. 6 .</head><label>56</label><figDesc>Fig. 5. Performance evaluation of variant models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Hyper-parameters sensitivity of RPT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Dr. Pengyang Wang is an assistant professor in the State Key Lab of Internet of Things and Smart Cities at the University of Macau. He received his Ph.D. in Computer Science from the University of Central Florida. He has broad interests in data mining and machine learning, especially in geospatial-temporal data modeling. He has published in related domains on top venues such as KDD, TKDE, IJCAI, WWW, AAAI etc. He also served as a (senior) program committee member among top conferences in the data mining and artificial intelligence communities. Meng Xiao was born in 1995. He received his B.S. degree from the East China University of Technology and graduated from China University of Geosciences (Wuhan) with a master's degree. He is currently working toward obtaining a Ph.D. degree at the University of Chinese Academy of Sciences. His main research interests include Data Mining, Graph Representation Learning, Hierarchical Multi-label Text Classification combined with the knowledge, and the Knowledge Graphs of the discipline system. Zhiyuan Ning received bachelor degree in 2018 from School of Information Engineering, Zhengzhou University. He is now a PHD candidate in the University of Chinese Academy of Sciences, and is carrying out his research work in Computer Network Information Center of Chinese Academy of Sciences. His research interests focus on natural language processing and knowledge graphs. Denghui Zhang received the BE degree from University of Science and Technology Beijing, China, 2015, the MS degree from Chinese Academy of Sciences, China, 2018. He is currently working toward the PhD degree in the Information System department at Rutgers University, USA. His research interests data business analytics, natural language processing, and representation learning. Dr. Yi Du was born in 1988. He received his Ph.D. degree from Institute of Software, Chinese Academy of Sciences in 2013. He is an associate professor at the Department of Big Data Technology and Application Development at Computer Network Information Center, Chinese Academy of Sciences. His research interests include big data and analytics. Dr. Yuanchun ZHOU was born in 1975. He received his Ph.D. degree from Institute of Computing Technology, Chinese Academy of Sciences, in 2006. He is a professor, Ph.D. supervisor, and the assistant director of Computer Network Information Center, Chinese Academy of Sciences, as well as the director of the Department of Big Data Technology and Application Development. His research interests include data mining, big data processing, and knowledge graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>•</head><label></label><figDesc>Ziyue Qiao is with Computer Network Information Center, Chinese Academy of Sciences, and University of Chinese Academy of Sciences, Beijing. E-mail: qiaoziyue@cnic.cn • Yanjie Fu is with Department of Computer Science, University of Central</figDesc><table /><note>Florida, Orlando. E-mail: yanjie.fu@ucf.edu • Pengyang Wang is with Department of Computer and Information Science, University of Macau, Macau. E-mail: pywang@um.edu.mo • Meng Xiao, Zhiyuan Ning, Yi Du, and Yuanchun Zhou are with Computer Network Information Center, Chinese Academy of Sciences, Beijing. E-mail: {shaow, ningzhiyuan, yidu, yuanchunzhou}@cnic.cn • Denghui Zhang are with Information System department at Rutgers University, USA. E-mail: denghui.zhang@rutgers.edu • Corresponding author: Yi Du.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>d |Di| } of researcher a i , we aim to use the Transformer to encode the text information in D i into the researcher representation u i . Considering that each researcher may have multiple documents with rich text and documents may have different properties and should be processed separately, while the original Transformer can only handle the inputs of a single sentence. Thus, we propose a two-level hierarchical Transformer model, which consists of a Document Transformer to first encode the text in each document into a document representation, and a Researcher Transformer to integrate multiple documents into the researcher representations.</figDesc><table /><note>Document Transformer. For each document d j ∈ D i , suppose its token sequence is {t 1 , t 2 , .., t |dj | } and the corresponding token embeddings are {t</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 1</head><label>1</label><figDesc>Pre-training Dataset Details.</figDesc><table><row><cell>Number of researchers</cell><cell>40,281</cell></row><row><cell>Number of documents</cell><cell>225,724</cell></row><row><cell>Number of tokens</cell><cell>14,391</cell></row><row><cell>Average documents per researcher</cell><cell>13.8</cell></row><row><cell>Average tokens per documents</cell><cell>19.7</cell></row><row><cell>Number of Collaborating</cell><cell>599,612</cell></row><row><cell>Number of Colleague</cell><cell>115,891</cell></row><row><cell>Number of CoVenue</cell><cell>2,012,935</cell></row><row><cell>Average researcher degree of Collaborating</cell><cell>29.8</cell></row><row><cell>Average researcher degree of Colleague</cell><cell>5.8</cell></row><row><cell>Average researcher degree of CoVenue</cell><cell>100</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Metapath2vec is a network embedding model, we conduct it on the researcher community graph to obtain node embeddings as pre-trained researcher representations. We set Collaborating-Colleague-CoVenue as the meta-path to sample paths on researcher community via random walk, we set walk length as 10, works per researcher as 5, size of window as 3, number of negative samples as 5. https://ericdongyx.github.io/metapath2vec/m2v.html. ASNE is a attribute network embedding method which can preserve both the community and semantic attributes of researchers. It concatenates the structure features and node attributes into a multi-layer perceptron to learn node embeddings. We use the semantic representations learned by Doc2vec as the input attribute embeddings, and set the the same weight for attribute embedding and structure embedding and the number of hidden layer as 2. https://github.com/lizigit/ASNE. BERT is a classic text pre-training model, we concatenate all documents into a sentence as inputs and output the [CLS] output as researcher representations. For a fair comparison, we train the BERT model on our dataset from the beginning. We set the layer of Transformer as 6 and the number of self-attention heads as 8. The maximum length of inputs is set as the same as the product of maximum lengths in Researcher Transformer and Document Transformer in RPT. https://github.com/codertimo/BERT-pytorch.</figDesc><table /><note>1. https://www.aminer.cn/citation size of window as 3, number of negtive samples as 5. https://pypi.org/project/gensim/.• Metapath2vec: • ASNE: • BERT:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 2 The</head><label>2</label><figDesc></figDesc><table><row><cell cols="4">hyper-parameter settings on three fine-tuning tasks. RC:</cell></row><row><cell cols="4">Researcher Classification; CP: Collaborating Prediction; TRR: Top-k</cell></row><row><cell cols="3">Researcher Retrieval.</cell><cell></cell></row><row><cell>Parameter</cell><cell>RC</cell><cell>CP</cell><cell>TRR</cell></row><row><cell>batch size</cell><cell>64</cell><cell>256</cell><cell>64</cell></row><row><cell>epochs</cell><cell>10</cell><cell>10</cell><cell>20</cell></row><row><cell>dropout rate</cell><cell>0.9</cell><cell>0.9</cell><cell>0.9</cell></row><row><cell>learning rate</cell><cell>1e-2</cell><cell>1e-3</cell><cell>1e-4</cell></row><row><cell>weight decay</cell><cell>1e-7</cell><cell>1e-7</cell><cell>1e-4</cell></row><row><cell>adam parameter(β 1 )</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 3</head><label>3</label><figDesc>Results of Researcher Classification.</figDesc><table><row><cell>Proportions(%)</cell><cell cols="2">10/10/80</cell><cell cols="2">20/10/70</cell><cell cols="2">30/10/60</cell></row><row><cell>Metric(F1)</cell><cell cols="6">Micro Macro Micro Macro Micro Macro</cell></row><row><cell>Doc2vec</cell><cell>0.781</cell><cell>0.759</cell><cell>0.799</cell><cell>0.780</cell><cell>0.804</cell><cell>0.787</cell></row><row><cell>Metapath2vec</cell><cell>0.760</cell><cell>0.733</cell><cell>0.769</cell><cell>0.743</cell><cell>0.784</cell><cell>0.763</cell></row><row><cell>ASNE</cell><cell>0.790</cell><cell>0.770</cell><cell>0.811</cell><cell>0.786</cell><cell>0.810</cell><cell>0.797</cell></row><row><cell>D&amp;M2vec</cell><cell>0.809</cell><cell>0.784</cell><cell>0.814</cell><cell>0.800</cell><cell>0.826</cell><cell>0.810</cell></row><row><cell>BERT</cell><cell>0.815</cell><cell>0.792</cell><cell>0.824</cell><cell>0.805</cell><cell>0.838</cell><cell>0.825</cell></row><row><cell>GraphSAGE</cell><cell>0.800</cell><cell>0.777</cell><cell>0.812</cell><cell>0.795</cell><cell>0.820</cell><cell>0.808</cell></row><row><cell>RGCN</cell><cell>0.820</cell><cell>0.802</cell><cell>0.825</cell><cell>0.807</cell><cell>0.839</cell><cell>0.823</cell></row><row><cell>BERT&amp;RGCN</cell><cell>0.824</cell><cell>0.804</cell><cell>0.832</cell><cell>0.815</cell><cell>0.842</cell><cell>0.821</cell></row><row><cell>RPT (fb)</cell><cell>0.827</cell><cell>0.805</cell><cell>0.848</cell><cell>0.833</cell><cell>0.852</cell><cell>0.837</cell></row><row><cell>RPT (e2e)</cell><cell>0.840</cell><cell>0.824</cell><cell>0.850</cell><cell>0.835</cell><cell>0.855</cell><cell>0.840</cell></row><row><cell cols="5">4.3 Task #2: Collaborating Prediction</cell><cell></cell><cell></cell></row><row><cell cols="7">Collaborating prediction is a traditional link prediction</cell></row><row><cell cols="7">problem that given the existing collaboration information</cell></row><row><cell cols="7">between authors, we aim to predict whether two researchers</cell></row><row><cell cols="7">will collaborate on a paper in the future, which can be used</cell></row><row><cell cols="7">to recommend potential new collaborators for researchers.</cell></row><row><cell cols="7">To be practical, we randomly sample the collaborating links</cell></row><row><cell cols="7">from 2013 to 2015 for training, in 2016 for validation, and</cell></row></table><note>2. DM: KDD, ICDM, WSDM. DB: SIGMOD, VLDB, ICDE. NLP: ACL, EMNLP, NAACL. CV: CVPR, ICCV, ECCV.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 4</head><label>4</label><figDesc>Results of Collaborating Prediction. 1k = 1000.</figDesc><table><row><cell>Number of links</cell><cell cols="2">1k/1k/10k</cell><cell cols="2">3k/1k/10k</cell><cell cols="2">5k/1k/10k</cell></row><row><cell>Metric(F1)</cell><cell>ACC</cell><cell>F1</cell><cell>ACC</cell><cell>F1</cell><cell>ACC</cell><cell>F1</cell></row><row><cell>Doc2vec</cell><cell cols="6">0.764 0.240 0.767 0.310 0.778 0.363</cell></row><row><cell>Metapath2vec</cell><cell cols="6">0.778 0.347 0.794 0.391 0.796 0.416</cell></row><row><cell>AHNE</cell><cell cols="6">0.796 0.483 0.809 0.506 0.816 0.508</cell></row><row><cell>D&amp;M2vec</cell><cell cols="6">0.796 0.389 0.797 0.421 0.809 0.449</cell></row><row><cell>BERT</cell><cell cols="6">0.785 0.378 0.799 0.520 0.805 0.543</cell></row><row><cell>GraphSAGE</cell><cell cols="6">0.777 0.308 0.781 0.477 0.795 0.482</cell></row><row><cell>RGCN</cell><cell cols="6">0.792 0.483 0.810 0.596 0.819 0.623</cell></row><row><cell>BERT&amp;RGCN</cell><cell cols="6">0.807 0.511 0.817 0.592 0.825 0.566</cell></row><row><cell>RPT (fb)</cell><cell cols="6">0.805 0.624 0.827 0.622 0.828 0.633</cell></row><row><cell>RPT (e2e)</cell><cell cols="6">0.829 0.623 0.840 0.641 0.860 0.674</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 5</head><label>5</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="2">Results of Top-k Researcher Retrieval</cell><cell></cell><cell></cell></row><row><cell>K</cell><cell>1</cell><cell>5</cell><cell>10</cell><cell>15</cell><cell>20</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>RPT M +C : RPT with main task L M ain and CRP task L CRP .</figDesc><table><row><cell></cell><cell>0.278</cell><cell>0.041</cell><cell>0.152</cell><cell>0.099</cell><cell>0.107</cell><cell>0.131</cell><cell>0.088</cell><cell>0.154</cell><cell>0.076</cell><cell>0.171</cell></row><row><cell cols="2">Metapath2vec 0.506</cell><cell>0.076</cell><cell>0.259</cell><cell>0.177</cell><cell>0.189</cell><cell>0.235</cell><cell>0.153</cell><cell>0.272</cell><cell>0.131</cell><cell>0.299</cell></row><row><cell>ASNE</cell><cell>0.514</cell><cell>0.077</cell><cell>0.288</cell><cell>0.192</cell><cell>0.198</cell><cell>0.244</cell><cell>0.159</cell><cell>0.279</cell><cell>0.134</cell><cell>0.304</cell></row><row><cell>D&amp;M2vec</cell><cell>0.580</cell><cell>0.077</cell><cell>0.326</cell><cell>0.185</cell><cell>0.216</cell><cell>0.227</cell><cell>0.167</cell><cell>0.253</cell><cell>0.138</cell><cell>0.271</cell></row><row><cell>BERT</cell><cell>0.594</cell><cell>0.089</cell><cell>0.299</cell><cell>0.205</cell><cell>0.195</cell><cell>0.246</cell><cell>0.149</cell><cell>0.272</cell><cell>0.124</cell><cell>0.291</cell></row><row><cell>GraphSAGE</cell><cell>0.722</cell><cell>0.104</cell><cell>0.389</cell><cell>0.253</cell><cell>0.252</cell><cell>0.304</cell><cell>0.191</cell><cell>0.333</cell><cell>0.158</cell><cell>0.355</cell></row><row><cell>RGCN</cell><cell>0.743</cell><cell>0.098</cell><cell>0.459</cell><cell>0.268</cell><cell>0.301</cell><cell>0.327</cell><cell>0.206</cell><cell>0.353</cell><cell>0.160</cell><cell>0.360</cell></row><row><cell>BERT&amp;RGCN</cell><cell>0.757</cell><cell>0.103</cell><cell>0.480</cell><cell>0.288</cell><cell>0.322</cell><cell>0.352</cell><cell>0.246</cell><cell>0.386</cell><cell>0.201</cell><cell>0.409</cell></row><row><cell>RPT (fb)</cell><cell>0.597</cell><cell>0.090</cell><cell>0.337</cell><cell>0.217</cell><cell>0.231</cell><cell>0.273</cell><cell>0.182</cell><cell>0.309</cell><cell>0.154</cell><cell>0.337</cell></row><row><cell>RPT (e2e)</cell><cell>0.787</cell><cell>0.106</cell><cell>0.525</cell><cell>0.309</cell><cell>0.357</cell><cell>0.381</cell><cell>0.275</cell><cell>0.421</cell><cell>0.226</cell><cell>0.447</cell></row><row><cell cols="5">performance? we propose three model variants to validate</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">the effectiveness of these tasks.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>• RPT M : RPT with only main task L M ain .• RPT M +H : RPT with main task L M ain and HMLM task L HM LM . •</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">ACKNOWLEDGMENTS</head><p>This research was supported by the Natural Science Foundation of China under Grant No. 61836013, Ministry of Science and Technology Innovation Methods Special work Project under grant 2019IM020100, Beijing Nova Program of Science and Technology under Grant No. Z191100001119090, Beijing Natural Science Foundation under Grant No.4212030 and Youth Innovation Promotion Association CAS.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Context-aware academic collaborator recommendation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1870" to="1879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Csteller: Forecasting scientific collaboration sustainability based on extreme gradient boosting</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">World Wide Web</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2749" to="2770" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Arnetminer: extraction and mining of academic social networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 14th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="990" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">metapath2vec: Scalable representation learning for heterogeneous networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="135" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised author disambiguation using heterogeneous graph convolutional network embedding</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Big Data (Big Data)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="910" to="919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A multi-label classification method using a hierarchical and transparent representation for paper-reviewer recommendation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Translations diversification for expert finding: A novel clustering-based approach</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Abin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Knowledge Discovery from Data (TKDD)</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Scholarly network similarities: How bibliographic coupling networks, citation networks, cocitation networks, topical networks, coauthorship networks, and coword networks relate to each other</title>
		<author>
			<persName><forename type="first">E</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1313" to="1326" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><forename type="middle">C</forename><surname>Kenton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
				<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="5753" to="5763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 2020: 37th International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep model based transfer and multi-task learning for biological image analysis</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Big Data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="322" to="333" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Gcc: Graph contrastive coding for graph neural network pre-training</title>
		<author>
			<persName><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1150" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Gpt-gnn: Generative pre-training of graph neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1857" to="1867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Don&apos;t stop pretraining: Adapt language models to domains and tasks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Marasović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.10964</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Scibert: A pretrained language model for scientific text</title>
		<author>
			<persName><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP-IJCNLP</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3606" to="3611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Specter: Document-level representation learning using citationinformed transformers</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2270" to="2282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Science of science</title>
		<author>
			<persName><forename type="first">S</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Bergstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Örner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Helbing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Milojević</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Radicchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sinatra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Uzzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vespignani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Waltman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-L</forename><surname>Barabási</surname></persName>
		</author>
		<ptr target="https://science.sciencemag.org/content/359/6379/eaao0185" />
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">359</biblScope>
			<biblScope unit="issue">6379</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Viziometrics: Analyzing visual information in the scientific literature</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>P.-S. Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName><surname>Howe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Big Data</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="117" to="129" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Academic social networks: Modeling, analysis, mining and applications</title>
		<author>
			<persName><forename type="first">X</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Network and Computer Applications</title>
		<imprint>
			<biblScope unit="volume">132</biblScope>
			<biblScope unit="page" from="86" to="103" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Aminer: Search and mining of academic social networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Intelligence</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="58" to="76" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Coauthor relationship prediction in heterogeneous bibliographic networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Advances in Social Networks Analysis and Mining</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="121" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Exploring dynamic research interest and academic influence for scientific collaborator recommendation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Bekele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientometrics</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="369" to="385" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Scientific collaborator recommendation in heterogeneous bibliographic networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 48th Hawaii International Conference on System Sciences</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="552" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Embedding-based recommendations on scholarly knowledge graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nayyeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vahdati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Yazdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Semantic Web Conference</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="255" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Scientific collaboration sustainability prediction based on h-index reciprocity</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Companion Proceedings of the Web Conference</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="71" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Reducing human effort and improving quality in peer code reviews using automatic static analysis and reviewer recommendation</title>
		<author>
			<persName><forename type="first">V</forename><surname>Balachandran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 35th International Conference on Software Engineering (ICSE)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="931" to="940" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A novel classification method for paper-reviewer recommendation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientometrics</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1293" to="1313" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Expert finding in a social network</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on database systems for advanced applications</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1066" to="1069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A survey on expert finding techniques</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Intelligent Information Systems</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="255" to="279" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Mining advisor-advisee relationships from research publication networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 16th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="203" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Identifying advisor-advisee relationships from co-author networks via a novel deep model</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">466</biblScope>
			<biblScope unit="page" from="258" to="269" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Academic rising star prediction via scholar&apos;s evaluation model and machine learning techniques</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientometrics</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="461" to="476" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Can scientific impact be predicted</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Big Data</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="18" to="30" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Task-guided and path-augmented heterogeneous network embedding for author identification</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth ACM International Conference on Web Search and Data Mining</title>
				<meeting>the Tenth ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="295" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Camel: Content-aware and meta-path augmented metric learning for author identification</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 World Wide Web Conference</title>
				<meeting>the 2018 World Wide Web Conference</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="709" to="718" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Shifu2: A network representation learning based model for advisor-advisee relationship mining</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Generative adversarial network based heterogeneous bibliographic network representation for personalized citation recommendation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Collaborative filtering with network representation learning for citation recommendation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Big Data</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Scientific article recommendation: Exploiting common author relations and historical preferences</title>
		<author>
			<persName><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Big Data</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="101" to="112" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deepinf: Social influence prediction with deep learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2110" to="2119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Semantic enhanced top-k similarity search on heterogeneous information networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Database Systems for Advanced Applications</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="104" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="69" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
				<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1422" to="1430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Selfdiscriminative learning for unsupervised document embedding</title>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wehbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter</title>
				<meeting>the 2019 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2465" to="2474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Graph contrastive learning with augmentations</title>
		<author>
			<persName><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Subgraph contrast for scalable self-supervised graph representation learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.10273</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deep graph infomax</title>
		<author>
			<persName><forename type="first">P</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li Ò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Infograph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization</title>
		<author>
			<persName><forename type="first">F.-Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.01000</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Multi-task self-supervised visual learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2051" to="2060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Multi-task self-supervised learning for robust speech recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pascual</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Swietojanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Monteiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Trmal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
				<imprint/>
	</monogr>
	<note>ICASSP</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Multi-task self-supervised learning for disfluency detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="9193" to="9200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Html: Hierarchical transformer-based multi-task learning for volatility prediction</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L J</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Smyth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference</title>
				<meeting>The Web Conference</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="441" to="451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Rethinking pre-training and self-training</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Deepwalk: online learning of social representations</title>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web</title>
				<meeting>the 24th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">2016</biblScope>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Roberta: A robustly optimized bert pretraining approach</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
				<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Hyperbolic graph attention network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">F</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Big Data</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Multi-label graph convolutional network representation learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Big Data</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Link prediction based on graph neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="5165" to="5175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Graphsaint: Graph sampling based inductive learning method</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Prasanna</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=BJe8pkHFwS" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Neural Information Processing Systems</title>
				<meeting>the 32nd International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4805" to="4815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Semantic Web Conference</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Scalable graph neural networks via bidirectional propagation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-R</forename><surname>Wen</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/file/a7789ef88d599b8df86bbee632b2994d-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">M</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="14" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Fast random walk with restart and its applications</title>
		<author>
			<persName><forename type="first">H</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth international conference on data mining (ICDM&apos;06)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="613" to="622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Tree structure-aware graph representation learning via integrated hierarchical aggregation and relational metric learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.10003</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V D</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Attributed social network embedding</title>
		<author>
			<persName><forename type="first">L</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2257" to="2270" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">His research interests include networked data mining, graph neural network, graph representation learning, pretraining, and knowledge-based graph. Dr. Yanjie Fu is an assistant professor in the Department of Computer Science at the University of Central Florida</title>
		<author>
			<persName><forename type="first">X</forename><surname>Rong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.2738</idno>
	</analytic>
	<monogr>
		<title level="m">He received his Ph.D. degree from Rutgers, the State University of New Jersey in 2016, the B.E. degree from University of Science and Technology of China in 2008, and the M.E. degree from Chinese Academy of Sciences in 2011</title>
				<meeting><address><addrLine>China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
		<respStmt>
			<orgName>University of Chinese Academy of Sciences</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>His research interests include data mining and big data analytics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
