<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Phoneme Recognition: Neural Networks vs Hidden Markov Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">A</forename><surname>Waibel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">IC Shikano IC ATR Interpreting Telephony Research Laborator &apos;Universitv of Toronto and Canahan Institute for Advanced Resea Carnegie</orgName>
								<orgName type="institution">Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">T</forename><surname>Hanazawa</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">IC Shikano IC ATR Interpreting Telephony Research Laborator &apos;Universitv of Toronto and Canahan Institute for Advanced Resea Carnegie</orgName>
								<orgName type="institution">Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">IC Shikano IC ATR Interpreting Telephony Research Laborator &apos;Universitv of Toronto and Canahan Institute for Advanced Resea Carnegie</orgName>
								<orgName type="institution">Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Phoneme Recognition: Neural Networks vs Hidden Markov Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DD422B393A218A2A574A2E18EC99402E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>neme recognition which is characterized by two important properties: 1.) Using a 3 layer arrangement of simple computing units, it can represent arbitrary nonlinear decision surfaces. The TDNN learns these decision surfaces automatically using error back-propagatioii[l]. 2.) he time-delay arrangement enables the network to discover acoustichonetic features and the temporal relationships between them indeendent of position in time and hence not blurred by temporal shifts in the input. For comparison, several discrete Hidden Markov Models (HMM) were trained to perform the same task, i.e., the speakerdependent recognition of the phonemes "B", "D", and "G" extracted We show that the TDNN "invented" well-known acoustic-phonetic to the same concept.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, the advent of new learning procedures and the availability of high speed parallel supercomputers have given rise t o a renewed interest in connectionist models of intelligence <ref type="bibr">[l]</ref>. These models are particularly interesting for cognitive tasks that require massive constraint satisfaction, i.e., the parallel evaluation of many clues and r interpretation in the light of numerous interrelated conause of the far-reaching implications t o speech recognietworks have recently been compared with other pattern recognition classifiers <ref type="bibr">[2]</ref> and explored as alternative to other speech recognition techniques (see <ref type="bibr">[2,</ref><ref type="bibr">3]</ref> for review). Some of these studies report very incouraging performance results <ref type="bibr">[4]</ref>, but others show neural nets as underperforming existing techniques. One possible explanamixed comparative performance results so far might be given by the inability of many neural network architectures to deal properly with the dynamic nature of speech. Various solutions t o this problem, however, are now beginning to emerge <ref type="bibr">[5,</ref><ref type="bibr">6,</ref><ref type="bibr">7,</ref><ref type="bibr">8]</ref> and continued work in this area is likely to lead to more powerful speech recognition systems in the future.</p><p>To capture the dynamic nature of speech a network must be able to 1.) represent temporal relationships between acoustic events, while at the same time 2 ) provide for rnvanance under translatron in time. The specific movement of a formant in time, for example, is an important cue to determining the identity of a voiced stop, but it is irrelevant whether the same set of events occurs a little sooner or later in the course of time. Without translation invariance a neural net requires precise segmentation, to allgn the input pattern properly. Since this is not always possible in practice, learned features tend to get blurred (in order to accommodate slight misalignments) and their performance deteriorates. In the present paper, we describe a Time Delay Neural Network (TDNN), which addresses both of these s3.3 aspects. We demonstrate through extensive performance evaluation that superior recognition results can be achieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Time Delay Neural Networks</head><p>To be useful for speec feed forward neural network must have a n it should have multiple layers and sufficien units in each of these layers. This is to 11 have the ability t o learn complex non Second, the network should have the ab ips between events in time. These events , b u t might also be the output of higher level feature detectors. Third, the actual features or abstractions learned by the network should be invariant under translation in time. Fourth the learning procedure should not require precise amount of tra' training data by ex hat the network is forced t o encode the egularity. In the following, we describe a of phonemes, in particular, the voiced stops "B", "D" an</p><p>The basic unit used in many neural networks computes the ted sum of its inputs and then passes this sum through a non function, most commonly a threshold or sigmoid fnnction <ref type="bibr">[2,</ref><ref type="bibr">1]</ref> TDNN, this basic unit is modified by introducing delays D1</p><p>J inputs of such a unit now will b e for each delay and one for the undela 16, for example, 48 weights will be nee to compute the weighted sum of the 16 inputs, with each input now In this way a TDNN unit has the ability to relate and compare current input with the past history of events. The sigmoid function was chosen as the non-linear output function F due to its convenient mathematical properties <ref type="bibr">[l,9]</ref>.</p><p>For the recognition of phonemes, a three layer net is constructed. Its overall architecture and a typical set of activities in the units are shown in Fig. <ref type="figure" target="#fig_2">2</ref>.</p><p>At the lowest level, 16 melscale spectral coefficients serve as input to the network. Input speech, sampled at 12 kIIz, was hamming windowed and a 256-point F F T computed every 5 msec. hfelscale coefficients were computed from the power spectrum[3] and adjacent coefficients in time collapsed resulting in an overall 10 msec frame rate. The coefficients of an input token (in this case 15 frames of speech centered around the hand labeled vowel onset) were then normalized to lie between -1.0 and $1.0 with the average at 0.0. Fig. <ref type="figure" target="#fig_2">2</ref> shows the resulting coefficients for the speech token "DA" as input to the network, where positive values are shown as black and negative values as grey squares.</p><p>This input layer is then fully interconnected to a layer of 8 time delay hidden units, where J = 16 and N = 2 (i.e., 16 coefficients   <ref type="table">delay 0, 1</ref> and<ref type="table">2</ref>). An alternative way of seeing this is depicted in Fig. <ref type="figure" target="#fig_2">2</ref>. It shows the inputs to these time delay units expanded out spatially into a 3 frame window, which is passed over the input spectrogram. Each unit in the first hidden layer now receives input (via 48 weighted connections) from the coefficients in the 3 frame window. The particular delay choices were motivated by earlier studies <ref type="bibr">[3]</ref>.</p><p>In the second hidden layer, each of 3 TDNN units looks at a 5 frame window of activity levels in hidden layer 1 (i.e., J = 8, N = 4). The choice of a larger 5 frame window in this layer was motivated by the intuition that higher level units should learn to make decisions over a wider range in time based on more local abstractions at lower levels.</p><p>Finally, the output is obtained by integrating (summing) the evidence from each of the 3 units in hidden layer 2 over time and connecting it to its pertinent output unit (shown in Fig. <ref type="figure" target="#fig_2">2</ref> over 9 frames for the " D output unit). In practice, this summation is implemented simply as another TDNN unit which has fixed equal weights to a row of unit firings over time in hidden layer 2.</p><p>When the TDNN has learned its internal representation, it performs recognition by passing input speech over the TDNN units. In terms of the illustration of Fig. <ref type="figure" target="#fig_2">2</ref> this is equivalent t o passing the time delay windows over the lower level units' firing patterns. At the lowest level, these firing patterns simply consist of the sensory input, i.e., the spectral coefficients.</p><p>Each TDNN unit outlined in this section has the ability to encode temporal relationships within the range of the N delays. Higher layers can attend to larger time spans, so local short duration features will be formed at the lower layer and more complex longer duration features at the higher layer. The learning procedure ensures that each of the units in each layer has its weights adjusted in a way that improves the network's overall performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Learning in a TDNN</head><p>Several learning techniques exist for optimization of neural networks[l,%]. For the present network we adopt the Back-propagation Learning Procedure <ref type="bibr">[l,9]</ref>. This procedure iteratively adjusts all the weights in the network so as to decrease the error obtained at its output units. To arrive at a translation invariant network, we need to ensure during learning that the network is exposed to sequences of patterns and that it is allowed (or encouraged) to learn about the most powerful cues and sequences of cues among them. Conceptually, the back-propagation procedure is applied to speech patterns that are stepped through in time. An equivalent way of achieving this result is to use a spatially expanded input pattern, i.e., a spectrogram plus some constraints on the weights. Each collection of TDNN-units described above is duplicated for each one frame shift in time. In this way the whole history of activities is available at once. Since the shifted copies of the TDNN-units are mere duplicates and are to look for the same acoustic event, the weights of the corresponding connections in the time shifted copies must be constrained to be the same. To realize this, we first apply the regular back-propagation forward and backward pass to all time shifted copies as if they were separate events. This yields different error derivatives for corresponding (time shifted) connections. Rather than changing the weights on time-shifted connections separately, however, we actually update each weight on corresponding connections by the same value, namely by the average of all corresponding time-delayed weight changes'. Fig. <ref type="figure" target="#fig_2">2</ref> illustrates this by showing in each layer only two connections that are linked to (constrained to have the same value as) their time shifted neighbors. Of course, this applies to all connections and all time shifts. In this way, the network is forced to discover useful acoustic-phonetic features in the input, regardless of when in time they actually occurred. This is an important property, as it makes the network independent of errorprone preprocessing algorithms, that otherwise would be needed for time alignment and/or segmentation.</p><p>The procedure described here is computationally rather expensive, due to the many iterations necessary for learning a complex multidimensional weight space and the number of learning samples. In our case, about 800 learning samples were used and between 20,000 and 50,000 iterations (step-size 0.002, momentum 0.1) of the backpropagation loop were run over all training samples. For greater learning speed, simulations were run on a 4 processor Alliant supercomputer and a staged learning strategy[3] mas used. Learning still took about 4 days, but additional substantial increases in learning speed are possible <ref type="bibr">[3]</ref>. Of course, this high computational cost applies only to learning. Recognition can easily be run in better than real-time</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Hidden Markov Models</head><p>As an alternative recognition approach we have implemented several Hidden Markov Models (HMM) aimed at phoneme recognition. Hh'IMs are currently the most successful and promising approach <ref type="bibr">[10,</ref><ref type="bibr">11,</ref><ref type="bibr">12]</ref> in speech recognition as they have been successfully applied to the whole spectrum of recognition tasks. HMhls' success is ir ability to cope with the variability in speech by modeling The HhiMs developed in our laboratory eme recognition, more specifically the voiced stops . More detail including results from experiments these models are given elsewhere[l3,3] and we will restrict ourselves to a brief description of our best configuration.</p><p>The acoustic front end for Hidden Markov Modeling is typically a vector quantizer that classlfies sequences of short-time spectra. Input speech was sampled at 12kH2, preemphasized by (1 -0.97 z-') and windowed using a 256-point Hamming window every 3 msec. Then a 12-order LPC analysis was carried out A codebook of 256 LPC from 216 phonetically balanced e Weighted Likelihood Ratio augmented with power values 31 was used as LPC distance measure for vector quantiza-HMM with four states and six transitions (the last state selfloop) was used in this study. The HMM probability e tramed using vector sequences of phonemes according to d-backward algorithm <ref type="bibr">[lO]</ref>. The vector sequences for "B", G include a consonant part and five frames of the follow-I. This is to model important transient informations, such nt movement and has lead to improvements over context inmodels <ref type="bibr">[13]</ref> The HMM was trained until convergence using 50 phoneme tokens of vector sequences per speaker and pho-Typically, about 10 to 20 learning Iterations were required about one hour on a VAX 8700 Floor values were set on the output probabilities to avoid errors caused by zero-probabilities. We have experimented with composite models, re trained using a combination of context-independent and endent probability values[l2], but in our case no signifi-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Recognition Experiments</head><p>We now turn to an experimental evaluation of the two techniques described in the previous sections. To provide a good framework for comparison, the same experimental conditions were given t o both methods For both, the same training data was used and both were tested on the same testing database as described below.</p><p>Experimental Conditions evaluation, we have used a large vocabulary database Japanese words <ref type="bibr">[3]</ref>. These words were uttered in isolale native Japanese speakers (MAU, MHT and MNM,   nnouncers). All utterances were recorded in a sound digitized at a 12 kHz sampling rate The database to a training set and a testing set of 2620 utterances he phoneme recognition task chosen for this experiment was the nition of the voiced stops, i e , the phonemes "B", "D" and "G".</p><p>The actual tokens were extracted from the utterances using manually selected acoustic-phonetic labels provided with the database[3] Both and the HMMs, were trained and tabase, no preselection of tokens was performed. All tof the three voiced stops were included. Since were extracted from entire utterances and not nificant amount of acoustic variability is introc context and the token's position within the our recognition algorithms. are only given the a token and must find their own ways of repretions of speech. Since recognition results based are not meaningful, we report in the following , i e., from performance evaluation tic tokens were extracted er the separate testing data set 'In Japanese, for example, a "G" is nasalzed, when it occurs embedded in a n utterance, but not in utterance irutial position[3].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>109</head><p>~</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>Table <ref type="table">1</ref> shows the results from th above. As can be seen, for all th siderable performance improvemen all three speakers, the error rate is r than four fold reduction in error.</p><p>ion experiments described</p><p>, the TDNN yields conur HMM. Averaged over om 6.3% to 1.5%, a more 1 ' I , 4 , q;,: .;:E::; ...... I . "</p><p>Figure <ref type="figure">4</ref>: TDNN activation patterns for centered and misaligned (30 msec) "DO" two cases we can see that the network has learned to use alternate internal representations to link variations in the sensory input t o the same higher level concepts. A good example is given by the firings of the third and fourth hidden unit in the first layer above the input layer. As can be seen from Fig. <ref type="figure">4</ref> and Fig. <ref type="figure" target="#fig_2">2</ref>, the fourth hidden unit fires particularly strongly after vowel onset in the case oE"DO", while the third unit shows stronger activation after vowel onset in the case of "DA" (see rows pointed to by the filled arrows). The connection strengths of only these two hidden units are displayed on grey background on the left of Fig. <ref type="figure">4</ref> and show the significance of these different firing patterns (here, white and black blobs represent positive and negative weights, respectively, and the magnitude of a weight is indicated by the size of the blob). The time delays are displayed spatially as a 3 frame window of 16 spectral coefficients. Conceptually, the weights in this window form a moving acoustic-phonetic feature detector, that fires when the pattern for which it is specialized is encountered in the input speech. Thus, hidden unit number 4 has learned to fire when a falling (or rising) second formant starting at around 1600 Hz is found in the input. As can be seen in Fig. <ref type="figure">4</ref>, this is the case for "DO" after voicing onset. In the case of"DA" (see Fig. <ref type="figure" target="#fig_2">2</ref>) in turn, the second formant does not fall significantly, and hidden unit 3 fires instead. The connection strengths for TDNN-unit 3 shown in Fig. <ref type="figure">4</ref> show that this unit has learned to look for a steady (or only slightly falling) second formant starting at about 1600 Hz. The connections in the second and third layer then link the different firing patterns observed in the first hidden layer into one and the same decision. Another interesting feature can be seen in the bottom hidden unit in hidden layer number 1 (see activation patterns in Fig. <ref type="figure" target="#fig_2">2</ref> and Fig. <ref type="figure">4</ref>, and [3] for weights). This unit has learned to take on the role of finding the segment boundary of the voiced stop. It does so in reverse polarity, i.e., it is always on except when the vowel onset of the voiced stop is encountered (see unfilled arrows in Fig. <ref type="figure">4</ref> and Fig. <ref type="figure" target="#fig_2">2</ref>). Indeed, the higher layer TDNNunits subsequently use this "segmenter" to base the final decision on the occurrence of the right lower features at the right point in time.</p><p>The right side of Fig. <ref type="figure">4</ref>, finally, demonstrates the shift-invariance of the network. Here the same token "DO" is misaligned by 30 msec. Despite the gross misalignment, the correct result was obtained reliably. The hidden units' feature detectors do indeed fire according to the events in the input speech, and are not negatively affected by the relative shift with respect to the input units.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have presented a Time Delay Neural Network for phoneme recog-nition. By use of two hidden layers in addition to an input and output layer it is capable of representing complex non-linear decision surfaces. Three important properties of the TDNNs have been observed. First, our TDNN was able to invent without human interference meaningful linguistic abstractions in time and frequency such as formant tracking and segmentation. Second, we have demonstrated that it has learned to form alternate representations linking different acoustic events with the same higher level concept. In this fashion it can implement trading relations between lower level acoustic events leading t o robust recognition performance despite considerable variability in the input speech. Third, we have seen that the network is translation-invariant and does not rely on precise alignment or segmentation of the input. We have compared the TDNN's performance with the best of our HMR4s on a speaker-dependent phoneme recognition task. The TDNN achieved a recognition of 96.5% compared t o 93.7% for the HMM, i.e., a fourfold reduction in error.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. A Time Delay Neural Network (TDNN) unit</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>-.... 1 +-a ..... ...... 3 'C I_... ... I . . m .Ij;. ...</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The TDNN architecture (input: " D A )over three frames with time delay 0, 1 and 2). An alternative way of seeing this is depicted in Fig.2. It shows the inputs to these time delay units expanded out spatially into a 3 frame window, which is passed over the input spectrogram. Each unit in the first hidden layer now receives input (via 48 weighted connections) from the coefficients in the 3 frame window. The particular delay choices were motivated by earlier studies[3].In the second hidden layer, each of 3 TDNN units looks at a 5 frame window of activity levels in hidden layer 1 (i.e., J = 8, N =</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Scatter plots showing log probabilities/activation levels for using an HMM (left) and A TDNN</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>'Note that in the experiments reported below these wejght changes were actually carried out &amp;er presentation of all training samples[9].</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Parallel Distributed Processing; Explorations in the Microstructure of Cognition</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986">1986</date>
			<publisher>MIT Press</publisher>
			<biblScope unit="volume">11</biblScope>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An introduction to computing with neural nets</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Lippmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE ASSP Magazine</title>
		<imprint>
			<biblScope unit="page" from="4" to="22" />
			<date type="published" when="1967-04">April 1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Phoneme Recognition Using Time-Delay Neural Networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Waibel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hanazawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shikano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lang</forename><forename type="middle">K</forename></persName>
		</author>
		<idno>TR-1-0006</idno>
		<imprint>
			<date type="published" when="1967-10">October 1967</date>
		</imprint>
		<respStmt>
			<orgName>ATR Interpreting Telephony Research Laboratories</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning the Hidden Structure of Speech</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Elman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zipser</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1967-02">February 1967</date>
		</imprint>
		<respStmt>
			<orgName>University of California, San Diego</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural-net classifiers useful for speech recognition</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Lippmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intemational Conference on Neural A&apos;etworks</title>
		<imprint>
			<date type="published" when="1987-06">June 1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Connectionist speech recognition</title>
		<author>
			<persName><surname>I&lt;</surname></persName>
		</author>
		<author>
			<persName><surname>Lang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987-07">July 1987</date>
		</imprint>
		<respStmt>
			<orgName>Carnegie-Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis proposal</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning phonetic features using connect.ionist networks: an experiment in speech recognition</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Watrous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shastri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intemational Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="1967-06">June 1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Neural computation by concentrating information in time</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Tank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Iiopfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings National Academy of Sciences</title>
		<meeting>National Academy of Sciences</meeting>
		<imprint>
			<date type="published" when="1987-04">April 1987</date>
			<biblScope unit="page" from="1896" to="1900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="page" from="533" to="536" />
			<date type="published" when="1966-10">October 1966</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Continuous speech recognition by statistical methods</title>
		<author>
			<persName><forename type="first">F</forename><surname>Jelinek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1976-04">April 1976</date>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="532" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Some experiments with largevocabulary isolated-word sentence recognition</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Bahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>I&lt;</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>De Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jelinek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">14</forename><forename type="middle">A</forename><surname>Mercer</surname></persName>
		</author>
		<author>
			<persName><surname>Picheny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<date type="published" when="1964-04">April 1964</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">BYBLOS: the BBN continuous speech recognition system</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">L</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">O</forename><surname>Dunham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">A</forename><surname>Kimball</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Krasner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">F</forename><surname>Kubala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rfakhoul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roucos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<date type="published" when="1967-04">April 1967</date>
			<biblScope unit="page" from="69" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Discrimination of Japanese voiced stops using Hidden Markov Model</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hanazawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kawabata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I&lt;</forename><surname>Shikano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the Acouslical Society of Japan</title>
		<imprint>
			<date type="published" when="1967-10">October 1967</date>
			<biblScope unit="page" from="19" to="20" />
		</imprint>
	</monogr>
	<note>in Japanese</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
