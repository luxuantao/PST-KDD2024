<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sentiment analysis using deep learning approaches: an overview</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-12-26">26 December 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Olivier</forename><surname>Habimana</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
								<address>
									<postCode>430074</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuhua</forename><surname>Li</surname></persName>
							<email>idcliyuhua@hust.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Ruixuan</forename><surname>Li</surname></persName>
							<email>rxli@hust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
								<address>
									<postCode>430074</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiwu</forename><surname>Gu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
								<address>
									<postCode>430074</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ge</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
								<address>
									<postCode>430074</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<postCode>110819</postCode>
									<settlement>Shenyang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Sentiment analysis using deep learning approaches: an overview</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-12-26">26 December 2019</date>
						</imprint>
					</monogr>
					<idno type="MD5">682990C6648D74280DEB381B45CAB656</idno>
					<idno type="DOI">10.1007/s11432-018-9941-6</idno>
					<note type="submission">Received 18 August 2018/Revised 2 December 2018/Accepted 4 June 2019/</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T04:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>sentiment analysis</term>
					<term>opinion mining</term>
					<term>deep learning</term>
					<term>neural network</term>
					<term>natural language processing (NLP)</term>
					<term>social network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Nowadays, with the increasing number of Web 2.0 tools, users generate huge amounts of data in an enormous and dynamic way. In this regard, the sentiment analysis appeared to be an important tool that allows the automation of getting insight from the user-generated data. Recently, deep learning approaches have been proposed for different sentiment analysis tasks and have achieved state-of-the-art results. Therefore, in order to help researchers to depict quickly the current progress as well as current issues to be addressed, in this paper, we review deep learning approaches that have been applied to various sentiment analysis tasks and their trends of development. This study also provides the performance analysis of different deep learning models on a particular dataset at the end of each sentiment analysis task. Toward the end, the review highlights current issues and hypothesized solutions to be taken into account in future work. Moreover, based on knowledge learned from previous studies, the future work subsection shows the suggestions that can be incorporated into new deep learning models to yield better performance. Suggestions include the use of bidirectional encoder representations from transformers (BERT), sentiment-specific word embedding models, cognition-based attention models, common sense knowledge, reinforcement learning, and generative adversarial networks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, there is a remarkable increase in the number of Web 2.0 tools like online social media and e-commerce websites where users freely express their ideas and thoughts. Owing to this increase, huge amounts of data are generated. Therefore, sentiment analysis was introduced as a tool for automatic extraction of insight and useful information from the user-generated data <ref type="bibr" target="#b0">[1]</ref>. Sentiment analysis is one of the natural language processing (NLP) tasks. It has attracted a large number of researchers and industry communities because of its usefulness and challenges <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>.</p><p>Sentiment analysis is a field of study whose main objective is to identify and examine the components of a person's opinion. According to the definition by Liu <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4]</ref>, an opinion normally consists of an entity, aspects of an entity, and the sentiment of aspect that represents its polarity. Furthermore, it includes the author of the opinion and the time when the opinion is expressed. The sentiments of an aspect are classified into different categories depending on the purpose of sentiment classification. For example, Yang et al. <ref type="bibr" target="#b4">[5]</ref> classified tweets into positive, negative and neutral classes. Similarly, <ref type="bibr">Kalchbrenner et</ref> al. <ref type="bibr" target="#b5">[6]</ref> classified the movie reviews into negative, somewhat negative, neutral, somewhat positive, and positive classes.</p><p>In past years, several approaches for sentiment analysis have been proposed and pioneered by Stone et al. <ref type="bibr" target="#b6">[7]</ref> who developed a computer system for content analysis using lexicon. Later, Pang et al. <ref type="bibr" target="#b7">[8]</ref> applied machine learning to find the sentiment expressed in text data. These traditional approaches have achieved good results but the feature engineering they rely on is a tedious task. Later, researchers realized that finding the sentiment for today's user-generated data requires deep understanding and efficient methods are needed to cope with it. Therefore, deep learning (DL) approaches evolved as efficient methods due to their capability of learning the text without manual feature engineering. DL approaches have been proved to outperform the traditional methods in sentiment analysis. Thus, we review the recent DL models that have been proposed for various tasks of sentiment analysis.</p><p>In contrast to existing surveys of sentiment analysis using DL methods, we discuss the most recent DL approaches and their variants that have been applied to different sentiment analysis tasks. The recent survey <ref type="bibr" target="#b8">[9]</ref> has reviewed DL approaches for sentiment analysis but our work is conducted from a different perspective. In addition, it did not address trending DL methods like deep reinforcement learning (DRL) and generative adversarial networks (GANs). Rojas-Barahona et al. <ref type="bibr" target="#b9">[10]</ref> emphasized on the technical overview of DL for sentiment analysis but they did not discuss most variants of DL approaches and recent studies. Therefore, we are sure that this review will give readers a complete idea of recent trends, with recent studies and new methods in sentiment analysis using DL approaches such as traditional attention mechanism, cognitive attention based models, DRL models, and GANs. Furthermore, different from the above-cited reviews, readers will get an introduction of sentiment-specific word embedding models and different datasets used in sentiment analysis. Moreover, to help readers quickly get the top performing models on a particular task and dataset, we provide the performance analysis of different DL models for different tasks of sentiment analysis. At the end of this paper, readers will know the recent research issues that need to be addressed and the future directions.</p><p>The rest of the review is structured as follows. Section 2 provides the background of sentiment analysis. In Section 3, we discuss word embedding models and offer a quick introduction of different DL approaches that are commonly applied in sentiment analysis. Section 4 introduces various real-world datasets that are commonly used in sentiment analysis. In Section 5, we discuss the applications and performance of DL approaches on various tasks of sentiment analysis. The current issues that are worthy to be addressed and future directions are highlighted in Section 6. Finally, Section 7 concludes the review with final remarks.</p><p>2 Background of sentiment analysis</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Application of sentiment analysis</head><p>The need for sentiment analysis ranges from individuals to large organizations and governments. An individual customer first checks the ratings of a product and opinions from other customers before making his/her purchasing decision, and business organizations use sentiment analysis tools to understand their customers' feeling. Several efforts <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref> have been made in applying sentiment analysis to customer reviews. Therefore, analyzing customer reviews have been proved to boost the relevant market and increase the confidence of customers <ref type="bibr" target="#b14">[15]</ref>. In addition, the governments analyze the feelings of the public about trending topics like elections and their policies. A recent case is the prediction of the 2016 USA presidential election <ref type="bibr" target="#b15">[16]</ref>. Furthermore, sentiment analysis can be used to enhance the capability of recommendation systems where users' interests can be identified <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Tasks of sentiment analysis</head><p>In satisfying the need of different individuals and organizations, sentiment analysis consists of five main tasks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4]</ref>, which are based on the five components of the opinion identified in Section 1. The first task deals with entity extraction and categorization. This task is perceived as named entity recognition (NER) that was first introduced in 2005 <ref type="bibr" target="#b18">[19]</ref>. Also, in 2004, Hu et al. <ref type="bibr" target="#b19">[20]</ref> introduced the second task, which involves aspect extraction and categorization. To address sentiment or opinion classification, the third task was popularized in 2002 <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b20">21]</ref>. In 2004, Kim et al. <ref type="bibr" target="#b21">[22]</ref> started the fourth task that handles opinion holder extraction and categorization. The fifth task, also considered as NER, was proposed to deal with time extraction and standardization. In the last task, all five components are extracted at the same time. In addition to the aforesaid tasks, Pozzi et al. <ref type="bibr" target="#b0">[1]</ref> consolidated other tasks including emotion detection suggested in 2005 <ref type="bibr" target="#b22">[23]</ref>, opinion spam detection instigated in 2008 <ref type="bibr" target="#b23">[24]</ref>, multi-lingual sentiment analysis initiated in 2009 <ref type="bibr" target="#b24">[25]</ref>, multi-modal sentiment analysis introduced in 2011 <ref type="bibr" target="#b25">[26]</ref> and opinion summarization <ref type="bibr" target="#b19">[20]</ref>. Furthermore, there are other tasks like sentiment dynamic tracking <ref type="bibr" target="#b26">[27]</ref> popularized in 2012 and sentiment collocation <ref type="bibr" target="#b27">[28]</ref> that involves the extraction of the targets and related opinion terms based on their correlation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Levels of sentiment analysis</head><p>Conducting sentiment analysis is more than classifying a document or a sentence into positive or negative classes. Indeed, finding the sentiment discussed in every aspect or feature of the entity is of prime importance. Therefore, depending upon the granularity required, the sentiment analysis task is performed at the document, sentence and aspect level. You can refer to <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4]</ref> for detailed explanations about levels of sentiment analysis.</p><p>In late 2002, the concept of document level sentiment analysis was popularized by <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b20">21]</ref>, which mainly focuses on finding the polarity of the whole opinionated document with respect to a single entity (e.g., classifying the whole review document which talks about a given T-shirt). Without hesitation, we express our gratitude to this task for popularizing sentiment analysis. However, document level sentiment analysis later proved to be limited for providing enough information, as it does not consider different sentences and aspects that a document may contain. Thus, in 2004, sentence level sentiment analysis appeared to alleviate the problem by assigning the polarity to each opinionated sentence in the document <ref type="bibr" target="#b21">[22]</ref>. For example, the sentence "This is a very good movie I saw!!" is classified as expressing positive sentiment. However, sentence level sentiment analysis suffers the same problem as the document level sentiment analysis for not providing enough information on what the customer really likes or dislikes, because a sentence may contain multiple entities with different aspects.</p><p>Consequently, to address the issue of dealing with multiple entities with different aspects, Hu et al. <ref type="bibr" target="#b19">[20]</ref> in 2004 started a new milestone of feature level sentiment analysis by introducing feature-based opinion mining and summarization technique, which takes into account different product's aspects and their sentiment polarity. Nowadays, sentiment analysis at this level is commonly known as aspect based sentiment analysis (ABSA) <ref type="bibr" target="#b3">[4]</ref>. Specifically, it analyzes different features of an entity and finds exactly what someone likes or dislikes about that entity or aspect of the entity in a discussion. Considering the following example, "The food is good but the service is bad", we can see clearly that the sentence is positive with respect to the aspect "food" but is negative with respect to the aspect "service". So, ABSA is one of the most challenging yet highly needed task of sentiment analysis <ref type="bibr" target="#b1">[2]</ref>. Moreover, today's reallife applications of sentiment analysis are based on this level <ref type="bibr" target="#b3">[4]</ref>. Thus, ABSA embodies many subtasks including aspect term extraction (ATE) <ref type="bibr" target="#b28">[29]</ref>, aspect-term sentiment analysis (ATSA) and aspect-category sentiment analysis (ACSA) <ref type="bibr" target="#b29">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Traditional approaches for sentiment analysis</head><p>The industrial and academic community have paid attention to sentiment analysis in order to assist in decision making. Consequently, researchers have proposed a large number of approaches in order to satisfy the need of sentiment analysis. The proposed traditional approaches have been proved to get good results by proper feature engineering. Thus, the commonly used features by these approaches in sentiment analysis are: part of speech (POS) tags <ref type="bibr" target="#b7">[8]</ref>, term position <ref type="bibr" target="#b7">[8]</ref>, opinion words and sentences <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref>, negation <ref type="bibr" target="#b32">[33]</ref>, term presence and frequency <ref type="bibr" target="#b33">[34]</ref>, and syntactic dependency <ref type="bibr" target="#b34">[35]</ref>. In quest of details about these features, you can refer to <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8]</ref>. Therefore, this subsection is a brief overview of different traditional approaches for sentiment analysis with their limitations and their source references. The traditional approaches for sentiment analysis are classified into two categories: lexicon-based and machine learning approaches <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b35">36]</ref>.</p><p>The lexicon-based approaches are types of the traditional approaches for sentiment analysis that use precompiled sentiment lexicons containing different words and their polarity to classify a given word into positive or negative sentiment class labels. The studies <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37]</ref> provide a detailed description of these approaches. Stone et al. <ref type="bibr" target="#b6">[7]</ref> started the task of sentiment analysis using the lexicon method in 1966. Later, different lexicons were proposed such as WordNet, WordNet-Affect, SenticNet, MPQA, and SentiWordNet <ref type="bibr" target="#b30">[31]</ref>. Following the popularity of lexicons, extensive research has been done for sentiment analysis based on lexicons <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b37">38]</ref>. These approaches do not require the training dataset. However, the construction of the sentiment lexicon construction for today's user-generated unstructured data is a challenging task. Consequently, machine learning approaches help to alleviate the problem.</p><p>Machine learning approaches are other traditional methods for sentiment analysis that are based on the machine learning algorithms to classify the words into their corresponding sentiment labels. The main benefit of machine learning approaches is their ability of representation learning. Pang et al. <ref type="bibr" target="#b7">[8]</ref> pioneered the use of these techniques for sentiment analysis. The surveys <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b35">36]</ref> give detailed explanations of these approaches. Machine learning algorithms require the training dataset which helps to automate the classifier and test dataset used for checking the operability of the classifier. Therefore, machine learning approaches are preferred for sentiment analysis due to their capacity for dealing with large amounts of data compared with lexicon based approaches <ref type="bibr" target="#b38">[39]</ref>. However, in case there are no human annotated datasets, the majority of people choose to use lexicon based approaches <ref type="bibr" target="#b39">[40]</ref>. Thus, extensive research has been done for sentiment analysis using machine learning approaches <ref type="bibr" target="#b40">[41]</ref><ref type="bibr" target="#b41">[42]</ref><ref type="bibr" target="#b42">[43]</ref>. As far as the good results for sentiment analysis are concerned, the two traditional approaches for sentiment analysis can be combined in order to gain the advantages of each approach. Thus, the recent advents <ref type="bibr" target="#b43">[44]</ref><ref type="bibr" target="#b44">[45]</ref><ref type="bibr" target="#b45">[46]</ref> are results for the combination of the two traditional approaches.</p><p>However, the traditional approaches for sentiment analysis are accused of being inefficient to cope with the new trend of data with dynamic nature of language, increase of high dimensional data, structural and cultural subtleties of short text like tweets. However, it is still a challenge to these traditional approaches to adjust a designed model for a specific task to a new task, especially lexicon based methods. Moreover, as mentioned in the introduction section, these approaches are based on the features engineering, which has been proved to be a tedious task with the present data <ref type="bibr" target="#b1">[2]</ref>. In addition, multiclass classification leads the performance of traditional approaches to be poor, degraded and limited <ref type="bibr" target="#b1">[2]</ref>. Thus, researchers mentioned the need of new approaches to solve these trending issues.</p><p>Consequently, to cope with the new trend of data, where efficient approaches for sentiment analysis are needed, researchers realized that the DL approaches give incredible results as affirmed by <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b47">48]</ref>, and hence they are adopted for sentiment analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Deep learning approaches</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Introduction</head><p>DL is an emerging branch of machine learning algorithms, which is inspired by artificial neural networks. It offers ways of learning the data representations in a supervised and unsupervised way with the help of the hierarchy of layers, which allow multiple processing <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b49">50]</ref>. Foremost, the adoption of DL approaches in sentiment analysis has been driven by their ability of automatic feature learning, where they can learn automatically and discover discriminative and exploratory input representations from data themselves <ref type="bibr" target="#b7">[8]</ref>. Moreover, their adoption has been motivated by the increase of the training data with multiclass classification and the success of word embeddings <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b51">52]</ref>. Besides, the availability of powerful computing resources like the graphics processing unit (GPU) that allows efficient matrix manipulation <ref type="bibr" target="#b52">[53]</ref> has also become the driving force to embrace DL approaches. Recently, researchers have proposed a large number of DL approaches, and most of them have been applied to sentiment analysis. These approaches have been proved to be effective methods in sentiment analysis, which is evidenced by many studies that have been successfully done. They have solved complex issues like domain adaptation, being able deal with the context in which the word appears, and to model long-range dependencies which can change the polarity of a statement in a given sentence <ref type="bibr" target="#b53">[54]</ref>. Consequently, this section first introduces word embedding approaches as the first data processing layer in DL methods and then proceed to various DL approaches with their trends of development. This study groups DL approaches into six categories: unsupervised pre-trained networks (UPNs), convolutional neural networks (CNNs), recurrent neural networks (RNNs), recursive neural networks (RvNNs), DRL, and hybrid neural networks. We bring to the readers' attention that in this review we mainly talk about two new DL approaches: GANs and DRLs, and we recommend the readers to survey <ref type="bibr" target="#b8">[9]</ref> for the graphical and mathematical details of remaining DL approaches. Accordingly, Figure <ref type="figure">1</ref> shows the detailed classification of the above-mentioned approaches and their proposed years.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Word embeddings</head><p>Word embeddings are types of word representation that aim at representing words' meaning in the form of vectors, where words with similar meaning and context are represented by similar vectors. Word embeddings are considered as important ingredients in sentiment analysis as well as in other NLP tasks, and they serve as first data processing layer in DL approaches <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b54">55]</ref>. Therefore, this subsection introduces different word embeddings that are commonly used in sentiment analysis.</p><p>Recent word embeddings follow the distribution hypothesis <ref type="bibr" target="#b55">[56]</ref>, where the words with the same context have similar meanings. Thus, the words with the same context or similar semantics create similar features and are classified in one class. Bengio et al. <ref type="bibr" target="#b56">[57]</ref> initiated word embeddings by designing a language model, which learns distributed representation for each word and the likelihood function for word sequences at the same time. Following the success of this model, extensive approaches have been suggested to improve the results and capture semantic and syntactic information. Collobert et al. <ref type="bibr" target="#b57">[58]</ref> constructed a pre-trained word embedding model based on the DL model, which can learn the features necessary to a specific task when the prior information is not enough. The latter work has inspired and laid the foundation of many recent studies in the domain.</p><p>Consequently, different pre-trained word embeddings have been proposed and are available to the public. The popular pre-trained word embeddings used in the sentiment analysis are results of the combination of different models for word representations. Accordingly, Mikolov et al. <ref type="bibr" target="#b58">[59]</ref> proposed word2vec 1) , which is the result of the combination of the skip-gram and continuous bag-of-word (CBOW). The CBOW model predicts the center word from its surrounding context words whereas the skip-gram model predicts surrounding context words when given a center word. Another popular word embedding model GloVe 2) means global vector has been developed by the NLP group at Stanford University <ref type="bibr" target="#b59">[60]</ref>. GloVe combines global matrix decomposition and local context window. Similarly, Joulin et al. <ref type="bibr" target="#b60">[61]</ref> proposed fasttex 3)  word embedding method where each word is represented by the character n-gram. Furthermore, these word embeddings can be refined using another word embedding. For example, Zou et al. <ref type="bibr" target="#b61">[62]</ref> constructed a model that can be applied to both GloVe and word2vec for the purpose of capturing both semantic and sentimental information of the words. Moreover, recently NLP has received a breakthrough bidirectional encoder representations from transformers (BERT) 4) language model that produces contextualized representation learning for different NLP tasks <ref type="bibr" target="#b62">[63]</ref>. BERT makes use of transformer <ref type="bibr" target="#b63">[64]</ref>, which is based on a self-attention mechanism that explores the contextual relationship between words or sub-words that make input text.</p><p>However, the above word embeddings represent the word distributions without taking into account any specific task. Therefore, to address the problem, researchers have recently suggested word embedding models, which are tailored to sentiment analysis. Tang et al. <ref type="bibr" target="#b64">[65]</ref> developed sentiment-specific word embedding (SSWE) that takes into account sentiment information whereas Zhou et al. <ref type="bibr" target="#b65">[66]</ref> suggested a word embedding model that works in a cross-lingual setting to solve the problem of the semantic gap between English-Chinese for sentiment classification. Besides, Fu et al. <ref type="bibr" target="#b66">[67]</ref> designed an SSWE that combines both local context and global sentiment information. Therefore, these characteristics of effectively understanding and representing the structure of words in the form of vectors lead the DL approaches to succeed in sentiment analysis.</p><p>Thus, the following subsections provide a brief introduction of DL approaches that are applied in sentiment analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Unsupervised pre-trained networks</head><p>UPNs are types of deep neural networks that allow the unsupervised algorithm to pre-train the layers of the network with unlabeled data, and then this becomes the initial state for the final stage. And in the final stage, the network is fine-tuned with the supervised training <ref type="bibr" target="#b67">[68]</ref>. Thus, the pre-training phase enables the network to converge quickly and learns the features from unlabeled data. Therefore, this capability of UPNs to learn features from unlabeled data through the reconstruction <ref type="bibr" target="#b68">[69]</ref> has driven efforts to apply them in sentiment analysis. The commonly used UPNs in sentiment analysis are autoencoders (AEs), deep belief networks (DBNs), and GANs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Autoencoders</head><p>In 1987, Ballard <ref type="bibr" target="#b69">[70]</ref> introduced autoencoders that allow the mapping of inputs to their outputs. Autoencoders are trained to reconstruct their inputs by minimizing the reconstruction error. Generally, they offer the following advantages: they can be used as standalone networks; they can serve as basic building  blocks of other DL architectures <ref type="bibr" target="#b70">[71]</ref>. In addition, autoencoders are considered as feature learning and dimensionality reduction techniques. In sentiment analysis, the commonly applied variants of autoencoders are denoising autoencoders (DAs) <ref type="bibr" target="#b71">[72]</ref> introduced in 2008 and stacked denoising autoencoders (SDAs). The DAs have shown the capability to increase the robustness to noisy data and to reconstruct corrupted data. Particularly, in sentiment analysis, DAs were applied in domain adaptation by training a model on a labeled dataset and apply it on another unlabeled dataset. However, one layer of DA is not enough to obtain good representations because it could increase the reconstruction error. Therefore, a good solution is to stack different DAs (SDAs) because they increase the depth of a network, which is good for obtaining good representations. Thus, SDA helps to keep the number of training parameters as low as possible. Recently, some methods have been proposed in sentiment analysis, e.g., Rong et al. <ref type="bibr" target="#b72">[73]</ref> proposed an autoencoder based method to address the issue of curse dimensionality in text document. Similarly, Zhou et al. <ref type="bibr" target="#b73">[74]</ref> designed a cross-lingual sentiment analysis model, which eliminates the gap between English and Chinese. However, autoencoders exhibit some limitations. First, it is very difficult to interpret the underlying mathematics behind them. Second, they can easily overfit. You can refer to <ref type="bibr" target="#b70">[71]</ref> for a detailed mathematical description of autoencoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Deep belief networks</head><p>DBNs are types of UPN initiated in 2006 <ref type="bibr" target="#b74">[75]</ref>. They contain multiple hidden units in the hidden layers that help to learn higher level representations from input variables. The DBNs comprise restricted Boltzmann machines (RBMs) that deal with higher level features in an unsupervised manner and a feed-forward network that updates the parameters of the model <ref type="bibr" target="#b68">[69]</ref>. Nevertheless, units between the hidden layers in DBNs are bidirectional, which makes DBNs differ from other feed-forward neural networks. Additionally, there are no connections between units within the same layers. This difference is accomplished by the RBM layers in the pre-trained phase. The main advantage of DBNs is that they can exploit a large amount of unlabeled data as they work on the principle of layer-wise pre-training. However, the primary cons of DBNs lie in the fact that they are difficult to train. Recently, Refs. <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b75">76]</ref> have tried to apply DBNs in sentiment analysis. For a detailed mathematical description of DBN, we recommend <ref type="bibr" target="#b70">[71]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Generative adversarial networks</head><p>The GAN is a kind of UPN invented by Ian Goodfellow at Google brain in 2014 <ref type="bibr" target="#b76">[77]</ref>. GAN has discriminative (D) and generative (G) models that are trained in an unsupervised and competitive fashion. The generative model learns the data distribution of target data while the discriminative model evaluates if examples are from the training set or from the generative model. We illustrate the GAN framework with G and D during the training process in Figure <ref type="figure" target="#fig_0">2</ref>.</p><p>In the original formulation of the GAN problem <ref type="bibr" target="#b76">[77]</ref>, it is perceived as a game of forging money. In the game, G plays the role of forging bills while D, the expert, tries to recognize those forged bills. Therefore, as D discovers the fake bills, G improves its skills of producing fake bills accordingly. Following the definition in <ref type="bibr" target="#b77">[78]</ref>, the GAN game is mathematically defined as follows: let x, z and R be the characteristic of real bills, fake bills and the metric that measures the real bill, respectively. Therefore, the task of D is to reduce the quantity of making fake bills R(G(z)) as close as possible to R(x), the standard of real bills. Meanwhile, G also keeps trying to find a way to increase R(G(z)). Roughly speaking, both models keep depending on each other but do not control each other's parameters. Thus, the game ends when both quantities are in a Nash equilibrium, i.e., when D has arrived at the optimum point.</p><p>In brief, the D and G play the minimax game, also called sum-zero-game, which is described in the following <ref type="bibr" target="#b76">[77]</ref>:</p><formula xml:id="formula_0">min G max D R(D, G) = E x ∼ p data [logD(x)] + E z ∼ p z [log(1 -D(G(z)))].<label>(1)</label></formula><p>Therefore, as described above, D wants to maximize the quantity R(D, G) by making both logD(x) = 1 and D(G(z)) = 0. Also, G wants to minimize the quantity R(D, G) by making D(G(z)) = 1.</p><p>In sentiment analysis, GAN models are at the inception stage since only a few relevant contributions are found. Li et al. <ref type="bibr" target="#b78">[79]</ref> proposed a GAN model for category sentence generation. The generator of the proposed model is a long short-term memory (LSTM) implemented as a reinforcement learning agent while CNN is implemented as a discriminator. Similarly, Vlachostergiou et al. <ref type="bibr" target="#b79">[80]</ref> designed a GAN model that is implemented using a denoising autoencoder in order to generate useful representations for different NLP tasks including sentiment analysis.</p><p>However, GANs exhibit the limitations that are mainly related to their training process. Li et al. <ref type="bibr" target="#b80">[81]</ref> highlighted that the training dynamics of GANs often present two challenges: mode collapse and vanishing gradients. To solve the issues, Li et al. <ref type="bibr" target="#b80">[81]</ref> proposed a model that guarantees the GANs to converge. Furthermore, the training of GANs requires finding the Nash equilibrium, which is considered to be less efficient than optimizing the actual objective function <ref type="bibr" target="#b77">[78]</ref>. Therefore, this problem is still an open issue for the GANs research community. Moreover, the application of GANs in the NLP domain is still hindered by convergence issues and the problem of dealing with discrete data <ref type="bibr" target="#b81">[82]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Convolutional neural networks</head><p>CNNs are types of feed-forward neural networks introduced in 1989 <ref type="bibr" target="#b82">[83]</ref>. The application of CNN in the artificial neural network domain was inspired by the process of the animal visual cortex. Each individual neuron of the visual cortex covers a small receptive field, and then receptive fields overlap to visualize the entire object. Thus, receptive fields are considered as filters in CNN. In principle, CNN has three types of layers, namely, the input layer, the feature extraction layers and the classification layer <ref type="bibr" target="#b68">[69]</ref>. The input layer takes the raw inputs and produces the embeddings. Next, the feature extraction layers, which include convolution and pooling layers, learn the relevant features. The convolution layer applies filters known as feature detectors to learn the features and produce the feature map. The pooling layer, also known as the dimensional reduction method, is used to extract relevant features, leaving those unnecessary ones. Finally, the features produced by the feature layers are passed to the classification layer, which is made of a fully connected network with a classifier.</p><p>The application of CNN in NLP was initiated by Collobert et al. <ref type="bibr" target="#b57">[58]</ref>. Since then, CNNs have attained inspiring results in NLP <ref type="bibr" target="#b83">[84,</ref><ref type="bibr" target="#b84">85]</ref>, especially in sentiment analysis where the first kickstart was led by <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b85">86]</ref>. Therefore, the success of CNNs is attributed to the following advantages. First, they have few parameters so that it takes CNNs less time to train compared to other full connected neural networks with the same number of hidden layers <ref type="bibr" target="#b70">[71]</ref>. Second, in sentiment analysis, they are good at learning local contextual features using filters. However, in sentiment analysis, CNNs are limited in modeling long-term dependencies because they need to be very deep <ref type="bibr" target="#b86">[87,</ref><ref type="bibr" target="#b87">88]</ref>, which leads them to be computationally expensive. Furthermore, their performance is conditioned on a large number of training samples, which are not always available. Thus, RNNs were introduced as efficient methods that help to model long-term dependencies in sequential input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Recurrent neural networks</head><p>RNNs are a member of feed-forward neural networks that follow the principle proposed by Elman in 1990 <ref type="bibr" target="#b88">[89]</ref> to process sequential information. However, RNNs slightly differ from other feed-forward neural networks by directed cycles, which play an important role in the propagation of activation function to the incoming input sequence. Therefore, RNNs are said to have a memory as they can easily remember the state of previous computations. The RNNs consider the output to depend on the previous computation. Thus, RNNs present the following key benefits that have stimulated their suitability for modeling sequential input. Primarily, RNNs have shown the capability of modeling sequences input vectors of any length and long-range dependencies <ref type="bibr" target="#b89">[90]</ref>. Secondly, RNNs easily take into account contextual information at each time step while processing sequences data. Although traditional RNNs were proved to get good results by modeling long-range dependencies, they are limited to a certain extent of range between dependencies. Their limit is linked to the gradient vector that increases or decreases proportionally with long-range dependencies. Thus, the problem caused by this increase and decrease is referred to as exploding gradient and vanishing gradient, respectively <ref type="bibr" target="#b91">[91]</ref>. Remedy to the problem is the invention of LSTM.</p><p>LSTM. In 1997, Hochereiter et al. <ref type="bibr" target="#b92">[92]</ref> proposed the LSTM to face the issue of vanishing gradient by extending the classic RNN with a gating mechanism. They introduced the forget gate that allows the memory cell to keep information for a long time or throw previous computation results. However, LSTM is accused of having a complex structure. Thus, gated recurrent unit (GRU) was designed as a simplified variant of LSTM.</p><p>GRU. GRU is another variant of RNN, which is similar to LSTM <ref type="bibr" target="#b93">[93]</ref>. GRU has two layers unlike LSTM, which has three layers. The first gate named reset gate governs the combination of new input and previous computations. The second gate dubbed update gate determines what information to keep from the previous computations. GRU is considered as a simplified LSTM model and more efficient in terms of computational power compared to LTSM and vanilla RNN.</p><p>Modeling the long-range dependencies is not the last step to improve RNNs capability. A recent trend is to allow a model to pick contextual information at every time step. Gratefully, attention mechanism <ref type="bibr" target="#b94">[94]</ref> helps to realize it. In 2015, inspired by the human vision capability of focusing on a region of interest, researchers in NLP introduced the attention mechanism. This technique helps to prioritize relevant parts of the given input sequence based on its weighted representation. Moreover, since 2016, the recent surge in sentiment analysis is to apply the cognitive based attention <ref type="bibr" target="#b95">[95,</ref><ref type="bibr" target="#b96">96]</ref>, which simulates the human reading capability of gaze-fixations on the essential part of the input to be retained. This mentioned form of attention is based on the computation of reading time for every word of the input using eye-tracking movement.</p><p>RNNs based models have been remarkably applied in sentiment analysis. For example, Ref. <ref type="bibr" target="#b97">[97]</ref> proposed an LSTM, Ref. <ref type="bibr" target="#b98">[98]</ref> designed a GRU, Ref. <ref type="bibr" target="#b99">[99]</ref> suggested bidirectional LSTM (Bi-LSTM), and Ref. <ref type="bibr" target="#b100">[100]</ref> explored attention based bidirectional (Bi-GRU). Although significant progress has been made in improving LSTM and GRU capability, they still have some pitfalls. Initially, it is hard to train them because the memory adds several weights to each node of sequence input during training. Fortunately, GPU has alleviated the problem of training complex models. Furthermore, they are not able to deal with sequential input of arbitrary shapes like a tree. Thus, recursive neural networks allow RNNs to deal with input of arbitrary shape.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Recursive neural networks</head><p>In 1996, Goller et al. <ref type="bibr" target="#b101">[101]</ref> introduced RvNNs, which is viewed as a generalized version of RNN. RvNNs allow the neural networks to deal with structured inputs of any shape like trees and graphs. In contrast to the original RNN that shares the weights across the whole input sequence, RvNNs allow the weights to be distributed across each node of the structured representation of the input. In sentiment analysis, the commonly used variants of RvNN are recursive neural tensor network (RNTN) and semi-supervised recursive autoencoder (RAE). RAE is a variant of RvNN designed in 2011 <ref type="bibr" target="#b102">[102]</ref>, which applies the same autoencoder to each node of the binary tree. The core purpose of RAE is to model the semantic representation of a sentence by taking into consideration the word order. Thus, while training RAE, the main goal is to minimize the sum of the reconstruction loss at each node of the tree.</p><p>RNTN was introduced in 2013 <ref type="bibr" target="#b103">[103]</ref>. Originally, RNTN was designed for sentiment analysis. Its main intention is to capture the sentiment of a sentence with any length by not only relying on its components but also exploring the order in which words are syntactically grouped. Thus, RNTN carries out this representation based on a tensor-based composition function, which is applied to all nodes of the tree. However, RNTN presents two distinctive features compared to RAE. Firstly, the tree structure is fixed. Secondly, the reconstruction loss in RNTN is ignored. Consequently, the last difference is counted as the main advantage of RNTN over RAE, as the reconstruction loss increases the computational overhead at each node.</p><p>RvNNs have shown impressive results in sentiment analysis due to their capability of representing the sequence input in the form of the tree and the ability to represent the context in which a word appears. Furthermore, their ability to learn semantic and syntactic information from the inputs leads them to succeed in sentiment analysis <ref type="bibr" target="#b104">[104,</ref><ref type="bibr" target="#b105">105]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Deep reinforcement learning</head><p>The DRL is an emerging area in the field of DL where the first kick-starting was in 2013 by the DeepMind Technologies group <ref type="bibr" target="#b106">[106]</ref>. Generally, DRL integrates the advantage of DL's great insight and that of reinforcement learning (RL)'s decision making and then gains the output control directly from crude input by an end-to-end learning process <ref type="bibr" target="#b107">[107]</ref>. Their integration resulted in a substantial breakthrough in a lot of tasks that require the great insight of high-dimension crude inputs and policy control <ref type="bibr" target="#b108">[108]</ref>. However, the DRL framework is different from other DL methods for the fact that it does not require supervision. Instead, it only emphasizes the interaction of an agent with its environment and then the agent directly gets a reward signal. Therefore, this is considered as the primary advantage it offers. Consequently, it can be used to solve the problems in which obtaining the target labels is difficult. You can refer to <ref type="bibr" target="#b109">[109]</ref>, for details of reinforcement learning from its inception.</p><p>DRL frameworks are based on Markov decision process (MDP) that helps to train an intelligent agent, which interacts with the environment at each time. The goal of this interaction is to maximize the longterm reward as shown in Figure <ref type="figure" target="#fig_1">3</ref>. At the time step t, the agent receives a high dimension observation from the environment and uses DL to get a specific feature representation of S t ∈ S. Afterwards, the intelligent agent judges adaptive expectations of various actions and maps the current state S t ∈ S to the action A t ∈ A guided by the policy π(A t |S t ). The policy is considered as a lookup table of the received states and the actions to be taken for those states. Later on, as a consequence of its actions, the agent receives a new reward R t+1 and achieves a new observation from the environment. DRL methods are systematically categorized into two main categories: value-based and policy-based DRL methods. The core idea of the value-based DRL method is to approximate the value function by using deep neural networks, which help to get good performance in large-scale discrete action space. The commonly used algorithm in this category is deep Q-network (DQN) <ref type="bibr" target="#b110">[110]</ref>. While the policy-based DRL algorithm emphasizes to optimize parametrized policies by targeting the long-term reward. For policy-based DRL, sentiment analysis has witnessed the use of the REINFORCE algorithm <ref type="bibr" target="#b111">[111]</ref> for training models with complex structure.</p><p>Sentiment analysis heavily relies on the learned representations produced by word embedding methods. However, these models lack a task-specific structure. Therefore, by considering the obtained word and sentence representations as a sequential decision problem, DRLs have shown the promising capability of automatically obtaining an optimized structure, which is useful in NLP, especially in sentiment analysis <ref type="bibr" target="#b112">[112,</ref><ref type="bibr" target="#b113">113]</ref>. Zhang et al. <ref type="bibr" target="#b112">[112]</ref> considered the current action of structure discovery to have an effect on the subsequent decisions, which can be considered in the policy gradient method. Like any other classification task, in this setting, the reward is the probability of predicting the correct label to the input sentence based on the obtained structured representation. Training different models with such algorithm have shown improvement in results compared to the supervised methods. Chen et al. <ref type="bibr" target="#b114">[114]</ref> enjoyed the beauty of REINFORCE by training a gated multi-modal embedding and LSTM with attention (GME-LSTM(A)) proposed for multi-modal sentiment analysis. In GME-LSTM(A) framework, the controller is implemented as an agent that interacts with the environment by receiving the weights and inputs. Afterwards, it decides to reject or accept the inputs based on the mean absolute error (MAE) received as a reward.</p><p>However, despite the advantages offered by reinforcement learning, it presents some disadvantages. First, it is very hard to design a model in which the reward function will be trained <ref type="bibr" target="#b115">[115]</ref>. Second, RL models suffer from data inefficiency, which is the high rate of agent-environment interactions <ref type="bibr" target="#b116">[116]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.8">Hybrid deep neural networks</head><p>Individual DL models have been extensively used and proved to produce impressive results in NLP <ref type="bibr" target="#b117">[117]</ref>, especially for sentiment analysis. Therefore, different researchers have tried to combine these approaches to improve the performance of their models by getting the benefits offered by each type. For example, CNNs are well-recognized models in extracting local features. On the other hand, RNNs are well-known to deal with long-range dependencies. Hence, the appealing idea is to integrate them so that the model can extract both types of features. Recently, researchers <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b118">118,</ref><ref type="bibr" target="#b119">119]</ref> have proposed various hybrid models to accomplish various tasks in sentiment analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Real world datasets for sentiment analysis</head><p>Getting real-world datasets in academic research is always of great value. Therefore, before we dive into DL approaches application and their performance on different tasks of sentiment analysis, it is worthwhile introducing the most popular real-world datasets that are publicly available and currently used in sentiment analysis literature. The details of each dataset are presented in Table <ref type="table" target="#tab_2">1</ref>. We bring to the readers' attention that user choice means that the splits for the dataset are not specified. Therefore, the user can randomly split as he/she wants.</p><p>• IMDB large movie review 5) <ref type="bibr" target="#b120">[120]</ref>. IMDB is a dataset that contains movie reviews and is used in binary sentiment classification.</p><p>• IMDB2 6) <ref type="bibr" target="#b121">[121]</ref>. The IMDB2 is a dataset commonly used in document level sentiment analysis. This dataset presents the advantage of providing opinion holder information. Note: the original name of this dataset is IMDB, so to differentiate it from the first IMDB Large Movie review we named it IMDB2.</p><p>• Stanford sentiment treebank (SSTb) from the Rotten Tomatoes review site. The SSTb dataset is used for binary classification (SST-2) and for fine-grained classification (SST-5) where the reviews are classified into five categories.</p><p>• Amazon product review datasets 8) <ref type="bibr" target="#b122">[122]</ref>. The review sentences in these datasets are annotated according to their star number from one to five. Amazon product review datasets are categorized according to the product type with reviews varying from 10261 to 8898041. Moreover, one may choose to use the whole dataset with all product reviews, which includes 142.8 million reviews.</p><p>• SemEval-[year]. These datasets are generated annually by the International Workshop on Semantic Evaluation. For example, SemEval2014 task 4 9) contains user reviews in laptop domain (SemEval2014-D1) and restaurant domain (SemEval2014-D2). The work <ref type="bibr" target="#b123">[123]</ref> used 2931 samples for D1 and 4712 samples for D2.</p><p>• Stanford Twitter sentiment corpus (STS) 10) <ref type="bibr" target="#b124">[124]</ref>. The STS dataset contains English tweets collected between April and June in 2009. The tweets in this dataset were grouped in positive and negative classes using emoticons as noisy labels.</p><p>• STS-Gold dataset for Twitter 11) <ref type="bibr" target="#b125">[125]</ref>. This dataset has been collected as a complement to the STS corpus in which the tweets and entities are labeled individually.</p><p>• Yelp challenge datasets 12) . Yelp datasets contain review texts given by users for products and services available at the Yelp review site. Review texts in these datasets contain the sentiment intensity ranging from one to five stars. Yelp datasets are generated in different rounds, for example, Yelp 2014, Yelp 2015, Yelp 2017 contain 1.2, 1.6, 4.1 million of review sentences, respectively.</p><p>• Multimodal corpus of sentiment intensity and subjectivity analysis (CMU-MOSI) 13) <ref type="bibr" target="#b126">[126]</ref>. CMU-MOSI is one of the datasets that are used in multimodal sentiment analysis. It consists of 93 annotated videos collected from YouTube that are divided into 2199 labeled clips/utterances. Each video consists of one speaker who comments on the movie.</p><p>• CMU multimodal opinion sentiment and emotion intensity (CMU-MOSEI) 14)  <ref type="bibr" target="#b127">[127]</ref>. CMU-MOSEI is the recently largest dataset in multimodal sentiment analysis and emotion recognition. CMU-MOSEI consists of 3229 annotated video clips that are divided into 22676 labeled clips/utterances. The videos are collected from YouTube for 1000 different speakers and cover 250 topics.</p><p>• Movie-review (MR) dataset 15)  <ref type="bibr" target="#b128">[128]</ref>. MR dataset consists of movie reviews collected from the IMDB reviews website and the sentences are labeled as positive and negative.</p><p>• Sanders Twitter sentiment dataset 16) [129]. This dataset comprises tweets about Google, Twitter, Apple and Microsoft products. The tweets in the dataset are classified into positive, negative, neutral, and irrelevant class.</p><p>• German-Deutsche Bahn 17) <ref type="bibr" target="#b130">[130]</ref>. German-Deutsche Bahn dataset consists of 21824 reviews about Deutsche Bahn German public train service.</p><p>• Arabic sentiment tweets dataset (ASTD) 18) [131] consists of 10006 tweets collected from EgyptTrends about political issues. The tweets in ASTD are categorized into positive, negative, mixed, and objective class.</p><p>• YouTube 19) <ref type="bibr" target="#b25">[26]</ref>. YouTube is a multimodal sentiment dataset collected on YouTube without considering any specific topic. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Deep learning applications on different sentiment analysis tasks</head><p>In this section, we summarize the application of DL models on various tasks of sentiment analysis. Our main objective is to depict the current progress of DL application on various sentiment analysis tasks and the results achieved on different real-world datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Document level sentiment analysis</head><p>A plethora of research has been conducted to perform sentiment analysis on the document level, which involves finding the polarity of the whole opinionated text. So, in this subsection, we present some representative studies in the domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Application</head><p>We sort out DL models designed for document level sentiment analysis into five parts: CNN, RNN with attention, RNN with user and product attention, adversarial network and hybrid models.</p><p>CNN based models. CNN based models have been applied to realize document level sentiment analysis by using different filters to learn local features from the input sentence. Johnson et al. <ref type="bibr" target="#b87">[88]</ref> introduced a deep pyramid CNN (DPCNN) model that addresses the issue of long-range dependencies in text documents without increasing the computational cost. The DPCNN model takes input from embeddings, and afterwards convolutional and max-pooling down sampling layers alternate in the form of a pyramid shape. In this way, the max-pooling layers allow a better representation of the long-range dependencies. Later on, Conneau et al. <ref type="bibr" target="#b86">[87]</ref> explored the usage of a very deep CNN (VDCNN) that learns the hierarchical representations of the document and learning long-range dependencies. To achieve its objective, VDCNN uses 29 convolutional layers with small convolutions to deal with character level information. The feature maps generated by the convolutional layers are passed to the k-max pooling, which produces fixed dimension features that are fed to the full connected classifier.</p><p>RNN with attention-based models. Attention mechanism has shown the capability to help RNN to deal with good representations of a document and to capture long-range dependencies at a low computational cost. In the same vein, Yang et al. <ref type="bibr" target="#b132">[132]</ref> designed a hierarchical attention network (HAN) model that uses two levels of Bi-GRU with attention to select relevant words and sentences in the construction of the document representation. The HAN model hierarchically constructs the representation of the sentence, and afterwards aggregates the sentence representations to obtain a full document representation sent to the classifier. Similarly, inspired by human reading capability, Long et al. <ref type="bibr" target="#b96">[96]</ref> proposed an LSTM model augmented with a cognition-based attention model (CBA), which is trained by cognition grounded eye-tracking data. Initially, the LSTM+CBA model predicts the total reading time of the sentence. Afterwards, to capture relevant words and sentences in the construction of document representation, the attention model constructed from the predicted reading time of the sentence, context, and syntactic features is applied.</p><p>RNN with the user and product attention based models. Researchers have demonstrated that it is useful to consider opinion holder preference and the product information in sentiment analysis. Chen et al. <ref type="bibr" target="#b97">[97]</ref> pioneered the trend by designing a model named neural sentiment and user product attention (NSC+UPA). The NSC+UPA jointly learns sentence and document representations using LSTM. Then, it applies the attention mechanism to the produced representations for prioritizing the most contributing user and product information. Similarly, Dou <ref type="bibr" target="#b12">[13]</ref> suggested a model dubbed user and product with deep memory network (UPDMN), which combines LSTM and deep memory model. The LSTM helps the model in the representation of the input document. The model uses the LSTM to construct the document representation. Subsequently, to compute the ratings of each document, the model applies deep memory layers that consist of the attention-based models. In the same manner, Wu et al. <ref type="bibr" target="#b13">[14]</ref> proposed an attention LSTM based model named hierarchical user attention and product attention (HUAPA). In contrast to NSC+UPA and UPDMN models, HUAPA separately learns the user and the product representations. And then, it aggregates them to make the final representations. Recently, Amplayo et al. <ref type="bibr" target="#b11">[12]</ref> designed a hybrid contextualized sentiment classifier (HCSC) that combines a CNN model used to learn short sentences and a Bi-LSTM that deals with long sentences. Certainly, HCSC is in accordance with the previous studies in some way but the main difference lies in a cold start attention model, which helps to tackle the cold start problem caused by user and product information in different reviewed texts. Adversarial network models. Motivated by the theory of domain adaptation where the prediction is accomplished using common features between sources and target domains, various researchers have performed document level sentiment analysis. Ganin et al. <ref type="bibr" target="#b133">[133]</ref> designed a model named domain-adversarial neural network (DANN), which is trained with gradient-based optimization directly on the input to a classification network. DANN model trains the classifier using both labeled and unlabeled data from the source domain and target, respectively. Afterwards, as the training progresses, the discriminative features from the source domain and indiscriminative features with regard to the change between domains are promoted. Similarly, Li et al. <ref type="bibr" target="#b134">[134]</ref> constructed an adversarial memory network (AMN) model that contains sentiment and domain classifier modules. Both modules are trained together to reduce the sentiment classification error and allow the domain classifier not to separate both domain samples. The attention mechanism is incorporated into the model to help in the selection of the pivots words, which are words useful for sentiment classification and shared between the source and target domains.</p><p>Hybrid model. To exploit the benefits offered by RNN and CNN in document level sentiment analysis, Tang et al. <ref type="bibr" target="#b119">[119]</ref> used a hybrid model that combines gated recurrent neural network (GRNN), CNN and LSTM. This model learns the representation of sentences using CNN or LSTM. At the final step, GRNN learns the semantic and the relationship between sentences to find the overall document sentiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Performance of DL models on document level sentiment classification</head><p>DL approaches have achieved overwhelming results on document level sentiment analysis as shown by Table <ref type="table" target="#tab_4">2</ref>. Based on the results, we observe that the majority and the top performing models on the four datasets (a) IMDB2, (b) Yelp full 2013, (c) Yelp full 2014, and (d) Yelp full 2015 are attention recurrent based models. Thus, this demonstrates the capability of recurrent based models to deal with sequential data like text <ref type="bibr" target="#b89">[90]</ref>. In addition, the top performing models are attention based, and this implies the advantages offered by the attention mechanism to prioritize important information and help RNN based models to address long-term dependencies. Furthermore, the benefit of considering user and product information in sentiment analysis is witnessed by the good performance attained by all models that take it into account <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b96">96]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Sentence level sentiment classification</head><p>In this subsection, we present different DL models that have been suggested on sentence level sentiment classification, which aims to find the class label of an opinionated sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Application</head><p>DL models proposed to realize sentence level sentiment classification are classified into six parts: UPN, CNN, RvNN, DRL, RNN, and RNN with cognition attention based models.</p><p>UPN based models. Initially, inspired by the dimensionality reduction capability of autoencoders, Rong et al. <ref type="bibr" target="#b72">[73]</ref> introduced an autoencoder based bagging prediction (AEBPA) model, which uses stacked autoencoder with multiple layers as feature leaning parts that help get the high-level representation of data. Each stacked auto-encoder pre-trains the network layers with unlabeled data, and finally the network is fine-tuned with the supervised training mechanism. Alike, Zhai et al. <ref type="bibr" target="#b135">[135]</ref> designed a model based on a semi-supervised autoencoder by which its loss function is relaxed to the Bregman divergence (SBDAE). The SBDAE solved the problem of task-irrelevant and high dimension. In addition, SBDAE takes into account the label information to guide the feature learning process by allowing the learned representation to be directly associated with the task of the interest. Similarly, Jin et al. <ref type="bibr" target="#b75">[76]</ref> suggested a DBN which is extended by the RBM unsupervised training phase. The RBM generates a hidden layer that serves as the input of the single layer feed-forward network, which is trained using the delta rule. Likewise, Jin et al. <ref type="bibr" target="#b10">[11]</ref> designed a model that incorporates the word positional and order information to DBN. The constructed DBN comprises the RBM used to train the initial weights in an unsupervised fashion which are transferred to the feed-forward network for backpropagation. Li et al. <ref type="bibr" target="#b78">[79]</ref> initiated the use of GANs in sentiment analysis by combining CNN, LSTM, and RL. In the proposed model, the combination of GANs with RL helps to increase the size of the dataset by generating enough sentences for classification. The LSTM implemented as an agent of RL is used as a generator to deal with the sentence structure at each time step whereas the CNN implemented as discriminator distinguishes the category of the sentence generated and provides the reward signal to LSTM. Similarly, Vlachostergiou et al. <ref type="bibr" target="#b79">[80]</ref> implemented GANs to learn useful representations in NLP, especially in sentiment analysis. Different from other GANs models, the discriminator of the proposed GANs, which is based on denoising autoencoder, does not perform binary classification of real or non-real data. Rather, it assigns the energy score to each of data received from the generator.</p><p>CNN based models. CNN based models have been extensively applied to sentence level sentiment analysis pioneered by <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b85">86]</ref>. Kim <ref type="bibr" target="#b85">[86]</ref> suggested a simple, yet efficient CNN model with two channels where each channel consists of a single convolution layer followed by max-pooling over time for sentence level sentiment analysis. In particular, this CNN applies multiple filters that contain kernels of different sizes in order to extract a large number of local features in the sentence. This shallow model gained success, and hence is used by other researchers as the baseline model. Similarly, to deal with short and long sentences using CNN, Kalchbrenner et al. <ref type="bibr" target="#b5">[6]</ref> proposed a dynamic CNN (DCNN) model that alternates a wide convolution and dynamic k-max pooling to learn the sentence's semantic structure. Compared to traditional max-pooling, the k-max pooling helps DCNN to deal with the sentence of any length. Following the popularity of CNN, Yin et al. <ref type="bibr" target="#b136">[136]</ref> designed a model named semantic lexicalaugmented CNN (SCNN) that extends CNN with lexical information of word. The SCNN model learns the sentiment information of the words using the SentiWordNet lexicon and the semantic features of the words using word2vec embedding model.</p><p>RvNN based models. To deal with input of arbitrary structure, RvNN models have been explored in sentiment analysis. Socher et al. <ref type="bibr" target="#b103">[103]</ref> initiated their use by constructing a recursive neural tensor network (RNTN) model that helps to deal with the compositionality, which is a challenging issue in sentiment analysis. The RNTN model learns the compositional vector representation for various phrases and requires the construction of a parse tree for the input sentence, which is the only preprocessing step. Later on, Tai et al. <ref type="bibr" target="#b137">[137]</ref> designed a recursive tree-LSTM model that exploits the syntactic properties of a text by modeling the state while considering the whole input sequence and different hidden states of random child units. Likewise, Kokkinos et al. <ref type="bibr" target="#b138">[138]</ref> designed a structured Tree-Bi-GRU coupled with the attention model. The incorporated attention model helps to visualize the relevant representation in the construction of the tree.</p><p>DRL based models. The success of DL models in sentiment analysis relies on the good representation of a sentence. Thus, different deep DRL models have been suggested to address the issue. Yogatama et al. <ref type="bibr" target="#b113">[113]</ref> initiated the task by integrating a tree LSTM with a reinforcement learning model. First, the tree LSTM model learns the sequential representation of the sentence. Afterwards, the RL model learns the tree structure of the represented sentence in order to find the appropriate representation. Subsequently, to find the class label for the obtained sentence representation, the RL applies the reward function, which is the likelihood to predict the correct label on the obtained sentence representation. Similarly, Zhang et al. <ref type="bibr" target="#b112">[112]</ref> suggested two models that are based on RL to learn the simplest structured representation of the text. The first model named information distilled LSTM (ID-LSTM) learns the sentence representation by filtering only the needed words in a sentence. The second model dubbed hierarchical structure LSTM (HS-LSTM) learns the representation of sentence from word level to phrase level. Finally, the classifier predicts the sentiment label for each word based on the resulting representations from HS-LSTM or ID-LSTM models.</p><p>RNN based models. Researchers have tried to perform sentiment analysis by not only relying on the use of plain DL approaches but also integrating with some other components. Correspondingly, Zharmagambetov et al. <ref type="bibr" target="#b46">[47]</ref> unified a deep RNN with decision tree (DeepRNN+DT) models. The Deep-RNN+DT model is built on top of word2vec pre-trained embedding and trained using the traditional RNN. At last, the random forest classifier assigns the sentiment label to each sentence. Similarly, Mousa et al. <ref type="bibr" target="#b139">[139]</ref> explored the use of a generative model named contextual Bi-LSTM with a language model (cBi-LSTM LM), which changes the structure of Bi-LSTM to learn the word's contextual information based on its right and left contexts. With the LM model based on the Bi-LSTM, a separate probability distribution is estimated for every sentiment from the training data. The same work constructed a simple discriminative model based on Bi-LSTM that was also applied on the same dataset. To get better results for sentiment classification, both generative and discriminative models were combined to form a single model cBi+LSTM LM+Bi-LSTM (we dub it CBLSTM+LM) classifier.</p><p>RNN with cognition attention-based model. Performance in sentiment analysis has not yet reached the desired level. Thus, a new signal in sentiment analysis is to apply the cognition attentionbased model. Inspired by the human reading capability, Mishra et al. <ref type="bibr" target="#b95">[95]</ref> introduced a hierarchical LSTM based model (Sentiment+PoS+Gaze, we dub it Sent+Gaze) trained by cognition grounded eyetracking data that predicts overall review text's sentiment. First, the proposed model predicts human gaze behavior, parts-of-speech tagging and word's syntactic information at the word level, and afterwards it fuses the predictions of word level to find the representation to be used so as to determine the overall review's sentiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Performance of DL models on sentence level sentiment classification</head><p>Table <ref type="table" target="#tab_5">3</ref> shows the inspiring results attained by DL methods on sentence level sentiment classification task. Foremost, the first observation is the remarkable performance of BERT introduced in Subsection 3.2. BERT improved the performance accuracy by 5.4% on (b) SST-2. Thus, this shows its power to produce and use contextualized representations. Furthermore, we observed that the top performing models on the four datasets (a) SST-5, (b) SST-2, (c) IMDB, and (d) MR are RNNs based models. Thus, the results confirm the capability of RNN for modeling sequential input. Moreover, the good performance of Tree-GRU(att) model <ref type="bibr" target="#b138">[138]</ref> on both SST-2 and SST-5 datasets can be interpreted in two ways. First, it is a justification for the good capability of the recursive model to represent the input sentence in a tree structure. Thus, this representation helps to learn semantic and syntactic information of input. Second, the good performance can be attributed to the attention mechanism that helps to prioritize contextual information in the formation of the tree. Finally, yet importantly, the results reveal that the new signal cognition based attention in Sent+Gaze <ref type="bibr" target="#b95">[95]</ref> is a promising step in sentiment analysis as it outperforms its counterparts on (d) MR dataset with a great improvement in accuracy of +6.9%, and the result it achieved on (c) IMDB dataset can be compared with others. Therefore, this is a traceable good point. We did not include in the table the performance results achieved by GANs models <ref type="bibr" target="#b78">[79,</ref><ref type="bibr" target="#b79">80]</ref> because they used the subset of existing datasets and different evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Aspect-based sentiment analysis</head><p>The ABSA is the most vital task in sentiment analysis as it provides detailed information on what someone likes or dislikes about an entity or aspect. ABSA encompasses two main subtasks: ATSA and ACSA. ATSA aims to find the sentiment polarity concerning the target entities/aspects that appear in the text while ACSA aims to predict the sentiment label of a given aspect with predefined categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Application</head><p>We classify the DL models in aspect-based sentiment classification into six categories: attention based models with aspect information, attention-based models with aspect context, RNN with attention memory, RNN with common sense knowledge, CNN, and hybrid models.</p><p>Attention based models with aspect information. Wang et al. <ref type="bibr" target="#b140">[140]</ref> designed an attention-based LSTM with aspect embedding (ATAE-LSTM) model for aspect sentiment analysis. The ATAE-LSTM model takes both the aspect embedding and word input vector as input in order to gain the advantages of aspect information. In this regard, LSTM produces the hidden vectors containing the information from the input aspect and the word input vector. Meanwhile, the attention mechanism helps the ATAE-LSTM model to attend an important part of the sentence when it is given input as a certain aspect that is targeted. Likewise, Tay et al. <ref type="bibr" target="#b141">[141]</ref> suggested an aspect fusion LSTM (AF-LSTM(CONV)) model for ABSA that incorporates aspect information into the neural network. Initially, AF-LSTM models the relationship between an aspect and its contextual words with a novel circular convolution of vectors. Afterwards, the resulted vector from the aspect fusion is passed to the attention layer, which produces the weighted representation of the sentence that is sent to the classifier.</p><p>Attention based models with aspect context. Modeling the association between the target aspect and the context where it appears have also been explored in ABSA. Accordingly, Zhang et al. <ref type="bibr" target="#b142">[142]</ref> proved this with a gated recurrent neural network (Bi-GRNN) model suggested for target sentiment analysis. The Bi-GRNN model deals with noisy text and learns the semantic and syntactic information from input tweets. In addition, it models the contextual information where a word appears. Similarly, Yang et al. <ref type="bibr" target="#b4">[5]</ref> designed an ABi-LSTM model that leverages the Bi-LSTM with an attention mechanism. The ABi-LSTM models the context where each target appears by processing the sentence in forward and backward directions. Afterwards, it applies the attention mechanism to prioritize the target information. Thus, ABi-LSTM demonstrated the capability to learn the target aspect and its features independent of any location in the sentence. Likewise, Yang et al. <ref type="bibr" target="#b143">[143]</ref> introduced a context entity aspect (CEA) model with a slight difference to the above methods of considering for multi-entity in ABSA. The CEA model performs ABSA by combining the entity and aspect information in a sentence. Thus, to achieve its objective, CEA consists of a triad of layers: interaction layer that deals with mixing the entity and its aspects information, a position layer that exploits the positional information of both aspect and entity, and lastly an LSTM with attention layer that converts the input sentence into context memory. At the final stage, the output of the LSTM with attention is passed to the classifier, which predicts the label of (entity-aspect) combination. In the similar context of using aspect contextual information, He et al. <ref type="bibr" target="#b144">[144]</ref> designed an attention-based LSTM model named LSTM+SynATT+TarRep (we call it LSTM+Sy+Tar) that grasps the opportunity of using semantic information of the target term by combining the representations from auto-encoder and attention model. The autoencoder is used to model the target and aspect embeddings information. Meanwhile, an attention mechanism implemented using the dependency parser is applied to model the context of the target and a small subset of its closely syntactical words in a sentence.</p><p>RNN with attention memory model. Chen et al. <ref type="bibr" target="#b145">[145]</ref> introduced a recurrent attention memory (RAM) model for target sentiment analysis. The RAM model integrates Bi-LSTM, GRU and attention mechanism. The Bi-LSTM generates the memory from word2vec or GloVe pre-trained embeddings. Afterwards, the multiple attentions model pays more attention to the more important parts of the sentence where the target appears. Lastly, results from multiple attentions are combined with GRU, and then softmax that predicts the polarity label of the target aspect is applied to the results. RAM model is evidenced to be able to detect the long-term sentiment.</p><p>RNN with commonsense knowledge model. With the advancement of DL, a new signal of integrating commonsense knowledge with DL models is attracting the community. Consequently, Ma et al. <ref type="bibr" target="#b146">[146]</ref> proposed a model named sentic LSTM for ABSA that supplements LSTM with conceptual level information of the target during the training phase. In addition, sentic LSTM implements the target attention that encodes target's information as well as sentence attention that computes the representation of the sentence with respect to aspect and target.</p><p>CNN based model. To explore the power of CNN based models for ABSA, Xue et al. <ref type="bibr" target="#b29">[30]</ref> suggested a gated convolutional network with aspect embedding (GCAE) model. GCAE with help of Gated Tanh-ReLU scrutinizes the important sentiment features with respect to a given aspect or entity. To realize ACSA, the proposed model uses two parallel CNNs whose outputs are combined with the gated unit. Similarly, to perform ATSA the model is extended with third CNN that extracts contextual information of aspect terms. Along the same line of using CNN, Huang et al. <ref type="bibr" target="#b147">[147]</ref> introduced two novel CNNs named CNN-parameterized filters (CNN-PF) and CNN-parametrized gate (CNN-PG). The CNN-PF uses CNN to extract information from the sentence and uses the aspect specific features as parameters to the CNN. While CNN-PG uses the aspect specific features like a gate that controls how information is passed to the pooling layer.</p><p>Hybrid model. In order to achieve better performance in ATSA, Li et al. <ref type="bibr" target="#b123">[123]</ref> introduced a hybrid model named TNet that combines Bi-LSTM and CNN, with context-preserving transformation (CPT) layer being inserted between them. The Bi-LSTM takes word embeddings as inputs, and then generates contextualized hidden representations. Afterwards, the CPT layer computes the contextual information of the target-specific features with an attention-based-Bi-LSTM and stores them in memory. Finally, CNN is applied to learn sentiment information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Performance of DL models on aspect-based sentiment analysis</head><p>Aspect-based sentiment analysis has attracted a large body of researchers. Hence, a plethora of DL models have been proposed and have achieved good results as depicted in Table <ref type="table" target="#tab_6">4</ref>. Like previous analysis in Subsections 5.1.2 and 5.2.2, there are two main observations. First, all best performing models on the three datasets, i.e., SemEval2014-D1, SemEval2014-D2 and SemEval2015-D2 contain RNN components. Thus, these results still confirm the power of RNN neural network models. Second, the significance of attention mechanism is manifested in all datasets and tasks of ABSA. This is evidenced by the fact that all models on ABSA are attention-based, except GCAE <ref type="bibr" target="#b29">[30]</ref> and PG-CNN/PF-CNN <ref type="bibr" target="#b147">[147]</ref> models, which do not contain attention mechanism. Furthermore, TNet <ref type="bibr" target="#b123">[123]</ref> has proved the power of combining Bi-LSTM that deals with long-term dependencies and CNN that learns local sentiment information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Aspect term extraction</head><p>Aspect extraction/ATE is task pioneered by Hu et al. <ref type="bibr" target="#b19">[20]</ref> in 2004. This task involves the extraction of aspects/targets in opinionated text and it acts as the prerequisite of ABSA and opinion summarization. Consider the following restaurant review text: "An average restaurant with great food, but poor service". This review sentence consists of two aspect terms, i.e., "food" and "service" of the restaurant, and hence they need to be extracted. Traditionally, this task was formulated as a sequence labeling task where the conditional random fields (CRF) <ref type="bibr" target="#b148">[148]</ref> and linguistic patterns (LP) <ref type="bibr" target="#b19">[20]</ref> were the methods mainly applied. Thus, both approaches proved to be inefficient because the good performance of CRF relies on the availability of a large number of features while linguistic patterns are manual features. Hence, DL was introduced as an efficient remedy to these problems. Therefore, in this subsection, we explore the application of DL approaches to the aspect term extraction task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">Application</head><p>We classify DL models in aspect term extraction into two categories: hybrid models and RNN with attention based models. Hybrid models. The application of DL approaches for aspect term extraction has been initiated by Poria et al. <ref type="bibr" target="#b16">[17]</ref>, where a CNN model augmented with the LP was introduced. The proposed model considers embeddings and POS tag features, and then a convolution layer extracts contextual features from the sentence. Afterwards, it learns if the sentence contains the aspect or not. Similarly, Wang et al. <ref type="bibr" target="#b149">[149]</ref> proposed a recursive neural conditional random field (RNCRF) for aspect term extraction. The RNCRF model integrates a recursive model based on the dependency tree of the sentence and a conditional random field (RF). The recursive model learns high-level features and the RF maps them in labels, which help to capture the context where the aspect appears. Likewise, Guo et al. <ref type="bibr" target="#b150">[150]</ref> extended Bi-LTSM with dependency transmission (DT) between recurrent units for aspect extraction. The proposed model takes the combination of POS tags, character and word-level embeddings as input, at the end of which the outputs of the model are sent to the classifier controlled by CRF layer that ensures valid predictions.</p><p>Besides, Xue et al. <ref type="bibr" target="#b151">[151]</ref> introduced a multi-task neural network (MTNA) model that considers both ATE and aspect category categorization (ACC) as complementary tasks. Therefore, MTNA takes input form pre-trained word embedding and applies a Bi-LSTM followed by CNN. Finally, for classification in ATE and ACC tasks, the outputs of both CNN and Bi-LSTM are combined. Similarly, to solve the problem of labeled datasets for ATE, which are very scarce, Giannakopoulos et al. <ref type="bibr" target="#b99">[99]</ref> suggested a Bi-LSTM with CRF classifier that performs supervised and unsupervised ATE. For a supervised ATE task, the proposed model uses a character-level Bi-LSTM followed by a word-level Bi-LSTM for extracting the structural feature of a sentence. Finally, to perform sequence labeling, the CRF layer utilizes the feature vector produced by the last Bi-LSTM. Besides, to accomplish unsupervised ATE, IOB (inside, outside, beginning) format is used for sequence labeling of different aspects that are present in review text. Lastly, a lexicon is used to determine the polarity of each aspect. Consequently, a new dataset is created on which Bi-LSTM &amp; CRF classifier is applied.</p><p>RNN with attention-based models. Li et al. <ref type="bibr" target="#b152">[152]</ref> suggested a memory interaction network (MIN) model, which consists of two jointly LSTM models that are used to deal with aspects and their related opinion terms in review text, respectively. MIN extends LSTMs with memory networks in order to allow the interactions between the two components. Finally, for the prediction, MIN ensures that aspects are from the sentimental sentence by introducing a third LSTM, which is used to discriminate sentimental and non-sentimental sentences. Similarly, Wang et al. <ref type="bibr" target="#b98">[98]</ref> proposed a coupled multi-layer attentions (CMLA) model, which does not require any linguistic pattern for preprocessing. CMLA consists of two GRU attention based models that interactively work together for exchanging features. The first attention GRU extracts the opinion terms while the second deals with aspect terms extraction. On the other hand, Li et al. <ref type="bibr" target="#b153">[153]</ref> performed ATE by first generating the opinion summary, which is used as the feature to guide the aspect prediction. And then, to accomplish the task, they proposed a truncated history-attention and selective transformation network (THA-STN) with two components where each one is built on top of LSTM that generates initial word representations. The first component named selective transformation network performs opinion summarization under the guidance of current target aspect. Subsequently, the attention mechanism is applied to generate a weighted sum of the whole opinion. Finally, the second component dubbed truncated history attention performs aspect detection task by combining the history representations and the weighted sum of opinion as features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">Performance of DL models on aspect term extraction</head><p>DL approaches have shown good capability in aspect term extraction as demonstrated in Table <ref type="table" target="#tab_7">5</ref>. First, the model sentic LSTM <ref type="bibr" target="#b146">[146]</ref> improved the F-score by 4.98% on (c) SemEval2015-D2 dataset. Thus, this result reveals that the integration of commonsense knowledge into the attention based model is a good point and can be applied to improve the result of a given model. Second, the role of attention mechanism is observed since the top performing models are attention based: CMLA <ref type="bibr" target="#b98">[98]</ref> on (a) SemEval2014-D1, sentic LSTM <ref type="bibr" target="#b146">[146]</ref> on (c) SemEval2015-D2, and THA-STH <ref type="bibr" target="#b154">[154]</ref> (d) SemEval2016-D2. Last but not least, the power of recursive neural networks is shown on (b) SemEval2014-D2 dataset where the model RNCRF <ref type="bibr" target="#b149">[149]</ref> leads since 2016.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Emotion detection</head><p>Emotion detection is the task of modeling a person's emotion state that may be anger, disgust, love, joy, optimism, pessimism, confusion, sadness, surprise, and trust. Thus, emotion detection is considered as a supervised multi-label classification problem. It was introduced in 2005 by Alm et al. <ref type="bibr" target="#b22">[23]</ref> using supervised machine learning. However, traditional approaches to emotion detection including lexicon based <ref type="bibr" target="#b155">[155]</ref> are no longer suitable for today's emotion states expressed in different language styles. Therefore, in this subsection we scrutinize the application of DL approaches as a solution to overcome the limitations of traditional approaches. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.1">Application</head><p>We categorize DL models that have been proposed for emotion detection task into three parts: attention, CNN, RNN based models.</p><p>Attention based models. Felbo et al. <ref type="bibr" target="#b156">[156]</ref> suggested an attention Bi-LSTM model named DeepMoji. DeepMoji extends the distance supervision by millions of emojis to classify the emotion, sentiment and detect the sarcasm in the text content. Specifically, emojis are used as noisy labels to pretrain models in absence of labeled datasets. In the same vein, Kim et al. <ref type="bibr" target="#b157">[157]</ref> suggested attentive CNN for emotion classification. Initially, to deal with limited tweets data, a preprocessing step is applied to map each emoji in the data to phrase/word that represents it. Afterwards, a self-attention model is used to generate the weighted representation of the sentence by which a CNN is applied to extracts various features for emotion classification. Similarly, Yu et al. <ref type="bibr" target="#b158">[158]</ref> suggested an attention-based transfer learning model named dual attention transfer network (DATN). DATN uses sentiment classification as a way of improving emotion classification. Specifically, the DATN model implements a shared LSTM and target-specific LSTM. The shared LSTM learns sentiment features shared between sentiment and emotion classification tasks whereas the target-specific LSTM layer deals with specific emotion features that are necessary for the emotion classification task.</p><p>CNN based model. Analyzing emotion in Chinese microblogs has not been left behind. Thus, Wang et al. <ref type="bibr" target="#b159">[159]</ref> performed emotion classification for Chinese microblogs by supplementing the skipgram language model with a CNN trained to perform multi-label classification of emotions. The proposed model presents the advantage of detecting multiple emotions that may be present in a single sentence.</p><p>RNN based models. Abdul-Mageed et al. <ref type="bibr" target="#b160">[160]</ref> extended the distance supervision with emotion hashtags to generate the dataset labels to be used for emotion classification. Afterwards, they developed a GRU based model to learn the representations that are necessary for the emotion classification task. However, different from above-discussed models that consider single genre emotion classification, a multitasking GRU based model named joint multi-task emotion (JMTE) was proposed by Tafreshi et al. <ref type="bibr" target="#b161">[161]</ref>. JMTE model helps to deal with multi-genres emotion classification for the following genres: blog posts, news headlines, movie reviews, and tweets. JMTE contains two branches: the first deals with emotion tweets while the second handles the remaining genres. Thus, the two branches share emotion cues, semantic, and syntactic emotion features among the genres. Finally, each branch performs a specific genre classification. DL models have achieved incredible results on emotion detection task as shown in Table <ref type="table" target="#tab_8">6</ref>. In view of the results, we observe that the application of DL to this task is on the onset stage. Therefore, we only show the results achieved on datasets under consideration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Multi-lingual sentiment analysis</head><p>With the advent of Web 2.0 tools, users often express their ideas in different languages. However, resourcepoor languages have been left behind in benefiting sentiment analysis. Therefore, multi-lingual sentiment analysis debut in 2009 <ref type="bibr" target="#b24">[25]</ref> as a task of sentiment analysis, which helps to classify opinionated text in different languages. Traditionally, the performance of the different models on the multi-lingual sentiment analysis task relies on the performance of machine translation, which is proved to be a headache task.</p><p>Recently, DL approaches have contributed to the boom of the task as they automatically learn features. Therefore, in this subsection, we survey DL approaches that have recently been proposed for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.1">Application</head><p>We arrange the DL models in multi-lingual sentiment analysis into four categories: UPN, CNN, RNN, and attention based models. UPN based models. Zhou et al. <ref type="bibr" target="#b73">[74]</ref> designed a denoising auto-encoder (CLSCDA) model for crosslingual sentiment analysis. The CLSCDA model explored the use of rich English resources for Chinese sentiment analysis by training English and Chinese classifiers on English labeled reviews and English-to-Chinese labeled reviews, respectively. For testing, the English classifier and Chinese classifier are tested on English-to-Chinese and Chinese reviews, respectively. Finally, to make the prediction, the results from both classifiers are combined. In the same way, Chen et al. <ref type="bibr" target="#b162">[162]</ref> proposed an adversarial deep averaging network (ADAN) model to realize cross-lingual sentiment analysis. ADAN comprises sentiment classifier and adversarial language identification scorer components that are concurrently trained. Both parts take input from a shared feature extractor. ADAN is trained with labeled SOURCE text data from English and unlabeled TARGET text data from Arabic and Chinese.</p><p>CNN based models. Researchers have investigated the use of CNN models to perform multi-lingual sentiment analysis. Consequently, Ruder et al. <ref type="bibr" target="#b163">[163]</ref> proposed a CNN model for multi-lingual aspect extraction and aspect sentiment analysis. The model takes embeddings combined with aspect vectors as input. Afterwards, the convolutional layer applies the filters, and then the max-pooling layer is used to learn the desired features for aspect sentiment analysis. Similarly, Becker et al. <ref type="bibr" target="#b164">[164]</ref> formulated a CNN that uses the character-based embedding and convolutional layers to learn features from English, Spanish, Portuguese and German. Consequently, the proposed model exhibited the capability of learning important features from all the four languages used during training at one pass, because it does not require the translation process. Besides, Attia et al. <ref type="bibr" target="#b165">[165]</ref> designed a language-independent CNN model for sentiment multi-lingual analysis. The suggested model presents the advantage of not relying on any language construct features like syntactic features, ontologies, and dictionaries. RNN based models. RNN based models have also been explored for addressing the problem of low resources languages. Can et al. <ref type="bibr" target="#b166">[166]</ref> proposed a reusable GRU model for sentiment analysis. The proposed GRU is trained using an English dataset and then the weights are saved. Afterwards, the model is evaluated using the English translated review texts from languages with limited resources. Likewise, Akhtar et al. <ref type="bibr" target="#b167">[167]</ref> designed a BiLSM based approach that performs aspect sentiment analysis in resourcepoor languages. Initially, they trained a bilingual word embedding and then solves the problem of data sparsity in resource-poor word embedding by supplementing it with word embeddings, which are created from resource-rich languages like English. Eventually, the BiLSM model is applied to learn different features necessary for classification.</p><p>Attention based model. Wang et al. <ref type="bibr" target="#b168">[168]</ref> investigated the use of the attention based CNN (ACNN) model to capture language-specific features. The ACNN is incorporated into the adversarial framework to deal with multi-lingual features. Thus, the model produces good results in bridging English tweets and Chinese Weibo messages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.2">Performance of DL models on multi-lingual sentiment analysis</head><p>As demonstrated in Table <ref type="table" target="#tab_9">7</ref>, DL models have produced good results on multi-lingual sentiment analysis. Foremost, we cannot compare the results on this task because there are no different models that have been proposed on a common dataset. A significant observation is that most of the models <ref type="bibr" target="#b73">[74,</ref><ref type="bibr" target="#b162">162,</ref><ref type="bibr" target="#b168">168]</ref> on this task follow the principle of the unsupervised network for training a multi-lingual classifier. In brief, DL models helped to reduce the burden of translation that was required to perform multi-lingual sentiment analysis. And this needs to be appreciated. However, there are few publicly available datasets for this task. Therefore, we encourage researchers to share the datasets considered in resource-poor languages sentiment analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Multi-modal sentiment analysis</head><p>The surge of social media use has brought a high demand for multi-modal sentiment analysis <ref type="bibr" target="#b25">[26]</ref> posed in 2011. This task aims at detecting the attitude of the speaker with the help of his verbal and non-verbal behaviors. By virtual of DL, the task becomes easier as different features of modalities are automatically learned with hidden layers of the network. Thus, in this subsection, we present the application of DL to multi-modal sentiment analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7.1">Application</head><p>DL models for multi-modal sentiment analysis are sorted into four categories: CNN, RNN, RNN with attention and DRL based models.</p><p>CNN based model. Cambria et al. <ref type="bibr" target="#b169">[169]</ref> proposed a model that extracts utterances' visual and textual features using CNN and applied openSMILE to learn acoustic features. Afterwards, the three kinds of feature are fused and then sent to SVM for the final classification of utterance. RNN based models. To exploit contextual features among utterances that make a video, Poria et al. <ref type="bibr" target="#b170">[170]</ref> designed a bidirectional contextual LSTM (bC-LSTM) model for multi-modal sentiment classification. bC-LSTM extracts visual features using 3D-CNN. The textual and audio features are extracted using CNN and openSMILE, respectively. Thus, the model considers the order and contextual information between utterance and its surroundings. Likewise, Zahed et al. <ref type="bibr" target="#b171">[171]</ref> proposed a tensor fusion network (TFN) model that learns the intra and inter-modalities features in an end-to-end manner. The TFN learns textual, visual and acoustic features using LSTM, FACET tool and COVAREP tool, respectively. Afterwards, instead of directly fusing the features, the model applies a tensor fusion layer that learns the dynamics of each of them explicitly. Finally, each utterance vector with all features is sent to the classifier. Similarly, Zahed et al. <ref type="bibr" target="#b127">[127]</ref> proposed a graph memory fusion network (graph-MFN) that learns different features of utterances by applying three parallel LSTMs for visual, audio and acoustic modalities. Subsequently, a dynamic fusion graph (DFG) is applied to learn the importance of each modality and fuses their features for final prediction.</p><p>RNN with attention-based models. Understanding human communication always requires modeling different modalities and their relationship. To address the issue, Zahed et al. <ref type="bibr" target="#b172">[172]</ref> proposed a model dubbed multi-attention recurrent network (MARN), which embodies two components: long-short term hybrid memory (LSTHM) and multi-attention block (MAB). LSTHM learns the view specific within modalities while MAB models the cross-view dynamics between modalities at each time step of recurrent. Similarly, Ghosaly et al. <ref type="bibr" target="#b100">[100]</ref> designed an approach named multi-modal multi-utterance-Bi-modal attention (MMMU-BA) that consists of three parallel Bi-GRUs for learning features of video, audio and text, respectively. Afterwards, the attention mechanism is applied to the outputs of each Bi-GRU to prioritize contextual utterances, which correlate to the target utterance.</p><p>DRL based model. Chen et al. <ref type="bibr" target="#b114">[114]</ref> constructed a model titled GME-LSTM(A), which integrates gated multi-modal embedding (GME) and LSTM with temporal attention (LSTM(A)). The GME performs multi-modal fusion by examining the multi-modals at each step and the attention mechanism helps to attend important modalities for the sentiment classification. The GME-LSTM(A) model is trained using reinforcement learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7.2">Performance of DL models on multi-modal sentiment analysis</head><p>DL models have shown the incredible capability of dealing with multi-modal sentiment analysis, as depicted in Table <ref type="table" target="#tab_10">8</ref>. First of all, we acknowledge Zahed et al. <ref type="bibr" target="#b127">[127]</ref> for creating a new large dataset for multi-modal sentiment analysis. In view of the results, we observe that the top scorer models <ref type="bibr" target="#b100">[100,</ref><ref type="bibr" target="#b172">172]</ref> on (a) CMU-MOSI, (b) CMU-MOSIE, and (d) YouTube datasets are RNN with attention-based models. Thus, the results are in strong agreement with previous studies, which have shown that RNNs are good models for dealing with sequential inputs. Furthermore, the attention mechanism incorporated into the models helps to prioritize important utterances. In a nutshell, the application of DL on multi-modal sentiment analysis has shown promising results. However, it is still at the baby stage on this task, so more interdisciplinary talents needs to participate in it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.8">Opinion summarization</head><p>Online users generate a huge volume of review texts that are useful to business organization and individual customers. Unfortunately, it is an overwhelming task to find the insight hidden in the reviews by reading through thousands of review texts. To remedy the problem, opinion summarization, also known as aspectbased opinion summarization, was introduced in 2004 <ref type="bibr" target="#b19">[20]</ref>. Opinion summarization focuses on generating quantitative concise summaries of a given aspect or entity and related sentiment from a reviewed text. Therefore, in this subsection, we review the application of DL in opinion summarization task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.8.1">Application</head><p>We group DL models applied in opinion summarization task into four categories: CNN, RNN with attention, DRL, and hybrid models. CNN based models. Wu et al. <ref type="bibr" target="#b173">[173]</ref> proposed a multitask CNN (MCNN) model for aspect-based opinion summarization. MCNN encompasses multi-channel CNN and single channel CNN. Multi-channel CNN handles the mapping between extracted aspects with respect to their sentiments whereas the single channel CNN learns the necessary features to perform the sentiment classification. Afterwards, the aggregator generates a report containing each aspect and its counts of positive and negative sentiment sentences. Likewise, Li et al. <ref type="bibr" target="#b174">[174]</ref> tackle opinion summarization in Chinese microblogs using a CNN and TextRank (TR) + maximal marginal relevance (MMR). CNN automatically learns the representative features from the input text. Afterwards, the outputs of CNN are sent to the TR algorithm that constructs the feature vector graph, and finally, the MMR selects the representative features to be used while constructing the abstract of the whole message.</p><p>RNN with attention-based models. Wang et al. <ref type="bibr" target="#b175">[175]</ref> suggested an attention-based Bi-LSTM model to generate the abstract of review text related to one topic. The proposed model takes multiple input text units and encodes their representations using the attention mechanism, which selects important words in the context of a particular topic. Finally, the importance estimation model produces the outputs importance scores for each text unit, which are used to form the abstract summary. Likewise, Yang et al. <ref type="bibr" target="#b176">[176]</ref> designed an approach for opinion summarization that works in the cross-domain setting. Initially, the model classifies the source and target domains by applying an LSTM. Then, a weakly supervised LDA is applied to learn the representations, which are useful for extracting the aspect of a given domain together with its sentiment information. Finally, the attention mechanism is applied to the produced representations. The attention mechanism discovers important aspects together with their corresponding sentiment information that are necessary to the abstractive summary. DRL based model. Yang et al. <ref type="bibr" target="#b177">[177]</ref> proposed an end-to-end model trained with the RL algorithm to deal with abstract review summarization. The proposed model consists of encoder and decoder parts. The encoder with multiple attentions learns the representations of context words, sentiment words, and aspects words. Later on, the decoder with attention fusion produces a summary of different aspects and their corresponding sentiment statistics.</p><p>Hybrid model. Angelidis et al. <ref type="bibr" target="#b178">[178]</ref> introduced a neural network model that works in the following manner. At first, with the help of attention encoder, a multi-seed aspect extractor predicts the aspect at the segment level. Afterwards, a CNN is applied to encode the segments' sentiments and produces the outputs, which are used by the attention based GRU in order to produce the final prediction of the document. Finally, the summary is generated by taking the opinions with the highest score for each aspect. We illustrate in Table <ref type="table" target="#tab_11">9</ref> the performance achieved by DL models for opinion summarization. However, we cannot compare the results on this task because the experiments were conducted on different datasets and they used different evaluation metrics. We observed from the results that the application of DL to this task is on the onset stage, and therefore we hope that it will attract more researchers' attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.9">Opinion spam detection</head><p>Recently, online products and services reviews have become the main source of information, which guides consumers and business organizations in making any decision. However, some kinds of reviews are harmful because they are forged. Therefore, in 2008, researchers led by Jindal et al. <ref type="bibr" target="#b23">[24]</ref> have paid attention to such types of reviews known as opinion spam/deceptive opinion spam, which is written to sound authentic, because users cannot easily judge their truthfulness. Therefore, in this subsection, we discuss several applications of DL approaches as a solution for detecting deceptive opinion spam.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.9.1">Application</head><p>DL models for opinion spam detection are grouped into CNN based and hybrid models. CNN based models. Zhao et al. <ref type="bibr" target="#b179">[179]</ref> suggested a CNN model for deceptive opinion spam detection. The proposed model extended the CNN with word order preserving pooling layer, which allows to keep the order of words in a sentence while analyzing deceptive opinion spam. Likewise, Li et al. <ref type="bibr" target="#b180">[180]</ref> analyzed deceptive spam by applying two consecutive CNNs for sentence representation and document representation, respectively. To compute the final representation of the document, each sentence is associated with its importance weight so that only contributing sentences are taken into consideration. In addition, in order to make the prediction, the document representation is supplemented with firstperson pronouns and POS tags.</p><p>Hybrid models. Researchers have tried to combine different models in order to gain the advantages offered by each one. Thus, Ren et al. <ref type="bibr" target="#b181">[181]</ref> combined CNN and Bi-GRU with attention mechanism. The model introduced the attention mechanism for detecting and prioritizing important cues of the opinion spam, which help to form the document representation. Likewise, Ren et al. <ref type="bibr" target="#b154">[154]</ref> suggested a model that aggregates CNN and Bi-GRU for document level opinion spam detection. The representations of different sentences that make a document are fed to CNN, which produces the sentence features, and then the Bi-GRU learns them to make the final representation to classify the document as spam or truth. Similarly, Zhang et al. <ref type="bibr" target="#b182">[182]</ref> designed a recurrent convolution network (RCNN) model that learns the contextual information of each word in review text. Specifically, the model represents each word using its deceptive and truthful contexts, and word embedding. After modeling contextual information of each word, the maxi-pooling is applied to select the maximum features to represent the whole review document. 5.9.2 Performance of deep learning models on opinion spam detection task DL models have performed well in opinion spam detection task as demonstrated in Table <ref type="table" target="#tab_12">10</ref>. Foremost, the observation is that the application of DL to this task is at the infancy stage as we only found five works. Therefore, more importance should be attached to the application of DL models to this task. Furthermore, the results reveal the power of hybrid models, as best performing approaches are all hybrid <ref type="bibr" target="#b154">[154,</ref><ref type="bibr" target="#b181">181,</ref><ref type="bibr" target="#b182">182]</ref>.</p><p>6 Current problems and future directions</p><p>Sentiment analysis helps to extract the hidden insight into the user-generated data. So far, researchers have suggested a large number of deep learning approaches for sentiment analysis that performed well in a large number of tasks. As deep learning research grows, we expect to see this trend to continue with the best models design. However, sentiment analysis has not yet reached the limit. Thus, in the subsequent subsections, we talk about unaddressed tasks, challenges to be addressed and suggestions to improve the performance of existing models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Unaddressed sentiment analysis tasks</head><p>Foremost, Subsection 2.2 highlights various sentiment analysis tasks. However, some of these tasks have not yet been addressed using deep learning approaches. Those tasks include opinion holder extraction and classification as well as time extraction and standardization. Therefore, we hypothesize that an obvious solution is to use different models that have been applied to other domains for NER. You can refer to the recent review of NER using deep learning <ref type="bibr" target="#b183">[183]</ref>. Moreover, an important observation is that there is an imbalance of deep learning application between various sentiment analysis tasks. Document level and sentence level sentiment analysis have dominated the rest. Hence, we recommend to keep forward as DL models have proved to gain inspiring results for all tasks including those that are dominated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6.2</head><p>Challenges to the future progress Dynamic sentiment analysis and tracking. Currently, most of the deep learning models suggested performing sentiment analysis by assuming that the text exhibits a static nature. However, it is still a challenge to deal with dynamic sentiment analysis and tracking, which involves the dynamic nature of streaming data like in social networks. Let us take an example of a Twitter conversation that involves many users talking about elections. This kind of situation raises the following challenges to sentiment analysis. First, the most challenging issue is the fast-paced change in the dataset at each time step. Second, vocabularies used by users can change over time. Third, the number of users that are involved in the conversation and their preferences can change over time. Therefore, performing sentiment analysis to this sort of situation is still an open research problem. To our knowledge, no deep learning model has been proposed to address this topic. Thus, owing to the capability of deep learning models, especially self-attention mechanism and recurrent neural network of modeling sequential input, we conjecture that modeling social networks with self-attention mechanism or recurrent neural networks by combining the sentiment links and content will help to deal with this situation. Sentiment analysis for heterogeneous information. Dealing with the heterogeneity of information and users in an online social network requires careful analysis and proper methods. Moreover, the interaction between these two components in a social network is still a challenging issue because it requires other additional information like modeling the dependency among users. For instance, modeling and predicting the sentiment information that will follow a president's tweet requires even capturing the influence of the speaker. Therefore, the same as the above issue, a possible solution is to combine the textual and network information by applying deep learning approaches like a recurrent neural network, convolutional neural network. Recently, attempts have been made in the network embedding field to predict the social influence using a convolution neural network with attention mechanism <ref type="bibr" target="#b184">[184]</ref> and sentiment link prediction using auto-encoder <ref type="bibr" target="#b185">[185]</ref>.</p><p>Language structure. Thankfully, deep learning models have solved the problem of emotion analysis based on emojis <ref type="bibr" target="#b156">[156,</ref><ref type="bibr" target="#b157">157]</ref>. However, an existing problem is to consider the nature of the language structure, which includes slangs that are used on social media. Let's take an example of the nature of language which is used to write a comment or a message that evolves over time like certain slangs and writing style used on Tweeter, i.e., "b4" means "before" and "kk" means "cool cool". Thus, we speculate that a straightforward solution is to follow the strategies adopted in <ref type="bibr" target="#b156">[156,</ref><ref type="bibr" target="#b157">157]</ref>. Different common slangs can be collected and then used to generate the dataset labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Suggestions for application of deep learning to sentiment analysis</head><p>Trending deep learning methods such as deep reinforcement learning, generative adversarial networks are at the inception stage in sentiment analysis. Therefore, further research should focus on these methods as they have shown to be promising solutions for complex issues. Precisely, deep reinforcement learning can solve the problem of resource-poor languages because it does not need labeled datasets.</p><p>Moreover, GANs can be used to bridge the gap between resource-poor languages and resource-rich languages like English. GANs for this task can be used by considering the resource-rich language data for training the generator and resource-poor languages data for training the discriminator. Furthermore, the new signals, including the tradition attention mechanism <ref type="bibr" target="#b94">[94]</ref>, self-attention mechanism <ref type="bibr" target="#b63">[64]</ref>, cognition-based attention models <ref type="bibr" target="#b95">[95,</ref><ref type="bibr" target="#b96">96]</ref>, and common sense knowledge <ref type="bibr" target="#b146">[146]</ref>, have shown the capability of increasing the performance of models, into which these new signals are incorporated. Therefore, we recommend researchers to focus on their use. Beyond using new signals, most of them have proved to increase the performance of the models when they are coupled with RNN based models. Therefore, future research should concentrate on them when they are coupled with other models than RNN based models. For example, Kim et al. <ref type="bibr" target="#b157">[157]</ref> took a leading step by combining CNN and self-attention model. Further, the self-attention model <ref type="bibr" target="#b63">[64]</ref> has alleviated the issue as it can be used as a standalone model or it can be coupled with other network models. Moreover, we recommend researchers to use sentiment-specific word embedding models <ref type="bibr" target="#b64">[65]</ref><ref type="bibr" target="#b65">[66]</ref><ref type="bibr" target="#b66">[67]</ref> by requesting the source-code to specific authors we cited. Last but not least, researchers are advised to grasp the advantages offered by BERT <ref type="bibr" target="#b62">[63]</ref>, which is the recent trending language model that produces contextualized representation learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>Sentiment analysis using deep learning approaches has attracted a large number of researchers. Hence, a plethora of deep learning models have been proposed and proved to give good results on various tasks of sentiment analysis. The success of the mentioned approaches is attributed to their capability of automatic feature learning and the success of word embedding models. Therefore, in this study, we first present the background of sentiment analysis, including its applications, levels, and tasks of sentiment analysis. Likewise, we give an introduction of traditional approaches for sentiment analysis and their drawbacks. We mainly discuss deep learning approaches and their applications on different tasks of sentiment analysis. Those tasks involve document level, sentence level, ABSA, multi-lingual, and multimodal sentiment analysis. Moreover, we address aspect term extraction, opinion summarization, emotion detection, and opinion spam detection tasks in our contribution. Beyond exploring their applications, we provide the performance analysis of the results they achieved on real-world datasets for each task. Finally, we highlight current issues that need to be addressed and provide suggestions for improvement including using new signals, new models like GANs, DRL models, usage of sentiment-specific word embedding models and BERT language model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2</head><label>2</label><figDesc>Figure 2 GAN framework with G and D during the training process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3</head><label>3</label><figDesc>Figure3Agent-environment interaction in reinforcement learning framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1</head><label>1</label><figDesc>Details of the datasets commonly used in sentiment analysis</figDesc><table><row><cell>Dataset</cell><cell>#Samples</cell><cell>#Train</cell><cell>#Dev</cell><cell>#Test</cell><cell>#Classes</cell><cell>Sentiment analysis task</cell><cell>Language</cell></row><row><cell>IMDB</cell><cell>50000</cell><cell>25000</cell><cell>-</cell><cell>25000</cell><cell>2</cell><cell>Sentence sentiment classification</cell><cell>English</cell></row><row><cell>IMDB2</cell><cell>348415</cell><cell>User choice</cell><cell>User choice</cell><cell>User choice</cell><cell>10</cell><cell>Document sentiment classification</cell><cell>English</cell></row><row><cell>SST-5</cell><cell>11855</cell><cell>8544</cell><cell>2210</cell><cell>1101</cell><cell>5</cell><cell>Sentence sentiment classification</cell><cell>English</cell></row><row><cell>SST-2</cell><cell>9613</cell><cell>6920</cell><cell>872</cell><cell>1821</cell><cell>2</cell><cell>Sentence sentiment classification</cell><cell>English</cell></row><row><cell>Amazon</cell><cell>[10K-143M]</cell><cell>User choice</cell><cell>User choice</cell><cell>User choice</cell><cell>2, 5</cell><cell>Sentence and ABSA</cell><cell>English</cell></row><row><cell>SemEval2014-D1</cell><cell>2931</cell><cell>2292</cell><cell>-</cell><cell>639</cell><cell>3</cell><cell>ABSA and ATE</cell><cell>English</cell></row><row><cell>SemEval2014-D2</cell><cell>4712</cell><cell>3591</cell><cell>-</cell><cell>1121</cell><cell>3</cell><cell>ABSA and ATE</cell><cell>English</cell></row><row><cell>SemEval2017</cell><cell>61873</cell><cell>43601</cell><cell>5988</cell><cell>12284</cell><cell>3</cell><cell>Sentence sentiment classification</cell><cell>English</cell></row><row><cell>STS</cell><cell>1.049M</cell><cell>1.048M</cell><cell>-</cell><cell>498</cell><cell>2</cell><cell>Sentence sentiment classification</cell><cell>English</cell></row><row><cell>STS-Gold</cell><cell>2034</cell><cell>User choice</cell><cell>User choice</cell><cell>User choice</cell><cell>2</cell><cell>Sentence sentiment classification</cell><cell>English</cell></row><row><cell>STS-Gold</cell><cell>58</cell><cell>User choice</cell><cell>User choice</cell><cell>User choice</cell><cell>2</cell><cell>Entity sentiment classification</cell><cell>English</cell></row><row><cell>Yelp</cell><cell>Above 1.2M</cell><cell>User choice</cell><cell>User choice</cell><cell>User choice</cell><cell>2, 5</cell><cell>Sentence, document and ABSA</cell><cell>English</cell></row><row><cell>HR</cell><cell>24348</cell><cell>User choice</cell><cell>User choice</cell><cell>User choice</cell><cell>2</cell><cell>Sentence sentiment classification</cell><cell>Chinese</cell></row><row><cell>MR</cell><cell>10662</cell><cell>8655</cell><cell>961</cell><cell>1046</cell><cell>2</cell><cell>Sentence sentiment classification</cell><cell>English</cell></row><row><cell>Sanders</cell><cell>5513</cell><cell>User choice</cell><cell>User choice</cell><cell>User choice</cell><cell>4</cell><cell>Sentence sentiment classification</cell><cell>English</cell></row><row><cell>Deutsche Bahn</cell><cell>21824</cell><cell>User choice</cell><cell>User choice</cell><cell>User choice</cell><cell>3</cell><cell>Multi-lingual sentiment</cell><cell>Deutsch</cell></row><row><cell>ASTD</cell><cell>10006</cell><cell>User choice</cell><cell>User choice</cell><cell>User choice</cell><cell>5</cell><cell>Multi-lingual sentiment</cell><cell>Arabic</cell></row><row><cell>YouTube</cell><cell>47 videos</cell><cell>User choice</cell><cell>-</cell><cell>User choice</cell><cell>3</cell><cell>Multimodal sentiment</cell><cell>English</cell></row><row><cell>CMU-MOSI</cell><cell>93 videos/</cell><cell>52 videos/</cell><cell>10 videos/</cell><cell>31 videos/</cell><cell>7</cell><cell>Multimodal sentiment</cell><cell>English</cell></row><row><cell></cell><cell>2199 utterances</cell><cell>1284 utterances</cell><cell>229 utterances</cell><cell>686 utterances</cell><cell></cell><cell>classification</cell><cell></cell></row><row><cell>CMU-MOSEI</cell><cell>3229 videos/</cell><cell>2250 videos/</cell><cell>300 videos/</cell><cell>679 videos/</cell><cell>7</cell><cell>Multimodal sentiment</cell><cell>English</cell></row><row><cell></cell><cell>22676 utterances</cell><cell>16216 utterances</cell><cell>1835 utterances</cell><cell>4625 utterances</cell><cell></cell><cell>and emotion classification</cell><cell></cell></row></table><note><p><p><p>7) </p><ref type="bibr" target="#b103">[103]</ref></p>. The SSTb dataset consists of movie reviews collected 5) http://ai.stanford.edu/ ∼ amaas/data/sentiment/. 6) https://github.com/nihalb/JMARS/tree/master/data. 7) https://nlp.stanford.edu/sentiment/.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>8) http://jmcauley.ucsd.edu/data/amazon/. 9) http://alt.qcri.org/semeval2014/task4/index.php?id=data-and-tools. 10) http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip. 11) https://github.com/pollockj/world mood/tree/master/sts gold v03. 12) https://www.yelp.com/dataset/challenge. 13) https://www.amir-zadeh.com/datasets. 14) https://github.com/A2Zadeh/CMU-MultimodalSDK. 15) https://www.cs.cornell.edu/people/pabo/movie-review-data/. 16) https://github.com/zfz/twitter corpus. 17) https://sites.google.com/view/germeval2017-absa/.</figDesc><table /><note><p>18) http://www.mohamedaly.info/datasets/astd. 19) http://projects.ict.usc.edu/youtube/.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2</head><label>2</label><figDesc>Performance of DL models on document level sentiment classification task</figDesc><table><row><cell></cell><cell></cell><cell>(a) IMDB2 dataset</cell><cell></cell><cell></cell><cell cols="2">(b) Yelp full 2013 dataset</cell><cell></cell></row><row><cell>Ref.</cell><cell>Year</cell><cell>Model</cell><cell>Accuracy (%)</cell><cell>Ref.</cell><cell>Year</cell><cell>Model</cell><cell>Accuracy (%)</cell></row><row><cell>[14]</cell><cell>2018</cell><cell>HUAPA</cell><cell>55.0</cell><cell>[14]</cell><cell>2018</cell><cell>HUAPA</cell><cell>68.3</cell></row><row><cell>[12]</cell><cell>2018</cell><cell>HCSC</cell><cell>54.2</cell><cell>[132]</cell><cell>2016</cell><cell>HAN</cell><cell>68.2</cell></row><row><cell>[97]</cell><cell>2016</cell><cell>NSC+UPA</cell><cell>53.3</cell><cell>[12]</cell><cell>2018</cell><cell>HCSC</cell><cell>65.7</cell></row><row><cell>[96]</cell><cell>2017</cell><cell>LSTM+CBA+UPA</cell><cell>52.1</cell><cell>[96]</cell><cell>2017</cell><cell>LSTM+CBA+UPA</cell><cell>65.5</cell></row><row><cell>[132]</cell><cell>2016</cell><cell>HAN</cell><cell>49.4</cell><cell>[119]</cell><cell>2015</cell><cell>LSTM-GRNN</cell><cell>65.1</cell></row><row><cell>[13]</cell><cell>2017</cell><cell>UPDMN</cell><cell>46.5</cell><cell>[97]</cell><cell>2017</cell><cell>NSC-UPA</cell><cell>65.0</cell></row><row><cell>[119]</cell><cell>2015</cell><cell>LSTM-GRNN</cell><cell>45.3</cell><cell>[13]</cell><cell>2017</cell><cell>UPDMN</cell><cell>63.9</cell></row><row><cell></cell><cell></cell><cell>(c) Yelp full 2014 dataset</cell><cell></cell><cell></cell><cell cols="3">(d) Yelp full 2015 dataset</cell></row><row><cell>Ref.</cell><cell>Year</cell><cell>Model</cell><cell>Accuracy (%)</cell><cell>Ref.</cell><cell>Year</cell><cell>Model</cell><cell>Accuracy (%)</cell></row><row><cell>[132]</cell><cell>2016</cell><cell>HAN</cell><cell>70.5</cell><cell>[132]</cell><cell>2016</cell><cell>HAN</cell><cell>71.00</cell></row><row><cell>[14]</cell><cell>2018</cell><cell>HUAPA</cell><cell>68.6</cell><cell>[88]</cell><cell>2017</cell><cell>DPCNN</cell><cell>69.42</cell></row><row><cell>[119]</cell><cell>2015</cell><cell>LSTM-GRNN</cell><cell>67.1</cell><cell>[119]</cell><cell>2015</cell><cell>LSTM-GRNN</cell><cell>67.60</cell></row><row><cell>[96]</cell><cell>2017</cell><cell>LSTM+CBA+UPA</cell><cell>66.8</cell><cell>[87]</cell><cell>2017</cell><cell>VDCNN</cell><cell>64.72</cell></row><row><cell>[97]</cell><cell>2017</cell><cell>NSC-UPA</cell><cell>66.7</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3</head><label>3</label><figDesc>Performance of DL models on sentence level sentiment classification task</figDesc><table><row><cell></cell><cell></cell><cell>(a) SST-5 dataset</cell><cell></cell><cell></cell><cell></cell><cell>(b) SST-2 dataset</cell><cell></cell></row><row><cell>Ref.</cell><cell>Year</cell><cell>Model</cell><cell>Accuracy (%)</cell><cell>Ref.</cell><cell>Year</cell><cell>Model</cell><cell>Accuracy (%)</cell></row><row><cell>[138]</cell><cell>2017</cell><cell>Tree-BiGRU(att)</cell><cell>52.4</cell><cell>[63]</cell><cell>2018</cell><cell>BERT LARGE</cell><cell>94.9</cell></row><row><cell>[137]</cell><cell>2015</cell><cell>Tree-LSTM</cell><cell>51.0</cell><cell>[138]</cell><cell>2017</cell><cell>Tree-BiGRU(att)</cell><cell>89.5</cell></row><row><cell>[112]</cell><cell>2018</cell><cell>ID-LSTM</cell><cell>50.0</cell><cell>[86]</cell><cell>2014</cell><cell>CNN</cell><cell>88.1</cell></row><row><cell>[112]</cell><cell>2018</cell><cell>HS-LSTM</cell><cell>49.8</cell><cell>[137]</cell><cell>2015</cell><cell>Tree-LSTM</cell><cell>88.0</cell></row><row><cell>[86]</cell><cell>2014</cell><cell>CNN</cell><cell>47.4</cell><cell>[136]</cell><cell>2017</cell><cell>SCNN</cell><cell>87.9</cell></row><row><cell>[103]</cell><cell>2013</cell><cell>RNTN</cell><cell>45.7</cell><cell>[103]</cell><cell>2013</cell><cell>RNTN</cell><cell>87.6</cell></row><row><cell></cell><cell></cell><cell>(c) IMDB dataset</cell><cell></cell><cell></cell><cell></cell><cell>(d) MR dataset</cell><cell></cell></row><row><cell>Ref.</cell><cell>Year</cell><cell>Model</cell><cell>Accuracy (%)</cell><cell>Ref.</cell><cell>Year</cell><cell>Model</cell><cell>Accuracy (%)</cell></row><row><cell>[139]</cell><cell>2017</cell><cell>cBLSTM+LM</cell><cell>92.83</cell><cell>[95]</cell><cell>2018</cell><cell>Sent+Gaze</cell><cell>89.0</cell></row><row><cell>[96]</cell><cell>2017</cell><cell>LSTM+CBA+LA</cell><cell>90.10</cell><cell>[112]</cell><cell>2018</cell><cell>HS-LSTM</cell><cell>82.1</cell></row><row><cell>[47]</cell><cell>2015</cell><cell>DeepRNN+DT</cell><cell>89.90</cell><cell>[112]</cell><cell>2018</cell><cell>ID-LSTM</cell><cell>81.6</cell></row><row><cell>[135]</cell><cell>2016</cell><cell>SBDAE</cell><cell>89.59</cell><cell>[86]</cell><cell>2014</cell><cell>CNN</cell><cell>81.5</cell></row><row><cell>[95]</cell><cell>2018</cell><cell>Sent+Gaze</cell><cell>89.42</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4</head><label>4</label><figDesc>Performance of DL models on ABSA task</figDesc><table><row><cell></cell><cell cols="3">(a) ATSA: SemEval2014-D1 dataset</cell><cell></cell><cell cols="3">(b) ATSA: SemEval2014-D2 dataset</cell></row><row><cell>Ref.</cell><cell>Year</cell><cell>Model</cell><cell>Accuracy (%)</cell><cell>Ref.</cell><cell>Year</cell><cell>Model</cell><cell>Accuracy (%)</cell></row><row><cell>[123]</cell><cell>2018</cell><cell>TNet</cell><cell>76.54</cell><cell>[143]</cell><cell>2018</cell><cell>CEA</cell><cell>80.98</cell></row><row><cell>[145]</cell><cell>2017</cell><cell>RAM</cell><cell>74.49</cell><cell>[144]</cell><cell>2018</cell><cell>LSTM+Sy+Tar</cell><cell>80.63</cell></row><row><cell>[143]</cell><cell>2018</cell><cell>CEA</cell><cell>72.88</cell><cell>[145]</cell><cell>2017</cell><cell>RAM</cell><cell>80.23</cell></row><row><cell>[144]</cell><cell>2018</cell><cell>LSTM+Sy+Tar</cell><cell>71.94</cell><cell>[147]</cell><cell>2018</cell><cell>PF-CNN</cell><cell>79.20</cell></row><row><cell>[147]</cell><cell>2018</cell><cell>PF-CNN</cell><cell>70.60</cell><cell>[147]</cell><cell>2018</cell><cell>PG-CNN</cell><cell>78.93</cell></row><row><cell>[30]</cell><cell>2018</cell><cell>GCAE</cell><cell>69.14</cell><cell>[30]</cell><cell>2018</cell><cell>GCAE</cell><cell>77.28</cell></row><row><cell>[147]</cell><cell>2018</cell><cell>PG-CNN</cell><cell>69.12</cell><cell>[140]</cell><cell>2016</cell><cell>ATAE-LSTM</cell><cell>77.20</cell></row><row><cell>[141]</cell><cell>2018</cell><cell>AF-LSTM (CONV)</cell><cell>68.81</cell><cell>[141]</cell><cell>2018</cell><cell>AF-LSTM (CONV)</cell><cell>75.44</cell></row><row><cell></cell><cell cols="3">(c) ACSA: SemEval2014-D2 dataset</cell><cell></cell><cell cols="3">(d) ATSA: SemEval2015-D2 dataset</cell></row><row><cell>Ref.</cell><cell>Year</cell><cell>Model</cell><cell>Accuracy (%)</cell><cell>Ref.</cell><cell>Year</cell><cell>Model</cell><cell>Accuracy (%)</cell></row><row><cell>[143]</cell><cell>2018</cell><cell>CEA</cell><cell>84.44</cell><cell>[144]</cell><cell>2018</cell><cell>LSTM+Sy+Tar</cell><cell>81.67</cell></row><row><cell>[140]</cell><cell>2016</cell><cell>ATAE-LSTM</cell><cell>84.00</cell><cell>[123]</cell><cell>2018</cell><cell>TNet</cell><cell>80.69</cell></row><row><cell>[141]</cell><cell>2018</cell><cell>AF-LSTM (CONV)</cell><cell>81.29</cell><cell>[146]</cell><cell>2018</cell><cell>sentic LSTM</cell><cell>76.47</cell></row><row><cell>[30]</cell><cell>2018</cell><cell>GCAE</cell><cell>79.35</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5</head><label>5</label><figDesc>Performance of DL models on aspect term extraction task</figDesc><table><row><cell></cell><cell cols="3">(a) SemEval2014-D1 dataset</cell><cell></cell><cell></cell><cell cols="3">(b) SemEval2014-D2 dataset</cell></row><row><cell>Ref.</cell><cell>Year</cell><cell>Model</cell><cell>F-measure (%)</cell><cell>Ref.</cell><cell>Year</cell><cell></cell><cell>Model</cell><cell>F-measure (%)</cell></row><row><cell>[98]</cell><cell>2017</cell><cell>CMLA</cell><cell>85.29</cell><cell>[149]</cell><cell>2016</cell><cell></cell><cell>RNCRF</cell><cell>89.33</cell></row><row><cell>[17]</cell><cell>2016</cell><cell>CNN-LP</cell><cell>82.32</cell><cell>[99]</cell><cell>2017</cell><cell cols="2">Bi-LSTM-CRF-unsup</cell><cell>88.00</cell></row><row><cell>[150]</cell><cell>2018</cell><cell>Bi-LSTM-DT</cell><cell>80.22</cell><cell>[17]</cell><cell>2016</cell><cell></cell><cell>CNN-LP</cell><cell>87.17</cell></row><row><cell>[153]</cell><cell>2018</cell><cell>THA-STN</cell><cell>79.52</cell><cell>[150]</cell><cell>2018</cell><cell></cell><cell>Bi-LSTM-DT</cell><cell>85.97</cell></row><row><cell>[149]</cell><cell>2016</cell><cell>RNCRF</cell><cell>78.42</cell><cell>[153]</cell><cell>2018</cell><cell></cell><cell>THA-STN</cell><cell>85.61</cell></row><row><cell>[99]</cell><cell>2017</cell><cell>Bi-LSTM-CRF-sup</cell><cell>77.96</cell><cell>[99]</cell><cell>2017</cell><cell cols="2">Bi-LSTM-CRF-sup</cell><cell>84.12</cell></row><row><cell>[152]</cell><cell>2017</cell><cell>MIN</cell><cell>77.58</cell><cell>[151]</cell><cell>2017</cell><cell></cell><cell>MTNA</cell><cell>83.65</cell></row><row><cell>[99]</cell><cell>2017</cell><cell>Bi-LSTM-CRF-unsup</cell><cell>73.04</cell><cell>[98]</cell><cell>2017</cell><cell></cell><cell>CMLA</cell><cell>77.80</cell></row><row><cell></cell><cell cols="3">(c) SemEval2015-D2 dataset</cell><cell></cell><cell></cell><cell cols="3">(d) SemEval2016-D2 dataset</cell></row><row><cell>Ref.</cell><cell>Year</cell><cell>Model</cell><cell>F-measure (%)</cell><cell>Ref.</cell><cell></cell><cell>Year</cell><cell>Model</cell><cell>F-measure (%)</cell></row><row><cell>[146]</cell><cell>2018</cell><cell>sentic LSTM</cell><cell>76.44</cell><cell>[153]</cell><cell></cell><cell>2018</cell><cell>THA-STN</cell><cell>73.61</cell></row><row><cell>[153]</cell><cell>2018</cell><cell>THA-STN</cell><cell>71.46</cell><cell>[151]</cell><cell></cell><cell>2017</cell><cell>MTNA</cell><cell>72.95</cell></row><row><cell>[98]</cell><cell>2017</cell><cell>CMLA</cell><cell>70.73</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>[151]</cell><cell>2017</cell><cell>MTNA</cell><cell>67.73</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6</head><label>6</label><figDesc>Performance of DL models on emotion detection task</figDesc><table><row><cell></cell><cell cols="2">(a) SemEval-18 dataset</cell><cell></cell><cell></cell><cell cols="2">(c) TweetEN dataset</cell><cell></cell></row><row><cell>Ref.</cell><cell>Year</cell><cell>Model</cell><cell>Accuracy (%)</cell><cell>Ref.</cell><cell>Year</cell><cell>Model</cell><cell>Accuracy (%)</cell></row><row><cell>[158]</cell><cell>2018</cell><cell>DATN2</cell><cell>59.7</cell><cell>[161]</cell><cell>2018</cell><cell>JMTE</cell><cell>80.0</cell></row><row><cell>[158]</cell><cell>2018</cell><cell>DATN1</cell><cell>58.2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>[157]</cell><cell>2018</cell><cell>ACNN</cell><cell>57.4</cell><cell></cell><cell cols="3">(d) BLG+HLN dataset</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Ref.</cell><cell>Year</cell><cell>Model</cell><cell>Accuracy (%)</cell></row><row><cell></cell><cell cols="2">(b) Ren-CECps dataset</cell><cell></cell><cell>[161]</cell><cell>2018</cell><cell>JMTE</cell><cell>84.0</cell></row><row><cell>Ref.</cell><cell>Year</cell><cell>Model</cell><cell>Accuracy (%)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>[158]</cell><cell>2018</cell><cell>DATN2</cell><cell>45.7</cell><cell></cell><cell></cell><cell>(e) MOV dataset</cell><cell></cell></row><row><cell>[158]</cell><cell>2018</cell><cell>DATN1</cell><cell>39.3</cell><cell>Ref.</cell><cell>Year</cell><cell>Model</cell><cell>Accuracy (%)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>[161]</cell><cell>2018</cell><cell>JMTE</cell><cell>92.0</cell></row><row><cell cols="4">5.5.2 Performance of DL models on emotion detection</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7</head><label>7</label><figDesc>Performance of different deep learning approaches at multi-lingual sentiment classification task</figDesc><table><row><cell>Ref.</cell><cell>Year</cell><cell>Model</cell><cell>Dataset</cell><cell>Accuracy (%)</cell></row><row><cell>[74]</cell><cell>2014</cell><cell>CLSCDA</cell><cell>NLP-CC</cell><cell>80.02</cell></row><row><cell>[162]</cell><cell>2016</cell><cell>ADAN</cell><cell>Arabic</cell><cell>55.33</cell></row><row><cell>[162]</cell><cell>2016</cell><cell>ADAN</cell><cell>Chinese</cell><cell>42.49</cell></row><row><cell>[165]</cell><cell>2018</cell><cell>CNN</cell><cell>Sanders</cell><cell>79.57</cell></row><row><cell>[165]</cell><cell>2018</cell><cell>CNN</cell><cell>Deutsche Bahn</cell><cell>75.45</cell></row><row><cell>[165]</cell><cell>2018</cell><cell>CNN</cell><cell>ASTD</cell><cell>65.58</cell></row><row><cell>[168]</cell><cell>2018</cell><cell>ACN</cell><cell>Twitter</cell><cell>81.90</cell></row><row><cell>[168]</cell><cell>2018</cell><cell>ACN</cell><cell>Weibo</cell><cell>82.43</cell></row><row><cell>[167]</cell><cell>2018</cell><cell>BiLSTM</cell><cell>Hindi</cell><cell>72.42</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8</head><label>8</label><figDesc>Performance of DL models on multi-modal sentiment analysis task</figDesc><table><row><cell></cell><cell></cell><cell>(a) CMU-MOSI dataset</cell><cell></cell><cell></cell><cell cols="3">(b) CMU-MOSIE dataset</cell></row><row><cell>Ref.</cell><cell>Year</cell><cell>Model</cell><cell>Accuracy (%)</cell><cell>Ref.</cell><cell>Year</cell><cell>Model</cell><cell>Accuracy (%)</cell></row><row><cell>[100]</cell><cell>2018</cell><cell>MMMU-BA</cell><cell>82.3</cell><cell>[100]</cell><cell>2018</cell><cell>MMMU-BA</cell><cell>79.80</cell></row><row><cell>[170]</cell><cell>2017</cell><cell>bC-LSTM</cell><cell>80.3</cell><cell>[127]</cell><cell>2018</cell><cell>Grah-MFN</cell><cell>76.90</cell></row><row><cell>[169]</cell><cell>2017</cell><cell>CNN</cell><cell>78.8</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>[171]</cell><cell>2017</cell><cell>TFN</cell><cell>77.1</cell><cell></cell><cell cols="2">(c) IEMOCAP dataset</cell><cell></cell></row><row><cell>[172]</cell><cell>2018</cell><cell>MARN</cell><cell>77.1</cell><cell>Ref.</cell><cell>Year</cell><cell>Model</cell><cell>Accuracy (%)</cell></row><row><cell>[114]</cell><cell>2017</cell><cell>GME-LSTM(A)</cell><cell>76.5</cell><cell>[170]</cell><cell>2017</cell><cell>bC-LSTM</cell><cell>76.10</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>[169]</cell><cell>2017</cell><cell>CNN</cell><cell>71.59</cell></row><row><cell></cell><cell></cell><cell>(d) YouTube dataset</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ref.</cell><cell>Year</cell><cell>Model</cell><cell>Accuracy (%)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>[172]</cell><cell>2018</cell><cell>MARN</cell><cell>54.2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>[171]</cell><cell>2017</cell><cell>TFN</cell><cell>47.1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9</head><label>9</label><figDesc>Performance of different deep learning approaches at opinion summarization task</figDesc><table><row><cell>Ref.</cell><cell>Year</cell><cell>Model</cell><cell>Dataset</cell><cell>Accuracy (%)</cell><cell>ROUGE1 (%)</cell><cell>BLEU (%)</cell></row><row><cell>[173]</cell><cell>2016</cell><cell>MCNN+w2v</cell><cell>ASM</cell><cell>84.10</cell><cell>-</cell><cell>-</cell></row><row><cell>[174]</cell><cell>2016</cell><cell>CNN</cell><cell>COAE2014</cell><cell>86.01</cell><cell>-</cell><cell>-</cell></row><row><cell>[177]</cell><cell>2018</cell><cell>MARS</cell><cell>Amazon</cell><cell>-</cell><cell>84.13</cell><cell>-</cell></row><row><cell>[176]</cell><cell>2018</cell><cell>CASAS</cell><cell>Amazon-el</cell><cell>-</cell><cell>63.55</cell><cell>-</cell></row><row><cell>[178]</cell><cell>2018</cell><cell>MILNET</cell><cell>OPOSUM</cell><cell>-</cell><cell>44.10</cell><cell>-</cell></row><row><cell>[175]</cell><cell>2016</cell><cell>AT+Bi-LSTM</cell><cell>RottenTomatoes</cell><cell>-</cell><cell>-</cell><cell>24.88</cell></row><row><cell cols="5">5.8.2 Performance of DL models on opinion summarization task</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10</head><label>10</label><figDesc>Performance of DL models on opinion spam detection task Hotel-Restaurant-Doctor spam review dataset</figDesc><table><row><cell>Ref.</cell><cell>Year</cell><cell>Model</cell><cell>Accuracy (%)</cell></row><row><cell>[182]</cell><cell>2018</cell><cell>DRI-RCNN</cell><cell>86.01</cell></row><row><cell>[181]</cell><cell>2016</cell><cell>Bi-GRNN(att)</cell><cell>84.10</cell></row><row><cell>[154]</cell><cell>2017</cell><cell>Bi-GRNN</cell><cell>83.60</cell></row><row><cell>[180]</cell><cell>2017</cell><cell>SWNN</cell><cell>80.10</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements This work was supported by National Key Research and Development Program of China (Grant Nos. 2016YFB0800402, 2016QY01W0202), National Natural Science Foundation of China (Grant Nos. U1836204, 61572221, 61433006, U1401258, 61572222, 61502185), Major Projects of the National Social Science Foundation (Grant No. 16ZDA092), and Guangxi High Level 1043 Innovation Team in Higher Education Institutions Innovation Team of ASEAN Digital Cloud Big Data Security and Mining Technology.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Challenges of sentiment analysis in social networks: an overview</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Pozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fersini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Messina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sentiment Anal Soc Netw</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Sentiment Analysis and Opinion Mining</title>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Morgan &amp; Claypool</publisher>
			<pubPlace>Williston</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Opinion mining and sentiment analysis</title>
		<author>
			<persName><forename type="first">B</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.1561/1500000011</idno>
	</analytic>
	<monogr>
		<title level="j">FNT Inf Retrieval</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="135" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<title level="m">Sentiment Analysis: Mining Opinions, Sentiments, and Emotions</title>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Attention-based LSTM for target-dependent sentiment classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W T</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st AAAI Conference on Artificial Intelligence</title>
		<meeting>the 31st AAAI Conference on Artificial Intelligence<address><addrLine>California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5013" to="5014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modeling sentences</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="655" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The general inquirer: a computer system for content analysis and retrieval based on the sentence as a unit of information</title>
		<author>
			<persName><forename type="first">P J</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><surname>Bales R F</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Namenwirth</surname></persName>
		</author>
		<idno type="DOI">10.1002/bs.3830070412</idno>
	</analytic>
	<monogr>
		<title level="j">Syst Res</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="484" to="498" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Thumbs up? sentiment classification using machine learning techniques</title>
		<author>
			<persName><forename type="first">B</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vaithyanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>Conference on Empirical Methods in Natural Language Processing<address><addrLine>Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep learning for sentiment analysis: a survey</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1002/widm.1253</idno>
	</analytic>
	<monogr>
		<title level="j">WIREs Data Min Knowl Discov</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep learning for sentiment analysis</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Rojas-Barahona</surname></persName>
		</author>
		<idno type="DOI">10.1111/lnc3.12228</idno>
	</analytic>
	<monogr>
		<title level="j">Lang Linguist Compass</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="701" to="719" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Incorporating positional information into deep belief networks for sentiment classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Industrial Conference on Data Mining</title>
		<meeting>Industrial Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cold-start aware user and product attention for sentiment classification</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Amplayo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2535" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Capturing user and product information for document level sentiment analysis with deep memory network</title>
		<author>
			<persName><forename type="first">Z-Y</forename><surname>Dou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="521" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improving review representations with user attention and product attention for sentiment classification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X-Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd AAAI Conference on Artificial Intelligence</title>
		<meeting>the 32nd AAAI Conference on Artificial Intelligence<address><addrLine>New Orleans</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5989" to="5995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentiment classification on business reviews</title>
		<author>
			<persName><forename type="first">A</forename><surname>Salinca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI Workshop on Semantic Machine Learning</title>
		<meeting>IJCAI Workshop on Semantic Machine Learning<address><addrLine>Melbourne</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An analysis of sentiments on facebook during the 2016 U.S. presidential election</title>
		<author>
			<persName><forename type="first">S</forename><surname>Alashri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Kandala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bajaj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining</title>
		<meeting>IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining<address><addrLine>San Francisco</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="795" to="802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Aspect extraction for opinion mining with a deep convolutional neural network</title>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gelbukh</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.knosys.2016.06.009</idno>
	</analytic>
	<monogr>
		<title level="j">Knowl-Based Syst</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="42" to="49" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Opinion recommendation using neural memory model</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1626" to="1637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mining knowledge from text using information extraction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bunescu</surname></persName>
		</author>
		<idno type="DOI">10.1145/1089815.1089817</idno>
	</analytic>
	<monogr>
		<title level="j">SIGKDD Explor Newsl</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="3" to="10" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Mining and summarizing customer reviews</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 10th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>Seattle</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="168" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews</title>
		<author>
			<persName><forename type="first">P</forename><surname>Turney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="417" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Determining the sentiment of opinions</title>
		<author>
			<persName><forename type="first">S M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on Computational Linguistics (COLING)</title>
		<meeting>the 20th International Conference on Computational Linguistics (COLING)<address><addrLine>Geneva</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Emotions from text: machine learning for text-based emotion prediction</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">O</forename><surname>Alm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sproat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on Human Language Technology and Empirical Methods in Natural Language Processing</title>
		<meeting>Conference on Human Language Technology and Empirical Methods in Natural Language Processing<address><addrLine>Vancouver</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="579" to="586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Opinion spam and analysis</title>
		<author>
			<persName><forename type="first">N</forename><surname>Jindal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on Web Search and Web Data Mining (WSDM)</title>
		<meeting>Conference on Web Search and Web Data Mining (WSDM)<address><addrLine>Palo Alto</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="219" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A machine learning approach to sentiment analysis in multilingual web texts</title>
		<author>
			<persName><forename type="first">E</forename><surname>Boiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Moens</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10791-008-9070-z</idno>
	</analytic>
	<monogr>
		<title level="j">Inf Retrieval</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="526" to="558" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Towards multimodal sentiment analysis: harvesting opinions from the web</title>
		<author>
			<persName><forename type="first">L-P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Multimodal Interfaces</title>
		<meeting>International Conference on Multimodal Interfaces<address><addrLine>Alicante</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="169" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Tracking sentiment and topic dynamics from social media</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International AAAI Conference on Weblogs and Social Media</title>
		<meeting>the 6th International AAAI Conference on Weblogs and Social Media</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="483" to="486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Encoding syntactic representations with a neural network for sentiment collocation extraction</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhao Y Y, Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11432-016-9229-y</idno>
	</analytic>
	<monogr>
		<title level="j">Sci China Inf Sci</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page">110101</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Aspect specific sentiment analysis using hierarchical deep learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lakkaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NIPS, Workshop on Deep Learning and Representation Learning</title>
		<meeting>the NIPS, Workshop on Deep Learning and Representation Learning</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Aspect based sentiment analysis with gated convolutional networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2514" to="2523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Lexicon-based methods for sentiment analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Taboada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Brooke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tofiloski</surname></persName>
		</author>
		<idno type="DOI">10.1162/COLI_a_00049</idno>
	</analytic>
	<monogr>
		<title level="j">Comput Linguist</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="267" to="307" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sentiment analysis on facebook group using lexicon based approach</title>
		<author>
			<persName><forename type="first">S</forename><surname>Akter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aziz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Electrical Engineering and Information Communication Technology</title>
		<meeting>International Conference on Electrical Engineering and Information Communication Technology<address><addrLine>Bangladesh</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A negation handling technique for sentiment analysis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Diamantini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mircoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Potena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Collaboration Technologies and Systems (CTS)</title>
		<meeting>International Conference on Collaboration Technologies and Systems (CTS)<address><addrLine>Orlando</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="188" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Author-specific sentiment aggregation for polarity prediction of reviews</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Joshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Language Resources and Evaluation Conference</title>
		<meeting>Language Resources and Evaluation Conference<address><addrLine>Reykjavik</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3092" to="3099" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Aspect based sentiment analysis in social media with classifier ensembles</title>
		<author>
			<persName><forename type="first">I</forename><surname>Perikos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Hatzilygeroundis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE/ACIS International Conference on Computer and Information Science</title>
		<meeting>IEEE/ACIS International Conference on Computer and Information Science</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="273" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Sentiment analysis algorithms and applications: a survey</title>
		<author>
			<persName><forename type="first">W</forename><surname>Medhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Korashy</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.asej.2014.04.011</idno>
	</analytic>
	<monogr>
		<title level="j">Ain Shams Eng J</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1093" to="1113" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A comparison of lexicon-based approaches for sentiment analysis of microblog posts</title>
		<author>
			<persName><forename type="first">C</forename><surname>Musto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Semeraro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Polignano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Information Filtering and Retrieval Co-located with XIII AI*IA Symposium on Artificial Intelligence</title>
		<meeting>the 8th International Workshop on Information Filtering and Retrieval Co-located with XIII AI*IA Symposium on Artificial Intelligence<address><addrLine>Pisa</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="59" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Lexicon based semantic detection of sentiments using expected likelihood estimate smoothed odds ratio</title>
		<author>
			<persName><forename type="first">H</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Qamar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bashir</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10462-016-9496-4</idno>
	</analytic>
	<monogr>
		<title level="j">Artif Intell Rev</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="113" to="138" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A comprehensive survey for sentiment analysis tasks using machine learning techniques</title>
		<author>
			<persName><forename type="first">E</forename><surname>Aydogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Akcayol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Symposium on Innovations in Intelligent Systems and Applications (INISTA)</title>
		<meeting>International Symposium on Innovations in Intelligent Systems and Applications (INISTA)<address><addrLine>Sinaia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Generate adjective sentiment dictionary for social media sentiment analysis using constrained nonnegative matrix factorization</title>
		<author>
			<persName><forename type="first">W</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International AAAI Conference on Weblogs and Social Media</title>
		<meeting>International AAAI Conference on Weblogs and Social Media<address><addrLine>Barcelona</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="273" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Sentiment analysis based on support vector machine and big data</title>
		<author>
			<persName><forename type="first">L</forename><surname>Povoda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Telecommunications and Signal Processing</title>
		<meeting>International Conference on Telecommunications and Signal Processing<address><addrLine>Vienna</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="543" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Sentiment analysis using support vector machine</title>
		<author>
			<persName><forename type="first">N</forename><surname>Zainuddin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Selamat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer, Communications, and Control Technology (I4CT)</title>
		<meeting>International Conference on Computer, Communications, and Control Technology (I4CT)<address><addrLine>Langkawi</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Supervised learning based approach to aspect based sentiment analysis</title>
		<author>
			<persName><forename type="first">N U</forename><surname>Pannal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Nawarathna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J T K</forename><surname>Jayakody</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer and Information Technology (CIT)</title>
		<meeting>IEEE International Conference on Computer and Information Technology (CIT)<address><addrLine>Nadi</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="662" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A hybrid approach to sentiment analysis</title>
		<author>
			<persName><forename type="first">O</forename><surname>Appel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chiclana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Congress on Evolutionary Computation (CEC)</title>
		<meeting>IEEE Congress on Evolutionary Computation (CEC)<address><addrLine>Vancouver</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4950" to="4957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A hybrid approach to sentiment analysis of news comments</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mukwazvure</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Supreethi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Reliability, Infocom Technologies and Optimization (ICRITO)</title>
		<meeting>International Conference on Reliability, Infocom Technologies and Optimization (ICRITO)<address><addrLine>Noida</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Real time sentiment analysis of tweets using naive bayes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gautam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Next Generation Computing Technologies (NGCT)</title>
		<meeting>International Conference on Next Generation Computing Technologies (NGCT)<address><addrLine>Dehradun</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="257" to="261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Sentiment analysis of a document using deep learning approach and decision trees</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zharmagambetov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Electronics Computer and Computation</title>
		<meeting>International Conference on Electronics Computer and Computation<address><addrLine>Kazakhstan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Sentiment analysis using convolutional neural network</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer and Information Technology</title>
		<meeting>IEEE International Conference on Computer and Information Technology</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2359" to="2364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">HDLTex: hierarchical deep learning for text classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kowsari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Heidarysafa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Machine Learning and Applications</title>
		<meeting>IEEE International Conference on Machine Learning and Applications<address><addrLine>Cancun</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="363" to="371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deep learning: methods and applications</title>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1561/2000000039</idno>
	</analytic>
	<monogr>
		<title level="j">FNT Signal Process</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="197" to="387" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Karafiat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Annual Conference of the International Speech Communication Association</title>
		<meeting>the 11th Annual Conference of the International Speech Communication Association</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Neural Information Processing Systems</title>
		<meeting>International Conference on Neural Information Processing Systems<address><addrLine>Lake Tahoe</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">How GPUs work</title>
		<author>
			<persName><forename type="first">D</forename><surname>Luebke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Humphreys</surname></persName>
		</author>
		<idno type="DOI">10.1109/mc.2007.59</idno>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="96" to="100" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Leveraging sparse and dense feature combinations for sentiment classification</title>
		<author>
			<persName><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hidey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Rambow</surname></persName>
		</author>
		<idno>ArXiv:1708.03940</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Sentiment analysis leveraging emotions and word embeddings</title>
		<author>
			<persName><forename type="first">M</forename><surname>Giatsoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Vozalis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Diamantaras</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.eswa.2016.10.043</idno>
	</analytic>
	<monogr>
		<title level="j">Expert Syst Appl</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="214" to="224" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Harris Z S. Distributional structure. WORD</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="146" to="162" />
			<date type="published" when="1954">1954</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Mach Learn Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing</title>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning</title>
		<meeting>International Conference on Machine Learning<address><addrLine>Helsinki</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations (ICLR)</title>
		<meeting>International Conference on Learning Representations (ICLR)<address><addrLine>Scottsdale</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Glove: global vectors for word representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Bag of tricks for efficient text classification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter</meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="427" to="431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Bilingual word embeddings for phrase-based machine translation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1393" to="1398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference of the North American Chapter</title>
		<meeting>Conference of the North American Chapter<address><addrLine>Minneapolis</addrLine></address></meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st Annual Conference on Neural Information Processing Systems</title>
		<meeting>the 31st Annual Conference on Neural Information Processing Systems<address><addrLine>Long Beach</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Learning sentiment-specific word embedding for twitter sentiment classification</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F R</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1555" to="1565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Learning bilingual sentiment word embeddings for cross-language sentiment classification</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F L</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing<address><addrLine>Beijing</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="430" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Learning sentiment-specific word embedding via global sentiment representation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F C</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd AAAI Conference on Artificial Intelligence</title>
		<meeting>the 32nd AAAI Conference on Artificial Intelligence<address><addrLine>New Orleans</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4808" to="4815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Why does unsupervised pre-training help deep learning?</title>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Mach Learn Res</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="625" to="660" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">J</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Deep</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName><surname>Learning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<pubPlace>Sebastopol: O&apos;Reilly Media</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Modular learning in neural networks</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Ballard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding the 6th National Conference on Artificial Intelligence</title>
		<meeting>eeding the 6th National Conference on Artificial Intelligence<address><addrLine>Seattle</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1987">1987</date>
			<biblScope unit="page" from="279" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Learning deep architectures for AI</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="DOI">10.1561/2200000006</idno>
	</analytic>
	<monogr>
		<title level="j">FNT Mach Learn</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="127" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine Learning</title>
		<meeting>the 25th International Conference on Machine Learning<address><addrLine>Helsinki</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1096" to="1103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Auto-encoder based bagging architecture for sentiment analysis</title>
		<author>
			<persName><forename type="first">W</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Visual Lang Comput</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="840" to="849" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Cross-lingual sentiment classification based on denoising autoencoder</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd CCF Conference on Natural Language Processing and Chinese Computing</title>
		<meeting>the 3rd CCF Conference on Natural Language Processing and Chinese Computing<address><addrLine>Shenzhen</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="181" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.2006.18.7.1527</idno>
	</analytic>
	<monogr>
		<title level="j">Neur Comput</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Improving deep belief networks via delta rule for sentiment classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Tools with Artificial Intelligence</title>
		<meeting>the 28th International Conference on Tools with Artificial Intelligence<address><addrLine>San Jose</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="410" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Neural Information Processing Systems</title>
		<meeting>the 27th International Conference on Neural Information Processing Systems<address><addrLine>Montréal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Tutorial: generative adversarial networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems</title>
		<meeting>Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">A generative model for category text generation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S H</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ins.2018.03.050</idno>
	</analytic>
	<monogr>
		<title level="j">Inf Sci</title>
		<imprint>
			<biblScope unit="volume">450</biblScope>
			<biblScope unit="page" from="301" to="315" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Learning representations of natural language texts with generative adversarial networks at document, sentence, and aspect level</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vlachostergiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Caridakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mylonas</surname></persName>
		</author>
		<idno type="DOI">10.3390/a11100164</idno>
	</analytic>
	<monogr>
		<title level="j">Algorithms</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">164</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">On the limitations of first-order approximation in GAN dynamics</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning<address><addrLine>Stockholm</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3011" to="3019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Adversarial feature matching for text generation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4006" to="4015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="DOI">10.1109/5.726791</idno>
	</analytic>
	<monogr>
		<title level="j">Proc IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Mach Learn Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for expert recommendation in community question answering</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H F</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11432-016-9197-0</idno>
	</analytic>
	<monogr>
		<title level="j">Sci China Inf Sci</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page">110102</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for text classification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Barrault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter<address><addrLine>Valencia</addrLine></address></meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1107" to="1116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Deep pyramid convolutional neural networks for text categorization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (ACL)<address><addrLine>Vancouver</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="562" to="570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Finding structure in time</title>
		<author>
			<persName><forename type="first">J</forename><surname>Elman</surname></persName>
		</author>
		<idno type="DOI">10.1207/s15516709cog1402_1</idno>
	</analytic>
	<monogr>
		<title level="j">Cogn Sci</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="179" to="211" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language modeling with controllable external memory</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Habimana O, et al. Sci China Inf Sci</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">111102</biblScope>
			<biblScope unit="page">33</biblScope>
			<date type="published" when="2020-01">January 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<meeting>IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)<address><addrLine>New Orleans</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5705" to="5709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">The vanishing gradient problem during learning recurrent neural nets and problem solutions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int J Unc Fuzz Knowl Based Syst</title>
		<imprint>
			<biblScope unit="volume">06</biblScope>
			<biblScope unit="page" from="107" to="116" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1997.9.8.1735</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Merrienboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations (ICLR)</title>
		<meeting>International Conference on Learning Representations (ICLR)<address><addrLine>San Diego</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Cognition-cognizant sentiment analysis with multitask subjectivity summarization based on annotators&apos; gaze behavior</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tamilselvam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dasgupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd AAAI Conference on Artificial Intelligence</title>
		<meeting>the 32nd AAAI Conference on Artificial Intelligence<address><addrLine>New Orleans</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5884" to="5891" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">A cognition based attention model for sentiment analysis</title>
		<author>
			<persName><forename type="first">Y F</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="462" to="471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Neural sentiment classification with user and product attention</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C C</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1650" to="1659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Coupled multi-layer attentions for co-extraction of aspect and opinion terms</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dahlmeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st AAAI Conference on Artificial Intelligence</title>
		<meeting>the 31st AAAI Conference on Artificial Intelligence<address><addrLine>San Francisco</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Unsupervised aspect term extraction with B-LSTM &amp; CRF using automatically labelled datasets</title>
		<author>
			<persName><forename type="first">A</forename><surname>Giannakopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Musat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hossmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</title>
		<meeting>the 8th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis<address><addrLine>Copenhagen</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="180" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Contextual inter-modal attention for multi-modal sentiment analysis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ghosaly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M S</forename><surname>Akhtary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chauhany</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3454" to="3466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Learning task dependent distributed representations by backpropagation through structure</title>
		<author>
			<persName><forename type="first">C</forename><surname>Goller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Küchler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Neur Netw</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="347" to="352" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Semi-supervised recursive autoencoders for predicting sentiment distributions</title>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on EMNLP</title>
		<meeting>Conference on EMNLP<address><addrLine>Scotland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="151" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Learning continuous phrase representations and syntactic parsing with recursive neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D C</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems, Deep Learning and Unsupervised Feature Learning Workshop</title>
		<meeting>Neural Information Processing Systems, Deep Learning and Unsupervised Feature Learning Workshop</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Semantic compositionality through recursive matrix-vector spaces</title>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D C</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Huval</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning<address><addrLine>Jeju Island</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1201" to="1211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<monogr>
		<title level="m" type="main">Playing atari with deep reinforcement learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<idno>ArXiv:abs/1312.5602</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<monogr>
		<title level="m" type="main">Deep reinforcement learning: an overview</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<idno>ArXiv:1701.07274</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning: an overview</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Mousavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schukat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Howley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SAI Intelligent Systems Conference</title>
		<meeting>SAI Intelligent Systems Conference</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="426" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<monogr>
		<title level="m" type="main">Reinforcement Learning: an Introduction</title>
		<author>
			<persName><forename type="first">R S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<idno type="DOI">10.1038/nature14236</idno>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Williams</surname></persName>
		</author>
		<idno type="DOI">10.1007/BF00992696</idno>
	</analytic>
	<monogr>
		<title level="j">Mach Learn</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Learning structured representation for text classification via reinforcement learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI Conference on Artificial Intelligence</title>
		<meeting>AAAI Conference on Artificial Intelligence<address><addrLine>New Orleans</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Learning to compose words into sentences with reinforcement learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<meeting><address><addrLine>Toulon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Multimodal sentiment analysis with word-level fusion and reinforcement learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM International Conference on Multimodal Interaction (ICMI)</title>
		<meeting>the 19th ACM International Conference on Multimodal Interaction (ICMI)<address><addrLine>Glasgow</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="163" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">On-line active reward learning for policy optimisation in spoken dialogue systems</title>
		<author>
			<persName><forename type="first">P-H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gasić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mrksić</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2431" to="2441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Data-efficient reinforcement learning with probabilistic model predictive control</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kamthe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Deisenroth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the 21st International Conference on Artificial Intelligence and Statistics<address><addrLine>Lanzarote</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1701" to="1710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">A language-independent neural network for event detection</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11432-017-9359-x</idno>
	</analytic>
	<monogr>
		<title level="j">Sci China Inf Sci</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page">92106</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">ELiRF-UPV at SemEval-2017 task 4: sentiment analysis using deep learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gonzlez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hurtado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval)</title>
		<meeting>the 11th International Workshop on Semantic Evaluations (SemEval)<address><addrLine>Vancouver</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="723" to="727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Document modeling with gated recurrent neural network for sentiment classification</title>
		<author>
			<persName><forename type="first">B</forename><surname>Tang D Y, Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1422" to="1432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Jointly modeling aspects, ratings and sentiments for movie recommendation (JMARS)</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C-Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 20th ACM SIGKDD Conference on Knowledge Discovery and Data Mining<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="193" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Addressing complex and subjective product-related queries with customer reviews</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International World Wide Web Conference</title>
		<meeting>the 25th International World Wide Web Conference<address><addrLine>Montréal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="625" to="635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Transformation networks for target-oriented sentiment classification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L D</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="946" to="956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<monogr>
		<title level="m" type="main">Twitter sentiment classification using distant supervision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Go</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bhayani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<ptr target="http://pdfs.semanticscholar.org/" />
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>52e2/bd533323ddf97073d034bae40a46eda55f34.pdf</note>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Evaluation datasets for Twitter sentiment analysis: a survey and a new dataset, the STS-Gold</title>
		<author>
			<persName><forename type="first">H</forename><surname>Saif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ferndez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st International Workshop on Emotion and Sentiment in Social and Expressive Media: Approaches and Perspectives from AI</title>
		<meeting>the 1st International Workshop on Emotion and Sentiment in Social and Expressive Media: Approaches and Perspectives from AI<address><addrLine>Turin</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="9" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<monogr>
		<title level="m" type="main">MOSI: multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pincus</forename><forename type="middle">E</forename></persName>
		</author>
		<idno>ArXiv:1606.06259</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Multimodal language analysis in the wild: CMU-MOSEI dataset and interpretable dynamic fusion graph</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vanbriesen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2236" to="2246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Seeing stars: exploiting class relationships for sentiment categorization with respect to rating scales</title>
		<author>
			<persName><forename type="first">B</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="page" from="115" to="124" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Sanders</surname></persName>
		</author>
		<author>
			<persName><surname>Sanders</surname></persName>
		</author>
		<ptr target="/reference/ReferencesPapers.aspx?ReferenceID=1613287" />
		<title level="m">Twitter sentiment corpus. Sanders Analytics LLC</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>S(lz5mqp453 edsnp55rrgjct55)</note>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">GermEval 2017: shared task on aspect-based sentiment in social media customer Feedback</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wojatzki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ruppert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Holschneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the GermEval 2017 Shared Task</title>
		<meeting>the GermEval 2017 Shared Task<address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">ASTD: arabic sentiment tweets dataset</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nabil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Atiya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2515" to="2519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName><forename type="first">Z C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ajakan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Mach Learn Res</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">End-to-end adversarial memory network for cross-domain sentiment classification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<meeting>the 26th International Joint Conference on Artificial Intelligence (IJCAI)<address><addrLine>Melbourne</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2237" to="2243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Semisupervised autoencoder for sentiment analysis</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th AAAI Conference on Artificial Intelligence</title>
		<meeting>the 30th AAAI Conference on Artificial Intelligence<address><addrLine>Phoenix</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1394" to="1400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Sentiment lexical-augmented convolutional neural networks for sentiment analysis</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd International Conference on Data Science in Cyberspace (DSC)</title>
		<meeting>the 2nd International Conference on Data Science in Cyberspace (DSC)<address><addrLine>Shenzhen</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="630" to="635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName><forename type="first">K S</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1556" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Structural attention neural networks for improved sentiment analysis</title>
		<author>
			<persName><forename type="first">F</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Potamianos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter<address><addrLine>Valencia</addrLine></address></meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="586" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">Contextual bidirectional long short-term memory recurrent neural network language models: a generative approach to sentiment analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mousa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter<address><addrLine>Valencia</addrLine></address></meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1023" to="1032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Attention-based LSTM for aspect-level sentiment classification</title>
		<author>
			<persName><forename type="first">Y Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="606" to="615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Learning to attend via word-aspect associative fusion for aspect-based sentiment analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Tuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Hui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd AAAI Conference on Artificial Intelligence</title>
		<meeting>the 32nd AAAI Conference on Artificial Intelligence<address><addrLine>New Orleans</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5956" to="5963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">Gated neural networks for targeted sentiment analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th AAAI Conference on Artificial Intelligence</title>
		<meeting>the 30th AAAI Conference on Artificial Intelligence<address><addrLine>Phoenix</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3087" to="3093" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Multi-entity aspect-based sentiment analysis with context, entity and aspect memory</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd AAAI Conference on Artificial Intelligence</title>
		<meeting>the 32nd AAAI Conference on Artificial Intelligence<address><addrLine>New Orleans</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6029" to="6036" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">Effective attention modeling for aspect-level sentiment classification</title>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H T</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics<address><addrLine>Santa Fe</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1121" to="1131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Recurrent attention network on memory for aspect sentiment analysis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><forename type="middle">L D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Copenhagen</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="452" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">Targeted aspect-based sentiment analysis via embedding commonsense knowledge into an attentive LSTM</title>
		<author>
			<persName><forename type="first">Y K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd AAAI Conference on Artificial Intelligence</title>
		<meeting>the 32nd AAAI Conference on Artificial Intelligence<address><addrLine>New Orleans</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5876" to="5883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">Parameterized convolutional neural networks for aspect level sentiment classification</title>
		<author>
			<persName><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Carley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1091" to="1096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">Extracting opinion targets in a single-and cross-domain setting with conditional random fields</title>
		<author>
			<persName><forename type="first">N</forename><surname>Jakob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing</title>
		<meeting>Empirical Methods in Natural Language Processing<address><addrLine>Massachusetts</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1035" to="1045" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">Recursive neural conditional random fields for aspect-based sentiment analysis</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dahlmeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="616" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">Recurrent neural CRF for aspect term extraction with dependency transmission</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CCF International Conference on Natural Language Processing and Chinese Computing</title>
		<meeting>CCF International Conference on Natural Language Processing and Chinese Computing</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="378" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">MTNA: a neural multi-task model for aspect category classification and aspect term extraction on restaurant reviews</title>
		<author>
			<persName><forename type="first">W</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Joint Conference on Natural Language Processing</title>
		<meeting>the 8th International Joint Conference on Natural Language Processing<address><addrLine>Taipei</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="151" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">Deep multi-task learning for aspect term extraction with memory interaction</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2886" to="2892" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">Aspect term extraction with history attention and selective transformation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 27th International Joint Conference on Artificial Intelligence<address><addrLine>Stockholm</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4194" to="4200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">Neural networks for deceptive opinion spam detection: an empirical study</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">F</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><forename type="middle">D</forename></persName>
		</author>
		<idno type="DOI">10.1016/j.ins.2017.01.015</idno>
	</analytic>
	<monogr>
		<title level="j">Inf Sci</title>
		<imprint>
			<biblScope unit="volume">385</biblScope>
			<biblScope unit="page" from="213" to="224" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">Lexicon based feature extraction for emotion text classification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bandhakavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Wiratunga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Padmanabhan</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patrec.2016.12.009</idno>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn Lett</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="133" to="142" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<analytic>
		<title level="a" type="main">Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm</title>
		<author>
			<persName><forename type="first">B</forename><surname>Felbo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mislove</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Søaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1615" to="1625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">AttnConvnet at SemEval-2018 task 1: attention-based convolutional neural networks for multi-label emotion classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Workshop on Semantic Evaluation</title>
		<meeting>the 12th International Workshop on Semantic Evaluation<address><addrLine>New Orleans</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="141" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">Improving multi-label emotion classification via sentiment classification with dual attention transfer network</title>
		<author>
			<persName><forename type="first">J F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Marujo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1097" to="1102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<analytic>
		<title level="a" type="main">Multi-label chinese microblog emotion classification via convolutional neural network</title>
		<author>
			<persName><forename type="first">Y Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Asia-Pacific Web Conference</title>
		<meeting>Asia-Pacific Web Conference</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="567" to="580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b160">
	<analytic>
		<title level="a" type="main">EmoNet: fine-grained emotion detection with gated recurrent neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abdul-Mageed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ungar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="718" to="728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<analytic>
		<title level="a" type="main">Emotion detection and classification in a multigenre corpus with joint multi-task deep learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tafreshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Diab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics<address><addrLine>Santa Fe</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2905" to="2913" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b162">
	<analytic>
		<title level="a" type="main">Adversarial deep averaging networks for cross-lingual sentiment classification</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Athiwaratkun</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00039</idno>
	</analytic>
	<monogr>
		<title level="j">Trans Assoc Comput Linguist</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="557" to="570" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b163">
	<analytic>
		<title level="a" type="main">INSIGHT-1 at SemEval-2016 task 5: deep learning for multilingual aspect-based sentiment analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ghaffari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Breslin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval</title>
		<meeting>SemEval<address><addrLine>San Diego</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="330" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b164">
	<analytic>
		<title level="a" type="main">An efficient deep neural architecture for multilingual sentiment analysis in Twitter</title>
		<author>
			<persName><forename type="first">W</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wehrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H E L</forename><surname>Cagnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Florida Artificial Intelligence Research Society Conference</title>
		<meeting>the 30th International Florida Artificial Intelligence Research Society Conference<address><addrLine>Florida</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="246" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b165">
	<analytic>
		<title level="a" type="main">Multilingual multi-class sentiment classification using convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Attia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Samih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Elkahky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference on Language Resources and Evaluation (LREC)</title>
		<meeting>the 11th International Conference on Language Resources and Evaluation (LREC)<address><addrLine>Miyazaki</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b166">
	<analytic>
		<title level="a" type="main">Multilingual sentiment analysis: an RNN-based framework for limited data</title>
		<author>
			<persName><forename type="first">E F</forename><surname>Can</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ezen-Can</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Can</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGIR 2018 Workshop on Learning from Limited or Noisy Data</title>
		<meeting>ACM SIGIR 2018 Workshop on Learning from Limited or Noisy Data<address><addrLine>Ann Arbor</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b167">
	<analytic>
		<title level="a" type="main">Solving data sparsity for aspect based sentiment analysis using cross-linguality and multi-linguality</title>
		<author>
			<persName><forename type="first">M S</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sawant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT<address><addrLine>New Orleans</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="572" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b168">
	<analytic>
		<title level="a" type="main">Personalized microblog sentiment classification via adversarial cross-lingual multi-task learning</title>
		<author>
			<persName><forename type="first">Wang</forename><forename type="middle">W</forename></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="338" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b169">
	<analytic>
		<title level="a" type="main">Benchmarking multimodal sentiment analysis</title>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computational Linguistics</title>
		<meeting>International Conference on Computational Linguistics<address><addrLine>Budapest</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="166" to="179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b170">
	<analytic>
		<title level="a" type="main">Context-dependent sentiment analysis in user-generated videos</title>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hazarika</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="873" to="883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b171">
	<analytic>
		<title level="a" type="main">Tensor fusion network for multimodal sentiment analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1103" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b172">
	<analytic>
		<title level="a" type="main">Multi-attention recurrent network for human communication comprehension</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd AAAI Conference on Artificial Intelligence</title>
		<meeting>the 32nd AAAI Conference on Artificial Intelligence<address><addrLine>New Orleans</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5642" to="5649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b173">
	<analytic>
		<title level="a" type="main">Aspect-based opinion summarization with convolutional neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Joint Conference on Neural Networks</title>
		<meeting>International Joint Conference on Neural Networks</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3157" to="3163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b174">
	<analytic>
		<title level="a" type="main">Mining opinion summarizations using convolutional neural networks in Chinese microblogging systems</title>
		<author>
			<persName><forename type="first">Q D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><forename type="middle">Z P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename></persName>
		</author>
		<idno type="DOI">10.1016/j.knosys.2016.06.017</idno>
	</analytic>
	<monogr>
		<title level="j">Knowl-Based Syst</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page" from="289" to="300" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b175">
	<analytic>
		<title level="a" type="main">Neural network-based abstract generation for opinions and arguments</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><forename type="middle">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT<address><addrLine>San Diego</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="47" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b176">
	<analytic>
		<title level="a" type="main">Cross-domain aspect/sentiment-aware abstractive review summarization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 27th ACM International Conference on Information and Knowledge Management<address><addrLine>Torino</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1531" to="1534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b177">
	<analytic>
		<title level="a" type="main">Aspect and sentiment aware abstractive review summarization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics<address><addrLine>Santa Fe</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1110" to="1120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b178">
	<analytic>
		<title level="a" type="main">Summarizing opinions: aspect extraction meets sentiment prediction and they are both weakly supervised</title>
		<author>
			<persName><forename type="first">S</forename><surname>Angelidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3675" to="3686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b179">
	<analytic>
		<title level="a" type="main">Towards accurate deceptive opinions detection based on word order-preserving CNN</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L M</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1155/2018/2410206</idno>
	</analytic>
	<monogr>
		<title level="j">Math Probl Eng</title>
		<imprint>
			<biblScope unit="volume">2018</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b180">
	<analytic>
		<title level="a" type="main">Document representation and feature combination for deceptive spam review detection</title>
		<author>
			<persName><forename type="first">L Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Ren</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neucom.2016.10.080</idno>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">254</biblScope>
			<biblScope unit="page" from="33" to="41" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b181">
	<analytic>
		<title level="a" type="main">Deceptive opinion spam detection using neural network</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">F</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Computational Linguistics</title>
		<meeting>the 26th International Conference on Computational Linguistics<address><addrLine>Osaka</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="140" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b182">
	<analytic>
		<title level="a" type="main">DRI-RCNN: an approach to deceptive review identification using recurrent convolutional neural network</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yoshida</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ipm.2018.03.007</idno>
	</analytic>
	<monogr>
		<title level="j">Inf Process Manage</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="576" to="592" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b183">
	<analytic>
		<title level="a" type="main">A survey on recent advances in named entity recognition from deep learning models</title>
		<author>
			<persName><forename type="first">V</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bethard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics<address><addrLine>Santa Fe</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2145" to="2158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b184">
	<analytic>
		<title level="a" type="main">DeepInf: social influence prediction with deep learning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>London</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2110" to="2119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b185">
	<analytic>
		<title level="a" type="main">SHINE: signed heterogeneous information network embedding for sentiment link prediction</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th ACM International Conference on Web Search and Data Mining</title>
		<meeting>the 11th ACM International Conference on Web Search and Data Mining<address><addrLine>Marina Del Rey</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="592" to="600" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
