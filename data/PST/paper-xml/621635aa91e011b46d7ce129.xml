<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient Graph Convolution for Joint Node Representation Learning and Clustering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chakib</forename><surname>Fettal</surname></persName>
							<email>chakib.fettal@etu.u-paris.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">UMR9010 and Informatique CDC</orgName>
								<orgName type="institution">Université de Paris Centre Borelli</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lazhar</forename><surname>Labiod</surname></persName>
							<email>lazhar.labiod@u-paris.fr</email>
							<affiliation key="aff1">
								<orgName type="institution">Université de Paris Centre Borelli UMR9010</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mohamed</forename><surname>Nadif</surname></persName>
							<email>mohamed.nadif@u-paris.fr</email>
							<affiliation key="aff2">
								<orgName type="institution">Université de Paris Centre Borelli UMR9010</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Efficient Graph Convolution for Joint Node Representation Learning and Clustering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3488560.3498533</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Graph Convolutional Networks</term>
					<term>Node Embedding</term>
					<term>Node Clustering</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Attributed graphs are used to model a wide variety of real-world networks. Recent graph convolutional network-based representation learning methods have set state-of-the-art results on the clustering of attributed graphs. However, these approaches deal with clustering as a downstream task while better performances can be attained by incorporating the clustering objective into the representation learning process. In this paper, we propose, in a unified framework, an objective function taking into account both tasks simultaneously. Based on a variant of the simple graph convolutional network, our model does clustering by minimizing the difference between the convolved node representations and their reconstructed cluster representatives. We showcase the efficiency of the derived algorithm against state-of-the-art methods both in terms of clustering performance and computational cost on the de facto benchmark graph clustering datasets. We further demonstrate the usefulness of the proposed approach for graph visualization through generating embeddings that exhibit a clustering structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>• Computing methodologies → Cluster analysis; Dimensionality reduction and manifold learning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In data science, low-dimensional representation learning is commonly used for visualization purposes, but it can also play a significant role in the clustering task, where the aim is to divide a dataset into homogeneous clusters. Indeed, working with a lowdimensional space can be useful when partitioning data, and a number of approaches are reported in the literature. Recently, the authors in <ref type="bibr" target="#b19">[20]</ref> have performed experiments on the sequential combination of deep representation learning techniques such as the autoencoder (AE), variational AE <ref type="bibr" target="#b3">[4]</ref> and convolutional AE <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b20">21]</ref>, and some popular clustering methods such as k-means; interactive Python Notebooks and further technical details can be found at 1 . They observed that this improves clustering results but that there is no 'one size fits all'. Thus, low-dimensional representation learning followed by cluster analysis can be helpful in data science. The k-means applied on data embeddings, derived from classical embedding methods such as the AE for instance, is a popular approach. This procedure is carried out sequentially and is referred to as the tandem approach <ref type="bibr" target="#b42">[43]</ref>. However, AE may sometimes be unsuitable for reducing dimension before clustering; it can fail to retain information which could be valuable for the clustering task. Hence, jointly optimizing for both tasks -representation learning and clustering-is a good alternative <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b23">24]</ref>. Learning representations that are both faithful to the data while simultaneously adjusting them to a have a clustering-friendly structure can lead to a better clustering performance <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b12">13]</ref>.</p><p>Clustering in the context of attributed graphs, which are graphs whose nodes and/or edges have attributes or features, despite being an important unsupervised task, has proved more impervious to such advances. Furthermore, some of these attributed graph clustering methods suffer from high spatial and/or computational complexity. Unlike most existing approaches, this paper aims to overcome this weakness by considering a joint graph embedding and clustering, which alternates iteratively between both tasks, that is to say between embedding and clustering. Attributed graphs are used to model a wide variety of real-world networks such as recommender systems <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b46">47]</ref>, computer vision <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b45">46]</ref>, Natural language processing <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b36">37]</ref> and physical systems <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b33">34]</ref>. Due to the irregular high-dimensional non-euclidean structure of graphs as well as the various node-level features it may contain, looking for suitable euclidean-representations that incorporate the structural and features' information of these graphs is an interesting challenge in machine learning <ref type="bibr" target="#b16">[17]</ref>. Recent literature proposes to learn these representations automatically. Loosely speaking, these representation learning methods aim at embedding the nodes into a low-dimensional space where in the embedded nodes' proximity should be similar enough to that of those in the original graph representation. These methods can be based on approaches such as factorization <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7]</ref>, random walks <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b28">29]</ref>, or neighborhood autoencoders <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b49">50]</ref>. Recently, the Graph Convolutional Network (GCN) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b22">23]</ref> has garnered a lot of attention due to its ability to learn high-quality graph representations, and by extension, its effectiveness for different graph-related tasks such as node classification, link prediction, and node clustering which is the task our paper focuses on. This node clustering, however, is generally performed as a downstream task but some efficient GCN-based approaches for the simultaneous embedding and clustering have recently emerged <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b47">48]</ref>. In this paper, we propose to rely on the GCN to develop a novel Graph Convolutional Clustering model referred to as GCC that is capable of taking into account both tasks simultaneously. Our contributions in this paper can be summarized as follows:</p><p>-We provide a variant of the GCN propagation matrix and demonstrate how it makes the GCN truly act as a low-pass filter. -We propose a new formulation combining the graph convolutional representation learning and the clustering processes and show how our proposed GCC approach is related to some other methods. -We derive an efficient algorithm referred to as GCC<ref type="foot" target="#foot_0">2</ref> and study its computational complexity in detail. We release the code<ref type="foot" target="#foot_1">3</ref> for easy reproducibility. -We perform experimentations to showcase the worth of our proposal both in terms of clustering and quality of embedding. This paper is organized as follows. Section 2 presents related works. Section 3 presents the proposed approach and its derivation. Section 4 is devoted to the proposed algorithm and its computational complexity study. In section 5, we compare GCC with the state-ofthe-art in terms of clustering and evaluate its performance in terms of embedding. Finally, Section 6 presents our conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Our contributions lie in the intersection of several research topics, graph representation learning, graph clustering and graph convolutional neural networks.</p><p>Unsupervised Graph Representation learning. Unsupervised Graph Representation learning is generally done either through contrastive learning or via autoencoders.</p><p>The contrastive methods learn representations in a self-supervised way. They commonly rely on maximizing mutual information. DGI <ref type="bibr" target="#b39">[40]</ref>, for example, maximizes mutual information between node and graph representations. InfoGraph <ref type="bibr" target="#b37">[38]</ref> expands the previous concept to graph substructures of different scales rather than just the node-level e.g. edges, triangles.</p><p>Autoencoder-based models learn embeddings by trying to reconstruct some property of the graph, generally the adjacency matrix. Variational Graph Autoencoders (VGAE) <ref type="bibr" target="#b21">[22]</ref> extend the concept of variational autoencoders to the graph context, it uses a GCN based encoder and a dot product decoder. Linear variational Graph autoencoders <ref type="bibr" target="#b32">[33]</ref> simplify VGAE by defining the encoder to be a linear transformation with a one-hop propagation matrix.</p><p>Graph Clustering. Graph clustering is the process of grouping nodes into clusters depending on the structure of the graph and/or node-level features. By only considering node attributes, classical clustering algorithms can be used to cluster the graph. Algorithms that rely on graph structure exclusively include the spectral clustering algorithm <ref type="bibr" target="#b27">[28]</ref> that optimizes the ratio and normalized-cut criteria. Graclus <ref type="bibr" target="#b10">[11]</ref> is mathematically equivalent to the spectral clustering algorithm but is faster due to not having to compute the eigenvalues of the graph Laplacian.</p><p>Finally, approaches that leverage both graph structure and node attributes commonly learn representations before applying classical clustering algorithms on them <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b48">49]</ref>. However, some recent works explored integrating the clustering loss directly into the objective.</p><p>Clustering-friendly Graph Representation Learning. Literature on joint representation learning and clustering claim that doing the two tasks simultaneously can improve clustering quality. DCN <ref type="bibr" target="#b43">[44]</ref> proposed to include the k-means clustering loss to the autoencoder loss as a regularization. Deep k-means <ref type="bibr" target="#b12">[13]</ref> followed on this concept by proposing a fully differentiable formulation of this problem. In the context of attributed graph clustering, joint clustering and embedding is starting to receive some attention. GEMSEC <ref type="bibr" target="#b30">[31]</ref> maximizes the information between labels and visual input data indices in order to self-label the data. AGC <ref type="bibr" target="#b47">[48]</ref> proposes to exploit high-order neighborhoods in the clustering process through an adaptive rule for neighborhood order selection. Deep Modularity Network <ref type="bibr" target="#b4">[5]</ref> clusters the graph by maximizing spectral modularity. Graph InfoClust (GIC) <ref type="bibr" target="#b26">[27]</ref> computes clusters by maximizing the mutual information between nodes contained in the same cluster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROPOSED METHOD</head><p>In this section we describe how we formulate the simultaneous node embedding and clustering problem. Then we propose a new model for solving it (as depicted in figure <ref type="figure" target="#fig_0">1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminaries and Notations</head><p>Let G = (𝑉 , A, X) be an attributed undirected graph where 𝑉 represents the vertex set consisting of nodes {𝑣 1 , ..., 𝑣 𝑛 }, A ∈ R 𝑛×𝑛 is a symmetric adjacency matrix where 𝑎 𝑖 𝑗 denotes the edge weight between nodes 𝑣 𝑖 and 𝑣 𝑗 , and X ∈ R 𝑛×𝑑 is a node-level feature matrix. Tr denotes the trace of a matrix. In what follows 𝑘 represents the number of clusters. 𝑓 is the embedding dimension. 1 𝑚 represents a column vector of 𝑚 ones. I 𝑚 represents an identity matrix of dimension 𝑚. If G is a matrix then m 𝑖 is its 𝑖-th row vector, m ′ 𝑗 is its 𝑗-th column vector and 𝑚 𝑖 𝑗 is the 𝑗-th element of the 𝑖-th row.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Joint Graph Representation Learning and Clustering</head><p>We formulate the simultaneous node embedding and clustering problem as follows min where enc 𝜃 1 is the encoding function, dec 𝜃 2 is the decoding function, agg(A, X) is some aggregate of A and X which represents the information contained in the graph (structure and node-features), G ∈ {0, 1} 𝑛×𝑘 the binary classification matrix, F ∈ R 𝑘×𝑑 play the role of centroids in the embedding space and 𝛼 is a coefficient that regulates the trade-off between seeking reconstruction and clustering.</p><formula xml:id="formula_0">𝜃 1 ,𝜃 2 ,G,F dec 𝜃 2 enc 𝜃 1 agg(A, X) − agg(A, X) 2 reconstruction term + 𝛼 enc 𝜃 1 agg(A, X) − GF 2 clustering regularization term s.t. G ∈ {0, 1} 𝑛×𝑘 , G1 𝑘 = 1 𝑛 (1)</formula><p>The clustering regularizer is the k-means clustering loss <ref type="bibr" target="#b24">[25]</ref> on the encoded observations. It penalizes transformations that do not result in a clustering-friendly representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Linear Graph Embedding</head><p>Linear graph autoencoders (LGAE) <ref type="bibr" target="#b32">[33]</ref> have shown that a linear encoder with an inner product decoder can be powerful enough to reach competitive results w.r.t more complex GCN-based models on the link prediction and node clustering tasks. Consequently, we also define our encoder to be a simple linear transformations i.e.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>enc(agg(</head><formula xml:id="formula_1">A, X); W 1 ) = agg(A, X)W 1</formula><p>In LGAE, the decoder attempts to reconstruct the adjacency matrix A rather than an aggregation of A and X. This means that this type of decoder is not suitable for our problem. Therefore, we also define the decoder as a simple linear transformation</p><formula xml:id="formula_2">dec(Z; W 2 ) = ZW 2</formula><p>where 𝑍 = agg(A, X)W 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Normalized Simple Graph Convolution</head><p>Our choice for the aggregate function is inspired by the simple graph convolution proposed in SGC <ref type="bibr" target="#b41">[42]</ref>. We set</p><formula xml:id="formula_3">agg(A, X) = T 𝑝 X<label>(2)</label></formula><p>but rather than have T be the symmetric normalized adjacency matrix with added self-loops, we define it to be • From the spectral perspective: The proposed normalization further shrinks the spectrum of the matrix to lie in [0, 1], as can be seen in figure <ref type="figure" target="#fig_2">2</ref>, which makes the filter truly low-pass. • From the spatial perspective: Each transformed vertex becomes a weighted-average of the neighbors which is more intuitive but it also takes into account column degree information unlike direct random walk adjacency normalization.</p><formula xml:id="formula_4">T = D T −1 (I + S)<label>(3)</label></formula><p>We further motivate this choice in the experiments section. Thereby, with this aggregation function, our problem turns into min</p><formula xml:id="formula_5">G,F,W 1 ,W 2 T 𝑝 X − T 𝑝 XW 1 W 2 2 + 𝛼 T 𝑝 XW 1 − GF 2 s.t. G ∈ {0, 1} 𝑛×𝑘 , G1 𝑘 = 1 𝑛<label>(4)</label></formula><p>Both terms of (4) make it possible to express a connection between the two tasks, the first term plays the role of linear autoencoder and the second the role of clustering in the embedding space. We decide in the following to give the same weight for the two terms (𝛼 = 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Graph Convolutional Clustering</head><p>To obtain a mutual supplementation between embedding and clustering, we assume W = W 1 = W 2 ⊤ and add an orthogonality </p><formula xml:id="formula_6">G,F,W T 𝑝 X − T 𝑝 XWW ⊤ 2 + T 𝑝 XW − GF 2 s.t. G ∈ {0, 1} 𝑛×𝑘 , G1 𝑘 = 1 𝑛 , W ⊤ W = I 𝑘<label>(5)</label></formula><p>Similar to <ref type="bibr" target="#b42">[43]</ref>, solving this problem can be proven to be equivalent to min</p><formula xml:id="formula_7">G,F,W T 𝑝 X − GFW ⊤ 2 s.t. G ∈ {0, 1} 𝑛×𝑘 , G1 𝑘 = 1 𝑛 , W ⊤ W = I 𝑘<label>(6)</label></formula><p>To prove this, we first decompose the reconstruction term</p><formula xml:id="formula_8">||T 𝑝 X − T 𝑝 XWW ⊤ || 2 = ||T 𝑝 X|| 2 + ||T 𝑝 XWW ⊤ || 2 − 2||T 𝑝 XW|| 2 = ||T 𝑝 X|| 2 − ||T 𝑝 XW|| 2 due to W ⊤ W = I 𝑘 .</formula><p>Similarly, the clustering regularization term can be decomposed as follows</p><formula xml:id="formula_9">||T 𝑝 XW − GF|| 2 = ||T 𝑝 XW|| 2 + ||GF|| 2 − 2Tr((T 𝑝 XW) ⊤ GF)</formula><p>Summing the two resulting expressions we get</p><formula xml:id="formula_10">||T 𝑝 X|| 2 + ||GF|| 2 − 2Tr((T 𝑝 XW) ⊤ GF) = ||T 𝑝 X − GFW ⊤ || 2 due to ||GFW ⊤ || = ||GF||</formula><p>Thus, optimizing ( <ref type="formula" target="#formula_6">5</ref>) is equivalent to optimizing (6) □.</p><p>Before tackling the resolution of this problem in section 4, we will first look at how our proposed GCC approach is related to some other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Connections to Existing Work</head><p>Simple Graph Convolution Variants. Similarly to SGC, the computation of T 𝑝 X can be considered to be a pre-processing step with a different propagation matrix T. This representation is then used for a downstream task. In the original paper that task was classification where the representation is fed to a linear regression model corresponding to a fully connected neural network layer with sigmoid activations. Other variants of the simple graph convolution can also be used as the aggregation function e.g. for the simple spectral graph convolution (S2GC) <ref type="bibr" target="#b48">[49]</ref> we have agg(A, X) = 1 𝑝 𝑝 𝑖=1 T 𝑖 X.</p><p>Graph Autoencoder and Linear Graph Autoencoder. Our model can be seen as a case of the non-probabilistic variant of the VGAE model adapted to graph clustering. Like VGAE, the encoder we use is a form of GCN but rather than an inner-product decoder, we use a linear decoder. The original graph autoencoder was used for link-prediction, i.e., it tried to reconstruct a completed version of the adjacency matrix A. In our case we reconstruct the convolved matrix T 𝑝 X Deep Clustering Network. From X, the DCN algorithm [44] also performs unsupervised clustering using a deep autoencoder; it uses an optimization objective that is a weighted combination of a reconstruction error and a clustering error. The DCN cost function is given by min</p><formula xml:id="formula_11">𝜃 1 ,𝜃 2 ,G,{f 𝑖 } ℓ 𝑔 𝜃 2 (𝑓 𝜃 1 (x 𝑖 ) , x 𝑖 ) + 𝜆 2 𝑖 𝑓 𝜃 1 (x 𝑖 ) − Gf 𝑖 2 2 s.t. 𝑠 𝑖 𝑗 ∈ {0, 1}, 1 ⊤ f 𝑖 = 1</formula><p>where 𝑓 is the encoder and 𝑔 is the decoder, {f 𝑖 } are the centroids, G is a membership matrix and ℓ is a loss function. If we take 𝜆 = 2, the encoder and decoder to be linear functions 𝑓 (x; W) = xW and 𝑔(x; W ⊤ ) = xW ⊤ with a semi-orthogonality constraint on W, the loss function to be the mean squared error and by considering the observations to be the rows of T 𝑝 X we get the problem as formulated in <ref type="bibr" target="#b4">(5)</ref>. As for the optimization process, the update rule is the same for the cluster assignment while it differs in the centroids, encoder and decoder updates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">OPTIMIZATION AND ALGORITHM</head><p>Directly optimization problem in ( <ref type="formula" target="#formula_7">6</ref>) is tricky so we use the following alternating iterative approach. The algorithm alternately fixes two of the matrices F, G and W and solves for the third one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Optimization Procedure</head><p>For each matrix, through fixing the two other matrices we obtain a formula which can be solved directly. The solutions to these modified problems are guaranteed to decrease the overall cost function monotonically. The initialization and update rules are described in what follows.</p><p>Initialization. We initialize W with the first 𝑓 components obtained from applying a randomized Principal Component Analysis (PCA) on T 𝑝 X. Matrices F and G are then obtained via a k-means on T 𝑝 XW.</p><p>Update Rule for F. By fixing G and W and solving for F we obtain a linear least squares problem. By setting the derivative to zero, we obtain the normal equation which is the optimal solution to the given problem. The update rule is then</p><formula xml:id="formula_12">F = (G ⊤ G) −1 G ⊤ T 𝑝 XW.<label>(7)</label></formula><p>Intuitively, each row vector f 𝑖 is set to the average of the embeddings XW that are assigned to cluster 𝑖. In the k-means algorithm this corresponds to the centroid update step.</p><p>Update Rule for W. By fixing G and F and solving for W, the update rule is as follows</p><formula xml:id="formula_13">W = UV ⊤ s.t. [U, Σ, V] = SVD (T 𝑝 X) ⊤ GF<label>(8)</label></formula><p>where Σ = (𝜎 𝑖𝑖 ), U, and V are respectively the singular values, the left and right singular vectors of the matrix (T 𝑝 X) ⊤ GF.</p><p>To prove this, fixing F and G in (6) leads to the following generalized Procrustes problem min</p><formula xml:id="formula_14">W T 𝑝 X − GFW ⊤ 2 s.t. W ⊤ W = I 𝑘 .<label>(9)</label></formula><p>As</p><formula xml:id="formula_15">||T 𝑝 X − GFW ⊤ || 2 = ||T 𝑝 X|| 2 + ||GFW ⊤ || 2 − 2Tr(WF ⊤ G ⊤ T 𝑝 X). and since ||GFW ⊤ || 2 = ||GF|| 2 (9) is equivalent to max W Tr WF ⊤ G ⊤ T 𝑝 X s.t. W ⊤ W = I 𝑘 . By taking [U, Σ, V] = SVD F ⊤ G ⊤ T 𝑝 X , we have Tr WF ⊤ G ⊤ T 𝑝 X = Tr WUΣV ⊤ = 𝑓 𝑖=1 𝜎 𝑖𝑖 &lt; w ′ 𝑖 U, v ′ 𝑖 &gt; ≤ 𝑓 𝑖=1 𝜎 𝑖𝑖 ||w ′ 𝑖 U|| × ||v ′ 𝑖 || = 𝑓 𝑖=1 𝜎 𝑖𝑖 = Tr(Σ).</formula><p>This implies that an upper bound for ( <ref type="formula" target="#formula_14">9</ref>) is attained when Tr(WUΣV ⊤ ) = Tr(Σ) or equivalently when V ⊤ WU = I meaning that the maximum is attained at W = VU ⊤ □.</p><p>Update Rule for G. By fixing F and W and solving for F, we get a problem that can be optimized with the assignment step of the k-means algorithm. The update rule is, then, given as</p><formula xml:id="formula_16">𝑔 𝑖 𝑗 * ←        1 if 𝑗 * = arg min 𝑗 (T 𝑝 XW) 𝑖 − f 𝑗 2 0 otherwise. (10)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">The GCC Algorithm</head><p>The steps in the GCC algorithm are outlined in Algorithm 1. The convergence of GCC is guaranteed, but it will only reach a local optimum according to the initial conditions. A possible strategy to overcome this is to run GCC several times and to select the best result relative to the objective function. The selection of the propagation order 𝑝 is integral to the overall performance of the algorithm. A smaller 𝑝 can mean insufficient neighborhood information is being propagated while a larger 𝑝 can cause over-smoothing of the graph signal. Figure <ref type="figure">3</ref> shows projections of the Cora dataset using the t-SNE algorithm <ref type="bibr" target="#b38">[39]</ref> for different values of 𝑝 (with a perplexity of 50).</p><p>With AGC in <ref type="bibr" target="#b47">[48]</ref>, the authors proposed to first select an interval of possible values for 𝑝 and then retain the first 𝑝 that is a local minimum of an intra-cluster metric. Since our loss function contains information about the clustering performance, it can serve as a metric for the selection of 𝑝. Thus, similarly to AGC, we select the 𝑝 via our loss function as follows: We stop and select 𝑝 = 𝑝 * if the change in square-root of the loss function ||T 𝑝 X−GFW ⊤ || between As our loss function w.r.t 𝑝 is always decreasing for every dataset in the interval we chose. We stop when the change in the loss is lower than a constant that is a function of the input dimensions rather than wait for a local minimum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Complexity Analysis</head><p>In what follows, we analyze the computational complexity of each operation in the GCC algorithm as well as the overall one.</p><p>Computing agg(A, X). The computational complexity of the 𝑝-th order simple graph convolution is O (𝑝 |𝐸|𝑑) as each multiplication costs |𝐸|𝑑 and 𝑝 such multiplications are needed.</p><p>Initializing W and G. Initializing W with PCA costs O (𝑛𝑑 log(𝑘)) operations as claimed in <ref type="bibr" target="#b15">[16]</ref>. For G, computing T 𝑝 XW takes O (𝑛𝑑 𝑓 ) Updating F. In ( <ref type="formula" target="#formula_12">7</ref>), the cost of computing the embeddings matrix</p><formula xml:id="formula_17">T 𝑝 XW is O (𝑛𝑑 𝑓 ).</formula><p>Since G is an indicator matrix, it can be stored as a vector rather than a matrix and multiplications that include it can be replaced by indexing operations. Thus, computing the transformation (G ⊤ G) −1 G ⊤ takes O (𝑛 +𝑘) and applying it on the embeddings T 𝑝 XW costs O (𝑛𝑓 ). Since 𝑛 &gt; 𝑘, the total complexity of the update of F is then O (𝑛𝑑 𝑓 ).</p><p>Updating W. In <ref type="bibr" target="#b7">(8)</ref>, computing (T 𝑝 X) ⊤ GF for the SVD costs O (𝑛𝑑 𝑓 ) because of 𝐺 being indices. The SVD operation itself costs O (𝑑 𝑓 2 ). As for calculating VU ⊤ , it is also in O (𝑑 𝑓 2 ). This brings us to a total of O (𝑛𝑑 𝑓 + 𝑑 𝑓 2 ) operations.</p><p>Updating G. T 𝑝 XW having already being computed, in <ref type="bibr" target="#b9">(10)</ref> the complexity of computing G comes from searching each embedded vector's closest centroid, this takes O (𝑛𝑘 𝑓 ).</p><p>Loss computation. Is in O (𝑑𝑘 𝑓 + 𝑛𝑑) or O (𝑛𝑑 𝑓 ) depending on the order of the multiplication and the indexing operation in the product GFW ⊤ .</p><p>Overall complexity. The totality of the previous operations cost O 𝑝 |𝐸|𝑑 + (𝑡 ′ +𝑡)𝑛𝑘 𝑓 +𝑡 ′ (𝑛𝑑 𝑓 +𝑑 𝑓 2 +𝑑𝑘 𝑓 ) +𝑛𝑑 log(𝑘) where 𝑡 ′ is the number of iterations of our algorithm (generally converges within 5-15 iteration).</p><p>For simplicity's sake we assume 𝑡 ′ = 𝑡, we can also assume that 𝑘, 𝑓 ≤ min(𝑛, 𝑑), which is often the case in graph datasets (this condition can always be satisfied by adding duplicate nodes or constant features), this allows us to set 𝑓 = 𝑘. Consequently, the total complexity is given as O (𝑝 |𝐸|𝑑 + 𝑡𝑛𝑑𝑘).</p><p>In comparison, computing T 𝑝 X and applying a k-means on it takes O (𝑝 |𝐸|𝑑 +𝑡𝑛𝑑𝑘), the same as our method. In practice, however, our algorithm is significantly faster than the k-means algorithm as its most theoretically heavy computations are matrix multiplications which can be efficiently performed on GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>To evaluate our proposed model, we conduct experiments on four datasets and compare it against a number of state-of-the-art approaches for the node clustering task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>We evaluate GCC on four widely-used attributed network datasets (Cora, Citeseer, Pubmed and Wiki). The nodes in Cora and Citeseer are associated with binary word vectors, while the ones in Pubmed and Wiki with tf-idf weighted word vectors. The summary statistics of the datasets are shown in table <ref type="table" target="#tab_0">1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">A Fair Comparison with Baseline Methods</head><p>In our work we focus on clustering and related methods. Below we look at how our GCC algorithm performs in comparison with stateof-the art unsupervised methods. Approaches that use information from the actual labels, be it supervised or semi-supervised, are not considered such as <ref type="bibr" target="#b39">[40]</ref>. The baseline methods are categorized as follows:</p><p>(1) Methods that use node-level features only. Spherical kmeans <ref type="bibr" target="#b17">[18]</ref> is k-means applied on data projected on the unit sphere. It will serve as the node features clustering baseline along with DCN <ref type="bibr" target="#b43">[44]</ref>.</p><p>(2) Methods that use graph structure only. Spectral is the spectral clustering algorithm with the normalized Laplacian as the input similarity matrix. (3) Methods that use both. LVAE <ref type="bibr" target="#b32">[33]</ref> is the linear graph variational autoencoder and LAE is its non-probabilistic version. GIC <ref type="bibr" target="#b26">[27]</ref>, AGE <ref type="bibr" target="#b7">[8]</ref> proposes a Laplacian smoothing filter that acts as a low-pass filter applied in adaptive learning scheme. S2GC proposes a new method for the aggregation of K-hop neighborhoods that is a trade-off of low-and high-pass filter bands, it then applies spectral clustering on the output of that operation. In the experiments we use the implementations that are publicly available on Github repositories of the authors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experimental Settings</head><p>To evaluate the clustering results, we employ three performance metrics: clustering Accuracy (Acc), Normalized Mutual Information (NMI) and clustering macro F1-score (F1). Larger values imply better performance. We report the mean values of the three metrics for each algorithm over 20 executions except for AGE which we average over three runs because of high execution time.</p><p>For our model, we set the embedding dimension 𝑓 = 𝑘 for each dataset. The propagation parameter 𝑝 is selected via the heuristic rule described prior; we obtain 𝑝 = 5 for citeseer, 𝑝 = 12 for Cora, 𝑝 = 150 for Pubmed and 𝑝 = 4 for Wiki. We row normalize the feature vectors and use tf-idf normalization on the binary word vectors of Citeseer and Cora so that all dataset are in tf-idf.</p><p>For other methods, we employ the parameters recommended by the authors for every dataset, For S2GC we expand the possible propagation order to {1, . . . , 150}, the same as GCC for a fair comparison. All models were run on the same machine with a 12GB memory GPU an a RAM of 12GB. Note that we could not run AGE, LAE and LVAE on Pubmed due to out of memory (OOM) issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Clustering Results</head><p>Clustering performances of the different methods are reported in table <ref type="table" target="#tab_1">2</ref>. Best performances are shown in bold. It is clear that the methods that use both A and X perform significantly better than those that use either of them individually. We see how GCC outperforms other attributed graph clustering methods in terms of accuracy, F1 and NMI except for Pubmed where S2GC and GCC are comparable. The algorithm is also stable as its standard deviation on the accuracy is 0.13 on Citeseer, 0.02 on Cora, 0.00 on Pubmed, and 1.58 on Wiki. We also report the average running time of each algorithm in table <ref type="table" target="#tab_2">3</ref>. Notice how the running times of our algorithm are the lowest on the four datasets when compared to the stare-of-the-art especially when comparing with the AGE algorithm. We mentioned earlier how algorithm has a similar theoretical complexity to that of k-means. We can see here that in practice our algorithm is faster on most datasets due to the fact that it can be efficiently run on a GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Embedding and Visualization</head><p>The GCC model offers the ability to display the cluster-based structure inherent to multivariate data. Figure <ref type="figure" target="#fig_6">4</ref> presents the lowerdimensional representations produced by our model projected on a 2-d space by t-SNE (with a perplexity of 50). We can see a clear difference in the structures of the projections of the raw data and those of the generated embeddings. To further judge the quality of the embedding, we use the R-squared measure, i.e., R2= Tr(𝑆 𝑏 )</p><p>Tr(𝑆 𝑡 ) , where 𝑆 𝑏 is the between-class scatter matrix and 𝑆 𝑡 is the total scatter matrix. We report this measure in Figure <ref type="figure" target="#fig_6">4</ref> on the true labels plots to quantify separability. The R-squared is larger for 2-d projections of the GCC embeddings on all four datasets. On Pubmed, the structure is less pronounced (0.47 vs 0.53) but we can still see the formation of three clusters.</p><p>This shows how our model can be efficiently used for data visualization to generate more interpretable embeddings or for a dimensionality reduction step before feeding the output representations to more complex clustering algorithms down the line. Note also that such visualisations can help the user in assessing the number of clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Choice of Propagation Matrix</head><p>We conduct experiments to further motivate our choice of propagation matrix. We compare the following propagation matrices:</p><formula xml:id="formula_18">i) Augmented symmetric norm. A sym = D−1/2 Ã D−1/2 . ii) Aug- mented random walk norm. A rw = D−1 Ã. iii) Our norm. A ours = T = D T −1 (I + D−1/2 Ã D−1/2 ).</formula><p>We do an analysis on the clustering accuracy of our model with these normalizations for propagation orders 𝑝 ∈ {1, .., 20}. These results are averaged over 20 runs and the same parameters are used for all normalizations. We see in figure <ref type="figure" target="#fig_7">5</ref> how our proposed propagation matrix offers the maximum accuracy on three out of the four datasets (A rw slightly outperforms it on Pubmed). We also see that it is more stable and well-behaved compared to the other two on all four datasets. The symmetric normalization especially is prone to large changes even for consecutive propagation orders. These results can be explained by the fact that the GCN when using the symmetric and random walk normalizations is not strictly low-pass. Figure <ref type="figure" target="#fig_8">6</ref> shows the frequency response functions for the GCN with the three propagation matrices for propagation order 𝑝 ∈ {1, 2, 3}. We see how the absolute value of the frequency response function is not always decreasing w.r.t the frequencies for A rw and A sym as opposed to A ours .    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we harnessed the simple formulation of the graph convolutional network to obtain an efficient model that addresses both node embedding and clustering in a unified framework. First, we provided a normalization that makes the GCN encoder act as a low pass filter in the strict sense. Secondly, we proposed a novel approach where the objective function to be optimized leverages information from both the GCN embedding reconstruction loss and the cluster structure of these embeddings. Thirdly, we derived GCC whose complexity has been rigorously studied. In doing so, we showed how GCC achieves better performances compared to other graph clustering algorithms in a more efficient manner. Note that all the compared methods are unsupervised in nature in order to have a fair comparison with our model. Our experiments demonstrated the interest of our approach. We also showed how GCC is related to other methods including some GCN variants. The proposed model is flexible model and can be extended in several directions, thus opening up opportunities for future research. For instance, in our approach we have assumed that the 𝛼 coefficient which regulates the trade-off between seeking reconstruction and clustering is equal to one, it would be interesting to investigate the choice of this value. On the other hand, while our focus in this work is clustering, it would be worthwhile to extend the problem, e.g., to co-clustering, which is a useful in a wide range of real-world scenarios like document clustering.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Schema of the GCC model: GCC creates an initial representation of the graph before iteratively learning to embed and cluster the data. The graph signal is represented by the colors of the node. Feature propagation results in a smoother signal.</figDesc><graphic url="image-1.png" coords="3,53.80,83.69,504.40,170.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>where S = D−1/2 Ã D−1/2 with Ã = A + I and D (resp. D T ) being the diagonal matrix of degrees of Ã (resp. I + S). The GCN, and by extension the SGC, do graph signal filtering with matrix I − S = I − D−1/2 (I − L) D−1/2 where L is the Laplacian of Ã. The frequency response function of this filter is ℎ( λ𝑙 ) = 1 − λ𝑙 where 𝜆 𝑙 is a frequency of the graph. In the GCN stacking 𝐾-layers, or equivalently raising S to power 𝐾 in SGC, implies doing the filtering with frequency response function ℎ 𝐾 ( λ𝑙 ) = (1 − λ𝑙 ) 𝐾 . This filter is low-pass on [0, 1] but not [0, 1.5]. We then propose to further add self-loops and row normalize matrix S. This has the following effects</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: frequency response of the proposed GCN filter plotted against the frequency on four real-world datasets</figDesc><graphic url="image-2.png" coords="4,89.84,78.02,168.17,168.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 1 :Figure 3 :</head><label>13</label><figDesc>Figure 3: Visualization of the Cora GCC-embeddings using t-SNE for different values of 𝑝. 𝑝 * and 𝑝 * − 1 is less than 𝑑 𝑛 for 𝑝 ∈ {0, . . . , 150}. The detailed rule is described in Algorithm 2.As our loss function w.r.t 𝑝 is always decreasing for every dataset in the interval we chose. We stop when the change in the loss is lower than a constant that is a function of the input dimensions rather than wait for a local minimum.</figDesc><graphic url="image-3.png" coords="5,330.66,309.51,212.59,212.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Algorithm 2 :</head><label>2</label><figDesc>Propagation order selection rule Input : -Adjacency matrix A -Feature matrix X -Number of clusters 𝑘 -Embedding dimension 𝑓 Output : Propagation order 𝑝 * for 𝑝 ∈ {2, . . . , 100} do G, F, W ← GCC(A,X, 𝑝, 𝑘, 𝑓 ); loss 𝑝 ← T 𝑝 X − GFW ⊤ ; if | loss 𝑝 − loss 𝑝−1 | &lt; 𝑑 𝑛 then 𝑝 * ← 𝑝 − 1 end end while k-means applied on T 𝑝 XW is in O (𝑡𝑛𝑘 𝑓 ) where 𝑡 is the number of iterations of k-means; ergo, the overall complexity of initialization is O (𝑛𝑑 log(𝑘) + 𝑛𝑑 𝑓 + 𝑡𝑛𝑘 𝑓 ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Left column: t-SNE projection of the original features colored according to the real labels. Middle column: t-SNE projection of the GCC embeddings colored according to the real labels. right column: t-SNE projection of the GCC embeddings colored according to the predicted labels. R-squared is used to measure of class separability for real classes (left and middle column), e.g., 0.49 vs 0.85 for Citeseer.</figDesc><graphic url="image-8.png" coords="8,77.90,292.03,453.96,104.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Accuracy with GCC using different propagation matrices averaged over 20 runs</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Frequency response plotted against the frequency for different propagation matrices on Cora. Left column: frequency response is 1−𝜆. Middle column: frequency response is (1 − 𝜆) 2 . Right column: frequency response is (1 − 𝜆) 3 .</figDesc><graphic url="image-9.png" coords="8,71.82,428.97,204.21,204.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Dataset statistics.</figDesc><table><row><cell>Dataset</cell><cell cols="4">#Nodes #Edges #Features #Classes</cell></row><row><cell>CiteSeer [36]</cell><cell>3327</cell><cell>4732</cell><cell>3703</cell><cell>6</cell></row><row><cell>Cora [36]</cell><cell>2708</cell><cell>5429</cell><cell>1433</cell><cell>7</cell></row><row><cell cols="2">PubMed [36] 19717</cell><cell>44338</cell><cell>500</cell><cell>3</cell></row><row><cell>Wiki [45]</cell><cell>2405</cell><cell>17981</cell><cell>4973</cell><cell>17</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Clustering performance on four datasets averaged over 20 runs. AGE was averaged over 3 runs. AGE, LAE and LVAE failed to scale to Pubmed; OOM denotes out of memory.</figDesc><table><row><cell>Method</cell><cell>Input</cell><cell></cell><cell>Citeseer</cell><cell></cell><cell></cell><cell>Cora</cell><cell></cell><cell></cell><cell>Pubmed</cell><cell></cell><cell></cell><cell>Wiki</cell></row><row><cell></cell><cell></cell><cell>Acc</cell><cell>F1</cell><cell>NMI</cell><cell>Acc</cell><cell>F1</cell><cell>NMI</cell><cell>Acc</cell><cell>F1</cell><cell>NMI</cell><cell>Acc</cell><cell>F1</cell><cell>NMI</cell></row><row><cell>Sph. k-means</cell><cell>X</cell><cell cols="11">42.64 40.16 19.91 33.97 30.93 15.33 59.51 58.16 31.26 33.65 23.30 29.90</cell></row><row><cell>DCN</cell><cell>X</cell><cell cols="2">19.16 11.44</cell><cell>2.91</cell><cell cols="2">20.01 11.81</cell><cell>2.32</cell><cell>15.87</cell><cell>7.06</cell><cell>4.07</cell><cell cols="2">44.28 17.14 12.45</cell></row><row><cell>Spectral</cell><cell>A</cell><cell>21.60</cell><cell>9.46</cell><cell>1.54</cell><cell>30.00</cell><cell>8.78</cell><cell>2.36</cell><cell cols="5">58.96 43.53 18.30 23.20 13.74 18.05</cell></row><row><cell>LAE (2020)</cell><cell cols="7">(A, X) 43.49 41.33 22.66 65.43 66.21 48.89</cell><cell></cell><cell>OOM</cell><cell></cell><cell cols="2">45.26 40.90 45.99</cell></row><row><cell>LVAE (2020)</cell><cell cols="7">(A, X) 39.46 38.26 20.53 64.11 65.31 48.47</cell><cell></cell><cell>OOM</cell><cell></cell><cell cols="2">47.38 42.92 47.79</cell></row><row><cell>AGE (2020)</cell><cell cols="7">(A, X) 57.85 55.01 35.74 69.17 67.30 56.91</cell><cell></cell><cell>OOM</cell><cell></cell><cell cols="2">53.79 41.39 52.63</cell></row><row><cell>GIC (2021)</cell><cell cols="12">(A, X) 68.78 64.02 43.82 70.45 68.95 52.55 64.30 64.86 26.02 46.46 40.29 48.24</cell></row><row><cell>S2GC (2021)</cell><cell cols="12">(A, X) 68.13 63.79 42.26 69.68 66.41 54.83 70.81 69.96 32.32 52.71 44.40 48.96</cell></row><row><cell>GCC (ours)</cell><cell cols="12">(A, X) 69.45 64.54 45.13 74.29 70.35 59.17 70.82 69.89 32.30 54.56 46.10 54.61</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Wall-clock time in seconds for different methods on the four datasets averaged over 20 runs (3 runs for AGE).</figDesc><table><row><cell>Method</cell><cell cols="4">CiteSeer Cora Pubmed Wiki</cell></row><row><cell>Sph. k-means</cell><cell>18.1</cell><cell>3.2</cell><cell>8.3</cell><cell>20.2</cell></row><row><cell>LAE</cell><cell>12.3</cell><cell>8.9</cell><cell>OOM</cell><cell>27.3</cell></row><row><cell>LVAE</cell><cell>11.9</cell><cell>6.3</cell><cell>OOM</cell><cell>29.3</cell></row><row><cell>AGE</cell><cell>2461</cell><cell>936.3</cell><cell>OOM</cell><cell>3058.7</cell></row><row><cell>GIC</cell><cell>8.4</cell><cell>5.7</cell><cell>13.9</cell><cell>8.3</cell></row><row><cell>S2GC</cell><cell>7.7</cell><cell>1.0</cell><cell>24.8</cell><cell>6.1</cell></row><row><cell>GCC</cell><cell>2.5</cell><cell>1.0</cell><cell>11.8</cell><cell>2.9</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">From now on, in order to distinguish between a model and its derived algorithm, we will use typewriter font for an algorithm. Consequently, GCC is the model and GCC its derived algorithm.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">https://github.com/chakib401/graph_convolutional_clustering</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Distributed large-scale natural graph factorization</title>
		<author>
			<persName><forename type="first">Amr</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nino</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shravan</forename><surname>Narayanamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vanja</forename><surname>Josifovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd international conference on World Wide Web</title>
				<meeting>the 22nd international conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="37" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A semi-NMF-PCA unified framework for data clustering</title>
		<author>
			<persName><forename type="first">Kais</forename><surname>Allab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lazhar</forename><surname>Labiod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Nadif</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2" to="16" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Simultaneous spectral data embedding and clustering</title>
		<author>
			<persName><forename type="first">Kais</forename><surname>Allab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lazhar</forename><surname>Labiod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Nadif</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="6396" to="6401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Variational autoencoder based anomaly detection using reconstruction probability</title>
		<author>
			<persName><forename type="first">Jinwon</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungzoon</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Special Lecture on IE</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Graph Clustering with Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Tsitsulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Palowitch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Workshop on Mining and Learning with Graphs (MLG)</title>
				<meeting>the 16th International Workshop on Mining and Learning with Graphs (MLG)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Self-labelling via simultaneous clustering and representation learning</title>
		<author>
			<persName><forename type="first">Yuki</forename><forename type="middle">M</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Laplacian Eigenmaps for Dimensionality Reduction and Data Representation</title>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1373" to="1396" />
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adaptive graph encoder for attributed graph embedding</title>
		<author>
			<persName><forename type="first">Ganqu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="976" to="985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">K-means clustering in a lowdimensional Euclidean space</title>
		<author>
			<persName><forename type="first">Geert</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soete</forename></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carroll</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">New approaches in classification and data analysis</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="212" to="219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3844" to="3852" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Weighted Graph Cuts without Eigenvectors A Multilevel Approach</title>
		<author>
			<persName><forename type="first">S</forename><surname>Inderjit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqiang</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><surname>Kulis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1944" to="1957" />
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Graph Neural Networks for Social Recommendation</title>
		<author>
			<persName><forename type="first">Wenqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Yin</surname></persName>
		</author>
		<idno type="DOI">10.1145/3308558.3313488</idno>
		<ptr target="https://doi.org/10.1145/3308558.3313488" />
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
				<meeting><address><addrLine>San Francisco, CA, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="417" to="426" />
		</imprint>
	</monogr>
	<note>WWW &apos;19)</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep k-means: Jointly clustering with k-means and learning representations</title>
		<author>
			<persName><forename type="first">Maziar</forename><surname>Moradi Fard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Thonet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Gaussier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">138</biblScope>
			<biblScope unit="page" from="185" to="192" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep clustering via joint convolutional autoencoder embedding and relative entropy minimization</title>
		<author>
			<persName><forename type="first">Kamran</forename><surname>Ghasedi Dizaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amirhossein</forename><surname>Herandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weidong</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
				<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5736" to="5745" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions</title>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Halko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><forename type="middle">A</forename><surname>Per-Gunnar Martinsson</surname></persName>
		</author>
		<author>
			<persName><surname>Tropp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM review</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="217" to="288" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Representation Learning on Graphs: Methods and Applications</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Data Eng. Bull</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="52" to="74" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Spherical k-Means Clustering</title>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Hornik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ingo</forename><surname>Feinerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Kober</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Buchta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
	<note>Articles</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">VAIN: Attentional Multi-agent Predictive Modeling</title>
		<author>
			<persName><forename type="first">Yedid</forename><surname>Hoshen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2701" to="2711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep learning-based clustering approaches for bioinformatics</title>
		<author>
			<persName><forename type="first">Md Rezaul</forename><surname>Karim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oya</forename><surname>Beyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Achille</forename><surname>Zappa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><forename type="middle">G</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dietrich</forename><surname>Rebholz-Schuhmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Cochez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Decker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Briefings in Bioinformatics</title>
		<imprint>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Convolutional Embedded Networks for Population Scale Clustering and Bio-ancestry Inferencing</title>
		<author>
			<persName><forename type="first">Md Rezaul</forename><surname>Karim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Cochez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Achille</forename><surname>Zappa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ratnesh</forename><surname>Sahay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dietrich</forename><surname>Rebholz-Schuhmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oya</forename><surname>Beyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Decker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Computational Biology and Bioinformatics</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Variational Graph Auto-Encoders</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS Workshop on Bayesian Deep Learning</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Efficient regularized spectral data embedding</title>
		<author>
			<persName><forename type="first">Lazhar</forename><surname>Labiod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Nadif</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Data Analysis and Classification</title>
				<imprint>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="99" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Least squares quantization in PCM</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Lloyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="129" to="136" />
			<date type="published" when="1982">1982. 1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling</title>
		<author>
			<persName><forename type="first">Diego</forename><surname>Marcheggiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1506" to="1515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Graph InfoClust: Maximizing Coarse-Grain Mutual Information in Graphs</title>
		<author>
			<persName><forename type="first">Costas</forename><surname>Mavromatis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PAKDD</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="541" to="553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">On Spectral Clustering: Analysis and an Algorithm</title>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yair</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing Systems: Natural and Synthetic</title>
				<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="849" to="856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">DeepWalk: online learning of social representations</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">3D Graph Neural Networks for RGBD Semantic Segmentation</title>
		<author>
			<persName><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5209" to="5218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Gemsec: Graph embedding with self clustering</title>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Benedek Rozemberczki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rik</forename><surname>Davies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 IEEE/ACM international conference on advances in social networks analysis and mining</title>
				<meeting>the 2019 IEEE/ACM international conference on advances in social networks analysis and mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Social regularized von Mises-Fisher mixture model for item recommendation</title>
		<author>
			<persName><forename type="first">Aghiles</forename><surname>Salah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Nadif</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1218" to="1241" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Simple and Effective Graph Autoencoders with One-Hop Linear Models</title>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Salha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Romain</forename><surname>Hennequin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michalis</forename><surname>Vazirgiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML-PKDD)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="319" to="334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Graph networks as learnable physics engines for inference and control</title>
		<author>
			<persName><forename type="first">Alvaro</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jost</forename><surname>Tobias Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Merel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4470" to="4479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Few-Shot Learning with Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Garcia</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Satorras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Estrach</forename><surname>Bruna</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=BJj6qGbRW" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="93" to="93" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A Graph-to-Sequence Model for AMR-to-Text Generation</title>
		<author>
			<persName><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the Association for Computational Linguistics, ACL 2018</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1616" to="1626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">InfoGraph: Unsupervised and Semi-supervised Graph-Level Representation Learning via Mutual Information Maximization</title>
		<author>
			<persName><forename type="first">Fan-Yun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=r1lfF2NYvH" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Visualizing High-Dimensional Data Using t-SNE</title>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Deep Graph Infomax. ICLR (Poster)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Structural deep network embedding</title>
		<author>
			<persName><forename type="first">Daixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1225" to="1234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amauri</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A general formulation of cluster analysis with dimension reduction and subspace separation</title>
		<author>
			<persName><forename type="first">Michio</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heungsun</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behaviormetrika</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="115" to="129" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Towards K-means-friendly Spaces: Simultaneous Deep Learning and Clustering</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><forename type="middle">D</forename><surname>Sidiropoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyi</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning (Proceedings of Machine Learning Research</title>
				<editor>
			<persName><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</editor>
		<meeting>the 34th International Conference on Machine Learning ( Machine Learning Research</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="3861" to="3870" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Network Representation Learning with Rich Text Information</title>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deli</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Graph R-CNN for Scene Graph Generation</title>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="690" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Attributed Graph Clustering via Adaptive Graph Convolution</title>
		<author>
			<persName><forename type="first">Xiaotong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19. International Joint Conferences on Artificial Intelligence Organization</title>
				<meeting>the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19. International Joint Conferences on Artificial Intelligence Organization</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4327" to="4333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Simple Spectral Graph Convolution</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR, Virtual Event</title>
				<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-05-03">2021. May 3-7, 2021</date>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deep learning for learning graph representations</title>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning: Concepts and Architectures</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="169" to="210" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
