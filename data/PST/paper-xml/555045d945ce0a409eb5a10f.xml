<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exploring Inter-feature and Inter-class Relationships with Deep Neural Networks for Video Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
							<email>zxwu@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
							<email>wongjun@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="department">Institute of Data Science and Technology</orgName>
								<orgName type="institution">Alibaba Group</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jian</forename><surname>Pu</surname></persName>
							<email>jianpu@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
							<email>xyxue@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Exploring Inter-feature and Inter-class Relationships with Deep Neural Networks for Video Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">86F0ACE3527FC696E736BABC6C287214</idno>
					<idno type="DOI">10.1145/2647868.2654931</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T04:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3.1 [Information Storage and Retrieval]: Content Analysis and Indexing-Indexing methods Multimodal Features</term>
					<term>Class Relationships</term>
					<term>Deep Neural Networks</term>
					<term>Action and Event Recognition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Videos contain very rich semantics and are intrinsically multimodal. In this paper, we study the challenging task of classifying videos according to their high-level semantics such as human actions or complex events. Although extensive efforts have been paid to study this problem, most existing works combined multiple features using simple fusion strategies and neglected the exploration of inter-class semantic relationships. In this paper, we propose a novel unified framework that jointly learns feature relationships and exploits the class relationships for improved video classification performance. Specifically, these two types of relationships are learned and utilized by rigorously imposing regularizations in a deep neural network (DNN). Such a regularized DNN can be efficiently launched using a GPU implementation with an affordable training cost. Through arming the DNN with better capability of exploring both the interfeature and the inter-class relationships, the proposed regularized DNN is more suitable for identifying video semantics. With extensive experimental evaluations, we demonstrate that the proposed framework exhibits superior performance over several state-of-the-art approaches. On the well-known Hollywood2 and Columbia Consumer Video benchmarks, we obtain to-date the best reported results: 65.7% and 70.6% respectively in terms of mean average precision.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Techniques for recognizing high-level semantics in diverse unconstrained videos can be deployed in many applications, such as Web video search and video surveillance systems. However, it is well-known that recognizing or classifying the video semantics is an extremely challenging task due to various factors, such as the semantic gap between low-level video features and the complex semantics. While significant progress has been made in recent years, most state-ofthe-art solutions often utilized a large set of features with simple fusion strategies to model high-level video semantics. For instance, two popular ways of combining multiple video features are early fusion and late fusion <ref type="bibr" target="#b41">[41]</ref>. Early fusion concatenates all the feature vectors into a long representation for model training and testing, while late fusion trains a model using each feature separately and combines the outputs of all the models. Both methods do not have the capability of explicitly modeling the correlations among the video features, which can be exploited for deriving better representations. In addition, the existing video classification methods often neglected the inter-class relationships among video semantics. Note that such semantic correlations can be exploited to boost the classification performance since knowing the presence of one class may help predict other correlated semantics contained in the same video. Although there exist a few works investigating multi-feature fusion or exploring the inter-class relationships, as will be discussed in the next section, they mostly addressed the two problems separately. Also, many existing methods are computationally expensive; thus, they are less feasible for large scale applications.</p><p>Realizing the limitations of the existing works, in this paper, we propose a unified framework based on deep neural network (DNN), which jointly learns feature relationships and class relationships, and simultaneously carries out video classification within the same framework utilizing the learned relationships. Figure <ref type="figure" target="#fig_0">1</ref> gives a conceptual diagram of the proposed approach. First, we extract various video features including local visual descriptors and audio descriptors. The features are then used as the inputs of a DNN, where the first two layers are input layer and feature transformation layer. The third layer of the network is called fusion layer, where structural regularization is imposed on the network weights to identify and utilize the feature relationships. Specifically, the regularization terms are designed based on the observation of two natural properties of the inter-feature relationships, correlation and diversity. The former means that different features may share some common patterns in a middle level representation lying between the original features and the high-level semantics. The latter emphasizes the unique characteristics of different features, The features are transformed (abstracted) using one layer of neurons before fusion. On the fusion layer, regularization on the network parameters is imposed to ensure that different features can share correlated dimensions while preserving their unique characteristics. As indicated by line width in the figure, some dimensions of different features may be highly correlated (the thick lines pointing to the same neuron). After that, the weights between the fusion and the output layer are also regularized to identify groups of classes. Both the learned inter-feature and inter-class relationships are utilized for improved classification performance.</p><p>which serve as complementary information for predicting the video semantics. Through modeling these two properties using a feature correlation matrix, we impose a trace-norm regularization over the fusion weights to reveal the hidden correlation and diversity of the features.</p><p>For the inter-class relationships, we impose regularizations on the weights of the final output layer to automatically identify the grouping structures of video classes, as well as the outlier classes. Semantic classes within the same group share commonalities or correlations that can be utilized as knowledge sharing for improved classification performance, while the outlier classes should be excluded from negative knowledge sharing. We will show that by imposing a similar trace-norm based regularization on the weights of the output layer, we are able to explore such complex inter-class relationships effectively to produce better video classification results. Therefore, this allows us to develop a unified framework using a regularized DNN, which can be easily implemented using a GPU with affordable training cost.</p><p>Notice that it is feasible to use raw video data as inputs instead of the hand-crafted features like the recent works on image classification using deep learning <ref type="bibr" target="#b22">[22]</ref>. In this case, the convolutional neural network (CNN) can be adopted for performing feature extraction from the raw data. The reasons of using the hand-crafted features in our proposed framework are two-folds. First, the hand-crafted features are widely used in video classification and remain the central components of some video analytical systems that generated recent state-of-the-art results on tasks like human action recognition <ref type="bibr" target="#b46">[46]</ref> and event recognition <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b23">23]</ref>. By using these features it is easy to make fair comparisons with the traditional semantic classification approaches such as the popular SVM classifiers. Second, extracting features using neural networks needs more layers of neurons that incur a significant number of additional parameters to be tuned, requiring much more training data. Note that in many video classification tasks, the amount of available training data is far less from sufficient for training a neural network with too many layers. Therefore, in this paper, we focus the proposed regularized DNN on the tasks of feature fusion and video semantic classification.</p><p>To the best of our knowledge, this work represents the first attempt to capture both the feature and the class relationships in a DNN for video classification. Our major contributions are summarized as follows:</p><p>1. We propose to impose structural regularization on the fusion layer in a DNN to identify the correlations of multiple features, while still maintaining their diversities. This unique capacity distinguishes the proposed method from most of the existing works that often adopted shallow fusion process without considering the deep exploration of the feature correlations.</p><p>2. We also propose to explore inter-class relationships through imposing a similar structure regularization on output layer of the DNN. Therefore, both the interfeature and the inter-class relationships are formulated and explored in a unified framework, which can be easily implemented with a GPU and trained with an affordable time cost.</p><p>3. Extensive empirical evaluations are provided to corroborate the effectiveness of the proposed framework in detail, and we attained to-date the highest performance on the widely used real-world benchmarks.</p><p>The remaining sections of this paper are organized as follows. Section 2 discusses related works. Section 3 elaborates the proposed framework, including both formulation and optimization. Extensive experimental results and comparisons with alternative methods and the state of the arts are reported and discussed in Section 4. Finally, Section 5 concludes this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>Extensive studies have been conducted in the field of video classification, and typical approaches often combined several multiple features in a standard machine learning pipeline using classifiers like the SVM. Most of the existing work focused on developing effective features <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b46">46]</ref>, novel recognition methods <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b39">39]</ref>, or comprehensive systems that integrate multiple features and classifiers for competitive classification performance <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b30">30]</ref>. Besides accuracy, efficiency is another important factor that should be considered in the design of a modern video classification system. Several recent studies focused on this issue by using efficient classification methods <ref type="bibr" target="#b27">[27]</ref> or parallel computing <ref type="bibr" target="#b49">[49]</ref>.</p><p>In the following we focus our discussion on works investigating multi-feature fusion, and on those exploring interclass relationships, which are more relevant to this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Fusing Multiple Features</head><p>As aforementioned, there exist two popular fusion strategies, known as early fusion and late fusion. Although both strategies are not able to explore hidden feature relationships such as the correlations between features, they are widely used in many state-of-the-art systems due to the simplicity <ref type="bibr" target="#b1">[1]</ref>. In addition, both of them require to design fusion weights that indicate the importance of each individual feature. The weights can be set as equal using heuristics (a.k.a. average fusion), or learned using the cross validation method. Some other works also employed multiple kernel learning (MKL) <ref type="bibr" target="#b5">[5]</ref> to estimate the fusion weights <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b31">31]</ref>, which may lead to performance gain that is nevertheless often observed to be insignificant <ref type="bibr" target="#b45">[45]</ref>.</p><p>More recently, several advanced feature fusion approaches have been proposed. In <ref type="bibr" target="#b50">[50]</ref>, an optimization framework was used for robust late fusion to derive better combination of multiple feature modalities. It seeks a shared low-rank matrix to remove noises of certain modalities, which requires to iteratively compute singular value decomposition with a cubic time complexity, and thus is less scalable for large scale real-world applications. In a following up work by Liu et al. <ref type="bibr" target="#b25">[25]</ref>, dynamic fusion was adopted to find the best feature combination for each sample. This approach was proved effective but is extremely time-consuming. In <ref type="bibr" target="#b17">[17]</ref>, Jiang et al. proposed to construct an audio-visual joint codebook based on the discovered correlations between audio and visual features for video concept classification. The approach pointed out a promising direction as this is among the first works performing deep mining of feature correlations. However, the used visual features were computed on each segmented patches from video frames, which is computationally prohibitive for most real-word scenarios. The approach was further enhanced in <ref type="bibr" target="#b18">[18]</ref>, where the temporal interaction of audio-visual features was investigated. Jhuo et al. <ref type="bibr" target="#b16">[16]</ref> improved the speed of training the audio-visual joint codebook by using standard local visual features like the SIFT, instead of the segmentation-based region features.</p><p>There are also a few studies on combining multiple features in neural networks, which are closely related to our work. A deep denoised auto-encoder was adopted to learn a shared representation from mutimodal inputs <ref type="bibr" target="#b32">[32]</ref>, and similarly, a deep Boltzmann machine was utilized to fuse visual and textual features <ref type="bibr" target="#b42">[42]</ref>. However, both methods integrated multiple features without elaborately considering the feature correlation and diversity. One important message delivered in this paper is that regularized fusion of multiple features is intuitively reasonable and empirically effective, compared with those "free" forms of fusion approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Exploring Inter-class Relationships</head><p>There are many existing works on exploring class relationships, often called context, for improved classification performance. In <ref type="bibr" target="#b44">[44]</ref>, Torralba et al. highlighted the importance of context in object detection. In <ref type="bibr">[6,</ref><ref type="bibr" target="#b37">37]</ref>, co-occurrence context was utilized to enforce object recognition in images. For video classification, Qi et al. <ref type="bibr" target="#b36">[36]</ref> proposed to use a multilabel learning method based on ideas from the Gibbs random field. Jiang et al. <ref type="bibr" target="#b19">[19]</ref> proposed a semantic diffusion method to utilize class-relationships for video annotation. The algorithm is also capable of adapting the pre-defined class relationships to a new test data domain. Weng et al. <ref type="bibr" target="#b47">[47]</ref> proposed a domain-adaptive method that not only explored the class relationships, but also utilized temporal structural information in long broadcast news videos for better annotation performance. Most of these approaches, however, are largely based on the co-occurrence statistics of the video classes, and cannot be used in the cases where the classes share commonalities but do not explicitly co-occur. Our approach can automatically learn such commonalities via a regularized DNN using a rigorous formulation.</p><p>The regularization terms used in our approach are partly motivated by recent advances in Multiple Task Learning (MTL) <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b52">52]</ref>. MTL trains multiple models simultaneously and improves the performance of a task (classifier) with the help of other related tasks. Recent years have witnessed practical successes of MTL in many applications, such as disease prediction <ref type="bibr" target="#b51">[51,</ref><ref type="bibr" target="#b53">53]</ref> and financial stock selection <ref type="bibr" target="#b13">[13]</ref>. Sharing commonalities between different tasks is the central idea of MTL and several approaches have been developed with regularizations on shared common patterns across classes <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b35">35]</ref>. These works considered the class relationships in classification or regression tasks with conventional shallow learning models, but never injected similar regularizations into neural networks.</p><p>In fact, multi-layered neural network can be considered as one of the earliest MTL models <ref type="bibr" target="#b8">[8]</ref> (see Figure <ref type="figure">2 (b)</ref>). In such a neural network, each unit in the output layer corresponds to a task and the hidden layer neurons can be regarded as shared common patterns. In this paper, we argue that imposing explicit forms of regularization ensures the deep modeling of complex class relationships for video classification, and thus generates better performance than the traditional neural network with implicit task sharing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">METHODOLOGY</head><p>This section elaborates our proposed method. We start from introducing the notations and the problem setup. Then a brief introduction of standard DNN is given to support later discussions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Notations and Problem Setup</head><p>Assume that we are given a training set with N video clips, which are associated with C semantic classes. Here each video clip is represented by M different features, such as various visual and audio descriptors. Therefore, we can denote each training sample as an (M + 1)-tuple:</p><formula xml:id="formula_0">(x 1 n , • • • , x m n , • • • , x M n , yn), n = 1, • • • , N, Class 1 … Class 1 … Class C Class 1 … Class C Class 1 … Class C (a) Single-feature Single-class (b) Single-feature Multi-class (c) Multi-feature Single-class (d) Multi-feature Multi-class … … … Class C … Figure 2: Illustration of different neural network structures. (b)</formula><p>is the most popular structure for multi-class prediction, while (d) was used in works like <ref type="bibr" target="#b42">[42]</ref> to combine multiple features, where features are processed separately in the network and then merged through a middle layer. In this paper, we impose regularizations on the same structure as shown in (d) to explore both the inter-feature and the inter-class relationships.</p><p>where x m n represents the m-th feature representation of the n-th video sample, and yn</p><formula xml:id="formula_1">= [yn1 • • • ync • • • ynC ] ∈ R C</formula><p>is the corresponding semantic label with the c-th element ync = 1 if the n-th video sample is associated with the cth semantic class. The goal is to train prediction models that can classify new test videos. A straightforward way is to independently train one classifier for each semantic class, and different features can be combined using either the early fusion or the late fusion scheme. However, such an independent training strategy does not explore the inter-feature as well as the inter-class relationships. Here, we propose a DNN based video classification model that carries out feature sharing in the fusion layer through exploring the correlation and diversity of multiple features, as shown in Figure <ref type="figure" target="#fig_0">1</ref>. In addition, the prediction layer of our deep neural network is also regularized to enforce knowledge sharing across different classes. Hence, both kinds of relationships are explicitly explored in a uniform learning process. Below we first introduce the standard DNN with a single feature and then present the details of our proposed regularized DNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">DNN Learning with Single Feature</head><p>Inspired by the biological nervous systems, DNN uses a large number of interconnected neurons to construct complex computational models. Through organizing the neurons in multiple layers, this method possesses strong non-linear abstraction capacity and is able to learn arbitrary mapping functions from inputs to outputs as long as being given sufficient training data. Below we briefly review a standard DNN with only one feature as the input, i.e., M = 1.</p><p>In a DNN with a total of L layers, we denote a l-1 and a l as the input and the output of the l-th layer for a single feature, l = 1, • • • , L, while W l and b l refer to the weight matrix and the bias vector of the l-th layer, respectively. The transition function from the (l -1)-th layer to the l-th layer can be formulated as:</p><formula xml:id="formula_2">a l = σ (W l-1 a l-1 + b l-1 ) , l &gt; 1; x, l = 1.<label>(1)</label></formula><p>Here σ(•) is a nonlinear sigmoid function, which is often defined as</p><formula xml:id="formula_3">σ(x) = 1 1 + e -x .</formula><p>Figure <ref type="figure">2</ref> (a) and (b) show two types of four-layered neural networks using a single feature as the input.</p><p>To derive the optimal weights for each layer, one can formulate the following optimization problem:</p><formula xml:id="formula_4">min W N i=1 (f (xi), yi) + λ1 2 L-1 l=1 W l 2 F ,<label>(2)</label></formula><p>where the first part measures the empirical loss on the training data by summing the discrepancy between the outputs of the network ŷi = aL = f (xi) and the ground-truth labels yi, and the second part is a regularization term preventing overfitting. For simplicity, we can absorb b into the weights coefficient W by adding an additional dimension to the feature vectors with a constant value one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Regularization on Feature Relationships</head><p>A single feature based DNN can be powerful in some cases. However, it can only be used with a single aspect of the data to perform semantic prediction. For complex data like the videos, the semantic information can be carried by different feature representations including both visual and audio clues. Note that simple fusion strategies, such as the early or late fusion, usually result in limited performance gain since the intrinsic relations among multiple feature representations are overlooked <ref type="bibr" target="#b4">[4]</ref>. In addition, such simple fusion methods often incur extra efforts for training the classifiers. Therefore, it is desired to obtain a compact yet meaningful fused representation that fully leverages the complementary clues from various features. Below we extend the basic DNN to a regularized variant that is able to accommodate the deep fusion process of multiple features.</p><p>We are given a total of M features</p><formula xml:id="formula_5">{x 1 i , • • • , x m i , • • • , x M n }, i = 1, • • • , n</formula><p>for each video sample. Motivated by the multisensory integration process of primary neurons in biological systems <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b43">43]</ref>, we propose to use one additional layer for the fusion of all the features, as shown in Figure <ref type="figure" target="#fig_0">1</ref>. Accordingly, the transition equation for this fusion layer can be written as the following:</p><formula xml:id="formula_6">aF = σ M m=1 W m E a m E + bE ,<label>(3)</label></formula><p>where E and F are the indices of the last layer of feature extraction and the fusion layer respectively (i.e., F = E +1).</p><p>Here a m E ∈ R P denotes the extracted mid-level representation for the m-th feature which is first linearly transformed by the weight W m E and then non-linearly mapped to the new representation aF using a sigmoid function.</p><p>Since all the feature representations correspond to the same video data, it is easy to understand that various features can be used to reveal the common latent patterns related to the video semantics. In addition, as mentioned earlier, different features could also be complementary because they have distinct characteristics. Therefore, the fusion process should aim to capture the relations among the features, while being able to reserve their unique characteristics at the same time. Instead of simply adding up multiple feature information, we specifically formulate an objective function that can regularize the fusion process to explore such correlations and diversity among the multiple features simultaneously. In particular, the weights,</p><formula xml:id="formula_7">W 1 E , • • • , W M E</formula><p>, which transform all features into a shared representation, are first vectorized into P dimensional vectors separately, where P is the product of the a m E 's (m = 1, • • • , M ) dimension and the aF 's dimension. Here, we assume the extracted features are of the same dimension. Then we stack these coefficient vectors into a matrix WE ∈ R P ×M , where each column of WE corresponds to the weights of a single feature. Hence, the element WE(i, j) is given as</p><formula xml:id="formula_8">WE(i, j) = W i E (j), i = 1, • • • , M, j = 1, • • • , P.</formula><p>Then we can formulate the following objective to design a regularized DNN: min</p><formula xml:id="formula_9">W,Ψ L + λ1 2 E l=1 M m=1 W m l 2 F + L-1 l=F W l 2 F + λ2 2 tr(WEΨ -1 W T E ) s.t. Ψ 0,<label>(4)</label></formula><p>where L = N i=1 (ŷi, yi). Compared with the objective function in Equation <ref type="formula" target="#formula_4">2</ref>for the standard single feature neural network, the above cost function includes one additional regularization term. Note that the matrix WE represents the coefficients over all the features. Here we use a symmetric and positive semidefinite matrix Ψ ∈ R M ×M to model the inter-feature correlation and introduce the last regularization term with the trace norm that can help learn the inter-feature relationship <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b52">52]</ref>. Note that the entries with large values in Ψ indicate strong feature correlations, while small-valued entries denote the diversity among different features since they are less correlated. The coefficients λ1 and λ2 control the contributions from different regularization terms. Finally, the objective of learning the regularized DNN is performed as a joint optimization procedure over the weight matrix W and the feature correlation matrix Ψ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Regularization on Class Relationships</head><p>To recognize or classify C semantic categories, one can simply adopt the one-vs-all strategy to independently train C classifiers. Figure <ref type="figure">2</ref> (a) and (c) illustrate this one-vs-all training scheme with a total of C four-layered neural networks for the single-feature and multi-feature settings, respectively. Clearly, each of these C neural networks is separately learned, where knowledge sharing among different semantic categories is completely neglected. However, it is well recognized that video semantics also share some commonality, which indicates that certain semantic categories could be strongly correlated <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b36">36]</ref>. Therefore, it is critical to explore such a commonality by simultaneously learning multiple video semantics, which can often lead to better learning performance. Note that, the commonality among multiple classes is often represented by the parameter sharing among different prediction models <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b26">26]</ref>. Compared with the popular SVM method, it is more natural for DNN to perform multi-class training simultaneously. As shown in Figure <ref type="figure">2</ref> (b), by adopting a set of C units in the output layer, a single-feature based DNN can be easily extended to multi-class problems, and this structure has been widely adopted. Motivated by the regularization framework used in the standard MTL methods <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b26">26]</ref>, here we present a regularized DNN that aims at training multiple classifiers simultaneously with deeper exploration of the class relationships. To enforce the semantic sharing, we extend the original objective for a standard DNN to the following form: min</p><formula xml:id="formula_10">W,Ω N i=1 (f (xi), yi) + λ1 2 L-1 l=1 W l 2 F + λ2tr(WL-1Ω -1 W T L-1 ). s.t. Ω 0.<label>(5)</label></formula><p>Note that some previous MTL works assumed that the class relationships are explicitly given and are ready for use as prior knowledge <ref type="bibr" target="#b26">[26]</ref>, while our method does not require this. Following the convex formulation of MTL <ref type="bibr" target="#b52">[52]</ref>, here we impose a trace norm regularization term over the coefficients WL-1 of the output layer with the class relationships augmented as a matrix variable Ω ∈ R C×C . Note that the constraint Ω 0 indicates that the class relationship matrix is positive semidefinite since it can be viewed as the similarity measure of the semantic classes. The coefficients λ1 and λ2 are regularization parameters. During the learning procedure, the optimal weight matrices {W l } L l=1 and the class relationship matrix Ω are simultaneously derived.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">The Unified Objective</head><p>To unify the above objectives into a joint framework, we now present a novel DNN formulation that explores both the inter-feature and the inter-class relationships. In our framework, we use one layer of neurons to fuse multiple features, where the objective is to bridge the gap between low-level features and the high-level video semantics. In the final layer of generating the predictions, we impose a trace norm regularization among different semantics to better learn the predictions of multiple classes. Mathematically, we incorporate the feature regularization in Equation <ref type="formula" target="#formula_9">4</ref>and the class regu-larization in Equation 5 into the following objective function:</p><formula xml:id="formula_11">min W,Ψ,Ω L + λ1 2 E l=1 M m=1 W m l 2 F + L-1 l=F W l 2 F + λ2 2 tr(WEΨ -1 W T E ) + λ3 2 tr(WL-1Ω -1 W T L-1 ), s.t. Ψ 0 tr(Ψ) = 1, Ω 0 tr(Ω) = 1,<label>(6)</label></formula><p>where λ1, λ2, and λ3 are regularization parameters. Compared with the original objective in Equation <ref type="formula" target="#formula_4">2</ref>, we have two trace-norm regularization terms that are tailored for the fusion of multiple features and the exploration of the interclass relationships, respectively. Two additional constraints tr(Ψ) = 1 and tr(Ω) = 1 are used to restrict the complexity, as suggested in <ref type="bibr" target="#b52">[52]</ref>. Finally, the above cost function is minimized with respect to the network weights {W l } L l=1 , the inter-feature relationship matrix Ψ, and the inter-class correlation matrix Ω.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Optimization</head><p>Among the variables in the minimization problem of Equation 6, two pairs of variables, i.e., (WE, Ψ) and (WL-1, Ω), are coupled with each other. Therefore, we adopt the alternative optimization method to iteratively minimize the objective with respect to <ref type="figure"></ref>and<ref type="figure">Ω</ref>.</p><formula xml:id="formula_12">W m l (l = 1, • • • L, m = 1, • • • , M ), Ψ,</formula><p>By fixing both Ψ and Ω, we first consider the minimization problem over W m l , which is degenerated to a set of unconstrained univariate optimization problems: min</p><formula xml:id="formula_13">W m l L + λ1 2 E l=1 M m=1 W m l 2 F + L-1 l=F W l 2 F + λ2 2 tr(WEΨ -1 W T E ) + λ3 2 tr(WL-1Ω -1 W T L-1 ). (<label>7</label></formula><formula xml:id="formula_14">)</formula><p>As all the terms of the above objective function are smooth, the gradient can be easily evaluated. Denote G m l as the gradient with respect to W m l , the weight matrix for the l-th layer and the m-th feature is updated as:</p><formula xml:id="formula_15">W m l = W m l -ηG m l ,<label>(8)</label></formula><p>where η is the step length of the gradient descent.</p><p>We then introduce the solution of minimizing the objective function over Ψ with other variables being fixed. The problem in Equation 6 degenerates to:</p><formula xml:id="formula_16">min Ψ tr(WEΨ -1 W T E ), s.t. Ψ 0 tr(Ψ) = 1.<label>(9)</label></formula><p>Adopting the Cauchy-Schwarz inequality, we obtain the analytical solution for the above minimization problem as:</p><formula xml:id="formula_17">Ψ = (W T E WE) 1 2 tr((W T E WE)<label>1 2 )</label></formula><p>.</p><p>Similarly, the optimal solution for Ω is derived as:</p><formula xml:id="formula_19">Ω = (W T L-1 WL-1) 1 2 tr((W T L-1 WL-1)<label>1 2 )</label></formula><p>. </p><formula xml:id="formula_21">W m l = W m l -ηG m l ;</formula><p>4: Update the feature relationship matrix Ψ according to Equation <ref type="formula" target="#formula_18">10</ref>:</p><formula xml:id="formula_22">Ψ = (W T E W E ) 1 2 tr((W T E W E )<label>1 2 )</label></formula><p>; 5: Update the class relationship matrix Ψ according to Equation 11:</p><formula xml:id="formula_23">Ω = (W T L-1 W L-1 ) 1 2 tr((W T L-1 W L-1 )<label>1 2</label></formula><p>)</p><p>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6: end for</head><p>Note that Zhang et al. adopted a similar solution as in Equation 11 to identify task correlations for a linear kernel based regression and classification task <ref type="bibr" target="#b52">[52]</ref>. However, our method integrates more complex structural regularizations in a neural network architecture, where both the inter-feature and the inter-class relationships are explored for a completely different application. Hence, the difference of our method is fairly significant. In our approach, the inter-feature and inter-class relationships are first estimated based on the corresponding weights in the neural networks. The relationships are then used in turn to adjust the network weights for improved classification performance. Using the trace norm allows us to derive the analytical solution in Equation 10 and Equation <ref type="formula" target="#formula_20">11</ref>, which satisfies our goal of learning the relationships Ψ and Ω based on W. More specifically, the training procedure of the proposed method is summarized in Algorithm 1. For each epoch, additional efforts are required to compute the gradient matrix G m l for updating W m l , as well as to update the matrices Ω and Ψ. The complexity of calculating the trace norms is the same as that of the 2 norm. The update of Ω and Ψ requires operations with a cubic complexity with respect to the number of features M and the number of video classes C, respectively. Note that these two numbers are often relatively much smaller than the number of data points used for DNN training. Therefore, the training cost of the proposed regularized DNN is very similar to that of a standard DNN. Our empirical study further confirms the efficiency of our method, as will be discussed in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Datasets and Evaluation Measure</head><p>We adopt three challenging benchmarks on action and event recognition to evaluate our method, as described in the following.</p><p>Hollywood2 <ref type="bibr" target="#b24">[24]</ref>. The Hollywood2 dataset is one of the most popular benchmarks on action recognition in videos. Collected from 69 Hollywood movies, it contains 1,707 action video clips covering 12 classes: answering phone, driving car, eating, fighting, getting out of car, hand shaking, hugging, kissing, running, sitting down, sitting up and standing up. Following <ref type="bibr" target="#b24">[24]</ref>, the dataset is split into a training set with 823 videos and a test set with 884 videos.</p><p>Columbia Consumer Videos (CCV) <ref type="bibr" target="#b20">[20]</ref>. The CCV dataset is a well-known benchmark on Internet consumer video analysis. It contains 9,317 videos collected from YouTube with annotations of 20 semantic classes, including objects (e.g., "cats"), scenes (e.g., "playground"), and events (e.g., "parade"). Since most of the classes are complex events, it requires a joint use of multiple feature clues like visual and audio representations to perform better classification. The dataset is evenly split into a training set and a test set.</p><p>CCV+. Since both the Hollywood2 and the CCV datasets are small in terms of the number of annotated classes, we additionally collected and annotated another 20 classes with in total of 5,159 video clips. These clips are merged with the CCV to form a larger dataset of 40 classes, named CCV+, containing 7,244 videos for training and 7,232 videos for testing. See Figure <ref type="figure" target="#fig_2">3</ref> for the list of class names and a few example frames.</p><p>For all the three datasets, the performance is measured by average precision (AP) for each class and mean AP (mAP) for overall results of all the classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Feature Representations</head><p>Visual Features. For the visual features, we consider the dense trajectory descriptors <ref type="bibr" target="#b46">[46]</ref>, which have exhibited strong performance on various benchmark datasets. Briefly, densely sampled local frame patches are first tracked over time and four descriptors are then computed for each trajectory: a 30-d trajectory shape descriptor, a 96-d histogram of oriented gradients (HOG) descriptor, a 108-d histogram of optical flow (HOF) descriptor, and a 108-d motion boundary histogram (MBH) descriptor. Finally, each descriptor is quantized into a 4,000-d bag-of-words representation, same as the settings used in <ref type="bibr" target="#b46">[46]</ref>.</p><p>Audio Features. It is well-known that the audio soundtracks contain useful clues for identifying some video semantics. Two types of video features are considered in our study. The first one is the popular MFCCs (Mel-Frequency Cepstral Coefficients), which are computed for every 32ms timewindow with 50% overlap and then quantized into a bag-ofwords representation. The second one is called Spectrogram SIFT <ref type="bibr" target="#b54">[54]</ref>. The 1-d soundtrack of a video is transformed into a 2-D image based on the constant-Q spectrogram, on which standard SIFT descriptors are extracted and quantized into the bag-of-words representation.</p><p>Note that all these visual and audio features are adopted because they have demonstrated strong performance on various benchmarks; however, evaluating the performance of each single feature in detail is beyond the scope of this work. All the representations are normalized with RootSift <ref type="bibr" target="#b2">[2]</ref>, which has been shown to be more suitable for histogrambased features than the conventional L2 normalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Compared Approaches</head><p>To validate the effectiveness of our method, we compare with the following approaches:</p><p>Early Fusion with Neural Networks (NN-EF). All the features are concatenated into a long vector and then used as the input to train a neural network.</p><p>Late Fusion with Neural Networks (NN-LF). A separate neural network is trained using each feature independently and then the outputs of all the networks are fused to obtain the final classification results.</p><p>Early Fusion with SVM (SVM-EF). The popular χ 2 kernel SVM is adopted and the features are combined on the kernel level before classification.</p><p>Late Fusion with SVM (SVM-LF). A separate SVM classifier is trained for each feature and prediction results are then combined.</p><p>Multimodal Deep Boltzmann Machines (M-DBM). It is a fusion model proposed in <ref type="bibr" target="#b42">[42]</ref>, where multiple feature representations are used as the inputs of the Deep Boltzmann Machines.</p><p>Discriminative Model Fusion (DMF) <ref type="bibr" target="#b40">[40]</ref>. As one of the earliest approaches in multimedia for context-based classification, DMF uses the outputs of an initial classifier, e.g., a standard DNN in our case, as features to train an SVM model as the second level classifier for final prediction.</p><p>Domain Adaptive Semantic Diffusion (DASD) <ref type="bibr" target="#b19">[19]</ref>. This method uses a graph diffusion formulation for contextbased classification. The prediction outputs of a normal DNN (without the regularizations) are used as inputs of the DASD in a post-processing refinement step. The approach also requires input of precomputed class relationships, which are usually estimated based on statistics of label co-occurrences in training data.</p><p>The first five approaches can be regarded as alternatives for feature fusion, while the last two approaches focus on the use of the class relationships. All the neural network based experiments are conducted on a single Nvidia Telsa K20 5GB GPU with MATLAB Parallel Computing Toolbox, which speeds up the training procedure by about 5 times than a decent Intel XEON CPU with 16 cores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results and Analysis</head><p>In this section, we first report results of our approach by disabling the regularizations on the output layer and the fusion layer respectively, in order to understand the contributions of only exploring the inter-feature or the inter-class relationships. This also ensures fair comparisons with the competing approaches. After that, we report results of using the entire framework, compare with recent state-of-theart results, and analyze the effect of the number of training samples. Finally, we provide discussions on computational efficiency.</p><p>Throughout the experiments, we set the learning rate of the neural networks to 0.7, fix λ1 to 3 × 10 -5 in order to prevent overfitting, and tune λ2 and λ3 in the same range as λ1. Following the conventional settings of DNN, we also adopt the mini batch gradient descent with the batch size being 70.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Effect of Exploring Feature Relationships</head><p>We first report results by only using fusion layer regularization on the DNN with the output layer regularization being disabled. all the compared methods. Note that the M-DBM approach also utilizes a neural network for feature fusion, but in a free manner without explicitly taking feature relations into the learning process. These results validate the effectiveness of imposing the proposed fusion regularization in neural networks. Notice that, since these adopted datasets are very challenging and have been widely used, an absolute performance gain of 2% is generally considered as a significant improvement.</p><p>Comparing across the alternative approaches, early fusion tends to generate better results than late fusion. This observation is consistent with recent works, where the early fusion was more popularly used <ref type="bibr" target="#b1">[1]</ref>. The neural network based approaches do not show significant gain over the SVM-based ones because the amount of training data in video classification is limited. With more training samples, the margin is expected to be significantly larger. In addition, for the contribution of the audio clues, we observed that for the classes with strong audio clues, such as "answering phone", adding audio features clearly improves the performance. On the contrary, for classes like "sitting down", using audio features may slightly degrade the result, which is easy to understand.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Effect of Exploring Class Relationships</head><p>Next we analyze the effect of solely imposing the regularization on the classification/output layer to explore the inter-class relationships, while disabling the regularization on the fusion layer. We compare our method, named as DNN-Classification Regularization (DNN-CR), with DMF and DASD and report the results in Table <ref type="table" target="#tab_1">2</ref>. Clearly, DNN-CR outperforms these two compared approaches, both of which use the outputs of the conventional DNN as inputs for context-based refinement. These results corroborate the effectiveness of the proposed regularization on the output layer. Note that the simple DMF is superior than the DASD because the latter requires pre-computed inter-class relationships, which are normally estimated based on the label cooccurrences in training data. However, some classes that share commonalities may not visually co-occur, resulting in  a very sparse class relationship matrix that is insufficient for context-based learning. Several other alternative methods such as those based on the conditional random field <ref type="bibr" target="#b37">[37]</ref> also rely on the relationship matrix and thus suffer from the same limitation.</p><p>To further show the power of our method in learning the class relationships, we visualize some results in Figure <ref type="figure" target="#fig_2">3</ref>. As discussed in Section 3, values in the matrix Ω can reflect the learned correlations among the classes. Hence, we can adopt spectral clustering on the matrix Ω to group the video semantics, which are then re-ordered for the ease of visualization. We see that many classes sharing commonalities are grouped together, which confirms that our method can reveal the hidden class relationships.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Results of the Entire Framework</head><p>We now discuss the results of the entire framework, i.e., using regularizations on both the fusion layer and the output layer. In addition, to evaluate the performance using different amounts of training data, we plot the performance w.r.t. the number of training samples in Figure <ref type="figure" target="#fig_3">4</ref>. Overall, substantial performance gains are observed from using the regularized DNN framework. Using regularizations on both layers also clearly achieves higher performance than solely imposing regularization on a single layer (i.e., only on W E ) with clear margins. When there are less training samples, the improvement of our method is even more significant. An improvement of around 100% is obtained on all the three datasets when there are just 10 training samples per class. This demonstrates that the regularized DNN requires much less training data to achieve comparable performance to the non-regularized DNN. In addition, comparing the performance across the three datasets using all the training samples, the gain from exploring the class relationships is more significant on CCV+. This is because CCV+ has more classes and thus contains richer inter-class relationships that are helpful for classification. Table <ref type="table" target="#tab_3">3</ref> further compares our best results (from regularization on both layers) with several recently reported results on the Hollywood2 and the CCV datasets. On Hollywood2, our proposed method achieves the best mAP of 65.7%, outperforming all the recent results <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b46">46]</ref>. All these works are based on the dense trajectory features, and they performed classification using the simple early fusion method. Note that Wang et al. <ref type="bibr" target="#b46">[46]</ref> and Oneata et al. <ref type="bibr" target="#b34">[34]</ref> used the Fisher vector to encode the features, which has been shown to be more effective than the traditional bag-of-words representation <ref type="bibr" target="#b38">[38]</ref>. However, the dimension of the Fisher vectors is too high to be used as inputs of the neural networks, since there is no sufficient training data to tune the numerous parameters. Therefore, we adopt the standard bag-of-words in this work, and it is very appealing to observe higher performance than the approaches using the Fisher vectors.</p><p>For the CCV dataset, several recent works have focused on the joint use of multiple audio-visual features. Xu et al. <ref type="bibr" target="#b48">[48]</ref> and Ye et al. <ref type="bibr" target="#b50">[50]</ref> adopted late fusion with specially designed methods to remove the noise of individually trained classifiers, and Jhuo et al. used a joint audio-visual codebook for classification <ref type="bibr" target="#b16">[16]</ref>. Our approach is fundamentally different from these state-of-the-art methods in its design and produces significantly higher performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Computational Efficiency</head><p>Finally, we briefly compare and discuss the computational efficiency, using the Hollywood2 dataset. The average training time of each epoch for NN-EF, NN-LF and M-DBM are presented in Table <ref type="table" target="#tab_4">4</ref>, using a GPU-based implementation as Hollywood2 mAP CCV mAP Mathe et al. <ref type="bibr" target="#b28">[28]</ref> 61.0% Jiang et al. <ref type="bibr" target="#b20">[20]</ref> 59.5% Jain et al. <ref type="bibr" target="#b15">[15]</ref> 62.5% Jhuo et al. <ref type="bibr" target="#b16">[16]</ref> 64.0% Oneata et al. <ref type="bibr" target="#b34">[34]</ref> 63.4% Xu et al. <ref type="bibr" target="#b48">[48]</ref> 60.3% Wang et al. <ref type="bibr" target="#b46">[46]</ref> 64.3% Ye et al. <ref type="bibr" target="#b50">[50]</ref> 64.0% Regularized DNN 65.7% Regularized DNN 70.6% mentioned in Section 4.1.3. Our proposed method is more efficient than NN-EF and NN-LF as our regularized DNN contains less parameters to be learned. Specifically, compared with the early fusion, our framework processes the features separately in the first two layers and thus avoids the parameters interacting among them. The late fusion method requires the training of separate networks, which is also more expensive. Note that we exclude the M-DBM approach in this comparison, because it requires significant additional time to pre-train the network for weight initialization. For all the methods, several hundreds of epochs are normally needed to finish the training process (several minutes in total). Once the training is done, all these neural network based methods are extremely fast in terms of performing predictions on testing videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>We have introduced a novel DNN framework that explores both inter-feature and inter-class relationships to achieve better classifications on video semantics. By imposing tracenorm based regularizations on a specially designed fusion layer and an output layer in the neural network, our method can learn a fused representation of multiple feature inputs and utilize the commonalities among the semantic classes for improved classification performance. Extensive experiments on popular benchmarks of action and event recognition have shown that our method consistently outperforms the alternative approaches as well as the recent state-of-theart works. In addition, the proposed method is also similar to even faster than the traditional approaches in terms of the model training, which is very important for large scale applications. One important future work is to add the function of learning feature representations directly from raw video data in the framework, which would require much more training data as discussed earlier but may lead to substantial further performance improvements. Therefore, it would also be interesting and valuable if this extension could be done with an effort of collecting and annotating a larger collection of videos, like the Image-Net effort for image analysis <ref type="bibr" target="#b11">[11]</ref>, which is urgently needed to stimulate the research on large scale video classification.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of the proposed DNN-based video classification framework. Various visual/audio features are first extracted and then used as inputs of a DNN.The features are transformed (abstracted) using one layer of neurons before fusion. On the fusion layer, regularization on the network parameters is imposed to ensure that different features can share correlated dimensions while preserving their unique characteristics. As indicated by line width in the figure, some dimensions of different features may be highly correlated (the thick lines pointing to the same neuron). After that, the weights between the fusion and the output layer are also regularized to identify groups of classes. Both the learned inter-feature and inter-class relationships are utilized for improved classification performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>) Algorithm 1 1 : 3 :</head><label>113</label><figDesc>Training Procedure of Regularized DNN Require: x m n : the representation of the m-th feature for the nth video sample; yn: the semantic label of the n-th video sample; Initialize W m l randomly, Ψ = 1 M I M and Ω = 1 C I C , where I M and I C are identity matrices; 2: for epoch = 1 to K do Back propagate the prediction error from layer L to layer 1 by evaluating the gradient G m l , and update the weight matrix W m l for each layer and each feature as:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Class relationships in CCV+ indicated by the learned matrix Ω. Example video frames of a few found class groups are shown at the bottom.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Performance on the three datasets using different number of training samples. We plot the results of DNN without regularization (red), DNN with regularization only on the fusion layer (black), and DNN with regularization on both the fusion and the output layers (blue). Consistent performance gains are obtained from imposing the proposed regularizations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Table1compares our approach, namely DNN-Fusion Regularization (DNN-FR), with the alternative feature fusion methods. As seen in the table, our approach achieves the best performance with clear gains over Performance comparison (mAP) on the three datasets, using approaches that only explore the inter-feature relationships.</figDesc><table><row><cell cols="2">Approaches Hollywood2</cell><cell>CCV</cell><cell>CCV+</cell></row><row><cell>NN-EF</cell><cell>62.0%</cell><cell>66.7%</cell><cell>70.5%</cell></row><row><cell>NN-LF</cell><cell>58.5%</cell><cell>61.9%</cell><cell>64.7%</cell></row><row><cell>SVM-EF</cell><cell>62.6%</cell><cell>67.5%</cell><cell>70.0%</cell></row><row><cell>SVM-LF</cell><cell>62.1%</cell><cell>64.9%</cell><cell>68.5%</cell></row><row><cell>M-DBM[42]</cell><cell>61.5%</cell><cell>67.2%</cell><cell>70.1%</cell></row><row><cell>DNN-FR</cell><cell>64.5%</cell><cell>69.1%</cell><cell>71.8%</cell></row><row><cell cols="2">Approaches Hollywood2</cell><cell>CCV</cell><cell>CCV+</cell></row><row><cell>DMF [40]</cell><cell>61.8%</cell><cell>67.6%</cell><cell>68.5%</cell></row><row><cell>DASD [19]</cell><cell>60.9%</cell><cell>66.8%</cell><cell>70.2%</cell></row><row><cell>DNN-CR</cell><cell>63.0%</cell><cell cols="2">69.3% 72.1%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison (mAP) on the three datasets, using approaches that only explore the inter-class relationships.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison with state-of-the-art results in the literature. Our method (Regularized DNN) achieves to-date the highest mAP on both the Hol-lywood2 and the CCV datasets.</figDesc><table><row><cell>Approaches</cell><cell>Training Time (s)</cell></row><row><cell>NN-EF</cell><cell>1.068±0.021</cell></row><row><cell>NN-LF</cell><cell>0.782±0.007</cell></row><row><cell>Regularized DNN</cell><cell>0.640±0.002</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Training time per epoch (seconds) of the neural network based approaches on the Hollywood2 dataset.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work was supported in part by a National 863 Program (#2014AA015101), a National Key Technologies Research and Development Program (#2013BAH09F01), a grant from NSF China (#61201387), and three grants from the Science and Technology Commission of Shanghai Municipality (#13PJ1400400, #13511504503, #12511501602).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The AXES submissions at TrecVid 2013</title>
		<author>
			<persName><forename type="first">R</forename><surname>Aly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIST TRECVID Workshop</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Three things everyone should know to improve object retrieval</title>
		<author>
			<persName><forename type="first">R</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Convex multi-task feature learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Argyriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Evgeniou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multimodal fusion for multimedia analysis: a survey</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Atrey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">El</forename><surname>Saddik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia systems</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multiple kernel learning, conic duality, and the smo algorithm</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Lanckriet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Using web co-occurrence statistics for improving image categorization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.5697</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Heterogeneous feature machines for visual recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Integrating low-rank and group-sparse structures for robust multi-task learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Subband autocorrelation features for video soundtrack classification</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">V</forename><surname>Cotton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Ellis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Structured feature selection and task relationship inference for multi-task learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge and information systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-task learning for stock selection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ghosn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Clustered multi-task learning: A convex formulation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-P</forename><surname>Vert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Better exploiting motion for better action recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bouthemy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Discovering joint audio-visual codewords for video event detection</title>
		<author>
			<persName><forename type="first">I.-H</forename><surname>Jhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Machine Vision and Applications</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Short-term audio-visual atoms for generic video concept classification</title>
		<author>
			<persName><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cotton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Loui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Audio-visual grouplet: temporal audio-visual interactions for general video concept classification</title>
		<author>
			<persName><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Loui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fast semantic diffusion for large-scale context-based image and video annotation</title>
		<author>
			<persName><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Consumer video understanding: A benchmark database and an evaluation of human and machine performance</title>
		<author>
			<persName><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Loui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM ICMR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning with whom to share in multi-task feature learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Cmu-informedia@ trecvid 2013 multimedia event detection</title>
		<author>
			<persName><forename type="first">Z.-Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-I</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIST TRECVID Workshop</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning realistic human actions from movies</title>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rozenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Sample-specific late fusion for visual category recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-T</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multi-task feature learning via efficient l 2, 1-norm minimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Classification using intersection kernel support vector machines is efficient</title>
		<author>
			<persName><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dynamic eye movement datasets and learnt saliency models for visual action recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mathe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Few-example video event retrieval using tag propagation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mazloom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM ICMR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Bbn viser trecvid 2012 multimedia event detection and multimedia event recounting systems</title>
		<author>
			<persName><forename type="first">P</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vazquez-Reina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Vitaladevuni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIST TRECVID Workshop</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multimodal feature fusion for robust event detection in web videos</title>
		<author>
			<persName><forename type="first">P</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vitaladevuni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tsakalidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Prasad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multimodal deep learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A normalization model of multisensory integration</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ohshiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Angelaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Deangelis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Neuroscience</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Action and event recognition with fisher vectors on a compact feature set</title>
		<author>
			<persName><forename type="first">D</forename><surname>Oneata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multiple task learning using iteratively reweighted least square</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Correlative multi-label video annotation</title>
		<author>
			<persName><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Objects in context</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Galleguillos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wiewiora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Image classification with the fisher vector: Theory and practice</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep Fisher networks for large-scale image classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multimedia semantic indexing using model vectors</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Naphade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Natsev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Early versus late fusion in semantic video analysis</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Multimodal learning with deep boltzmann machines</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Multisensory integration: current issues from the perspective of the single neuron</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Stanford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Neuroscience</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Contextual priming for object detection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Multiple kernels for object detection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gulshan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Cross-domain multicue fusion for concept-based video indexing</title>
		<author>
			<persName><forename type="first">M.-F</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-Y</forename><surname>Chuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Feature weighting via optimal thresholding for video analysis</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Large-scale multimedia semantic concept modeling using robust subspace bagging and mapreduce</title>
		<author>
			<persName><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-O</forename><surname>Fleury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Merler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Workshop on Large-scale Multimedia Retrieval and Mining</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Robust late fusion with rank minimization</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I.-H</forename><surname>Jhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Multi-modal multi-task learning for joint prediction of multiple regression and classification variables in alzheimer&apos;s disease</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A convex formulation for learning task relationships in multi-task learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A multi-task learning formulation for predicting disease progression</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A novel audio fingerprinting method robust to time scale modification and pitch shifting</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
