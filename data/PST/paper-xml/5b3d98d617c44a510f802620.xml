<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Structurally-sensitive Multi-scale Deep Neural Network for Low-Dose CT Denoising</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chenyu</forename><surname>You</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Qingsong</forename><surname>Yang</surname></persName>
							<email>yangq4@rpi.edu</email>
						</author>
						<author>
							<persName><forename type="first">Hongming</forename><surname>Shan</surname></persName>
							<email>shanh@rpi.edu</email>
						</author>
						<author>
							<persName><forename type="first">Lars</forename><surname>Gjesteby</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Guang</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Shenghong</forename><surname>Ju</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhuiyang</forename><surname>Zhang</surname></persName>
							<email>yzhang@scu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Zhen</forename><surname>Zhao</surname></persName>
							<email>zhaozhen8810@126.com</email>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Yi</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Wenxiang</forename><surname>Cong</surname></persName>
							<email>congw@rpi.edu</email>
						</author>
						<author>
							<persName><forename type="first">Ge</forename><surname>Wang</surname></persName>
							<email>wangg6@rpi.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Departments of Bioengineering and Electrical Engineering</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Biomedical Engineering</orgName>
								<orgName type="institution">Rensselaer Polytechnic Institute</orgName>
								<address>
									<postCode>12180</postCode>
									<settlement>Troy</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Department of Radiology</orgName>
								<orgName type="department" key="dep2">Medical School</orgName>
								<orgName type="laboratory">Jiangsu Key Laboratory of Molecular and Func-tional Imaging</orgName>
								<orgName type="institution">Zhongda Hospital</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Southeast University</orgName>
								<address>
									<postCode>210009</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="institution">Sichuan University</orgName>
								<address>
									<postCode>610065</postCode>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">Department of Radiology</orgName>
								<orgName type="institution">Wuxi No</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution">People&apos;s Hospital</orgName>
								<address>
									<postCode>214000</postCode>
									<settlement>Wuxi</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Structurally-sensitive Multi-scale Deep Neural Network for Low-Dose CT Denoising</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9A0C993AC15841433942B0F3288F21D2</idno>
					<idno type="DOI">10.1109/ACCESS.2018.2858196</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T07:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Machine Leaning</term>
					<term>Low dose CT</term>
					<term>Image denoising</term>
					<term>Deep learning</term>
					<term>Loss Function</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Computed tomography (CT) is a popular medical imaging modality and enjoys wide clinical applications. At the same time, the x-ray radiation dose associated with CT scannings raises a public concern due to its potential risks to the patients. Over the past years, major efforts have been dedicated to the development of Low-Dose CT (LDCT) methods. However, the radiation dose reduction compromises the signal-to-noise ratio (SNR), leading to strong noise and artifacts that downgrade CT image quality. In this paper, we propose a novel 3D noise reduction method, called Structurally-sensitive Multi-scale Generative Adversarial Net (SMGAN), to improve the LDCT image quality. Specifically, we incorporate three-dimensional (3D) volumetric information to improve the image quality. Also, different loss functions for training denoising models are investigated. Experiments show that the proposed method can effectively preserve structural and textural information in reference to normal-dose CT (NDCT) images, and significantly suppress noise and artifacts. Qualitative visual assessments by three experienced radiologists demonstrate that the proposed method retrieves more information, and outperforms competing methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>X -RAY computed tomography (CT) is one of the most popular imaging modalities in clinical, industrial, and other applications <ref type="bibr" target="#b0">[1]</ref>. Nevertheless, the potential risks (i.e., a chance to induce cancer and cause genetic damage) of ionizing radiation associated with medical CT scans cause a public concern <ref type="bibr" target="#b1">[2]</ref>. Studies from the National Council on Radiation Protection and Measurements (NCRP) demonstrate a 600% increase in medical radiation dose to the US population from 1980 to 2006, showing both great successes of the CT technology and an elevated alert to patients <ref type="bibr" target="#b2">[3]</ref>.</p><p>The main drawback of radiation dose reduction is to increase the image background noise, which could severely compromise diagnostic information. How to minimize the exposure to ionizing radiation while maintaining diagnostic utility of low-dose CT (LDCT) has been a challenge for researchers, who follows the well-known ALARA (as low as reasonably achievable) guideline <ref type="bibr" target="#b0">[1]</ref>. Numerous methods were designed for LDCT noise reduction. These methods can be categorized as follows: <ref type="bibr" target="#b0">(1)</ref> Sinogram filtering-based techniques <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b8">[9]</ref>: these methods directly process projection data in the projection domain <ref type="bibr" target="#b5">[6]</ref>. The main advantage of these methods is computational efficiency. However, they may result in loss of structural information and spatial resolution <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b9">[10]</ref>; <ref type="bibr" target="#b1">(2)</ref> Iterative reconstruction (IR) <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b19">[20]</ref>: IR techniques may potentially produce high signal-to-noise ratio (SNR). However, these methods require a substantial computational cost and troublesome parametric turning; (3) Image space denoising techniques <ref type="bibr" target="#b19">[20]</ref>- <ref type="bibr" target="#b26">[27]</ref>: these techniques can be performed directly on reconstructed images so that they can be applied across various CT scanners at a very low cost. Examples are non-local means-based filters <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b20">[21]</ref>, dictionary-learning-based K-singular value decomposition (K-SVD) method <ref type="bibr" target="#b19">[20]</ref> and the block-matching 3D (BM3D) algorithms <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>. Even though these algorithms greatly suppress noise and artifacts, edge blurring or resolution loss may persist in processed LDCT images.</p><p>Deep learning (DL) has recently received a tremendous attention in the field of medical imaging <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, such as brain image segmentation <ref type="bibr" target="#b29">[30]</ref>, image registration <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, image classification <ref type="bibr" target="#b32">[33]</ref>, and LDCT noise reduction <ref type="bibr" target="#b33">[34]</ref>- <ref type="bibr" target="#b39">[40]</ref>. For example, Chen et al. <ref type="bibr" target="#b34">[35]</ref> proposed a Residual Encoder-Decoder Convolutional Neural Network (REN-CNN) to predict NDCT images from noisy LDCT images. This method greatly reduces the background noise and artifacts. However, a limitation is that the results look blurry sometimes since the method targets minimizing the mean-squared error between the generated LDCT and corresponding NDCT images. To cope with this problem, the generative adversarial network (GAN) <ref type="bibr" target="#b40">[41]</ref> offers an attractive solution. In the GAN, the generator G learns to capture a real data distribution P r while the discriminator D attempts to discriminate between the synthetic data distribution and the real counterpart. Note that the loss used in GAN, called the adversarial loss, measures the distance between the synthetic data distribution and the real one in order to improve the performance of G and D simultaneously. Originally, GAN uses the Jensen-Shannon (JS) divergence to evaluate the similarity of the two data distributions <ref type="bibr" target="#b40">[41]</ref>. However, several problems exist in training GAN, such as unstable training and non-convergence. To address these issues, Arjovsky et al. introduced the Wasserstein distance instead of the Jensen-Shannon divergence to improve the neural network training <ref type="bibr" target="#b41">[42]</ref>. We will discuss more details on this aspect in Section II-D3.</p><p>In our previous work <ref type="bibr" target="#b36">[37]</ref>, we first introduced the perceptual loss to capture perceptual differences between denoised LDCT images and the reference NDCT images, providing the perceptually better results for clinical diagnosis at a cost of low scores in traditional image quality metrics. Since the traditional image quality metrics evaluate the generated images with reference to the gold-standard in generic ways, minimizing the perceptual loss does not ensure the results optimal in terms of the traditional image quality metrics. To address this discrepancy and inspired by the work in <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b42">[43]</ref>, here we propose a novel 3D clinical Structurally-sensitive Multi-scale Generative Adversarial Network (SMGAN) to capture subtle structural features while maintaining high visual sensitivity. The proposed structurally-sensitive loss leverages a combination of adversarial loss <ref type="bibr" target="#b41">[42]</ref>, perceptually-favorable structural loss, and pixel-wise L 1 loss. Moreover, to validate the diagnostic quality of images processed by our method, we report qualitative image assessments by three expert radiologists. Systematically, we demonstrate the feasibility and merits of mapping LDCT images to corresponding NDCT images in the GAN framework.</p><p>Our main contributions in this paper are summarized as follows:</p><p>1) To keep the underlying structural information in LDCT images, we adopt a 3D CNN model as a generator based on WGAN which can enhance the image quality for better diagnosis. 2) To measure the structural difference between generated LDCT images and the NDCT gold-standard, a structurally-sensitive loss is used to enhance the accuracy and robustness of the algorithm. Different from <ref type="bibr" target="#b36">[37]</ref>, we replace the perceptual loss with a combination of L 1 loss and structural loss. 3) To compare the performance of the 2D and the 3D models, we perform an extensive evaluation on their convergence rate and denoising performance. This paper is organized as follows: Section II introduces the proposed approach and analyzes the impact of each component loss function on the image quality. Section III presents the experimental design and results. Section IV discusses relevant issues. Finally, the concluding remarks and future plans are given in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Problem Inversion</head><p>Assuming that y ∈ R H×W ×D denotes the original LDCT image, and x ∈ R H×W ×D denotes the corresponding NDCT image, the relationship between them can be expressed as:</p><formula xml:id="formula_0">y = T (x) +<label>(1)</label></formula><p>where T : R H×W ×D → R H×W ×D is a generic noising process that degrades a real sample x of NDCT to a corresponding LDCT sample y in a non-linear way. stands for the additive noise and unmodeled factors, and H, W , D are height, width and depth respectively. From another standpoint, considering that the real NDCT distribution P r is unknown, we focus on extracting information to recover desired images x from the noisy LDCT images y. In general, the noise distribution in CT images is regarded as the mixture of Poisson quantum noise and Gaussian electronic noise <ref type="bibr" target="#b43">[44]</ref>. Compared with traditional denoising methods, the DL-based method is capable of effectively modeling any type of data distributions since the DL-based denoising model itself can be easily adapted to any practical noise model with statistical properties of typical noise distributions in a combination. Therefore, the proposed DL-based denoising network is to solve the inverse problem T † ≈ T 1 to retrieve feasible images x, and the solution can be expressed as:</p><formula xml:id="formula_1">T † y = x ≈ x<label>(2)</label></formula><p>As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, the overall network comprises three parts. Part 1 is the generator G, part 2 is the Structurally-Sensitive loss (SSL) function, and part 3 is the discriminator D. G maps a volumetric LDCT image to the NDCT feature space, thereby estimating a NDCT image. The SSL function computes the structurally-sensitive dissimilarity which encodes multi-scale structural information. The loss computed by the SSL function aims to improve the ability of G to generate realistic results. D distinguishes a pair of synthetic and real NDCT images. If D can identify the input image as "synthetic" or "real" correctly and tell us the discrepancy between the estimated CT image and the corresponding real NDCT image, we will know if G yields a high-quality estimation or not. With the indication from D, G can optimize its performance. Also, D can upgrade its ability as well. Hence, G and D are in competition: G attempts to generate a convincing estimate to an NDCT image while D aims to distinguish the estimated image from real NDCT images. See Sections II-C and II-D for more details. For your convenience, the summary of notations that we use in this paper is in Table <ref type="table">V</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. 3D Spatial Information</head><p>The advantages of using 3D spatial information are evident. Hence, volumetric imaging and 3D visualization have become standards in diagnostic radiology <ref type="bibr" target="#b44">[45]</ref>. There is a large amount of 3D NDCT and LDCT volumetric images available in practice. However, most of the networks are of 2D-based architecture. With a 3D network architecture, adjacent crosssection slices from a 3D CT image volume exhibit strong spatial correlation which we can utilize to preserve more information than with 2D models.</p><p>As mentioned above, here we use a 3D ConvNet as the generator and introduce a 3D Structurally-Sensitive loss (SSL) function. Accordingly, we extract 3D image patches and use a 3D filter instead of a 2D filter. The generator in our network takes 3D volumetric LDCT patches as the input and process them with 3D non-linear transform operations. For convenience and comparison, 2D and 3D denoising networks are referred to as SMGAN-2D and SMGAN-3D respectively. The details of the network architecture are in the following Section II-C. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Network Structure</head><p>Inspired by the studies in <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, we introduce our proposed SMGAN-3D network structure. First, in Section II-C1 we present the 3D generator G which captures local anatomical features. Then, in Section II-C2 we define the 3D SSL function which guides the learning process . Finally, we outline the 2.5D discriminator D in Section II-C3.</p><p>1) 3D CNN Generator: The generator G consists of eight 3D convolutional (Conv) layers. The first 7 layers each has 32 filters, and the last layer has only 1 filter. The odd-numbered convolutional layers apply 3 × 3 × 1 filters, while the evennumbered convolutional layers use 3 × 3 × 3 filters. The size of the extracted 3D patches is 80 × 80 × 11 as the input to our whole network; see Fig. <ref type="figure" target="#fig_0">1</ref>. Note that the variable n denotes the number of the filters and s denotes the stride size, which is the step size of the filer when moving across an image so that n32s1 stands for 32 feature maps with a unit stride. Furthermore, a pooling layer after each Conv layer may lead to loss of subtle textural and structural information. Therefore, the pooling layer is not applied in this network. The Rectified Linear Unit (ReLU) <ref type="bibr" target="#b45">[46]</ref> is our activation function after each Conv layer.</p><p>2) Structurally-Sensitive Loss (SSL) Function: The proposed 3D SSL function measures the patch-wise discrepancy between a 3D output from the 3D ConvNet and the 3D NDCT image in the spatial domain. This measure is back-propagated <ref type="bibr" target="#b46">[47]</ref> through the neural network to update the parameters of the network; see Section II-D for more details.</p><p>3) Discriminator: The discriminator D consists of six convolutional layers with 64, 64, 128, 128, 256, and 256 filters and the kernel size of 3 × 3. Two fully-connected (FC) layers produce 1024 and 1 feature maps respectively. Each layer is followed by a leaky ReLU defined as max(0, x)α max(0, -x) <ref type="bibr" target="#b45">[46]</ref>, where α is a small constant. A stride of one pixel is applied for odd-numbered Conv layers and a stride of two pixels for even-numbered Conv layers. The input fed to D is of the size 64 × 64 × 3, which comes from the output of G. The reason why we use a 2D filter in D is to reduce the computational complexity. Since the adversarial loss between each two adjacent slices in one volumetric patch contribute equally to the weighted average in one iteration, it can be easily computed. Following the suggestion in <ref type="bibr" target="#b41">[42]</ref>, we do not use the sigmoid cross entropy layer in D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Loss Functions for Noise Reduction</head><p>In this sub-section, we evaluate the impact of different loss functions on LDCT noise reduction. This justifies the use of a hybrid loss function for optimal diagnostic quality.</p><p>1) L 2 loss: The L 2 loss can efficiently suppress the background noise, but it could make the denoised results unnatural and blurry. This is expected due to its regressionto-mean nature <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b47">[48]</ref>. Furthermore, the L 2 loss assumes that background noise is white Gaussian noise, which is independent of local image features <ref type="bibr" target="#b48">[49]</ref> and not desirable for LDCT imaging.</p><p>The formula of L 2 loss is expressed as:</p><formula xml:id="formula_2">L 2 = 1 HW D ||G(y) -x|| 2 2<label>(3)</label></formula><p>where H, W , D stand for the height, width, and depth of a 3D image patch respectively, x denotes the gold-standard (NDCT), and G(y) represents the generated result from the source (LDCT) image y. It is worth noting that since the L 2 loss has appealing properties of differentiability, convexity, and symmetry, the mean squared error (MSE) or L 2 loss is still a popular choice in denoising tasks <ref type="bibr" target="#b49">[50]</ref>.</p><p>2) L 1 Loss: The L 1 and L 2 losses are both the mean-based measures, the impacts of these two loss functions are different on denoising results. Compared with the L 2 loss, the L 1 loss does not over-penalize large differences or tolerate small errors between denoised and gold-standard images. Thus, the L 1 loss can alleviate some drawbacks of the L 2 loss we mentioned earlier. Additionally, the L 1 loss enjoys the same fine characteristics as L 2 loss except for the differentiability.</p><p>The formula for the L 1 loss is written as:</p><formula xml:id="formula_3">L 1 = 1 HW D |G(y) -x|<label>(4)</label></formula><p>As shown in Figs. <ref type="figure" target="#fig_2">3</ref><ref type="figure" target="#fig_3">4</ref><ref type="figure" target="#fig_4">5</ref><ref type="figure" target="#fig_5">6</ref>, compared with the L 2 loss, the L 1 loss suppresses blurring, but does not help reduce blocky artifacts.</p><p>For more details, see Section III.</p><p>3) Adversarial Loss: The Wasserstein distance with the regularization term was proposed in <ref type="bibr" target="#b47">[48]</ref>, which is formulated as</p><formula xml:id="formula_4">L adv = -E[D(x)] + E[D(z)] + λE[(||∇ xD( x)|| 2 -1) 2 ] (5)</formula><p>where the first two terms are for the Wasserstein distance, and the third term implements the gradient penalty. Note that z denotes G(y) for brevity. x is uniformly sampled along the straight line between a pair of points sampled from G and corresponding NDCT images.</p><p>4) Structural Loss: Medical images contain strong feature correlations. For example, their voxels have strong interdependencies. The structural similarity index (SSIM) <ref type="bibr" target="#b48">[49]</ref> and the multi-scale structural similarity index (MS-SSIM) <ref type="bibr" target="#b50">[51]</ref> are perceptually motivated metrics, and perform better in visual pattern recognition than mean-based metrics <ref type="bibr" target="#b48">[49]</ref>. To measure the structural and perceptual similarity between two images, the SSIM <ref type="bibr" target="#b48">[49]</ref> is formulated as follows:</p><formula xml:id="formula_5">SSIM (x, z) = 2µ x µ z + C 1 µ 2 x + µ 2 z + C 1 * 2σ xz + C 2 σ 2 x + σ 2 z + C 2 (6) = l(x, z) * cs(x, z)<label>(7)</label></formula><p>where C 1 , C 2 are constants and µ x , µ z , σ x , σ z , σ xz denote means, standard deviations and cross-covariance of the image pair (x, z) from G and the corresponding NDCT image respectively. l(x, z), cs(x, z) are the first term and second factor we defined in Eqn. 6.</p><p>The multiscale SSIM provides more flexibility for multiscale analysis <ref type="bibr" target="#b50">[51]</ref>. The formula for MS-SSIM <ref type="bibr" target="#b50">[51]</ref> is expressed as:</p><formula xml:id="formula_6">M S SSIM (x, z) = M j=1 SSIM (x j , z j )<label>(8)</label></formula><p>where x j , z j are the local image content at the j th level, and M is the number of scale levels. Clearly, SSIM is a special case of MS-SSIM.</p><p>The formula for the structural loss (SL) is generally expressed as:</p><formula xml:id="formula_7">L SL = 1 -M S SSIM (x, z)<label>(9)</label></formula><p>Note that the loss can be easily back-propagated to update weights in the network, since it can be differentiated <ref type="bibr" target="#b42">[43]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5) Objective Function:</head><p>As mentioned in the recent studies <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b42">[43]</ref>, minimizing the L 2 loss leads to over-smoothed appearance. The adversarial loss in GAN may yield sharp images, but it does not exactly match the corresponding real NDCT images <ref type="bibr" target="#b36">[37]</ref>. The perceptual loss computed by a VGG network <ref type="bibr" target="#b46">[47]</ref> evaluates the perceptual differences between the generated images and real NDCT images in a high-level feature space instead of the voxel space. Since the VGG network is trained on a large dataset of natural images, not CT images, it may result in distortions of processed CT images. To tackle these issues, we propose to utilize different loss terms together for high image quality.</p><p>As revealed in <ref type="bibr" target="#b42">[43]</ref>, the L 1 loss allows noise suppression and SNR improvement. However, it blurs anatomical structures to some extent. In contrast, the structural loss discourages blurring and keeps high contrast resolution. To have the merits of both loss functions, the structural sensitive loss (SSL) is expressed as:</p><formula xml:id="formula_8">L SSL = τ × L SL + (1 -τ ) × L 1 (<label>10</label></formula><formula xml:id="formula_9">)</formula><p>where τ is the weighting factor to balance between structure preservation in the first term (from Eq. 9) and noise suppression in the second term (from Eq. 4). Nevertheless, the above-mentioned two losses may still miss some diagnostic features. Hence, the adversarial loss is incorporated to keep textural and structural features as much as possible. In summary, the overall objective function of SMGAN is expressed as:</p><formula xml:id="formula_10">L obj = L SSL + β × L adv (<label>11</label></formula><formula xml:id="formula_11">)</formula><p>where β is the weight for the adversarial loss. In the last step of the network, we compare the difference between the output volume and the target volume, and then the error can be backpropagated for optimization <ref type="bibr" target="#b51">[52]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENTS AND RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Datasets and Setup</head><p>To show the effectiveness of the proposed network for LDCT noise reduction, we used a real clinical dataset, published by Mayo Clinic for the 2016 NIH-AAPM-Mayo Clinic Low Dose CT Grand Challenge <ref type="bibr" target="#b52">[53]</ref>. The Mayo dataset consists of 2,378 normal dose CT (NDCT) and low dose (quarter dose) CT (LDCT) images from 10 anonymous patients. The reconstruction interval and slice thickness in the dataset were 0.8mm and 1.0mm respectively.</p><p>For limited data, the denoising performance of DL-based methods depends on the size of the training datasets, so large-scale valid training datasets can improve the denoising performance. However, it is worth noting that the training image library may not contain many valid images. To enhance the performance of the network, the strategies we utilized are as follows. First of all, in order to improve generalization performance of the network and avoid over-fitting, we adopted the "10-fold cross validation" strategy. The original dataset was partitioned into 10 equal size subsets. Then, a single subset was used in turn as the validation subset and the rest of data were utilized for training. Moreover, considering the limited number of CT images, we applied the overlapping patches strategy because it can not only consider patch-wise spatial interconnections, but also significantly increase the size of the training patch dataset <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b54">[55]</ref>.</p><p>For data preprocessing, the original LDCT and NDCT images are of 512 × 512 pixels. Since directly processing the entire patient images is computationally inefficient and infeasible, our denoising model was applied to image patches. First, we applied the overlapped sliding window with a sliding size of 1 × 1 × 1 to obtain image patches and then randomly extracted 100,100 pairs of training patches and 5,100 pairs for validation from remaining patient images of the same size 80 × 80 × 11. Then, the "10-fold cross validation" strategy is used to ensure the accuracy of the proposed algorithm. Next, the CT Hounsfield Unit (HU) scale was normalized to [0, 1] before the images were fed to the network.</p><p>For qualitative comparison, in order to validate the performance of our proposed methods (SMGAN-2D and SMGAN-3D), we compare them with eight state-of-the-art denoising methods, including CNN-L2 (L 2 -net), CNN-L1 (L 1net), structural-loss net (SL-net), multi-scale structural-loss net (MSL-net), WGAN, BM3D <ref type="bibr" target="#b24">[25]</ref>, RED-CNN <ref type="bibr" target="#b34">[35]</ref>, and WGAN-VGG <ref type="bibr" target="#b36">[37]</ref>. Among these existing denoising methods, BM3D is a classical image space denoising algorithm. WGAN-VGG represents a 2D perceptual-loss-based network, and RED-CNN refers to a 2D pixel-wise network. Note that the parameter settings in these methods <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b36">[37]</ref> had been followed per the suggestions from the original papers.</p><p>For quantitative comparison, to evaluate the effectiveness of the proposed methods, three metrics were chosen to perform image quality evaluation, including peak signal-to-noise ratio (PSNR), structural similarity index (SSIM) <ref type="bibr" target="#b50">[51]</ref>, and rootmean-square error (RMSE).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Parameter Selection</head><p>In our experiments, the Adam optimization algorithm was implemented for our network training <ref type="bibr" target="#b55">[56]</ref>. In the training phase, the mini-batch size was 64. The hyperparameter λ for the balance between the Wasserstein distance and gradient penalty was set 10, per the suggestion from the original paper <ref type="bibr" target="#b41">[42]</ref>. The parameter β for the trade-off between adversarial loss and mixture loss was set be 10 -3 . The parameter τ was set to 0.89. The slope of the leaky ReLu activation function was set to 0.2. The networks are implemented in the TensorFlow <ref type="bibr" target="#b56">[57]</ref> on an NVIDIA Titan Xp GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Network Convergence</head><p>To examine the robustness of different denoising algorithms, ten methods corresponding to the L 1 loss (L 1 ), structural loss (SL), and Wasserstein distance were separately trained in the same settings as that for SMGAN-3D. Note that the parameter settings of RED-CNN, WGAN-VGG, and BM3D from the original papers had been followed <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b36">[37]</ref>. In addition, the size of the input patches of the 2D network is 80 × 80 while our proposed 3D model uses training patches with the size of 80 × 80 × 11. We calculated the averaged loss value achieved by different methods versus the number of epochs as the measure of convergence in Fig. <ref type="figure" target="#fig_1">2</ref>. In Fig. <ref type="figure" target="#fig_1">2a</ref> and<ref type="figure" target="#fig_1">2b</ref>, in terms of L 1 and SL, we observe As shown in Fig. <ref type="figure" target="#fig_1">2c</ref>, we can evaluate the convergence performance of WGAN. It can be seen that our proposed SMGAN-2D has the mildest oscillatory behavior compared with the other three models and reaches a stable state after the 13 th epoch. Moreover, the SMGAN-3D oscillates in a relatively large range in the training process. This is because our proposed SMGAN-3D considers 3D structural information which results in a relatively larger vibrating amplitude in the training process. However, the curve still oscillates close to </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Denoising Performance</head><p>To demonstrate the effectiveness of the proposed network, we perform the qualitative comparisons over three representative abdominal images presented in Figs. <ref type="figure" target="#fig_4">3, 5</ref> and<ref type="figure" target="#fig_6">7</ref>. For better evaluations of the image quality with different denoising models, zoomed regions-of-interest (ROIs) are marked by red rectangles and shown in Figs. <ref type="figure" target="#fig_5">4, 6</ref> and<ref type="figure" target="#fig_7">8</ref> respectively. Note that all results from different denoising models focus on two aspects: content restoration and noise-reduction. All CT images in axial view are displayed in the angiography window [-160, 240]HU.</p><p>The real NDCT images and corresponding LDCT images are presented in Figs. <ref type="figure" target="#fig_2">3a</ref> and<ref type="figure" target="#fig_2">3b</ref>. As observed, there are distinctions between ground truth (NDCT) images and LDCT images. Figs. <ref type="figure" target="#fig_2">3a</ref> and<ref type="figure" target="#fig_6">7a</ref> show the lesions/metastasis. Fig. <ref type="figure" target="#fig_4">5a</ref> presents focal fatty sparing/focal fat. In Figs. 4a, 6a and 8a, these lesions can be clearly observed in NDCT images; in contrast, from Figs. <ref type="figure" target="#fig_3">4b,</ref><ref type="figure" target="#fig_5">6b</ref>, and 8b, it can be seen that the original LDCT image is noisy, and lacks structural features for task-based clinical diagnosis. All adopted denoising models suppress noise to some extent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Comparison with CNN-based denoising methods:</head><p>To study the robustness of the adversarial learning framework in SMGAN-3D, we compared SMGAN-3D with the CNN-based methods, including CNN-L2, CNN-L1, RED-CNN <ref type="bibr" target="#b34">[35]</ref>, SLnet and MSL-net. It is worth noting that CNN-L2, CNN-L1, and RED-CNN are mean-based denoising methods, and SLnet and MSL-net are SL-based denoising methods. All of the methods greatly reduce the noise compared with LDCT images. Our proposed method preserves more structural details, thereby yielding better image quality, compared with the other five methods.</p><p>Mean-based methods can effectively reduce noise, but the side effect is impaired image contents. In Fig. <ref type="figure" target="#fig_2">3c</ref>, L 2 -net greatly suppresses the noise, but blurs some crucial structural information in the porta hepatis region. Meanwhile, some waxy artifacts can still be observed in Fig. <ref type="figure" target="#fig_5">6c</ref>. L 2 -net does not produce good visual quality because it assumes that the noise is independent of local characteristics of the images. Even though it retains high SNR, its results are not clinically preferable. Compared with L 2 -net, in Figs. <ref type="figure" target="#fig_2">3d</ref> and<ref type="figure" target="#fig_4">5d</ref>, it can been seen that L 1 -net encourages less blurring and preserves more structural information. However, as observed in Fig. <ref type="figure" target="#fig_3">4d</ref>, it still over-smooths some anatomical details. Meanwhile, in Fig. <ref type="figure" target="#fig_5">6d</ref>, there are some blocky effects marked by the blue arrow. The results obtained by RED-CNN <ref type="bibr" target="#b34">[35]</ref> deliver high SNR but blur the vessel details as shown in Figs. <ref type="figure" target="#fig_3">4i</ref> and<ref type="figure" target="#fig_5">6i</ref>.</p><p>For SL-based methods, as observed in Figs. <ref type="figure" target="#fig_2">3e</ref> and<ref type="figure" target="#fig_4">5e</ref>, SL-net generates images with higher contrast resolution and preserves texture of real NDCT images better than L 2 -net and L 1 -net. However, Figs. <ref type="figure" target="#fig_3">4e</ref> and<ref type="figure" target="#fig_5">6e</ref> show that SL-net does not preserve the structural features well, and there still remain small streak artifacts. Subsequently, in Figs. <ref type="figure" target="#fig_3">4e</ref> and<ref type="figure" target="#fig_3">4f</ref>, SLnet and MSL-net have low frequency image intensity variance because SSIM/MS-SSIM is insensitive to uniform biases <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b50">[51]</ref>. On the other hand, L 1 -net preserves the overall image intensity, but it does not preserve high contrast resolution well as SL-net and MSL-net do.</p><p>From Figs. <ref type="figure" target="#fig_6">7</ref> and<ref type="figure" target="#fig_7">8</ref>, we can see mean-based and SLbased methods work well with effective noise suppression and artifact removal. However, the illustrations in Fig. <ref type="figure" target="#fig_7">8</ref> show that these methods blur the local strutural features. Our proposed SMGAN-based methods present a better edge preservation than the competing methods.</p><p>Overall, the observations above support the following statements. First, although the voxel-wise methods show good noise-reduction properties, to some extent they blur the contents and lead to the loss of structural details because they optimize the results in the voxel-wise manner. Second, SLbased methods better preserve texture than mean-based methods, but they cannot preserve overall image intensity. Third, the results produced by the proposed SMGAN-3D demonstrate the benefits of the combination of two loss functions and the importance of the adversarial training <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>.</p><p>2) Comparison with WGAN-based denoising methods: To evaluate the effectiveness of our proposed objective function, we compare our method with existing WGAN-based networks, including WGAN and WGAN-VGG. Considering the importance of clinical image quality and specific structural features   for medical diagnosis, we adopted the adversarial learning method <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref> in our experiments because WGAN could help to capture more structural information. Nevertheless, based on our prior experience, utilizing WGAN alone may yield stronger noise than other selected approaches, because it only maps the data distribution from LDCT to NDCT without consideration of local voxel intensity and structural correlations. The observations demonstrate that the noise texture is coarse in the images, as shown in Fig. <ref type="figure" target="#fig_3">4g</ref> and Fig. <ref type="figure" target="#fig_7">8g</ref>, which support our intuition. Indeed, the images of WGAN-VGG <ref type="bibr" target="#b36">[37]</ref>, as shown in Fig. <ref type="figure" target="#fig_2">3j</ref>, exhibit better visual quality with respect to more details and share structural details similar to NDCT images according to human perceptual evaluations. However, Figs. 4j (marked by the red circle) and 6j (marked by the green circle) suggest that it may severely distort the original structural information. A possible reason is that the VGG network <ref type="bibr" target="#b46">[47]</ref> is a pre-trained deep CNN network based on natural images, and the structural information and contents of natural images are different from medical images.</p><p>Compared with WGAN and WGAN-VGG, our proposed SMGAN-3D, as shown in Figs. 4l (marked by the red circle) and 6l (marked by the green circle), can more clearly visualize the metastasis and better preserve of the portal vein.</p><p>In Figs. <ref type="figure" target="#fig_6">7</ref> and<ref type="figure" target="#fig_7">8</ref>, it can be found that the SMGAN-based methods can achieve better anatomical feature preservations and visual quality than other state-of-the-art methods.</p><p>The experimental results demonstrate that our proposed objective function is essential to capture more accurate anatomical details.</p><p>3) Comparison with Image space denoising: To validate the robustness of DL-based methods, we compared our method with the image space denoising method. Figs. <ref type="figure" target="#fig_3">4h</ref> and<ref type="figure" target="#fig_5">6h</ref> show that BM3D blurs the low-contrast lesion marked by the red circle and smooths specific features marked by the blue arrow. In contrast, SMGAN-3D exhibits better on the low-contrast lesion and yields sharper features as shown in Figs. <ref type="figure" target="#fig_3">4l</ref> and<ref type="figure" target="#fig_5">6l</ref>.</p><p>4) Comparison with 2D-based SMGAN network: In order to evaluate the 3D structural information, we compared SMGAN-3D with SMGAN-2D. As shown in Fig. <ref type="figure" target="#fig_3">4l</ref>, our proposed SMGAN-3D generated the results with better subtle details than SMGAN-2D and enjoys more similar statistical noise properties to the corresponding NDCT images. The reasons why SMGAN-3D outperforms SMGAN-2D are follows. First, SMGAN-3D incorporates 3D structural information to improve image quality. Second, SMGAN-2D takes input slice by slice, thus potentially leading to the loss of spatial correlation between adjacent slices.</p><p>Figs. 7 and 8 demonstrate that the SMGAN-3D can be used to provide improved anatomical feature preservation over other state-of-the-art methods.</p><p>In summary, we compared our proposed methods with existing methods, and it can be clearly observed that SMGAN-3D achieves robust performance in noise suppression, artifact removal, and texture preservation. Note that we recommend the reader to see ROIs (in Fig. <ref type="figure" target="#fig_3">4</ref> and<ref type="figure" target="#fig_5">6</ref>) or zoom in to better evaluate our results. To further validate the generalization ability of our proposed model, we conclude more details in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Quantitative analysis</head><p>We performed the quantitative analysis with respect to three selected metrics (PNSR, SSIM, and RMSE). Then, we investigated the statistical properties of the denoised images for each noise-reduction algorithm. Furthermore, we performed a blind reader study with three radiologists on 10 groups of images. Note that quantitative full-size measurements are in Table <ref type="table" target="#tab_0">I</ref> and image quality assessments of ROIs are in Fig. <ref type="figure" target="#fig_8">9</ref>. The NDCT images are chosen as the gold-standard.</p><p>1) Image quality analysis: As shown in Table <ref type="table" target="#tab_0">I</ref>, RED-CNN scores the highest PSNR and RMSE, and ranks the second place in SSIM. Since the properties of PSNR and RMSE are regression to the mean, it is expected that RED-CNN, a meanbased regressiom optimization, has better performance than other feature-based models. For SL-net and MSL-net, it is not surprising that both models achieve the highest SSIM scores due to the adoption of structural similarity loss. However, a good score measured by image quality metrics does not ensure the preservation of high-level feature information and Although mean-based approaches, such as L 1 -net, L 2 -net, enjoy high metric scores, they may over-smooth the overall image contents and lose feature characteristics, which do not satisfy our HVS requirements because mean-based methods favor the regression toward the mean. Meanwhile, WGAN-VGG satisfies HVS requirements, but gets the lowest scores in the three selected metrics. The reason for the lowest scores is that WGAN-VGG may suffer from loss of subtle structural information or noise features, which may severely affect the diagnostic accuracy. The proposed SMGAN-2D outperforms the feature-based method WGAN-VGG with reference to the three metrics, illustrating the robust denoising capability of our proposed loss function. Compared with the SMGAN-2D model, SMGAN-3D achieves higher scores in PSNR and SSIM since it incorporates 3D spatial information. To further validate the performance of each denoising model with  respect to clinically significant local details, we performed the quantitative analysis over ROIs. The summary of the quantitative results from ROIs is shown in Fig. <ref type="figure" target="#fig_8">9</ref>. It is worth noting that the quantitative results of the ROIs follow a similar trend to that of the full-size images.</p><p>2) Statistical analysis: To quantitatively evaluate the statistical properties of processed images by different denoising models, we calculate the mean CT number (Hounsfield Unit) and standard deviations (SDs) of ROIs, as shown in Table <ref type="table" target="#tab_1">II</ref>. For each denoising model, the percent error of the mean and SD values were calculated in comparison to those of the reference (NDCT) images. The lower percent errors correspond to more robust denoising models. As shown in Table <ref type="table" target="#tab_1">II</ref>, L 1 -net, L 2 -net, SL-net, MSL-net, BM3D, RED-CNN, and WGAN-VGG generate high percent errors in SD with respect to the NDCT images. There are blocky and over-smoothing effects in the images which match our visual inspections. Specifically, for Fig. <ref type="figure" target="#fig_7">8</ref>, the absolute difference in SD between BM3D and NDCT is the largest among all of the denoising models, which indicates that BM3D has the most noticeable blurring effects. The standard deviation of BM3D supports our visual observations as shown in Figs. <ref type="figure" target="#fig_5">4h, 6h,</ref> and<ref type="figure" target="#fig_7">8h</ref>. The mean values of WGAN, WGAN-VGG, SL-net and SMGAN-2D deviated much from that of the NDCT image in Fig. <ref type="figure" target="#fig_3">4</ref>. This indicates that WGAN, WGAN-VGG, and SMGAN-2D effectively reduce the noise level but compromise significant content information. Nevertheless, the SD value of SMGAN-2D is close to that of NDCT, which indicates that it supports HVS requirements. From the quantitative analysis in Table <ref type="table" target="#tab_1">II</ref>, it can be observed that our proposed SMGAN-3D achieves the best matching SD to the NDCT images out of all other methods. Overall, SMGAN-3D is a highly competitive denoising model for clinical use.</p><p>3) Visual assessments: To validate clinical image quality of processed results, three radiologists performed a visual assessment on 10 groups of images. Each group includes an original LDCT image with lesions, the corresponding reference NDCT image, and the processed images by different denoising methods. NDCT, considered as the gold-standard, is the only labeled image in each group. All other images were evaluated on sharpness, noise suppression, diagnostic acceptability, and contrast retention using a five-point scale (5 = excellent and 1 = unacceptable). We invited three radiologists with mean clinical experience of 12.3 years to join our study. Note that these results were evaluated independently and the overall image quality score for each method was computed an averaging score from the four evaluation criteria. For different methods, the final score is presented as mean ± SD (average score of three radiologists ± standard deviation). The final quantitative results are listed in Table <ref type="table" target="#tab_2">III</ref>.</p><p>As observed, the original LDCT images have the lowest scores because of their severe image quality degradation. All denoising models improve the scores to some extent in this study. From Table <ref type="table" target="#tab_2">III</ref>, RED-CNN obtains the highest score in noise suppression. Compared to all other methods, our proposed SMGAN-3D scores best with respect to sharpness, diagnostic acceptability, and contrast retention. Furthermore, voxel-wise optimization (CNN-L2) has the best visuallyassessed image noise suppression, but it suffers from relatively low scores in sharpness and diagnostic acceptability, indicating a loss of image details. The proposed SMGAN-3D model gets a superior overall image quality score relative to the 2D model, which indicates that a 3D model can enhance CT image denoising performance by incorporating spatial information from adjacent slices.</p><p>In brief, the visual assessment demonstrates that SMGAN-3D has powerful capabilities in noise reduction, subtle image structure and edge preservation, and artifact removal. Most importantly, it satisfies the HVS requirements as shown in Figs. <ref type="figure" target="#fig_2">3</ref><ref type="figure" target="#fig_3">4</ref><ref type="figure" target="#fig_4">5</ref><ref type="figure" target="#fig_5">6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Computational Cost</head><p>In CT reconstruction, there is a trade-off between the computational cost and the image quality. In this aspect, a DL-based algorithm has great advantages in computational efficiency. Although the training of DL-based methods is timeconsuming, it can rapidly perform the denoising tasks on took 0.534s and 4.864s respectively in the validation phase on a NVIDA Titan GPU. Compared with the results in <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b58">[59]</ref>, our method took significantly less time. For example, the computational cost for soft threshold filtering (STF)-based TV minimization in the ordered-subset simultaneous algebraic reconstruction technique (OS-SART) framework took 45.1s per iteration on the same computing platform. Hence, it is clear that once the model is trained, it requires far less putational overhead than an iterative reconstruction method given that other conditions are equal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. DISCUSSIONS</head><p>As mentioned before, different emphases on visual evaluation and traditional image quality metrics were extensively investigated. When training with only the mean-based losses (L 1 -net, L 2 -net, RED-CNN), the results can achieve high scores in quantitative metrics and yield promising results with substantial noise reduction. When training with the featurebased methods (WGAN-VGG), the results can meet HVS requirements for visualization since they preserve more structural details than mean-based methods. However, these methods suffer from the potential risk of content distortion since a perceptual loss is computed based on a network <ref type="bibr" target="#b46">[47]</ref> trained on a natural image dataset. Practically and theoretically, even though adversarial learning can prevent smoothing in the image, and capture structural characteristics, they may often result in severe loss of diagnostic information. To integrate the best characteristics of these loss functions, we have proposed a hybrid loss function to deliver the LDCT image quality optimally.</p><p>Although our proposed network has achieved high-quality denoised LDCT images, there are still rooms for potential improvements. First and foremost, some feature edges in the processed results still look blurry. Also, some structural variations between NDCT and LDCT do not perfectly match. A possible way to enhance correlation between NDCT and LDCT is to design a network with a better modeling capability, which is the work we have started. As far as our reader study is concerned, although visual assessment may be subject to intra-as well as inter-operator variability, on average such assessment can still evaluate different algorithms effectively, especially in a pilot study. In our follow-up study, we will invite more radiologists to rate the results, and then quantify inter-operator variability in a task-specific fashion, and also study intra-operator variability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In conclusion, we have presented a 3D CNN-based method for LDCT noise reduction. As a follow-up to our previous work <ref type="bibr" target="#b36">[37]</ref>, a 3D convolutional neural network is utilized to improve the image quality in the 3D contextual setting. In addition, we have highlighted that the purpose of loss functions is to preserve high-resolution and critical features for diagnosis. Different from the state-of-the-art LDCT denoising method used in <ref type="bibr" target="#b35">[36]</ref>, an efficient structurally-sensitive loss has been included to capture informative structural features. Moreover, we have employed the Wasserstein distance to stabilize the training process for GAN. We have performed the quantitative and qualitative comparison of the image quality. The assessments have demonstrated that SMGAN-3D can produce results with higher-level image quality for clinical usage compared with the existing denoising networks <ref type="bibr" target="#b33">[34]</ref>- <ref type="bibr" target="#b36">[37]</ref>.</p><p>In the future, we will extend our model to other medical imaging modalities in a task-specific manner. Moreover, we plan to incorporate more advanced denoising models such as the networks mentioned in <ref type="bibr" target="#b59">[60]</ref>- <ref type="bibr" target="#b61">[62]</ref> for LDCT reconstruction. Finally, we are also interested in making our denoising software robust over different scanners.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A DIFFERENT TRAINING SETS FOR SMGAN-3D TRAINING</head><p>We randomly splitted the Mayo dataset <ref type="bibr" target="#b52">[53]</ref> into four different training sets,each with 5,000 image patches of size 80 × 80 × 11 pixels. Then, different training sets were used to validate the generalizability of our proposed 3D SMGAN model. The results are presented in Fig. <ref type="figure" target="#fig_9">10</ref> and Table <ref type="table" target="#tab_3">IV</ref>.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: The overall structure of the proposed SMGAN network. Note that the variable n denotes the number of filters and s denotes the stride size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Comparison of loss function value versus the number of epochs with respect to different algorithms. (a) L1 Loss, (b) Structural Loss, and (c) Wasserstein Distance curves.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Results from abdomen CT images. (a) NDCT, (b) LDCT, (c) CNN-L2, (d) CNN-L1, (e) SL-net, (f) MSL-net, (g) WGAN (h) BM3D, (i) RED-CNN, (j) WGAN-VGG, (k) SMGAN-2D, and (l) SMGAN-3D. The red rectangle indicates the region zoomed in Fig. 4. The display window is [-160, 240]HU.</figDesc><graphic coords="6,61.20,356.66,122.40,122.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Zoomed parts of the region of interests (ROIs) marked by the red rectangle in Fig. 3. (a) NDCT, (b) LDCT, (c) CNN-L2, (d) CNN-L1, (e) SL-net, (f) MSL-net, (g) WGAN, (h) BM3D, (i) RED-CNN, (j) WGAN-VGG, (k) SMGAN-2D and (l) SMGAN-3D. The red circle indicates the metastasis and the green and blue arrows indicate two subtle structure parts. The display window is [-160,240]HU.</figDesc><graphic coords="7,115.15,249.45,57.60,69.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Results from abdomen CT images. (a) NDCT, (b) LDCT, (c) CNN-L2, (d) CNN-L1, (e) SL-net, (f) MSL-net, (g) WGAN (h) BM3D, (i) RED-CNN, (j) WGAN-VGG, (k) SMGAN-2D, and (l) SMGAN-3D. The red rectangle indicates the region zoomed in Fig. 6. This display window is [-160, 240]HU.</figDesc><graphic coords="8,61.20,356.66,122.40,122.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: Zoomed parts of the region of interests (ROIs) marked by the red rectangle in Fig. 5. (a) NDCT, (b) LDCT, (c) CNN-L2, (d) CNN-L1, (e) SL-net, (f) MSL-net, (g) WGAN, (h) BM3D, (i) RED-CNN, (j) WGAN-VGG, (k) SMGAN-2D and (l) SMGAN-3D. The red circle indicates the metastasis and the green and blue arrows indicates two subtle structures. The display window is [-160,240]HU.</figDesc><graphic coords="9,115.15,271.85,57.60,80.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: Results from abdomen CT images. (a) NDCT, (b) LDCT, (c) CNN-L2, (d) CNN-L1, (e) SL-net, (f) MSL-net, (g) WGAN (h) BM3D, (i) RED-CNN, (j) WGAN-VGG, (k) SMGAN-2D, and (l) SMGAN-3D. The red rectangle indicates the region zoomed in Fig. 8. This display window is [-160, 240]HU.</figDesc><graphic coords="10,61.20,356.66,122.40,122.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 :</head><label>8</label><figDesc>Fig. 8: Zoomed parts of the region of interests (ROIs) marked by the red rectangle in Fig. 7. (a) NDCT, (b) LDCT, (c) CNN-L2, (d) CNN-L1, (e) SL-net, (f) MSL-net, (g) WGAN, (h) BM3D, (i) RED-CNN, (j) WGAN-VGG, (k) SMGAN-2D and (l) SMGAN-3D. The red and the green circles indicate subtle edges. The display window is [-160,240]HU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 :</head><label>9</label><figDesc>Fig. 4 Fig. 6 Fig. 8</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>FigsFig. 10 :</head><label>10</label><figDesc>Figs. 10a -10d Figs. 10e -10h Figs. 10i -10l PSNR SSIM RMSE PSNR SSIM RMSE PSNR SSIM RMSE Case1 26.678 0.811 0.0463 25.842 0.776 0.0510 26.538 0.812 0.0472 Case2 26.759 0.814 0.0459 25.848 0.781 0.0510 26.544 0.814 0.0470 Case3 26.589 0.807 0.0468 25.701 0.772 0.0519 26.455 0.806 0.0475 Case4 26.903 0.815 0.0452 25.914 0.782 0.0506 26.662 0.816 0.0464</figDesc><graphic coords="13,315.11,450.60,61.19,61.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Quantitative results associated with different approaches in Figs.3 and 5.</figDesc><table><row><cell></cell><cell>Fig. 3</cell><cell>Fig. 5</cell><cell>Fig. 7</cell></row><row><cell></cell><cell>PSNR SSIM RMSE</cell><cell>PSNR SSIM RMSE</cell><cell>PSNR SSIM RMSE</cell></row><row><cell>LDCT</cell><cell>22.818 0.761 0.0723</cell><cell>21.558 0.659 0.0836</cell><cell>24.169 0.737 0.0618</cell></row><row><cell>CNN-L1</cell><cell>27.791 0.822 0.0408</cell><cell>26.794 0.738 0.0457</cell><cell>29.162 0.807 0.0348</cell></row><row><cell>CNN-L2</cell><cell>27.592 0.819 0.0418</cell><cell>26.630 0.736 0.0466</cell><cell>28.992 0.806 0.0355</cell></row><row><cell>SL-net</cell><cell>26.864 0.831 0.0453</cell><cell>25.943 0.745 0.0504</cell><cell>28.069 0.813 0.0395</cell></row><row><cell>MSL-net</cell><cell>27.667 0.831 0.0414</cell><cell>26.685 0.744 0.0469</cell><cell>28.902 0.812 0.0359</cell></row><row><cell>WGAN</cell><cell>25.727 0.801 0.0517</cell><cell>24.655 0.711 0.0585</cell><cell>26.782 0.781 0.0458</cell></row><row><cell>BM3D</cell><cell>27.312 0.809 0.0431</cell><cell>26.525 0.728 0.0472</cell><cell>28.959 0.794 0.0356</cell></row><row><cell>RED-CNN</cell><cell>28.279 0.825 0.0385</cell><cell>27.243 0.743 0.0444</cell><cell>29.679 0.811 0.0328</cell></row><row><cell cols="2">WGAN-VGG 26.464 0.811 0.0475</cell><cell>25.300 0.722 0.0543</cell><cell>27.161 0.793 0.0419</cell></row><row><cell cols="2">SMGAN-2D 26.627 0.821 0.0466</cell><cell>25.507 0.732 0.0530</cell><cell>27.731 0.795 0.0406</cell></row><row><cell cols="2">SMGAN-3D 26.569 0.824 0.0473</cell><cell>25.372 0.739 0.0538</cell><cell>27.398 0.794 0.0411</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II :</head><label>II</label><figDesc>Statistical properties of the images in Figs. 4, 6 and 8. These are the ROIs indicated by the red rectangles in Figs. 3, 5 and 7. Note that the relative percentage difference of NDCT values versus the rest of models is added to aid readers.</figDesc><table><row><cell></cell><cell cols="2">Fig. 4</cell><cell cols="2">Fig. 6</cell><cell cols="2">Fig. 8</cell></row><row><cell></cell><cell>Mean</cell><cell>SD</cell><cell>Mean</cell><cell>SD</cell><cell>Mean</cell><cell>SD</cell></row><row><cell>NDCT</cell><cell>115.282</cell><cell>45.946</cell><cell>56.903</cell><cell>58.512</cell><cell>51.225</cell><cell>73.297</cell></row><row><cell>LDCT</cell><cell>114.955 (-0.2837%)</cell><cell>74.299 (61.709%)</cell><cell>57.228 (0.571%)</cell><cell>85.854 (46.729%)</cell><cell>50.142 (-2.114%)</cell><cell>89.346 (21.896%)</cell></row><row><cell>CNN-L1</cell><cell>115.809 (0.4571%)</cell><cell>28.532 (-37.9010%)</cell><cell>57.709 (1.416%)</cell><cell>42.315 (-27.682%)</cell><cell>50.917 (-0.6013%)</cell><cell>66.359 (-9.466%)</cell></row><row><cell>CNN-L2</cell><cell>117.191 (1.656%)</cell><cell>29.933 (-34.852%)</cell><cell>58.956 (3.608%)</cell><cell>43.411 (-25.808%)</cell><cell>52.229 (1.960%)</cell><cell>66.922 (-8.698%)</cell></row><row><cell>SL-net</cell><cell>131.333 (13.923%)</cell><cell>35.844 (-21.987%)</cell><cell>68.471 (20.329%)</cell><cell>50.789 (-13.199%)</cell><cell>63.874 (24.693%)</cell><cell>72.718 (-0.790%)</cell></row><row><cell>MSL-net</cell><cell>118.395 (2.701%)</cell><cell>32.548 (-29.160%)</cell><cell>63.271 (11.191%)</cell><cell>46.979 (-19.711%)</cell><cell>57.052 (11.375%)</cell><cell>69.519 (-5.154%)</cell></row><row><cell>WGAN</cell><cell>105.461 (-8.519%)</cell><cell>42.659 (-7.154%)</cell><cell>48.432 (-14.887%)</cell><cell>54.306 (-7.188%)</cell><cell>42.417 (-17.195%)</cell><cell>70.904 (-3.265%)</cell></row><row><cell>BM3D</cell><cell>114.058 (-1.062%)</cell><cell>31.515 (-31.409%)</cell><cell>25.649 (-54.925%)</cell><cell>69.411 (18.627%)</cell><cell>15.183 (-70.360%)</cell><cell>100.08 (36.540%)</cell></row><row><cell>RED-CNN</cell><cell>116.642 (1.180%)</cell><cell>27.194 (-40.813%)</cell><cell>57.985 (1.902%)</cell><cell>42.048 (-28.138%)</cell><cell>51.272 (0.0918%)</cell><cell>66.961 (-8.644%)</cell></row><row><cell>WGAN-VGG</cell><cell>108.229 (-6.118%)</cell><cell>36.721 (-20.078%)</cell><cell>54.450 (-4.311%)</cell><cell>48.660 (-16.838%)</cell><cell>44.959 (-12.232%)</cell><cell>67.059 (-8.511%)</cell></row><row><cell>SMGAN-2D</cell><cell>108.758 (-5.659%)</cell><cell>40.948 (-10.878%)</cell><cell>51.243 (-9.947%)</cell><cell>53.065 (-9.309%)</cell><cell>48.230 (-5.847%)</cell><cell>72.073 (-1.670%)</cell></row><row><cell>SMGAN-3D</cell><cell>115.569 (0.749%)</cell><cell>43.654 (-6.723%)</cell><cell>54.356 (-4.476%)</cell><cell>56.552 (-3.350%)</cell><cell>55.378 (8.107%)</cell><cell>73.303 (-0.00821%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III :</head><label>III</label><figDesc>Visual assessment scores by three radiologist readers.</figDesc><table><row><cell></cell><cell>Sharpness</cell><cell>Noise Suppression</cell><cell>Diagnostic Acceptability</cell><cell>Contrast Retention</cell><cell>Overall Quality</cell></row><row><cell>LDCT</cell><cell>2.55±1.43</cell><cell>1.55±0.80</cell><cell>1.85±0.96</cell><cell>1.75±0.83</cell><cell>1.93±1.01</cell></row><row><cell>CNN-L1</cell><cell>2.80±0.81</cell><cell>3.30±0.71</cell><cell>2.70±0.78</cell><cell>2.75±0.77</cell><cell>2.89±0.77</cell></row><row><cell>CNN-L2</cell><cell>2.12±0.42</cell><cell>3.98±0.58</cell><cell>1.93±0.78</cell><cell>2.07±0.83</cell><cell>2.53±0.55</cell></row><row><cell>SL-net</cell><cell>2.95±0.86</cell><cell>3.15±0.65</cell><cell>2.70±0.71</cell><cell>2.80±0.81</cell><cell>2.90±0.76</cell></row><row><cell>MSL-net</cell><cell>3.01±0.94</cell><cell>3.16±0.57</cell><cell>2.87±0.83</cell><cell>2.84±0.69</cell><cell>2.97±0.76</cell></row><row><cell>WGAN</cell><cell>3.30±0.56</cell><cell>2.80±0.81</cell><cell>3.15±0.91</cell><cell>3.45±1.02</cell><cell>3.09±0.66</cell></row><row><cell>BM3D</cell><cell>2.21±1.08</cell><cell>3.29±0.80</cell><cell>2.21±0.86</cell><cell>2.29±0.88</cell><cell>2.50±0.91</cell></row><row><cell>RED-CNN</cell><cell>3.29±0.88</cell><cell>3.79±0.70</cell><cell>3.51±0.70</cell><cell>3.46±1.12</cell><cell>3.51±0.85</cell></row><row><cell>WGAN-VGG</cell><cell>3.35±0.91</cell><cell>3.50±1.07</cell><cell>3.35±0.91</cell><cell>3.45±1.02</cell><cell>3.41±0.94</cell></row><row><cell>SMGAN-2D</cell><cell>3.25±0.65</cell><cell>3.48±0.66</cell><cell>3.32±0.58</cell><cell>3.21±0.78</cell><cell>3.32±0.67</cell></row><row><cell>SMGAN-3D</cell><cell>3.56±0.73</cell><cell>3.59±0.68</cell><cell>3.58±0.46</cell><cell>3.61±1.02</cell><cell>3.59±0.72</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV :</head><label>IV</label><figDesc>Quantitative results associated with different training sets for SMGAN-3D in Figs. 10.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>2169-3536 (c) 2018 IEEE. Translations and content mining are permitted for academic research only. Personal use is also permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2018.2858196, IEEE Access</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX B SUMMARY OF NOTATIONS</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Computed tomography -an increasing source of radiation exposure</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Brenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Hall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">New Eng. J. Med</title>
		<imprint>
			<biblScope unit="volume">357</biblScope>
			<biblScope unit="issue">22</biblScope>
			<biblScope unit="page" from="2277" to="2284" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Projected cancer risks from computed tomographic scans performed in the united states in 2007</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>De González</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mahesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-P</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bhargavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mettler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Land</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Arch. Intern. Med</title>
		<imprint>
			<biblScope unit="volume">169</biblScope>
			<biblScope unit="issue">22</biblScope>
			<biblScope unit="page" from="2071" to="2077" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">National council on radiation protection and measurements report shows substantial medical exposure increase</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Schauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">W</forename><surname>Linton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="293" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Sinogram noise reduction for low-dose CT by statistics-based nonlinear filters</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SPIE</title>
		<meeting>of SPIE</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">5747</biblScope>
			<biblScope unit="page">2059</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Penalized weighted least-squares approach to sinogram noise reduction and image reconstruction for lowdose X-ray computed tomography</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1272" to="1283" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Ray contribution masks for structure adaptive sinogram filtering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Balda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Heismann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1228" to="1239" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Structure adaptive anisotropic image filtering</title>
		<author>
			<persName><forename type="first">G.-Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Firmin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Underwood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Image Process</title>
		<meeting>IEEE Int. Conf. Image ess</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="135" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Discriminative feature representation to improve projection data inconsistency for low dose ct imaging</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Coatrieux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2499" to="2509" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Nonlocal prior bayesian tomographic reconstruction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Imaging and Vision</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="133" to="146" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Projection space denoising with bilateral filtering and CT noise modeling for dose reduction in CT</title>
		<author>
			<persName><forename type="first">A</forename><surname>Manduca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Trzasko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Khaylova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Kofler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Mccollough</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Fletcher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Phys</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4911" to="4919" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A constrained, totalvariation minimization algorithm for low-intensity x-ray ct</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">Y</forename><surname>Sidky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Duchin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ullberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Phys</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">S1</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Distance-driven projection and backprojection in three dimensions</title>
		<author>
			<persName><forename type="first">B</forename><surname>De Man</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Basu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Med. Biol</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">2463</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Properties of preprocessed sinogram data in x-ray computed tomography</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Whiting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Massoumzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">A</forename><surname>Earl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>O'sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Phys</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3290" to="3303" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Statistical image reconstruction for polyenergetic X-ray computed tomography</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">A</forename><surname>Elbakri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Fessler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="89" to="99" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Low-dose CT reconstruction via edge-preserving total variation regularization</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Med. Biol</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page">5949</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adaptive-weighted total variation minimization for sparse data toward low-dose x-ray computed tomography image reconstruction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Med. Biol</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page">7923</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Low-dose X-ray CT reconstruction via dictionary learning</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1682" to="1697" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Tensor-based dictionary learning for spectral CT reconstruction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="142" to="154" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Image reconstruction in circular cone-beam computed tomography by constrained, total-variation minimization</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">Y</forename><surname>Sidky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Med. Biol</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page">4777</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improving abdomen tumor low-dose CT images using a fast dictionary learning based processing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Coatrieux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Toumoulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Med. Biol</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page">5803</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Low-dose computed tomography image restoration using previous normal-dose scan</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Phys</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="5713" to="5731" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adaptive nonlocal means filtering based on local noise level for CT denoising</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Trzasko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Blezek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Fletcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Mccollough</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Manduca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Phys</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A review of image denoising algorithms, with a new one</title>
		<author>
			<persName><forename type="first">A</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Coll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multiscale Model. Simul</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="490" to="530" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Image processing assisted algorithms for optical projection tomography</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cheddad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Svensson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sharpe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Georgsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Ahlgren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Block matching 3D random noise filtering for absorption optical projection tomography</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Feruglio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Vinegoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sbarbati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weissleder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Med. Biol</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page">5401</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Artifact suppressed dictionary learning for low-dose ct image processing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Coatrieux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2271" to="2292" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">3d feature constrained reconstruction for low dose ct imaging</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Coatrieux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Circuits Syst. Video Technol</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A perspective on deep imaging</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="8914" to="8924" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Machine learning will transform radiology significantly within the next 5 years</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kalra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Orton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Phys</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2041" to="2044" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks for multi-modality isointense infant brain image segmentation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="214" to="224" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Scalable high performance image registration framework by unsupervised deep feature representations learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning for Medical Image Analysis</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="245" to="269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Region-adaptive deformable registration of ct/mri pelvic images via learning-based image IEEE Trans. Image Process</title>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Classification of amyloid status using machine learning with histograms of oriented 3d gradients</title>
		<author>
			<persName><forename type="first">L</forename><surname>Cattell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Platsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Declerck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Schnabel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D N</forename><surname>Initiative</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage: Clinical</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="990" to="1003" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Low-dose CT via convolutional neural network</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomed. Opt. Express</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="679" to="694" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Low-dose CT with a residual encoder-decoder convolutional neural network</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Kalra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2524" to="2535" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Generative adversarial networks for noise reduction in low-dose CT</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Wolterink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Leiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Viergever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Išgum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2536" to="2545" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Low dose CT image denoising using a generative adversarial network wasserstein distance and perceptual loss</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Kalra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.00961</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">A deep convolutional neural network using directional wavelets for low-dose x-ray ct reconstruction</title>
		<author>
			<persName><forename type="first">E</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Ye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.09736</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">3D convolutional encoder-decoder network for low-dose CT via transfer learning from a 2D trained network</title>
		<author>
			<persName><forename type="first">H</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Kruger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05656</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Improving low-dose ct image using residual convolutional network</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Coatrieux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">705</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
	</analytic>
	<monogr>
		<title level="j">Wasserstein GAN</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Loss functions for image restoration with neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Gallo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Frosio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput. Imaging</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="47" to="57" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Comparison between prelog and post-log statistical models in ultra-low-dose CT reconstruction</title>
		<author>
			<persName><forename type="first">L</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Alessio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Kinahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Kalra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>De Man</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="707" to="720" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Three-dimensional volume rendering of spiral ct data: theory and method</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Calhoun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Kuszyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Heath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Carley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">K</forename><surname>Fishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiographics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="745" to="764" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 27th Int. Conf. Machine Learning</title>
		<meeting>27th Int. Conf. Machine Learning</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances Neural Information Processing Systems Conf</title>
		<meeting>Advances Neural Information essing Systems Conf</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5769" to="5779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Mean squared error: Love it or leave it? a new look at signal fidelity measures</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="117" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Multiscale structural similarity for image quality assessment</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Asilomar Conf. Signals, Syst., Comput</title>
		<meeting>IEEE Asilomar Conf. Signals, Syst., Comput</meeting>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1398" to="1402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Low dose ct grand challenge</title>
		<author>
			<persName><surname>Aapm</surname></persName>
		</author>
		<ptr target="http://www.aapm.org/GrandChallenge/LowDoseCT/#" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Image denoising and inpainting with deep neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="341" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="295" to="307" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Tensorflow: Large-scale machine learning on heterogeneous distributed systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Gpu-based acceleration for interior tomography</title>
		<author>
			<persName><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="757" to="770" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Gpu-accelerated regularized iterative reconstruction for few-view cone beam ct</title>
		<author>
			<persName><forename type="first">D</forename><surname>Matenine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Goussard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Després</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical physics</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1505" to="1517" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Deformable convolutional networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<idno>CoRR, abs/1703.06211</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Hongming Shan received the bachelor&apos;s degree from Shandong University of Technology, China in 2011 and obtained the PhD degree from Fudan University, China in 2017. He is currently a postdoctoral scholar at the Rensselaer Polytechnic Institute. His research interests include machine/deep learning, computer vision, dimension reduction, and biomedical imaging</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Chenyu You received the B.S. degree in Electrical Engineering with a minor in Mathematics from Rensselaer Polytechnic Institute</title>
		<title level="s">Proc. Adv. Neural Inf. Process. Syst</title>
		<meeting><address><addrLine>Troy, NY; Stanford, CA, USA; Beijing, China; Troy, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2017. 2013. 2014</date>
			<biblScope unit="page" from="3859" to="3869" />
		</imprint>
	</monogr>
	<note>Lars Gjesteby received the B.S. degree in Biomedical engineering from the Rensselaer Polytechnic Institute. where he is currently pursuing the Ph.D. degree in biomedical engineering under the supervision of G. Wang. His current research interest include x-ray computed tomography, metal artifact reduction, deep learning, and the combination of CT and MRI for simultaneous imaging</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">USA, as a Postdoctoral Researcher. His research interests include computed tomography reconstruction, image processing and cone-beam CT system development. Shenghong Ju received the MD and Ph</title>
		<author>
			<persName><forename type="first">Guang</forename><surname>Li Received The</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Ph</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Her research is mainly in the field of molecular imaging, functional imaging and radiomics. Zhuiyang Zhang received the B.S</title>
		<meeting><address><addrLine>Nanjing, China; Troy, NY; China; Suzhou, China; Wuxi, China; Nanjing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Rensselaer technic Institute</publisher>
			<date type="published" when="1986">2008. 2011. 2016. 2016. 2006. 1986. 1999. 2017</date>
		</imprint>
		<respStmt>
			<orgName>degrees from Biomedical Engineering Department, Southwest University ; D. degree from Medical School of Southeast University, Nanjing ; Medical School of Soochow University ; D. degree from Medical School of Southeast University ; Zhongda Hospital Southeast University</orgName>
		</respStmt>
	</monogr>
	<note>His research interest is focused on molecular imaging</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">He is Clark Crossan &amp; Chair Professor and the Director of the Biomedical Imaging Center, RPI, USA. He authored the papers on the first spiral/helical cone-beam/multi-slice CT algorithm. Currently, there are over 100 million medical CT scans yearly with a majority in the spiral cone-beam mode. He pioneered bioluminescence tomography. His group published the first papers on interior tomography and omni-tomography</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><forename type="middle">B S</forename></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Ph</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1982, the M.S. degree in applied mathematics from Harbin Institute of Technology, Harbin, in 1988, and the Ph.D. degree in optical imaging from Beijing University of Science and Technology, Beijing</title>
		<meeting><address><addrLine>Chengdu, China; Harbin, China; China; Rensselaer Polytechnic Institute, Troy, NY, USA; Buffalo, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">2005. 2008. 2012. 2014 to 2015. 1998</date>
		</imprint>
		<respStmt>
			<orgName>College of Computer Science, Sichuan University ; Department of Biomedical Engineering, Rensselaer Polytechnic Institute, Troy, NY ; Computer Science, Sichuan University ; University at Buffalo The state University of New York</orgName>
		</respStmt>
	</monogr>
	<note>His results were featured in Nature, Science, and PNAS. and recognized with awards. He wrote&gt; 430 peer-reviewed journal publications. He is Fellow of the IEEE, SPIE, OSA, AIMBE, AAPM, and AAAS</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
