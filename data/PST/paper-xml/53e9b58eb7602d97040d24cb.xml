<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Robust and accurate iris segmentation in very noisy iris images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Peihua</forename><surname>Li</surname></persName>
							<email>peihualj@hotmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Xiaomin</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Lijuan</forename><surname>Xiao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Qi</forename><surname>Song</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Heilongjiang University</orgName>
								<address>
									<addrLine>Xue Fu Street No. 74</addrLine>
									<settlement>Harbin</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Hei Long Jiang Province</orgName>
								<address>
									<postCode>150080</postCode>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Robust and accurate iris segmentation in very noisy iris images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D8ADC19071A1359F4AB697111A1FDA8E</idno>
					<idno type="DOI">10.1016/j.imavis.2009.04.010</idno>
					<note type="submission">Received 15 December 2008 Received in revised form 13 April 2009 Accepted 16 April 2009</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T02:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Iris segmentation Integro-differential operator K-Means clustering RANSAC algorithm</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Iris segmentation plays an important role in an accurate iris recognition system. In less constrained environments where iris images are captured at-a-distance and on-the-move, iris segmentation becomes much more difficult due to the effects of significant variation of eye position and size, eyebrows, eyelashes, glasses and contact lenses, and hair, together with illumination changes and varying focus condition. This paper contributes to robust and accurate iris segmentation in very noisy images. Our main contributions are as follows: (1) we propose a limbic boundary localization algorithm that combines K-Means clustering based on the gray-level co-occurrence histogram and an improved Hough transform, and, in possible failures, a complementary method that uses skin information; the best localization between this and the former is selected. (2) An upper eyelid detection approach is presented, which combines a parabolic integro-differential operator and a RANSAC (RANdom SAmple Consensus)-like technique that utilizes edgels detected by a one-dimensional edge detector. (3) A segmentation approach is presented that exploits various techniques and different image information, following the idea of focus of attention, which progressively detects the eye, localizes the limbic and then pupillary boundaries, locates the eyelids and removes the specular highlight.</p><p>The proposed method was evaluated in the UBIRIS.v2 testing database by the NICE.I organizing committee. We were ranked #4 among all participants according to the evaluation results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Accurate iris segmentation is a prerequisite for a successful iris recognition system <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b0">1]</ref>. Two well-known, distinct approaches to iris segmentation are attributed to Daugman <ref type="bibr" target="#b1">[2]</ref> and Wildes <ref type="bibr" target="#b11">[12]</ref>, respectively. Both methods have proven quite effective in iris segmentation. As pointed out by Wildes <ref type="bibr" target="#b11">[12]</ref>, however, when the iris images become more cluttered, both methods may fail to give good results. Great effort has been devoted to achieving satisfactory segmentation results <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b3">4]</ref>. Although the techniques proposed by different researchers vary considerably, they assume, without exception, that the iris images are captured under circumstances in which the subject is cooperative. For iris segmentation in less constrained environments, Proença et al. built a noisy iris image database, UBIRIS <ref type="bibr" target="#b9">[10]</ref>, and proposed a new segmentation methodology <ref type="bibr" target="#b8">[9]</ref>.</p><p>When subjects are less cooperative, or even at-a-distance and on-the-move, analysis of the iris images captured becomes much more challenging. In these situations, the iris images generally contain broader face regions surrounding the eye, and hence the eye position and size may vary significantly. In addition, the effects of illumination changes and poor focus, as well as eyebrows, eye-lashes, hair, and contact lenses or glasses become much more significant.</p><p>For evaluating the performance of different iris segmentation approaches in very noisy iris images, an international contest, NI-CE.I (noisy iris challenge evaluation -Part I) <ref type="bibr" target="#b7">[8]</ref>, was launched and attracted 97 registered research teams from 35 countries around the world. The organizing committee released the second version of the UBIRIS (UBIRIS.v2) in which iris images were captured at-a-distance and on-the-move, together with the ground truth to the participants. The participants were required to submit their executable programs, and the final evaluation was independently performed by the organizing committee.</p><p>We participated in the NICE.I, submitting an executable program based completely on the algorithm described in this paper, and were ranked #4 among all participants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Proposed iris segmentation method</head><p>Fig. <ref type="figure">1</ref> gives an overview of the proposed segmentation method, which consists of, roughly speaking, eye detection, limbic and then pupillary boundary localization, followed by upper and lower eyelid detection. Given an eye image, Algorithm A, as shown in Fig. <ref type="figure">2</ref> , where e iris is a prescribed threshold, the result is considered to be accurate and we proceed to locate the upper and lower eyelids with Algorithm C as shown in Fig. <ref type="figure" target="#fig_2">4</ref>. Conversely, if CID K-Means &lt; e iris , the detection via Algorithm A might be inaccurate. In this case, we first examine whether the eye is closed: in that situation no further processing is necessary and the procedure ends; otherwise, Algorithm B illustrated in Fig. <ref type="figure">3</ref> is triggered to exploit skin information for again localizing the limbic boundary and outputting a value CID skin as well. The localization result from Algorithm B is considered to be better and is accepted if the following two conditions are satisfied simultaneously: (1) CID skin &gt; CID K-Means ; (2) the number of pixels detected as skin should be very small, if any, within the detected circular boundary. Otherwise, the localization result from Algorithm B is not better and that from Algorithm A should be accepted instead. Once the limbic boundary is localized, we perform consecutively pupillary boundary localization, upper and lower eyelid detection, and specular highlight removal.</p><p>Algorithm A, as shown in Fig. <ref type="figure">2</ref>, localizes the limbic and pupillary boundaries using K-Means clustering and an improved Hough transform. It starts with eye detection based on the AdaBoost algorithm <ref type="bibr" target="#b10">[11]</ref>. Our idea in doing this is to eliminate the face region surrounding the eye so that the subsequent edge points will be detected more accurately <ref type="bibr" target="#b11">[12]</ref>. The detected eye region is then clipped to get a coarse eye region. In the case that no eye is detected, the original image is used. Next, K-Means clustering based on the gray-level co-occurrence histogram is performed to segment the (clipped) eye region. On the segmented region, edge detection is applied to get an edge map on which an elliptical Hough transform is employed to obtain a fine region containing the eye. In this fine region, an improved circular Hough transform is presented to localize the limbic boundary. Finally, CID K-Means is output by Algorithm A to indicate the accuracy of the circle characterizing the limbic boundary.</p><p>Algorithm B, as shown in Fig. <ref type="figure">3</ref>, provides a complementary method, in case of the possible failure of Algorithm A for localizing the limbic boundary. In contrast to the gray-level co-occurrence histogram in Algorithm A, Algorithm B uses skin information for image segmentation. The segmented image is next dilated followed by edge detection plus an elliptical Hough transform. Then, similar to what is done in Algorithm A, the same improved circular Hough transform is applied and a similar value CID skin is output by Algorithm B.</p><p>The flow diagram of Algorithm C is illustrated in Fig. <ref type="figure" target="#fig_2">4</ref>. Upper eyelid detection is challenging in the less constrained environment, and Algorithm C accomplishes this task by introducing a parabolic integro-differential operator combined with a RANSAC-like <ref type="bibr" target="#b2">[3]</ref> technique. In contrast, lower eyelid detection is simpler and one-dimensional edgels detection plus the RANSAC algorithm for parabola fitting is used for this objective. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Localizing the limbic and pupillary boundaries</head><p>The algorithm for localizing the limbic and pupillary boundaries using K-Means clustering and the improved Hough transform is illustrated in Fig. <ref type="figure">2</ref> and explained in the second paragraph of the preceding section. Fig. <ref type="figure" target="#fig_3">5</ref> gives an illustration of a specific example. The eye detection algorithm determines the coarse eye location and correspondingly clips a region, which is then segmented based on K-Means clustering followed by edge detection. An ellipse is obtained via the Hough transform on the edge map and a fine eye region can be obtained. The limbic boundary is localized by using the improved circular Hough transform, and then the pupillary boundary is determined via the circular integro-differential operator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Eye detection based on AdaBoost</head><p>In the iris images captured at-a-distance and on-the-move, the position and size of the eye in the image vary significantly. To facilitate subsequent processing, it is necessary to locate eyes in images. According to the proposal of Viola and Jones <ref type="bibr" target="#b10">[11]</ref>, we trained a lefteye detector (16 layers) and a right-eye detector (16 layers), respectively, using rectangle features based on the AdaBoost algorithm. Positive and negative eye examples are hand-labeled in images collected from the Internet or captured by ourselves. Our particular requirement is that the eye detectors trained should have very low false positive rate (approximately equal to zero), and, of course, the correct detection rates are low in these detectors. In practice, we first use the left-eye detector and, if it fails to detect an eye, the right-eye detector is triggered for the second detection. The experiment in the UBIRIS.v2 training database shows that our detectors achieve a detection rate of 64% without false positives. Images in which the detectors fail to find an eye mostly contain large eyes with very little surrounding skin region. Our strategy for an input iris image is to clip a coarse eye region for the subsequent processing if an eye is detected; otherwise, the whole image is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Image segmentation using K-Means clustering based on the graylevel co-occurrence histogram</head><p>A normalized gray-level co-occurrence histogram, similar to the color correlogram in Ref. <ref type="bibr" target="#b4">[5]</ref>, is proposed as a feature set for use in the K-Means clustering algorithm. For a pixel z Ã ¼ ðx Ã ; y Ã Þ, let Iðz Ã Þ denote its gray intensity value. The gray levels of I are quantized into m bins. Let S Ã be the set of pixels within the upright square centered at z Ã with side length d</p><formula xml:id="formula_0">Ã , that is, S z Ã ¼ fzjkz À z Ã k 1 6 d Ã g,</formula><p>where k Á k 1 represents the infinity norm. The gray-level co-occurrence histogram is defined as</p><formula xml:id="formula_1">p d ðu; vÞ ¼ 1 N d X z2S z Ã X z 0 2Sz d u;IðzÞ d v;Iðz 0 Þ<label>ð1Þ</label></formula><p>where u; v ¼ 1; . . . ; m; S z ¼ fz 0 jkz 0 À zk 1 ¼ dg; N d is a normalizing constant so that the sum of p d ðu; vÞ becomes one, and d u;IðzÞ denotes a Kronecker Delta function that equals 1 if IðzÞ belongs to the uth bin, and otherwise equals 0. The gray-level co-occurrence histogram p d ðu; vÞ represents joint spatial and gray-level distribution, which gives the probability that a pixel z of gray level u is separated from another pixel z 0 of gray level v by distance d.</p><p>We here introduce an adaptive quantization scheme of the gray levels that can accurately represent the gray-level distribution with much fewer bins. First the histogram with 64 uniform bins is computed in the eye region and is then filtered with a Gaussian. The significant peaks and valleys are extracted from the filtered histogram data. One peak together with its left-and right-hand side valleys automatically determine a bin subinterval. The firstand second-order derivatives of the filtered histogram are also computed to determine the inflection points, if any. Any subinterval containing inflection points should be further divided, selecting them as endpoints of smaller subintervals. Fig. <ref type="figure">6</ref> shows an example of adaptive quantization of gray levels, in which five bins are finally obtained (please note the inflection point denoted by ).</p><p>In the clipped eye region, we can reasonably set K ¼ 4 because there are four approximately homogeneous regions corresponding to the iris, sclera, skin, and pupil and eyelashes (note that the two have similar gray levels). For each pixel, the m Â m co-occurrence histogram is computed and used as a one-dimensional feature 1D detection of the upper eyelid edgels Locating the upper eyelid using a parabolic Integro-Differential operator plus a RANSAC-like algorithm 1D detection of the lower eyelid edgels Locating the lower eyelid using the RANSAC algorithm for parabolic fitting  vector, and Bhattacharyya coefficient is adopted as distance measure to perform K-Means clustering. After segmentation a canny edge detector is applied to get an edge map of the eye region.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Limbic boundary localization with an improved Hough transform</head><p>The circular Hough transform is applied to the edge map previously obtained to get the limbic boundary. Noting that low gray intensity values of pixels inside and high intensity values outside the circle characterize the limbic boundary, the gradient of a true edgel on the limbic boundary should have the same direction as the outer normal of the circle at that point. During the Hough transform we impose this restriction on the edgels to vote, and an edgel is eliminated from voting if it violates this constraint. Let the gradient of an edgel with spatial coordinate ðx; yÞ be ½I x ; I y T , its direction be h e , and the outer normal direction be h c at the same point of the circle parameterized by the triple ðx c ; y c ; rÞ, we require jh e À h c j 6 e h ð2Þ where e h is a prescribed threshold. Using the inner product of two vectors, the constraint (2) can be written as</p><formula xml:id="formula_2">ðI y ðx À x c Þ À I x ðy À y c ÞÞ 2 ðI 2 x þ I 2 y Þððx À x c Þ 2 þ ðy À y c Þ 2 Þ 6 arccos 2 e h<label>ð3Þ</label></formula><p>Notice that in Eq. ( <ref type="formula">2</ref>) we have to compute an inefficient inverse trigonometric function h e ¼ arctan À1 ðI y =I x ), which is however avoided in Eq. ( <ref type="formula" target="#formula_2">3</ref>). The edgels from the edge detector are usually noisy and the circle with maximum voting number does not necessarily correspond to the true limbic boundary. In addition, the Hough transform tends to favor larger circles that have more edge points. To solve this, rather than selecting only the circle of maximum voting number, we choose, in the Hough transform array the top 10 circles as candidates for further selection. The integro-differential operator <ref type="bibr" target="#b1">[2]</ref> CIDðx c ; y c ; rÞ ¼ G r ðrÞ Ã @ @r</p><formula xml:id="formula_3">I Cx c ;y c ;r Iðx; yÞ 2pr ds<label>ð4Þ</label></formula><p>where the Ã stands for convolution operation, G r ðrÞ is a Gaussian with standard deviation r, C xc ;y c ;r denotes the circular arc, acts as a circular edge detector that measures the average intensity variation along the circular boundary. We use it as a measure to select, from the top 10 candidates produced by the Hough transform, the best one that has the maximum CID. Fig. <ref type="figure" target="#fig_6">7a</ref>, b, and c shows, respectively, the edge map, the top 10 circles from the improved Hough transform, and the top 1 from the Hough transform (green circle) along with the one with maximum CID (red circle), denoted by CID K-Means , which is more accurate than the former.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Pupillary boundary localization</head><p>Once the outer circle representing the limbic boundary is defined, the pupil center can be confined in a small square region within the outer circle. In addition, as is known in physiology, the ratio between the radii of iris and the pupil is around 3:1. So the pupil radius can be restricted within the interval ½r=4; 2r=3. A specular highlight may interfere with pupillary boundary detection so we remove it with the method introduced in Section 6. The circular integro-differential algorithm <ref type="bibr" target="#b1">[2]</ref> is used to localize the pupillary boundary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Closed-eye determination and complementary method for localizing the limbic boundary</head><p>If the limbic boundary localized by the algorithm that uses K-Means clustering based on the gray-level co-occurrence histogram is not accurate, i.e., CID K-Means &lt; e iris , then we proceed to determine whether the eye is closed. In this case, the process may terminate; otherwise, Algorithm B that depends on the skin information is triggered to localize the limbic boundary again.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Closed-eye determination</head><p>As shown in Fig. <ref type="figure" target="#fig_7">8</ref>, the histogram of the eye region is computed and is then filtered with a Gaussian. The leftmost peak in the histogram corresponds to the gray level of eyelashes, and so the valley that immediately follows is identified as the segmentation threshold to get a binary image. Then, this binary image is dilated and an edge map is produced. The edge link algorithm is applied to the edge map and the longest edge list is extracted; it corresponds to the contour of the closed eye. The upper part of the contour is extracted, and a parabola is fit with the least square algorithm. If the parabola does not open to the bottom and its curvature is close to zero, then the eye is judged as being closed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Skin detection for the limbic boundary localization</head><p>If the eye is not closed as just discussed, then Algorithm B as shown in Fig. <ref type="figure">3</ref> is triggered. The algorithm begins with skin detection in the coarse eye region determined by the eye detectors. The binary image obtained is then dilated and edge detection is applied. The procedures that follow are the same as those in Algorithm A and are thus omitted here. Finally, the CID of the circle determined by Algorithm B is output as CID skin .</p><p>We use a color histogram to represent the skin or non-skin distributions in iris images. The skin pixels as positive examples are hand-labeled from training examples, and pixels in eyelashes, iris, and sclera regions are labeled as negative examples. Let pðIjskinÞ and pðIjnon-skinÞ be skin and non-skin histograms computed from the positive and negative examples, respectively. The skin classifier is derived via likelihood ratio, and a pixel is regarded as skin if pðIjskinÞ pðIjnon-skinÞ</p><formula xml:id="formula_4">&gt; e skin<label>ð5Þ</label></formula><p>The threshold e skin can be adjusted to trade off between the false positive rate and the correct detection rate. The ROC curve is plotted in Fig. <ref type="figure" target="#fig_8">9</ref> where the threshold labeled by Ã is selected. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Locating the upper and lower eyelids</head><p>Having localized the limbic boundary, we propose to use an integro-differential parabolic arc operator to detect the upper eyelid. During the development process we used a RANSAC-like technique to constrain the possible candidate parabolas. The constraint is that a valid potential parabola should be near at least a certain percentage of the eyelid edgels detected by the one-dimensional edge detector, considering the fact that edgels may be noisy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Locating the upper eyelid</head><p>The upper eyelid is modeled by a parabola of the form </p><formula xml:id="formula_5">y ¼ aðx À bÞ 2 þ c<label>ð6Þ</label></formula><p>where G r ðcÞ is a Gaussian with standard deviation r, C a;b;c and L a;b;c denote, respectively, the parabolic arc and its length. The integrodifferential operator is in effect a parabolic arc edge detector, which searches in the parameter of the parabola the one whose smoothed integral derivative is maximum. Notice that when occlusion occurs the eyelid boundary above the iris may be weak and difficult to distinguish, thus the segment of the parabolic arc within this region is excluded when calculating the curve integral with Eq. <ref type="bibr" target="#b6">(7)</ref>.</p><p>Given the iris position ðx c ; y c Þ and radius r, we can detect the eyelid boundary edgels along the vertical line segments</p><formula xml:id="formula_7">x ¼ d 0 with y À 2r 6 y 6 y c þ r=2<label>ð8Þ</label></formula><p>where d 0 varies in intervals ½x c À 3r;</p><formula xml:id="formula_8">x c À r or ½x c þ r; x c þ 3r. First, a</formula><p>one-dimensional signal is extracted according to Eq. ( <ref type="formula" target="#formula_7">8</ref>), then the signal is smoothed with a Gaussian kernel and finally the gradient is computed and the point of maximum gradient that is larger than a prescribed threshold e e is regarded as the edgel of the eyelid boundary. Fig. <ref type="figure" target="#fig_10">10</ref> shows an example of the upper eyelid detection, where Fig. <ref type="figure" target="#fig_10">10a</ref> shows the search regions (cyan dashed) along with a one-dimensional signal (yellow, vertical line segment), and Fig. <ref type="figure" target="#fig_10">10b</ref> provides the edgels (yellow points) p i ; i ¼ 1; . . . ; N e (note that only a portion of the edgels are accurate) and the localized parabola.    The proposed algorithm for locating the upper eyelid is explained below. The search ranges of parameters a; b; c are discretized by step a; step b; step c, respectively. For each three-tuple ða; b; cÞ in the search space, a RANSAC-like technique is used to determine whether the three-tuple is accepted as a potential candidate or just rejected. Given ða; b; cÞ, the number N p of the detected edgels p i ; i ¼ 1; . . . ; N e , is counted for which the distance of the edgel to the parabola is below a threshold e d . If the percentage of p over N e is above another threshold e N , the parabola characterized by the three-tuple is a valid candidate for which f ða; b; cÞ is computed according to Eq. <ref type="bibr" target="#b6">(7)</ref>. Finally, we get the parabola characterized by the three-tuple ða Ã ; b Ã ; c Ã Þ with maximum f.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Locating the lower eyelid</head><p>Compared to the upper eyelid, the detection of the lower eyelid is much simpler. At the beginning, we perform one-dimensional search to detect edgels of the eyelid boundary. The one-dimensional search is performed along the vertical line segments of the form</p><formula xml:id="formula_9">x ¼ d 0 with y c 6 y 6 y c þ 3r=2<label>ð9Þ</label></formula><p>where d 0 varies in the interval ½x c À r=2; x c þ r=2. Because the edgels detected in this way may be noisy, the RANSAC algorithm <ref type="bibr" target="#b2">[3]</ref> is used to fit the parabolic arc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Specular highlight removal</head><p>As we have obtained the limbic and pupillary boundaries as well as the upper and lower eyelids, it is straightforward to remove the specular highlight. We first compute the histogram of the seg-mented region and filter it with a Gaussian. Since the rightmost peak in the filtered histogram corresponds to the specular highlight, we select the valley to the left of the rightmost peak as the threshold. In this manner, the pixels whose intensity values are higher than the threshold are regarded as highlight pixels. Fig. <ref type="figure" target="#fig_11">11a,</ref><ref type="figure">b</ref>, and c provides, respectively, the segmentation result of an example image, the histogram of the segmented image region, and the final binary image after the highlight is removed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Experimental results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Iris database and evaluation measures</head><p>In the iris database of UBIRIS.v2 used by the NICE.I organizing committee, the training database that was released to the participants comprises 500 color iris images together with the ground truth, while the testing database was retained by the organizing committee for independent evaluation.</p><p>The segmentation output is given by binary images where the iris pixels are denoted by black and the non-iris pixels by white. Two measures are adopted by the NICE.I organizing committee for evaluating the proposed algorithm. The first one, classification error rate, which computes the average of the fractions of disagreeing pixels in all testing images, is of the form</p><formula xml:id="formula_10">E 1 ¼ 1 N Á W Á H X H i¼1 X W j¼1</formula><p>Oði; jÞ Gði; jÞ where N; W; H denote, respectively, the number of testing images and the image width and height, and Oði; jÞ and Gði; jÞ represent the output binary image and the ground truth. The second measure is the average of the false positive rate (FP) and false negative rate (FN)  Fig. <ref type="figure">12</ref>. Seventeen images where our method produced the highest error rates.</p><formula xml:id="formula_11">E 2 ¼ ðFP þ FNÞ=2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Evaluation results and discussion</head><p>According to the independent evaluation of the NICE.I organizing committee, our method obtained E 1 ¼ 0:022, and E 2 ¼ 0:068 with FP ¼ 0:018 and FN ¼ 0:118, and we were ranked #4 among all participants.</p><p>Fig. <ref type="figure">12</ref> shows all 17 images where our method produced the highest error rates returned by the NICE.I organizing committee. In each column, three consecutive images belong to one group, in which the top one shows the test image along with the limbic and pupillary boundaries, the upper and lower eyelids; the middle one shows the binary image we obtained after the segmentation process; and the bottom shows the false positives (green) and the false negatives (red).</p><p>The reasons behind these error cases are analyzed as follows: (1) the eye region clipped by the AdaBoost eye detectors contains nearly only the iris, rather than the eye, resulting in the subsequent localization error. This analysis accounts for failures in images #1 and #2. (2) The error cases of images #4 and #5 are primarily due to inaccurate upper eyelid location. (3) The main reason leading images #6 to #10 along with image #3 to high error rates is that we actually do not deal with eyelashes in our methodology. (4) For images #11 to #14, the specular highlight is not appropriately removed, which we think is because the thresholds found as discussed in Section 6 were not accurate. <ref type="bibr" target="#b4">(5)</ref> In images #15 and #16, the errors resulting from inaccurate pupillary localization are significant, along with those from the inaccurate eyelid boundaries and highlights. As to the last one, image #17, a combination of imperfect localization of the upper eyelid, the pupillary boundary and limbic boundary explains the errors introduced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>This paper has proposed a method to perform the segmentation of the iris in noisy images. Following the idea of focus of attention, the proposed method proceeds progressively: first, the eye position and size are determined; second, in the eye region the limbic and then the pupillary boundaries are localized; third, the upper and lower eyelids are located; and finally the specular highlight is removed. The limbic boundary localization is an important step, and we thus introduce two complementary methods to determine the limbic boundary reliably, which use the gray-level co-occurrence histogram and skin information, respectively. The proposed approach has proven quite effective, according to the independent evaluation of the NICE.I organizing committee in the UBIRS.v2, a challenging iris database. We were ranked #4 among all participants.</p><p>Our future work will concern eyelash detection which is currently not handled, and optimization of the code or development of parallel algorithms so that the software can run in real time.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Fig.1gives an overview of the proposed segmentation method, which consists of, roughly speaking, eye detection, limbic and then pupillary boundary localization, followed by upper and lower eyelid detection. Given an eye image, Algorithm A, as shown in Fig.2, uses K-Means clustering based on the gray-level co-occurrence histogram and an improved Hough transform to localize the limbic</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .Fig. 2 .Fig. 3 .</head><label>123</label><figDesc>Fig. 1. Flow diagram of the iris segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Flow diagram of Algorithm C that locates the upper and lower eyelids.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Limbic boundary localization using K-Means clustering and the improved Hough transform.</figDesc><graphic coords="3,97.91,574.75,387.74,163.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>5 Fig. 6 .</head><label>56</label><figDesc>Fig. 6. Adaptive quantization of the gray levels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>According to the meanings of the three parameters of the parabola, we can determine their ranges. The parabola should open to the bottom and its curvature should be less than that of the iris, thus a should be in the interval ð0; 1=ð2rÞÞ. According to the geometric relationship between the eyelid and the iris, we can further constrain the domains of the parabola vertex ðb; cÞ : b 2 ½x c À r=2; x c þ r=2; c 2 ½y c À 3r=2; y c þ 2r=3, where ðx c ; y c Þ is the iris center and r stands for the iris radius.Motivated by Daugman's paper<ref type="bibr" target="#b1">[2]</ref>, an integro-differential operator for parabolic localization is proposed as follows arg max a;b;c f ða; b; cÞ ¼ G r ðcÞ Ã @ @c Z C a;b;c Iðx; yÞ L a;b;c ds</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Selection of the best circle as the limbic boundary (for explanation please see the text).</figDesc><graphic coords="5,106.41,67.92,369.03,64.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Illustration of closed-eye determination.</figDesc><graphic coords="5,92.24,170.59,397.08,167.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. ROC curve for the skin classifier.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 10c and</head><label></label><figDesc>Fig.10c and dshows the filtered one-dimensional signal along the yellow line segment and its gradient, where Ã denotes the edgel detected.The proposed algorithm for locating the upper eyelid is explained below. The search ranges of parameters a; b; c are discretized by step a; step b; step c, respectively. For each three-tuple ða; b; cÞ in the search space, a RANSAC-like technique is used to determine whether the three-tuple is accepted as a potential candidate or just rejected. Given ða; b; cÞ, the number N p of the detected edgels p i ; i ¼ 1; . . . ; N e , is counted for which the distance of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Upper eyelid localization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. An example of specular highlight removal.</figDesc><graphic coords="7,44.00,70.85,495.79,115.27" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>P. Li et al. / Image and Vision Computing 28 (2010) 246-253</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported by the National Natural Science Foundation of China (No. 60673110), supported in part by the Program for New Century Excellent Talents of Heilongjiang Province (1153-NCET-002), the Science and Technology Project of Educational Bureau of Heilongjiang Province (1151G033), the Scientific Research Foundation for the Returned Overseas Chinese Scholars, State Education Ministry and Ministry of Personnel, the Science and Technology Innovation Research Project (2006RFLXG030) of Harbin Science and Technology Bureau.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Image understanding for iris biometrics: a survey</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hollingsworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Flynn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vision Image Understand</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="281" to="307" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">High confidence visual recognition of persons by a test of statistical independence</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Daugman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1148" to="1160" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="381" to="395" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Iris localization via pulling and pushing</title>
		<author>
			<persName><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="366" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Image indexing using color correlograms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Ravi</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zabih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="762" to="768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A study on eyelid localization considering image focus for iris recognition</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">K</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1698" to="1704" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Experiments with an improved iris segmentation algorithm</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Flynn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="118" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The NICE.I: noisy iris challenge evaluation -Part I</title>
		<author>
			<persName><forename type="first">H</forename><surname>Proença</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Alexandre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE First International Conference on Biometrics: Theory, Applications and Systems -BTAS 2007</title>
		<imprint>
			<biblScope unit="page" from="27" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Iris segmentation methodology for non-cooperative recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Proença</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Alexandre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEE Proc. Vision Image Signal Process</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">153</biblScope>
			<biblScope unit="page" from="199" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">UBIRIS: a noisy iris image database</title>
		<author>
			<persName><forename type="first">H</forename><surname>Proença</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Alexandre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 13th International Conference on Image Analysis and Processing -ICIAP 2005</title>
		<imprint>
			<biblScope unit="page" from="970" to="977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Robust real-time face detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="154" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Iris recognition:an emerging biometric technology</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1348" to="1363" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
