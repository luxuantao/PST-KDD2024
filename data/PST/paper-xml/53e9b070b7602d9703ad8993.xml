<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Split Tiling for GPUs: Automatic Parallelization Using Trapezoidal Tiles</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tobias</forename><surname>Grosser</surname></persName>
							<email>tobias.grosser@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">École Normale Supérieure</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Albert</forename><surname>Cohen</surname></persName>
							<email>albert.cohen@inria.fr</email>
							<affiliation key="aff1">
								<orgName type="institution">École Normale Supérieure</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Paul</forename><forename type="middle">H J</forename><surname>Kelly</surname></persName>
							<email>p.kelly@doc.ic.ac.uk</email>
							<affiliation key="aff2">
								<orgName type="institution">Imperial College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">J</forename><surname>Ramanujam</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Louisiana State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">P</forename><surname>Sadayappan</surname></persName>
							<email>sadayappan.1@osu.edu</email>
							<affiliation key="aff4">
								<orgName type="institution">Ohio State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sven</forename><surname>Verdoolaege</surname></persName>
							<email>sven.verdoolaege@inria.fr</email>
							<affiliation key="aff5">
								<orgName type="institution">École Normale Supérieure</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Split Tiling for GPUs: Automatic Parallelization Using Trapezoidal Tiles</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8EB073EFAB5235B08F42A1AF00215D7A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>D.3.4 [Programming Languages]: Processor-Compilers</term>
					<term>Optimization Algorithms</term>
					<term>Performance Polyhedral model</term>
					<term>GPGPU</term>
					<term>CUDA</term>
					<term>code generation</term>
					<term>compilers</term>
					<term>loop transformations</term>
					<term>index set splitting</term>
					<term>time tiling</term>
					<term>stencil</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Tiling is a key technique to enhance data reuse. For computations structured as one sequential outer "time" loop enclosing a set of parallel inner loops, tiling only the parallel inner loops may not enable enough data reuse in the cache. Tiling the inner loops along with the outer time loop enhances data locality but may require other transformations like loop skewing that inhibit inter-tile parallelism.</p><p>One approach to tiling that enhances data locality without inhibiting inter-tile parallelism is split tiling, where tiles are subdivided into a sequence of trapezoidal computation steps. In this paper, we develop an approach to generate split tiled code for GPUs in the PPCG polyhedral code generator. We propose a generic algorithm to calculate index-set splitting that enables us to perform tiling for locality and synchronization avoidance, while simultaneously maintaining parallelism, without the need for skewing or redundant computations. Our algorithm performs split tiling for an arbitrary number of dimensions and without the need to construct any large integer linear program. The method and its implementation are evaluated on standard stencil kernels and compared with a state-of-the-art polyhedral compiler and with a domain-specific stencil compiler, both targeting CUDA GPUs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION AND RELATED WORK</head><p>Advances in technology over the last few decades have yielded significantly different rates of improvement in the computational performance of processors relative to the cost of a memory access. Because of the significant mismatch between computational latency and throughput when compared to main memory latency and bandwidth, the use of hierarchical memory systems and the exploitation of significant data reuse in the higher levels (i.e., closest to the processor) of the memory hierarchy are critical for high performance. Tiling is a key loop-level compiler transformation technique to enable high data reuse in higher levels of the memory hierarchy.</p><p>The effective use of cache and scratchpad memory in GPUs is critical for high performance, since the computational performance (in GFlops) on current GPUs is well over an order of magnitude higher than the aggregate bandwidth to global memory (in Gigawords per second). While a sizable number of applications have been developed for GPUs by application developers using CUDA and OpenCL, the expertise and effort needed to develop GPU applications that reach high performance is limiting its potential use. Therefore, there is a growing recognition of the need to provide higher level programming abstractions for GPUs if their use is to be broadened.</p><p>The recent development of the OpenACC standard <ref type="bibr" target="#b16">[16]</ref> is an outcome of this recognition. With OpenACC, just as with the popular OpenMP model for shared-memory multi-core programming, the developer uses parallelization directives, retaining the control flow of a sequential program. The OpenACC compiler is responsible for (i) automatically off-loading the execution of the marked regions of code for execution on a GPU, (ii) handling the management of memory on the GPU, (iii) data transfers between the host and GPU memory, and (iv) the generation of effective GPU code. With current OpenACC compilers, especially when supported with user control over data regions, the compiler can be very effective in managing data transfers between CPU and GPU. However, many challenges remain in the automatic generation of effective GPU code for the marked OpenACC regions of the program.</p><p>Several recent efforts address the automatic parallelization of sequential code for GPUs <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b2">2,</ref><ref type="bibr" target="#b4">4,</ref><ref type="bibr" target="#b23">23]</ref> and many of the techniques developed in these projects can be expected to be incorporated into OpenACC compilers. In this paper, we focus on a class of computations for which effective tiled code generation for GPUs remains a challenge. The computational pattern we focus on is that of stencil computations, characterized by an outer sequential "time" loop that iterates over a sequence of inner parallel loops that sweep multi-dimensional grids with neighbor-based updates. When the multi-dimensional data grids are larger than cache capacity, untiled implementations of such codes result in repeated cache misses during each "time" loop iteration leadind to low performance that is limited by the available bandwidth to main memory. Thus, tiling also along the time dimension is critical to achieving high performance with such codes. Recent advances in polyhedral compilation and tools provide an effective solution to this problem for multi-core CPUs <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b5">5]</ref>. However, the effective schemes for timetiling on multi-core CPUs do not translate well to GPUs. Generation of high-performance time-tiled code for GPUs remains a challenge due to many issues, including the need to exploit high degrees of SIMD parallelism, limited synchronization capabilities in GPUs, the criticality of avoiding thread divergence, and the limited amount of cache and scratch-pad memory. Several recent studies focus on optimizing stencil computations for multicore CPUs and GPUs <ref type="bibr">[3, 6-8, 10, 11, 14, 18-20]</ref> Christen et al. <ref type="bibr" target="#b6">[6]</ref> develop the PATUS stencil compiler that uses a stencil description and a machine mapping description to generate efficient CPU and GPU code. The GPU support of PATUS is reported to be still preliminary and, in contrast to our approach, it does not perform any time tiling. Strzodka et al. <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b19">19]</ref> use time skewing with cache-size oblivious parallelograms to reduce memory system pressure and improve parallelism in stencils on CPUs, but do not address GPU-specific challenges. Han et al. <ref type="bibr" target="#b10">[10]</ref> develop pattern-based optimization of stencil codes on CPUs and GPUs using a proposed extension to OpenMP. Micikevicius et al. <ref type="bibr" target="#b14">[14]</ref> handtune a 3-D finite difference computation stencil. Datta et al. <ref type="bibr" target="#b7">[7,</ref><ref type="bibr">8]</ref> highlight the importance of auto-tuning and develop an optimization and auto-tuning framework for stencil computations, targeting multi-core systems, NVIDIA GPUs, and Cell SPUs. But Han et al., <ref type="bibr">Micikevicius et al. and Datta et al.</ref> do not consider time-tiled implementations on GPUs. Nguyen et al. <ref type="bibr" target="#b15">[15]</ref> develop a data blocking scheme that optimizes memory bandwidth and computation resources on GPU devices. Di and Xue <ref type="bibr" target="#b9">[9]</ref> address tile size selection for GPU kernels related to stencils. However, these works do not address automatic code generation for GPUs.</p><p>Tang et al. <ref type="bibr" target="#b20">[20]</ref> propose the Pochoir stencil compiler which uses a DSL embedded in C++ to produce high-performance code for stencil computations using cache-oblivious parallelograms for parallelism on x86 targets; they do not address issues specific to GPU code generation for stencil computations. Meng and Skadron <ref type="bibr" target="#b13">[13]</ref> develop a performance model for the evaluation of ghost zones for stencil computations on GPU architectures, relying on userprovided annotations for GPU code generation; they do not consider fully automated code generation.</p><p>The standard approach to time-tiling of stencil computations requires skewing of the iteration space along the spatial dimensions relative to the time dimension. Skewing makes subsequent rectangular tiling along spatial as well as the time dimension legal, thereby enabling time-tiling and enhanced data reuse. While this alleviates the impact of the memory bandwidth constraint for the time-tiled code, inter-tile dependencies are created along the spatial dimensions, restricting inter-tile parallelism to be only along diagonal wavefronts in the iteration space. In this paper, we develop a novel static code generation approach for regular stencils targetting GPUs using split tiling. The idea of alternating between different phases was inspired by the approach of Strout et al. <ref type="bibr" target="#b17">[17]</ref>, which dynamically generates sparse tiles for irregular stencils. Both split and overlapped tiling have been proposed as alternative tiling techniques to avoid the loss of inter-tile parallelism along the spatial dimensions <ref type="bibr" target="#b12">[12]</ref>. Overlapped tiling achieves this by introducing redundant computations. In contrast, split tiling does not require redundant computations, but introduces instead differently shaped tiles and the need for additional synchronization. Overlapped tiling is used to enhance tile-level concurrency on multicore systems by Krishnamoorthy et al. <ref type="bibr" target="#b12">[12]</ref>. Holewinski et al. <ref type="bibr" target="#b11">[11]</ref> use overlapped tiling to generate code for GPUs but do not consider split tiling. Hierarchical overlapped tiling is described by Zhou et al. <ref type="bibr" target="#b24">[24]</ref>, and evaluated in the context of OpenCL code generation but only for CPUs. To date, no compiler algorithms have been developed for the generation of effective split-tiled code for (multi-statement) stencil codes.</p><p>In this paper, we present a novel polyhedral approach for generating split-tiled code for GPUs. We propose a generic algorithm to calculate a schedule for index set splitting that enables us to perform tiling while simultaneously maintaining parallelism, without the need for skewing or redundant computations. Our algorithm performs split tiling for an arbitrary number of dimensions and without the need to construct any large integer linear programming problem instances. The method and its implementation are evaluated on several stencil kernels and compared with a state-of-the-art domain-specific GPU-stencil compiler. The newly developed algorithm is being incorporated into the publicly available PPCG tool flow <ref type="bibr" target="#b23">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">ILLUSTRATIVE EXAMPLE</head><p>In this section, we use a simple example to elaborate on the problem addressed in this paper. The following simple computation kernel contains fine-grained parallelism at the inner loop dimension.</p><formula xml:id="formula_0">f o r ( t = 1 ; t &lt; n ; t ++) f o r ( i = 1 ; i &lt; m -1 ; i ++) R : A[ t ] [ i ] = A[ t -1][ i -1] + A[ t -1][ i + 1 ] ;</formula><p>The polyhedral representation (visualized in Figure <ref type="figure" target="#fig_1">2</ref>) is:</p><formula xml:id="formula_1">D R = {A[t, i] : 1 ≤ t &lt; n ∧ 1 ≤ i &lt; m -1} S R = {A[t, i] → [t, i]} P R = {A[t, i] → A[t -1, i -1]; A[t, i] → A[t -1, i + 1]}.</formula><p>D R describes the set of statement instances executed in this kernel, S R maps each instance to an execution time and P R describes the dependences that must be respected when changing the execution order of different statement instances. Details on the polyhedral model and its representation can be found in <ref type="bibr" target="#b21">[21]</ref>. # pragma omp p a r a l l e l f o r s h a r e d ( t 1 , l b 1 , ub1 ) p r i v a t e ( t 2 , t 3 , t 4 )</p><formula xml:id="formula_2">f o r ( t 2 = l b 1 ; t 2 &lt;= ub1 ; t 2 ++) f o r ( t 3 = max ( max ( 1 , 3 2 * t 1 -32 * t 2 ) , 3 2 * t 2 -m+ 2 ) ; t 3 &lt;= min ( min ( n -1 ,32 * t 2 + 3 0 ) , 3 2 * t 1 -32 * t 2 + 3 1 ) ; t 3 ++) f o r ( t 4 = max ( 3 2 * t 2 , t 3 + 1 ) ; t 4 &lt;= min ( 3 2 * t 2 +31 , t 3 +m-2 ) ; t 4 ++) A[ t 3 ][ -t 3 + t 4 ] = A[ t 3 -1][-t 3 + t 4 -1]+A[ t 3 -1][-t 3 + t 4 + 1 ] ; ; }</formula><p>In the given form, tiling with rectangular tiles in the 2D iteration space is not feasible because we would have mutual interdependences between spatially adjacent tiles. The standard approach to tiling such a code is to first skew the iteration space so that no dependences are oriented along the negative direction along any dimension. The Pluto algorithm developed by Bondhugula et al. <ref type="bibr" target="#b5">[5]</ref> can generate such tiled code for any affine, imperfectly nested loop. For our example this yields the code in Listing 1, 1 whose execution is depicted in Figure <ref type="figure" target="#fig_1">2</ref>.</p><p>We can see that while iterations at a fixed time value are parallel for different values of i within a rectangular tile, there are intertile dependences between horizontally adjacent tiles. This inter-tile parallelism is not available along the i dimension. Inter-tile parallelism is only feasible among tiles in a common diagonal wavefront.</p><p>Such an automatic parallelization approach may be used effectively for multicore CPUs; however, it is not effective for GPUs, which require a very high degree of thread-level parallelism within a thread block and multiple concurrent thread blocks to utilize parallelism across the streaming multiprocessors in a GPU. We claim that wavefront parallelism is not optimal for GPUs, due to the low number of parallel tiles at the beginning and at the end of the computation. An even more effective approach using "diamond" tiles has very recently been developed for multi-core CPUs <ref type="bibr" target="#b3">[3]</ref>, but it 1 Generated with 'polycc -tile -parallel'.</p><p>remains to be evaluated on GPUs. As illustrated in the next section, the split tiling approach does not skew the iteration space but instead creates trapezoid shaped tiles of suitable shape so that they can be executed in a small number of phases. In each phase, we have a set of concurrently executable tiles with intra-tile parallelism that can be mapped to threads in a thread block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">SPLIT TILING OVERVIEW</head><p>Figure <ref type="figure" target="#fig_2">3</ref> illustrates split tiling for the previously discussed example. Two execution phases are needed for each time-tile. The green upright trapezoidal tiles in a time band can be executed in parallel. Each of these tiles is atomically executable since there are no incoming dependence edges from either neighboring purple tile. The only incoming dependences are from iteration points from a previous time band that have already completed. In mapping this to a GPU, different green tiles map to different thread blocks, and the points along the spatial dimension within each tile map to threads within a thread block. After all the green tiles in a time band have executed, all purple tiles in the same time band can execute concurrently, since all incoming dependences are either from points in the previous time band or points within a green tile in the same time band that have already executed.</p><p>Let us now provide an overview of the code generation approach for this simple example. Split tiling divides the iteration space into subspaces and tiles each subspace with a different schedule. For our example we define the subspaces T G , the green tile space, and T P , the purple tile space. The schedule for T G is S G and the schedule for T P is S P . The overall schedule S is a map where S G is used for all elements in T G and S P is used for all elements in T P . For each subspace we can now define a schedule that executes the convex subsets (our tiles) in parallel. S G and S P define such schedules. For our example we define T G , T P , S P and S P as follows (tile size 64 × 32):</p><formula xml:id="formula_3">T G = {A[t, i] : (∃i ,t : t = t mod 32 ∧ i = i mod 64 ∧ i -t ≤ 0 ∧ i + t ≤ 64 -2)} T P = {A[t, i] : (∃i ,t : t = t mod 32 ∧ i = i mod 64 ∧ i -t &lt; 64 ∧ i + t &gt; 64 -2)} S G = {A[t, i] → [t , 0, i ,t, i] : t = t/32 * 32 ∧ i = i/64 * 64} S P = {A[t, i] → [t , 1, i ,t, i] : t = t/32 * 32 ∧ i = (i -31)/64 * 64}</formula><p>Applying the schedule S with a code generator like CLooG yields the code in Listing 2. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">SPLIT TILING ALGORITHM</head><p>We present our new algorithm in steps, starting with the main idea applied to a single statement stencil, then generalizing it to multi-statement kernels, and refining the method with necessary optimizations. The algorithm is implemented in PPCG. 2   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Core algorithm</head><p>Given an iteration space and a set of dependences, we want to construct a schedule that implements split tiling. We start from a kernel where there is a single outermost loop that carries all dependences such that the remaining inner dimensions are fully parallel. Such a kernel is typical for a single statement stencil. As a first step, we partition the iteration space by placing equally distanced hyperplanes orthogonal to the axis of the time dimension. The different partitions form bands of fixed height (the height of the tiles). As the time dimension increases from band to band and as all dependences are carried by the time dimension, the bands can and must be executed sequentially. To obtain parallelism, we split the iterations within a single band into tiles of different colors, such that dependences may enforce an execution order between the different colors, but that within a single color all tiles can be executed in parallel.</p><p>To partition the band into different colors, we derive a tile shape for each color such that the full band can be covered with these shapes. The tile shape of the first color C 0 is constructed by choosing an arbitrary point X. X will be the apex of a pyramid that contains all iterations within the band that are needed to satisfy the (transitive) dependences of X. To construct this pyramid, we calculate the dependence distance vectors of the program and attach all of them to X. Together they form the upper part of a pyramid. We now extend the dependence vectors backward until their length along the time dimension matches the tile height we are aiming for. The convex hull of the extended dependence vectors forms a pyramid. This pyramid is the minimal set of points that we consider as the shape of the first color. In some cases it is preferable to have a shape that is wider along certain space dimensions. We can form 2 http://repo.or.cz/w/ppcg.git such wider shapes by "stretching" the initial pyramid along these space dimensions. Stretching along a dimension means to position two copies of the original shape, such that the positions of the copies only differ in the dimension along which we stretch. The stretched shape is now the convex hull of the two shapes.</p><p>In addition to the first color, we derive one color for each space dimension in the input. The shape of a color C x (where x corresponds to some space dimension) is derived by stretching the pyramid of C 0 along the x-dimension and by subsequently subtracting the shapes of all previously calculated colors.</p><p>In the case of more than one space dimension, additional colors are needed. Besides the initial color C 0 and the colors for individual dimensions, we introduce a color for each combination of dimensions. This means, for a 3D input, the colors C xy , C xz , C yz as well as C xyz are introduced. Their tile shapes are derived by stretching the initial pyramid along the set of dimensions they are named after. This can be compared to calculating the different faces of a cube, where the pyramid itself forms the shape of a vertex, the pyramids stretched along a single dimension form the differently oriented edges, the pyramids stretched along two dimensions form the facets and the pyramid stretched along all three dimensions forms the cube itself. Stretching the pyramid along more than one dimension (e.g., along x-y-z) is done recursively. We select one dimension (e.g., y) and calculate the union of the tile shapes that correspond to the colors of the remaining dimensions (here C xy , C x , C z , C 0 ). This union is then replicated along the selected dimension, the convex hull of the entire construct is calculated, and finally the previous colors as well as their replicated copies are subtracted.</p><p>The split tiling schedule is constructed by tiling the original iteration space with the previously calculated tile shapes, such that the sequential execution of the different bands as well as of the different colors is ensured. Tiles of the same color and within the same band are mapped to parallel dimensions. The iterations within the tiles are executed according to the original schedule. As the index set splitting is calculated without considering the bounds of the iteration space, there is no constraint on the shape of the iteration space. Only as the very last step, we constrain the schedule to the actual iteration space.</p><p>The left part of Figure <ref type="figure" target="#fig_6">4</ref> shows a split tiling of the Jacobi 2D kernel. The pyramid that forms the first color was placed in the center of the iteration space. At time step one (the upper left illustration) the number of elements in the first color is still large. When going down to time step two and three we are moving up the pyramid such that the number of elements executed becomes smaller. At time step three, color one consists only of a single point, the summit of the pyramid. The shape of color two forms the connection between two vertical neighbors of color one. The shape is nonconvex and resembles a simple butterfly. Color three now forms the horizontal connection between two neighboring shapes of color one. Color four, the last color constructed, fills the space enclosed by the previously calculated colors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Tile shape simplification</head><p>The previously introduced split-tiling algorithm starts from a single pyramid that exactly covers the dependence vectors. Depending on the dependence vectors, such a minimal pyramid may not always be parallel to the axes of the iteration space. In case it is not, such as in Figure <ref type="figure" target="#fig_6">4</ref>, subsequent tile shapes may have a nonconvex form. Such non-convex tile shapes are undesirable, not only because they increase the required amount of communication between the different tiles, but they also introduce more complex control flow structures. To avoid such complex control flow structures, we normally widen the original pyramid to create a rectan-Listing 2: Split tiled code f o r ( c1 = 0 ; c1 &lt;=M-1; c1 +=32) { l b = 0 ; ub = min (N-2 , c1 +N-3 ) ; #pragma omp p a r a l l e l f o r s h a r e d ( c1 , ub , l b ) p r i v a t e ( c3 , t , i ) </p><formula xml:id="formula_4">f o r ( c3 = l b ; c3 &lt;= ub ; c3 +=64) f o r ( t = max ( 1 , c1 ) ; t &lt;= min ( min (M-1 , c1 + 3 1 ) , c1-c3 +N-2 ) ; t ++) f o r ( i = max(1 , -c1 + c3 + t ) ; i &lt;= min (N-2 , c1 +c3-t + 6 2 ) ; i ++) A[ t ] [ i ] = A[ t -1][ i -1] + A[ t -1][ i +</formula><formula xml:id="formula_5">f o r ( i = max ( 1 , c1 +c3-i + 6 3 ) ; i &lt;= min (N-2,-c1 + c3 + t + 6 3 ) ; j ++) A[ t ] [ i ] = A[ t -1][ i -1] + A[ t -1][ i + 1 ]</formula><p>; } gular base. This can avoid the construction of non-convex tiles. Figure <ref type="figure" target="#fig_6">4</ref> illustrates the tile shapes resulting from the widening of the original pyramid to a rectangular base. As can been seen, this leads to greatly simplified (and convex) shapes. </p><formula xml:id="formula_6">f o r ( t = 0 ; t &lt; T ; t ++) f o r ( i = 0 ; i &lt; N ; i ++) A[ t ] [ i ] += A[ t -1][ i + 1 ] + A[ t -2][ i -3 ] ;</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Multi-statement loop nests</head><p>Up to this point, our split-tiling algorithm is only defined for kernels typical of single-statement stencils. In this section, we extend it to stencil computations that apply more than one statement in each iteration of the time loop. Such kernels have an additional sequential dimension with a known non-parametric number of iterations that models the lexicographic order of the different statements.</p><p>Figure <ref type="figure" target="#fig_5">6</ref> shows a simple two-statement kernel. Its iteration domain is {S[t, i] : 0 ≤ t &lt; T ∧ 0 ≤ i &lt; N; P[t, i] : 0 ≤ t &lt; T ∧ 0 ≤ i &lt; N} and its execution order is defined by the following schedule</p><formula xml:id="formula_7">{S[t, i] → [t, 0, i]; P[t, i] → [t, 1, i]}. The following dependences ex- ist: {S[t, i-2] → P[t, i]; P[t -1][i-1] → S[t, i]; S[t -1][i] → S[t][i]; P[t - 1][i] → S[t -1][i]}. Mapped into the scheduling space, this yields {[t, 0, i-2] → [t, 1, i]; [t -1, 1, i-1] → [t, 0, i]; [t -1, 0, i] → [t, 0, i]; [t - 1, 1, i] → [t, 1, i]}.</formula><p>By analyzing the dependences, we see that the two outermost dimensions both carry loop dependences. This means our split-tiling algorithm is not directly applicable.</p><p>By applying a simple pre-transformation we can canonicalize the code such that it is again possible to use the previously presented split-tiling algorithm. We detect that only the outermost time dimension can have parametric size related to the number of time steps executed. The size of the second sequential dimension is independent of the number of executed time steps. As the second dimension represents the lexical order of the statements in the source code, its size is bounded by the number of statements in the source code. As the integer value of this bound is available at compile time, we can fold the two time dimensions into a single one. For a two statement kernel this transformation can be described by the following mapping</p><formula xml:id="formula_8">{[t, 1, i] → [2t + 1, i] : 0 ≤ l ≤ 1}. Applying this mapping on the original schedule gives us {S[t, i] → [2t, i]; P[t, i] → [2t + 1, i]} as well as the dependences {[2t, i -2] → [2t + 1, i]; [2t -1, i -1] → [2t, i]; [t -2, i] → [t, i]}.</formula><p>After this transformation, all dependences are again carried by the outermost dimensions and the inner parallel dimensions remain unchanged. Now, the previously presented split-tiling algorithm can be applied. </p><formula xml:id="formula_9">f o r ( t = 0 ; t &lt; T ; t ++) { f o r ( i = 0 ; i &lt; N; i ++) S : A [ 1 ] [ i ] += A [ 0 ] [ i + 1] f o r ( i = 0 ; i &lt; N; i ++) P : A [ 0 ] [ i ] += A [ 1 ] [ i + 2] }</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CUDA CODE GENERATION</head><p>To generate CUDA code, we extended the polyhedral GPU code generator PPCG <ref type="bibr" target="#b23">[23]</ref>. PPCG is a state-of-the-art GPU code generator that translates static control programs in C to CUDA enabled programs. For certain classes of computations (e.g., linear algebra kernels), this produces very efficient code that reaches the performance of highly-tuned libraries. For stencil computations, PPCG performs a basic mapping where for each time step, a new kernel instance is spawned and each kernel applies a single stencil to just a couple of data points. This mapping exposes a high level of parallelism, but at each time step all data is read from and written to global memory. This means the global memory bandwidth becomes the limiting factor of the stencil computation.</p><p>By adding sipport for split-tiling, we enabled PPCG to produce time-tiled CUDA code for stencil like computations. Such code executes several iterations of the time loop within each kernel and keeps intermediate results in shared memory. This significantly lowers the pressure on the global memory bandwidth and consequently allows a higher computational throughput.</p><p>The split-tiling support for PPCG was developed by enhancing  and parameterizing the polyhedral optimization infrastructure already available in PPCG. We specifically avoided the development of a domain-specific code generator, but aimed instead at enhancing an existing GPU optimization framework. From a user's point of view, this provides a smoother experience as the same framework can be used for a wide range of kernels. The only difference is that it is now possible to obtain improved code for stencil computations. From the developer's point of view, the use of a uniform optimization framework has several benefits. Developing on top of an existing infrastructure speeds up the development of the CUDA code generator. It also enabled us to develop generic features and optimizations that can be beneficial for PPCG itself, but that show immediate benefits for split tiling if parameterized accordingly. The uniform framework makes it again very easy to specify and communicate the necessary parameters to the relevant transformations.</p><p>When generating split-tiled CUDA code we start from the C code of the program. This code is read by the polyhedral extraction tool pet <ref type="bibr" target="#b22">[22]</ref> which is available from within PPCG. Based on the extracted polyhedral description, we check if the program is suitable for split tiling. If this is the case, we derive a split-tiled schedule according to the generic algorithm described above. This new schedule is now provided to the generic PPCG transformation infrastructure where it replaces the PPCG internal schedule optimizer as well as the PPCG internal tiling. Instead, we parameterize PPCG with information about the schedule we provide. This information includes the number of dimensions of the entire schedule, the number of outer loops that should be executed on the host side, the first parallel loop that should be mapped to the GPU, the dimensionality of the tiles, the number of parallel dimensions in the tiles as well as information about the number of dimensions that should be considered when keeping values in shared memory.</p><p>PPCG uses this information to map the split tiles to the GPU. The mapping itself is rather straightforward. The tile loop of the time dimension is generally kept in the host code where it loops over a sequence of kernel calls. Each kernel call executes a set of thread blocks which in turn execute the parallel tiles as available at a certain time point using a one-to-one mapping from tiles to thread blocks. Within a tile, the parallel loops that enumerate the space dimensions are mapped to individual threads in a way that ensures coalesced memory accesses. The non-parallel loop for the time dimension is executed sequentially in each kernel. __synchthreads calls are introduced to ensure that each time step is finished before the next one is started.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Shared memory usage</head><p>The most important optimization for split tiling is the use of shared memory. The standard code that PPCG generates for stencils only uses shared memory to take advantage of special reuse within a single calculation. Such special reuse rarely happens for stencils and the additional synchronization overhead often outweighs the benefits of shared memory usage. However, with split-tiling, we can now take advantage of reuse along the time dimension. This means, all calculations within a single tile can be performed in shared memory. Only at the beginning and at the end of the kernel execution, the used values need to be loaded from and stored back into global memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Instruction level parallelism</head><p>On CUDA architectures several kinds of parallelism are available. Parallelism due to the execution of parallel threads is the most obvious one. However, even when generating code without split tiling, PPCG maps by default several data points to a single thread. Mapping several data points to a single thread ensures that there is a certain number of instructions between two subsequent __syncthreads calls. Exposing this instruction level parallelism is beneficial as it helps to hide memory access latency. Our split tiling implementation uses loop unrolling to increase the amount of available instruction level parallelism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Full/partial tile separation</head><p>One way to avoid overhead due to the evaluation of boundary conditions is to use full/partial tile separation. The idea here is to generate specialized code for full tiles as well as for tiles that intersect with the iteration space boundary (i.e., partial tiles). Due to the absence of checks for the iteration space boundaries, the code of the full tiles evaluates a lot less conditionals. This is beneficial not only due to the reduced number of evaluated conditions, but also as it opens up new possibilities for loop-invariant code motion. Our split-tiling compiler automatically performs full/partial tile separation.  To evaluate the performance of our split-tiling implementation we compare it against two compilers: (i) PPCG <ref type="bibr" target="#b23">[23]</ref>, a state-of-theart GPU code generator; (and) (ii) Holewinski et al.'s <ref type="bibr" target="#b11">[11]</ref> recent GPU stencil compiler, that we refer to as overtile (since it uses overlapped tiling).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">EVALUATION</head><p>We evaluate on a couple of different kernels consisting of jacobi-1d (3pt, 5pt, 7pt), jacobi-2d (5pt) as well as a 9pt poisson solver. The experiments are run on a NVIDIA GeForce GTX 470 desktop GPU as well as on a NVIDIA NVS 5200M mobile GPU. The performance is shown in GFLOPS and includes both the time of the computation itself as well as the data transfer to the GPU.</p><p>The results in Figure <ref type="figure" target="#fig_8">7</ref> show that the split-tiled code is consistently faster than both PPCG and overtile. Especially for the one dimensional test cases, we can see an almost 4x improvement over PPCG and between 30% and 40% over overtile. For the 2D cases on the mobile GPU, the performance of the different tools is again closer. On the desktop GPU, on the other hand, the difference between split tiling as well as PPCG and overtile is again considerable. 3  3 Unfortunately, we cannot report overtile numbers for the poisson solver, as nvcc 4 and 5 both crashed on the CUDA file that overtile produced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSIONS</head><p>We have presented a novel polyhedral method for generating split-tiled code for GPUs. It is based on an index-set splitting which is directly derived from dependence vectors, without the need of any additional algorithm to calculate valid tiling hyperplanes. Our algorithm has been implemented in a prototype extension of the PPCG tool flow. It is capable of tiling multi-statement stencils over both time and parallel loops, achieving excellent multi-level exploitation of the parallelism and local memory resources of a modern GPU. Future work includes the investigation of hybrid schemes for imperfectly nested loops, combining affine transformations with index-set splitting <ref type="bibr" target="#b3">[3]</ref>, evaluating the impact on a wider range of CPU and GPU targets, and providing a formal definition of the algorithms.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The iteration space of the simple kernel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Pluto generated code for the simple kernel (tile size 3 × 4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Split tiling for the simple example (tile size 10 × 4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>1 ] ; l b = max( -64 , -64 * f l o o r d (-c1 +M-3 ,64) -64); ub = min (N-34,-c1 +M+N-6 6 ) ; #pragma omp p a r a l l e l f o r s h a r e d ( c1 , ub , l b ) p r i v a t e ( c3 , t , i ) f o r ( c3 = l b ; c3 &lt;= ub ; c3 +=64) f o r ( t = max ( max ( max ( 1 , c1 ) , c1-c3 -62) , c1 +c3-N+ 6 5 ) ; t &lt;= min (M-1 , c1 + 3 1 ) ; t ++)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Example</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Two statement kernel</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Split tiled jacobi-2d kernel</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>jacobi-1d-3ptjacobi-1d-5pt jacobi-1d-7pt jacobi-2d-5pt poisson-2dof split-tiling on NVS 5200M split ppcg overtile</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Performance results on different stencil compilers</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. This work is partly funded by a Google European Fellowship in Efficient Computing, by the European FP7 project CARP id. 287767 as well as EPSRC (Project ref EP/I00677X/1).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Static compilation analysis for host-accelerator communication optimization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Coelho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Irigoin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Keryell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Languages and Compilers for Parallel Computing (LCPC&apos;11)</title>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2011-10">Oct. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Par4all: From convex array regions to heterogeneous computing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Creusillet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Even</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Keryell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Goubier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guelton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">O</forename><surname>Mcmahon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">X</forename><surname>Pasquier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Péan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Villalon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012-01">Jan. 2012</date>
			<pubPlace>Paris, France</pubPlace>
		</imprint>
	</monogr>
	<note>In IMPACT&apos;12</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Tiling stencil computations to maximize parallelism</title>
		<author>
			<persName><forename type="first">V</forename><surname>Bandishti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Pananilath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Bondhugula</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SC &apos;12</title>
		<meeting>SC &apos;12<address><addrLine>Los Alamitos, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Automatic c-to-cuda code generation for affine programs</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Baskaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ramanujam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sadayappan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th joint European conference on Theory and Practice of Software, international conference on Compiler Construction, CC&apos;10/ETAPS&apos;10</title>
		<meeting>the 19th joint European conference on Theory and Practice of Software, international conference on Compiler Construction, CC&apos;10/ETAPS&apos;10<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="244" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A practical automatic polyhedral parallelizer and locality optimizer</title>
		<author>
			<persName><forename type="first">U</forename><surname>Bondhugula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hartono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ramanujam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sadayappan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM SIGPLAN conference on Programming language design and implementation, PLDI&apos;08</title>
		<meeting>the 2008 ACM SIGPLAN conference on Programming language design and implementation, PLDI&apos;08<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="101" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Patus: A code generation and autotuning framework for parallel iterative stencil computations on modern microarchitectures</title>
		<author>
			<persName><forename type="first">M</forename><surname>Christen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Schenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Burkhart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 IEEE International Parallel &amp; Distributed Processing Symposium, IPDPS &apos;11</title>
		<meeting>the 2011 IEEE International Parallel &amp; Distributed Processing Symposium, IPDPS &apos;11<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="676" to="687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Optimization and performance modeling of stencil computations on modern microprocessors</title>
		<author>
			<persName><forename type="first">K</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kamil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Oliker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shalf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Yelick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Review</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="129" to="159" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Stencil computation optimization and auto-tuning on state-of-the-art multicore architectures</title>
		<author>
			<persName><forename type="first">K</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Volkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Oliker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shalf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yelick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SC &apos;08</title>
		<meeting>SC &apos;08<address><addrLine>Piscataway, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Model-driven tile size selection for DOACROSS loops on GPUs</title>
		<author>
			<persName><forename type="first">P</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th international conference on Parallel processing -Volume Part II, Euro-Par&apos;11</title>
		<meeting>the 17th international conference on Parallel processing -Volume Part II, Euro-Par&apos;11<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="401" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Pads: A pattern-driven stencil compiler-based tool for reuse of optimizations on gpgpus</title>
		<author>
			<persName><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPADS</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="308" to="315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">High-performance code generation for stencil computations on gpu architectures</title>
		<author>
			<persName><forename type="first">J</forename><surname>Holewinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-N</forename><surname>Pouchet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sadayappan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM international conference on Supercomputing, ICS &apos;12</title>
		<meeting>the 26th ACM international conference on Supercomputing, ICS &apos;12<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="311" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Effective automatic parallelization of stencil computations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Krishnamoorthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Baskaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Bondhugula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ramanujam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rountev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sadayappan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 ACM SIGPLAN conference on Programming language design and implementation, PLDI&apos;07</title>
		<meeting>the 2007 ACM SIGPLAN conference on Programming language design and implementation, PLDI&apos;07<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="235" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A performance study for iterative stencil loops on gpus with ghost zone optimizations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Skadron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Parallel Programming</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="115" to="142" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">3d finite difference computation on gpus using cuda</title>
		<author>
			<persName><forename type="first">P</forename><surname>Micikevicius</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2nd Workshop on General Purpose Processing on Graphics Processing Units, GPGPU-2</title>
		<meeting>2nd Workshop on General Purpose Processing on Graphics Processing Units, GPGPU-2<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="79" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">3.5-d blocking optimization for stencil computations on modern cpus and gpus</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Satish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chhugani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dubey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SC &apos;10</title>
		<meeting>SC &apos;10<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m">The OpenACC standard</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Combining performance aspects of irregular gauss-seidel via sparse tiling</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Strout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ferrante</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kreaseck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LCPC</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="90" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cache oblivious parallelograms in iterative stencil computations</title>
		<author>
			<persName><forename type="first">R</forename><surname>Strzodka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shaheen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pajak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICS</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="49" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Cache accurate time skewing in iterative stencil computations</title>
		<author>
			<persName><forename type="first">R</forename><surname>Strzodka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shaheen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pajak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">41st International Conference on Parallel Processing</title>
		<imprint>
			<date type="published" when="2011">2012. 2011</date>
			<biblScope unit="volume">0</biblScope>
			<biblScope unit="page" from="571" to="581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The pochoir stencil compiler</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Kuszmaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-K</forename><surname>Luk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Leiserson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM symposium on Parallelism in algorithms and architectures, SPAA &apos;11</title>
		<meeting>the 23rd ACM symposium on Parallelism in algorithms and architectures, SPAA &apos;11<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="117" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Counting affine calculator and applications</title>
		<author>
			<persName><forename type="first">S</forename><surname>Verdoolaege</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">First International Workshop on Polyhedral Compilation Techniques (IMPACT&apos;11)</title>
		<meeting><address><addrLine>Charmonix, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-04">Apr. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Polyhedral extraction tool</title>
		<author>
			<persName><forename type="first">S</forename><surname>Verdoolaege</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Grosser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Second International Workshop on Polyhedral Compilation Techniques (IMPACT&apos;12)</title>
		<meeting><address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-01">Jan. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Polyhedral parallel code generation for CUDA</title>
		<author>
			<persName><forename type="first">S</forename><surname>Verdoolaege</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Juega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">I</forename><surname>Gómez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tenllado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Catthoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Architecture and Code Optimization(TACO)</title>
		<imprint>
			<date type="published" when="2012-12">Dec. 2012</date>
		</imprint>
	</monogr>
	<note>Selected for presentation at the HiPEAC 2013 Conf</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hierarchical overlapped tiling</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-P</forename><surname>Giacalone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Garzarán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Padua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th Intl. Symp. Code Gen. and Opt., CGO &apos;12</title>
		<meeting>the 10th Intl. Symp. Code Gen. and Opt., CGO &apos;12<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="207" to="218" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
