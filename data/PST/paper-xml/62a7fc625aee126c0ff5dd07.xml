<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bilateral Dependency Optimization: Defending Against Model-inversion Attacks</title>
				<funder>
					<orgName type="full">RIKEN Collaborative Research Fund and HKBU CSD Departmental Incentive Grant</orgName>
				</funder>
				<funder ref="#_SMQzCWY">
					<orgName type="full">Japan</orgName>
				</funder>
				<funder ref="#_RAHGAnh">
					<orgName type="full">JSPS Grants-in-Aid for Scientific Research (KAKENHI)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiong</forename><surname>Peng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Feng</forename><surname>Liu</surname></persName>
							<email>fengliu.ml@gmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Jingfen</forename><surname>Zhang</surname></persName>
							<email>jingfeng.zhang@riken.jp</email>
						</author>
						<author>
							<persName><forename type="first">Long</forename><surname>Lan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Junjie</forename><surname>Ye</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
							<email>tongliang.liu@sydney.edu.au</email>
						</author>
						<author>
							<persName><forename type="first">Bo</forename><surname>Han</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Hong Kong Baptist University Hong Kong SAR</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">The University of Melbourne Melbourne</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">RIKEN AIP Tokyo</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Hong Kong Baptist University Hong Kong SAR</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">The Hong Kong Polytechnic University Hong Kong SAR</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">The University of Sydney Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution">Hong Kong Baptist University Hong Kong SAR</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="department">KDD &apos;22</orgName>
								<address>
									<addrLine>August 14-18</addrLine>
									<postCode>2022</postCode>
									<settlement>Washington</settlement>
									<region>DC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Bilateral Dependency Optimization: Defending Against Model-inversion Attacks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3534678.3539376</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep neural networks</term>
					<term>model-inversion attacks</term>
					<term>privacy leakage</term>
					<term>statistical dependency</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Through using only a well-trained classifier, model-inversion (MI) attacks can recover the data used for training the classifier, leading to the privacy leakage of the training data. To defend against MI attacks, previous work utilizes a unilateral dependency optimization strategy, i.e., minimizing the dependency between inputs (i.e., features) and outputs (i.e., labels) during training the classifier. However, such a minimization process conflicts with minimizing the supervised loss that aims to maximize the dependency between inputs and outputs, causing an explicit trade-off between model robustness against MI attacks and model utility on classification tasks. In this paper, we aim to minimize the dependency between the latent representations and the inputs while maximizing the dependency between latent representations and the outputs, named a bilateral dependency optimization (BiDO) strategy. In particular, we use the dependency constraints as a universally applicable regularizer in addition to commonly used losses for deep neural networks (e.g., cross-entropy), which can be instantiated with appropriate dependency criteria according to different tasks. To verify the efficacy of our strategy, we propose two implementations of BiDO, by using two different dependency measures: BiDO with constrained covariance (BiDO-COCO) and BiDO with Hilbert-Schmidt Independence Criterion (BiDO-HSIC). Experiments show that BiDO achieves the * Equal contribution.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>With the development of Machine learning (ML) algorithms, deep neural networks (DNNs) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b33">34]</ref> are increasingly adopted in various privacy-sensitive applications, such as facial recognition <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b32">33]</ref>, medical diagnoses <ref type="bibr" target="#b21">[22]</ref>, and intelligent virtual assistants <ref type="bibr" target="#b23">[24]</ref>. Since training DNNs could involve processing sensitive and proprietary datasets in privacy-related applications, there are great concerns about privacy leakage. To protect the privacy of individuals whose personal information is used during the training, enterprises typically release only well-trained DNNs through MLas-a-services platforms, wherein users can download pre-trained models (e.g., Pytorch Hub) or query the model via some sort of programming or user interfaces (e.g., Amazon Recognition), which are referred to as white-box access and black-box access, respectively. ), the dependency between inputs and outputs, into the training objective as a regularizer (the previous method to defend against model-inversion attacks <ref type="bibr" target="#b35">[36]</ref>). We use constrained covariance as the dependency measure ? (?, ? ), ? denotes the balancing hyper-parameter of the regularization term. It is clear that the previous method causes an explicit accuracy drop (from ? 85% (a) to ? 0.15% (b)) when we increase ? beyond some threshold (from ? = 5 (a) to ? = 20 (b)).</p><p>Unfortunately, the white-box access or black-box access can still cause privacy leakage. Recent studies have revealed that DNNs may expose sensitive information in the training data under various privacy attacks <ref type="bibr" target="#b24">[25]</ref>. For example, in the growing paradigm of federated learning <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b39">40]</ref>, wherein a malicious collaborator could infer private information of other data centers through their uploaded model updates (i.e., gradients) <ref type="bibr" target="#b9">[10]</ref>. More importantly, by using only a well-trained classifier (i.e., a target model), modelinversion (MI) attacks can recover the data used for training the classifier, leading to the privacy leakage of the training data.</p><p>The first MI attack <ref type="bibr" target="#b4">[5]</ref> has been proposed to predict genetic markers which are used as part of the input features, where an adversary was given the model and other possible auxiliary information. In recent works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b38">39]</ref>, MI attacks have been widely adopted to recover facial images of any person from a well-trained face recognition model, once the adversary has successfully reconstructed the facial images of individuals in the private training set, they could abuse them to break into otherwise secure systems.</p><p>Since the MI adversary exploits the correlation between model inputs and outputs for successful attacks, a recent work <ref type="bibr" target="#b35">[36]</ref> named mutual information regularization based defense (MID) (see Figure <ref type="figure" target="#fig_5">2(a)</ref>) utilizes a unilateral dependency optimization strategy, i.e., minimizing the dependency between inputs and outputs during training the model. Specifically, they introduce a mutual information regularizer into the training objective, which penalizes the mutual information between model inputs and outputs.</p><p>However, such a minimization process conflicts with minimizing the supervised loss that aims to maximize the dependency between inputs and outputs, causing an explicit trade-off between model robustness against MI attacks and model utility on classification tasks. We further conducted experiments to verify this claim, as illustrated in Figure <ref type="figure" target="#fig_0">1</ref>, once the balancing hyper-parameter ? of the dependency term crosses some threshold, the model cannot learn anything from the training set. Besides, since mutual information is notoriously expensive to compute, MID resorted to the variational approximations of the mutual information term rather than the actual quantity.</p><p>In this paper, we propose to minimize the dependency between the latent representations and the inputs while maximizing the dependency between latent representations and the outputs, named a bilateral dependency optimization (BiDO) strategy (see Figure <ref type="figure" target="#fig_5">2(b)</ref>). The former constraint limits redundant information propagated from the inputs to the latent representations which can be exploited by the adversary, thus improving the model's ability to prevent privacy leakage; while the latter drives the latent layers to learn discriminative representations which ensure model utility on classification tasks.</p><p>Specifically, we use the dependency constraints as a universally applicable regularizer in addition to commonly used losses for DNNs (e.g., cross-entropy), which can be realized with appropriate dependence criteria according to different tasks. To verify the efficacy of our strategy, we propose two implementations of BiDO, by using two different dependency measures: BiDO with constrained covariance <ref type="bibr" target="#b7">[8]</ref> (BiDO-COCO) and BiDO with Hilbert-Schmidt independence criterion <ref type="bibr" target="#b6">[7]</ref> (BiDO-HSIC).</p><p>Experiments show that BiDO achieves the state-of-the-art defense performance for a variety of datasets (including CelebA <ref type="bibr" target="#b20">[21]</ref>, MNIST <ref type="bibr" target="#b15">[16]</ref> and CIFAR-10 <ref type="bibr" target="#b14">[15]</ref>), target models (including LeNet <ref type="bibr" target="#b15">[16]</ref>, VGG16 <ref type="bibr" target="#b28">[29]</ref>, and ResNet-34 <ref type="bibr" target="#b8">[9]</ref>) and MI attacks (including generative MI attack (GMI) <ref type="bibr" target="#b38">[39]</ref>, knowledge-enriched distributional MI attack (KED-MI) <ref type="bibr" target="#b1">[2]</ref> and variational MI attack (VMI) <ref type="bibr" target="#b34">[35]</ref>), while suffering a minor classification-accuracy drop compared to the well-trained classifier with no defense. As a highlight, when we evaluate the defense performance on CelebA against KED-MI, BiDO-HSIC reduces attack accuracy for more than 29%, top-5 attack accuracy for more than 20% while only suffering ? 6% classification accuracy decline (see Table <ref type="table">2</ref>). These empirical results show the effectiveness of BiDO and light up a novel road to defend against MI attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS</head><p>In this section, we briefly review MI attacks, the defense strategies against MI attacks, and dependency measures that are used to see how two random variables are dependent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Model-inversion Attacks</head><p>The general goal of privacy attacks against the ML model is to extract information about the training data or to extract the model itself (model extraction attacks <ref type="bibr" target="#b13">[14]</ref>). The attacks related to exploiting training data information can be further categorized into membership inference attacks <ref type="bibr" target="#b2">[3]</ref>, model inversion attacks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b38">39]</ref>, and property inference attacks <ref type="bibr" target="#b29">[30]</ref>, each with a different specific goal. In this paper, we focus on the defense mechanism against MI attacks, which aims at inferring sensitive information in the training set given only access to a well-trained target model.</p><p>The first MI attack was proposed in the context of genomic privacy <ref type="bibr" target="#b4">[5]</ref>, where the authors try to infer private genomic attributes about individuals given the linear regression model that uses them as input features, the response of the model, as well as other nonsensitive features of the input. Algorithmically, they formulated the MI attack as an optimization problem in the input space, seeking for sensitive features that achieve the maximum likelihood under the target model. Fredrikson et al. <ref type="bibr" target="#b3">[4]</ref> extended the attack algorithm to more challenging tasks, e.g., recovering facial images of a person from a face recognition model, and more complex target models, e.g., decision trees and shallow neural networks.</p><p>However, when the attack scenarios involve recovering private data which lies in high-dimensional and continuous data spaces (e.g., image-based spaces), their attack algorithm failed, since directly optimizing over the input space could result in imperceptible and unnatural features. To handle this problem, MI attacks based on deep generators <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b22">23]</ref> were proposed <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b38">39]</ref>. GMI <ref type="bibr" target="#b38">[39]</ref> proposed a two-stage attack framework, which first pre-trains generative adversarial networks (GANs) <ref type="bibr" target="#b22">[23]</ref> on a public auxiliary dataset to distill generic prior, and then uses it to guide the gradient-based attack in the latent space of the generator. KED-MI <ref type="bibr" target="#b1">[2]</ref> addressed two limitations in GMI. First, they leverage the target model to label the public dataset and train the discriminator to differentiate not only the real and fake samples but also the labels, which enables the generator to distill the private knowledge customized for the specific classes of interest in the target model; second, instead of generating a representative image for a given label, they propose to explicitly parameterize the private data distribution and solve the attack optimization over the distributional parameters. VMI <ref type="bibr" target="#b34">[35]</ref> views the MI attack problem as a variational inference problem and provides a unified framework for analyzing existing MI attacks from a Bayesian probabilistic perspective.</p><p>Moreover, Yang et al. <ref type="bibr" target="#b36">[37]</ref> studied the black-box MI attack and trained a separate generator that swaps inputs and outputs of the target model, using an architecture of autoencoder. Salem et al. <ref type="bibr" target="#b25">[26]</ref> studied the black-box MI attack for online learning, where the adversary has access to the versions of the target model before and after an online update and the goal is to recover the training data used to perform the update.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Defending against MI Attacks</head><p>Improving model robustness against MI attacks is critical to the privacy protection of training data, however, the research on such defense mechanisms is still limited. Differential privacy (DP) is a widely adopted privacy-protection technique whose effectiveness is theoretically guaranteed. In <ref type="bibr" target="#b4">[5]</ref>, DP was used to add noise to the objective function of the optimization problem, while in <ref type="bibr" target="#b38">[39]</ref> DP was used to add noise to the gradient during the optimization of the model. Despite the rigorous theoretical privacy guarantee of DP, the aforementioned works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b38">39]</ref> have experimentally shown that current DP mechanisms do not mitigate the MI attacks while retaining desirable model utility at the same time.</p><p>In a recent work, Wang et al. <ref type="bibr" target="#b35">[36]</ref> introduced mutual information regularization based defense (MID) against MI attacks, which also provides insights into the empirical inefficacy of differential privacy techniques from a theoretical viewpoint. They propose to limit the dependency between the model inputs and outputs directly by introducing a mutual information regularizer into the training objective, which penalizes the mutual information between the inputs and outputs during the training process. However, since mutual information is unfortunately intractable in continuous and high-dimensional settings, MID resorted to mutual information approximations rather than the actual quantity. Moreover, though limiting correlation between the model inputs and outputs seems intuitive to defend against MI attacks, such a minimization process conflicts with minimizing the supervised loss that aims to maximize the dependency between inputs and outputs, causing an explicit trade-off between model robustness against MI attacks and model utility on classification tasks.</p><p>There are also defense mechanisms for black-box attacks that are relatively easy to be defended. In this setting, the defender can only modify output confidence scores to weaken the correlation between the inputs and outputs. Such a modification could be injecting noise into the confidence scores <ref type="bibr" target="#b25">[26]</ref>, reducing their precision <ref type="bibr" target="#b4">[5]</ref>, or purifying the distinguishable pattern among them <ref type="bibr" target="#b37">[38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Statistical Dependency Measures</head><p>Measuring dependency among random variables is well established in statistics. Well-known measures include constrained covariance <ref type="bibr" target="#b7">[8]</ref>, kernel canonical correlation <ref type="bibr" target="#b0">[1]</ref>, and Hilbert-Schmidt independency criterion <ref type="bibr" target="#b6">[7]</ref>. Recently, researchers have found various applications of these measures, where they are utilized to force DNNs to learn discriminative features. Examples include self-supervised learning of image representations <ref type="bibr" target="#b17">[18]</ref>, causal representation learning <ref type="bibr" target="#b26">[27]</ref>, clustering algorithms <ref type="bibr" target="#b31">[32]</ref> and feature selection <ref type="bibr" target="#b30">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PRELIMINARIES</head><p>In this section, we present the definitions of two statistical dependency measures that we used to realize the BiDO defense mechanism, namely the constrained covariance (COCO) <ref type="bibr" target="#b7">[8]</ref> and the Hilbert-Schmidt independency criterion (HSIC) <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Constrained Covariance (COCO)</head><p>COCO is a kernel-based dependency measure between random variables. Given function classes F , G containing subspaces ? ? F and ? ? G, and a probability measure P ?? , COCO is defined as:</p><formula xml:id="formula_0">COCO(?, ? ) = sup ? ??,? ?? [E(? (? )?(? )) -E(? (? )) E(?(? ))] . (1)</formula><p>For the sets of continuous functions F and G that are bounded by 1, COCO(?, ? ) = 0 if and only if ? and ? are independent <ref type="bibr" target="#b7">[8]</ref>, and larger values of the measure correspond to stronger dependency.</p><p>Given samples {(? ? , ? ? )} ? ?=1 drawn i.i.d. from P ?? , the empirical estimate of COCO, COCO(?, ? ), is defined as:</p><formula xml:id="formula_1">sup ? ??,? ?? ? ? ? ? ? ? 1 ? ? ?? ?=1 ? (? ? )?(? ? ) - 1 ? 2 ? ?? ?=1 ? (? ? ) ? ?? ?=1 ?(? ? ) ? ? ? ? ? ? .</formula><p>When F and G are reproducing kernel Hilbert spaces (RKHS <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref>), inner products in an RKHS are by definition kernel functions: ? (? ? , ? ? ) = ? (? ? ), ? (? ? ) F and ? (? ? , ? ? ) = ? (? ? ),? (? ? ) G . Given ? and ? their respective unit balls, then</p><formula xml:id="formula_2">COCO(?, ? ) = 1 ? ?? ? ? 2 ,<label>(2)</label></formula><p>where ? = ??? and ? = ??? , ? ? ? = ? (? ? , ? ? ) and ? ? ? = ? (? ? , ? ? ) are the kernel matrices, and ? = ? -1 ? 11 ? is the centering matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Hilbert-Schmidt Independency Criterion (HSIC)</head><p>HSIC is another kernel-based measure which improved from COCO. For a universal kernel, HSIC(?, ? ) = 0 if and only if ? and ? are independent. The larger value of HSIC represents the stronger dependency. HSIC measures the dependency between ? and ? by first taking a nonlinear feature transformation of each, say ? : X ? F and ? : Y ? G (with F and G RKHS), and then evaluating the norm of the cross-covariance between those features:</p><formula xml:id="formula_3">HSIC(?, ? ) = E[? (? ) ? (? ) ? ] -E[? (? )] E[? (? )] ? 2 HS .</formula><p>(3) Here ??? HS is the Hilbert-Schmidt norm, which in finite dimensions is the usual Frobenius norm. HSIC measures the scale of the correlation in these nonlinear features, which allows it to identify nonlinear dependencies between ? and ? with appropriate features ? and ? . Let (? ? , ? ? ), (? ?? , ? ?? ) be independent copies of (?, ? ), we can compute the HSIC value using the following equation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HSIC(?, ?</head><formula xml:id="formula_4">) = E ? (?, ? ? )? (?, ? ? ) -2 E ? (?, ? ? )? (?, ? ?? ) + E ? (?, ? ? ) E ? (?, ? ? ) .<label>(4)</label></formula><p>HSIC is also straightforward to estimate: given samples {(? ? , ? ? )} ? ?=1 drawn i.i.d. from the joint distribution of (?, ? ), Gretton et al. <ref type="bibr" target="#b6">[7]</ref> propose an estimator</p><formula xml:id="formula_5">HSIC(?, ? ) = 1 (? -1) 2 Tr(???? ) ,<label>(5)</label></formula><p>where ? ? ? = ? (? ? , ? ? ) and ? ? ? = ? (? ? , ? ? ) are the kernel matrices, and ? is the centering matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DEFENSE VIA BILATERAL DEPENDENCY OPTIMIZATION</head><p>This section shows our defense strategy against MI attacks. First, we will introduce some key concepts in defending against MI attacks. The model owner has a validation dataset D val to test the classifier performance, which is drawn from the same underlying private data distribution as D tr . The goal of ? ? is to make accurate inference on the unseeing dataset D te . The classifier is trained to predict ? from ? by optimizing:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Key Concepts</head><formula xml:id="formula_6">L (? ) = E ?? [? (? ? (? ), ? )] ? 1 ? ? ?? ?=1 ? (? ? (? ? ), ? ? ),<label>(6)</label></formula><p>where ? : R ? ? R ? ? R is a loss function, e.g., cross-entropy. The training process usually terminates when the classification accuracy achieves the best on D val . The model owner releases the welltrained classifier ? ? for downloading, or as a black-box interface.</p><p>As for the latter setting, the users can only query ? ? with their local data ? ? D te and receive a prediction score vector ? from ? ? , which is probability distribution of the classifier's confidence over all possible classes, and the ?-th element ? ? ? (?) is the probability of the data ? belonging to class ?.</p><p>Adversary. Given access to the "target classifier" ? ? , the adversary aims at inferring private information about the training data. In early works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b38">39]</ref>, the adversary tried to recover the exact instances in the training set. For example, for a class of label ?, the following optimization problem is solved to synthesize the corresponding input: max ? log ? ? ? (?). However, in real scenarios, a given label should naturally correspond to a distribution of training samples, therefore the most recent works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b34">35]</ref> propose to recover a data distribution w.r.t. label ? in the MI attack, which should approximate the class conditional distribution ? (? |? = ?).</p><p>Defense. The general goal of defending against MI attacks is to design a strategy to train the target classifier ? ? on D tr such that the access to the well-trained classifier does not leak much information about the class conditional distribution ? (? |? = ?) w.r.t. some specific label ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Bilateral Dependency Optimization</head><p>In this section, we will introduce our bilateral dependency optimization (BiDO) strategy. The overview of our defense strategy is illustrated in Figure <ref type="figure" target="#fig_5">2</ref>. Given a feedforward DNN ? ? : R ? ? ? R ? parameterized by ? with ? latent layers, and an input ? , we denoted by ? ? ? R ? ? ? , ? ? {1, . . . , ? } the ?-th latent representation under input ? . We define the training objective of BiDO with a universally applicable regularizer as follows:</p><formula xml:id="formula_7">L (? ) = L (? ) + ? ? ? ?? ?=1 ? (?, ? ? ) -? ? ? ?? ?=1 ? (? ? , ? ),<label>(7)</label></formula><p>where L is the standard loss given by Eq. ( <ref type="formula" target="#formula_6">6</ref>), ? is a dependency measure and ? ? , ? ? ? R + are balancing hyper-parameters. Together, the second and third terms in Eq. ( <ref type="formula" target="#formula_7">7</ref>) form the BiDO strategy. The former constraint is proposed to limit redundant information propagated from the inputs to the latent representations which could be exploited by the MI adversary for successful attacking, thus improving the model's ability to prevent privacy leakage. Moreover, note that minimizing ? (?, ? ? ) alone would also lead to the loss of useful information, so it is necessary to keep the ? (? ? , ? ) term to make sure ? ? is informative enough of ? as well as to maintain the discriminative nature of the classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Realizations of BiDO</head><p>The BiDO strategy provides an appealing defense principle. Next, we present two concrete realizations of the universally applicable regularizer, namely BiDO-COCO and BiDO-HSIC. When we use COCO as the dependency measure, the training objective in Eq. ( <ref type="formula" target="#formula_7">7</ref>) is realized as:</p><formula xml:id="formula_8">LCOCO (? ) = L (? ) + ? ? ? ?? ?=1 COCO(?, ? ? ) -? ? ? ?? ?=1 COCO(? ? , ? ) . (<label>8</label></formula><formula xml:id="formula_9">)</formula><p>Following the COCO estimator in Eq. ( <ref type="formula" target="#formula_2">2</ref>), we have  Compute value of the predefined elements associated with measure ?; (in HSIC, compute kernel matrices for ? , ? and ? ? using ? ? , ? ? , ? ? respectively inside a mini-batch.)</p><formula xml:id="formula_10">COCO(?, ? ? ) = 1 ? ?? ? ? ? ? ? 2 , (<label>9a</label></formula><formula xml:id="formula_11">) COCO(? ? , ? ) = 1 ? ?? ? ? ? ? ? 2 . (<label>9b</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>Compute L (? ) via Eq. (7); ? ? ? + 1; 9: end while 10: return ? ? Similarly, when we use HSIC as the dependency measure, the training objective in Eq. ( <ref type="formula" target="#formula_7">7</ref>) is realized as:</p><formula xml:id="formula_12">LHSIC (? ) = L (? ) + ? ? ? ?? ?=1 HSIC(?, ? ? ) -? ? ? ?? ?=1 HSIC(? ? , ? ). (<label>10</label></formula><formula xml:id="formula_13">)</formula><p>Following the HSIC estimator in Eq. ( <ref type="formula" target="#formula_5">5</ref>), the HSIC of each term is:</p><formula xml:id="formula_14">HSIC(?, ? ? ) = 1 (? -1) 2 Tr(? ? ?? ? ? ? ) ,<label>(11a)</label></formula><formula xml:id="formula_15">HSIC(? ? , ? ) = 1 (? -1) 2 Tr(? ? ? ?? ? ? ) .<label>(11b)</label></formula><p>We present the overall algorithm in Algorithm 1. In the given realizations, we perform SGD over LCOCO / LHSIC : both L and COCO/HSIC can be evaluated empirically over mini-batches. For the latter, we use the estimator Eq. (2)/Eq. ( <ref type="formula" target="#formula_5">5</ref>), restricted over the current batch. As we have ? samples in a mini-batch, the complexity of calculating the empirical COCO/HSIC is ? (? 3 )/? (? 2 ) for a single layer. Thus, the overall complexity for Eq. ( <ref type="formula" target="#formula_8">8</ref>)/Eq. ( <ref type="formula" target="#formula_12">10</ref>) is ? (?? 3 )/? (?? 2 ) <ref type="bibr" target="#b6">[7]</ref>. This computation is highly parallelizable, thus, the additional computation time of BiDO-COCO or BiDO-HSIC is small when compared to training a neural network via cross-entropy only. Note that, BiDO-COCO and BiDO-HSIC are only shown as examples of how to implement BiDO. We can also implement BiDO with other dependency measures, such as kernel canonical correlation <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In this section, we present the experimental evaluation for verifying the efficacy of BiDO against different attacks in the white-box setting. Note that, here we do not focus on the black-box attack methods <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b36">37]</ref>, which are relatively easy to be defended <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b37">38]</ref>. The baseline that we will compare against is MID <ref type="bibr" target="#b35">[36]</ref>, which achieved the state-of-the-art defense results among the thread of generic defense mechanisms against MI attacks on DNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setting</head><p>Datasets. We study defense models built for different classification tasks, including face recognition, digit classification, and object classification. For face recognition, we use the CelebFaces Attributes Dataset (CelebA) <ref type="bibr" target="#b20">[21]</ref>. For digit classification, we use the MNIST handwritten digit data <ref type="bibr" target="#b15">[16]</ref>. For object classification, we use the CIFAR-10 dataset <ref type="bibr" target="#b14">[15]</ref>.</p><p>Target Classifiers. We adopt the same target classifiers as in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b38">39]</ref>. For the face recognition task, we use VGG16 adapted from <ref type="bibr" target="#b28">[29]</ref> and ResNet-34 adapted from <ref type="bibr" target="#b8">[9]</ref>. For digit classification on MNIST, we use LeNet adapted from <ref type="bibr" target="#b15">[16]</ref>. For object classification, we use VGG16 adapted from <ref type="bibr" target="#b28">[29]</ref>.</p><p>Privacy-Leakage Evaluation Methods. We evaluate the efficacy of BiDO against the following white-box MI attacks, which are the most effective ones presented in the literature thus far.</p><p>? Generative MI attack (GMI) <ref type="bibr" target="#b38">[39]</ref> is the first white-box MI attack algorithm based on GAN <ref type="bibr" target="#b22">[23]</ref> to achieve remarkable performance against DNNs. GMI solves the MAP to recover the most possible private attribute via gradient descent. The key idea of GMI is to leverage public data to learn a generic prior for the private training data distribution and use it to regularize the optimization problem underlying the attack. ? Knowledge-Enriched Distributional MI attack (KED-MI) <ref type="bibr" target="#b1">[2]</ref> is an improvement on GMI, which addresses two key limitations in GMI. Firstly, they propose to tailor the training objective for building an inversion-specific GAN, where they leverage the target model to label the public dataset and train the discriminator to differentiate not only the real and fake samples but also the labels. Secondly, they recover the private data distribution by explicitly parameterizing it and solving the attack optimization over the distributional parameters. ? Variational MI attack (VMI) <ref type="bibr" target="#b34">[35]</ref> provides a unified framework for analyzing existing methods from the Bayesian probability perspective. In the implementation, the author leverage the more powerful StyleGAN <ref type="bibr" target="#b10">[11]</ref> as the generator which allows for fine-grained control. They also leverage the more powerful deep flow models <ref type="bibr" target="#b12">[13]</ref> to approximate the extended latent space of a StyleGAN. This implementation can leverage the hierarchy of learned representations, and perform targeted attacks while preserving diversity.</p><p>Evaluation Protocol. We evaluate the performance of a defense mechanism based on the privacy-utility tradeoff. MID and BiDO have some hyper-parameters which we can tune to vary the model robustness and model performance. For BiDO, we can vary the balancing hyper-parameters ? ? and ? ? in Eq. ( <ref type="formula" target="#formula_7">7</ref>); for MID, we can tune the weight parameter ?. When preventing privacy leakage on CelebA, for GMI (or KED-MI) and VMI, we let the adversary attack the same identity 5 times and 100 times, respectively, and average the results of each attack to calculate attack accuracy; besides, note that all recovered images from each attack are used to calculate the FID value. When preventing privacy leakage on MNIST and CIFAR-10, we let the adversary attack the same class for 100 times, We mitigate the instability caused by the small number of classes by conducting 5 individual attacking trials and average the results.</p><p>Evaluation Metrics. Evaluating the MI attack performance requires gauging the amount of private information about a target label leaked through the synthesized images. We conduct quantitative evaluations as well as qualitative evaluations through visual inspection. The following shows the quantitative metrics that we use to evaluate the attack performance.</p><p>? Attack accuracy (Attack Acc). To measure the performance of an attack, we build an "evaluation classifier" to predict the identities of the reconstructed images. This metrics measure how the generated samples resemble the target class.</p><p>If the evaluation classifier achieves high accuracy, the attack is then thought to be successful. To ensure a fair and informative evaluation, the evaluation classifier should be as accurate as possible. ? Fr?chet inception distance (FID). We employ the widely used FID to measure both the quality and diversity of the reconstructed images, which reveals how much more detailed information is leaked from the reconstructed images. FID measures the similarity between fake and real images in the embedding space given by the features of a convolutional neural network (the evaluation classifiers in the defense task). More specifically, it computes the differences between the estimated means and covariances assuming a multivariate normal distribution on the features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>Performance Evaluation on CelebA. Since facial recognition models are widely used in real scenarios, in this section, we mainly evaluate the efficacy of MID and BiDO for preventing privacy leakage on CelebA against various white-box attacks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b38">39]</ref>.</p><p>The defense results against GMI are presented in Table <ref type="table" target="#tab_1">1</ref>, where we have models with varying model robustness and model utility by tuning the hyper-parameters. As shown in Table <ref type="table" target="#tab_1">1</ref>, BiDO consistently achieves better utility-privacy trade-off than MID, it is noteworthy that BiDO-HSIC (c) outperforms MID (c) by a large margin in terms of attack accuracy (6.47 vs. 15.73), top-5 attack accuracy (16.07 vs. 35.27) and FID (143.63 vs. 126.24) with even better model performance (80.35 vs. 78.70). When compared with the classifier with no defense, BiDO-HSIC (c) significantly reduces the attack accuracy for more than 10% (6.47 vs. 16.73) and top-5 attack accuracy for ? 20% <ref type="bibr">(35.93 vs. 16</ref>.07) while only suffering ? 6% classification accuracy decline (80.35 vs. 86.21).</p><p>The defense results against KED-MI are illustrated in Table <ref type="table">2</ref> where we use the same parameter settings as in defending against GMI. Except the case that BiDO-COCO (a) performs slightly worse than MID (a), on the whole BiDO could be more preferable due to its broad applicability. BiDO-HSIC can significantly prevent privacy leakage, as a highlight, BiDO-HSIC (c) outperforms MID (c) by a large margin in terms of attack accuracy (46.53 vs. 69.60), top-5 attack accuracy (73.00 vs. 92.00) and FID (250.88 vs. 235.61) with We also verify the performance of BiDO in Figure <ref type="figure" target="#fig_4">3</ref>. Since KED-MI can recover the distribution w.r.t. a specific identity, we select two images from the training set that are most similar to the recovered ones. The target classifiers trained with BiDO and MID have comparable performance. We can see that BiDO can block attack much better than MID. For instance, the reconstructions for MID can still retain many facial features of the target individuals (e.g., columns 1, 2, 3), while the reconstructions for the classifier trained with BiDO are either with severely damaged facial features (e.g., columns 7, 8 for BiDO-COCO and columns 5, 8 for BiDO-HSIC) or completely different from the target individuals (e.g., columns 1, 3 for BiDO-COCO and columns 1, 6 for BiDO-HSIC).</p><p>The defense results against VMI are shown in Table <ref type="table" target="#tab_3">4</ref>. Note that, although we set hyper-parameter for the dependency regularization term ? = 0, the classifier trained with MID still cannot achieve comparable classification accuracy as the classifier with no defense. We speculate the reason is: solving the variational approximation of mutual information requires modifying the last layer of the network, which makes it vulnerable to the data pre-processing method used in VMI. While BiDO does not suffer from such a problem.</p><p>Performance Evaluation on MNIST and CIFAR-10. In this section, we evaluate privacy leakage on MNIST and CIFAR-10 against KED-MI. Table <ref type="table" target="#tab_2">3</ref> shows the defense results, where BiDO achieves superior defense performance than MID on both datasets. Take results on MNIST for example, the classifier trained with MID exposes more information about the training data in terms of attack accuracy (51.75 vs. 42.52) than the classifier with no defense, even with worse model performance (97.42 vs. 99.94), while BiDO effectively improves the model's robustness with a slight decline The qualitative defense results against KED-MI are presented in Figure <ref type="figure" target="#fig_6">4</ref>, we can see that BiDO greatly prevents information leakage, the recovered digits for the classifier trained with BiDO are virtually different from the target ones; while for MID, the recovered images may leak more information about the target digits (e.g., digits 2, 3).</p><p>Ablation Study. In this section, we turn our attention to how the last two terms in the loss function in Eq. ( <ref type="formula" target="#formula_2">2</ref>) and Eq. ( <ref type="formula" target="#formula_5">5</ref>) affect model robustness against MI attacks. As illustrated in Table <ref type="table">5</ref>, removing any parts leads to a worse privacy-utility trade-off. Specifically, for BiDO-COCO, removing the COCO(? ? , ? ) penalty (row (2)) brings little improvement in model robustness, because the value of ? (?, ? ? ) is too small to affect the training process; while for BiDO-HSIC, removing the penalty on HSIC(? ? , ? ) (row ( <ref type="formula" target="#formula_5">5</ref>)) degrades the classification accuracy significantly, which means coupling learning to labels via the third term is integral to maintaining useful label-related information in latent layers; for BiDO-COCO, removing COCO(?, ? ? ) penalty (row (3)) degrades defense performance since the dependency between inputs and latent representations makes the model vulnerable to MI attacks; while for BiDO-HSIC, without HSIC(?, ? ? ) penalty (row ( <ref type="formula" target="#formula_6">6</ref>)), the model is overfitted on the training set, thus the classification accuracy decreased. The two dependency terms combined by proper hyper-parameters ? ? and ? ? (rows (4, 7)) achieve a better privacy-utility trade-off. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>For defending against model-inversion (MI) attacks, we propose a bilateral dependency optimization (BiDO) strategy, which aims at minimizing the dependency between the latent representations and the inputs while maximizing the dependency between latent representations and the outputs. In particular, the BiDO strategy is achieved by using the dependency constraints as a universally applicable regularizer in addition to commonly used losses for deep neural networks, which can be instantiated with appropriate dependency criteria according to different tasks. We verified the efficacy of BiDO by using two different dependency measures: BiDO with constrained covariance (BiDO-COCO) and BiDO with Hilbert-Schmidt Independence Criterion (BiDO-HSIC). Experiments show that BiDO achieves the state-of-the-art defense performance for a variety of datasets, target models and MI attacks, which lights up a novel road to defend against MI attacks.  <ref type="table">5</ref>: Ablation study on BiDO. Rows (2-3)/Rows <ref type="bibr" target="#b4">(5)</ref><ref type="bibr" target="#b5">(6)</ref> indicate the effect of removing each component of the learning objective in Eq. (2) (row (4))/Eq. (5) (row <ref type="bibr" target="#b6">(7)</ref>). We evaluate each objective over Accuracy (%) and privacy leakage indicated by Attack Acc/Attack Acc5?standard deviation (%) and FID on CelebA against KED-MI. We set ? ? as 10 and 0.1, ? ? as 50 and 1, for BiDO-COCO and BiDO-HSIC, respectively. Bold values represent the setting that achieves the best trade-off between model robustness and model utility. The notion ? (?) indicates smaller (larger) values are preferred. See Section 5.1 for details. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS AND DISCLOSURE OF FUNDING</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rows</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Minimizing the supervised loss conflicts with minimizing the dependency between inputs and outputs. These figures illustrate the test accuracy of a face recognition model during training, which incorporates ? (?, ? ), the dependency between inputs and outputs, into the training objective as a regularizer (the previous method to defend against model-inversion attacks<ref type="bibr" target="#b35">[36]</ref>). We use constrained covariance as the dependency measure ? (?, ? ), ? denotes the balancing hyper-parameter of the regularization term. It is clear that the previous method causes an explicit accuracy drop (from ? 85% (a) to ? 0.15% (b)) when we increase ? beyond some threshold (from ? = 5 (a) to ? = 20 (b)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 : 3 : 4 :</head><label>234</label><figDesc>Figure 2: Overview of MID framework vs. bilateral dependency optimization (BiDO) framework. BiDO forces DNNs to learn robust latent representations by minimizing ? (?, ? ? ) to limit redundant information propagated from the inputs to the latent representations while maximizing ? (? ? , ? ) to keep the latent representations informative enough of the label.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>7 :</head><label>7</label><figDesc>Backward Propagation: ? ? ? -?? L (? ); 8:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Qualitative comparison for preventing privacy leakage from a face recognition model trained with CelebA dataset. The first two rows show ground-truth images for target identities, images in a column belong to the same person; the third row shows reconstructed images by inverting a well-trained classifier with no defense; the fourth and last two rows illustrate the defense results of MID and BiDO, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Table 2 :</head><label>2</label><figDesc>Privacy leakage indicated by Attack Acc/Attack Acc5?standard deviation (%) and FID on CelebA against KED-MI. For the MID and BiDO, we vary the weight parameter ? and (? ? , ? ? ) in Eq. (7) respectively to trade-off between model robustness and model utility. Bold values represent the setting that achieves the best trade-off between model robustness and model utility. The notion ? (?) indicates smaller (larger) values are preferred. See Section 5.1 for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Qualitative comparison for preventing privacy leakage from a digit classification model trained with MNIST. The first row shows ground-truth images for target digits; the second row shows reconstructed images by inverting a well-trained classifier with no defense; the third and last two rows illustrate the results of MID and BiDO, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>XP and BH were supported by the RGC Early Career Scheme No. 22200720, NSFC Young Scientists Fund No. 62006202, Guangdong Basic and Applied Basic Research Foundation No. 2022A1515011652, Table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Given a training dataset D tr = {(? ? , ? ? )} ? ?=1 , where ? ? ? R ? ? , ? ? ? {0, 1} ? are i.i.d. samples drawn from joint distribution ? ?? . The model owner trains a ?-way classifier ? ? : R ? ? ? R ? parameterized by weights ? ? R ? ? .</figDesc><table /><note><p>Model Owner &amp; Target Classifier.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Privacy leakage indicated by Attack Acc/Attack Acc5?standard deviation (%) and FID on CelebA against GMI. For the MID and BiDO, we vary the weight parameter ? and (? ? , ? ? ) in Eq. (7) respectively to trade-off between model robustness and model utility. Bold values represent the setting that achieves the best trade-off between model robustness and model utility. The notion ? (?) indicates smaller (larger) values are preferred. See Section 5.1 for evaluation details.</figDesc><table><row><cell></cell><cell cols="5">Varying settings Accuracy ? Attack Acc ? Attack Acc5 ? FID ?</cell></row><row><cell>No Def.</cell><cell>-</cell><cell>86.21</cell><cell>16.73?3.32</cell><cell>35.93?4.88</cell><cell>128.81</cell></row><row><cell></cell><cell>MID (a)</cell><cell>53.94</cell><cell>8.13?2.21</cell><cell>20.33?3.16</cell><cell>139.75</cell></row><row><cell>MID</cell><cell>MID (b)</cell><cell>68.39</cell><cell>12.40?3.63</cell><cell>27.93?4.04</cell><cell>133.04</cell></row><row><cell></cell><cell>MID (c)</cell><cell>78.70</cell><cell>15.73?4.11</cell><cell>35.27?4.52</cell><cell>126.24</cell></row><row><cell></cell><cell>BiDO-COCO (a)</cell><cell>53.39</cell><cell>7.40?2.01</cell><cell>19.80?4.21</cell><cell>148.68</cell></row><row><cell>BiDO-COCO</cell><cell>BiDO-COCO (b)</cell><cell>74.53</cell><cell>11.00?3.38</cell><cell>28.80?5.10</cell><cell>136.49</cell></row><row><cell></cell><cell>BiDO-COCO (c)</cell><cell>81.55</cell><cell>13.67?3.49</cell><cell>32.07?3.41</cell><cell>133.92</cell></row><row><cell></cell><cell>BiDO-HSIC (a)</cell><cell>53.49</cell><cell>1.67?1.04</cell><cell>4.93?1.64</cell><cell>180.07</cell></row><row><cell>BiDO-HSIC</cell><cell>BiDO-HSIC (b)</cell><cell>70.31</cell><cell>3.13?1.69</cell><cell>9.27?4.13</cell><cell>136.49</cell></row><row><cell></cell><cell>BiDO-HSIC (c)</cell><cell>80.35</cell><cell>6.47?2.57</cell><cell>16.07?2.91</cell><cell>134.63</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Privacy leakage indicated by Attack Acc/Attack Acc5?standard deviation (%) and FID?standard deviation on MNIST and CIFAR-10 against KED-MI. Bold values represent the setting that achieves the best trade-off between model robustness and model utility. The notion ? (?) indicates smaller (larger) values are preferred. See Section 5.1 for details.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>MNIST</cell><cell></cell><cell></cell><cell cols="2">CIFAR-10</cell><cell></cell></row><row><cell></cell><cell cols="3">Accuracy ? Attack Acc ? Attack Acc5 ?</cell><cell>FID ?</cell><cell cols="3">Accuracy ? Attack Acc ? Attack Acc5 ?</cell><cell>FID ?</cell></row><row><cell>No Def.</cell><cell>99.94</cell><cell>42.52?10.09</cell><cell>95.24?5.24</cell><cell>245.64?14.52</cell><cell>96.17</cell><cell>72.16?17.7</cell><cell>99.56?2.20</cell><cell>167.85?5.44</cell></row><row><cell>MID</cell><cell>97.42</cell><cell>51.75?14.65</cell><cell>100.00?0.00</cell><cell>218.06?17.48</cell><cell>89.04</cell><cell>61.40?13.88</cell><cell>99.72?1.44</cell><cell>176.84?20.36</cell></row><row><cell>BiDO-COCO</cell><cell>99.51</cell><cell>7.76?8.47</cell><cell>84.40?12.95</cell><cell>305.34?19.44</cell><cell>95.39</cell><cell>56.20?17.79</cell><cell>98.92?3.63</cell><cell>179.51?16.25</cell></row><row><cell>BiDO-HSIC</cell><cell>99.61</cell><cell>4.36?5.70</cell><cell cols="2">76.96?12.10 322.00?16.96</cell><cell>93.79</cell><cell>58.24?18.63</cell><cell>99.08?3.99</cell><cell>171.60?12.45</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Privacy leakage indicated by Attack Acc/Attack Acc5?standard deviation (%) and FID on CelebA against VMI. Bold values represent the setting that achieves the best tradeoff between model robustness and model utility. The notion</figDesc><table><row><cell>No Def.</cell><cell>69.27</cell><cell>36.90?22.70</cell><cell>61.50?21.24</cell><cell>0.9295</cell></row><row><cell>MID 1</cell><cell>52.52</cell><cell>29.05?23.99</cell><cell>51.05?28.52</cell><cell>0.9854</cell></row><row><cell>BiDO-COCO</cell><cell>59.34</cell><cell>29.45?16.54</cell><cell>54.25?21.01</cell><cell>0.9705</cell></row><row><cell>BiDO-HSIC</cell><cell>61.14</cell><cell cols="3">30.25?23.46 53.35?27.65 0.9665</cell></row></table><note><p>? (?) indicates smaller (larger) values are preferred. See Section 5.1 for details. Accuracy ? Attack Acc ? Attack Acc5 ? FID ? 2 1 MID with hyper-parameter ? = 0. 2 Following the original setting, ?2 normalized features are used to calculate the FID value. in classification accuracy. For example, BiDO-HSIC remarkably reduces attack accuracy by ? 38% (4.36 vs. 42.52) with only ? 0.3% classification accuracy decline (99.61 vs. 99.94).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>ObjectivesAccuracy ? Attack Acc ? Attack Acc5 ? FID ?</figDesc><table><row><cell></cell><cell>(1)</cell><cell></cell><cell></cell><cell>L (? )</cell><cell></cell><cell>85.31</cell><cell>62.73?4.19</cell><cell>87.73.20?2.88 226.82</cell></row><row><cell></cell><cell>(2)</cell><cell></cell><cell>L (? ) + ? ?</cell><cell cols="2">? ?=1 COCO(?, ? ? )</cell><cell>85.04</cell><cell>63.40?3.19</cell><cell>87.93?3.30</cell><cell>218.10</cell></row><row><cell>BiDO-COCO</cell><cell>(3)</cell><cell></cell><cell>L (? ) -? ?</cell><cell cols="2">? ?=1 COCO(? ? , ? )</cell><cell>75.13</cell><cell>59.00?4.74</cell><cell>80.80?3.09</cell><cell>255.09</cell></row><row><cell></cell><cell>(4)</cell><cell>L (? ) + ? ?</cell><cell cols="2">? ?=1 COCO(?, ? ? ) -? ?</cell><cell>? ?=1 COCO(? ? , ? )</cell><cell>74.53</cell><cell>48.73?4.61</cell><cell>75.53?4.13</cell><cell>266.28</cell></row><row><cell></cell><cell>(5)</cell><cell></cell><cell>L (? ) + ? ?</cell><cell cols="2">? ?=1 HSIC(?, ? ? )</cell><cell>0.83</cell><cell>0.67?0.37</cell><cell>3.00?1.23</cell><cell>720.35</cell></row><row><cell>BiDO-HSIC</cell><cell>(6)</cell><cell></cell><cell>L (? ) -? ?</cell><cell cols="2">? ?=1 HSIC(? ? , ? )</cell><cell>64.73</cell><cell>27.47?4.80</cell><cell>52.53?4.78</cell><cell>289.21</cell></row><row><cell></cell><cell>(7)</cell><cell>L (? ) + ? ?</cell><cell cols="2">? ?=1 HSIC(?, ? ? ) -? ?</cell><cell>? ?=1 HSIC(? ? , ? )</cell><cell>76.36</cell><cell>31.40?4.38</cell><cell>55.20?3.09</cell><cell>276.21</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p><rs type="funder">RIKEN Collaborative Research Fund and HKBU CSD Departmental Incentive Grant</rs>. JFZ was supported by <rs type="funder">JSPS Grants-in-Aid for Scientific Research (KAKENHI)</rs>, <rs type="person">Early-Career Scientists</rs>, Grant Number <rs type="grantNumber">22K17955</rs>, <rs type="programName">Japan JST Strategic Basic Research Programs, ACT-X</rs>, Grant Number <rs type="grantNumber">JPMJAX21AF</rs>, <rs type="funder">Japan</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_RAHGAnh">
					<idno type="grant-number">22K17955</idno>
					<orgName type="program" subtype="full">Japan JST Strategic Basic Research Programs, ACT-X</orgName>
				</org>
				<org type="funding" xml:id="_SMQzCWY">
					<idno type="grant-number">JPMJAX21AF</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A DATA LINKS AND CODE</head><p>Data links and code can be found in this repository: https://github. com/xpeng9719/Defend_MI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B EXPERIMENTAL DETAILS B.1 Datasets and Evaluation Classifiers</head><p>For defending against different MI attacks, we adopt the same dataset settings and evaluation classifiers as the original papers. Please refer to GMI <ref type="bibr" target="#b38">[39]</ref>, KED-MI <ref type="bibr" target="#b1">[2]</ref> and VMI <ref type="bibr" target="#b34">[35]</ref> for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Training Configurations of Target Classifiers</head><p>CelebA. For GMI and KED-MI, the target classifier for CelebA was a VGG16, a pretrained checkpoint from Pytorch Hub (https:// pytorch.org/hub/) is utilized to initialize parameters of the classifier. It was trained using Adam <ref type="bibr" target="#b11">[12]</ref> (learning rate=1e-4, batch size=64) for 50 epochs. For VMI, the target classifier was a ResNet-34. It was trained using SGD with Nestrov momentum (learning rate=1e-1, batch size=64, momentum=0.9, weight decay=5e-4) for 100 epochs. Learning rate decayed by a factor of 0.2 at 60 and 80 epochs.</p><p>MNIST. For KED-MI, the target classifer for MNIST is a adapted LeNet. It is trained using Adam <ref type="bibr" target="#b11">[12]</ref> (learning rate=1e-1, batch size=64) for 20 epochs. CIFAR-10. The target classifier for CIFAR-10 was a VGG16, a pretrained checkpoint from Pytorch Hub is utilized to initialize parameters of the classifier. It was trained using Adam <ref type="bibr" target="#b11">[12]</ref> (learning rate=1e-4, batch size=64) for 30 epochs. Learning rate decayed by a factor of 0.5 at 25 epochs.</p><p>Table <ref type="table">6</ref>: Hyper-parameter settings on CelebA against GMI and KED-MI: For the MID and BiDO, the balancing hyperparameters are ? and (? ? , ? ? ) in Eq. <ref type="bibr" target="#b6">(7)</ref>, respectively.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C ALGORITHM DETAILS AND BALANCING HYPER-PARAMETER SETTINGS</head><p>In both BiDO-COCO and BiDO-HSIC, we apply Gaussian kernels for inputs ? and latent representations ? , and a linear kernel for labels ? . For Gaussian kernels, we set ? = 5 ? ?, where ? is the dimension of the corresponding random variable.</p><p>Hyper-parameter settings used in Table <ref type="table">1</ref> and Table <ref type="table">2</ref> are reported in Table <ref type="table">6</ref>. When we conduct evaluation on CelebA against GMI and KED-MI, for BiDO-COCO, we have tested ? ? ? <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b14">15]</ref>, </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Kernel Independent Component Analysis</title>
		<author>
			<persName><forename type="first">Francis</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="48" />
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Knowledge-Enriched Distributional Model Inversion Attacks</title>
		<author>
			<persName><forename type="first">Si</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Kahla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoxi</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Label-only membership inference attacks</title>
		<author>
			<persName><forename type="first">Florian</forename><surname>Christopher A Choquette-Choo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Tramer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><surname>Papernot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Model inversion attacks that exploit confidence information and basic countermeasures</title>
		<author>
			<persName><forename type="first">Matt</forename><surname>Fredrikson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Somesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Ristenpart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CCS</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Privacy in Pharmacogenetics: An End-to-End Case Study of Personalized Warfarin Dosing</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Fredrikson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Lantz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Somesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Ristenpart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Security</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A kernel two-sample test</title>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="723" to="773" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Measuring statistical dependence with Hilbert-Schmidt norms</title>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note>In ALT</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Kernel methods for measuring independence</title>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralf</forename><surname>Herbrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Sch?lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Evaluating gradient inversion attacks and defenses in federated learning</title>
		<author>
			<persName><forename type="first">Yangsibo</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samyak</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Analyzing and improving the image quality of stylegan</title>
		<author>
			<persName><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Glow: Generative flow with invertible 1x1 convolutions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Durk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Dhariwal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Thieves on sesame street! model extraction of bert-based apis</title>
		<author>
			<persName><forename type="first">Kalpesh</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Singh Tomar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><forename type="middle">P</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Gradientbased learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1998">1998. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">FedRS: Federated Learning with Restricted Softmax for Label Distribution Non-IID Data</title>
		<author>
			<persName><forename type="first">Xin-Chun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">De-Chuan</forename><surname>Zhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Selfsupervised learning with kernel dependence maximization</title>
		<author>
			<persName><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Pogodin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danica</forename><forename type="middle">J</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Meta Two-Sample Testing: Learning Kernels for Testing with Limited Data</title>
		<author>
			<persName><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenkai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danica</forename><forename type="middle">J</forename><surname>Sutherland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning deep kernels for non-parametric two-sample tests</title>
		<author>
			<persName><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenkai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangquan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danica</forename><forename type="middle">J</forename><surname>Sutherland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>Xiaogang Wang, and Xiaoou Tang</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dipole: Diagnosis prediction in healthcare via attention-based bidirectional recurrent neural networks</title>
		<author>
			<persName><forename type="first">Fenglong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radha</forename><surname>Chitta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanzeng</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arXiv</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Can Virtual Assistants Produce Recommendations?</title>
		<author>
			<persName><forename type="first">Dimitrios</forename><surname>Rafailidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yannis</forename><surname>Manolopoulos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>In WIMS</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A survey of privacy attacks in machine learning</title>
		<author>
			<persName><forename type="first">Maria</forename><surname>Rigaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arXiv</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Updates-Leak: Data Set Inference and Reconstruction Attacks in Online Learning</title>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Salem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apratim</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Backes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Security</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Anirudh Goyal, and Yoshua Bengio. 2021. Toward causal representation learning</title>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><forename type="middle">Rosemary</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arXiv</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arXiv</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Information leakage in embedding models</title>
		<author>
			<persName><forename type="first">Congzheng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananth</forename><surname>Raghunathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CCS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Feature Selection via Dependence Maximization</title>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Bedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A dependence maximization view of clustering</title>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName><forename type="first">Yaniv</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc'aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Kuan-Chieh</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Khisti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alireza</forename><surname>Makhzani</surname></persName>
		</author>
		<title level="m">Variational Model Inversion Attacks</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Improving robustness to model inversion attacks via mutual information regularization</title>
		<author>
			<persName><forename type="first">Tianhao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoxi</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">Ziqi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ee-Chien</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenkai</forename><surname>Liang</surname></persName>
		</author>
		<title level="m">Adversarial neural network inversion via auxiliary knowledge alignment</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Defending model inversion and membership inference attacks via prediction purification</title>
		<author>
			<persName><forename type="first">Ziqi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bohan</forename><surname>Xuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ee-Chien</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Zhang</surname></persName>
		</author>
		<idno>arXiv</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The secret revealer: Generative model-inversion attacks against deep neural networks</title>
		<author>
			<persName><forename type="first">Yuheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoxi</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengzhi</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenxiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Towards Fair Federated Learning</title>
		<author>
			<persName><forename type="first">Zirui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingyang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changxin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lanjun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
