<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Summarizing Itemset Patterns Using Probabilistic Models *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chao</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science and Engineering</orgName>
								<orgName type="institution">The Ohio State University</orgName>
								<address>
									<settlement>Columbus</settlement>
									<region>OH</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Srinivasan</forename><surname>Parthasarathy</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science and Engineering</orgName>
								<orgName type="institution">The Ohio State University</orgName>
								<address>
									<settlement>Columbus</settlement>
									<region>OH</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Summarizing Itemset Patterns Using Probabilistic Models *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">30A7004B0F0B494C7881289EB884ED49</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.2.8 [Database Applications]: Data Mining Itemset pattern summarization</term>
					<term>probabilistic graphical model</term>
					<term>Markov Random Field</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose a novel probabilistic approach to summarize frequent itemset patterns. Such techniques are useful for summarization, post-processing, and end-user interpretation, particularly for problems where the resulting set of patterns are huge. In our approach items in the dataset are modeled as random variables. We then construct a Markov Random Fields (MRF) on these variables based on frequent itemsets and their occurrence statistics. The summarization proceeds in a level-wise iterative fashion. Occurrence statistics of itemsets at the lowest level are used to construct an initial MRF. Statistics of itemsets at the next level can then be inferred from the model. We use those patterns whose occurrence can not be accurately inferred from the model to augment the model in an iterative manner, repeating the procedure until all frequent itemsets can be modeled. The resulting MRF model affords a concise and useful representation of the original collection of itemsets. Extensive empirical study on real datasets show that the new approach can effectively summarize a large number of itemsets and typically significantly outperforms extant approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The problem of mining frequent patterns, particularly associations among items in transactional datasets, is an important one with many applications. Efficient algorithms to compute frequent patterns and rules associated with these patterns exist <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b7">7]</ref>. However, often times in many real-world problems the number of frequent patterns mined for various parametric settings is extremely large leaving the end-user swamped when it comes to interpreting the results. As a result researchers have turned to various strategies to summarize the patterns the user is asked to examine. This has led to researchers investigating the use of closed itemsets <ref type="bibr" target="#b14">[14]</ref>, maximal itemsets <ref type="bibr" target="#b9">[9]</ref>, non-derivable itemsets <ref type="bibr" target="#b4">[4]</ref>, and more recently profile patterns <ref type="bibr" target="#b17">[17]</ref>, for this purpose.</p><p>Closed itemsets and non-derivable itemsets are lossless forms of compressing frequent itemsets, i.e. the full list of frequent itemsets and associated frequency counts (used for computing association rules) can be exactly derived from the compressed representation. Maximal itemsets allow greater compression when compared with closed patterns, but the representation is lossy -the list of frequent itemsets can be exactly computed but the exact frequency counts associated with each frequent itemset cannot be determined. It is important to note that the number of closed, maximal or non-derivable itemsets can still be very large, thus further compression is necessary. Top-k patterns <ref type="bibr" target="#b11">[11]</ref>, computes the k most frequent closed itemsets and presents it as the approximate summary to the end-user. Choosing an appropriate k for a given domain is usually not easy and there are no theoretical guarantees on the level of approximation for a given k. Afrati et al. <ref type="bibr" target="#b1">[1]</ref> use K itemsets to recover a collection of frequent itemsets. Like maximal itemsets the method cannot recover the frequency information from the summary. Recently Yan et al. <ref type="bibr" target="#b17">[17]</ref> proposed pattern profiles, an approach to compress closed itemsets. The authors demonstrate that the approach can effectively summarize itemsets, resulting in good compression while retaining high accuracy. However, from an efficiency perspective it is not clear how well this approach will scale to large datasets. The proposed strategy needs to repeatedly scan the original dataset in order to achieve good summarization quality. Obviously, this can become very expensive when dealing with large datasets. Furthermore, the resulting pattern profiles can be quite unbalanced in terms of their size and distribution. There is no easy way to control this problem which can result in poor interpretability.</p><p>In this paper we present a new approach to summarizing itemsets. Our approach relies on some well established ideas in graphical models and probabilistic inference. The key idea is to derive a graphical model summary of the data from the set of frequent non-derivable patterns. This serves as the profile summary of the dataset. Subsequently the list of frequent itemsets and associated counts can be computed using standard probabilistic inference methods <ref type="bibr" target="#b13">[13,</ref><ref type="bibr">6]</ref>. The resultant profile summary can be thought of as generalized non-derivable patterns. This is a very nice property considering that in many cases, non-derivable patterns are already able to offer a very condensed representation of a collection of frequent itemsets <ref type="bibr" target="#b4">[4]</ref>. Furthermore, there is no need to scan the original dataset during the summarization. This is desirable for truly large datasets where repeated scans can be prohibitive.</p><p>Our experimental results on several real datasets show that the proposed approach compares favorably with profile patterns on the axes of accuracy, space and performance. Specifically we find on real datasets that the proposed approach is much more accurate given the same space budget, and more often than not significantly faster than profile patterns.</p><p>The rest of the paper is organized as follows. We define the problem of the itemset pattern summarization and briefly go over the related work in probabilistic graphical models in Section 2. In Section 3 we detail our proposed probabilistic model-based itemset summarization approach. We present experimental results in Section 4. Finally, we discuss future work and conclude in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">PROBLEM STATEMENT &amp; BACKGROUND</head><p>Let I be a set of items, i1, i2, . . ., i d . A subset of I is called an itemset. The size of an itemset is the number of items it contains. An itemset of size k is a k-itemset. A transactional dataset is a collection of itemsets, D = {t1, t2, . . . , tn}, where ti ⊆ I. For any itemset α, we write the transactions that contain α as Dα = {ti|α ⊆ ti and ti ∈ D}. In the probabilistic model context, each item is modeled as a random variable.</p><p>Definition 1. ((σ-)Frequent itemset). For a transactional dataset D, an itemset α is (σ-)frequent if |Dα| ≥ σ, where |Dα| is called the support of α in D, denoted as s(α), and σ is a user-specified non-negative threshold. |Dα|  |D| is called the relative support of α in D. Frequent itemsets satisfy the important Apriori property: any subset of a frequent itemset is also frequent <ref type="bibr" target="#b2">[2]</ref>. All existing frequent itemset mining algorithms rely on this property to prune the search space. Since the number of subsets of a large frequent itemset is explosive, it is more appropriate to mine closed frequent itemsets or non-derivable frequent itemsets only. We define these below.</p><p>Definition 2. (Closed frequent itemset). A frequent itemset α is closed if there does not exist an itemset β such that α ⊂ β and Dα = D β .</p><p>Definition 3. ((Non-)derivable frequent itemset). A frequent itemset α is derivable if its support can be exactly inferred from the support of its sub-itemsets based on the inclusion-exclusion principle. Otherwise it is non-derivable. We refer the reader to <ref type="bibr" target="#b4">[4]</ref> for more information about nonderivable patterns and the inclusion-exclusion principle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Itemset Pattern Summarization</head><p>We define the itemset pattern-summarization problem as follows: given a collection of frequent itemsets, we want to find a more concise representation such that the original collection of itemsets and their support information can be reasonably recovered. Additionally, the summarization should be tunable in terms of controlling the trade-off amongst often competing metrics, namely summarization quality, summary size or compactness, and efficiency.</p><p>As noted above, closed itemsets and non-derivable itemsets have been shown to be two successful concise representations for a collection of frequent itemsets. In many cases, they can significantly reduce the number of itemsets in the representation without information loss. However, they can still be quite large, which is why new summarization schemes are necessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Pattern Profile Summarization Approach</head><p>Recently, Yan et al. <ref type="bibr" target="#b17">[17]</ref> proposed a pattern summarization approach for frequent itemsets. The key notion is pattern profile, which can be viewed as a generalization of closed itemsets. Specifically, a pattern profile is a triple &lt; a, b, c &gt; where a is a set of items, b is a distribution vector on these items and c is the relative support of the whole pattern profile. A frequent itemset is a special pattern profile where the distribution vector entirely consists of 1s. Essentially, a pattern profile is a compressed representation of similar itemsets and can be used to summarize them. In the scheme proposed by Yan et al. <ref type="bibr" target="#b17">[17]</ref>, pattern profiles are compared based on the Kullback-Leibler (KL) divergence between their distribution vectors. The principle is that the pattern profiles having smaller KL divergence are more similar than those having larger KL divergence. Based on this similarity measure, the traditional k-means clustering algorithm is applied to cluster the itemsets into K groups. Then, a representative profile pattern is identified for each group and used as a compressed representation for that group of itemsets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Markov Random Fields</head><p>A Markov Random Field (MRF) is an undirected graphical model in which vertices represent variables and edges represent correlations between variables. The joint distribution associated with an undirected graphical model can be factorized as follows:</p><formula xml:id="formula_0">p(X) = 1 Z(ψ) Y C i ∈C ψC i (XC i )</formula><p>where C is the set of maximal cliques associated with the undirected graph; ψCi is a potential function over the variables of clique Ci and 1 Z(ψ) is a normalization term. A clique is a subset of vertices in the graph that are fully-connected. A maximal clique is a clique that cannot have more vertices added and still remain a valid clique. We associate with each maximal clique a non-negative and real-valued potential function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Using Frequent Itemsets to Construct an MRF</head><p>The idea of using frequent itemsets to construct an MRF was first proposed by Pavlov et al. <ref type="bibr" target="#b15">[15]</ref>. A k-itemset and its support represents a k-way statistic and can be viewed as a constraint on the true underlying distribution that generates the data. Given a set of itemset constraints, a maximum entropy distribution satisfying all these constraints is selected as the estimate for the true underlying distribution. This maximum entropy distribution is essentially equivalent to an MRF. There is a simple algorithm, called iterative scaling that one can use to construct an MRF from a given set of itemsets.</p><p>While our work is clearly inspired by the work by Pavlov et al. <ref type="bibr" target="#b15">[15]</ref>, there are significant distinctions between the two. Their goal is to estimate query selectivity. When a query Q with variables xQ is posed in real-time, all itemsets whose variables are subsets of xQ are selected to construct an MRF on xQ in an online fashion. Then the selectivity of Q is estimated from the model. When a new query is posed, a new model must be constructed from scratch. This approach is inherently online and local. In contrast, our goal is to summarize the itemsets. Our MRF is global in that it attempts to integrate the information of all itemsets seen thus far. This global character allows for more accurate support estimations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">ALGORITHM</head><p>Our goal is to use a small number of itemsets to construct an MRF over all involved variables and use it to summarize a much larger collection of frequent itemsets. We rely on the following lemmas (their proofs can be found in our technical report <ref type="bibr" target="#b16">[16]</ref>).</p><p>Lemma 1. Given a transactional dataset D, the MRF M constructed from all of its σ-frequent itemsets is equivalent to M , the MRF constructed from only its σ-frequent nonderivable itemsets.</p><p>Lemma 2. Given an itemset α and all of its non-derivable sub-itemsets and their support estimations, i.e., ŝ1, . . ., ŝl (true supports are s1, . . ., s l , respectively), if these estimations are error bounded by e1, . . ., e l , i.e., | ŝ1 -s1| ≤ e1, . . ., | ŝl -s l | ≤ e l , then the support estimation for α, ŝ(α), derived from these sub-itemsets is error bounded by e1 + e2 + . . . + e l .</p><p>Motivated by the above lemmas, we focus on the task of summarizing non-derivable itemsets. These patterns capture the non-redundant distribution information of the data according to Lemma 1. Furthermore, if we summarize these patterns well, the summarization quality for all other derivable patterns will be error-bound according to Lemma 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Summarizing Itemsets Using MRFs</head><p>The idea behind our proposed summarization technique is simple. We use statistics of smaller itemsets to construct an MRF and then use this model to infer the supports of larger itemsets. If the estimations are accurate enough (within a user-specified error tolerance), we bypass the corresponding patterns. Otherwise we use the extra information from these itemsets to augment the model. Our summarization proceeds in a level-wise fashion. First, all 1-itemsets are collected and used to construct an MRF. Then, we infer the supports for all 2-itemsets. We bypass those 2-itemsets whose supports are well-estimated from the model and use the information of all skewed 2-itemsets to augment it. We move on to process all 3-itemsets and so on. This process will be repeated level-by-level until we process all the itemsets. At the end of the process, all itemsets remaining in the resulting model provide a concise representation of the original collection of itemsets.</p><p>Essentially, we select the skewed itemsets and add their information to the probabilistic model as it is constructed. Thus, we expect that the final resulting model will be able to faithfully capture the most significant dependency information in the data, summarizing the original patterns well. In other words, we try to reduce the original collection of itemsets by eliminating redundancy. The MRF fully specifies the conditional independence in the data, thus if an itemset does not introduce any extra significant dependency information to the current model, it will be pruned. Furthermore, we introduce a parameter δ to tune the granularity of the summarization. δ specifies the error tolerance during the summarization. If the estimation error is within the tolerance, we bypass the corresponding pattern. Otherwise we label it as skewed. By specifying the error tolerance, the parameter δ provides a mechanism to trade-off summarization accuracy for space.</p><p>Itemset Summarize(C, δ) Input : C, collection of f requent itemsets; δ, error tolerance threshold; Output : R, reduced collection of itemsets; 1. Obtain all 1-itemsets in C and their supports, use them to initialize R; 2. k ← 2; 3. while k &lt; M AX LEV EL 4.</p><p>U se itemsets in R to construct an M RF M ; 5.</p><p>Obtain all k-itemsets in C and their supports; 6.</p><p>for each k-itemset p : 7.</p><p>U se M to estimate s(p), calculate estimation error e; 8.</p><p>if e &gt; δ then add p to R; 9.</p><p>k ← k + 1; 10. return R;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1: Itemset pattern summarization algorithm</head><p>The formal summarization algorithm is presented in Figure <ref type="figure">1</ref>. Note that this algorithm can be easily extended to a more general summarization scheme that can be applied to summarize any collection of itemsets. We do not pursue this direction in our study, focusing on the summarization of a complete collection of σ-frequent itemsets. The time complexity of the summarization algorithm is dominated by the MRF construction, which we will describe below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Constructing MRFs</head><p>As was mentioned in Section 2, the iterative scaling algorithm can be used to construct an MRF from a set of itemsets. Figure <ref type="figure" target="#fig_0">2</ref> presents a high-level outline of a computationally efficient version of the algorithm given by Jelinek <ref type="bibr" target="#b12">[12]</ref>. During the construction process, we need to iterate over all itemset constraints and repeatedly update the model to force it to satisfy the current itemset constraint. The model update relies on the support estimation for the current itemset constraint. Thus, we need to continuously make inferences on the current model. If the iterative scaling algorithm runs for k iterations and there are m itemset constraints, the time complexity of the algorithm will be O(k × m × t), where t is the average inference time over a constraint. Therefore, efficient inference is crucial to the running time of the construction algorithm. In our study, we exploit and evaluate two inference engines, the Junction Tree inference algorithm and the Markov Chain Monte Carlo (MCMC) inference algorithm.</p><p>Iterative Scaling(C) Input : C, collection of itemsets; Output : M RF M; 1. Obtain all involved variables v and initialize parameters of M; //typically unif orm over v; 2. while (N ot all constraints are satisf ied) 3.</p><p>for (each constraint C i ) 4.</p><p>U pdate M to f orce it to satisf y C i ; 5. return M; </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Junction Tree Inference Algorithm</head><p>The junction tree algorithm <ref type="bibr" target="#b13">[13]</ref> is a commonly-used exact inference engine for probabilistic models. The general inference problem is to calculate the marginal probability of a set of variables, given the observed values of another set of variables. In our context, there are no observed variables, and our goal is to calculate the marginal probability associated with the itemsets.</p><p>Specifically, the junction tree algorithm decomposes the original model into a hyper-tree, in which each tree node consists of a set of variables in the original graphical model. Two sets of variables associated with two tree nodes can overlap, with the overlapped part called their separator. Each tree node corresponds to a unique maximum clique in the graph formed by triangulating the original model. The junction tree must satisfy the running intersection property, i.e., for every pair of cliques V and W , all cliques on the path between V and W contain V ∩ W . Each tree node has an expectation on the marginal probability over its associated variables, called its belief. The beliefs of all the tree nodes propagate along all distinct paths to achieve a global consistency, which implies that the inference problem within a tree node can be solved independently of the other tree nodes.</p><p>The time complexity of the junction tree algorithm is exponential in the maximum number of variables contained in a tree node (also known as the treewidth of the underlying graphical model). If the underlying MRF is relatively simple (with a relatively low treewidth), then the junction tree algorithm can yield exact inferences very efficiently. However, when the model becomes complex (the treewidth becomes large), then the exact inference will become slow, sometimes even intractable. In such cases we must resort to approximate inference algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">MCMC Inference Algorithm</head><p>MCMC is a method for generating approximate samples from complicated distributions <ref type="bibr">[6]</ref>. In our study, we use a type of MCMC algorithm known as Gibbs sampling to draw dependent samples from the joint posterior distribution, from which we evaluate the marginal probabilities corresponding to the itemsets. We specify a full conditional distribution p(xi|xi) for each variable xi in our MRF, where -xi is the set of variables in the graph not including xi. Then we draw samples from it. Note that the Markov property indicates that it is sufficient to condition on the neighboring variables of xi in the MRF. Gibbs sampling proceeds by sampling each xi from its conditional distribution, given the current values of the other neighboring variables. Marginal probabilities can be estimated by summing over the samples. Note that the Gibbs sampling scheme yields approximate inferences. The quality of the approximate inference is reasonably good when the sample size is large enough. To diminish the effect of the starting distribution, we generally discard a certain number of early iterations, referred to as burn-in <ref type="bibr">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Generalized Non-derivable Itemsets</head><p>The probabilistic model-based summarization scheme returns a subset of the original collection of itemsets to the end-user. Similar to the conclusions of Yan et al. <ref type="bibr" target="#b17">[17]</ref> that pattern profiles can be viewed as generalized closed itemsets, the resulting itemsets in our summarization approach can be viewed as generalized non-derivable itemsets. First, we construct the probabilistic models based on the non-derivable itemsets. As a result, all the itemsets in the final summary are non-derivable. Second, we allow for a certain error tolerance when summarizing the itemsets. If a particular itemset follows the currently-known conditional independence struc- ture specified by the model, we consider its support to be known. Note that there might be cases where we are not able to derive an itemset's support based solely on the inclusionexclusion principle. We are able to derive it according to the further conditional independence information, however. Essentially, we relax the requirement for an itemset to be "derivable", which will significantly increase the number of derivable patterns. Furthermore, the greater relaxation, the greater number of derivable patterns, which will result in a more compressed summarization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTAL RESULTS</head><p>In this section, we examine the performance of our proposed approach on real datasets. We compare our probabilistic model-based summarization (abbreviated as PM) against the state-of-the-art pattern profile summarization scheme (abbreviated as PP). The summarization algorithm is implemented in C++. The junction tree and Gibbs sampling inference algorithms 1 are implemented using Intel's Open-Source Probabilistic Networks Library (https://sourceforge.net/projects/openpnl/). Also, we implement the pattern profile summarization algorithm in C++ and tune it to achieve performance similar to that reported in <ref type="bibr" target="#b17">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>Unless otherwise noted, all the experiments were conducted on a Pentium 4 2.66GHz machine with 1GB RAM running Linux 2.6.8. We use the implementation of the apriori algorithm in <ref type="bibr" target="#b3">[3]</ref> to collect the frequent itemsets and the corresponding closed itemsets. We use the implementation in <ref type="bibr" target="#b5">[5]</ref> to collect frequent non-derivable itemsets. We detail the datasets and performance metrics considered in our evaluation in the text below. Datasets: We use four publicly available datasets in our experiments. They are the Chess, Accidents, Mushroom and Microsoft Anonymous Web datasets. The first three datasets are publicly available at the FIMI repository (http://fimi.cs.helsinki.fi/) and the last Web dataset is publicly available at the UCI KDD archive (http://kdd.ics.uci.edu/). The main characteristics of the datasets are summarized in Table 1. The Chess and Mushroom datasets are relatively dense. The Web dataset is quite sparse and the Accidents dataset is somewhere in between.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summarization accuracy:</head><p>Definition 4. Restoration Error. Given a collection of itemsets Φ = {α1, α2, . . . , α l }, the quality of a pattern summarization can be evaluated by the following average relative error (called restoration error):</p><formula xml:id="formula_1">E = P α k ∈Φ |s(α k )-ŝ(α k )| s(α k )</formula><p>where s is the true support and ŝ is its estimation. Restoration error measures the average relative error between the estimated support of a pattern and its true support. Summary size: In order to make a fair comparison between the two approaches, we need to consider the sum- 1 The results on Gibbs sampling are omitted for space reasons and can be found in our technical report <ref type="bibr" target="#b16">[16]</ref>. mary size. The comparison should be made between summarizations which are of the same size. In our study, we use the number of bytes taken by a summarization to quantify its size. Specifically, we assume an item in the summary takes 2 bytes (a short integer) and a floating point support in the summary takes 4 bytes. For example, the following itemset takes 8 bytes, {(item1, item2), 0.1} and the following pattern profile takes 22 bytes. {(item1, item2, item3), (1.0, 0.8, 0.6), (0.1)} Summarization time: We also consider the time taken to summarize the itemsets. Making a fair timing comparison between the two summarization schemes is not easy. Both our approach and pattern profile approach are iterative processes. The running times are highly dependent of the convergence criteria, which can be rather subjective. We report the timing results of those summarizations from which we collect the accuracy results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Level</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results on the Chess &amp; Accidents Datasets</head><p>First, we report the experimental results on the Chess dataset. For this set of experiments, we set σ = 2000 to collect the frequent itemsets. As a result, there are 166581 frequent itemsets, from which 1276 itemsets are non-derivable. We also collect all the 68967 closed frequent itemsets at this support level for the pattern profile summarization scheme.</p><p>Figure <ref type="figure">3a</ref> presents the summarization quality as we vary the error tolerance threshold. For our approach we report results on summarizing all itemsets and on summarizing all non-derivable itemsets. For the pattern profile approach, we only report the results on summarizing all itemsets. As a baseline, the results based on a naive independence model are also plotted in the figure. We see that the probabilistic model-based summarization scheme effectively summarizes the itemsets. The restoration error of all frequent patterns is slightly higher than that of non-derivable patterns. This is expected, considering our approach focuses on summarizing non-derivable patterns. It is worth pointing out that the restoration error on all frequent itemsets is also very small, supporting our claim that non-derivable patterns play a key role in representing the whole collection of frequent itemsets. Furthermore, it can be clearly seen that the restoration error increases as we raise the error tolerance threshold. This is due to the fact that we will lose more information with larger error tolerance thresholds. Particularly, the summarization with the threshold above 0.25 becomes equivalent to the naive independence model-based summarization. The advantage of our approach over the pattern profile approach is clearly demonstrated in the figure. For the pattern profiles of the same size, the restoration error is much higher than that of our approach and is actually quite close to that of the naive independence model.</p><p>Figure <ref type="figure">3b</ref> presents the summary sizes with different error tolerance thresholds. The sizes of the original collection of patterns and the naive independence model are also plotted here for reference purpose. As one can see, our summaries use a very small amount of space to represent a much larger  collection of itemsets. For example, the summary takes 398 bytes at an error threshold of 0.05 to summarize itemsets of size 12480 bytes, about a 30-fold reduction. Figure <ref type="figure">3c</ref> presents the timing performance of our approach. As one can see, our approach quickly summarizes the itemsets of this dataset. In all cases, the summarization takes less than 5 seconds. In contrast, the pattern profile approach does not finish before it exhausts the memory. When we submit the summarization job to a computer with more memory (4GB RAM) and the same CPU speed at the Ohio Supercomputer Center, the pattern profile approach takes about 40 minutes to finish. Another trend is that our approach takes more time when using a lower error tolerance threshold, since the models with lower thresholds are more complex.</p><p>Table <ref type="table" target="#tab_1">2</ref> presents the distribution of the skewed itemsets at different levels with respect to different error tolerance thresholds. As can be seen from the table, the numbers of skewed itemsets are very small at all the thresholds. For example, at the threshold of 0.05, there are 6, 14 and 2 skewed 2, 3 and 4-itemsets, respectively. As we raise the threshold, the overall number of skewed itemsets decreases.</p><p>The results on the Accidents dataset are quite similar to that on the Chess dataset and are omitted in the interest of space. However, the complete results can be found in our technical report <ref type="bibr" target="#b16">[16]</ref>.</p><p>Note that both of the two datasets are relatively dense and largely satisfy the independence assumption. For this kind of dataset, the MRF based summarization scheme works extremely well. Interestingly, we note that on these two datasets, the frequent non-derivable patterns are much fewer than the frequent closed patterns. Next we examine the performance of our approach on the skewed datasets that do not satisfy the independence assumption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results on the Mushroom &amp; Microsoft Web Datasets</head><p>First, we report the experimental results on the Mushroom dataset. For this set of experiments, we set σ = 2031 (a support threshold of 25%) to collect the frequent itemset patterns, resulting in 5545 frequent itemsets, from which 688 are closed and 534 are non-derivable.</p><p>Figure <ref type="figure">4a</ref>-c present the restoration error, summary sizes, and summarization times, respectively. Figure <ref type="figure">4a</ref> shows that the independence assumption does not hold well on this dataset. The restoration errors for all itemsets and non-derivable itemsets are 20% and 39%, respectively. The restoration errors for all frequent patterns and non-derivable patterns are reasonably low, and are much lower than that of the pattern profile summaries of the same size. Note that both approaches work better than the naive independence model. Figure <ref type="figure">4b</ref> shows that the summaries take relatively more space, compared with that on the previous two datasets. Figure <ref type="figure">4c</ref> shows that our approach is faster than the pattern profile approach.</p><p>Table <ref type="table" target="#tab_3">3</ref> presents the distribution of the skewed itemsets. Compared with the previous two datasets, the proportion of the skewed itemsets is much higher on this dataset, which explains why we need more space when summarizing.</p><p>We also report the results on the sparse Microsoft Web dataset. The results overall are quite similar to that on the Mushroom dataset. Our approach consistently outperforms the pattern profile approach in terms of restoration error. The proportion of the skewed itemsets is relatively high, so we have to use more space and time to summarize the patterns. Again, the complete results can be found in <ref type="bibr" target="#b16">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Result Summary &amp; Discussion</head><p>The experimental results have shown that the probabilistic model-based summarization scheme overall is very efficient and effective in summarizing itemsets. In most cases, it outperforms the pattern profile summarization scheme.</p><p>When datasets are dense and largely satisfy the conditional independence assumption, there usually exists a large amount of redundancy in the corresponding itemsets. In such cases our approach will be extremely efficient and effective. On the other hand, when datasets become sparse and do not satisfy conditional independence assumption well, the summarization task for our approach becomes more difficult. As a result, we have to spend more space and time on summarizing the corresponding itemsets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSIONS</head><p>In this paper, we have presented a novel approach for summarizing itemset patterns using MRFs, which exploit conditional independence relations among the items in the transactional data. The success of our approach on all the tested real-world datasets indicates that the conditional independence structure is quite common. This is particularly true when dealing with relatively dense datasets, whose dense structure leads to significant redundancy in mined itemsets. As a result, our approach is a viable option for many real-world datasets. Given the same summary size, our approach can achieve up to a 4-fold improvement in accuracy compared with the pattern profile approach. On certain datasets, it is orders of magnitude faster than the pattern profile approach.</p><p>In the future, we would like to examine the scalability of our approach on truly large-scale collections of itemsets which can result in very complex MRFs. We intend to evaluate and adapt ideas from the approximate probabilistic inference field for this purpose. It would be interesting to evaluate the viability of the proposed approach in a streaming or incremental setting.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Iterative scaling algorithm</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Results on the Chess dataset:(a)Restoration error (b)Summary size (c)Summarization time Results on the Mushroom dataset:(a)Restoration error (b)Summary size (c)Summarization time</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>General characteristics of the datasets. k is the number of distinct items, n is the number of records, m is the number of total items and d = m kn is the density index.</figDesc><table><row><cell></cell><cell>k</cell><cell>n</cell><cell>m</cell><cell>d</cell></row><row><cell>Chess</cell><cell>75</cell><cell>3196</cell><cell>118252</cell><cell>0.493</cell></row><row><cell>Accidents</cell><cell>468</cell><cell>340183</cell><cell>11500870</cell><cell>0.0722</cell></row><row><cell>Mushroom</cell><cell>119</cell><cell>8124</cell><cell>186852</cell><cell>0.193</cell></row><row><cell>Web</cell><cell>294</cell><cell>32711</cell><cell>98654</cell><cell>0.0102</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Skewed itemset distribution on the Chess dataset when varying error threshold</figDesc><table><row><cell></cell><cell>Itemsets No.</cell><cell cols="6">Skewed Itemsets No. (Varying δ) 0.05 0.10 0.15 0.20 0.25 0.30</cell></row><row><cell>1</cell><cell>31</cell><cell>31</cell><cell>31</cell><cell>31</cell><cell>31</cell><cell>31</cell><cell>31</cell></row><row><cell>2</cell><cell>335</cell><cell>6</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>3</cell><cell>653</cell><cell>14</cell><cell>19</cell><cell>2</cell><cell>1</cell><cell>0</cell><cell>0</cell></row><row><cell>4</cell><cell>257</cell><cell>2</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>Sum</cell><cell>1276</cell><cell>53</cell><cell>50</cell><cell>33</cell><cell>32</cell><cell>31</cell><cell>31</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Skewed itemset distribution on the Mushroom dataset when varying error threshold</figDesc><table /></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work is supported in part by the following research grants: DOE Award No. DE-FG02-04ER25611; NSF CAREER Grant IIS-0347662;</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Approximating a collection of frequent sets</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">N</forename><surname>Afrati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gionis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mannila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="12" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast algorithms for mining association rules in large databases</title>
		<author>
			<persName><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on Very Large Data Bases</title>
		<meeting>the 20th International Conference on Very Large Data Bases</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="487" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Efficient implementations of apriori and eclat</title>
		<author>
			<persName><forename type="first">C</forename><surname>Borgelt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE ICDM Workshop on Frequent Itemset Mining Implementations</title>
		<meeting>the IEEE ICDM Workshop on Frequent Itemset Mining Implementations</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mining all non-derivable frequent itemsets</title>
		<author>
			<persName><forename type="first">T</forename><surname>Calders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Goethals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th European Conference on Principles of Data Mining and Knowledge Discovery</title>
		<meeting>the 6th European Conference on Principles of Data Mining and Knowledge Discovery</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Depth-first non-derivable itemset mining</title>
		<author>
			<persName><forename type="first">T</forename><surname>Calders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Goethals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIAM 2005 International Conference on Data Mining</title>
		<meeting>the SIAM 2005 International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Carlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
		<title level="m">Bayesian data analysis</title>
		<imprint>
			<publisher>Chapman &amp; Hall/CRC</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cache-conscious frequent pattern mining on a modern processor</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ghoting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Buehrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Parthasarathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dubey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Very Large Data Bases</title>
		<meeting>the 31st International Conference on Very Large Data Bases</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="577" to="588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Genmax: An efficient algorithm for mining maximal frequent itemsets</title>
		<author>
			<persName><forename type="first">K</forename><surname>Gouda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Zaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="223" to="242" />
			<date type="published" when="2005-11">November 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Data mining, hypergraph transversals, and machine learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gunopulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Khardon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mannila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Toivonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixteenth ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems</title>
		<meeting>the Sixteenth ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="209" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mining frequent patterns without candidate generation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data</title>
		<meeting>the 2000 ACM SIGMOD International Conference on Management of Data</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mining top-k frequent closed patterns without minimum support</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tzvetkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2002 IEEE International Conference on Data Mining</title>
		<meeting>the 2002 IEEE International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="211" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Jelinek</surname></persName>
		</author>
		<title level="m">Statistical Methods for Speech Recognition</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Local computations with probabilities on graphical structures and their application to expert systems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lauritzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Speigelhalter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society, Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">157224</biblScope>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Discovering frequent closed itemsets for association rules</title>
		<author>
			<persName><forename type="first">N</forename><surname>Pasquier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bastide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Taouil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lakhal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Database Theory -ICDT &apos;99, 7th International Conference</title>
		<meeting><address><addrLine>Jerusalem, Israel</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">January 10-12, 1999. 1999</date>
			<biblScope unit="page" from="398" to="416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Beyond independence: probabilistic models for query approximation on binary transaction data</title>
		<author>
			<persName><forename type="first">D</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mannila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Smyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1409" to="1421" />
			<date type="published" when="2003-11">November 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Summarizing itemset patterns using probabilistic models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Parthasarathy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
		<respStmt>
			<orgName>The Ohio State University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Summarizing itemset patterns: a profile-based approach</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the Eleventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="314" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Charm: An efficient algorithm for closed itemset mining</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Zaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Hsiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second SIAM International Conference on Data Mining</title>
		<meeting>the Second SIAM International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">New algorithms for fast discovery of association rules</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Zaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Parthasarathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Mitsunori Ogihara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the Third International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="283" to="286" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
