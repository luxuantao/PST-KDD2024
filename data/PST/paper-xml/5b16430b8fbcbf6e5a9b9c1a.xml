<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Offline Continuous Handwriting Recognition Using Sequence to Sequence Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-02-07">February 7, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jorge</forename><surname>Sueiras</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Rey Juan Carlos University. Madrid</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Rey Juan Carlos University. Madrid</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Victoria</forename><surname>Ruiz</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Rey Juan Carlos University. Madrid</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Rey Juan Carlos University. Madrid</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Angel</forename><surname>Sanchez</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Rey Juan Carlos University. Madrid</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Rey Juan Carlos University. Madrid</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Jose</forename><forename type="middle">F</forename><surname>Velez</surname></persName>
							<email>jose.velez@urjc.es</email>
							<affiliation key="aff0">
								<orgName type="institution">Rey Juan Carlos University. Madrid</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Rey Juan Carlos University. Madrid</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Offline Continuous Handwriting Recognition Using Sequence to Sequence Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-02-07">February 7, 2018</date>
						</imprint>
					</monogr>
					<idno type="MD5">E318AF86EEA2CD04931E8E47F1379781</idno>
					<idno type="DOI">10.1016/j.neucom.2018.02.008</idno>
					<note type="submission">Received date: 1 August 2017 Revised date: 4 December 2017 Accepted date: 1 February 2018 Preprint submitted to Neurocomputing</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T04:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Artificial Intelligence</term>
					<term>Handwriting Recognition</term>
					<term>Convolutional Neural Networks</term>
					<term>Recurrent Neural Networks</term>
					<term>Sequence to Sequence Networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper proposes the use of a new neural network architecture that combines a deep convolutional neural network with an encoder-decoder, called sequence to sequence, to solve the problem of recognizing isolated handwritten words.</p><p>The proposed architecture aims to identify the characters and contextualize them with their neighbors to recognize any given word. Our model proposes a novel way to extract relevant visual features from a word image. It combines the use of a horizontal sliding window, to extract image patches, and the application of the LeNet-5 convolutional architecture to identify the characters.</p><p>Extracted features are modeled using a sequence-to-sequence architecture to encode the visual characteristics and then to decode the sequence of characters in the handwritten text image. We test the proposed model on two handwritten databases (IAM and RIMES) under several experiments to determine the optimal parametrization of the model. Competitive results above those presented in the current state-of-the-art, on handwriting models, are achieved. Without using any language model and with closed dictionary, we obtain a word error rate in the test set of 12.7% in IAM and 6.6% in RIMES.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Highlights</head><p>• A new deep neural architecture to offline handwriting recognition is proposed.</p><p>• Convolutional and recurrent layers to get visual and sequence features were used.</p><p>• Optimal parametrization of the model was found by systematic experiments • Best published results are obtained over IAM and RIMES databases with test lexicon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The offline continuous handwritten text recognition problem <ref type="bibr" target="#b0">[1]</ref> consists in the automatic transcription of images that contain handwritten text. The problem has been extensively studied in the literature <ref type="bibr" target="#b1">[2]</ref> [1] [3] <ref type="bibr" target="#b3">[4]</ref>. Handwriting recognition presents relevant applications such as bank check processing, au-5 tomatic address reading <ref type="bibr" target="#b4">[5]</ref> or content extraction from digitalized databases of historical documents <ref type="bibr" target="#b5">[6]</ref>. Despite of recent advances, significant differences in the writing of individuals and imprecise nature of handwritten characters make this recognition task hard and it still remains as an open research problem.</p><p>The problem of handwriting recognition is very complex and usually includes 10 several steps <ref type="bibr" target="#b2">[3]</ref>: text line detection and segmentation, breaking text line into words and word recognition. In this paper we put the focus into the word recognition problem. This is, we present a model that recognize the handwriting words that appear in an image.</p><p>Before the irruption of deep neural networks in 2009, the best results for 15 recognition of continuous offline handwritten texts were obtained by using Hidden Markov Models (HMM) techniques <ref type="bibr" target="#b6">[7]</ref>. Since 2009, recurrent neural networks and deep feedforward neural networks (in particular, the bi-directional and multi-dimensional Long Short-Term Memory or LSTM) have won several international handwriting competitions <ref type="bibr" target="#b0">[1]</ref> <ref type="bibr" target="#b7">[8]</ref>. <ref type="bibr" target="#b20">20</ref> Deep Convolutional Neural Networks (CNN) provide current state of the art results in handwriting isolated character recognition <ref type="bibr" target="#b8">[9]</ref>. In this paper, we propose to take advantage of using a CNN to extract relevant features from word images. We aim to extract relevant features in order to identify the characters included in the word, without needing to segment it into characters. <ref type="bibr" target="#b25">25</ref> Next, we use these CNN extracted features as the input to a Sequence to Sequence model (Seq2Seq). Seq2Seq architectures are designed to solve problems that need transform one input sequence signal into other output sequence signal of different length. These architectures have been proven to successfully solve complex problems, such us machine language translation <ref type="bibr" target="#b9">[10]</ref>, speech recogni-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T tion <ref type="bibr" target="#b10">[11]</ref> and lip-reading <ref type="bibr" target="#b11">[12]</ref>. Based on these architectures, we propose a new Seq2Seq model for the offline handwriting problem.</p><p>The main contribution of this paper is the novel architecture proposal for the offline handwriting recognition problem. This architecture combines a CNN with a Seq2Seq model. The CNN is applied over a sequence of image patches 35 obtained from the word image using a horizontal sliding window. So, the CNN models the visual attributes of the handwritten words and provides a sequence of visual features from each part of the word image. This feature sequence is used as input to the Seq2Seq model. Across these inputs, the Seq2Seq network identifies the characters of the word using its encoder-decoder functionality. <ref type="bibr" target="#b38">40</ref> In addition, we analyze different parametrizations in the proposed architecture and identify the most relevant ones to optimize the accuracy of the model.</p><p>Finally, the generalization capacity is validated using two different corpora in different languages, and our results are compared to other related approaches. This paper is focused on Latin character languages, although we think that 45 the proposed solution could be used on other character sets. Our approach has been applied on the character dictionaries available for the two corpora analyzed (IAM <ref type="bibr" target="#b13">[13]</ref> and RIMES <ref type="bibr" target="#b14">[14]</ref> databases). These databases include English and French languages with uppercase/lowercase letters, digits and some punctuation signs, but they do not cover the complete ASCII printable character table.  <ref type="bibr" target="#b15">[15]</ref> and Kozielski et al. <ref type="bibr" target="#b16">[16]</ref> who employ Principal Component Analysis (PCA) to extract components over fixed-size frames of 8 × 32 pixels. Bideault et al. <ref type="bibr" target="#b17">[17]</ref> also extract features, in this case using Histograms of Oriented Gradients (HOG). Graves et al. <ref type="bibr" target="#b0">[1]</ref> consider characteristics extracted of each column pixel, such as mean and other moments, center of gravity, transitions and other 65 aggregations. In the other hand, the works by Graves et al. <ref type="bibr" target="#b0">[1]</ref> and Bluche et al. <ref type="bibr" target="#b18">[18]</ref> use directly the pixel features as input of the handwriting model.</p><p>There are also two main options to decode the handwriting prediction output for converting it in the sequence of characters that identifies the handwritten words. The first one consists in using an HMM (see for example Bluche et al. 70 <ref type="bibr" target="#b19">[19]</ref> and Doetsch et al. <ref type="bibr" target="#b15">[15]</ref>), which is the most traditional way to decode the word. The second way is using the Connectionist Temporal Classification (CTC) objective function introduced by Graves et al. <ref type="bibr" target="#b20">[20]</ref>. Several authors also apply this second option (see Graves et al. <ref type="bibr" target="#b0">[1]</ref>, Bluche et al. <ref type="bibr" target="#b18">[18]</ref> and Voigtlaender et al. <ref type="bibr" target="#b21">[21]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>75</head><p>In the last years, the main model architectures applied to the offline handwriting problem are Recurrent Neural Networks (RNN), specially Long Short-Term Memory (LSTM) networks. Several authors have proposed RNN-based models with different strategies to extract features and different decoding processes. Kozielski et al. <ref type="bibr" target="#b16">[16]</ref> applied PCA features over an architecture that 80 included a HMM baseline layer followed by a LSTM layer with a final HMM decoder. Some other works, like <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b21">[21]</ref> and <ref type="bibr" target="#b18">[18]</ref>, proposed a RNN variant named Multidimensional Recurrent Neural Network (MDRNN), introduced in Graves et al. <ref type="bibr" target="#b22">[22]</ref>, which exploits the bidimensional nature of the handwritten images.</p><p>In all cases, the authors proposed a deep architecture with several Multidimen-85 sional Long Short Term Memory (MDLSTM) layers and the CTC decoder at the top. Doetsch et al. <ref type="bibr" target="#b15">[15]</ref> proposed a LSTM modification of the activation functions inside the gated units over PCA extracted features and using a HMM decoder. Stuner et al. <ref type="bibr" target="#b23">[23]</ref> used two architectures: a Bidirectional LSTM with two layers and a MDLSTM architecture with three layers. Finally, Sabatelli and</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T Shkarupa <ref type="bibr" target="#b24">[24]</ref> have proposed in 2016 a Seq2Seq model to recognize Gothic handwritten texts with two main differences over our approach: they neither used convolutions to extract features from the handwriting images, nor an attention mechanism into the Seq2Seq architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">High level description of the word prediction model 95</head><p>In this section we describe the model architecture at high level. It is based on the Seq2Seq approach, introduced by Sutskever et al. <ref type="bibr" target="#b9">[10]</ref>. The model transforms a variable-length sequence of handwritten word image slides into the variable-length sequence of the characters that conform the previous word.</p><p>The proposed architecture is inspired by two of the main cognitive psychol-100 ogy models relative to handwriting word recognition: the Serial Letter Recognition (SLR) and the Parallel Letter Recognition (PLR) models <ref type="bibr" target="#b25">[25]</ref>. The SLR model states that words are read letter by letter, serially from left to right. On the other hand, the PLR model states that letters within a word are recognized simultaneously, and this joint information is used to recognize the word. The 105 advantage of our architecture is that it considers the two previous models, and so, it decodes a word character by character but considering the whole input into each decoded character.</p><p>Our proposed model has three main components: a convolutional reader, an encoder and a decoder with an attention mechanism. These components are 110 combined into a Seq2Seq architecture as shown in Fig. <ref type="figure" target="#fig_1">1</ref>.</p><p>The convolutional reader function C is a deep CNN that converts each input image patch p into a vector of visual features x. The inputs to this convolutional reader consist in a sequence of patches obtained from a word image. Note that some patches can include a part of a letter or an intersection between two 115 letters. The convolutional model extracts feature vectors of features of these patches related to the letters that appears in it. So, the convolutional reader is applied over a sequence of patches generating a sequence of convolutional features. The attention mechanism A helps to put the focus on specific parts of the 125 encoder output features h that are relevant for specific output letters. For example, the first letters need to put the attention in the first elements of the encoder output features, that represent the first part of the word image. On the other side, the last letters need to put the attention in the final part of the encoder features, that are associated with the last part of the word image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>130 Finally, a decoder D gets the previous encoder output transformed by the attention mechanism A to predict letter by letter the word. The decoder consists in a LSTM that generates each current letter l t combining the state vector c, the encoder outputs transformed by the attention mechanism a and the previous</p><formula xml:id="formula_0">A C C E P T E D M A N U S C R I P T letter l t-1 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>135</head><p>We can summarize all the previous processes by the following sequence of equations:</p><formula xml:id="formula_1">x = C(p) (c, h) = E(x) a = A(c, h) l t = D(a, c, l t-1 ) (1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Architecture and algorithmic details</head><p>In this section, we describe the implementation of the proposed model and the relevant details about its components. We start with a description of the 140 data preprocesing, which facilitate the recognition task of handwriting text.</p><p>Next, we continue with a detailed description of the Seq2Seq architecture used by our model. In this description we explain the details of the five model components: convolutional reader, LSTM layers, encoder, decoder and attention mechanism, respectively. Fig. <ref type="figure" target="#fig_0">2</ref> shows a detailed schema of the complete model 145 architecture. This figure follows the notation introduced in Section 3.</p><p>Finally, we point out some relevant implementation details including: hardware, libraries, parametrization of the optimization algorithm and data augmentation strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Data Preprocesing 150</head><p>The data used in our experiments were preprocessed to correct some usual characteristics of the handwriting text that difficult the recognition task. This preprocessing was applied at line-of-text level. In particular, we identified the baseline and the corpus line, corrected the line skew, corrected the slant and normalize the height of the characters based on baseline and corpus line.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>155</head><p>To detect the baseline we applied a Random Sample Consensus (RANSAC) regression, proposed by Fischler et al. <ref type="bibr" target="#b26">[26]</ref>, over the lower points of each column in the image of a given line. We applied the same method over the higher points the slant, we first identified the slant angle α using the algorithm detailed in <ref type="bibr" target="#b27">[27]</ref>. Next, by knowing the slant angle α and the image width w, we corrected the slant using the affine transformation matrix defined by Eq. ( <ref type="formula" target="#formula_3">2</ref>).</p><formula xml:id="formula_2">A C C E P T E D M A N U S C R I P T</formula><formula xml:id="formula_3">  1 -α 0.5 • w • α 0 1 0  <label>(2)</label></formula><p>Finally, we adjusted the height of the characters by resizing it over the corpus line and under the baseline to a half of the height of the main part of the line (the</p><formula xml:id="formula_4">165</formula><p>part which is located between the baseline and the corpus line). Fig. <ref type="figure" target="#fig_3">3</ref> shows one example of an original line of the IAM database and the result produced after the described normalization process.</p><p>Next, we extract, for each original word image, a sequence of horizontal overlapped slides. These slides can include a part of a letter, one letter or more  model to view the word image not globally but as a sequence of patches. These patches are characterized by two parameters: the patch size (i.e. its horizontal size) and the patch step (i.e. the number of pixels that we move to the right to extract the next patch). Fig. <ref type="figure" target="#fig_4">4</ref> shows an example of an original word image 175 and its corresponding extracted patches.</p><formula xml:id="formula_5">A C C E P T E D M A N U S C R I P T</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Model components</head><p>The proposed solution is designed to model each letter l i in the word w = (l 1 , l 2 , ..., l n ) as a conditional probability of the input image patches P atches = (p 1 , p 2 , ..., p m ) and the probability distribution of the previous letters of the 180 word l j with j &lt; i.</p><formula xml:id="formula_6">P (l i |P atches, l &lt;i )<label>(3)</label></formula><p>Therefore we model the output probability distribution of a word given the sequence of patches, as follows: </p><formula xml:id="formula_7">A C C E P T E D M A N U S C R I P T</formula><formula xml:id="formula_8">= i P (l i |P atches, l &lt;i )<label>(4)</label></formula><p>In the next subsections, we detail all the elements of the architecture that implements this model. We apply this architecture model to each patch of the input sequences extracted from a word image. From each patch, the CNN computes a vector that contains the visual features related to the character or characters included in   <ref type="table" target="#tab_0">1</ref>.</p><p>Following the notation of Fig. <ref type="figure" target="#fig_0">2</ref>, the output of this CNN is a sequence of vector features x = (x 1 , x 2 , ..., x n ) that encapsulates the visual features of the patches extracted from a handwritten word image.</p><formula xml:id="formula_9">210 x i = C(p i ) (5)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">LSTM layers</head><p>Long Short-Term Memory (LSTM) networks are a special type of Recurent Neural Networks (RNN) which are able to learn long-term dependencies. Fig. <ref type="figure" target="#fig_8">6</ref>, provided by Graves et al. <ref type="bibr" target="#b29">[30]</ref>, includes a schema of the LSTM cell. LSTM networks were introduced by Hochreiter and Schmidhuber <ref type="bibr" target="#b30">[31]</ref> and include two 215 ways to translate the previous information across the net: the output vector h and the state vector c, that combined using three gates, are explicitly designed to store and propagate long-term dependencies.</p><p>The gate i, which is named the input gate, learns which values will be updated in the state vector. The gate f , which is named the forget gate, learns </p><formula xml:id="formula_10">A C C E P T E D M A N U S C R I P T</formula><formula xml:id="formula_11">225 f t = σ (W xf x t + W hf h t-1 + W cf c t-1 + b f ) i t = σ (W xi x t + W hi h t-1 + W ci c t-1 + b i ) c t = f t c t-1 + i t tanh (W xc x t + W hc h t-1 + b c ) o t = σ (W xo x t + W ho h t-1 + W co c t + b o ) h t = o t tanh (c t )<label>(6)</label></formula><p>where W and b are trainable parameters, σ is the sigmoid function, x is the input sequence, i is the input gate, f is the forget gate, o is the output gate, c</p><p>is the state vector and h is the output vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3.">Encoder</head><p>The goal of this component is to capture the horizontal relationships exist-230 ing among the input data, which consist in a sequence of overlapped horizontal</p><formula xml:id="formula_12">A C C E P T E D M A N U S C R I P T</formula><p>patches of handwritten word images. To do that, the encoder receives the sequence of vector features x t , that represent each input image patch p t generated by the convolutional reader, and generates two outputs: a sequence of state vectors c t and a sequence of output vectors h t , each one for each encoder step t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>235</head><p>&lt; c e i , h e t &gt;= LST M (x e t , c e t-1 , h e t-1 )</p><p>The encoder is implemented using LSTM layers. We experimented with different LSTM architectures for the encoder. We try it using one LSTM layer, two LSTM layers stacked and even changing simple LSTM layers by bidirectional LSTM layers using one, two and three stacked layers. We also experimented with the size of the layers in order to identify the optimal architecture for the 240 encoder. In subsection 5.4 we detail the results of these experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4.">Decoder</head><p>The decoder is also based on a LSTM network combined with an attention mechanism. For each decoding step t the LSTM generates a state vector c d t and an output vector h d t using as input the context vector a t provided by the atten-245 tion mechanism, concatenated with the probability distribution of the previous letter predicted l t-1 . The initial state of the decoder c d 0 is initialized with the last state of the encoder c e n and the initial letter is initialized by the special character &lt; GO &gt;.</p><formula xml:id="formula_14">α t = A(h e t , c e t ) &lt; c d t , h d t &gt;= LST M (concat(l t-1 , a i ), c d t-1 , h d t-1 ) c d 0 = c e last l 0 =&lt; GO &gt;<label>(8)</label></formula><p>The probability distribution of the output character for each step is gener-250 ated by a dense layer with the sof tmax activation function applied over the output sequence concatenated with the attention mechanism output.</p><formula xml:id="formula_15">P (y i |X) = sof tmax(dense(concat(h i , a i )))<label>(9)</label></formula><p>A The recurrent nature of a LSTM decoder will try to predict the next character of a word using the prediction of the previous character. This means that the decoder could be acting as a character level language model. However, our model is trained at word level and this fact limit considerably the capacity of the 260 decoder to act as a true language model (a true character language model typically needs characters of the previous words to capture the grammar). For this reason, we made that the decoder prediction depends also on the final encoder state and on the output of an attention mechanism, which will be described in the next subsection. These two components will provide to the decoder the 265 additional features needed to predict correctly the output characters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.5.">Attention mechanism details</head><p>Our model uses the attention mechanism proposed by Chorowski et al. <ref type="bibr" target="#b10">[11]</ref>.</p><p>This mechanism allows the decoder to access to the features of the LSTM encoder at every decoding step. For each step, we calculate the attention states 270 as a function of the outputs and the states of the encoder. This allows that each step of the decoder puts its attention in a specific part of the output of the encoder. This is very important on the architecture, because it allows the decoder to play attention to the input data part that is more relevant to decode the current letter. For example, if the decoder is estimating the first letter of 275 the word, it can put the attention in the features that are generated by the first part of the word image. And so on, with all the letters to be predicted.</p><p>The detailed equations of the attention mechanism are:</p><formula xml:id="formula_16">e t,l = w T tanh(W c e t-1 + V h e l + b) a t,l = exp(e t,l ) L l=1</formula><p>exp(e t,l ) <ref type="bibr" target="#b9">(10)</ref> where w and b are trainable weight and bias vectors, W and V are trainable matrices, c e is the encoder state and h e is the encoder output. Finally, l runs 280 from the first output letter to the last letter L. The attention mechanism is implemented as a neural network layer that combines the states and the outputs of the encoder with the hyperbolic tangent as activation function. Then, we apply the sof tmax function to normalize the columns of the matrix E, in order to add one by column in the attention outputs a t,l .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>285</head><p>Reading a handwritten word could be considered as a monotonous task, because you need to read the left letters before the right ones. But this fact is not true for algorithms. Reading letters of a word, written using a latin script, from left to right is a conventionalism, and it is possible reading them from right to left without loss of information. So, to allow the model considering all the 290 neighborhood information, the selected attention mechanism has no monotonicity restrictions.</p><p>To illustrate this assumption, we plot in Fig. <ref type="figure" target="#fig_10">7</ref> the outputs a t,l of the attention mechanism for two words of the validation partition. These figures present in the horizontal axis the image patch number and in the vertical axis 295 the predicted character position. The figures show that, in general, the attention mechanism is working monotonically, but for some character positions it uses information from patches at the left of what will be a monotonous behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Evaluation results</head><p>In this section, we describe the evaluation results of the model applied on 300 two well known handwritting datasets. The IAM offline handwritting <ref type="bibr" target="#b13">[13]</ref> and the RIMES databases <ref type="bibr" target="#b14">[14]</ref>.</p><p>Two metrics were used to evaluate the model: the Character Error Rate (CER), measured as the Levenstein distance between the predicted and the real character sequence of the word, and the Word Error Rate (WER), defined as 305 the percentage of words incorrectly identified. We evaluated these metrics with the direct output of the model and also after the application of a basic language model based on dictionary search in standard lexicons.</p><p>In the process of optimizing the selected architecture, we performed several experiments by changing the parametrization of the model to determine the 310 effects in the error of changing different parameters. In subsections 5.4 and 5.5</p><p>we describe the results of these experiments.</p><p>We compare the best results obtained by the proposed model with other approaches over the same databases and with the same lexicons, when it is possible. These results are included in subsection 5.6</p><p>315 Finally, we dedicate a final subsection to analyze the nature of the error by investigating the error types that we got, in order to propose possible improvements to the current model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Databases overview</head><p>The IAM database was introduced by Marti et al. <ref type="bibr" target="#b13">[13]</ref>. It consists on The RIMES database <ref type="bibr" target="#b14">[14]</ref> consists on images of handwritten letters from sim-330 ulated French mail. We used the ICDAR 2011 competition partition provided by the A2IA Lab, which is the maintainer of the database. It includes 51,739 words to train, 7,464 words to validation and 7,776 words to test, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Implementation details</head><p>To use our framework with these two datasets (IAM and RIMES), we need 335 to re-size the word images to a fixed height of 48 pixels and a fixed width of 192 pixels. As we want to keep the aspect ratio of the words, usually we need to complete the images with white pixels in the right part of them.</p><p>First, we extract the sequence of overlapped image patches. Due to the height of the images, all the patches have a height of 48 pixels. We experi- To implement our solution we used the TensorFlow library <ref type="bibr" target="#b31">[32]</ref> and a GeForce Titan X Pascal GPU. As trainer we used the Adam stochastic optimization 350 algorithm, with a batch size of 256 and with a dropout of 0.5 <ref type="bibr" target="#b32">[33]</ref> in the dense layers of the convolutional part and in the cells of the encoder LSTM layers. An initial learning rate of 0.001 was used and this rate was decreased by 2% in each epoch. We applied a stopping criteria of 10 epochs without any improvement in the Word Error Rate (WER) on the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>355</head><p>In each epoch, we randomized the order of the training data. In addition,</p><p>we implemented an online randomized data augmentation process that included:</p><formula xml:id="formula_17">A C C E P T E D M A N U S C R I P T</formula><p>±10 percent of zoom, ±10 percent of displacement, ±10 degrees of skew, and 3 × 3 dilation and erosion transformations.</p><p>The different experiments took between 60 and 120 epochs. Depending on 360 the stopping criteria and the model parameters, each experiment lasted between 10 and 20 hours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Lexicons used</head><p>We provide different results for each considered database (IAM and RIMES).</p><p>In particular, we offer three main results for each experiment. First, the results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>365</head><p>obtained by the direct application of the model (without any lexicon usage); second, the results using the standard validation lexicons mainly used in the literature; and third the results using the test set lexicon.</p><p>In the case of the IAM database, the standard validation lexicon used was the 50,000 most frequent words of the Lancaster-Oslo/Bergen (LOB) <ref type="bibr" target="#b33">[34]</ref> and</p><formula xml:id="formula_18">370</formula><p>the Brown <ref type="bibr" target="#b34">[35]</ref> corpora, when previously the IAM paragraphs from the Brown corpus were removed. In the case of the RIMES database, we use as standard lexicon the set of all the words present in the train and validation partitions.</p><p>With the previous settings, three different groups of results are shown: the raw accuracy of the visual model without any lexicon, the improvement obtained 375 when using search over the standard lexicon of each database, and finally, the error obtained when eliminating the out-of-vocabulary (OOV) words in the test lexicon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Analysis of the image segmentation strategy</head><p>In this first set of experiments, we analyzed the effect over the final model 380 accuracy of the different parametrizations corresponding to the initial process of image patches creation described in subsection 4.1. We considered two main parameters: the patch width and the step size to create the patches. For example, a patch width of 5 and a step size of 2 means that the first patch is created with the columns from 1 to 5, the second patch with the columns from 3 to 7,  We started analyzing the step size with a fixed width of 10 and a fixed encoder architecture of a bidirectional LSTM with 2 layers and a layer size of 256. Fig. <ref type="figure" target="#fig_14">8</ref> shows that for both databases a larger step size produces a larger error, and that the better validation error is achieved with a step size of 2. 390 So, we determined an optimal value of 2 for the step size, and a fixed encoder architecture of a bidirectional LSTM with 2 layers of size 256. Because we used a convolutional architecture over the patches, the patch width must have a minimal size to apply the convolutions. This minimal width is 5, because the initial convolution layer has a filter of size 5 × 5. We tested sizes between 5 and 395 25 in intervals of 5. Fig. <ref type="figure" target="#fig_16">9</ref> shows that smaller or larger patch sizes produced a greater error, and the minimum error was achieved for the patch size of 10 pixels.  Minimal WER was achieved with a patch size of 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Analysis of the LSTM encoder parametrization</head><p>Next, we analyzed the effect of the encoder LSTM architecture and its size 400 on the final error. To do this, we considered four different architectures for the encoder component that included unidirectional LSTM with 1 and 2 layers and bidirectional LSTM with 1, 2 and 3 layers. We also experimented with different sizes for the LSTM layers. In particular, we experimented with 64, 128, 256 and 512 cell sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>405</head><p>In the first experiment we used a fixed encoder size of 256 and the optimal step size and patch width found in the previous experiments. Fig. <ref type="figure" target="#fig_1">10</ref> shows that best error results were obtained with an encoder architecture of two bidirectional LSTM (BLSTM) layers <ref type="bibr" target="#b7">[8]</ref>.</p><p>For the previous optimal architectures, we experimented with different LSTM 410 cell sizes for the encoder and obtained the results shown in Fig. <ref type="figure" target="#fig_1">11</ref>. We observed that optimal result is achieved with the 256 cell size.  First, we compared the WER and CER results obtained directly by our visual model with optimal parametrization for the validation set. In all cases, the selected parametrization for our model was the previously identified as optimal in the validation set in the experiments described in subsections 5.4 and 5.5 450 respectively. In summary, a step size of 2 pixels, a patch width of 10 and an encoder architecture of 2 bidirectional LSTM of size 256 were used. The results are shown in Table <ref type="table" target="#tab_1">2</ref>. The visual models by Pham et al. <ref type="bibr" target="#b32">[33]</ref> and by Bluche et al. <ref type="bibr" target="#b35">[36]</ref> were a multidimensional LSTM architecture with a CTC decoder extensively used in handwriting recognition with excellent results. Bluche <ref type="bibr" target="#b36">[37]</ref> 455   challenge <ref type="bibr" target="#b39">[41]</ref>. The three subtasks corresponding to three sizes of dictionaries: a 99 words dictionary chosen randomly among test words, the test words dictionary and the train + test words dictionary. For this three subtasks our optimal model obtained a WER of 98.30%, 93.77% and 92, 17% respectively.</p><formula xml:id="formula_19">proposed</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7.">Error analysis 490</head><p>This section analyses in detail the errors of the presented models in order to identify possible improvement lines of work. The first analysis was performed in the previous subsection, by studying the influence of the OOV words in the error. These errors can be reduced by improving the lexicons and, specially, by adding to the system some language models at character level that can identify 495 OOV words.</p><p>First, we analyze how the errors change in terms of the word length. Fig. <ref type="figure" target="#fig_19">12</ref> shows the WER with the standard lexicons in the IAM and RIMES databases respect to the word length. We observe that the error increases with the word length. This is related with the previous analysis on the OOV influence in the 500 errors, because the words with less letters are articles, prepositions and similar words that typically appears in the lexicons.</p><p>To eliminate the influence of the OOV words in the analysis of the error versus the word length, we show in Fig. <ref type="figure" target="#fig_21">13</ref> the WER test error using the test lexicon. We observed that effectively most of the errors in long words are   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we have presented a system for recognizing offline handwritten words. The proposed architecture is inspired in the way that the humans try to identify handwritten words and it is based in the Seq2Seq architecture with The obtained results, when applying our visual model over two standard databases, are competitive with those other published in the literature. We have achieved in some cases the best published results with a direct visual model or with a lexicon search.</p><p>545</p><p>As future research we plan to test if this model can also work well with other languages like Chinese or Arabian. Also, we will test the model at line level. With respect to the error analysis, we plan to explore new possibilities by including language models, at character and word levels, in order to improve the decoder component of the model. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>50 2 .</head><label>2</label><figDesc>Related workOver the last decades, different approaches have been applied to the handwriting word recognition problem (see Plamondon et al.<ref type="bibr" target="#b2">[3]</ref>). The main differences between the authors appear in two key elements of the proposed solutions: a) the strategy to extract features from the handwriting image, and b) the way 55 to decode the output of the classifier to predict the sequence of characters which form part of a given word. Two main strategies are commonly used to extract image features: applying different Computer Vision techniques to detect them or use directly the pixels A C C E P T E D M A N U S C R I P T columns of the image as raw features. The first strategy is applied by Doetsch 60 et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Components of the high-level model architecture for word recognition</figDesc><graphic coords="8,138.26,282.60,145.01,64.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Detailed model architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Line preprocessing example: (a) Original line and (b) preprocessing results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Word patch extraction: (a) Original image word and (b) extracted input model patches.</figDesc><graphic coords="11,142.15,176.34,293.14,292.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: CNN architecture schema used in the convolutional encoder component</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>185 4 .</head><label>4</label><figDesc>2.1. Convolutional readerThe objective of the convolutional reader is to extract the visual features which are relevant to identify the characters appearing in the input patches of the word image.Our model uses a deep convolutional architecture (CNN) stacking two groups 190 of one convolutional and one max pooling layers and finishing with one dense layer. Figure5shows the detailed CNN architecture of the model proposed.This architecture was inspired in the LeNet-5 architecture, published originally by LeCun et al.<ref type="bibr" target="#b28">[28]</ref> to solve the problem of identifying isolated handwriting characters. Similar architectures achieved good results to solve the handwriting 195 character identification problem over different handwritten character databases [29].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>200 A C C E P T E D M A N U S C R I P T</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Long short-term memory cell</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>C C E P T E D M A N U S C R I P T The decoder starts with the special signal &lt; GO &gt; as input to decode the first character of the word image. It ends when at the output appears the special character &lt; EN D &gt;, by selecting the sequence of previous output characters 255 as the decoded word.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Example of attention mechanism outputs for two words and their respective original handwritten word images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>320</head><label></label><figDesc>several form images of handwritten English text. The database is divided in train, validation and test partitions and it has been used extensively in the literature. See for example Doetsch et al. [15], Bluche et al. [18] or Graves et al. [1]. The partition consists on 6,482, 976 and 2,915 lines for the training, validation and test sets, respectively. That means a total of 55,081, 8,895 and 325 25,920 words in each partition. For the analysis, we selected the words marked that are segmented correctly in the IAM database, with this selection we obtain A C C E P T E D M A N U S C R I P T a final 47,952, 7,558 and 20,306 words for the training, validation and test sets, respectively</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>340</head><label></label><figDesc>mented with different widths for the patches and with different levels of patch overlapping. The results of these experiments are presented in subsection 5.4In addition, we also performed experiments with the layers and size of the encoder using unidirectional and bidirectional LSTM, one or two layers and different layer sizes (from 64 to 512). The results of these experiments are pre-345 sented in subsection 5.5. In all the experiments the model weights are randomly initialized. For the decoder, we used a single unidirectional LSTM of the same size of the encoder LSTM layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: WER in validation and test sets vs step size for IAM and RIMES databases. Minimal WER was achieved with a step size of 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: WER in validation and test sets vs patch size for IAM and RIMES databases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 10 :Figure 11 :</head><label>1011</label><figDesc>Figure 10: WER in validation and test sets vs encoder architecture for IAM and RIMES databases. Minimal WER achieved in 2 layers BLSTM architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head></head><label></label><figDesc>two visual models on pixels features: first a deep MLP/HMM architecture and next a RNN/HMM architecture. Our proposed model outperforms significantly the MLP/HMM and the RNN/HMM architectures, and provides competitive results respect to the MDLSTM/CTC architecture obtaining the best result for the WER in IAM database. 460 In the second comparative, we process the outputs of the visual model finding the closest word in terms of Levenstein distance over standard lexicons for IAM and RIMES databases, extensively used in the handwriting literature and A C C E P T E D M A N U S C R I P T</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>505Figure 12 :</head><label>12</label><figDesc>Figure 12: WER in test vs word length.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: WER in test vs word length with the test lexicon.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>530</head><label></label><figDesc>the addition of a convolutional network. Our model has three main components oriented to capture the different visual aspects of the handwriting text. First, a convolutional network, that extracts visual features from several image patches, trying to obtain relevant features of the characters present in the word. Second, a Recurrent Neural Network, that captures the sequential relationships among 535 the previous extracted features. Finally, another Recurrent Neural Network, that decodes the sequence of characters to predict the input word. The main advantage of the proposed model is the integrated management A C C E P T E D M A N U S C R I P T of local aspects of a word image related with: the characters that compose the word (convolutional network), the sequential horizontal aspects of the word 540 image (encoder) and the sequence of characters in the word (decoder).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head></head><label></label><figDesc>S C R I P T neural networks, in: Proceedings of the 23rd international conference on</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparative of WER results for different dense layer sizes of the convolutional reader.</figDesc><table><row><cell>Database</cell><cell>Dense size</cell></row><row><cell></cell><cell>512 1024 2048</cell></row><row><cell>IAM</cell><cell>21,1 18,9 Not convergence</cell></row><row><cell>RIMES</cell><cell>13,6 12,8 13,2</cell></row><row><cell cols="2">it. The image patches are the inputs, and the convolutional features obtained</cell></row></table><note><p>from the last dense layer are the outputs. We tested three different sizes for this final dense layer (respectively, with 512, 1024 and 2048 neurons). These tests were made over the optimal architecture obtained in section 5. They let us to identify that the 1024 size shows the best WER over the validation set. We 205 detail the results of these experiments, calculating the WER on the standard lexicon introduced in subsection 5.3, in Table</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparative of results of the visual model without lexicon.</figDesc><table><row><cell></cell><cell>Database</cell><cell>IAM</cell><cell></cell><cell></cell><cell>RIMES</cell></row><row><cell></cell><cell></cell><cell>WER</cell><cell>CER</cell><cell>WER</cell><cell>CER</cell></row><row><cell></cell><cell>Our method</cell><cell cols="4">23.8±0.05 8.8±0.02 15.9±0.4 4.8±0.1</cell></row><row><cell></cell><cell cols="2">Pham et al. [33] 35.1</cell><cell>10.8</cell><cell>28.5</cell><cell>6.8</cell></row><row><cell></cell><cell cols="2">Bluche [37] MLP 54.2</cell><cell>15.6</cell><cell>59.5</cell><cell>17.8</cell></row><row><cell></cell><cell cols="2">Bluche [37] RNN 24.7</cell><cell>7.3</cell><cell>20.9</cell><cell>5.6</cell></row><row><cell></cell><cell>Bluche [36]</cell><cell>24.6</cell><cell>7.9</cell><cell>12.6</cell><cell>2.9</cell></row><row><cell></cell><cell cols="5">pre-processing corrections, and one of the three previous post-processing lexi-</cell></row><row><cell></cell><cell cols="5">cons. We show the comparatives in Tables 2, 3 and 4, respectively, in terms of</cell></row><row><cell>445</cell><cell cols="5">WER and CER in the test set. The differences observed are caused mainly to</cell></row><row><cell></cell><cell cols="2">the different visual models used.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparative of results of the visual model with a standard lexicon.</figDesc><table><row><cell>Database</cell><cell></cell><cell>IAM</cell><cell></cell><cell>RIMES</cell></row><row><cell></cell><cell>WER</cell><cell>CER</cell><cell>WER</cell><cell>CER</cell></row><row><cell>Our method</cell><cell cols="4">19.7±0.03 9.5±0.03 13.1±0.2 5.7±0.1</cell></row><row><cell cols="2">Bluche [37] MLP 25.5</cell><cell>8.0</cell><cell>26.1</cell><cell>7.2</cell></row><row><cell cols="2">Bluche [37] RNN 16.7</cell><cell>5.3</cell><cell>16.4</cell><cell>4.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Comparative of results of the visual model with the test lexicon.</figDesc><table><row><cell>Database</cell><cell></cell><cell>IAM</cell><cell cols="2">RIMES</cell></row><row><cell></cell><cell>WER</cell><cell>CER</cell><cell>WER</cell><cell>CER</cell></row><row><cell>Our method</cell><cell cols="4">12.7±0.03 6.2±0.02 6.6±0.2 2.6±0.1</cell></row><row><cell cols="2">Almazan et al. [39] 20.1</cell><cell>11.2</cell><cell></cell></row><row><cell>Bluche et al. [40]</cell><cell>20.5</cell><cell></cell><cell>9.2</cell></row></table><note><p><p><p><p><p><p>described in subsection 5.3. The results can be viewed in Table</p>3</p>. In this case the available results are provided by Bluche</p><ref type="bibr" target="#b36">[37]</ref> </p>with the previous deep 465 MLP/HMM and the RNN/HMM architectures. In this case, results are also competitive obtaining the best WER result on RIMES database.</p>In the third comparative, we used the test lexicon as a dictionary to find the closest word. Although this measure can not be applied to evaluate the unconstrained performance of the system , with this metric we can evaluate 470 the impact in the error of the out-of-vocabulary (OOV) words not present in the standard lexicons. The results are presented in</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>In this case, we obtained a significant reduction of WER and CER errors, with competitive results for both databases. Poznanski et al.[38] also used the test set lexicon, obtaining better results that those shown in Table4. However, these authors 475 used a limited set of characters in their results for the two databases. This makes their results not comparable with those shown in Table4.</figDesc><table><row><cell>The big differences between the errors with standard lexicons and with test</cell></row><row><cell>lexicons point out that the OOV errors are very important. The respective</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>than one letters. These slides are the inputs to our model, and they enable the</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="220" xml:id="foot_1"><p>the information from the previous state that can be thrown away. With the</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was funded by the Spanish Ministry of Economy and Competitiveness under projects numbers TIN2014-57458-R and TIN2017-85221-R.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>In summary, the optimal parametrization for the both IAM and RIMES databases is a model that uses patches with 10 pixels of width and a step size of 2 pixels, and with an encoder with a cell size of 256 and two bidirectional 415 LSTM layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Comparative with other approaches</head><p>This work proposes a novel visual model to solve the problem of handwriting word recognition. The fair comparative with other visual models proposed by other authors must be in equal conditions of image pre-processing and prediction 420 post-processing. But this is very difficult because different authors use their own processes, that include specific image pre-processing and specific language models for the post-processing, and usually only provide their final results .</p><p>The pre-processing stage is the minor problem, because practically all the authors use the same databases, with the same partition in train, validation and 425 test. So, they perform a similar word image correction, that includes some type of contrast enhancement, and skew and slant corrections. That is the reason why we expect similar results in the final corrected word images. However, in the post-processing we find more difficulties, because the usage of different language models (i.e. at word or at character levels) which combined with 430 different lexicons, can produce very different error results.</p><p>In order to provide a fair comparative with other authors, centered in the performance of the visual models, and to facilitate the comparative to future authors, we present three sets of error results. First, the direct error of the visual model without post-processing. Second, the error using direct search in 435 the standard lexicons described in subsection 5.3. Third, the error using the test set lexicon in order to identify the error component produced by the outof-lexicon words. For all of them, we provide the WER and CER errors over the test set, and a confidence error interval, calculated with 10 repetitions of the optimal model. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Offline handwriting recognition with multidi-555 mensional recurrent neural networks, in: Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="545" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Sayre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine recognition of handwritten words: A project report</title>
		<imprint>
			<date type="published" when="1973">1973</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="213" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Online and off-line handwriting recognition: a 560 comprehensive survey</title>
		<author>
			<persName><forename type="first">R</forename><surname>Plamondon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Srihari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="63" to="84" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Offline handwritten english character recognition based on convolutional neural network, in: Document Analysis A</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu ; C C E P T E D M A N U S C R I P T Systems</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 10th IAPR International Workshop on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="125" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Role of offline handwritten character recognition system in various applications</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N</forename><surname>Manisha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Krishna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Applications</title>
		<imprint>
			<biblScope unit="volume">135</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="30" to="33" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Handwritten 570 text recognition for historical documents in the transcriptorium project</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bosch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Depuydt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>De Does</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First International Conference on Digital Access to Textual Cultural Heritage</title>
		<meeting>the First International Conference on Digital Access to Textual Cultural Heritage</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="111" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Markov models for offline handwriting recognition: A survey</title>
		<author>
			<persName><forename type="first">T</forename><surname>Plötz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Fink</surname></persName>
		</author>
		<idno type="DOI">10.1007/575s10032-009-0098-4</idno>
		<ptr target="http://dx.doi.org/10.1007/s10032-009-0098-4" />
	</analytic>
	<monogr>
		<title level="j">Int. J. Doc. Anal. Recognit</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="269" to="298" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A novel connectionist system for unconstrained handwriting recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bertolami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bunke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="855" to="868" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">End-to-end text recognition with convolutional neural networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition (ICPR), 2012 21st International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="3304" to="3308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with 585 neural networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Attentionbased models for speech recognition</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="577" to="585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="87" to="103" />
		</imprint>
	</monogr>
	<note>Lip reading in the wild</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName><surname>A C C E P T E D M A N U S C R I P T</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The iam-database: an english sentence database for offline handwriting recognition</title>
		<author>
			<persName><forename type="first">U.-V</forename><surname>Marti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bunke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Document Analysis and Recognition</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="46" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rimes evaluation campaign for handwritten mail processing</title>
		<author>
			<persName><forename type="first">E</forename><surname>Augustin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Carré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grosicki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Brodin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Geoffrois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Prêteux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Frontiers in Handwriting Recognition (IWFHR&apos;06)</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="231" to="235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fast and robust training of recurrent neu-600 ral networks for offline handwriting recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Doetsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kozielski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Frontiers in Handwriting Recognition (ICFHR), 2014 14th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="279" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improvements in rwth&apos;s system for offline handwriting recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kozielski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Doetsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Document Analysis and Recognition (IC-605 DAR), 2013 12th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="935" to="939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Bideault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mioulet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chatelain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Paquet</surname></persName>
		</author>
		<title level="m">Spotting handwritten words and regex using a two stage blstm-hmm architecture</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="94020G" to="94020" />
		</imprint>
	</monogr>
	<note>SPIE/IS&amp;T Electronic Imaging, International Society for Optics and Photonics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Bluche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Messina</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.03286</idno>
		<title level="m">Scan, attend and read: End-to-end handwritten paragraph recognition with mdlstm attention</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Feature extraction with convolutional neural networks for handwritten word recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bluche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kermorvant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Document Analysis 615 and Recognition (ICDAR), 2013 12th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="285" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent Machine learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Handwriting recognition with large multidimensional long short-term memory recurrent neural networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Doetsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Frontiers in Handwriting Recognition (ICFHR), 2016 15th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="228" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multidimensional recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fernndez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 International Conference on Artificial Neural Networks (ICANN)</title>
		<meeting>the 2007 International Conference on Artificial Neural Networks (ICANN)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Cohort of lstm and lexicon verification for handwriting recognition with gigantic lexicon</title>
		<author>
			<persName><forename type="first">B</forename><surname>Stuner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chatelain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Paquet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.07528</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint 630</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Offline handwriting recognition using lstm recurrent neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sabatelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Shkarupa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th Benelux Conference on Artificial Intelligence (BNAIC)</title>
		<meeting>the 28th Benelux Conference on Artificial Intelligence (BNAIC)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="88" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">The science of word recognition, Advanced Reading Technology</title>
		<author>
			<persName><forename type="first">K</forename><surname>Larson</surname></persName>
		</author>
		<imprint>
			<publisher>Microsoft Corporation</publisher>
			<biblScope unit="volume">635</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="381" to="395" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A lexicon driven approach to handwritten word 640 recognition for real-time applications</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Govindaraju</surname></persName>
		</author>
		<idno type="DOI">10.1109/34.588017</idno>
		<ptr target="http://dx.doi.org/10.1109/34.588017" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="366" to="379" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>-R. Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, speech and signal processing (icassp), 2013 ieee international conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<title level="m">Tensorflow: Large-scale machine learning on heterogeneous distributed systems</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dropout improves 660 recurrent neural networks for handwriting recognition</title>
		<author>
			<persName><forename type="first">V</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bluche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kermorvant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Frontiers in Handwriting Recognition (ICFHR), 2014 14th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="285" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">The tagged {LOB} corpus: User\&apos;s manual</title>
		<author>
			<persName><forename type="first">S</forename><surname>Johansson</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Manual of information to accompany a stan-665 dard corpus of present-day edited American English, for use with digital computers</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">N</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kučera</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
		</imprint>
		<respStmt>
			<orgName>Brown University, Department of Linguistics</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Joint line segmentation and transcription for end-to-end handwritten paragraph recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bluche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="838" to="846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Deep neural networks for large vocabulary handwritten text recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bluche</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
		<respStmt>
			<orgName>Université Paris Sud-Paris XI</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Word spotting and recognition with embedded attributes</title>
		<author>
			<persName><forename type="first">J</forename><surname>Almazán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fornés</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Valveny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2552" to="2566" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Tandem hmm with convolutional neural network for handwritten word recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bluche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kermorvant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal 680 Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page" from="2390" to="2394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Icdar 2009 handwriting recognition competition</title>
		<author>
			<persName><forename type="first">E</forename><surname>Grosicki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">El</forename><surname>Abed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Document Analysis and Recognition, 2009. ICDAR&apos;09. 10th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1398" to="1402" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
