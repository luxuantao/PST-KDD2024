<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">D.-W</forename><surname>Kim</surname></persName>
							<email>dwkim@cau.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of BioSystems</orgName>
								<address>
									<country>Korea Advanced</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Science and Technology (KAIST)</orgName>
								<address>
									<postCode>305-701</postCode>
									<settlement>Daejeon</settlement>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Chung-Ang University</orgName>
								<address>
									<postCode>156-756</postCode>
									<settlement>Seoul</settlement>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Department of Electrical Engineering and Computer Science</orgName>
								<orgName type="department" key="dep2">Department of BioSystems</orgName>
								<orgName type="department" key="dep3">Institute of Sci-ence and Technology (KAIST)</orgName>
								<address>
									<postCode>305-701</postCode>
									<settlement>Daejeon</settlement>
									<country>Korea Advanced, Republic of Korea</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7DDBF7EE1FED7C4A51DF23FDC6F2CAB0</idno>
					<idno type="DOI">10.1109/TNN.2006.884673</idno>
					<note type="submission">received February 20, 2006; revised June 27, 2006.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Letters</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Density-Induced Support Vector Data Description</head><p>KiYoung Lee, Dae-Won Kim, Kwang H. Lee, and Doheon Lee Abstract-The purpose of data description is to give a compact description of the target data that represents most of its characteristics. In a support vector data description (SVDD), the compact description of target data is given in a hyperspherical model, which is determined by a small portion of data called support vectors. Despite the usefulness of the conventional SVDD, however, it may not identify the optimal solution of target description especially when the support vectors do not have the overall characteristics of the target data. To address the issue in SVDD methodology, we propose a new SVDD by introducing new distance measurements based on the notion of a relative density degree for each data point in order to reflect the distribution of a given data set. Moreover, for a real application, we extend the proposed method for the protein localization prediction problem which is a multiclass and multilabel problem. Experiments with various real data sets show promising results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Index Terms-Data domain description, density-induced support vector data description (D-SVDD), one-class classification, outlier detection, support vector data description (SVDD).</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>The purpose of data description (also called one-class classification) is to give a compact description of a set of data referred to as target data. It is usually used for outlier detection (the detection of uncharacterized objects in a target data set) or for conventional multiclass classification problems especially where some of the classes are undersampled <ref type="bibr" target="#b0">[1]</ref>. Several approaches such as density estimation approach <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, boundary prediction approach <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, and reconstruction approach using clustering methods <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b5">[6]</ref> have been used to finding out the compact description. When negative data that should be rejected in a compact description are available, conventional multiclass classification methods were also used for finding the compact description of target data <ref type="bibr" target="#b6">[7]</ref>.</p><p>Recently, Tax and Duin <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b4">[5]</ref> have invented support vector data description (SVDD) which was inspired by support vector machines (SVMs) <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b9">[10]</ref>. In an SVDD <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b4">[5]</ref>, the compact description of target data is given as a hypersphere (a; R) with minimum volume containing most of the target data; the objective function</p><formula xml:id="formula_0">O O = R 2 + C + n i=1 i</formula><p>subject to (x i 0 a) 111 (x i 0 a) R 2 + i and i 0; where n is the total number of target data and the parameter C + (&gt; 0) gives the tradeoff between volume of a hypersphere and the number of errors <ref type="bibr" target="#b4">[5]</ref>. Analogous to SVMs <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, the data description is described by a few training data called support vectors <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b4">[5]</ref>, and the kernel trick <ref type="bibr" target="#b8">[9]</ref> is utilized to find a more flexible data description in a highdimensional feature space <ref type="bibr" target="#b4">[5]</ref>.</p><p>Despite the usefulness of an SVDD <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b4">[5]</ref>, however, the conventional SVDD (C-SVDD) has limitations to reflect overall characteristics of a target data set with respect to its density distribution. In the C-SVDD, as mentioned before, the small portion of data called support vectors fully determine the solution of target data description, whereas all of the nonsupport vectors have no influence on the solution of target description in the C-SVDD, regardless of the density distribution. However, the region around a nonsupport vector with higher density degree should be included in a compact description rather than other regions in order to more correctly identify the data description of the given data set. Hence, the solution solely based on the support vectors, without considering the density distribution, can miss the optimal solution.</p><p>To address the previous problem in the C-SVDD, we propose a density-induced SVDD (D-SVDD) to reflect the density distribution of a target data set by introducing the notion of a relative density degree for each data point. By using density-induced distance measurements both for target data and for negative data based on the proposed relative degrees, the D-SVDD can shift the center of hypersphere to the denser region based on the assumption that there are more data points in a denser region. Moreover, for a real application, we extend the D-SVDD to the protein localization prediction problem which is a multiclass and multilabel challenge.</p><p>The structure of this letter is organized as follows. In Section II, we introduce two methods for extracting relative density degrees, and theoretically formulate the proposed D-SVDD with new distance measurements based on the degrees. Section III highlights the potentials of the proposed approach through various experimental examples. In Section IV, we apply the proposed method to a protein localization prediction problem. Concluding remarks are presented in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. DENSITY-INDUCED SVDD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Relative Density Degree and Density Induced Distance</head><p>To reflect the density distribution into the search of optimal solutions of SVDD, we first introduce the notion of relative density degrees. The relative density degree for a data point represents how dense the region of the corresponding data point is compared to other regions in a given data set. Even though several approaches can be applied to extract relative density degrees, in this letter, we propose two methods to extract the relative density degree for each data point from a data set using a nearest neighborhood approach (Method I) and a Parzen-window approach (Method II).</p><p>In Method I, by using d(x i ; x K i ), the distance between x i and x K i (the Kth nearest neighborhood of x i ), and the mean distance of Kth nearest neighborhoods of all target data, = K , the relative density degree i for x i is defined by</p><formula xml:id="formula_1">i = exp ! 2 = K d(x i ; x K i ) ; i= 1; 1 11;n (2)</formula><p>where = K = (1=n) n i=1 d(x i ; x K i ); n is the number of data in a target class, and 0 ! 1 is a weighting factor. Note that this method reports higher relative density degree i for the data point in a higher density region: The data point with lower distance from its Kth nearest neighborhood has a higher i value.</p><p>In Method II, the relative density degree i is defined by the exponentially weighted Parzen-window density that is normalized by = (the mean of all the Parzen-window density degrees); that is, i is defined by i = exp ! 2 Par(x i ) = ; i= 1; 1 11;n</p><p>where</p><formula xml:id="formula_3">Par(xi) = (1=n) n j=1 (1= (2) d s) exp(0(1=2s)(xi 0 xj) 2 ), = = (1=n) n</formula><p>i=1 Par(xi), d is the feature dimension of input data, and s is the smoothing parameter of the Parzen-window density <ref type="bibr" target="#b12">[13]</ref>. Note that this method also reports higher i for the data point with higher Parzen-window density regarding to the mean of Parzen-window densities of all data.</p><p>After calculating the relative density degrees, to incorporate the degrees into searching the optimal description in the SVDD methodology, we propose new geometric distance measures called density-induced distance measures. First, to incorporate the density degrees of target data into searching the optimal description in an SVDD, we propose a new geometric distance called a positive density-induced distance.</p><p>Suppose that each target data point can be represented as (xi; i), where i is the relative density degree of xi. We define a positive den- sity-induced distance + i between target data point xi and the center of a hyperspherical model (a; R) of a target data set as</p><formula xml:id="formula_4">+ i fi(xi 0 a) 1 (xi 0 a)g 1=2 (4)</formula><p>where a and R are the center and the radius of the hypersphere, respectively. Note that + i increases with growing i . Hence, to enclose the data point with increased + i owing to a higher i , the radius of a minimum-sized hypersphere should be increased. The data point with higher relative density degree has stronger influence on the search of the minimum-sized hypersphere.</p><p>To incorporate the density degrees of negative data, we similarly define a negative density-induced distance 0 l between negative data x l and a, the center of the hyperspherical description of a target data set as 0 l 1 l (x l 0 a) 1 (x l 0 a) 1=2 : <ref type="bibr" target="#b4">(5)</ref> Note that, contrary to + l ; 0 l decreases with an increasing l . Hence, to exclude the negative data point with a decreased 0 l owing to a higher l , the radius of a compact hypersphere should be decreased; the negative data point with a higher relative density degree gives a higher penalty on the search of the compact hypersphere for a target data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Mathematical Formulation of D-SVDD</head><p>First, we find the optimal hypersphere that includes most target data by reflecting the relative density degrees of the target data. In this case, When negative data is available, our proposed method can also utilize them to improve the description of the target data set. In this case, using the negative density-induced distance and another slack variable l ( 0) for the possibility of training error in each negative data, we find the optimal hypersphere that includes most target data and excludes most negative data. Here, l = R 2 0( 0 l ) 2 for negative training error data. Thus, the new objective function is defined as</p><formula xml:id="formula_5">O = R 2 + C + n i=1 i + C 0 m l=1 l (8)</formula><p>subject to i(xi0a)1(xi0a) R 2 +i and (1= l )(x l 0a)1(x l 0a) &lt; R 2 0 l , where m is the number of negative data and C 0 &gt; 0 is a control parameter similar to that of the C-SVDD <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b4">[5]</ref>.</p><p>Similar to the previous case, the dual form of this case can be represented by maximizing D() for a target data point; otherwise, y k = 01) and N is the total number of data (N = n + m). Here, 0 k = i for target data x i and 0 k = 1= l for negative data x l . Note that the dual form of this case retains the Lagrange multipliers i and omits the other variables and Lagrange multipliers i . Moreover, when i = 1 (and l = 1), this dual repre- sentation is equivalent to the formalism of a C-SVDD <ref type="bibr" target="#b4">[5]</ref>; this means that the proposed method can act like the C-SVDD.</p><formula xml:id="formula_6">D() = N k=1 0 k 0 k x k 1 x k 0 1 T N p=1 N q=1 0 p 0 q 0 p 0 q xp 1 xq</formula><p>As seen in ( <ref type="formula">7</ref>) and ( <ref type="formula">9</ref>), the dual forms of the objective function of D-SVDD are represented entirely in terms of inner products of input vector pairs. Thus, we can kernelize D-SVDD to find a more flexible description of D-SVDD. The kernelized version of the dual representation in ( <ref type="formula">9</ref>) is</p><formula xml:id="formula_7">D() = N k=1 0 k 0 k K(x k ; x k ) 0 1 T N p=1 N q=1 0 p 0 q 0 p 0 q K(x p ; x q ) (<label>10</label></formula><formula xml:id="formula_8">)</formula><p>where K( 1; 1 ) is a kernel function <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref> and the constraints are the same with those of (9). As shown in ( <ref type="formula">7</ref>), <ref type="bibr" target="#b8">(9)</ref>, and (10), the dual forms are linearly constrained optimization problems. Thus, to solve these problems, we adapt the Powell's TOLMIN procedure <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, which solves linearly constrained optimization problems by solving a sequence of quadratic programming subproblems to minimize the sum of constraint or bound violations. After solving the dual form in <ref type="bibr" target="#b9">(10)</ref>, the center a of solution can be calculated by</p><formula xml:id="formula_9">a = N k=1 0 k 0 k x k T (11)</formula><p>and the radius R is calculated by the + i distance between a and any target data x i of which 0 &lt; 0 i &lt; C + . Note that, different from the C-SVDD <ref type="bibr" target="#b4">[5]</ref>, the center of the optimal hypersphere is weighted by the k ; the center is shifted to a higher density region.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. COMPARISON RESULTS</head><p>To investigate the success of these attempts and analyze the proposed method, we conducted various tests with five real data sets: the BREAST CANCER Wisconsin Database, the HEPATITIS Database, the IRIS Plant Database, the WINE Recognition Database from the University of California at Irvine KDD Archive <ref type="bibr" target="#b17">[18]</ref>, and the LEUKEMIA Database of Golub et al. <ref type="bibr" target="#b18">[19]</ref>. After preparing the test data sets, we compared the performance of the proposed method with five other well-known methods: a k-nearest-neighbor data description method (k-NNDD) <ref type="bibr" target="#b4">[5]</ref>, a Parzen-window density method <ref type="bibr" target="#b4">[5]</ref>, a C-SVDD, a k-nearest neighbor classifier (k-NNC), and SVMs. The model parameters and other kernel parameters were found by cross validation to identify the best solutions of each method <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b9">[10]</ref>.</p><p>The average error rates and those standard deviations of prediction accuracies of ten independent runs of twofold cross validations are given in Tables <ref type="table" target="#tab_0">I</ref> and<ref type="table" target="#tab_1">II</ref>. The label of a target data class is indicated in the first columns of these two tables and the data in other classes are the candidates of negative data. When only target data is used in training (Table <ref type="table" target="#tab_0">I</ref>), for the BREAST CANCER data set, the three versions of the k-NNDD method showed 26.94%, 33.40%, and 36.55% as average error rates when the label of a target class is 0. For the same data sets, the Parzen-window density method showed 4.62% error rate; and the C-SVDD showed 5.89%, 5.94%, and 5.09% error, respectively. The proposed D-SVDD, however, showed 5.12%, 4.15%, and 4.15% error rates when Method-I of (2) was used, and showed 5.24%, 5.24%, and 4.51% error rates when Method-II of was used. That was the C-SVDD comparable to the Parzen-window density method, and these two methods outperformed highly the k-NNDD method, and the proposed D-SVDDs outperformed slightly the C-SVDD in all versions used including the Parzen-window method, and the D-SVDDs with a Gaussian radial basis function (RBF) kernel function had the best performance (as indicated in bold type). Moreover, there was no big difference between the D-SVDD with the Method-I and D-SVDD with Method-II.</p><p>Similar results were obtained for other data sets, but the improvement of the D-SVDD over the C-SVDD was more conspicuous. Results on the IRIS data set, for example, the average error rate of the C-SVDD with a Gaussian RBF kernel function was 6.84% in all. On the contrary, the D-SVDDs showed 4.78% and 4.80% error rates according to the relative density extraction methods, respectively. Especially for the LEUKEMIA data set, the improvement was most prominent; the D-SVDD with Method-II obtained a 5.92% error rate, whereas the C-SVDD showed a 16.58% error rate when a Gaussian RBF kernel function was used.</p><p>When negative data was also used in training (Table <ref type="table" target="#tab_1">II</ref>), similar phenomena with the previous case occurred, but the error rates of the proposed methods were highly decreased. In this case, the performance of the proposed methods were comparable with the SVMs and the k-NNC; the proposed method showed even better performance than the SVMs and the k-NNC for HEPATITIS, LEUKEMIA, and WINE data sets at the given test conditions. For the WINE data set, for example, the average error rate was 1.22% for the D-SVDD with Method-I, whereas for the C-SVDD, the error rate was 7.75%. The result was better than those of k-NNC (2.51%) and SVMs (1.40%). These kinds of improvement were more remarkable for the LEUKEMIA data set (Table <ref type="table" target="#tab_1">II</ref>).</p><p>From Tables <ref type="table" target="#tab_0">I</ref> and<ref type="table" target="#tab_1">II</ref>, we conclude that the proposed method showed better prediction accuracies than the conventional data description methods including the C-SVDD for all of the tested cases, regardless of the types of kernel functions or regardless of whether the negative data was used in training or not. Moreover, the best performance was obtained when the D-SVDD with Gaussian kernel functions were used. Furthermore, when negative data was used in training (Table <ref type="table" target="#tab_1">II</ref>), the performance of the D-SVDD was comparable to those of the conventional well-known multiclass classification methods such as SVMs and k-NNC; despite, it is generally accepted that multiclass classification methods outperform data description methods <ref type="bibr" target="#b4">[5]</ref>. That is because multiclass classification methods are invented to give the best separation without considering the volume of the data description <ref type="bibr" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. APPLICATION TO PROTEIN LOCALIZATION PREDICTION</head><p>Subcellular protein localization, the location where a protein resides within a cell, is one of the key functional characteristics of proteins <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>. An automatic and efficient prediction method for the protein subcellular localization is highly required owing to the need for large-scale genome analysis <ref type="bibr" target="#b19">[20]</ref>. From a machine learning point of view, a data set of protein localization has several characteristics: The data set has too many classes, it is a "multilabel" data set, and it is too "imbalanced" <ref type="bibr" target="#b20">[21]</ref>. Even though many previous works have been invented for the prediction of protein subcellular localization, none of them have tackled such characteristics effectively <ref type="bibr" target="#b19">[20]</ref>.</p><p>We currently think that the proposed D-SVDD is one of good candidate methods for protein localization prediction. It is a one-class classification method, which is suitable for imbalanced data sets since it finds a compact description for a target data independently from other data <ref type="bibr" target="#b4">[5]</ref>. Moreover, it is easily used for the data set whose number of classes is large owing to linear complexity with regard to the number of classes. However, basically the proposed D-SVDD is not for a multiclass and multilabel problem. For the protein localization problem, thus, we extend the proposed D-SVDD method by adopting the "one-versus-the other" approach as the following procedure.</p><p>1) If a training data set is given, we divide it by class into a target data set and a negative data set. For a label l i , for instance, a data point whose label set has l i is included in the target data; otherwise, it is included in the negative data set. 2) If a target data set and a negative data set are prepared for each class, we find the optimal boundary of the target data by using the cross-validation method. 3) We calculate the degree of membership for each class l i for a test data point x t using a scoring function</p><formula xml:id="formula_10">f (xt; li) = R i d(x t ; a i )<label>(12)</label></formula><p>where (a i ; R i ) is the optimal hypersphere of the target data that are included in the class label l i . Note that this scoring function reports a higher value for a test data point with smaller Euclidean distance between x t and a i regarding distance of R i . 4) Finally, according to the values of the scoring function for all classes, we rank the labels, and report them. With this procedure, we can easily and intuitively extend the D-SVDD for a multiclass and multilabel classification problem like the protein localization prediction.</p><p>To evaluate the performance of the extended D-SVDD method, we represent a protein in three different ways using several famous ways <ref type="bibr" target="#b19">[20]</ref>, and make three data sets: Dataset-I, Dataset-II, and Dataset-III (see Table <ref type="table" target="#tab_2">III</ref>). In Dataset-I, we used pair-AAC features, and gapped-AAC for sequence information including the amino acid composition (AAC) <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b21">[22]</ref>. In the Dataset-II, using 2372 unique motifs from InterPro Database, we represent a protein in a vector in a 2372-dimensional space <ref type="bibr" target="#b19">[20]</ref>. For Dataset-III, we combine the  previous two features only for the proteins that have more than one motif in the unique motif set.</p><p>For multilabel learning paradigms, only one measure is not sufficient to evaluate the performance of a predictor owing to the variety of correctness in prediction <ref type="bibr" target="#b22">[23]</ref>. Thus, we use three measures (Measure-I, Measure-II, and Measure-III) for the evaluation of a protein localization predictor. First, to check the overall success rate regarding the total number of the unique proteins N , we define Measure-I as</p><formula xml:id="formula_11">(1=N) N i=1 [L(Pi);Y k i ],</formula><p>where N is the total number of different proteins, L(P i ) is the true label set of a protein P i ; Y i k is the predicted top-k labels by a predictor. Moreover, [L(P i );Y k i ] = 1 if any label in Y i k is in L(P i ); otherwise, it is 0. We used k = 3 in this letter since the numbers of true localization sites of most proteins are less than or equal to 3 <ref type="bibr" target="#b20">[21]</ref>.</p><p>To check the overall success rate regarding the total number of classified proteins in each class Ñ, we define Measure-II as (1)=( Ñ) N i=1 9[L(P i );Y k i ], where Ñ is the total number of classified proteins in each class, L(P i ) is the true label set of a protein P i ; Y i k is the predicted top-k i labels by a predictor, and the 9[ 1; 1 ] function returns the correct number of labels which is predicted correctly.</p><p>Finally, to check the average rate of the success rates of each class, we define Measure-III as (1=) l=1 (</p><formula xml:id="formula_12">(1)=(n l ) n i=1 1[Y k i ; l]),</formula><p>where is a total number of classes, l is a label index, ñl is the number of proteins in the lth label, Y k i is the predicted top-ki label of a protein P i by a predictor. Here, 1[Y k i ; l] = 1 if any label in Y k i is equal to l; otherwise, it is 0.</p><p>For competitive analysis, we compared the performance of the proposed method with the ISort method <ref type="bibr" target="#b19">[20]</ref> to the three data sets. Up to now, ISort method showed the best performance for the prediction of yeast protein multiple localization <ref type="bibr" target="#b19">[20]</ref>. For the proposed method, we used a Gaussian RBF kernel function <ref type="bibr" target="#b9">[10]</ref>, and used Method-I for relative density degree extraction owing to a high dimensional feature space.</p><p>The results of the ISort method and the proposed method of a twofold cross validation for the three data sets are given in Table <ref type="table" target="#tab_3">IV</ref>. From these results, we could conclude that the extended D-SVDD method outperformed the ISort method for all three data sets, regardless of the kind of evaluation measure. Moreover, motif information could increase the prediction accuracy of the two methods considered even though the coverage of motif information is lower than that of AAC-based information. Furthermore, the best performance was obtained when both features were used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this letter, we have proposed a novel method incorporating density distribution of a given target data set when identifying the hyperspherical data description of the data set. To reflect the density distribution, we associated each data point with a relative density degree, and proposed two kinds of density-induced distance measurement based on the degrees. Using the distance measurements, we developed a new SVDD method named D-SVDD. It was demonstrated that the proposed method outperformed the other data description methods including the conventional SVDD for all tested data sets, regardless of the kind of kernel function and regardless of the use of negative data in a training stage. When the information of negative data was available, the performance of the proposed method was also comparable to well-known multi class classifiers such as k-NNC and SVMs. Moreover, for a real application, we extended the proposed method to the protein localization prediction problem which is an imbalanced multiclass and multilabel problem, and observed promising results.</p><p>Currently, the time complexity of the proposed method is O(kN 3 )</p><p>where N is the number of quadratic programming subproblems to solve linearly constrained optimization problems <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>. Thus, to apply the proposed method to larger data sets, more research on reducing the computational time is required. Moreover, more formal justification of the proposed method is valuable for future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>we use + i and slack variable i ( 0) for the permission of training error for each target data point. Then, we can obtain the optimal hypersphere (a; R) by minimizing the objective function O O = R 2 + C + n i=1 i (6) subject to i (x i 0 a) 1 (x i 0 a) R 2 + i where C + (&gt; 0) is the control parameter in the C-SVDD [5]. Note that i = ( + i ) 2 0 R 2 for training error data; otherwise, it is 0. It implies that i also contains the information of a relative density degree of x i .Similar to C-SVDD<ref type="bibr" target="#b0">[1]</ref>,<ref type="bibr" target="#b4">[5]</ref>, the dual problem can be obtained by maximizing D() using Lagrange multipliers j x i 1 x j<ref type="bibr" target="#b6">(7)</ref> subject to n i=1 i = 1; 0 i C + and T = n i=1 i i .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>( 9 )</head><label>9</label><figDesc>subject to N k=1 y k k = 1; 0 i C + ; 0 l C 0 ; T = N k=1 y k k 0 k , and 0 k = y k k , where y k is the label of x k (y k = 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="3,54.30,97.06,484.00,299.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="4,45.78,104.16,498.00,300.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I AVERAGE</head><label>I</label><figDesc>ERROR RATES AND THOSE STANDARD DEVIATIONS (%) OF TEN INDEPENDENT RUNS FOR FIVE DATA SETS WHEN TRAINED USING ONLY TARGET DATA relative density degree 0</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II AVERAGE</head><label>II</label><figDesc>ERROR RATES AND THOSE STANDARD DEVIATIONS (%) OF TEN INDEPENDENT RUNS FOR FIVE DATA SETS WHEN TRAINED USING BOTH TARGET DATA AND NEGATIVE DATA</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III CHARACTERISTICS</head><label>III</label><figDesc>OF PROTEINS IN THE ORIGINAL HUH ET AL. DATA SET AND THREE TRAINING DATA SETS</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV PREDICTION</head><label>IV</label><figDesc>PERFORMANCE (%) OF ISORT AND EXTENDED D-SVDD TO THE THREE DATA SETS</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank the anonymous reviewers for their helpful comments and Chung Moon Soul Center for Bioinformation and Bioelectronics, KAIST and the IBM Shared University Research (SUR) program for providing research and computing facilities.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported by the National Research Laboratory under Grant 2005-01450 and the Korean Systems Biology Research under Grant 2005-00343 from the Ministry of Science and Technology.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rival-Model Penalized Self-Organizing Map</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Yiu-ming Cheung and Lap-tak Law</head><p>Abstract-As a typical data visualization technique, self-organizing map (SOM) has been extensively applied to data clustering, image analysis, dimension reduction, and so forth. In a conventional adaptive SOM, it needs to choose an appropriate learning rate whose value is monotonically reduced over time to ensure the convergence of the map, meanwhile being kept large enough so that the map is able to gradually learn the data topology. Otherwise, the SOM's performance may seriously deteriorate. In general, it is nontrivial to choose an appropriate monotonically decreasing function for such a learning rate. In this letter, we therefore propose a novel rival-model penalized self-organizing map (RPSOM) learning algorithm that, for each input, adaptively chooses several rivals of the best-matching unit (BMU) and penalizes their associated models, i.e., those parametric real vectors with the same dimension as the input vectors, a little far away from the input. Compared to the existing methods, this RPSOM utilizes a constant learning rate to circumvent the awkward selection of a monotonically decreased function for the learning rate, but still reaches a robust result. The numerical experiments have shown the efficacy of our algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Index Terms-Constant learning rate, rival-model penalized self-organizing map (RPSOM), self-organizing map (SOM).</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Self-organizing map (SOM) <ref type="bibr" target="#b8">[9]</ref> and its variants, e.g., see <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, and <ref type="bibr" target="#b16">[17]</ref>, are one of the popular data visualization techniques that  provide a topological mapping from the input space to the output space.</p><p>Typically, an SOM map possesses a regular one or two-dimensional (2-D) grid of nodes. Each node (also called neurons interchangeably) in the grid is associated with a parametric real vector called model or weight that has the same dimension as the input vectors. The task of SOM is to learn those models so that the similar high-dimensional input data are mapped into one-dimensional (1-D) or 2-D output space with the topology as unchanged as possible. That is, the similar data in the input space under a measurement are placed physically close to each other on the map. This topology-reserved feature of SOM leads it to the wide applications to data visualization <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b5">[6]</ref>, data clustering <ref type="bibr" target="#b2">[3]</ref>, image analysis <ref type="bibr" target="#b9">[10]</ref>, data mining <ref type="bibr" target="#b12">[13]</ref>, and so forth.</p><p>In general, a conventional adaptive SOM needs to initialize a learning rate and gradually reduces its value over time to ensure the convergence of the map. Usually, a small initial value of learning rate is prone to make the models stabilized at some locations of input space in an early training stage. As a result, the map is not well established. Hence, by rule of a thumb, the learning rate is often initialized at a relatively large value, and then gradually reduced over time using a monotonically decreasing function. If we reduce the learning rate very slowly, the map can learn the topology of inputs well with the small quantization error, but the map convergence needs a large number of iterations and becomes quite time-consuming. On the other hand, if we reduce the learning rate too quickly, the map will be likely trapped into a local suboptimal solution and finally led to the large quantization error. To the best of our knowledge, it is a nontrivial task to select an appropriate learning rate, in particular its associated monotonically decreasing function. In the literature, a two-phase training of the SOM has been further proposed by <ref type="bibr" target="#b13">[14]</ref>, which utilizes two learning rates to solve the previous selection problem in training the SOM. In the first phase, it keeps a large learning rate that aims at capturing the rough topological structure of the training data quickly. In general, the resulting map in the first phase is prone to error and the topological structure may not be well established. In the second phase, a much smaller learning rate is utilized to the trained map from the first phase, which aims at the fine-tuning topological map to ensure the map convergence. Nevertheless, the performance of the training algorithm is still sensitive to the time-varied learning rate.</p><p>In this letter, we therefore propose a new rival-model penalized self organizing map (RPSOM) learning algorithm inspired by the idea of the rival penalized competitive learning (RPCL) <ref type="bibr" target="#b14">[15]</ref> and its recently improved variant, named rival penalization controlled competitive learning (RPCCL) approach <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. For each input, the RPSOM adaptively chooses several rivals of the best-matching unit (BMU) and penalizes their associated models a little far away from</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">One-class classification: Concept-learning in the absence of counter-examples</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M J</forename><surname>Tax</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ph.D. dissertation, Electr. Eng., Math. Comp. Sci., Delft Univ. Technology</title>
		<imprint>
			<date type="published" when="2001-06">Jun. 2001</date>
			<pubPlace>Delft, The Netherlands</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">C</forename><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NCRB</title>
		<imprint>
			<biblScope unit="volume">4288</biblScope>
			<date type="published" when="1994">1994</date>
			<pubPlace>Birmingham, U.K.</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Mixture Density Networks Neural Computation Research Group, Aston Univ.</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Outliers in statistical pattern recognition and an application to automatic chromosome classification</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Gallegos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="525" to="539" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Distance-based outliers: Algorithms and applications</title>
		<author>
			<persName><forename type="first">E</forename><surname>Knorr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Tucakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VLDB J</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="237" to="253" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Support vector data description</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M J</forename><surname>Tax</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P W</forename><surname>Duin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="45" to="66" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Neural Networks for Pattern Recognition</title>
		<meeting><address><addrLine>London, U.K.</addrLine></address></meeting>
		<imprint>
			<publisher>Oxford Univ. Press</publisher>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Network constraints and multi-objective optimization for one-class classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Moya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="463" to="474" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An introduction to kernel-based learning algorithms</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rätsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tsuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="181" to="201" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Learning with Kernels</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<title level="m">The Nature of Statistical Learning Theory</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A support vector machine approach for detection of microcalsifications</title>
		<author>
			<persName><forename type="first">I</forename><surname>El-Naqa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wernik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Galatsanos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nishikawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1552" to="1563" />
			<date type="published" when="2002-12">Dec. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Distributed support vector machines</title>
		<author>
			<persName><forename type="first">A</forename><surname>Navia-Vázquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gutiérrez-González</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Parrado-Hernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Navarro-Abellán</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1091" to="1097" />
			<date type="published" when="2006-07">Jul. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On estimation of a probability density function and mode</title>
		<author>
			<persName><forename type="first">E</forename><surname>Parzen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Math. Stat</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1065" to="1076" />
			<date type="published" when="1962">1962</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Effects of kernel function on Nu support vector machines in extreme cases</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ikeda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2006-01">Jan. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On utilizing search methods to select subspace dimensions for kernel-based nonlinear subspace classifiers</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Oommen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="136" to="141" />
			<date type="published" when="2005-01">Jan. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A tolerant algorithm for linearly constrained optimizations calculations Univ</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J D</forename><surname>Powell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DAMTP Rep. NA</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<date type="published" when="1988">1988</date>
			<pubPlace>Cambridge, Cambridge, U.K.</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">TOLMIN: A Fortran package for linearly constrained optimizations calculations Univ</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J D</forename><surname>Powell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DAMTP Report NA</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="1989">1989</date>
			<pubPlace>Cambridge, Cambridge, U.K.</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">UCI Repository of Machine Learning Database</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Merz</surname></persName>
		</author>
		<ptr target="http://www.ics.uci.edu/mlearn/ML-Repository.html/" />
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Molecular classification of cancer: Class discovery and class prediction by gene expression monitoring</title>
		<author>
			<persName><forename type="first">T</forename><surname>Golub</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">286</biblScope>
			<biblScope unit="page" from="531" to="537" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Predicting protein localization in budding yeast</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">D</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinf</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="944" to="950" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Global analysis of protein localization in budding yeast</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">K</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Falvo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Gerke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Carroll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Howson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Weissman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">K</forename><surname>O'shea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">425</biblScope>
			<biblScope unit="page" from="686" to="691" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Prediction of protein subcellular locations by support vector machines suing compositions of amino acid and amino acid paris</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kanehisa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinf</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1656" to="1663" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">MMAC: A new multi-class, multi-label associative classification approach</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Thabtah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cowling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th Int. Conf. Data Mining</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="217" to="224" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
