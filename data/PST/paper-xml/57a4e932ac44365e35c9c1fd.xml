<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Robust Single Image Super-Resolution via Deep Networks with Sparse Prior</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ding</forename><surname>Liu</surname></persName>
							<email>dingliu2@illinois.edu</email>
						</author>
						<author>
							<persName><forename type="first">W</forename><surname>Han</surname></persName>
						</author>
						<author>
							<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
							<email>t-huang1@illinois.edu</email>
						</author>
						<author>
							<persName><forename type="first">B</forename><surname>Wen</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution" key="instit1">Beckman Institute</orgName>
								<orgName type="institution" key="instit2">Univerisity of Illinois at Urbana-Champaign</orgName>
								<address>
									<postCode>61801</postCode>
									<settlement>Urbana</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical and Computer Engineering and Coordinated Science Laboratory</orgName>
								<orgName type="institution">Univerisity of Illinois at Urbana-Champaign</orgName>
								<address>
									<postCode>61801</postCode>
									<settlement>Urbana</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Adobe Systems Inc</orgName>
								<address>
									<postCode>95110</postCode>
									<settlement>San Jose</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Snapchat Inc</orgName>
								<address>
									<postCode>90291</postCode>
									<settlement>Venice</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Robust Single Image Super-Resolution via Deep Networks with Sparse Prior</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A120D7FA83E7EBE4F6F627CEAC53B71D</idno>
					<idno type="DOI">10.1109/TIP.2016.2564643</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2016.2564643, IEEE Transactions on Image Processing</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>image super-resolution</term>
					<term>deep neural networks</term>
					<term>sparse coding</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Single image super-resolution is an ill-posed problem which tries to recover a high-resolution image from its lowresolution observation. To regularize the solution of the problem, previous methods have focused on designing good priors for natural images such as sparse representation, or directly learning the priors from a large data set with models such as deep neural networks. In this paper, we argue that domain expertise from the conventional sparse coding model can be combined with the key ingredients of deep learning to achieve further improved results. We demonstrate that a sparse coding model particularly designed for super-resolution can be incarnated as a neural network with the merit of end-to-end optimization over training data. The network has a cascaded structure which boosts the SR performance for both fixed and incremental scaling factors. The proposed training and testing schemes can be extended for robust handling of images with additional degradation such as noise and blurring. A subjective assessment is conducted and analyzed in order to thoroughly evaluate various SR techniques. Our proposed model is tested on a wide range of images, and it significantly outperforms existing state-of-the-art methods for various scaling factors both quantitatively and perceptually.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Single image super-resolution is usually cast as an inverse problem of recovering the original high-resolution (HR) image from one low-resolution (LR) observation image. Since the known variables in LR images are greatly outnumbered by the unknowns in typically desired HR images, this problem is highly ill-posed and has limited the use of SR techniques in many practical applications <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>.</p><p>A large number of single image SR methods have been proposed, exploiting various priors of natual images to regularize the solution of SR. Analytical priors, such as bicubic interpolation, work well for smooth regions; while image models based on statistics of edges <ref type="bibr" target="#b2">[3]</ref> and gradients <ref type="bibr" target="#b3">[4]</ref> can recover sharper structures. In the patch-based SR methods, HR patch candidates are represented as the sparse linear combination of dictionary atoms trained from external databases <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>,</p><p>or recovered from similar examples in the LR image itself at different locations and across different scales <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>. A regression model is built between LR and HR patches in <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. A comprehensive review of more SR methods can be found in <ref type="bibr" target="#b11">[12]</ref>.</p><p>More recently, inspired by the great success achieved by deep learning <ref type="bibr" target="#b12">[13]</ref> in other computer vision tasks, people begin to use neural networks with deep architecture for image SR. Multiple layers of collaborative auto-encoders are stacked together in <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref> for robust matching of self-similar patches. Deep convolutional neural networks (CNN) <ref type="bibr" target="#b15">[16]</ref> and deconvolutional networks <ref type="bibr" target="#b16">[17]</ref> are designed that directly learn the non-linear mapping from LR space to HR space in a way similar to coupled sparse coding <ref type="bibr" target="#b5">[6]</ref>. As these deep networks allow end-to-end training of all the model components between LR input and HR output, significant improvements have been observed over their shadow counterparts.</p><p>The networks in <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b15">[16]</ref> are built with generic architectures, which means all their knowledge about SR is learned from training data. On the other hand, people's domain expertise for the SR problem, such as natural image prior and image degradation model, is largely ignored in deep learning based approaches. It is then worthwhile to investigate whether domain expertise can be used to design better deep model architectures, or whether deep learning can be leveraged to improve the quality of handcrafted models.</p><p>In this paper, we extend the conventional sparse coding model <ref type="bibr" target="#b4">[5]</ref> using several key ideas from deep learning, and show that domain expertise is complementary to large learning capacity in further improving SR performance. First, based on the learned iterative shrinkage and thresholding algorithm (LISTA) <ref type="bibr" target="#b17">[18]</ref>, we implement a feed-forward neural network in which each layer strictly correspond to one step in the processing flow of sparse coding based image SR. In this way, the sparse representation prior is effectively encoded in our network structure; at the same time, all the components of sparse coding can be trained jointly through backpropagation. This simple model, which is named sparse coding based network (SCN), achieves notable improvement over the generic CNN model <ref type="bibr" target="#b15">[16]</ref> in terms of both recovery accuracy and human perception, and yet has a compact model size. Moreover, with the correct understanding of each layer's physical meaning, we have a more principled way to initialize the parameters of SCN, which helps to improve optimization speed and quality.</p><p>A single network is only able to perform image SR by a particular scaling factor. In <ref type="bibr" target="#b15">[16]</ref>, different networks are trained for different scaling factors. In this paper, we propose a cascade of multiple SCNs to achieve SR for arbitrary factors. This approach, motivated by the self-similarity based SR approach <ref type="bibr" target="#b7">[8]</ref>, not only increases the scaling flexibility of our model, but also reduces artifacts for large scaling factors. Moreover, inspired by the multi-pass scheme of image denoising <ref type="bibr" target="#b18">[19]</ref>, we demonstrate that the SR results can be further enhanced by cascading multiple SCNs for SR of a fixed scaling factor. The cascade of SCNs (CSCN) can also benefit from the end-to-end training of deep network with a specially designed multi-scale cost function.</p><p>In practical SR scenarios, the real LR measurements usually suffer from various types of corruptions, such as noise and blurring. Sometimes the degradation process is even too complicated or unclear. We propose several schemes using our SCN to robustly handle such practical SR cases. When the degradation mechanism is unknown, we fine-tune the generic SCN with the requirement of only a small amount of real training data and manage to adapt our model to the new scenario. When the forward model for LR generation is clear, we propose an iterative SR scheme incorporating SCN with additional regularization based on priors from the degradation mechanism.</p><p>Subjective assessment is important to the SR technology because the commercial products equipped with such technology are usually evaluated subjectively by the end users. In order to thoroughly compare our model with other prevailing SR methods, we conduct a systematic subjective evaluation among these methods, in which the assessment results are statistically analyzed and one score is given for each method.</p><p>In short, the contributions of this paper include:</p><p>• combining the domain expertise of sparse coding and the merits of deep learning to achieve better SR performance with faster training and smaller model size; • exploring network cascading for arbitrary scaling factors in order to further enhance SR performance; • utilizing SCN to robustly handle the practical SR scenarios with non-ideal LR measurements. • conducting a subjective evaluation on a number of recent state-of-the-art SR methods; This paper is built upon our previous work in <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref> with several additional contributions. First, we incorporate one more network cascading technique of SCN which further improves the SR performance in <ref type="bibr" target="#b20">[21]</ref>. Second, a novel method of coping with the practical SR problems is presented which elaborates both of the training and testing schemes. Third, we introduce in detail the system of subjective assessment and its scoring mechanism. Finally, we provide a more comprehensive experiment section for qualitative and quantitative analysis, which includes extensive experimental results for the practical SR methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Image SR Using Sparse Coding</head><p>The sparse representation based SR method <ref type="bibr" target="#b4">[5]</ref> models the transform from each local patch y ∈ R my in the bicubicupscaled LR image to the corresponding patch x ∈ R mx in the HR image. The dimension m y is not necessarily the same</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>+ +</head><p>Fig. <ref type="figure">1</ref>. A LISTA network <ref type="bibr" target="#b17">[18]</ref> with 2 time-unfolded recurrent stages, whose output α is an approximation of the sparse code of input signal y. The linear weights W , S and the shrinkage thresholds θ are learned from data.</p><p>as m x when image features other than raw pixel is used to represent patch y. It is assumed that the LR(HR) patch y(x) can be represented with respect to an overcomplete dictionary D y (D x ) using some sparse linear coefficients α y (α x ) ∈ R n , which are known as sparse code. Since the degradation process from x to y is nearly linear, the patch pair can share the same sparse code α y = α x = α if the dictionaries D y and D x are defined properly. Therefore, for an input LR patch y, the HR patch can be recovered as</p><formula xml:id="formula_0">x = D x α, s.t. α = arg min z ∥y -D y z∥ 2 2 + λ∥z∥ 1 , (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where ∥•∥ 1 denotes the ℓ 1 norm which is convex and sparsityinducing, and λ is a regularization coefficient.</p><p>In order to learn the dictionary pair (D y , D x ), the goal is to minimize the recovery error of x and y, and thus the loss function L in <ref type="bibr" target="#b5">[6]</ref> is defined as</p><formula xml:id="formula_2">L = 1 2 ( γ∥x -D x z∥ 2 2 + (1 -γ)∥y -D y z∥ 2 2 ) ,<label>(2)</label></formula><p>where γ (0 &lt; γ ≤ 1) balances the two reconstruction errors. Then the optimal dictionary pair {D * x , D * y } can found by minimizing the empirical expectation of (2) over all the training LR/HR pairs, min Dx,Dy</p><formula xml:id="formula_3">1 N N ∑ i=1 L(D x , D y , x i , y i ) s.t. z i = arg min α ∥y i -D y α∥ 2 2 + λ∥α∥ 1 , i = 1, 2, ..., N, ∥D x (:, k)∥ 2 ≤ 1, ∥D y (:, k)∥ 2 ≤ 1, k = 1, 2, ..., K.</formula><p>(</p><p>Since the objective function in (2) is highly nonconvex, the dictionary pair D y , D x ) is usually learned alternatively while keeping the other fixed <ref type="bibr" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Network Implementation of Sparse Coding</head><p>There is an intimate connection between sparse coding and neural network, which has been well studied in <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b17">[18]</ref>. A feed-forward neural network as illustrated in Fig. <ref type="figure">1</ref> is proposed in <ref type="bibr" target="#b17">[18]</ref> to efficiently approximate the sparse code α of input signal y as it would be obtained by solving (1) for a given dictionary D y . The network has a finite number of recurrent stages, each of which updates the intermediate sparse code according to</p><formula xml:id="formula_5">z k+1 = h θ (W y + Sz k ),<label>(4)</label></formula><p>where h θ is an element-wise shrinkage function defined as</p><formula xml:id="formula_6">[h θ (a)] i = sign(a i )(|a i | -θ i ) + with positive thresholds θ.</formula><p>Different from the iterative shrinkage and thresholding algorithm (ISTA) <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref> which finds an analytical relationship between network parameters (weights W , S and thresholds θ) and sparse coding parameters (D y and λ), the authors of <ref type="bibr" target="#b17">[18]</ref> learn all the network parameters from training data using a back-propagation algorithm called learned ISTA (LISTA). In this way, a good approximation of the underlying sparse code can be obtained within a fixed number of recurrent stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Generic Convolutional Neural Network for SR</head><p>As an successful example of deep learning for single image SR, Dong et al. <ref type="bibr" target="#b15">[16]</ref> propose a fully convolutional neural network to directly learn the mapping from the input LR image and the output HR image. It is designed to utilize three convolutional layers to minic the patch extraction and representation, non-linear mapping and reconstruction of the sparse representation based SR methods, respectively. Due to the end-to-end training strategy that jointly optimizes all the parameters and the large learning capacity of neural networks, this method notably outperforms its conventional shadow counterpart.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. SPARSE CODING BASED NETWORK FOR IMAGE SR</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Network Architecture</head><p>Given the fact that sparse coding can be effectively implemented with a LISTA network, it is straightforward to build a multi-layer neural network that mimics the processing flow of the sparse coding based SR method <ref type="bibr" target="#b4">[5]</ref>. Same as most patchbased SR methods, our sparse coding based network (SCN) takes the bicubic-upscaled LR image I y as input, and outputs the full HR image I x . Fig. <ref type="figure" target="#fig_0">2</ref> shows the main network structure, and each of the layers is described in the following.</p><p>The input image I y first goes through a convolutional layer H which extracts feature for each LR patch. There are m y filters of spatial size s y ×s y in this layer, so that our input patch size is s y ×s y and its feature representation y has m y dimensions.</p><p>Each LR patch y is then fed into a LISTA network with a finite number of k recurrent stages to obtain its sparse code α ∈ R n . Each stage of LISTA consists of two linear layers parameterized by W ∈ R n×my and S ∈ R n×n , and a nonlinear neuron layer with activation function h θ . The activation thresholds θ ∈ R n are also to be updated during training, which complicates the learning algorithm. To restrict all the tunable parameters in our linear layers, we do a simple trick to rewrite the activation function as</p><formula xml:id="formula_7">[h θ (a)] i = sign(a i )θ i (|a i |/θ i -1) + = θ i h 1 (a i /θ i ). (5)</formula><p>Eq. ( <ref type="formula">5</ref>) indicates the original neuron with an adjustable threshold can be decomposed into two linear scaling layers and a unit-threshold neuron, as shown in the top-right of Fig. <ref type="figure" target="#fig_0">2</ref>. The weights of the two scaling layers are diagonal matrices defined by θ and its element-wise reciprocal, respectively.</p><p>The sparse code α is then multiplied with HR dictionary D x ∈ R mx×n in the next linear layer, reconstructing HR patch x of size s x ×s x = m x .</p><p>In the final layer G, all the recovered patches are put back to the corresponding positions in the HR image I x . This is realized via a convolutional filter of m x channels with spatial size s g ×s g . The size s g is determined as the number of neighboring patches that overlap with the same pixel in each spatial direction. The filter will assign appropriate weights to the overlapped recoveries from different patches and take their weighted average as the final prediction in I x .</p><p>As illustrated in the bottom of Fig. <ref type="figure" target="#fig_0">2</ref>, after some simple reorganizations of the layer connections, the network described above has some adjacent linear layers which can be merged into a single layer. This helps to reduce the computation load as well as redundant parameters in the network. The layers H and G are not merged because we apply additional nonlinear normalization operations on patches y and x, which will be detailed in Sec. VI-A.</p><p>Thus, there are totally 5 trainable layers in our network: 2 convolutional layers H and G, and 3 linear layers shown as gray boxes in Fig. <ref type="figure" target="#fig_0">2</ref>. The k recurrent layers share the same weights and are therefore conceptually regarded as one. Note that all the linear layers are actually implemented as convolutional layers applied on each patch with filter spatial size of 1×1, a structure similar to the network in network <ref type="bibr" target="#b24">[25]</ref>. Also note that all these layers have only weights but no biases (zero biases).</p><p>Mean square error (MSE) is employed as the cost function to train the network, and our optimization objective can be expressed as</p><formula xml:id="formula_8">min Θ ∑ i ∥SCN (I (i) y ; Θ) -I (i) x ∥ 2 2 ,<label>(6)</label></formula><p>where I (i) y and I (i)  x are the i-th pair of LR/HR training data, and SCN (I y ; Θ) denotes the HR image for I y predicted using the SCN model with parameter set Θ. All the parameters are optimized through the standard back-propagation algorithm. Although it is possible to use other cost terms that are more correlated with human visual perception than MSE, our experimental results show that simply minimizing MSE leads to improvement in subjective quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Advantages over Previous Models</head><p>The construction of our SCN follows exactly each step in the sparse coding based SR method <ref type="bibr" target="#b4">[5]</ref>. If the network parameters are set according to the dictionaries learned in <ref type="bibr" target="#b4">[5]</ref>, it can reproduce almost the same results. However, after training, SCN learns a more complex regression function and can no longer be converted to an equivalent sparse coding model. The advantage of SCN comes from its ability to jointly optimize all the layer parameters from end to end; while in <ref type="bibr" target="#b4">[5]</ref> some variables are manually designed and some are optimized individually by fixing all the others.</p><p>Technically, our network is also a CNN and it has similar layers as the CNN model proposed in <ref type="bibr" target="#b15">[16]</ref> for patch extraction and reconstruction. The key difference is that we have a LISTA sub-network specifically designed to enforce sparse representation prior; while in <ref type="bibr" target="#b15">[16]</ref> a generic rectified linear unit (ReLU) <ref type="bibr" target="#b25">[26]</ref> is used for nonlinear mapping. Since SCN is designed based on our domain knowledge in sparse coding, we are able to obtain a better interpretation of the filter responses and have a better way to initialize the filter parameters in training. We will see in the experiments that all these contribute to better SR results, faster training speed and smaller model size than a vanilla CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Network Cascade</head><p>In this section, we investigate two different network cascade techniques in order to fully exploit our SCN model in SR applications.</p><p>1) Network Cascade for SR of a Fixed Scaling Factor: First, we observe that the SR results can be further improved by cascading multiple SCNs trained for the same objective in <ref type="bibr" target="#b5">(6)</ref>, which is inspired by the multi-pass scheme in <ref type="bibr" target="#b18">[19]</ref>. The only difference for training these SCNs is to replace the bicubic interpolated input by its latest HR estimate, while the target output remains the same.</p><p>The first SCN plays as a function approximator to model the non-linear mapping from the bicubic upscaled image to the ground-truth image. The following SCN plays as another function approximator, with the starting point changed to a better estimate: the output of its previous SCN.</p><p>In other words, the cascade of SCNs as a whole can be considered as a new deeper network having more powerful learning capability, which is able to better approximate the mapping between the LR inputs to the HR counterparts, and these SCNs can be trained jointly to pursue even better SR performance.</p><p>2) Network Cascade for Scalable SR: Like most SR models learned from external training examples, the SCN discussed previously can only upscale images by a fixed factor. A separate model needs to be trained for each scaling factor to achieve the best performance, which limits the flexibility and scalability in practical use. One way to overcome this difficulty is to repeatedly enlarge the image by a fixed scale until the resulting HR image reaches a desired size. This practice is commonly adopted in the self-similarity based methods <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b13">[14]</ref>, but is not so popular in other cases for the fear of error accumulation during repetitive upscaling.</p><p>In our case, however, it is observed that a cascade of SCNs trained for small scaling factors can generate even better SR results than a single SCN trained for a large scaling factor, especially when the target scaling factor is large (greater than 2). This is illustrated by the example in Fig. <ref type="figure" target="#fig_1">3</ref>. Here an input image is magnified by ×4 times in two ways: with a single SCN×4 model through the processing flow (a) → (b) → (d); and with a cascade of two SCN×2 models through (a) → (c) → (e). It can be seen that the input to the second cascaded SCN×2 in (c) is already sharper and contains less artifacts than the bicubic×4 input to the single SCN×4 in (b), which naturally leads to the better final result in (e) than the one in (d).</p><p>To get a better understanding of the above observation, we can draw a loose analogy between the SR process and a communication system. Bicubic interpolation is like a noisy channel through which an image is "transmitted" from LR domain to HR domain. And our SCN model (or any SR algorithm) behaves as a receiver which recovers clean signals from noisy observations. A cascade of SCNs is then like a set of relay stations that enhance signal-to-noise ratio before the signal becomes too weak for further transmission. Therefore, cascading will work only when each SCN can restore enough useful information to compensate for the new artifacts it introduces as well as the magnified artifacts from previous stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Training Cascade of Networks:</head><p>Taking into account the two aforementioned cascade techniques, we can consider the cascade of all SCNs as a deeper network (CSCN), in which the final output of the consecutive SCNs of the same ground truth is connected to the input of the next SCN with bicubic interpolation in the between. To construct the cascade, besides stacking several SCNs trained individually with respect to (6), we can also optimize all of them jointly as shown in Fig. <ref type="figure" target="#fig_2">4</ref>. Without loss of generality, we assume each stage in Sec. III-C2 has the same scaling factor s. Let Îj,k (j &gt; 0, k &gt; 0) denote the output image of the j-th SCN in the k-th stage upscaled by a total of ×s k times. In the same stage, each output of SCNs is compared with the associated ground truth image I k according to the MSE cost, leading to a multi-scale objective function:  where i denotes the data index, and j, k denotes the SCN index. For simplicity of notation, Î0,k specially denotes the bicubic interpolated image of the final output in the (k -1)-th stage upscaled by a total of ×s k-1 times. This multi-scale objective function makes full use of the supervision information in all scales, sharing a similar idea as heterogeneous networks <ref type="bibr" target="#b26">[27]</ref>. All the layer parameters {Θ j,k } in ( <ref type="formula" target="#formula_9">7</ref>) could be optimized from end to end by back-propagation. The SCNs share the same training objective can be trained simultaneously, taking advantage of the merit of deep learning. For the SCNs with different training objectives, we use a greedy algorithm here to train them sequentially from the beginning of the cascade so that we do not need to care about the gradient of bicubic layers. Applying back-propagation through a bicubic layer or its trainable surrogate will be considered in future work.</p><formula xml:id="formula_9">min {Θ j,k } ∑ i ∑ j ∑ k SCN ( Î(i) j-1,k ; Θ j,k ) -I (i) k 2 2 ,<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. ROBUST SR FOR REAL SCENARIOS</head><p>Most of recent SR works generate the LR images for both training and testing by downscaling HR images using bicubic interpolation <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b27">[28]</ref>. However, this assumption of the forward model may not always hold in practice. For example, the real LR measurements are usually blurred, or corrupted with noise. Sometimes, the LR generation mechanism may be complicated, or even unknown. We now investigate the practical SR problem, and propose two approaches to handle such non-ideal LR measurements, using the generic SCN. In the case that the underlying mechanism of the real LR generation is unclear or complicated, we propose the datadriven approach by fine-tuning the learned generic SCN with a limited number of real LR measurements as well as their corresponding HR counterparts. On the other hand, if the real training samples are unavailable but the LR generation mechanism is clear, we formulate this inverse problem as the regularized HR image reconstruction problem which can be solved using iterative methods. The proposed methods demonstrate the robustness of our SCN model to different SR scenarios. In the following, we elaborate the details of these two approaches, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data-Driven SR by Fine-Tuning</head><p>Deep learning models can be efficiently transferred from one task to another by re-using the intermediate representation in the original neural network <ref type="bibr" target="#b28">[29]</ref>. This method has proven successful on a number of high-level vision tasks, even if there is a limited amount of training data in the new task <ref type="bibr" target="#b29">[30]</ref>.</p><p>The success of super-resolution algorithms usually highly depends on the accuracy of the model of the imaging process. When the underlying mechanism of the generation of LR images is not clear, we can take advantage of the aforementioned merit of deep learning models by learning our model in a data-driven manner, to adapt it for a particular task. Specifically, we start training from the generic SCN model while using very limited amount of training data from a new SR scenario, and manage to adapt it to the new SR scenario and obtain promising results. In this way, it is demonstrated that the SCN has the strong capability of learning complex mappings between the non-ideal LR measurements to their HR counterparts as well as the high flexibility of adapting to various SR tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Iterative SR with Regularization</head><p>The second approach considers the case that the mechanism of generating the real LR images is relatively simple and clear, indicating the training data is always available if we synthesize LR images with the known degradation process. We propose an iterative SR scheme which incorporates the generic SCN model with additional regularization based on taskrelated priors (e.g. the known kernel for deblurring, or the data sparsity for denoising). In this section, we specifically discuss handling blurred and noisy LR measurements in details as examples, though the iterative SR methods can be generalized to other practical imaging models.</p><p>1) Blurry Image Upscaling: The real LR images can be generated with various types of blurring. Directly applying the generic SCN model is obviously not optimal. Instead, with the known blurring kernel, we propose to estimate the regularized version of the HR image Îx based on the directly upscaled image Ĩx by the learned SCN as follows:</p><formula xml:id="formula_10">Îx = arg min I ∥I -Ĩx ∥ 2 , s.t. D•B • I = I 0 y (8)</formula><p>where I 0 y is the original blurred LR input, and the operators B and D are blurring and sub-sampling respectively. Similar to the previous work <ref type="bibr" target="#b4">[5]</ref>, we use back-projection to iteratively estimate the regularized HR input on which our model can perform better. Specifically, given the regularized estimate Îi-1</p><p>x at iteration i -1, we estimate a less blurred LR image I i-<ref type="foot" target="#foot_0">1</ref> y by downsampling Îi x using bicubic interpolation. The upscaled Ĩi</p><p>x by learned SCN serves the regularizer for the i-th iteration as following:</p><formula xml:id="formula_11">Îi x = arg min I ∥I - Ĩi x ∥ 2 2 + ∥D•B • I -I 0 y ∥ 2 2 (<label>9</label></formula><formula xml:id="formula_12">)</formula><p>Here we use penalty method to form an unconstrained problem. The upscaled HR image Ĩi x can be computed as SCN (I i-1 y , Θ). The same process is repeated until convergence. We have applied the proposed iterative scheme to LR images generated from Gaussian blurring and sub-sampling as an example. The empirical performance is illustrated in Sec. VI.</p><p>2) Noisy Image Upscaling: Noise is a ubiquitous cause of corruption in image acquisition. State-of-the-art image denoising methods usually adopt priors such as patch similarity <ref type="bibr" target="#b30">[31]</ref>, patch sparsity <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b18">[19]</ref>, or both <ref type="bibr" target="#b32">[33]</ref>, as regularizer in image restoration. In this section, we propose a regularized noisy image upscaling scheme, for specifically handling noisy LR images, in order to obtain improved SR quality. Though any denoising algorithm can be used in our proposed scheme, here we apply spatial similarity combined with transform domain image patch group-sparsity as our regularizer <ref type="bibr" target="#b32">[33]</ref>, to form the regularized iterative SR problem as an example.</p><p>Similar to the method in Sec. IV-B1, we iteratively estimate the less noisy HR image from the denoised LR image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Given the denoised LR estimate</head><p>Îi-1 y at iteration i -1, we directly upscale it, using the learned generic SCN, to obtain the HR image Îi-1</p><p>x . It is then downsampled using bicubic interpolation, to generate the LR image Ĩi y , which is used in the fidelity term in the i-th iteration of LR image denoising. The same process is repeated until convergence. The iterative LR image denoising problem is formulated as follows:</p><formula xml:id="formula_13">{ Îi y , {α i } } = arg min I,{αi} ∥I - Ĩi y ∥ 2 2 + N ∑ j=1 { ∥W 3D G j I -α j ∥ 2 2 + τ ∥α j ∥ 0 } (<label>10</label></formula><formula xml:id="formula_14">)</formula><p>where the operator G j generates the 3D vectorized tensor, which groups the j-th overlapping patch from the LR image I, together with the spatially similar patches within its neighborhood by block matching <ref type="bibr" target="#b32">[33]</ref>. The codes {α j } of the patch groups in the domain of 3D sparsifying transform W 3D are sparse, which is enforced by the l 0 norm penalty <ref type="bibr" target="#b33">[34]</ref>. The weight τ controls the sparsity level, which normally depends on the remaining noise level in Ĩi y <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b33">[34]</ref>. In <ref type="bibr" target="#b9">(10)</ref>, we use the patch group sparsity as our denoising regularizer. The 3D sparsifying transform W 3D can be  commonly used analytical transforms, such as discrete cosine transform (DCT) or Wavelets. The state-of-the-art BM3D denoising algorithm <ref type="bibr" target="#b32">[33]</ref> is based on such an approach, but further improved by more sophisticated engineering stages. In order to achieve the best practical SR quality, we demonstrate the empirical performance comparison using BM3D as the regularizer in Sec. VI. Additionally, our proposed iterative method is a general practical SR framework, which is not dedicated to SCN. One can conveniently extend it to other SR methods, which generates Ĩi y in i th iteration. The performance comparison of these methods is illustrated in Sec. VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. SUBJECTIVE EVALUATION PROTOCOL</head><p>Subjective perception is an important metric to evaluate SR techniques for commercial use, other than the quantitative evaluation. In order to more thoroughly compare various SR methods and quantify the subjective perception, we utilize an online platform for subjective evaluation of SR results from several methods <ref type="bibr" target="#b35">[36]</ref>, including bicubic, SC <ref type="bibr" target="#b5">[6]</ref>, SE <ref type="bibr" target="#b8">[9]</ref>, selfexample regression (SER) <ref type="bibr" target="#b36">[37]</ref>, CNN <ref type="bibr" target="#b15">[16]</ref> and CSCN. Each participant is invited to conduct several pair-wise comparisons of SR results from different methods. The SR methods of displayed SR images in each pair are randomly selected. Ground truth HR images are also included when they are available as references. For each pair, the participant needs to select the better one in terms of perceptual quality. A snapshot of our evaluation web page 1 is shown in Fig. <ref type="figure" target="#fig_3">5</ref>.</p><p>Specifically, there are SR results over 6 images with different scaling factors: "kid"×4, "chip"×4, "statue"×4, "lion"×3, "temple"×3 and "train"×3. The images are shown in Fig. <ref type="figure" target="#fig_4">6</ref>. All the visual comparison results are then summarized into a 7×7 winning matrix for 7 methods (including ground truth). A Bradley-Terry <ref type="bibr" target="#b37">[38]</ref> model is calculated based to these results and the subjective score is estimated for each method according to this model. In the Bradley-Terry model, the probability that an object X is favored over Y is assumed to be p(X ≻ Y ) = e sX e sX + e sY = 1 1 + e sY -sX , <ref type="bibr" target="#b10">(11)</ref> where s X and s Y are the subjective scores for X and Y . The scores s for all the objects can be jointly estimated by maximizing the log likelihood of the pairwise comparison observations:</p><formula xml:id="formula_15">max s ∑ i,j w ij log ( 1 1 + e sj -si ) , (<label>12</label></formula><formula xml:id="formula_16">)</formula><p>where w ij is the (i, j)-th element in the winning matrix W , meaning the number of times when method i is favored over method j. We use the Newton-Raphson method to solve Eq. ( <ref type="formula" target="#formula_15">12</ref>) and set the score for ground truth method as 1 to avoid the scale ambiguity.</p><p>The experiment results are detailed in Sec. VI</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXPERIMENTS</head><p>We evaluate and compare the performance of our models using the same data and protocols as in <ref type="bibr" target="#b27">[28]</ref>, which are commonly adopted in SR literature. All our models are learned from a training set with 91 images, and tested on Set5 <ref type="bibr" target="#b38">[39]</ref>, Set14 <ref type="bibr" target="#b39">[40]</ref> and BSD100 <ref type="bibr" target="#b40">[41]</ref> which contain 5, 14 and 100 images respectively. We have also trained on other different larger data sets, and observe marginal performance change (around 0.1dB). The original images are downsized by bicubic interpolation to generate LR-HR image pairs for both training and evaluation. The training data are augmented with translation, rotation and scaling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation Details</head><p>We determine the number of nodes in each layer of our SCN mainly according to the corresponding settings used in sparse coding <ref type="bibr" target="#b5">[6]</ref>. Unless otherwise stated, we use input LR patch size s y =9, LR feature dimension m y =100, dictionary size n=128, output HR patch size s x =5, and patch aggregation filter size s g =5. All the convolution layers have a stride of 1. Each LR patch y is normalized by its mean and variance, and the same mean and variance are used to restore the final HR patch x. We crop 56×56 regions from each image to obtain fixed-sized input samples to the network, which produces outputs of size 44×44.</p><p>To reduce the number of parameters, we implement the LR patch extraction layer H as the combination of two layers: the first layer has 4 trainable filters each of which is shifted to 25 fixed positions by the second layer. Similarly, the patch combination layer G is also split into a fixed layer which aligns pixels in overlapping patches and a trainable layer whose weights are used to combine overlapping pixels. In this way, the number of parameters in these two layers are reduced by more than an order, and there is no observable loss in performance. We employ a standard stochastic gradient descent algorithm to train our networks with mini-batch size of 64. Based on the understanding of each layer's role in sparse coding, we use Harr-like gradient filters to initialize layer H, and use uniform weights to initialize layer G. All the remaining three linear layers are related to the dictionary pair (D x , D y ) in sparse coding. To initialize them, we first randomly set D x and D y with Gaussian noise, and then find the corresponding layer weights as in ISTA <ref type="bibr" target="#b22">[23]</ref>:</p><formula xml:id="formula_17">w 1 = C • D T y , w 2 = I -D T y D y , w 3 = (CL) -1 • D x (13)</formula><p>where w 1 , w 2 and w 3 denote the weights of the three subsequent layers after layer H. L is the upper bound on the largest eigenvalue of D T y D y , and C is the threshold value before normalization. We empirically set L=C=5.</p><p>The proposed models are all trained using the CUDA ConvNet package <ref type="bibr" target="#b12">[13]</ref> on a workstation with 12 Intel Xeon 2.67GHz CPUs and 1 GTX680 GPU. Training a SCN usually takes less than one day. Note that this package is customized for classification networks, and its efficiency can be further optimized for our SCN model.</p><p>In testing, to make the entire image covered by output samples, we crop input samples with overlap and extend the boundary of original image by reflection. Note we shave the image border in the same way as <ref type="bibr" target="#b15">[16]</ref> for objective evaluations to ensure fair comparison. Only the luminance channel is processed with our method, and bicubic interpolation is applied to the chrominance channels, as their high frequency components are less noticeable to human eyes. To achieve arbitrary scaling factors using CSCN, we upscale an image by times repeatedly until it is at least as large as the desired size. Then a bicubic interpolation is used to downscale it to the target resolution if necessary.</p><p>When reporting our best results in Sec. VI-C, we also use the multi-view testing strategy commonly employed in image classification. For patch-based image SR, multi-view testing is implicitly used when predictions from multiple overlapping patches are averaged. Here, besides sampling overlapping patches, we also add more views by flipping and transposing the patch. Such strategy is found to improve SR performance for general algorithms at the sheer cost of computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Algorithm Analysis</head><p>We first visualize the four filters learned in the first layer H in Fig. <ref type="figure" target="#fig_5">7</ref>. The filter patterns do not change much from the initial first and second order gradient operators. Some additional small coefficients are introduced in a highly structured form that capture richer high frequency details.</p><p>The performance of several networks during training is measured on Set5 in Fig. <ref type="figure" target="#fig_6">8</ref>. Our SCN improves significantly over sparse coding (SC) <ref type="bibr" target="#b5">[6]</ref>, as it leverages data more effectively with end-to-end training. The SCN initialized according to (13) can converge faster and better than the same model with  random initialization, which indicates that the understanding of SCN based on sparse coding can help its optimization.</p><p>We also train a CNN model <ref type="bibr" target="#b15">[16]</ref> of the same size as SCN, but find its convergence speed much slower. It is reported in <ref type="bibr" target="#b15">[16]</ref> that training a CNN takes 8×10 8 back-propagations (equivalent to 12.5×10 6 mini-batches here). To achieve the same performance as CNN, our SCN requires less than 1% back-propagations. The network size of SCN is mainly determined by the dictionary size n. Besides the default value n=128, we have tried other sizes and plot their performance versus the number of network parameters in Fig. <ref type="figure" target="#fig_7">9</ref>. The PSNR of SCN does not drop too much as n decreases from 128 to 64, but the model size and computation time can be reduced significantly, as shown in Table <ref type="table" target="#tab_0">I</ref>. Fig. <ref type="figure" target="#fig_7">9</ref> also shows the performance of CNN with various sizes. Our smallest SCN can achieve higher PSNR than the largest model (CNN-L) in <ref type="bibr" target="#b41">[42]</ref> while only using about 20% parameters.</p><p>Different numbers of recurrent stages k have been tested for SCN, and we find increasing k from 1 to 3 only improves performance by less than 0.1dB. As a tradeoff between speed and accuracy, we use k=1 throughout the paper.</p><p>In Table <ref type="table" target="#tab_0">II</ref>, different network structures with cascade for scalable SR in Sec. III-C2 (in each row) are compared at different scaling factors (in each column). SCN×a denotes the model trained with fixed scaling factor a without any cascade technique. For a fixed a, we use SCN×a as a basic module and apply it one or more times to super-resolve images for different  <ref type="table" target="#tab_0">II</ref>. It is observed that SCN×2 can perform as well as the scalespecific model for small scaling factor (1.5), and much better for large scaling factors <ref type="bibr">(3 and 4)</ref>. Note that the cascade of SCN×1.5 does not lead to good results since artifacts quickly get amplified through many repetitive upscalings. Therefore, we use SCN×2 as the default building block for CSCN, and drop the notation ×2 when there is no ambiguity. The last row in Table <ref type="table" target="#tab_0">II</ref> shows that a CSCN trained using the multi-scale objective in <ref type="bibr" target="#b6">(7)</ref> can further improve the SR results for scaling factors 3 and 4, as the second SCN in the cascade is trained to be robust to the artifacts generated by the first one.</p><p>As shown in <ref type="bibr" target="#b41">[42]</ref>, the amount of training data plays an important role in the field of deep learning. In order to evaluate the effect of various amount of data on training CSCN, we change the training set from a relatively small set of 91 images (Set91) <ref type="bibr" target="#b27">[28]</ref> to two other sets: the 199 out of 200 training images<ref type="foot" target="#foot_1">2</ref> in BSD500 dataset (BSD200) <ref type="bibr" target="#b40">[41]</ref>, and a subset of 7,500 images from the ILSVRC2013 dataset <ref type="bibr" target="#b43">[44]</ref>. A model of exactly the same architecture without any cascade is trained on each data set, and another 100 images from the ILSVRC2013 dataset are included as an additional test set. From Table <ref type="table" target="#tab_0">III</ref>, we can observe that the CSCN trained on BSD200 consistently outperforms its counterpart trained on Set91 by around 0.1dB on all test data. However, the performance of the model trained on ILSVRC2013 is slightly different from the one trained on BSD200, which shows the saturation of the performance as the amount of training data increases. The inferior quality of images in ILSVRC2013 may be a hurdle to further improve the performance. Therefore, our method is robust to training   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison with State of the Arts</head><p>We compare the proposed CSCN with other recent SR methods on all the images in Set5, Set14 and BSD100 for different scaling factors. Table <ref type="table" target="#tab_4">IV</ref> shows the PSNR and structural similarity (SSIM) <ref type="bibr" target="#b44">[45]</ref> for adjusted anchored neighborhood regression (A+) <ref type="bibr" target="#b42">[43]</ref>, CNN <ref type="bibr" target="#b15">[16]</ref>, CNN trained with larger model size and much more data (CNN-L) <ref type="bibr" target="#b41">[42]</ref>, the proposed CSCN, and CSCN with our multi-view testing (CSCN-MV). We do not list other methods <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b45">[46]</ref> whose performance is worse than A+ or CNN-L.</p><p>It can be seen from Table <ref type="table" target="#tab_4">IV</ref> that CSCN performs consistently better than all previous methods in both PSNR and SSIM, and with multi-view testing the results can be further improved. CNN-L improves over CNN by increasing model parameters and training data. However, it is still not as good as CSCN which is trained with a much smaller size and on a much smaller data set. Clearly, the better model structure of CSCN makes it less dependent on model capacity and training data in improving performance. Our models are generally more advantageous for large scaling factors due to the cascade structure. A larger performance gain is observed on Set5 than the other two test sets because Set5 has more similar statistics as the training set.</p><p>The visual qualities of the SR results generated by sparse coding (SC) <ref type="bibr" target="#b5">[6]</ref>, CNN and CSCN are compared in Fig. <ref type="figure" target="#fig_8">10</ref>. Our approach produces image patterns with shaper boundaries and richer textures, and is free of the ringing artifacts observable in the other two methods.</p><p>Fig. <ref type="figure" target="#fig_9">11</ref> shows the SR results on the "chip" image compared among more methods including the self-example based method (SE) <ref type="bibr" target="#b8">[9]</ref> and the deep network cascade (DNC) <ref type="bibr" target="#b13">[14]</ref>. SE and DNC can generate very sharp edges on this image, but also introduce artifacts and blurs on corners and fine structures due to the lack of self-similar patches. On the contrary, the CSCN method recovers all the structures of the characters without any distortion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Robustness to Real SR Scenarios</head><p>We evaluate the performance of the proposed practical SR methods in Sec. IV, by providing the empirical results of several experiments for the two aforementioned approaches.</p><p>1) Data-driven SR by Fine-tuning: The proposed method in Sec. IV-A is data-driven, and thus the generic SCN can be easily adapted for a particular task, with a small amount of training samples. We demonstrate the performance of this method in the application of enlarging low-DPI scanned document images with heavy noise. We first obtain several pairs of LR and HR images by scanning a document under two settings of 150DPI and 300DPI. Then we fine-tune our generic CSCN model using only one pair of scanned images for a few iterations. Fig. <ref type="figure" target="#fig_1">13</ref> illustrates the visualization of the upscaled image from the 150DPI scanned image. As shown by the SR results in Fig. <ref type="figure" target="#fig_1">13</ref>, the CSCN before adaptation is very sensitive to LR measurement corruption, so the enlarged texts in (b) are much more corrupted than they are in the nearest neighbor upscaled image (a). However, the adapted CSCN model removes almost all the artifacts and can restore clear texts in (c), which is promising for practical applications such as quality enhancement of online scanned books and restoration of legacy documents. 2) Regularized Iterative SR: We now show experimental results of practical SR for blurred and noisy LR images, using the proposed regularized iterative methods in Sec. IV-B. We first compare the SR performance on blurry images using the proposed method in Sec. IV-B1 with several other recent methods <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref>, using the same test images and settings. All these methods are designed for blurry LR input, while our model is trained on sharp LR input. As shown in Table V, our model achieves much better results than the competitors. Note the speed of our model is also much faster than the conventional sparse coding based methods.</p><p>To test the performance of upscaling noisy LR images, we simulate additive Gaussian noise for the LR input images at 4 different noise levels (σ = 5, 10, 15, 20) as the noisy input images. We compare the practical SR results in Set5 obtained from the following algorithms: directly using SCN, our proposed iterative SCN method using BM3D as denoising regularizer (iterative BM3D-SCN), and fine-tuning SCN with additional noisy training pairs. Note that knowing the underlying corruption model of real LR image (e.g., noise distribution or blurring kernel), one can always synthesizes real training pairs for fine-tuning the generic SCN. In other words, once the iterative SR method is feasible, one can always apply our data-driven method for SR alternatively. However, the other way around is not true. Therefore, the knowledge of the corruption model of real measurements can be considered as a stronger assumption, compared to providing real training image pairs. Correspondingly, the SR performances of these two methods are evaluated when both can be applied. We also provide the results of methods directly using another generic SR model: CNN-L <ref type="bibr" target="#b41">[42]</ref>, and the similar iterative SR method involving CNN-L (iterative BM3D-CNN-L).</p><p>The practical SR results are listed in Table <ref type="table" target="#tab_6">VI</ref>. We observed the improved PSNR using our proposed regularized iterative S-R method over all noise levels. The proposed iterative BM3D-SCN achieves much higher PSNR than the method of directly using SCN. The performance gap (in terms of SR PSNR) between iterative BM3D-SCN and direct SCN becomes larger, as the noise level increases. Similar observation can be found in the result comparison of iterative BM3D-CNN-L and direct CNN-L. Compared to the method of fine-tuning SCN, the iterative BM3D-SCN method demonstrates better empirical performance, with 0.3 dB improvement on average. The iterative BM3D-CNN-L method provides comparable results, compared to the iterative BM3D-SCN method, which demonstrates that our proposed regularized iterative SCN scheme can be easily extended for other SR methods, and is able to effectively handle noisy LR measurements.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Kernel</head><p>Gaussian σ = 1.0 Gaussian σ = 1.6 Method CSR <ref type="bibr" target="#b46">[47]</ref> NLM <ref type="bibr" target="#b47">[48]</ref> SCN CSR <ref type="bibr" target="#b46">[47]</ref> GSC <ref type="bibr" target="#b48">[49]</ref>  An example of upscaling noisy LR images using the aforementioned methods is demonstrated in Fig. <ref type="figure" target="#fig_11">12</ref>. Both findtuning SCN and iterative BM3D-SCN are able to significantly suppress the additive noise, while many artifacts induced by noise are observed in the SR result of direct SCN. It is notable that the fine-tuning SCN method performs better recovering the texture and the iterative BM3D-SCN method is preferable in smooth regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Subjective Evaluation</head><p>We have a total of 270 participants giving 720 pairwise comparisons over 6 images with different scaling factors, which are shown in Fig. <ref type="figure" target="#fig_4">6</ref>. Not every participant completed all the comparisons but their partial responses are still useful. Fig. <ref type="figure" target="#fig_2">14</ref> shows the estimated scores for the 6 SR methods in our evaluation, with the score for ground truth method normalized to 1. As expected, all the SR methods have much lower scores than ground truth, showing the great challenge   Subjective SR quality scores for different methods including bicubic, SC <ref type="bibr" target="#b5">[6]</ref>, SE <ref type="bibr" target="#b8">[9]</ref>, SER <ref type="bibr" target="#b36">[37]</ref>, CNN <ref type="bibr" target="#b15">[16]</ref> and the proposed CSCN. The score for ground truth result is 1.</p><p>such difference when seeing the two images side by side, and therefore make consistent ratings. The CNN model becomes less competitive in the subjective evaluation than it is in PSNR comparison. This indicates that the visually appealing image appearance produced by CSCN should be attributed to the regularization from sparse representation, which can not be easily learned by merely minimizing reconstruction error as in CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSIONS</head><p>We propose a new model for image SR by combining the strengths of sparse coding and deep network, and make considerable improvement over existing deep and shallow SR models both quantitatively and qualitatively. Besides producing good SR results, the domain knowledge in the form of sparse coding can also benefit training speed and model compactness. Furthermore, we investigate the cascade of network for both fixed and incremental scaling factors so as to enhance SR performance. In addition, the robustness to real SR scenarios is discussed for handling non-ideal LR measurements. More generally, our observation is in line with other recent extensions made to CNN with better domain knowledge for different tasks.</p><p>In future work, we will apply the SCN model to other problems where sparse coding can be useful. The interaction between deep networks for low-level and high-level vision tasks, such as <ref type="bibr" target="#b49">[50]</ref>, will also be explored.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Top left: the proposed SCN model with a patch extraction layer H, a LISTA sub-network for sparse coding (with k recurrent stages denoted by the dashed box), a HR patch recovery layer Dx, and a patch combination layer G. Top right: a neuron with an adjustable threshold decomposed into two linear scaling layers and a unit-threshold neuron. Bottom: the SCN re-organized with unit-threshold neurons and adjacent linear layers merged together in the gray boxes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. SR results for the "Lena" image upscaled by 4 times. (a) → (b) → (d) represents the processing flow with a single SCN×4 model. (a) → (c) → (e) represents the processing flow with two cascaded SCN×2 models. PSNR is given in parentheses.</figDesc><graphic coords="5,54.98,224.18,112.78,112.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Training cascade of SCNs with multi-scale objectives.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. The user interface of a web-based image quality evaluation, where two images are displayed side by side and local details can be magnified by moving mouse over the corresponding region.</figDesc><graphic coords="6,455.33,213.42,88.10,58.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. The 6 images used in subjective evaluation.</figDesc><graphic coords="6,399.84,272.12,90.99,54.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. The four learned filters in the first layer H.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. The PSNR change for ×2 SR on Set5 during training using different methods: SCN; SCN with random initialization; CNN. The horizontal dash lines show the benchmarks of bicubic interpolation and sparse coding (SC).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .</head><label>9</label><figDesc>Fig.9. PSNR for ×2 SR on Set5 using SCN and CNN with various network sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 13. Low-DPI scanned document upscaled by ×4 times using different methods.</figDesc><graphic coords="9,324.78,651.20,225.66,117.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. The "chip" image upscaled by ×4 times using different methods.</figDesc><graphic coords="11,28.98,157.38,192.02,157.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>(a) direct SCN (b) fine-tuning SCN (c) iterative BM3D-SCN PSNR=24.00dB PSNR=27.54dB PSNR=27.86dB</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. The "building" image corrupted by additive Gaussian noise of σ = 10 and then upscaled by ×2 times using different methods.</figDesc><graphic coords="12,71.83,56.11,154.20,231.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I TIME</head><label>I</label><figDesc>CONSUMPTION FOR SCN TO UPSCALE THE "BABY" IMAGE FROM 256×256 TO 512×512 USING DIFFERENT DICTIONARY SIZE n.</figDesc><table><row><cell>n</cell><cell>64</cell><cell>96</cell><cell>128</cell><cell>256</cell><cell>512</cell></row><row><cell cols="6">time (s) 0.159 0.192 0.230 0.445 1.214</cell></row><row><cell cols="6">upscaling factors, which is shown in each row of Table</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>TABLE IV PSNR (SSIM) COMPARISON ON THREE TEST DATA SETS AMONG DIFFERENT METHODS. RED INDICATES THE BEST AND BLUE INDICATES THE SECOND BEST PERFORMANCE. THE PERFORMANCE GAIN OF OUR BEST MODEL OVER ALL THE OTHERS' BEST IS SHOWN IN THE LAST ROW.</figDesc><table><row><cell>Data Set</cell><cell></cell><cell>Set5</cell><cell></cell><cell></cell><cell>Set14</cell><cell></cell><cell></cell><cell>BSD100</cell><cell></cell></row><row><cell>Upscaling</cell><cell>×2</cell><cell>×3</cell><cell>×4</cell><cell>×2</cell><cell>×3</cell><cell>×4</cell><cell>×2</cell><cell>×3</cell><cell>×4</cell></row><row><cell>A+ [43]</cell><cell>36.55 (0.9544)</cell><cell>32.59 (0.9088)</cell><cell>30.29 (0.8603)</cell><cell>32.28 (0.9056)</cell><cell>29.13 (0.8188)</cell><cell>27.33 (0.7491)</cell><cell>30.78 (0.8773)</cell><cell>28.18 (0.7808)</cell><cell>26.77 (0.7085)</cell></row><row><cell>CNN [16]</cell><cell>36.34 (0.9521)</cell><cell>32.39 (0.9033)</cell><cell>30.09 (0.8530)</cell><cell>32.18 (0.9039)</cell><cell>29.00 (0.8145)</cell><cell>27.20 (0.7413)</cell><cell>31.11 (0.8835)</cell><cell>28.20 (0.7794)</cell><cell>26.70 (0.7018)</cell></row><row><cell>CNN-L [42]</cell><cell>36.66 (0.9542)</cell><cell>32.75 (0.9090)</cell><cell>30.49 (0.8628)</cell><cell>32.45 (0.9067)</cell><cell>29.30 (0.8215)</cell><cell>27.50 (0.7513)</cell><cell>31.36 (0.8879)</cell><cell>28.41 (0.7863)</cell><cell>26.90 (0.7103)</cell></row><row><cell>CSCN</cell><cell>37.00 (0.9557)</cell><cell>33.18 (0.9153)</cell><cell>30.94 (0.8755)</cell><cell>32.65 (0.9081)</cell><cell>29.41 (0.8234)</cell><cell>27.71 (0.7592)</cell><cell>31.46 (0.8891)</cell><cell>28.52 (0.7883)</cell><cell>27.06 (0.7167)</cell></row><row><cell>CSCN-MV</cell><cell>37.21 (0.9571)</cell><cell>33.34 (0.9173)</cell><cell>31.14 (0.8789)</cell><cell>32.80 (0.9101)</cell><cell>29.57 (0.8263)</cell><cell>27.81 (0.7619)</cell><cell>31.60 (0.8915)</cell><cell>28.60 (0.7905)</cell><cell>27.14 (0.7191)</cell></row><row><cell>Our</cell><cell>0.55</cell><cell>0.59</cell><cell>0.65</cell><cell>0.35</cell><cell>0.27</cell><cell>0.31</cell><cell>0.24</cell><cell>0.19</cell><cell>0.24</cell></row><row><cell>Improvement</cell><cell>(0.0029)</cell><cell>(0.0083)</cell><cell>(0.0161)</cell><cell>(0.0034)</cell><cell>(0.0048)</cell><cell>(0.0106)</cell><cell>(0.0036)</cell><cell>(0.0042)</cell><cell>(0.0088)</cell></row><row><cell cols="5">data and can benefit marginally from a larger set of training</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>images.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V PSNR</head><label>V</label><figDesc>OF ×3 UPSCALING ON LR IMAGES WITH DIFFERENT BLURRING KERNELS.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VI PSNR</head><label>VI</label><figDesc>VALUES FOR ×2 UPSCALING NOISY LR IMAGES IN SET5 BY DIRECTLY USING SCN (DIRECT SCN), DIRECTLY USING CNN-L (DIRECT CNN-L), SCN AFTER FINE-TUNING ON NEW NOISY TRAINING DATA (FINE-TUNING SCN), THE ITERATIVE METHOD OF BM3D &amp; SCN (ITERATIVE BM3D-SCN), AND THE ITERATIVE METHOD OF BM3D &amp; CNN-L (ITERATIVE BM3D-CNN-L). CNN-L 33.42 31.16 29.62 28.59 in SR problem. The bicubic interpolation is significantly worse than other SR methods. The proposed CSCN method outperforms other previous state-of-the-art methods by a large margin, demonstrating its superior visual quality. It should be noted that the visual difference between some image pairs is very subtle. Nevertheless, the human subjects are able to</figDesc><table><row><cell>σ</cell><cell>5</cell><cell>10</cell><cell>15</cell><cell>20</cell></row><row><cell>Direct SCN</cell><cell cols="4">30.23 25.11 21.81 19.45</cell></row><row><cell>Direct CNN-L</cell><cell cols="4">30.47 25.32 21.91 19.46</cell></row><row><cell>Fine-tuning SCN</cell><cell cols="4">33.03 31.00 29.46 28.44</cell></row><row><cell>Iterative BM3D-SCN</cell><cell cols="4">33.51 31.22 29.65 28.61</cell></row><row><cell>Iterative BM3D-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>www.ifp.illinois.edu/ ∼ wang308/survey</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Since one out of 200 training images coincides with one image in Set5, we exclude it from our training set.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Limits on super-resolution and how to break them</title>
		<author>
			<persName><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1167" to="1183" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fundamental limits of reconstruction-based superresolution algorithms under local translation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="83" to="97" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Image upsampling via imposed edge statistics</title>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">95</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Image up-sampling using total-variation regularization with a new observation model</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Aly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dubois</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1647" to="1659" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Image super-resolution via sparse representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Coupled dictionary training for image super-resolution</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3467" to="3478" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Image super-resolution with sparse neighbor embedding</title>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3194" to="3205" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Super-resolution from a single image</title>
		<author>
			<persName><forename type="first">D</forename><surname>Glasner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bagon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Image and video upscaling from local selfexamples</title>
		<author>
			<persName><forename type="first">G</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Single-image super-resolution using sparse regression and natural image prior</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">I</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kwon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1127" to="1133" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Similarity constraints-based structured output regression machine: An approach to image super-resolution</title>
		<author>
			<persName><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Single-image super-resolution: a benchmark</title>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="372" to="386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep network cascade for image super-resolution</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="49" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Self-tuned deep super resolution</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="184" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Image super-resolution with fast approximate convolutional sparse coding</title>
		<author>
			<persName><forename type="first">C</forename><surname>Osendorfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Van Der Smagt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="250" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning fast approximations of sparse coding</title>
		<author>
			<persName><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Structured overcomplete sparsifying transform learning with convergence guarantees and applications</title>
		<author>
			<persName><forename type="first">B</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ravishankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bresler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Computer Vision</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="167" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<title level="m">Sparse Coding and its Applications in Computer Vision</title>
		<imprint>
			<publisher>World Scientific</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep networks for image super-resolution with sparse prior</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="370" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Fast inference in sparse coding algorithms with applications to object recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1010.3467</idno>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An iterative thresholding algorithm for linear inverse problems with a sparsity constraint</title>
		<author>
			<persName><forename type="first">I</forename><surname>Daubechies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Defrise</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">De</forename><surname>Mol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications on Pure and Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1413" to="1457" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sparse coding via thresholding and local competition in neural circuits</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Rozell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Baraniuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Olshausen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2526" to="2563" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4400</idno>
		<title level="m">Network in network</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted Boltzmann machines</title>
		<author>
			<persName><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Heterogeneous network embedding via deep architectures</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Anchored neighborhood regression for fast example-based super-resolution</title>
		<author>
			<persName><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1920" to="1927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Building high-level features using large scale unsupervised learning</title>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="8595" to="8598" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning and transferring mid-level image representations using convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1717" to="1724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A non-local algorithm for image denoising</title>
		<author>
			<persName><forename type="first">A</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Coll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">K-SVD: An algorithm for designing overcomplete dictionaries for sparse representation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Aharon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bruckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4311" to="4322" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Image denoising by sparse 3D transform-domain collaborative filtering</title>
		<author>
			<persName><forename type="first">K</forename><surname>Dabov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2080" to="2095" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Video denoising by online 3d sparsifying transform learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ravishankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bresler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Online sparsifying transform learning -part i: Algorithms</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ravishankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bresler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Process</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="625" to="636" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning super-resolution jointly from external and internal examples</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4359" to="4371" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Fast image super-resolution based on in-place example regression</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1059" to="1066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Rank analysis of incomplete block designs: I. the method of paired comparisons</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Terry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="page" from="324" to="345" />
			<date type="published" when="1952">1952</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Lowcomplexity single-image super-resolution based on nonnegative neighbor embedding</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roumy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guillemot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-L</forename><forename type="middle">A</forename><surname>Morel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">On single image scale-up using sparse-representations</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zeyde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Protter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Curves and Surfaces</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="711" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="416" to="423" />
			<date type="published" when="2001-07">July 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A+: Adjusted anchored neighborhood regression for fast super-resolution</title>
		<author>
			<persName><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">De</forename><surname>Smet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Single image super-resolution from transformed self-exemplars</title>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Centralized sparse representation for image restoration</title>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1259" to="1266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Single image super-resolution with non-local means and steering kernel regression</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4544" to="4556" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Geometry constrained sparse coding for single image super-resolution</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1648" to="1655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">His research experience encompasses using deep learning to solve low-level vision problems, including image superresolution, image restoration and image denoising</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012, and the M.S. degree from the University of Illinois at Urbana-Champaign</title>
		<meeting><address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2016. 2014</date>
		</imprint>
	</monogr>
	<note>Ding Liu (S&apos;15) received the B.S. degree from the Chinese University of. He has research interests in the broad area of computer vision, image processing and deep learning</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
