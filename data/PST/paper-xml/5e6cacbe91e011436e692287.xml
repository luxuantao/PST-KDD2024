<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Graph Neural Network Approach for Scalable Wireless Power Control</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-07-19">19 Jul 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yifei</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of ECE</orgName>
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Information Science and Technology</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<postCode>201210</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuanming</forename><surname>Shi</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Jun</forename><surname>Zhang</surname></persName>
							<email>jun-eie.zhang@polyu.edu.hk</email>
							<affiliation key="aff2">
								<orgName type="department">Dept. of EIE</orgName>
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Khaled</forename><forename type="middle">B</forename><surname>Letaief</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of ECE</orgName>
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Information Science and Technology</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<postCode>201210</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Graph Neural Network Approach for Scalable Wireless Power Control</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-07-19">19 Jul 2019</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1907.08487v1[cs.IT]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Resource allocation</term>
					<term>geometric deep learning</term>
					<term>graph neural networks</term>
					<term>wireless networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep neural networks have recently emerged as a disruptive technology to solve NP-hard wireless resource allocation problems in a real-time manner. However, the adopted neural network structures, e.g., multi-layer perceptron (MLP) and convolutional neural network (CNN), are inherited from deep learning for image processing tasks, and thus are not tailored to problems in wireless networks. In particular, the performance of these methods deteriorates dramatically when the wireless network size becomes large. In this paper, we propose to utilize graph neural networks (GNNs) to develop scalable methods for solving the power control problem in K-user interference channels. Specifically, a K-user interference channel is first modeled as a complete graph, where the quantitative information of wireless channels is incorporated as the features of the graph. We then propose an interference graph convolutional neural network (IGCNet) to learn the optimal power control in an unsupervised manner. It is shown that one-layer IGCNet is a universal approximator to continuous set functions, which well matches the permutation invariance property of interference channels and it is robust to imperfect channel state information (CSI). Extensive simulations will show that the proposed IGCNet outperforms existing methods and achieves significant speedup over the classic algorithm for power control, namely, WMMSE.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Effective resource allocation plays a crucial role for performance optimization in wireless networks. However, typical resource allocation problems, such as power control <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, are non-convex and computationally challenging. Moreover, they need to be solved in a real-time manner to accommodate the time variation of wireless channels. Great efforts have been put to develop effective algorithms for wireless resource allocation, and design solutions have been obtained with powerful convex optimization based approaches. Nevertheless, the resulting algorithms still fall short, given the increasing density of wireless networks and the more stringent latency requirement of emerging mobile applications.</p><p>Inspired by the recent successes of deep learning, researchers have attempted to apply deep learning based methods to solve NP-hard optimization problems in wireless networks <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b10">[11]</ref>. As a classic wireless resource allocation problem, power control in the K-user interference channel has attracted most of the attention <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b4">[5]</ref>. The first attempts came from <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, which applied MLP and CNN, respectively, to approximate the classic weighted minimum mean square error (WMMSE) algorithm <ref type="bibr" target="#b11">[12]</ref> and accelerate the computation. Unsupervised learning and an ensembling mechanism were employed in <ref type="bibr" target="#b2">[3]</ref> to achieve better performance than the suboptimal WMMSE algorithm.</p><p>However, MLP and CNN, which are designed for image processing, may not be suitable for problems in wireless communication. In particular, the performance of these methods degrades dramatically when the network size becomes large. This is because MLP and CNN fail to exploit the underlying topology of wireless networks. To enable more efficient learning, spatial convolution <ref type="bibr" target="#b3">[4]</ref> and graph embedding <ref type="bibr" target="#b4">[5]</ref> have been proposed to exploit the Euclidean geometry of the users' geolocations. These methods are scalable to large-size networks. However, they have major disadvantage, namely, they can not utilize the instantaneous channel state information (CSI), which can not be embedded into the Euclidean space. This leads to poor performance in fading channels. Another drawback is that they have trouble in dealing with problems with heterogeneity, e.g., weighted sum rate maximization. A comparison of the existing works for K-user interference channel power control is shown in Table <ref type="table" target="#tab_0">I</ref>.</p><p>Graph neural networks (GNNs) can effectively exploit non-Euclidean data <ref type="bibr" target="#b12">[13]</ref>, e.g., CSI. In this paper, to overcome the limitations mentioned above, we propose to employ GNNs for wireless power control in K-user interference channels. Specifically, a K-user interference channel can be naturally modeled as a complete graph, where the quantitative information of wireless networks, e.g., CSI, is incorporated as the features of the graph. Based on the principle of graph neural networks, we propose interference graph convolutional networks (IGCNet) to learn the optimal resource allocation in an unsupervised manner. It is shown that IGCNet is a universal approximator of continuous set functions, which well preserves the permutation invariance property of the interference links. Extensive simulations will demonstrate that the proposed IGCNet not only outperforms the state-of-the art optimization-based WMMSE algorithm and existing learning- </p><formula xml:id="formula_0">O(K 2 ) O(K 2 ) O(K 2 ) O(K + N 2 ) O(K 2 ) O(K 2 )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sample complexity</head><p>Medium Medium Medium Large Small Small Ability to incorporate instantaneous CSI Ability to solve weighted problems based methods under various system configurations, but also achieves significant speedup over WMMSE. Furthermore, we will show that the proposed IGCNet can handle estimation uncertainty, e.g., CSI uncertainty, both theoretically and empirically. For reproducibility, the code to produce the results in this paper has been made available on github <ref type="foot" target="#foot_0">1</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PRELIMINARIES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. System Model</head><p>We consider the power control problem in a K-user interference channel with K single-antenna transceiver pairs. The received signal at the k-th receiver is given by</p><formula xml:id="formula_1">y k = h kk s k + j =k h kj s j + n k ,</formula><p>where h kk ∈ C denotes the direct-link channel between the k-th transmitter and receiver, h kj ∈ C denotes the cross-link channel between transmitter j and receiver k, s k ∈ C denotes the data symbol for the k-th receiver, and n k ∼ CN (0, σ 2 k ) is the additive Gaussian noise.</p><p>The signal-to-interference-plus-noise ratio (SINR) for the k-th receiver is given by</p><formula xml:id="formula_2">SINR k = |h kk | 2 p k i =k |h ki | 2 p i + σ 2 k , where p k = E[s 2 k ]</formula><p>is the power of the k-th transmitter, and 0 ≤ p k ≤ P max .</p><p>Denote p = [p 1 , • • • , p K ] as the power allocation vector. The objective is to find the optimal power allocation to maximize the weighted sum rate, and the problem is formulated as</p><formula xml:id="formula_3">maximize p K k=1 w k log 2 (1 + SINR k ) subject to 0 ≤ p k ≤ P max , ∀k,<label>(1)</label></formula><p>where w k is the weight for the k-th pair. The channel matrix is defined as</p><formula xml:id="formula_4">H = [h 1 , • • • , h K ] T and h i = [h 1i , • • • , h Ki ] T , i = 1, • • • , K.</formula><p>This problem is known to be NP-hard <ref type="bibr" target="#b6">[7]</ref>. Although several optimization-based methods have been proposed in <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b13">[14]</ref>, they are computationally demanding, and thus cannot be applied for real-time implementation <ref type="bibr" target="#b0">[1]</ref>. To alleviate the computation burden while achieving near-optimal performance, machine learning based methods have been proposed. Specifically, MLP <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b2">[3]</ref> and CNN <ref type="bibr" target="#b1">[2]</ref> have been used to approximate the input-output mapping of this problem. The optimization-based methods involve many iterations, with each iteration having a time complexity of O(K 2 ). In contrast, the total complexities of these learning-based methods are O(K 2 ), and thus they can achieve significant speedups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Existing Approaches' Limitations</head><p>In this subsection, we identify the performance deterioration phenomenon of existing methods using MLP or CNN <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref>.</p><p>Fig. <ref type="figure">II</ref> illustrates MLP and CNN based approaches for power control. From the numerical experiments in <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b2">[3]</ref>, we observe a performance loss when K gets larger. For example, in <ref type="bibr" target="#b0">[1]</ref>, the performance gap to the WMMSE algorithm is 3% when K = 10 and becomes 12% when K = 30. From the perspective of approximation theory, an MLP with a sufficient number of parameters can learn anything if we have sufficient training samples <ref type="bibr" target="#b14">[15]</ref>. However, in practice, there are lots of redundancy in an MLP because it is fully connected. Such redundancy causes overfitting and makes it difficult to train. This is the reason for the performance loss when the input and output dimensions are large.</p><p>CNN has demonstrated its effectiveness in solving such performance deterioration problems in image analysis applications <ref type="bibr" target="#b15">[16]</ref>, but it is not effective for wireless power control. Specifically, for images, the geometric property means that adjacent pixels are meaningful to be considered together <ref type="bibr" target="#b15">[16]</ref>. In CNN, a 2D convolution kernel is applied to each patch (adjacent pixels) in the image. The weights in the neural network are shared among different patches. This leads to a significant reduction in the number of parameters, which leads to a lower sample complexity and also makes it easy for training. This accounts for the superior performance of CNN in image processing. Unfortunately, the geometric property in images does not hold for a channel matrix since a 3 × 3 patch does not contain any specific meaning for the power control problem. Thus, although using CNN can reduce the number of parameters, it suffers from a large performance degradation, as will be further shown in Section IV-A.</p><p>There have been some attempts to leverage the geometry of users' geolocation to achieve scalability, i.e., the ability to deal with large-size wireless systems. One study <ref type="bibr" target="#b3">[4]</ref> applied the idea of convolution in the spatial domain. Specifically, the whole considered area is first divided into N -by-N grids, followed by computing the number of active users in each grid as its density. The spatial convolution is a convolution operator  on the density grid. In this scenario, the neighbor grids are useful because the nearest users will cause the strongest interferences. One major drawback of this work is that it requires a large number of samples for training. To address this issue, <ref type="bibr" target="#b4">[5]</ref> proposed to use distance quantization and graph embedding. However, spatial convolution and distance quantization are merely operating on the distances and can not incorporate instantaneous CSI. Thus, it results in poor performance when fading exists (as shown in Table V in <ref type="bibr" target="#b3">[4]</ref>). Furthermore, they are not able to deal with the weighted problems.</p><p>In the next section, we will discuss the geometric properties of the K-user interference channels and design the corresponding neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. LEARNING OPTIMAL POWER CONTROL ON INTERFERENCE GRAPH</head><p>In this section, we first model a K-user interference channel as a complete graph, followed by a brief introduction to GNNs. Under the framework of GNNs, we propose IGCNet to learn the optimal power control on an interference graph in an unsupervised manner. The theoretical analysis for IGCNet is presented at the end of this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Graph Representation and Geometric Properties</head><p>In this subsection, we model the K-user interference channel as a complete graph with vertex and edge labels. We view the i-th transmitter-receiver pair as the i-th vertex. The vertex label contains the state of the direct channel and the weight of the i-th pair, i.e., (h ii , w i ). One edge between two vertices indicates an interference link, with label as the states of the interference channels h ij and h ji . An illustration of a 3-user interference channel is shown in Fig. <ref type="figure" target="#fig_2">2</ref>.</p><p>We next discuss the geometry of the interference channel by looking at the map from the channel matrix and weights to the optimal power control vector.</p><p>Proposition 1. For a given i, let f i (•, •) denote the function that maps the channel matrix and the weights to the optimal power allocation of the i-th transmitter, i.e., p * i = f i (H, w), and let Π denote any permutation matrix satisfying This can be interpreted as the unordered property of interference channels : It is the collection of interference channel coefficients instead of the ordering of these coefficients that matter. The irrelevance in the ordering leads to the permutation invariance property of the channel matrix. This property suggests that only considering the neighborhood elements, such as in CNN, is meaningless because the elements are no longer close to each other after the permutation. This invariance property indicates that all the edges with the same end node are homogeneous, and will allow us to share weights among all the edges of a node. In other words, we can restrict the hypothesis space of the designed neural network for one node to the space of set functions, which leads to GNNs.</p><formula xml:id="formula_5">(Π T HΠ) ii = h ii . Then, p * i = f i (H, w) = f i (Π T HΠ, Π T w).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Graph Neural Networks</head><p>In this subsection, we give a brief introduction to GNNs, and one can refer to <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b16">[17]</ref> for a more detailed information. GNNs deal with learning problems with graph data or non-Euclidean data. There are many sucessful applications of GNNs such as recommendation systems <ref type="bibr" target="#b17">[18]</ref> and solving combinatorial problems <ref type="bibr" target="#b18">[19]</ref>. GNNs utilize the graph structure of data, the node features, and the edge features to learn a good representation of the vertices. Like MLP or CNN, GNNs have layer-wise structures. In each layer, for each vertex, GNNs update the representation of this node by aggregating features from its edges and its neighbor vetices. Specifically, the update rule of the k-th layer at vertex v in GNNs is</p><formula xml:id="formula_6">α (k) v = AGGREGATE (k) ({β (k−1) u : u ∈ N (v)}, {γx : x ∈ E(v)}) β (k) v = COMBINE (k) (β (k−1) v , α (k) v )<label>(2)</label></formula><p>where N (v) denotes the set of the neighbors of v, E(v) denotes the set of edges with v as one end node, AGGREGATE(•) and COMBINE(•) are two functions, β</p><formula xml:id="formula_7">(k) v</formula><p>denotes the k-th layer's output feature of vertex v, and α</p><formula xml:id="formula_8">(k) v</formula><p>is an intermediate variable.</p><p>The design of the two functions in GNNs is crucial and leads to different kinds of GNNs <ref type="bibr" target="#b16">[17]</ref>. The most popular GNNs are listed as follows.</p><p>1) Graph Convolutional Network <ref type="bibr" target="#b19">[20]</ref>: It uses the mean pooling and relu as the aggregation and combination functions,</p><formula xml:id="formula_9">β (k) v = RELU(W (k) • 1 |N (v)| + 1 u β (k−1) u )</formula><p>where u ∈ N (v) ∪ {v} and {W (k) } are the weight matrices to be learned and RELU(x) = MAX(0, x). 2) Structure2Vec <ref type="bibr" target="#b5">[6]</ref>: It uses the sum pooling and relu as the aggregation and combination functions,</p><formula xml:id="formula_10">β (k) v = RELU W 1 β (k−1) v + W 2 u β (k−1) u ,</formula><p>where u ∈ N (v), and W 1 and W 2 are the weight matrices to be learned. 3) Graph Isomorphism Network <ref type="bibr" target="#b16">[17]</ref>: It uses the MLP and sum pooling as the aggregation and combination functions,</p><formula xml:id="formula_11">β (k) v = MLP (k) (1 + (k) )β (k−1) v + u β (k−1)</formula><p>u where u ∈ N (v), MLP (k) is an MLP, and different MLPs are used in different layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Interference Graph Convolutional Networks</head><p>In this paper, we design the aggregation and combination functions of a GNN following two principles based on the observations in Section III-A. First, the designed neural network should capture the permutation invariance property of the interference channel, as stated in Proposition 1. Second, the designed neural network should be robust to the inaccurate measurements, e.g., imperfect CSI, which is critical for the practical implementation. Thus, the proposed aggregation functions (neural networks) should not only be a good approximator of set functions, but also robust to the corruptions of edge labels. For the first requirement, the idea is to use a symmetric function on transformed elements in the point set to approximate a general function defined on the set <ref type="bibr" target="#b20">[21]</ref>:</p><formula xml:id="formula_12">AGGREGATE({x 1 , • • • , x n }) ≈ g(o(x 1 ), • • • , o(x n )), where f : 2 R N → R, o : R N → R K and g : R K ×• • •×R K → R is a symmetric function.</formula><p>For the implementation of this paper, we use a 3-layer MLP as o(•), i (•) and max(•) as g(•), and a 3-layer MLP as COMBINE(•). Specifically, the update rule is</p><formula xml:id="formula_13">γ (k) u,v = MLP1(h uv , h vu , w v , h uu , β (k−1) u ) α (k) v = CONCAT MAX u (γ u,v }), u γ u,v , u ∈ N (v) β (k) v = MLP2(α (k) v , h vv , β (k−1) v , w v ),<label>(3)</label></formula><p>where MAX({•}) is to take the largest value in a set, MLP1 and MLP2 denote two different MLPs, CONCAT denotes the operation that concatenates two vectors together, and γ</p><formula xml:id="formula_14">(k) u,v</formula><p>denotes the feature vector of the edge connecting vertex u and vertex v. It will be shown in the next subsection that the proposed aggregation function also satisfies the second requirement. An illustration of the proposed network structure and parameter setting is shown in Fig. <ref type="figure" target="#fig_3">3</ref>.</p><p>The loss function adopted is the negative sum rate, as in <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>,</p><formula xml:id="formula_15">= −E H K k=1 w k log 2 1 + |h kk | 2 p k (θ) i =k |h ki | 2 p i (θ) + σ 2 k ,</formula><p>where θ denotes the parameters of the neural networks and p i (θ) is the power value generated by the neural networks. Note that this loss function is differentiable and can be directly optimized by stochastic gradient descent. Thus, IGCNet is an unsupervised method and it only needs the channel matrices as samples without labels for training.</p><p>The proposed IGCNet can also deal with other objective functions in the K-user interference channel. To achieve this goal, one can simply replace the loss function with the negative objective function to be maximized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Theoretical Analysis</head><p>In this subsection, we show the universal approximation property and robustness of the proposed IGCNet.  Theorem 1 states that one-layer IGCNet is a universal approximator of continuous set functions. Note that the aggregation function of GNNs is a set function. Thus, IGCNet has the aggregation function with the most powerful representation ability in the class of GNNs. Besides, it well respects the permutation invariance property of the interference channel because the set function is permutation invariant to the input.</p><formula xml:id="formula_16">Theorem 1. (Universal Approximation) Suppose f : X → R is a continuous set function. Then, ∀ &gt; 0, ∃ a continuous function h and a symmetric function g(x 1 , • • • , x n ) = ζ • POOLING, such that ∀S ∈ X , f (S) − ζ POOLING xi∈S ({h(x i )}) &lt; where x 1 , • • • , x n is the elements in S,</formula><formula xml:id="formula_17">)}) = MAX xi∈S ({h(x i )}) or POOLING xi∈S ({h(x i )}) = xi∈S h(x i ) or POOLING xi∈S ({h(x i )}) = 1 |S| xi∈S h(x i ). Theorem 2. (Robustness) [21] Suppose u : X → R p such that u = MAX xi∈S and f = γ • u. Then, (a) ∀S, ∃C S , N S ∈ X , f (T ) = f (S) if C S ⊂ T ⊂ N S ; (b) |C S | ≤ p … … MLP1 MLP1 MLP2 AGGREGATE COMBINE</formula><p>Theorem 2 states that f (S) remains the same up to the corruptions of the input if all the features in C S are preserved and C S only contains a bounded number of features, smaller than p. This means that only a small proportion of features are critical and IGCNet is robust to the corruptions of other features. The practical meaning is that IGCNet can provide a near-optimal solution even when some CSI is not available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. SIMULATIONS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Gaussian Interference Channel Power Control</head><p>In this subsection, in order to demonstrate the effectiveness of IGCNet, we follow the system setting of <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b2">[3]</ref> to set up simulations. Under this system setting, all the weights are the same and the channel coefficients are taken from the standard normal distribution. In the experiment, three network setups K ∈ {10, 20, 30} are considered. We mainly compare the proposed IGCNet with the following five benchmarks:</p><p>1) WMMSE <ref type="bibr" target="#b11">[12]</ref>: This is the most popular optimizationbased algorithm for the K-user interference channel power control. It is also used as a benchmark in <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref>. 2) MLP <ref type="bibr" target="#b0">[1]</ref>: It leverages MLP to learn the input-output mapping of WMMSE. 3) PCNet <ref type="bibr" target="#b2">[3]</ref>: It employs MLP and an unsupervised loss function to learn near-optimal power allocation. 4) DPC <ref type="bibr" target="#b1">[2]</ref>: CNN and the unsupervised loss function are used in this method to learn a near-optimal power control. 5) Baseline: We find a fixed proportion of pairs with the largest coefficients w i |h ii | 2 , and set the power of these pairs as P max , while the power for other pairs are set as 0. This algorithm ignoring the interference is the simplest but effective heuristic algorithm and we shall use it as the baseline. We do not compare with <ref type="bibr" target="#b3">[4]</ref> and <ref type="bibr" target="#b4">[5]</ref> as they can not incorporate instantaneous CSI.</p><p>We generate 20000 training samples, i.e., network realizations, to train MLP, PCNet, and DPC as in <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref> while the number of training samples used for IGCNet is 2000. The test dataset contains 500 network realizations. We use a 5-layer IGCNet and adopt the adam optimizer with a learning rate of 10 −3 to train IGCNet. The simulation results are shown in Table <ref type="table" target="#tab_2">II</ref>.</p><p>It is shown that the proposed IGCNet not only outperforms the learning-based method, but also achieves a better performance than WMMSE. We also see that the performance of IGCNet is stable while other learning-based methods suffer from performance degradation when K increases. The difference between PCNet and IGCNet is that IGCNet utilizes the graph structure and the weights are shared. Thus, it suggests that leveraging the graph structure of the interference channel is useful for maintaining good performance when the network size is large. Besides, we observe that the performance gap between other learning-based methods and the baseline vanishes when K = 30. This may imply that these models can hardly learn the impact of interference when K is large. We next test the performance of IGCNet in the weighted sum rate maximization. We take w i from the uniform distribution in [0, 1] and test the performance of IGCNet, WMMSE, and the baseline algorithm. The results are shown in Table <ref type="table" target="#tab_3">III</ref>.</p><p>From Table <ref type="table" target="#tab_3">III</ref>, we see that the performance of IGCNet is better than other benchmarks. This demonstrates that IGCNet can handle the weighted problem without requiring a large amount of samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Generalization</head><p>An important test of the usefulness of neural network's design is its ability to generalize to different layouts and link distributions <ref type="bibr" target="#b3">[4]</ref>. In this subsection, we test the generalization performance of the proposed IGCNet in two different settings.</p><p>1) Varying User Locations: We first test the performance of the proposed algorithm under the situation where the locations of users in each sample vary. The transmitters are uniformly distributed in the square region [0, 100] × [0, 100] meters. The receivers are uniformly distributed within <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10]</ref> meters away from the transmitter. The adopted channel model is</p><formula xml:id="formula_18">h ij = 10 −L(dij )/20 φ ij s ij g ij</formula><p>where the path loss model is L(d ij ) = 148.1 + 37.6 log 2 (d ij ), s kl is the shadowing coefficient, the standard deviation of log-norm shadowing is 8dB, φ ij = 9dBi is the transmit antenna power gain, g ij ∼ CN (0, 1) is the small scale fading, and the noise power is σ 2 k = −102dBm. We use equal weights to test the performance of different algorithms, with the results shown in Table <ref type="table" target="#tab_4">IV</ref>. We see that IGCNet also has a superior performance under the system settings with varying user locations. 2) Varying Distance Distribution: It was reported in <ref type="bibr" target="#b3">[4]</ref> that spatial convolution is sensitive to the link distance distribution. We also check the performance of IGCNet when the link distance distribution in the test is different from that in the training. We follow <ref type="bibr" target="#b3">[4]</ref> to set up the simulation. The link distance is uniformly distributed in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10]</ref> meters during training. In the test, the link distance is uniformly distributed in [l r , u r ] meters, where l r is uniform in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b19">20]</ref> meters and u r is uniform in [l r , 20] meters. The performance of IGCNet is 101.2% compared to WMMSE in this situation. It shows that the performance of IGCNet under this setting is still good.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Robustness</head><p>In this subsection, the situation with partial CSI and noisy CSI are tested. We use the pre-trained model for K = 30 in Section IV-B1.</p><p>1) Partial CSI: We simulate the situation where CSI of some links cannot be obtained. To test the performance under this partial CSI setting, we set a fixed proportion of h ij with the largest distance as 0. We define the relative performance as the sum rate achieved by IGCNet with the partial CSI divided by the that achieved by the case with full CSI. The relative performance of IGCNet versus the missing CSI ratio is shown in Fig. <ref type="figure" target="#fig_4">4</ref>. We see that IGCNet achieves 92% performance of the full CSI case when 70% of the links are set to 0 in the available CSI, which verifies the robustness shown in Theorem 2. 2) Noisy CSI: We simulate the situation where the CSI is inaccurate. We use the pre-trained model K = 30 in Section IV-B1. To test the performance under the noisy CSI setting, we use the noisy channel matrix Ĥ = H + N as the input of IGCNet, where N ∼ CN (0, σ 2 I). We define the relative noise variance as η = σ 2 ×K×K H 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F</head><p>. The relative performance of IGCNet versus the missing CSI ratio is shown in Fig. <ref type="figure" target="#fig_4">4</ref>. We see that IGCNet achieves its 92% performance when the relative noise variance is 10%. This suggests IGCNet is robust to CSI inaccuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Time Comparison</head><p>It was reported in <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b2">[3]</ref> that learning-based methods have less computation time than the optimization-based methods. We also compare the average running time for WMMSE and IGCNet under the system setting in Section IV-B1, as shown in Table <ref type="table" target="#tab_5">V</ref>. It can be concluded that IGCNet is significantly faster than WMMSE, up to about 65x speedup when K = 30. This is because WMMSE involves many iterations, and each iteration has time complexity O(K 2 ) while the total complexity of IGCNet is O(K 2 ).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Ablation Study</head><p>We provide the ablation study of IGCNet in terms of the numbers of training samples and the number of layers of IGCNet. We assume Rayleigh fading and K = 30.</p><p>We first study the impact of the numbers of training samples. We set the numbers of training samples as {50, 200, 500, 2000, 5000, 20000} and observe the test performance of IGCNet. The results are shown in Table <ref type="table" target="#tab_6">VI</ref>. We see that the performance first increases then becomes stable when the number of samples increases. It also suggests 2000 samples are sufficient for this setting, which are much less than the samples needed for previous works. We then study the impact of the number of layers. m-hop information is gathered if a m-layer IGCNet is used. Intuitively, IGCNet with a larger number of layers will have better performance. We set the numbers of layers as {1, 3, 5, 7, 9} and observe the test performance of IGCNet. The performance improves as the number of layers increase. This is not surprising because IGCNet with more layers captures more information. We also see a huge performance gain from 1-layer IGCNet to 3-layer IGCNet, which shows that multi-hop information is crucial for the performance. From Table <ref type="table" target="#tab_2">II</ref> and Table <ref type="table" target="#tab_7">VII</ref>, we find that 1-layer IGCNet still outperforms MLP, PCNet, and DPC, which demonstrates the benefits of leveraging the graph structure.</p><p>In summary, the extensive simulations listed in Section IV have shown that 1) IGCNet not only outperforms other state-of-the-art learning-based methods, but also has better performance than the most popular optimization-based method, WMMSE, under various system configurations. 2) IGCNet can generalize to different layouts and link distributions. 3) IGCNet is robust to partial CSI and noisy CSI. 4) IGCNet achieves significant speedups over WMMSE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>In this paper, we developed a novel graph neural network for the K-user interference channel power control problem. The unique advantages include scalability, ability to incorporate instantaneous CSI, and ability to solve weighted problems. This is achieved by leveraging the geometric property and graph structure of the interference channels. For future directions, it will be interesting to test the effectiveness of IGCNet in other wireless resource allocation problems. We envisioned that machine learning based methods will play a critical role in future wireless networks <ref type="bibr" target="#b21">[22]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>An illustration of power control via an MLP. The channel matrix is first flattened into a 1D vector, which results in the structure information loss. An illustration of power control via a CNN. Only nearby elements are put together in the 2D convolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. CNN and MLP for K-user interference channel power control.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The 3-user interference channel and the corresponding graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The structure of aggregation and combination functions in the proposed IGCNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The relative performance versus the missing ratio of CSI.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. The relative performance versus the relative noise variance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I A</head><label>I</label><figDesc>COMPARISON OF DIFFERENT METHODS FOR POWER CONTROL IN THE K-USER INTERFERENCE CHANNEL.</figDesc><table><row><cell></cell><cell>MLP [1]</cell><cell>DPC [2]</cell><cell>PCNet [3]</cell><cell>Spatial Convolution [4]</cell><cell>Graph Embedding [5]</cell><cell>This paper</cell></row><row><cell>Neural networks used</cell><cell>MLP</cell><cell>CNN</cell><cell>MLP</cell><cell>Spatial Convolution [4]</cell><cell>Structure2Vec [6]</cell><cell>Proposed IGCNet</cell></row><row><cell>Training scheme</cell><cell cols="5">Supervised Supervised or Unsupervised Unsupervised Supervised or Unsupervised Supervised or Unsupervised</cell><cell>Unsupervised</cell></row><row><cell>Scalability</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Time complexity</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>ζ is a continuous function, and POOLING is a pooling operation, i.e., POOLING xi∈S ({h(x i</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II AVERAGE</head><label>II</label><figDesc>SUM RATE UNDER EACH SETTING. THE RESULTS ARE NORMALIZED BY THE SUM RATE ACHIEVED BY WMMSE.</figDesc><table><row><cell>IGCNet</cell><cell>MLP</cell><cell>PCNet</cell><cell>DPC</cell><cell>Baseline</cell></row><row><cell cols="4">K = 10 102.6% 98.2% 101.4% 95.1%</cell><cell>89.1%</cell></row><row><cell cols="4">K = 20 102.7% 92.3% 90.2% 83.1%</cell><cell>86.6%</cell></row><row><cell cols="4">K = 30 102.4% 85.3% 87.6% 79.3%</cell><cell>84.4%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III AVERAGE</head><label>III</label><figDesc>SUM RATE UNDER EACH SETTING. THE RESULTS ARE NORMALIZED BY THE SUM RATE ACHIEVED BY WMMSE.</figDesc><table><row><cell>IGCNet</cell><cell cols="2">MLP Baseline</cell></row><row><cell cols="2">K = 10 106.4% 93.7%</cell><cell>92.5%</cell></row><row><cell cols="2">K = 20 106.9% 86.4%</cell><cell>87.5%</cell></row><row><cell cols="2">K = 30 104.7% 81.3%</cell><cell>87.9%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV AVERAGE</head><label>IV</label><figDesc>SUM RATE UNDER EACH SETTING. THE RESULTS ARE NORMALIZED BY THE SUM RATE ACHIEVED BY WMMSE.</figDesc><table><row><cell>IGCNet</cell><cell cols="2">MLP PCNet Baseline</cell></row><row><cell cols="2">K = 10 103.4% 83.7% 86.4%</cell><cell>75.9%</cell></row><row><cell cols="2">K = 20 104.4% 70.7% 86.3%</cell><cell>78.0%</cell></row><row><cell cols="2">K = 30 104.2% 63.1% 83.0%</cell><cell>75.5%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V AVERAGE</head><label>V</label><figDesc>RUNNING TIME FOR THE ALGORITHMS UNDER EACH SETTING (IN MILLISECONDS).</figDesc><table><row><cell></cell><cell cols="3">K = 10 K = 20 K = 30</cell></row><row><cell>IGCNet</cell><cell cols="3">0.14ms 0.27ms 0.48ms</cell></row><row><cell cols="2">WMMSE 9.31ms</cell><cell>24.1ms</cell><cell>31.4ms</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VI AVERAGE</head><label>VI</label><figDesc>SUM RATE OF IGCNET WITH DIFFERENT NUMBERS OF TRAINING SAMPLES.</figDesc><table><row><cell># Samples</cell><cell>50</cell><cell>200</cell><cell>500</cell><cell>2000</cell><cell>5000</cell><cell>20000</cell></row><row><cell cols="7">Performance 97.6% 101.2% 101.4% 102.4% 102.5% 102.4%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VII AVERAGE</head><label>VII</label><figDesc>SUM RATE FOR IGCNET WITH DIFFERENT NUMBERS OF LAYERS.</figDesc><table><row><cell># Layers</cell><cell>1</cell><cell>3</cell><cell>5</cell><cell>7</cell><cell>9</cell></row><row><cell cols="6">Performance 94.7% 101.3% 102.4% 102.5% 102.7%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://github.com/yshenaw/Globecom2019</note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported by the Hong Kong Research Grants Council under Grant No. 16210719.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to optimize: Training deep neural networks for interference management</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Sidiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="5438" to="5453" />
			<date type="published" when="2018-10">Oct. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep power control: Transmit power control scheme based on convolutional neural network</title>
		<author>
			<persName><forename type="first">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-H</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Commun. Lett</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1276" to="1279" />
			<date type="published" when="2018-04">Apr. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Towards optimal power control via ensembling deep neural networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.10025</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Spatial deep learning for wireless scheduling</title>
		<author>
			<persName><forename type="first">W</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Areas Commun</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<date type="published" when="2019-06">Jun. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Graph embedding based wireless link scheduling with few training samples</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">Y</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02871</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Discriminative embeddings of latent variable models for structured data</title>
		<author>
			<persName><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learning</title>
				<meeting>Int. Conf. Mach. Learning</meeting>
		<imprint>
			<date type="published" when="2016-06">Jun. 2016</date>
			<biblScope unit="page" from="2702" to="2711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Power control in wireless cellular networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Found. Trends Networking</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="381" to="533" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Group sparse beamforming for green Cloud-RAN</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">B</forename><surname>Letaief</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Wireless Commun</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="2809" to="2823" />
			<date type="published" when="2014-05">May 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">LORM: Learning to optimize for resource management in wireless networks with few training samples</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">B</forename><surname>Letaief</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.07998</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Modelaided wireless artificial intelligence: Embedding expert knowledge in deep neural networks towards wireless systems optimization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zappone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Di Renzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Debbah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.01672</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning to branch: Accelerating resource allocation in wireless networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">Y</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.01819</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An iteratively weighted MMSE approach to distributed sum-utility maximization for a mimo interfering broadcast channel</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Razaviyayn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="4331" to="4340" />
			<date type="published" when="2011-09">Sept. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.00596</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fractional programming for communication systemspart i: Power control and beamforming</title>
		<author>
			<persName><forename type="first">K</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="2616" to="2630" />
			<date type="published" when="2018-05">May 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multilayer feedforward networks are universal approximators</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hornik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stinchcombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="359" to="366" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Approximating CNNs with Bag-of-local-Features models works surprisingly well on ImageNet</title>
		<author>
			<persName><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learning Representation</title>
				<meeting>Int. Conf. Learning Representation</meeting>
		<imprint>
			<date type="published" when="2019-05">May 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learning Representation</title>
				<meeting>Int. Conf. Learning Representation</meeting>
		<imprint>
			<date type="published" when="2019-05">May 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Int. Conf. Knowl. Discovery Data Mining</title>
				<meeting>ACM Int. Conf. Knowl. Discovery Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018-08">Aug. 2018</date>
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Combinatorial optimization with graph convolutional networks and guided tree search</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
				<meeting>null</meeting>
		<imprint>
			<date type="published" when="2018-12">Dec. 2018</date>
			<biblScope unit="page" from="539" to="548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learning Representation</title>
				<meeting>Int. Conf. Learning Representation</meeting>
		<imprint>
			<date type="published" when="2017-04">Apr. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vision Pattern Recognition</title>
				<meeting>IEEE Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">B</forename><surname>Letaief</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-J</forename><forename type="middle">A</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.11686</idno>
		<title level="m">The roadmap to 6G-AI empowered wireless networks</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
