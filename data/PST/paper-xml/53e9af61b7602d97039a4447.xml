<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Iconic and multi-stroke gesture recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Don</forename><surname>Willems</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Donders Institute for Brain, Cognition and Behaviour</orgName>
								<orgName type="institution">Radboud University Nijmegen</orgName>
								<address>
									<postBox>P.O. Box 9104</postBox>
									<postCode>6500 HE</postCode>
									<settlement>Nijmegen</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ralph</forename><surname>Niels</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Donders Institute for Brain, Cognition and Behaviour</orgName>
								<orgName type="institution">Radboud University Nijmegen</orgName>
								<address>
									<postBox>P.O. Box 9104</postBox>
									<postCode>6500 HE</postCode>
									<settlement>Nijmegen</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marcel</forename><surname>Van Gerven</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Donders Institute for Brain, Cognition and Behaviour</orgName>
								<orgName type="institution">Radboud University Nijmegen</orgName>
								<address>
									<postBox>P.O. Box 9104</postBox>
									<postCode>6500 HE</postCode>
									<settlement>Nijmegen</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Louis</forename><surname>Vuurpijl</surname></persName>
							<email>l.vuurpijl@donders.ru.nl</email>
							<affiliation key="aff0">
								<orgName type="department">Donders Institute for Brain, Cognition and Behaviour</orgName>
								<orgName type="institution">Radboud University Nijmegen</orgName>
								<address>
									<postBox>P.O. Box 9104</postBox>
									<postCode>6500 HE</postCode>
									<settlement>Nijmegen</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Iconic and multi-stroke gesture recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0945BA2AA925E5DDCD39B634CA469BC4</idno>
					<idno type="DOI">10.1016/j.patcog.2009.01.030</idno>
					<note type="submission">Received 11 August 2008 Received in revised form 6 January 2009 Accepted 12 January 2009</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Iconic gestures Multi-stroke gesture recognition Feature selection</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Many handwritten gestures, characters, and symbols comprise multiple pendown strokes separated by penup strokes. In this paper, a large number of features known from the literature are explored for the recognition of such multi-stroke gestures. Features are computed from a global gesture shape. From its constituent strokes, the mean and standard deviation of each feature are computed. We show that using these new stroke-based features, significant improvements in classification accuracy can be obtained between 10% and 50% compared to global feature representations. These results are consistent over four different databases, containing iconic pen gestures, handwritten symbols, and upper-case characters. Compared to two other multi-stroke recognition techniques, improvements between 25% and 39% are achieved, averaged over all four databases.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The research described in this paper is motivated by the development of pen input recognition technologies for iconic gestures. Such gestures have a visually meaningful shape and are therefore easier to learn and remember by the users of pen-aware systems than abstract gestures which have no obvious relation between shape and semantics <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. In the ongoing ICIS project <ref type="bibr" target="#b2">[3]</ref>, iconic gestures are used to indicate events or objects on interactive maps. ICIS aims at the domain of crisis management, where pen input devices like a tabletPC or PDA are used to convey messages. The typical pen interactions that emerge in these scenarios were explored in <ref type="bibr" target="#b3">[4]</ref>. The categorization of the obtained pen gestures showed that next to route descriptions and markings of locations, the iconic sketchings of, e.g., cars, fires, casualties, accidents, or persons occurred quite frequently. In accordance with these observations, we designed and collected a suitable set of iconic gestures for specifying objects and events. The acquired database is called NicIcon <ref type="bibr" target="#b4">[5]</ref> and is publicly available via http://www.unipen.org.</p><p>A wide range of pen gesture recognition systems have been described in the literature, like Rubine's GRANDMA system <ref type="bibr" target="#b5">[6]</ref>, Quickset <ref type="bibr" target="#b6">[7]</ref>, SILK <ref type="bibr" target="#b7">[8]</ref>, and iGesture <ref type="bibr" target="#b8">[9]</ref>. For a recent review, the reader is referred to <ref type="bibr" target="#b0">[1]</ref>. The majority of these systems target either the recognition of command gestures <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref> (e.g., arrow up/down/left/right for scrolling, or gestures for performing delete/undo actions) or the sketches and drawings for design applications <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13]</ref>. Most gesture recognition systems employ Rubine's 13 global features, which are computed from a complete gesture shape. Rubine's features have mainly been used for recognizing single-stroke gestures like the unistroke <ref type="bibr" target="#b13">[14]</ref> or grafitti alphabets <ref type="bibr" target="#b14">[15]</ref>. Unfortunately, they are only moderately successful when applied to multi-stroke pen input <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16]</ref>.</p><p>Multi-stroke gestures pose similar problems to recognition technologies as handwritten characters or symbols. Shape variations, differences in stroke ordering, and a varying number of strokes have to be taken into account (see Fig. <ref type="figure">1</ref>). There are several approaches to tackle these problems. The first employs modeling of stroke sequences. For example, using hidden Markov models (HMMs) <ref type="bibr" target="#b18">[19]</ref> or dynamic graphical models <ref type="bibr" target="#b19">[20]</ref>, each stroke is mapped to an individual stroke model, which can be implemented as HMM states or nodes from a graphical model. The second approach captures variability in stroke length and stroke sequences through feature representations such as chain codes or spatio-temporal resampling <ref type="bibr" target="#b20">[21]</ref>. Third, dynamic programming algorithms such as dynamic time warping (DTW) <ref type="bibr" target="#b21">[22]</ref> can be employed for performing non-linear curve matching. Finally, to improve the processing of multi-stroke gestures, more elaborate and distinguishing features can be computed from a global multi-gesture shape <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16]</ref>, similar to Rubine's algorithms.</p><p>The current paper focuses on the latter approach: the design and evaluation of new features for multi-stroke gesture recognition. To this end, four publicly available databases containing multi-stroke gestures will be explored. The distinguishing properties of different groups of features are evaluated for these datasets, by using the best individual N (BIN) feature selection algorithm <ref type="bibr" target="#b22">[23]</ref> and two wellknown classification algorithms. The results will be compared to two alternative methods: classification based on spatio-temporally resampled gesture trajectories <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b23">24]</ref> and based on DTW <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b24">25]</ref>.</p><p>Fig. <ref type="figure">1</ref>. Multi-stroke gestures with varying shapes and different number of strokes. Depicted are the classes `car' and `fire' from the NicIcon collection (first and second row), capitals `E' and `A' from both the UNIPEN <ref type="bibr" target="#b16">[17]</ref> and the IRONOFF <ref type="bibr" target="#b17">[18]</ref> databases (third and fifth row), and symbols from the UNIPEN database (fourth row).</p><p>In the next section, we will briefly describe the four databases. In Section 3, we will elaborate on different feature sets that can be employed for multi-stroke gesture recognition. In particular, new features will be presented that are based on the observation that 90% of the iconic gestures contained in the NicIcon dataset and a large portion of handwritten gestures contained in other data collections are drawn in multiple strokes. For each gesture, the features are computed along the complete gesture shape as well as along each individual stroke. As we will show through feature selection and recognition performances (Sections 4 and 5), adding mean and standard deviations of the individual stroke features has a very positive impact, which may also be of value for other applications in pattern recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Databases containing multi-stroke gestures</head><p>For the experiments described in this paper, we have considered four different collections of multi-stroke gestures. The first is the NicIcon <ref type="bibr" target="#b4">[5]</ref> database of iconic gestures which was recently made publicly available. Since for this paper, we have used a modified segmentation algorithm for isolating pen gestures, we briefly report on the differences with respect to <ref type="bibr" target="#b4">[5]</ref> in Section 2.1. The other three databases are well known and comprise the UNIPEN 1d collection of handwritten symbols <ref type="bibr" target="#b16">[17]</ref>, the UNIPEN 1b collection of handwritten capital characters, and the handwritten capitals contained in the IRONOFF database <ref type="bibr" target="#b17">[18]</ref>. From each collection, we excluded the singlestroked samples. From the remaining samples, three subsets were extracted (a training set and a test set for optimizing a classifier and an evaluation set which is kept hidden until final assessments). Stratified random sampling was used, such that each subset contains the same relative number of samples per class. The data were divided such that training, test, and evaluation sets contain 36%, 24% and 40% of the samples, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">The NicIcon database of iconic pen gestures</head><p>The gesture repertoire from the NicIcon database was based on the icon lexicon from the IconMap application developed by Fitriani and Rothkrantz <ref type="bibr" target="#b1">[2]</ref>. In IconMap, users can convey information about crisis situations by clicking on a well-designed set of icons. Although, as discussed in <ref type="bibr" target="#b1">[2]</ref>, iconic communication for this domain is new, the icon shapes used in IconMap are based on a standard set of icon classes used by the governments of the United States, Australia and New Zealand <ref type="bibr" target="#b25">[26]</ref>. From the set of icons in IconMap, we constructed an icon lexicon containing the fourteen classes depicted in Fig. <ref type="figure" target="#fig_0">2</ref> and representing a representative subset of the messages contained in <ref type="bibr" target="#b25">[26]</ref>. It should be noted that these iconic gestures were collected in a laboratory setting where subjects were sitting at a desk, filling in well-designed boxed forms. Since this is far from the envisaged mobility contexts, these data should be considered as a first step toward the design of pen input recognition technology for interactive maps. On the other hand, collecting isolated handwritten characters or words for training handwriting recognition systems is a common approach. Consider, for example, the IAM database <ref type="bibr" target="#b26">[27]</ref> and databases containing non-Wester scripts like Japanese <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref>, Tamil <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref>, and Arabic <ref type="bibr" target="#b31">[32]</ref>.</p><p>The automated segmentation of the online data in iconic gestures reported in <ref type="bibr" target="#b4">[5]</ref> employed both temporal information and spatial layout characteristics, resulting in 24,441 samples. However, due to a particular way of entering gestures the temporal stroke ordering for several samples was disturbed. We modified the segmentation algorithm such that we were able to recover these samples, resulting in a total of 26,163 iconic gestures. By discarding gestures with only one pendown stroke, in total 23,641 iconic gestures were selected. Table <ref type="table" target="#tab_0">1</ref> shows the distribution of samples distinguished in the fourteen gesture classes. The table also shows the average number of strokes that users employ to draw an iconic gesture class. On average, 5.2 strokes are used for each iconic gesture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Handwritten capitals and symbols</head><p>To assess the generalizability of our approach, three standard online databases containing multi-stroke gestures are explored as well. The data contain uppercase characters from the UNIPEN <ref type="bibr" target="#b16">[17]</ref> and IRONOFF <ref type="bibr" target="#b17">[18]</ref> collections and a selection of symbols from the UNIPEN collection. The IRONOFF database contains isolated characters, digits, and cursive words written by French writers. We used the IRONOFF `B-forms' subset, containing 10,679 isolated capitals written by 412 different writers. The UNIPEN train_r01_v07 release contains a heterogeneous collection of characters, digits and words collected from writers frp, different countries of origin. From this collection, the `1b' subset contains 28,069 isolated uppercase characters and the `1d' subset contains 17,286 isolated symbols, from which 4833 samples containing ten classes were selected (`=', `;', `:', `!', `$', `#', `%', `+', `?' and ` * '). Table <ref type="table">2</ref> depicts the number of multi-stroke gestures selected from these three collections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Features for multi-stroke recognition</head><p>From each of the four databases described above, three feature sets are computed, each on a different level of detail. The g-48 set contains 48 features computed from a global gesture trajectory.  For each class, the average number of strokes s is given, counting both pendown and penup strokes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2</head><p>Statistics of the selected IRONOFF capitals and UNIPEN capitals and symbols. As we will argue in Section 3.2, these features cannot always distinguish between certain gesture classes, in particular if class separation requires a more detailed representation. The second set of features considers gestures at the stroke level and contains features computed from each stroke along the gesture trajectory, including the mean and standard deviation of these feature values. At the finest level of detail, features are computed from each coordinate, as originally proposed by Guyon and LeCun in <ref type="bibr" target="#b20">[21]</ref>. In the next subsections, we will describe these three feature sets: the g-48, the s--, and the c-n sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Global features: the g-48 feature set</head><p>As mentioned in the Introduction, most gesture recognition systems employ Rubine's thirteen features <ref type="bibr" target="#b5">[6]</ref>. Among these features are the length and the angle of the bounding box diagonal, the distance between the first and the last point, the cosine and the sine of the angle between the first and the last point, the total gesture length, the total angle traversed, and the duration of the gesture. In <ref type="bibr" target="#b4">[5]</ref>, classification experiments on iconic gestures were presented which employed Rubine's features and an additional fifteen other global features (see Figs. <ref type="figure">3</ref> and<ref type="figure">4</ref> for some examples of these features). The classification accuracy using these 28 global features on the NicIcon database was significantly lower than when using features computed at the coordinate level from spatially resampled pen gestures. These findings corroborate other reports on using global features for gesture recognition <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16]</ref> and indicate the need for improved feature representations.</p><p>As a result of our quest for more and better distinguishing features, we have recently compiled a survey on features for pen computing, which is available as a technical report <ref type="bibr" target="#b32">[33]</ref>. We have Fig. <ref type="figure">5</ref>. Iconic gesture representing a casualty icon. Pendown strokes are odd numbered. The penup stroke `4' is marked with an arrow for distinguishing from stroke `6'. The new s--feature set contains features computed for each of the 9 strokes (segmented by transitions between pendown and penup). For each global feature f from the g-48 set, ( f ) and ( f ) over all strokes were added as feature values. This was repeated for (i) all strokes, (ii) only the pendown strokes, and (iii) only the penup strokes. For each sample the value for two features is given for the global feature (Fg), for the mean feature value over the strokes (Fm), and for the standard deviation value over the strokes (Fs). fc and (fc) denote the mean value and the value of the standard deviation of the feature over all samples in that class, f denotes the feature value of that sample, and 'Offset' denotes the offset of the feature value from the average feature value for the class in standard deviations for that class (Offset=|(f -fc)/ (fc)|). If the offset has a high value, the feature value is an outlier for that class. Note that the feature values are normalized to a mean value of 0.0 and standard deviation of 1.0 over all samples from all classes.</p><p>included features from, e.g., <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref>. The features described in <ref type="bibr" target="#b32">[33]</ref> also contain local features computed along the coordinates from a gesture shape, such as chain code representations. These local features were excluded and for the current paper, we have selected 20 new global features, additional to the 28 features used in <ref type="bibr" target="#b4">[5]</ref>. The resulting g-48 feature set is described in Appendix A. For details on the feature computation algorithms, the reader is referred to <ref type="bibr" target="#b32">[33]</ref>. Table <ref type="table">7</ref> in Appendix A depicts which of the g-48 features are rotation or size dependent. No features require both scaling and rotation. For features like area and trajectory length, size normalization is required. Other features, such as horizontal or vertical distance between samples, require rotation normalization. Scale normalization was performed by scaling a gesture's bounding box to unit size. Rotation was performed by aligning a gesture's principal axis to the x-axis, as depicted in Fig. <ref type="figure">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Stroke-level features: the s--feature set</head><p>In the current paper, we will compare the g-48 feature set to feature representations computed from each individual stroke. These stroke-level features comprise both the mean and the standard deviation of the g-48 feature values computed over (i) all constituent strokes, (ii) penup strokes only, and (iii) pendown strokes only (see Fig. <ref type="figure">5</ref>). Note that for the penup/pendown ratio and the pendown count, the distinction in penup/pendown strokes is irrelevant. Care should be taken in cases where the pen is lifted too far from the tablet to be sampled as they may result in unreliable or incorrect feature values. For most tablets, such penfar events can be detected. However, since the amount of such cases is very limited (0.0% for the IRONOFF capitals, less than 0.1% for the UNIPEN capitals and symbols, and less than 0.2% for the NicIcon dataset) and since all feature values of the samples containing penfar events are within normal range, we decided to discard penfar events.</p><p>To understand why global features computed over the complete gesture shape cannot always properly distinguish between multistroke gestures, please consider Table <ref type="table" target="#tab_1">3</ref>. All these examples are correctly distinguished by the stroke-level features but not by the global features. As can be observed, these examples exhibit some specific characteristics making them harder to classify. In the first example (a), the participant made the cross bold, using multiple strokes, in the second example (b) the participant made a correction, in (c) the participant made a spurious double stroke on the left side, in (d) the icon is more rectangular than normal for this class (should be an elongated triangle), and in (e) the wrong box (a diamond instead of a triangle) was drawn.</p><p>Apparently, features computed at the level of individual strokes do allow classes to be discriminated in cases where the global features fail. These cases occur most often when gestures are written in a sloppy fashion, when writers make corrections, or when (parts of) a sample is re-drawn using multiple similar strokes.</p><p>The resulting feature set contains 758 features and is called the s--set. In Appendix A, a concise discussion is provided explaining how we arrived at this number of features. Although and are very common measures in statistical pattern recognition (e.g., for estimating probability density functions or for regularization purposes), to our knowledge, the mean and standard deviation of features computed from sub-strokes of a gesture trajectory have not been used before as features for multi-stroke recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Coordinate-level features: the c-30 and the c-60 feature sets</head><p>To assess the suitability of our new features, the s--feature set is compared to a set of features computed at the coordinate level. We use both the c-30 and the c-60 features, described in <ref type="bibr" target="#b23">[24]</ref>. These are computed from gestures spatially resampled to n = 30 or 60 coordinates. The c-30 features have extensively been used for character recognition. For each out of n points, the (x, y, z)-coordinates, the running angles and angular velocities are computed, resulting in 3 • n + 2 • (n -1) + 2 • (n -2) features. As explained in <ref type="bibr" target="#b23">[24]</ref>, a typical resampling of Western characters requires n = 30 (204 features). Given that many of the collected iconic gestures have a more complex shape than the average Western character, we also explored resampling to n = 60 (414 features), resulting in a better coverage of the original trajectory with resampled points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Feature selection and classifier design</head><p>To explore the suitability of the 758 features from the s--feature set, the following method was employed. For each dataset, we computed the s--stroke-level features and the c-30 and c-60 coordinate features. All features were normalized through mean shifting <ref type="bibr" target="#b36">[37]</ref> to a common scale with an average of zero and standard deviation of one. Second, eight different subsets were selected, as listed below: The distinction between features with and without and was made such that their individual contribution to recognition accuracy could be assessed. Third, each of these feature sets was divided into three subsets (36% train, 24% test, and 40% evaluation), using stratified random sampling. The fourth step in the comparison between features entailed feature selection using the BIN feature selection algorithm <ref type="bibr" target="#b22">[23]</ref>, which is described in Section 4.1. The final step in the feature assessment process used the selected features (through BIN) to design different classifiers based on the train and test sets. The description of the different classifiers involved is given in Section 4.2. The results of these elaborate explorations are presented in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Feature selection from each feature subset</head><p>The BIN feature selection algorithm <ref type="bibr" target="#b22">[23]</ref> was used to select a suitable subset of features from the first six feature sets (the stroke-level features). BIN tests the discriminative power of each of the features individually. The features are ranked according to the recognition performance for each individual feature using a support vector machine (SVM) classifier. The SVM was trained on each feature using the train set and the recognition performance was tested on the test set. Training the SVM was performed using the default parameter settings (see also Section 4.2.1). Like most other feature selection methods, the BIN method cannot guarantee to yield the best combination of features <ref type="bibr" target="#b37">[38]</ref>. Furthermore, BIN does not explore linear or non-linear combinations of selected features, which in certain cases will lead to sub-optimal solutions <ref type="bibr" target="#b38">[39]</ref>. Yet, we decided to use this method because it is relatively straight-forward, efficient and can be used to discard a relevant subset of poor features <ref type="bibr" target="#b37">[38]</ref>.</p><p>We used BIN feature selection for each dataset and each of the stroke-level feature sets, resulting in 24 feature rankings. For each ranking, the `accumulated' recognition performance was computed by training and testing an SVM classifier on r features, where r is varied from 1 (the feature with the highest rank) to the number of features. As an example, consider Fig. <ref type="figure" target="#fig_3">6</ref>. For each ranking, the fraction of features yielding the maximum accumulated recognition performance was determined. This fraction was subsequently used for further experiments and the remaining features were discarded. Each fraction is reported in Table <ref type="table" target="#tab_3">4</ref>, and ranges from 0.11 (for Fgms) to 0.99 (for Fg), both for the symbol dataset. On average, the fraction of features at which maximum performance was achieved was 0.5 ( =0.2).</p><p>Another result from the BIN feature selection process concerns the relative amount of features from the categories g-48, , and . For each combination of dataset and the three feature sets Fgm, Fgs, and Fgms, the relative amount of selected features from the corresponding BIN experiments was computed. Averaged over all datasets, the ratios (g-48/ / ) are (0.48/0.52/0.0) for Fgm, (0.49/0.0/0.51) for Fgs, and (0.35/0.37/0.28) for the Fgms features, respectively. These results indicate that, according to the BIN method, each of the g-48, , and features provides a similar contribution to recognition performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Classifier design and learning</head><p>Three classifiers (multi-layered perceptron (MLP), SVM, and DTW) were used for generating recognition scores on the various feature sets computed from each database. Each classifier used the train set and test set for learning and tuning of control parameters. Only  For each dataset and feature subset, the fraction f of features at which maximal BIN performance is achieved (on the testset) is depicted. The results show a consistent improvement when adding and features.</p><p>after this optimization process, the classification performance on the evaluation set was used as the evaluation measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">Feature-based classification using MLP and SVM</head><p>Two common feature-based classifiers were used: the MLP and the SVM <ref type="bibr" target="#b39">[40]</ref>. The MLP neural network implementation uses the generalized delta rule with momentum. The parameters varied for the MLP were learning rate, momentum, and number of hidden units. Training each MLP was performed until performance on the test set reached a maximum performance, as determined through cross-validation. We used LIBSVM <ref type="bibr" target="#b40">[41]</ref>, public domain software implementing a multi-class SVM-classifier (C-SVC). Besides the traditional linear kernel, non-linear kernels were employed in order to achieve the highest possible classification performance. We tested polynomial, radial basis function and sigmoid kernels. Each of these kernels has their own parameters which we varied: gamma for all non-linear kernels and degree and coef 0 for the polynomial and the sigmoid kernel. Additionally, for all kernels, we tried different cost parameters C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">Template matching using DTW</head><p>The dynamic time warping (DTW) algorithm described in <ref type="bibr" target="#b24">[25]</ref> computes the DTW distance between two data samples by summing the normalized Euclidean distances between the matching coordinates of a known prototypical data sample A and an unknown sample B. For the experiments reported in this paper, the DTW classifier uses all training samples as prototypes. Whether two coordinates A i and B j match is decided using three conditions: (i) the continuity condition, which is satisfied when index i is on the same relative position on A as index j is on B (the amount in which the relative positions are allowed to differ is controlled by a parameter c), (ii) the boundary condition, which is satisfied if both i and j are at the first, or at the last position of their sample, (iii) the penup/pendown condition, which is satisfied when both i and j are produced with the pen on the tablet, or when they are both produced with the pen above the tablet. A i and B j match if either the boundary condition, or both other conditions are satisfied. Classification of a test sample is performed through nearest neighbour matching with the DTW distance function. Each DTW classifier was optimized by varying parameter c, which controls the strictness of the continuity condition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>In this section the results of our feature assessments on four databases containing multi-stroke gestures are presented. First, in Section 5.1, the classification results on the 24 feature subsets derived from the s--features are discussed. Subsequently, in Section 5.2, these results are compared to the results achieved with DTW and with the classifiers trained on the c-30 and c-60 features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Evaluation of feature subsets computed from the s--features</head><p>For each feature subset and database, a SVM and an MLP classifier were optimized following the method described in the previous section. Table <ref type="table" target="#tab_3">4</ref> shows the corresponding classification results on the evaluation datasets, containing the remaining 40% of the data.</p><p>As can be expected, the SVM classifier outperforms the MLP. Compared to the global features Fg, adding mean and standard deviation features computed at the stroke level improves classification accuracy. The results are consistent over different databases and both </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 6</head><p>The notation and definitions used in Table <ref type="table">7</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Notation</head><formula xml:id="formula_0">= b if a &gt; b else b = a Lowest x value xmin = min1 i N xi = min1 i N (si • e1) Lowest y value ymin = min1 i N yi = min1 i N (si • e2) Principal components p i Angle of first principal axis = arctan p 1 • e2 p 1 • e1 Length of first principal axis = 2max0 n&lt;N |p 2 • (c -sn)| Length of second principal axis = 2max0 n&lt;N |p 1 • (c -sn)| Centroid = 1 N n sn Velocity vi = si+1 -si-1 ti+1 -ti-1 Acceleration ai = vi+1 -vi-1 ti+1 -ti-1</formula><p>classifiers. The best performances are achieved when using features from all three feature sets. Relative improvements in error rates when comparing the Fgms and Fg features range from 10% (for the MLP classifiers on the UNIPEN data) to 40% and 50% for the SVM and MLP on the IRONOFF database. Averaged over the databases and all classifiers, the improvement between Fgms and Fg is 25%. The Fg, Fm, and Fs features achieve comparable recognition rates. This is in accordance with the observations from Section 4, where it was observed that the fractions of the global features and strokebased mean and standard deviation features are similar. Considering the point at which maximum BIN performances are achieved (fraction f ) as listed in Table <ref type="table" target="#tab_3">4</ref>, no definite conclusions can be drawn. When averaging over the four datasets, f decreases from 0.69 (for Fg features) to 0.37 (for Fgms), but this is mostly due to the remarkable drop from 0.99 to 0.11 (for the UNIPEN symbols).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Comparison to other multi-stroke recognition techniques</head><p>Table <ref type="table" target="#tab_4">5</ref> depicts the classification results of the MLP and SVM classifiers, optimized on the c-30 and c-60 features. Furthermore, the results from the DTW classifier are presented, using the complete trainset as prototype set. For convenience to the reader, we have included the results on the Fgms features from Table <ref type="table" target="#tab_3">4</ref>.</p><p>For both the SVM and the MLP types of classifiers, significant improvements are observed between the Fc30 and the Fc60 coordinate features and the Fgms features. Error rates drop between 0% (for the UNIPEN capitals) and 80%. Averaged over the four databases and all classifiers, the improvement is 39%. Comparing the results of the SVM classifiers trained on the Fgms features to DTW, the improvement is 25%, averaged over all databases. It should be noted that the DTW classifier employs all training samples as prototypes for matching. Allograph matching techniques like DTW in most cases employ a significantly lower amount of prototypes, e.g. obtained through hierarchical clustering <ref type="bibr" target="#b23">[24]</ref>. This implies that the DTW classification results presented in Table <ref type="table" target="#tab_4">5</ref> should be considered as an upper bound.</p><p>The performance on the IRONOFF dataset is much lower than the performance reported for the other three databases. This effect is consistent for all classifiers, feature representations, and feature subsets. Our explanation for this effect is that the isolated capitals from the IRONOFF database contain only one character class sample per writer, which makes the IRONOFF recognition results per definition writer-independent. For the other databases, more data samples are available per writer.</p><p>Nevertheless, when comparing the obtained recognition rates to reported performances from the literature, our achieved performances on UNIPEN and IRONOFF capitals are competitive. It should be noted that comparisons to the literature are hard to make, since we have excluded single-stroked gestures from these data, which in general are easier to recognize. Furthermore, the ratio between the available amount of training data versus the amount of evaluation data and the distribution of samples over the different data subsets may differ from experiments reported elsewhere.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion and future research</head><p>Inspired by our research on pen-centric interactive map technologies, this paper focuses on the design and evaluation of feature sets and classifiers for multi-stroke pen gesture recognition. We have implemented and evaluated an elaborate set of 48 global features, compiled from various works from the literature. By computing these features on each constituent penup and pendown stroke along a gesture trajectory, additional mean and standard deviation features were devised. Through different normalizations on size and rotation, a large feature vector of 758 features was constructed. Feature selection using the BIN method was employed on features computed from four publicly available databases: the NicIcon collection of iconic pen gestures, the UNIPEN and IRONOFF uppercase characters, and a subset from the UNIPEN symbols category.</p><p>Several configurations of selected feature subsets were assessed on recognition accuracy, using different classifiers to compute recognition performances. The BIN feature selection method appeared to be very successful in selecting subsets of features. A particularly interesting conclusion to draw is that a major part of the selected features (about 2  3 ) comprise our new mean and standard deviation features. This implies that according to BIN, global features are equally important as and .</p><p>The impact on recognition performance is significant: the new features yield an improvement between 10% and 50% compared to the global features. Furthermore, compared to the DTW trajectorymatching technique and to local features computed at the level of coordinate sequences, improvements between 25% and 39%, averaged over all four databases, were achieved.</p><p>We are currently further developing and assessing our pen input recognition technologies in more elaborate experiments, involving pen input data acquired in more realistic situations. As the envisaged context is emergency service personnel, working in stressful circumstances, we are considering experimental conditions including drawing from memory, drawing under pressure of time, or drawing in multi-task settings. Finally, given the large impact of our new features on recognition performance, we hope to achieve similar improvements for other tasks, like Asian or Arabic character recognition. Our results may also translate to other application areas where mean and standard deviations of features computed from sub-segments may prove to be successful.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Examples of fourteen different iconic gestures, produced by the same participant contributing to the NicIcon data collection. Penup strokes are depicted in light gray.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>selected number of samples, average number of strokes (including both penup and pendown) and the classes containing the lowest and highest average number of strokes ( min , max ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 . 7 Fig. 4 .</head><label>374</label><figDesc>Fig. 3. Examples of g-48 features computed from the complete trajectory: area, centroid offset, and closure.</figDesc><graphic coords="3,318.14,354.00,237.60,141.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>Fig.6. Recognition scores on the Fgms features of SVMs trained with varying number of features ranked according to BIN. The maximum scores are indicated with starred dots and correspond to the highest recognition score, achieved with a limited number of features. As can be observed, incorporating additional features beyond these points does not improve performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Unit vectors (x-and y-axes) spanning R 2 e1 = (1, 0) and e2 = (0, 1) Pen trajectory with N sample points T = { 1 , 2 , ... , N } Sample i = {si, fi, ti} Position si = (xi, yi) Area of the convex hull A Angle between subsequent segments sn = arccos (sn -sn-1) • (sn+1 -sn) sn -sn-1 sn+1 -sn Length along the x-axis a = max1 i&lt;j N |xi -xj| Length along the y-axis b = max1 i&lt;j N |yi -yj| Center of the bounding box c = xmin + 1 2 {xmax -xmin}, ymin + 1 2 {ymax -ymin} Longestedge-length of the bounding box a = a if a &gt; b else a = b Shortest edge-length of the bounding box b</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Distribution of the 23,641 gestures over the fourteen icon classes.</figDesc><table><row><cell>Description</cell><cell>Icon</cell><cell># Samples</cell><cell>s</cell><cell>Description</cell><cell>Icon</cell><cell># Samples</cell><cell>s</cell></row><row><cell>Accident</cell><cell></cell><cell>1831</cell><cell>5.9</cell><cell>Gas</cell><cell></cell><cell>1870</cell><cell>5.1</cell></row><row><cell>Bomb</cell><cell></cell><cell>1667</cell><cell>3.4</cell><cell>Injury</cell><cell></cell><cell>1862</cell><cell>7.4</cell></row><row><cell>Car</cell><cell></cell><cell>1842</cell><cell>5.9</cell><cell>Paramedics</cell><cell></cell><cell>1867</cell><cell>5.6</cell></row><row><cell>Casualty</cell><cell></cell><cell>1863</cell><cell>4.8</cell><cell>Person</cell><cell></cell><cell>1868</cell><cell>7.5</cell></row><row><cell>Electricity</cell><cell></cell><cell>1358</cell><cell>3.1</cell><cell>Police</cell><cell></cell><cell>1864</cell><cell>4.4</cell></row><row><cell>Fire</cell><cell></cell><cell>182</cell><cell>3.3</cell><cell>Roadblock</cell><cell></cell><cell>1839</cell><cell>3.1</cell></row><row><cell>Fire brigade</cell><cell></cell><cell>1858</cell><cell>7.2</cell><cell>Flood</cell><cell></cell><cell>1870</cell><cell>3.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3</head><label>3</label><figDesc>Examples for which classification using only global features (Fg) yields a wrong result and classification with only stroke-level features yields the correct result.</figDesc><table><row><cell>Sample</cell><cell></cell><cell>fc ± (fc)</cell><cell>f</cell><cell>Offset</cell><cell>fc ± (fc)</cell><cell>f</cell><cell>Offset</cell></row><row><cell>a</cell><cell></cell><cell>Length</cell><cell></cell><cell></cell><cell>Number of crossings</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Fg</cell><cell>1.26 ± 0.87</cell><cell>8.42</cell><cell>8.27</cell><cell>0.32 ± 2.58</cell><cell>12.54</cell><cell>4.74</cell></row><row><cell>paramedics</cell><cell>Fm</cell><cell>0.45 ± 0.82</cell><cell>0.33</cell><cell>0.14</cell><cell>0.19 ± 2.34</cell><cell>0.64</cell><cell>0.19</cell></row><row><cell></cell><cell>Fs</cell><cell>1.11 ± 0.89</cell><cell>1.05</cell><cell>0.08</cell><cell>0.28 ± 2.67</cell><cell>1.88</cell><cell>0.60</cell></row><row><cell>b</cell><cell></cell><cell>Length</cell><cell></cell><cell></cell><cell>Closure</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Fg</cell><cell>0.20 ± 0.61</cell><cell>2.28</cell><cell>3.40</cell><cell>0.89 ± 0.53</cell><cell>-0.24</cell><cell>2.12</cell></row><row><cell>gas</cell><cell>Fm</cell><cell>-0.07 ± 0.44</cell><cell>0.30</cell><cell>0.85</cell><cell>0.92 ± 0.50</cell><cell>0.60</cell><cell>0.64</cell></row><row><cell></cell><cell>Fs</cell><cell>-1.15 ± 0.27</cell><cell>-1.12</cell><cell>0.11</cell><cell>-1.14 ± 0.36</cell><cell>-1.30</cell><cell>0.46</cell></row><row><cell>c</cell><cell></cell><cell>Initial angle (sine)</cell><cell></cell><cell></cell><cell>Area</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Fg</cell><cell>0.31 ± 0.91</cell><cell>-2.09</cell><cell>2.65</cell><cell>1.52 ± 0.76</cell><cell>0.15</cell><cell>1.81</cell></row><row><cell>paramedics</cell><cell>Fm</cell><cell>0.52 ± 0.73</cell><cell>0.97</cell><cell>0.62</cell><cell>0.89 ± 0.84</cell><cell>1.45</cell><cell>0.66</cell></row><row><cell></cell><cell>Fs</cell><cell>-0.13 ± 0.99</cell><cell>-0.22</cell><cell>0.08</cell><cell>1.34 ± 0.69</cell><cell>1.82</cell><cell>0.70</cell></row><row><cell>d</cell><cell></cell><cell>Average Curvature</cell><cell></cell><cell></cell><cell>Rectangularity</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Fg</cell><cell>-0.49 ± 0.66</cell><cell>0.21</cell><cell>1.06</cell><cell>-1.05 ± 0.62</cell><cell>0.32</cell><cell>2.20</cell></row><row><cell>firebrigade</cell><cell>Fm</cell><cell>0.34 ± 1.21</cell><cell>0.77</cell><cell>0.35</cell><cell>-0.04 ± 0.10</cell><cell>-0.04</cell><cell>0.05</cell></row><row><cell></cell><cell>Fs</cell><cell>0.57 ± 1.50</cell><cell>0.45</cell><cell>0.08</cell><cell>-0.04 ± 0.11</cell><cell>-0.04</cell><cell>0.05</cell></row><row><cell>e</cell><cell></cell><cell>Absolute Curvature</cell><cell></cell><cell></cell><cell cols="2">Standard deviation pressure</cell><cell></cell></row><row><cell></cell><cell>Fg</cell><cell>-0.77 ± 0.64</cell><cell>-0.12</cell><cell>1.01</cell><cell>0.36 ± 1.01</cell><cell>1.57</cell><cell>1.19</cell></row></table><note><p>firebrigade Fm -0.99 ± 0.49 -0.74 0.50 0.04 ± 1.04 -0.12 0.15 Fs 0.06 ± 0.72 0.60 0.74 0.55 ± 1.10 -0.06 0.55</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc>Classification results for each of the feature subsets, using SVM and MLP classifiers.</figDesc><table><row><cell>Group</cell><cell>Icons</cell><cell></cell><cell></cell><cell>IRONOFF</cell><cell></cell><cell></cell><cell>UpSym</cell><cell></cell><cell></cell><cell>UpCaps</cell><cell></cell><cell></cell></row><row><cell></cell><cell>MLP</cell><cell>SVM</cell><cell>f</cell><cell>MLP</cell><cell>SVM</cell><cell>f</cell><cell>MLP</cell><cell>SVM</cell><cell>f</cell><cell>MLP</cell><cell>SVM</cell><cell>f</cell></row><row><cell>Fg</cell><cell>97.3</cell><cell>98.7</cell><cell>0.47</cell><cell>88.3</cell><cell>90.8</cell><cell>0.61</cell><cell>94.8</cell><cell>95.4</cell><cell>0.99</cell><cell>94.4</cell><cell>95.5</cell><cell>0.70</cell></row><row><cell>Fm</cell><cell>97.0</cell><cell>98.2</cell><cell>0.62</cell><cell>88.1</cell><cell>89.3</cell><cell>0.42</cell><cell>94.4</cell><cell>95.2</cell><cell>0.64</cell><cell>91.9</cell><cell>93.8</cell><cell>0.27</cell></row><row><cell>Fs</cell><cell>96.4</cell><cell>97.7</cell><cell>0.55</cell><cell>88.5</cell><cell>89.0</cell><cell>0.26</cell><cell>93.2</cell><cell>94.4</cell><cell>0.69</cell><cell>92.1</cell><cell>93.5</cell><cell>0.47</cell></row><row><cell>Fgm</cell><cell>98.3</cell><cell>99.2</cell><cell>0.37</cell><cell>90.3</cell><cell>91.5</cell><cell>0.59</cell><cell>95.3</cell><cell>96.1</cell><cell>0.86</cell><cell>94.3</cell><cell>95.7</cell><cell>0.34</cell></row><row><cell>Fgs</cell><cell>97.9</cell><cell>99.1</cell><cell>0.35</cell><cell>91.3</cell><cell>92.8</cell><cell>0.55</cell><cell>94.9</cell><cell>96.3</cell><cell>0.58</cell><cell>94.2</cell><cell>96.4</cell><cell>0.55</cell></row><row><cell>Fgms</cell><cell>98.7</cell><cell>99.2</cell><cell>0.53</cell><cell>91.9</cell><cell>92.9</cell><cell>0.50</cell><cell>95.4</cell><cell>96.4</cell><cell>0.11</cell><cell>95.1</cell><cell>96.5</cell><cell>0.34</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5</head><label>5</label><figDesc>Performance comparison between the Fgms features, the coordinate features c-30 and c-60 and the DTW classifier.</figDesc><table><row><cell>Database</cell><cell>Fc30</cell><cell></cell><cell>Fc60</cell><cell></cell><cell>DTW</cell><cell>Fgms</cell><cell></cell></row><row><cell></cell><cell>MLP</cell><cell>SVM</cell><cell>MLP</cell><cell>SVM</cell><cell></cell><cell>MLP</cell><cell>SVM</cell></row><row><cell>Icons</cell><cell>96.2</cell><cell>97.0</cell><cell>95.9</cell><cell>96.8</cell><cell>98.5</cell><cell>98.7</cell><cell>99.2</cell></row><row><cell>IRONOFF</cell><cell>88.6</cell><cell>89.9</cell><cell>88.4</cell><cell>89.5</cell><cell>93.5</cell><cell>91.9</cell><cell>92.8</cell></row><row><cell>UpSym</cell><cell>92.6</cell><cell>93.3</cell><cell>93.1</cell><cell>94.1</cell><cell>94.0</cell><cell>95.4</cell><cell>96.4</cell></row><row><cell>UpCaps</cell><cell>94.3</cell><cell>95.4</cell><cell>95.1</cell><cell>95.6</cell><cell>95.5</cell><cell>95.1</cell><cell>96.4</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was supported by the Dutch Ministry of Economic Affairs, Grant no. BSIK03024 and the Netherlands Organization for Scientific Research (NWO), Project no. 634.000.434.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 7</head><p>The g-48 features. </p><p>In this appendix, the 48 g-48 features are described. These features comprise a selection from the features described in our technical report <ref type="bibr" target="#b32">[33]</ref>. The g-48 features contain purely global features computed from a complete gesture trajectory and are listed in Table <ref type="table">7</ref>. Some of the feature descriptions contained in Table <ref type="table">7</ref> use the notation and definitions specified in Table <ref type="table">6</ref>.</p><p>Included in Table <ref type="table">7</ref> are the equations used to calculate the features i used in our technical report. Since this technical report also contains some features computed along the running trajectory, like chaincode information, some feature indices extend over 48 (like 60 ). From each of the 48 plain features, various corresponding derived feature values were added. The column marked N i specifies how many features were derived from a feature i . N i is computed as follows:</p><p>where i , pud i , ns i , and nr i can be considered as booleans with values {0, 1}. These booleans indicate, respectively, whether (i) the stroke-based mean and standard deviations can be computed, (ii) whether pressure-based information regarding penup and pendown can be determined, (iii) whether i depends on size normalization or (iv) depends on rotation normalization. The parameter co i indicates how many coordinates are required to represent a feature. For example, for angular information co i is 2, represented by sin() and cos(). Certain features cannot be explained in one single equation. For these features, the reader is referred to <ref type="bibr" target="#b32">[33]</ref>.</p><p>About the author-DON WILLEMS received his M.Sc. degree in Cognitive Science in 2003 at the Radboud University in Nijmegen in The Netherlands. From 2000 to 2004 he was a scientific software developer at the Max Planck Institute for Psycholinguistics. Since April 2004 he has been working as a researcher and a Ph.D. student at the Nijmegen Institute for Cognition and Information, where he is involved in research on pen gesture recognition systems for crisis management applications. He joined the Agrotechnology and Food Sciences Group at Wageningen University in September 2008 where he is working on knowledge intensive processes in the agrifood domain. His research interests focus mainly on the application of techniques from artificial intelligence and pattern recognition on human computer interaction.</p><p>About the author-RALPH NIELS received his M.Sc. degree in Artificial Intelligence from the Radboud University Nijmegen, The Netherlands, in 2004. His master thesis was about the use of Dynamic Time Warping for intuitive handwriting recognition. After his graduation, he joined the cognitive artificial intelligence group of the Nijmegen Institute of Cognition and Information as a Ph.D. student. His thesis, which is planned for 2009, focuses on the use of allographic information for forensic writer identification. <ref type="bibr">VAN GERVEN received</ref>  About the author-LOUIS VUURPIJL received his Ph.D. in computer science in 1998 for research on neural networks and parallel processing. He has been involved in various forms of image processing and neural network-based image recognition such as the detection of ground-cover classes in satellite imagery. Louis Vuurpijl has been affiliated with the NICI since 1995, conduction research on pen computing, image retrieval, online handwriting recognition, forensic document analysis, and multimodal interaction. He is an assistant professor and lectures on artificial intelligence, robotics, and cognitive science. Louis Vuurpijl is member of the board of the international Unipen Foundation and is involved in several national and European research projects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>About the author-MARCEL</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Advances in Computer Graphics</title>
		<author>
			<persName><forename type="first">F</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ch. Research on User-Centered Design and Recognition of Pen Gestures</title>
		<imprint>
			<biblScope unit="volume">4035</biblScope>
			<biblScope unit="page" from="312" to="323" />
			<date type="published" when="2006">2006. 2006</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin/Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A visual communication language for crisis management</title>
		<author>
			<persName><forename type="first">S</forename><surname>Fitrianie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rothkrantz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Intelligent Control and Systems (Special Issue of Distributed Intelligent Systems)</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="208" to="216" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Designing interactive maps for crisis management</title>
		<author>
			<persName><forename type="first">D</forename><surname>Willems</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vuurpijl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth International Conference on Information Systems for Crisis Response and Management (ISCRAM2007)</title>
		<meeting>the Fourth International Conference on Information Systems for Crisis Response and Management (ISCRAM2007)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="159" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pen gestures in online map and photograph annotation tasks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Willems</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vuurpijl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Workshop on Frontiers in Handwriting Recognition (IWFHR06)</title>
		<meeting>the Tenth International Workshop on Frontiers in Handwriting Recognition (IWFHR06)<address><addrLine>La Baule, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="297" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The NicIcon collection of handwritten icons</title>
		<author>
			<persName><forename type="first">R</forename><surname>Niels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Willems</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vuurpijl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICFHR8, the Eleventh International Conference on Frontiers of Handwriting Recognition</title>
		<meeting><address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="296" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Specifying gestures by example</title>
		<author>
			<persName><forename type="first">D</forename><surname>Rubine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="329" to="337" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Quickset: multimodal interaction for distributed applications</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcgee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oviatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pittman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MULTIMEDIA &apos;97: Proceedings of the Fifth ACM International Conference on Multimedia, ACM</title>
		<meeting><address><addrLine>NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="31" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Interactive sketching for the early stages of user interface design</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Landay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Dannenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI95 Computer Human Interaction</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="43" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Igesture: a general gesture recognition framework</title>
		<author>
			<persName><forename type="first">B</forename><surname>Signer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Kurmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norrie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Document Analysis and Recognition (ICDAR2007)</title>
		<meeting>the Ninth International Conference on Document Analysis and Recognition (ICDAR2007)<address><addrLine>Curitiba, Brazil</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="954" to="958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A trainable gesture recognizer</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lipscomb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="895" to="907" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Experimental evaluation of an on-line scribble recognizer</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fonseca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jorge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1311" to="1319" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Find new meaning in your ink with tablet PC APIs in Windows Vista</title>
		<author>
			<persName><forename type="first">M</forename><surname>Egger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006-05">May 2006</date>
		</imprint>
		<respStmt>
			<orgName>Microsoft Corporation</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Satin: a toolkit for informal ink-based applications</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Landay</surname></persName>
		</author>
		<idno>UIST00</idno>
	</analytic>
	<monogr>
		<title level="m">Thirteenth Annual ACM Symposium on User Interface Software and Technology</title>
		<meeting><address><addrLine>San Diego, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="63" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Graffiti: Wow! Pen Computing Magazine</title>
		<author>
			<persName><forename type="first">C</forename><surname>Blickenstorfer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="30" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Richardson</surname></persName>
		</author>
		<title level="m">Touch-typing with a stylus, in: CHI &apos;93: Proceedings of the INTERACT &apos;93 and CHI &apos;93 Conference on Human Factors in Computing Systems</title>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="80" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An experimental comparison of machine learning for adaptive sketch recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Mathematics and Computation</title>
		<imprint>
			<biblScope unit="volume">185</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1138" to="1148" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">UNIPEN project of on-line data exchange and recognizer benchmarks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schomaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Plamondon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liberman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Janet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings ICPR&apos;94</title>
		<meeting>ICPR&apos;94</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="29" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The IRESTE on/off (IRONOFF) dual handwriting database</title>
		<author>
			<persName><forename type="first">C</forename><surname>Viard-Gaudin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Lallican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Binter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Knerr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Document Analysis and Recognition, ICDAR`99</title>
		<meeting>the International Conference on Document Analysis and Recognition, ICDAR`99<address><addrLine>Bangalore, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="455" to="458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">HMM-based on-line multi-stroke sketch recognition</title>
		<author>
			<persName><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-X</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Cybernetics</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005. 2005</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="4564" to="4570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sketch recognition in interspersed drawings using timebased graphical models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sezgin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Graphics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="500" to="510" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Design of a neural network character recognizer for a touch terminal</title>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Albrecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Le Cun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="105" to="119" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Experiments with adaptation strategies for a prototype-based recognition system for isolated handwritten characters</title>
		<author>
			<persName><forename type="first">V</forename><surname>Vuori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Oja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kangas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Document Analysis and Recognition</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="150" to="159" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Feature selection and extraction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Webb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Statistical Pattern Recognition</title>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="305" to="359" />
		</imprint>
	</monogr>
	<note>second ed.</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Finding structure in diversity: a hierarchical clustering method for the categorization of allographs in handwriting</title>
		<author>
			<persName><forename type="first">L</forename><surname>Vuurpijl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schomaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICDAR4</title>
		<meeting>ICDAR4</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="387" to="393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Automatic allograph matching in forensic writer identification</title>
		<author>
			<persName><forename type="first">R</forename><surname>Niels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vuurpijl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schomaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Pattern Recognition and Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="81" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Symbology Reference, Version 2.20</title>
		<ptr target="http://www.fgdc.gov/HSWG" />
		<imprint>
			<date type="published" when="2005-09-14">September 14, 2005</date>
		</imprint>
		<respStmt>
			<orgName>Homeland Security Working Group</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A full English sentence database for off-line handwriting recognition</title>
		<author>
			<persName><forename type="first">U</forename><surname>Marti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bunke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth International Conference on Document Analysis and Recognition (ICDAR&apos;99)</title>
		<meeting>the Fifth International Conference on Document Analysis and Recognition (ICDAR&apos;99)<address><addrLine>Bangalore, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="705" to="708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Two on-line Japanese character databases in Unipen format</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jaeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nakagawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Document Analysis and Recognition</title>
		<meeting>the International Conference on Document Analysis and Recognition</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="566" to="570" />
		</imprint>
	</monogr>
	<note>ICDAR`01</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Collection of on-line handwritten Japanese character pattern databases and their analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nakagawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Document Analysis and Recognition</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="69" to="81" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Experiences in collection of handwriting data for online handwriting recognition in Indic scripts</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Bhaskarabhatla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Madhvanath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth International Conference on Linguistic Resources and Evaluation (LREC)</title>
		<meeting>the Fourth International Conference on Linguistic Resources and Evaluation (LREC)</meeting>
		<imprint>
			<publisher>CDROM</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Handwritten character databases of Indic scripts</title>
		<author>
			<persName><forename type="first">U</forename><surname>Bhattacharya</surname></persName>
		</author>
		<ptr target="http://www.isical.ac.in/∼ujjwal/download/database.html" />
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Ifn/enit-database of handwritten Arabic words</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pechwitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maddouri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Märgner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ellouze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Amiri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Seventh Colloque International Francophone sur l&apos;Ecrit et le Document (CIFED02)</title>
		<meeting><address><addrLine>Hammamet, Tunis</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Definitions for features used in online pen gesture recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Willems</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Niels</surname></persName>
		</author>
		<ptr target="http://unipen.nici.ru.nl/NicIcon/" />
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
		<respStmt>
			<orgName>NICI, Radboud University Nijmegen</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Comparison of combined shape descriptors for irregular objects</title>
		<author>
			<persName><forename type="first">J</forename><surname>Iivarinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Peura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Särelä</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Visa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eighth British Machine Vision Conference, BMVC&apos;97</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Clark</surname></persName>
		</editor>
		<meeting><address><addrLine>Essex, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="430" to="439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Efficiency of simple shape descriptors</title>
		<author>
			<persName><forename type="first">M</forename><surname>Peura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Iivarinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Visual Form Analysis</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Arcelli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Cordella</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Sanniti</surname></persName>
		</editor>
		<editor>
			<persName><surname>Di Baja</surname></persName>
		</editor>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>World Scientific</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="443" to="451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A practical approach for writer-dependent symbol recognition using a writer-independent symbol recognizer</title>
		<author>
			<persName><forename type="first">J</forename><surname>Laviola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J R</forename><surname>Zeleznik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1917" to="1926" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The estimation of the gradient of a density function, with applications in pattern recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Fukunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hostetler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="32" to="40" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Statistical pattern recognition: a review</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Duin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="37" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Evaluating feature selection methods for learning in data mining applications</title>
		<author>
			<persName><forename type="first">S</forename><surname>Piramuthu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Journal of Operational Research</title>
		<imprint>
			<biblScope unit="volume">156</biblScope>
			<biblScope unit="page" from="483" to="494" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<title level="m">The Nature of Statistical Learning Theory</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">LIBSVM: a library for support vector machines</title>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="www.csie.ntu.edu.tw/∼cjlin/libsvm" />
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
