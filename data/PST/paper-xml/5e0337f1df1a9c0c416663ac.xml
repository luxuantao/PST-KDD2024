<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Local Semantic Siamese Networks for Fast Tracking</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhiyuan</forename><surname>Liang</surname></persName>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
						</author>
						<title level="a" type="main">Local Semantic Siamese Networks for Fast Tracking</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8B02BF08FE8BC8AC28E0C7A0CBCD1C49</idno>
					<idno type="DOI">10.1109/TIP.2019.2959256</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2019.2959256, IEEE Transactions on Image Processing IEEE TRANSACTIONS ON IMAGE PROCESSING 1 This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2019.2959256, IEEE Transactions on Image Processing IEEE TRANSACTIONS ON IMAGE PROCESSING 2 This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2019.2959256, IEEE Transactions on Image Processing</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T17:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Visual object tracking</term>
					<term>Siamese deep network</term>
					<term>Local feature representation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning a powerful feature representation is critical for constructing a robust Siamese tracker. However, most existing Siamese trackers learn the global appearance features of the entire object, which usually suffers from drift problems caused by partial occlusion or non-rigid appearance deformation. In this paper, we propose a new Local Semantic Siamese (LSSiam) network to extract more robust features for solving these drift problems, since the local semantic features contain more fine-grained and partial information. We learn the semantic features during offline training by adding a classification branch into the classical Siamese framework. To further enhance the representation of features, we design a generally focal logistic loss to mine the hard negative samples. During the online tracking, we remove the classification branch and propose an efficient template updating strategy to avoid aggressive computing load. Thus, the proposed tracker can run at a high-speed of 100 Frameper-Second (FPS) far beyond real-time requirement. Extensive experiments on popular benchmarks demonstrate the proposed LSSiam tracker achieves the state-of-the-art performance with a high-speed. Our source code is available at https://github.com/ shenjianbing/LSSiam.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>As one of the most challenging tasks in computer vision, visual object tracking has received increasing attention in the last decades. It is also the foundation of many complex vision topics, such as multiple object tracking <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b24">[25]</ref> and video segmentation <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b51">[52]</ref>. Recently, a lot of deep learning algorithms have been applied to object tracking.</p><p>There are two major tracking strategies in deep tracking methods. One is based on tracking-by-detection and the other is based on template matching. The tracking-by-detection methods regard visual object tracking as a classification task. The classifier is trained to distinguish the target from background, and then updated according to the estimation results of previous frames. This kind of trackers contains some correlation filter trackers with deep features like C-COT <ref type="bibr" target="#b9">[10]</ref>, ECO <ref type="bibr" target="#b7">[8]</ref>, CREST <ref type="bibr" target="#b38">[39]</ref>, TRACA <ref type="bibr" target="#b5">[6]</ref> and some trackers based on deep networks such as MDNet <ref type="bibr" target="#b34">[35]</ref>. However, the online updating strategy is time-consuming and the classifier is prone to overfit the results of recent frames. The other template matching based methods extract the target template and select the most similar candidate patch at the current frame by metric learning. The representative tracking methods are a series of Siamese trackers <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b50">[51]</ref>,</p><p>This work was supported in part by the Beijing Natural Science Foundation under Grant 4182056. Specialized Fund for Joint Building Program of Beijing Municipal Education Commission.</p><p>Z. Liang and J. Shen are with Beijing Laboratory of Intelligent Information Technology, School of Computer Science, Beijing Institute of Technology, Beijing 100081, P. R. China. (Email: shenjianbingcg@gmail.com)</p><p>Corresponding author: Jianbing Shen</p><p>Ground-truth SiamFC SiamRPN Ours Fig. <ref type="figure">1</ref>. Visualization of the tracking results compared with SiamFC <ref type="bibr" target="#b2">[3]</ref> and SiamPRN <ref type="bibr" target="#b28">[29]</ref>. From top to bottom rows, the sequences are Bolt2, CarDark, and Box in OTB-100 dataset, respectively. Our tracker can distinguish the target in non-rigid deformation, background cluttering and partial occlusion scenarios successfully.</p><p>[54], <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b58">[59]</ref>, which utilize cross-correlation operation for computational efficiency. For instances, SiamFC <ref type="bibr" target="#b2">[3]</ref> constructs a fully-convolutional network learned on a large-scale dataset during the offline training, and tracks the target with template patch of the first frame on online tracking. To improve the discrimination of deep features, DSiam <ref type="bibr" target="#b17">[18]</ref> introduces two types of correlation filters to online update target features and alleviate background cluttering respectively. SA-Siam <ref type="bibr" target="#b18">[19]</ref> proposes two complementary Siamese networks to extract both global appearance and semantic features. FlowTrack <ref type="bibr" target="#b58">[59]</ref> merges multi-frame features of the target in a temporal domain via optical flow. RASNet <ref type="bibr" target="#b45">[46]</ref> adds three kinds of attention modules upon appearance features of the target to set weights for cross-correlation operation. Thus, the feature embedding and similarity measurement are decoupled to alleviate overfitting. However, all of previous methods pay more attention to global features but ignore the local or semantic information of targets during offline training, which limits the ability of network to learn more robust features.</p><p>Local semantic features contain both high-level category and partial fine-grained information of objects. Compared with global features extracted by other Siamese trackers, they are more effective for challenging situations such as non-rigid ob-ject deformation, background cluttering and partial occlusion. As shown in Fig. <ref type="figure">1</ref>, the targets are under heavy deformation (Bolt2), background cluttering (CarDark), and partial occlusion (Box). Both classical SiamFC <ref type="bibr" target="#b2">[3]</ref> and recent SiamRPN <ref type="bibr" target="#b28">[29]</ref> trackers based on global features suffer from the drift problems under these challenging situations. In contrast, the proposed LSSiam tracker can track the target successfully. There are two major reasons for obtaining better tracking performance. Firstly, the local semantic features contain more fine-grained semantic information thus they are discriminative to non-rigid objects and background area. Secondly, when partial occlusion happens, semantic parts which are not occluded can help recognize and locate the objects. To introduce the local semantic feature into visual object tracking, we apply a group of 1×1 convolutional filters to detect informative parts of objects. Besides, these filters are trained with an additional category supervision. Thus both local fine-grained and highlevel semantic information are learned during training.</p><p>In this paper, we propose a novel Local Semantic Siamese (LSSiam) framework to learn more robust features for fast object tracking by training an end-to-end feature extraction network. The proposed LSSiam framework is composed of three components. Firstly, the local semantic networks are trained to encode the high-order statistics by a classification branch. Secondly, a residual channel attention block is applied to select useful semantic parts for tracking. Thirdly, to further enhance the discriminant power, a focal logistic loss is designed to reweight the importance of negative samples during the training phase. After offline training, we remove the classification branch to reduce the computation cost during online tracking. The local semantic networks are more lightweight compared with the backbone networks of other Siamese trackers, and the classification branch does not require additional computation during online tracking. Besides, an effective online template updating strategy is proposed to handle long-term object deformation. Unlike other online updating methods <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b50">[51]</ref>, our tracker just extracts and updates the target templates at key-frames adaptively to reduce the computation cost.</p><p>The main contribution is summarized as follows:</p><p>• A novel LSSiam framework is proposed to learn more robust feature representation. The local semantic features require smaller network and are learned via an auxiliary classification branch, which is removed during tracking. • A generally focal logistic loss and an efficient online template updating strategy are proposed to further improve performance for offline training and online tracking, respectively. The offline one will enhance metric learning by hard negative sample mining, and the online one tackles the object deformation and keeps high speed. • Our LSSiam tracker achieves a high-speed of 100 FPS with favorable tracking accuracy compared with the stateof-the-art trackers on popular benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Siamese Network Based Trackers</head><p>Siamese network based trackers <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b58">[59]</ref> follow template matching strategy. These trackers take a pair of image patches as input and output the similarity score map by crosscorelation operation. One input patch is the target template and the other is the search area, which is cropped from original image at the centre location of target prediction in the previous frame. Then target prediction of current frame is obtained through largest value of the output score map. A remarkable advantage of Siamese trackers is that they require no or little online training, so they are suitable for real-time vision tasks.</p><p>SiamFC <ref type="bibr" target="#b2">[3]</ref> trains a network on the VID dataset of ILSVRC <ref type="bibr" target="#b37">[38]</ref>, where two branches have the same parameters. CFNet <ref type="bibr" target="#b41">[42]</ref> expands SiamFC by a differentiable correlation filter layer and updates the target template on tracking step. Inspired by CFNet, FlowTrack <ref type="bibr" target="#b58">[59]</ref> proposes to online update the target template with optical flow. It achieves higher accuracy score on popular benchmarks but performs tracking slower than CFNet. DSiam <ref type="bibr" target="#b17">[18]</ref> updates the features of target and search region with an aggressive updating strategy. The parameters of two correlation filter layers are updated based on the prediction of the previous frame. MemTrack <ref type="bibr" target="#b50">[51]</ref> develops an attentional ConvLSTM network to update the template. The network is trained in an end-to-end manner during offline training and the features of target are updated during online tracking. These trackers obtain better performance in terms of object deformation compared with the trackers without online updating. However, they are not suitable for fast tracking situation. For example, FlowTrack performs tracking at 12 FPS, and the speed of DSiam is 22 FPS.</p><p>To maintain the high-speed of Siamese network, some methods discard online updating, and turn to learn a robust feature representation instead. SA-Siam <ref type="bibr" target="#b18">[19]</ref> introduces a twofold Siamese network and multi-layer features to improve tracking performance. Both appearance and semantic features are extracted to provide complementary information for inferring. RASNet <ref type="bibr" target="#b45">[46]</ref> uses multiple attention module for metric learning. The cross-correlation operation is weighted by these attention values from different feature dimensions to prevent overfitting. SiamPRN <ref type="bibr" target="#b28">[29]</ref> introduces the region proposal network to avoid the time-consuming multi-scale estimation step, and DaSiamRPN <ref type="bibr" target="#b57">[58]</ref> expands it by learning a distractor-aware feature embedding. These methods present the target from a global perspective, while the partial information of the target is ignored. StructSiam <ref type="bibr" target="#b53">[54]</ref> proposes to detect local pattens of the target, StructSiam extracts the structured appearance features and learns the relationship among them to infer the appearance deformation of the target, while the proposed LSSiam extracts local semantic features for tracking. Compared with structured appearance features, the local semantic features contain both high-level category information and mid-level partial information, which are more robust to challenging situations such as partial occlusion <ref type="bibr" target="#b6">[7]</ref> and nonrigid appearance deformation. Furthermore, we propose a new online updating strategy to adapt object deformation. Unlike previous online updating approaches, we just update the target template at key-frames for fast visual tracking. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Fine-grained Image Recognition</head><p>Fine-grained image recognition aims to distinguish subcategories of the same super-category, such as cars, birds, and aircrafts. Some bottom-up approaches first detect the semantic parts and then classify the object into subcategories <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b55">[56]</ref>. Other top-down approaches are based on the end-to-end encoding networks <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b46">[47]</ref>. These approaches enhanced the CNN mid-level feature representation by encoding the higher-order statistics of convolutional feature maps. Wang et al. <ref type="bibr" target="#b46">[47]</ref> learned discriminative mid-level features for fine-grained recognition without extra bounding box annotations. Inspired by their method, we propose to train local semantic Siamese network for visual tracking. An auxiliary classification task branch is introduced during training to encode the high-level semantic information and is removed during tracking to maintain the high speed. Our method takes full advantage of semantic annotations (bounding boxes and class labels) of training data to enhance feature representation. Different from <ref type="bibr" target="#b46">[47]</ref>, our LSSiam network learns local semantic detectors with class labels rather than sub-class labels. Our LSSiam abandons global features since they are sensitive to partial occlusion. The local semantic features are categoryaware and contain more fine-grained part information. Moreover, we take advantage of focal logistic loss to mine the hard negative samples during training. The discrimination of local features is further improved by paying more attention to the hard negative samples, such as intra-class distractors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Local Feature Representation</head><p>Local feature representation has been applied in many vision tasks such as person re-identification <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b54">[55]</ref>, object detection <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b56">[57]</ref>, image classification <ref type="bibr" target="#b3">[4]</ref>. In compare to global features, local features contain part information of objects. Sun et al. <ref type="bibr" target="#b40">[41]</ref> proposed a strong person re-identification baseline based on part features. A refined part pooling method is further introduced to conduct soft partition of human body. Thus, the human parts from different images are aligned and finegrained information is extracted. Chen et al. <ref type="bibr" target="#b4">[5]</ref> associated the image and language information in both global and local views to tackle person re-identification. The network is enhanced by global image-sentence classification and local part-phrase matching. For object detection, CoupleNet <ref type="bibr" target="#b56">[57]</ref> combined the global and local features of proposals. The complementary information promotes to detect different scales of objects. BagNet <ref type="bibr" target="#b3">[4]</ref> proposed a deep bag-of-feature model for local features to achieve the high performance of classification results. In this paper, we train a novel Siamese network to extract local semantic features for visual tracking. By adding an auxiliary classification task branch into the traditional Siamese network, high-level semantic statistics are encoded into local features. It improves the robust of our tracker and reduces the parameters of network architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. OUR APPROACH</head><p>Our tracking framework consists of three main steps. The proposed Local Semantic Siamese network (Section III-A) is offline trained at the first step, then it is fine-tuned by the focal logistic loss (Section III-B) at the second step. At the last step, our tracker performs tracking with the proposed online template updating strategy (Section III-C).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Local Semantic Siamese Network</head><p>Local semantic feature contains partial and semantic information of objects. It is implemented by adding local detectors and category supervision into the traditional Siamese network. As shown in Fig. <ref type="figure" target="#fig_0">2</ref>, the proposed LSSiam network consists of 4 blocks, including the mid-level feature block, the local feature block, the residual channel attention block and the filter supervision module. There are two branches in LSSiam network. The exemplar branch encodes the feature of target and the instance branch encodes the feature of search area. For example, the instance branch takes the search image patch x of the current frame as input and outputs the mid-level appearance features ϕ ma (x) by the mid-level feature block. Then, the local feature block takes ϕ ma (x) as input and outputs the corresponding local semantic features ϕ ls (ϕ ma (x)).</p><p>The exemplar branch has the same architecture of the instance branch but an extra residual channel attention block g(•) is introduced to adaptively select useful semantic parts of target template z. The filter supervision module is applied during training phase to force the local feature block to detect local semantic part of input image patches and is removed during tracking phase. The response map f (z, x) is generated by a cross-correlation layer:</p><formula xml:id="formula_0">f (z, x) = g(ϕ ls (ϕ ma (z))) * ϕ ls (ϕ ma (x)) + b1 (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where * is the cross-correlation operation and b ∈ R denotes the bias. Since the value of the response map corresponds to the confidence to be the target, the target is predicted to be centred at the maximum value location. We modify a pretrained AlexNet <ref type="bibr" target="#b27">[28]</ref> as the mid-level feature block. Since local semantic features require a relatively small receptive field for detecting local parts of objects, the only first 4 convolutional layers of AlexNet are preserved. Given a pair of input image patches with dimension of 127×127×3 and 255×255×3, the resolution of output response map f (z, x) is 17×17. Finally, the response map is upsampled by bicubic interpolation to 272×272 for higher location accuracy. The local feature block consists of two 1×1 convolutional layers. The first 1×1 convolutional layer translates global features into local ones. A batch normalization layer and a ReLU layer are followed. Inspired by <ref type="bibr" target="#b45">[46]</ref>, we regard each 1×1 filter as a type of local semantic part detector for a specific class. These filters are divided into different groups and are trained in an end-to-end fashion with patch-level category annotations through the filter supervision module. The second 1×1 convolutional layer is an embedding layer. It makes the local semantic features suitable for cross-correlation operation, and also reduces the channel dimension of input features. As shown in Fig. <ref type="figure" target="#fig_1">3</ref>, the filter supervision module promotes the local feature block to learn a group of 1×1 filters for every class, and it is removed during tracking. Suppose there are M classes of training data and each class has k types of local semantic detectors, then a total kM 1×1 filters are required. In Fig. <ref type="figure" target="#fig_1">3</ref>, the dimension of input features is H×W ×kM , and each k dimensions correspond to a specific class. To guide the filters to extract local semantic features, a Global Max Pooling (GMP) layer is used to simply detect the most informative parts of objects. After the GMP layer, a kM -dimensional vector is generated, where every k dimension of the vector is related to one semantic class. Then, the Cross-Channel Pooling (CCP) layer <ref type="bibr" target="#b45">[46]</ref> divides the feature vector into M groups and averages the values across every k dimensions, since each type of local semantic features is considered to be equally important. The pooled M -dimensional vector is finally fed into a classification loss layer, and the category-aware local semantic detectors are trained in an end-to-end manner. We choose M -way softmax log loss as the classification loss:</p><formula xml:id="formula_2">L cls (y, V ) = -log e V (y) M i=1 e V (i)<label>(2)</label></formula><p>where y ∈ {1, • • • , M } is the ground-truth label. V is a Mdimensional vector and V (i) is the prediction score of the class i. The filter supervision module consists of two pooling layers and a loss layer, so no learnable parameters are required. The parameters of local semantic filters are directly adjusted by the loss function. Fig. <ref type="figure" target="#fig_2">4</ref> shows the examples of feature maps extracted by the local feature block. The first row is the input patches cropped from OTB-100 benchmark. The second and third rows are the representative feature maps related to the targets. By introducing additional category supervision, the local feature block could capture partial information of objects.</p><p>For "human" category, features of the 174th and the 67th channel are related to the head and the feet part respectively. For "car" category, features of the 38th and the 42th channel are related to the chassis and the car light part, respectively. The features extracted by the local feature block are more discriminative to classes that have appeared in the training data. However, the class of the target can differ from the training data. Some of local semantic detectors may be not discriminative to a new target, which reduces the robustness of semantic features. We address this problem by adding a residual channel attention block g(•) into the exemplar branch. This block selects useful semantic features of the target by setting soft weights across channels. Fig. <ref type="figure" target="#fig_3">5</ref> shows more details about the residual channel attention block. Inspired by SENet <ref type="bibr" target="#b20">[21]</ref>, the residual channel attention block is composed of a global average pooling layer, a dimension reduction layer with reduction ration 4, a ReLU layer, a dimension increasing layer and a sigmoid activation layer. Here, channel attention may lose some useful information about the target when its value is close to zero, we combine original features and weighted features in a residual form:</p><formula xml:id="formula_3">g(ϕ(z)) = ϕ(z) + ξ ϕ(z) (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>where z is the target template and ϕ ls (ϕ ma (z)) is denoted as ϕ(z) for simplicity. ξ is the normalized channel weights. and + are channel-wise multiplication and element-wise summation, respectively. The residual channel attention block is lightweight and only conducted at key-frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Focal Logistic Loss for Siamese Tracker</head><p>SiamFC <ref type="bibr" target="#b2">[3]</ref> employs a discriminative method to locate target in the search area. The network is trained on the positive and negative pairs by the logistic loss:</p><formula xml:id="formula_5">L tra (y, v) = log(1 + e -yv )<label>(4)</label></formula><p>v ∈ R is the prediction score and y ∈ {+1, -1} is the groundtruth label, which is set based on the distance from the target centre to the sample centre. The mini-batch loss is computed by weighted averaging the individual losses of all samples. SiamFC uses a weighting function to assign a training weight for the sample u:</p><formula xml:id="formula_6">ω[u] = 1 2|Dp| , u ∈ D p 1 2|Dn| , u ∈ D n<label>(5)</label></formula><p>where D p and D n are sets of the positive and negative samples, respectively.</p><formula xml:id="formula_7">ω[u + ] = ω[u -] = 1 2 .</formula><p>Observing that there are some noise and easy negative samples in D n , SiamFC treats all the negative samples equally. Fig. <ref type="figure" target="#fig_4">6</ref> gives an example of training samples. The objects marked by yellow bounding boxes in the first row are the targets. The padding region marked by the red bounding box in the second row can be viewed as noise, and the background region dissimilar to the target in the first row can be viewed as easy negative samples. The intra-class distractors marked by green bounding boxes have similar appearance to targets, so they are more important for training. The large amount of noise and easy negative samples prevent the network from learning a more discriminative feature representation. It is necessary to mine the hard negative samples in training phase.</p><p>Inspired by focal loss <ref type="bibr" target="#b29">[30]</ref>, we add a modulating term into the tracking loss and propose a focal logistic loss to mine the hard negative samples. Focal loss classifies training data into 4 categories, that are hard positive samples, hard negative samples, easy positive samples and easy negative samples. It mines both hard positive samples and hard negative samples to improve the performance of baseline model. However, directly applying focal loss into Siamese trackers cannot improve even reduce performance. SiamFC considers an image patch as a positive sample if it is within the radius threshold of the centre, thus the focal loss will regard the patch located at the centre of the target as the easiest positive sample and assign the lowest importance weight to it, which causes a drift problem during tracking phase. To address it, we propose to design an imbalanced modulating term on positive and negative samples.</p><formula xml:id="formula_8">L tra (y, v) = 1 - 1 1 + e -yv γ(1-y) log(1 + e -yv ) =    log(1 + e -yv ) y = +1 e -yv 1+e -yv 2γ log(1 + e -yv ) y = -1 (6)</formula><p>where γ is a constant. For negative samples with groundtruth label -1, the focal logistic loss assigns a relatively small weight for noise and easy samples, and it maintains a large weight for hard ones. For positive samples with ground-truth label +1, the focal logistic loss is degenerated into the logistic loss. The total loss is a combination of the tracking loss L tra and the classification loss L cls L = L tra + λL cls <ref type="bibr" target="#b6">(7)</ref> where λ is a balancing weight based on the magnitude of L tra and L cls . The Siamese network is trained in an end-toend fashion. The local semantic detectors are learned with the extra supervision of category information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Online Template Updating</head><p>Traditional Siamese trackers take the target patch cropped from the first frame as exemplar, and select the most similar instance patch in a search window. The tracking failure occurs when the target appearance deforms heavily. To tackle this problem, some correlation filter trackers update the filters online, such as C-COT <ref type="bibr" target="#b9">[10]</ref>, ECO <ref type="bibr" target="#b7">[8]</ref>, CREST <ref type="bibr" target="#b38">[39]</ref> and DSiam <ref type="bibr" target="#b17">[18]</ref>. However, online updating the parameters of Siamese network is time-consuming, and the tracker is prone to overfit to results of previous frames for the lack of training samples. We propose an effective strategy to adapt the appearance deformation. During the tracking phase, the parameters of the network are fixed. Instead of fine-tuning the network, we update the template features at key-frames.</p><p>The proposed LSSiam tracker extracts the target features of the first frame ϕ(z 0 ) and starts tracking. In every tracking step, a new target snapshot is generated according to the predicted response map of the current frame. The complementary appearance information is maintained by selecting a set of useful template snapshots S = {z 1 , • • • , z i , • • • , z n } of the target from previous key-frames. Suppose z i is a selected snapshot, and the number of snapshots in S is n. The target template is updated by S in the feature space. To reduce the influence of background region in snapshots, we first compute a spatial attention based on the similarity between the target template z 0 and the snapshot z i as follow:</p><formula xml:id="formula_9">a p = N exp(ϕ(z 0 ) T p ϕ(z i ) p ) N k=1 exp(ϕ(z 0 ) T k ϕ(z i ) k )<label>(8)</label></formula><p>where a p is attention weight at the spatial location p. N is the total number of spatial locations in feature space and a p = N . Then, z i is aligned with z 0 based on the spatial attention:</p><formula xml:id="formula_10">ϕ( ẑi ) p = a p ϕ(z i ) p<label>(9)</label></formula><p>where ϕ(z i ) p and ϕ( ẑi ) p are the original and aligned features of z i at spatial location p, respectively.</p><p>When a new frame comes, features extracted from the first frame and the snapshot set S are combined to adapt target deformation:</p><formula xml:id="formula_11">ϕ(z) = αϕ(z 0 ) + 1 -α n n i=1 ϕ( ẑi )<label>(10)</label></formula><p>where the influence factor α balances weights between target template at the initial frame and template snapshots in S. The aggressive template updating strategy needs more computational cost and tends to overfit recent frames, so we only select the snapshots and update the target template at key-frames. Extract instance features ϕ(x t ) centred at p t-1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>Compute the response map V t with Eq. 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8:</head><p>Estimate the location p t and the scale s t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>9:</head><p>Compute confidence score Conf (V t ) with Eq. 11.</p><p>10:</p><formula xml:id="formula_12">cnt = cnt + 1. 11: if cnt ≥ T inter ∧ Conf (V t ) ≥ T conf then 12:</formula><p>if |S| reaches the upper bound then 13: delete the oldest snapshot in S. update ϕ(z) with Eq. 9 and Eq. 10.</p><p>18: cnt = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>19:</head><p>end if 20: until end of the sequence In order to ensure the variety of snapshots, we set a relatively large time interval to select snapshots. To ensure the accuracy of snapshots, the confidence score of snapshots being the target is considered. We take the average peakto-correlation energy (APCE) proposed in LMCF <ref type="bibr" target="#b44">[45]</ref> as the confidence score, because it reflects the confidence of prediction being the target by the fluctuation of the response map. The confidence score of response map V is defined as:</p><formula xml:id="formula_13">Conf (V ) = |V max -V min | 2 mean w,h (V w,h -V min ) 2<label>(11)</label></formula><p>where V max and V min are the maximum and minimum value of V , respectively. V w,h is the response value located at (w, h). When the response map has fewer noise and only one shaper peak, i.e., the target is appearing in the search area, the confidence score will become higher. Otherwise, the confidence score will drop a lot. Only the snapshot with the confidence score and the frame interval are larger than their thresholds can be selected. The oldest snapshot of S is discarded when the |S| reaches its upper bound. An example of the template updating process is depicted in Fig. <ref type="figure" target="#fig_5">7</ref>. The horizontal dashed line in red color is the threshold of confidence score. The target template of frame 120 is selected at the beginning and the frame interval is re-counted. When a new frame comes, the confidence score is computed according to the response map. The template in frame 172 is selected because both the confidence score and the frame interval is larger than the thresholds. At frame 213, the frame interval is large enough while the confidence score is lower than the threshold due to occlusion. The new coming templates have not been updated until it reaches the 328th frame.</p><p>The whole process of tracking with proposed online template strategy is summarized in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS</head><p>We perform experimental evaluations on two popular benchmarks to validate our approach. Section IV-A shows the implementation details of network and parameters. In section IV-B, the benchmark datasets and evaluation protocols are described. The experimental results on Object Tracking Benchmark (OTB) and Visual Object Tracking (VOT) benchmarks are demonstrated in Section IV-C and Section IV-D, respectively. Finally, we perform the ablation analysis in Section IV-E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation details</head><p>Network Architecture: The mid-level feature block is implemented by a modified AlexNet <ref type="bibr" target="#b27">[28]</ref> pre-trained on Im-ageNet dataset <ref type="bibr" target="#b10">[11]</ref>. To be consistent with SiamFC network <ref type="bibr" target="#b2">[3]</ref>, the stride of the first convolutional layer is modified from 4 to 2, and the zero padding of each layer is removed. The rest parameters of the proposed LSSiam network are randomly initialized with Gauss distribution. The fc layers in the residual channel attention block are implemented by 1×1 convolution layers, and the dimension reduction ratio is 4. The size of exemplar image patch z is 127×127×3, and the size of the instance image patch x is 255×255×3. The resolution of the response map generated by the network is 17×17.</p><p>Training: Our method is implemented with MatConvNet toolkit <ref type="bibr" target="#b42">[43]</ref>. The network is trained on the VID dataset of ILSVRC <ref type="bibr" target="#b37">[38]</ref>, which consists of almost 4, 500 videos with 30 classes of samples. These classes include animals and vehicles and all of them are used for training. The number of local semantic detectors k for each class is 20, so a total of 600 1×1 filters are required. The kernel size of the second convolutional layer in the local feature block is 1×1×600×256. The constant γ is set to 1 in Eq. 6. The balance factor λ in Eq. 7 is set to 1×10 -4 based on the magnitude of tracking loss and classification loss. We take logistic loss as the tracking loss and train the network over 50 epoches with the learning rate from 10 -2 to 10 -5 . Then, the logistic loss is replaced by focal logistic loss, and the network is fine-tuned over 10 epoches with the learning rate from 10 -4 to 10 -5 . Each epoch consists of 53, 200 sampled pairs. The SGD optimization is used in both training and fine-tuning steps with a mini-batch of 32.</p><p>Tracking: The tracking hyperparameters are the same for all the benchmarks. For scale estimation, the search regions are cropped on 3 scales, i.e. 1.041 {-1,0,1} . The scale penalty is 0.9745. The linear interpolation factor for scale updating is 0.56. For target location, the upsampling factor of the response map is 16. The weight of cosine window is 0.18 to dampen the response map. For online template updating, the upper bound number of the snapshots in S is 3. The template updating ratio α in Eq. 10 is 0.2. The confidence score threshold is set to 12. The frame interval threshold is set to 40 frames. To adapt the horizontal flip of the target, the horizontally flipped template of initial frame z 0 is added to the snapshot set S at the beginning. Our machine is equipped with a single NVIDIA GeForce 1080 Ti and an Intel Core i7-6800K at 3.4 GHz, and our software platform is Matlab 2016b + CUDA 8.0 + cudnn. Our method runs at a high-speed of 100 Frames-Per-Second.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Datasets and metrics</head><p>The proposed LSSiam tracker is evaluated with recent stateof-the-art trackers in popular benchmarks including OTB-2013 <ref type="bibr" target="#b48">[49]</ref>, OTB-50 <ref type="bibr" target="#b49">[50]</ref>, OTB-100 <ref type="bibr" target="#b49">[50]</ref>, VOT-2016 <ref type="bibr" target="#b26">[27]</ref>, and VOT-2017 <ref type="bibr" target="#b25">[26]</ref>. The OTB toolkit <ref type="bibr" target="#b49">[50]</ref> and VOT toolkit <ref type="bibr" target="#b26">[27]</ref> are taken to evaluate the results in this section.</p><p>OTB benchmark: The OTBs <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref>  videos. The overlap success rate and distance precision metric <ref type="bibr" target="#b48">[49]</ref> are two standard evaluation metrics on OTB benchmark. The overlap success rate measures the intersection over union (IoU) between ground-truth and tracked bounding boxes. The success plot shows the rate of frames whose IoU score is larger than a given threshold. We apply the overlap success rate at a threshold of 0.5 to rank the trackers, which corresponds to the PASCAL evaluation criterion. The tracking methods are ranked by Area-Under-the-Curve (AUC) of success plots. The precision metric is defined as the percentage of frames where the Euclidean distance from tracking results to ground truth is smaller than a threshold. The threshold of the distance is 20 by default for all trackers.</p><p>VOT benchmark: We use VOT-2016 and VOT-2017 to evaluate the performance. The VOT-2016 benchmark consists of 60 sequences with 5 types of challenges including motion change, camera motion, scale change, illumination change and occlusion. In VOT-2017, 10 easy sequences from VOT-2016 are replaced by more challenging ones. VOT-2017 introduced a new real-time challenge, where the tracking results are updated for each frame at frequency higher than or equal to the video frame rate. The VOT benchmark considers more about short-term tracking tasks. The tracker starts tracking from the first frame and it is re-initialized when the tracking process failures. For VOT benchmark, the accuracy, the robustness and Expected Average Overlap (EAO) are applied to evaluate the performance of tracking methods. The accuracy is calculated according to the overlap between tracking results and the ground truth. The robustness score is based on the number of times the tracker fails. The EAO is the major evaluation metric in VOT. It considers both the bounding box overlap ratio (accuracy) and the re-initialization times (robustness).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison on OTB benchmark</head><p>We perform a comprehensive comparison of our LSSiam tracker with other state-of-the-art methods on OTB-2013, OTB-50 and OTB-100 datasets. Then, we analyse the robustness of local semantic feature representation under different challenging situations. Finally, qualitative evaluation on visual results is discussed.</p><p>Overall Comparison. As shown in Fig. <ref type="figure" target="#fig_6">8</ref>, we compare our tracker with 9 state-of-the-art trackers including SiamFC <ref type="bibr" target="#b2">[3]</ref>, Cfnet2 <ref type="bibr" target="#b41">[42]</ref>, HP <ref type="bibr" target="#b12">[13]</ref>, SiamFC tri <ref type="bibr" target="#b11">[12]</ref>, SiamRPN <ref type="bibr" target="#b28">[29]</ref>, PTAV <ref type="bibr" target="#b13">[14]</ref>, Staple <ref type="bibr" target="#b0">[1]</ref>, BACF <ref type="bibr" target="#b15">[16]</ref> and TRACA <ref type="bibr" target="#b28">[29]</ref>. Notice that SiamFC, Cfnet2, HP, SiamFC tri and SiamRPN are Siamese trackers. Staple, BACF and TRACA are real-time correlation filtering trackers. PTAV used a correlation filtering tracker to provide the tracking inference and used a Siamese tracker to correct the inference error. The precision and succes plots of one-pass evaluation (OPE) on three benchmarks are used to evaluate the performance, where the trackers are initialized at the first frame and start tracking without re-initialization. In the figure legend, the AUC score of success plot and the average distance precision score with a default threshold of 20 pixels for every tracker are reported. Our tracker outperforms other trackers in five evaluation metrics except the precision score for OPE on OTB-2013 benchmark. In this metric, our LSSiam achieves the 3rd rank (0.884), which is very close to the 2nd SiamRPN (0.884). TRACA (0.898) ranks the 1st in precision on OTB-2013 while ranks more than the 4th on other metrics. On OTB-50, our tracker achieves the best results with 0.825/0.604 in terms of Precision/AUC score. SiamRPN ranks the 2nd with 81.0%/59.2%, and PTAV ranks 3rd with 0.806/0.581 in terms of Precision/AUC score. On OTB-100, our tracker also achieves the best results with 0.863/0.646% in terms of Precision/AUC score. SiamRPN ranks the 2nd with 0.851/0.637, and PTAV ranks 3rd with 0.841/0.631 in terms of Precision/AUC score. Furthermore, in comparison to SiamFC, our LSSiam achieves a relative gain of 9.3%/9.2%, 19.2%/17.1% and 11.9%/11.0% in Precision/AUC score on OTB-2013, OTB-50 and OTB-100, respectively.</p><p>Table I lists a detailed comparison between our LSSiam tracker and other state-of-the-art Siamese trackers including DSiam <ref type="bibr" target="#b17">[18]</ref>, StructSiam <ref type="bibr" target="#b53">[54]</ref>, MemTrack <ref type="bibr" target="#b50">[51]</ref>, SA-Siam <ref type="bibr" target="#b18">[19]</ref>, RASNet <ref type="bibr" target="#b45">[46]</ref>, GCT <ref type="bibr" target="#b59">[60]</ref>, TADT <ref type="bibr" target="#b60">[61]</ref>, Cfnet2, SiamFC, HP, SiamFC tri and SiamRPN. To evaluate the computational efficiency of these trackers, an additional speed metric (FPS) is introduced. Notice that high-speed trackers should run far beyond real-time requirement (30 FPS), we use a threshold of 60 FPS to divide them into real-time trackers and high-speed ones. The real-time trackers consist of DSiam, StructSiam, TADT, MemTrack, SA-Siam and GCT. The high-speed trackers consist of RASNet, HP, Cfnet2, SiamFC, SiamFC tri and SiamRPN. Compared with high-speed trackers, our LSSiam tracker outperforms others in terms of accuracy on OTB-50 and OTB-100 benchmarks. On OTB-2013, RASNet achieves the best result in AUC score. LSSiam ranks 2nd with a higher FPS than RASNet. Among all real-time trackers, TADT achieves the 1st rank in terms of AUC score. It applies VGG-16 <ref type="bibr" target="#b61">[62]</ref> as the base network and learns target-aware features by online sample-specific adaptation, but cannot meet the highspeed requirement. SiamRPN ranks 1st with 165 FPS, and it uses region proposal networks <ref type="bibr" target="#b36">[37]</ref> to discard the multiscale input process. Our LSSiam ranks 2nd with 100 FPS, and the comparison results demonstrate that our LSSiam tracker achieves both high speed and the state-of-the-art performance.</p><p>Attribute-based Performance Analysis. In OTB-100 dataset, the video sequences are annotated with 11 attributes for different challenging factors, namely Fast Motion (FM), Scale Variation (SV), Low Resolution (LR), Occlusion (OCC), Illumination Variation (IV), Deformation (DEF), Motion Blur (MB), In-Plane Rotation (IPR), Out-of-Plane Rotation (OPR), Out-of-View (OV), and Background Clutters (BC). To demonstrate the robustness of local semantic features for visual object tracking, we evaluate our method with 9 state-of-the-art real-time trackers including SiamFC, Cfnet2, SiamFC tri, HP, SiamRPN, Staple, BACF, TRACA, and PTAV. Fig. <ref type="figure" target="#fig_7">9</ref> illustrates the overlap success plots of OPE under challenging situations on OTB-100 dataset. Our approach outperforms other trackers in 8 subsets: FM, BC, MB, LR, OCC, OV, OPR, SV, especially in LR and OV. For IPR attribute, SianRPN ranks 1st with an  Qualitative Evaluation. As shown in Fig. <ref type="figure" target="#fig_8">10</ref>, our LSSiam method is compared with other six state-of-the-art highspeed trackers (SiamFC, Cfnet2, HP, SiamFC tri, SiamRPN and TRACA) on some challenging sequences from OTB-100 dataset. The first row is the most challenging sequence (Board) where the target suffers from background cluttering, partial occlusion and out-of-plane rotation. Other trackers fail due to different challenges while our LSSiam performs better than them. The sequence (Singer2) in second row is an example of illumination variation situation. The SiamFC, Cfnet2, HP and SiamFC tri suffer a drift problem and LSSiam tracks the target accurately in terms of either precision or overlap. For the sequence (DragonBaby) in third row in Fig. <ref type="figure" target="#fig_8">10</ref>, all the trackers except LSSiam lose the target occasionally due to fast motion and occlusion. Our method updates the target template efficiently so it is robust to appearance and scale deformation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Comparison on VOT benchmark</head><p>In this section, we compare our tracker with the state-ofthe-art trackers on VOT-2016 benchmark. We also conduct the real-time experiments on VOT-2017 benchmark.</p><p>Evaluation on VOT-2016. We compare our LSSiam with 11 recent trackers including SA-Siam <ref type="bibr" target="#b18">[19]</ref>, MemTrack <ref type="bibr" target="#b50">[51]</ref>, StructSiam <ref type="bibr" target="#b53">[54]</ref>, SiamRPN <ref type="bibr" target="#b28">[29]</ref>, SiamRN <ref type="bibr" target="#b26">[27]</ref>, SiamFC <ref type="bibr" target="#b2">[3]</ref>, KCF <ref type="bibr" target="#b19">[20]</ref>, CREST <ref type="bibr" target="#b38">[39]</ref>, C-COT <ref type="bibr" target="#b9">[10]</ref>, MDNet <ref type="bibr" target="#b34">[35]</ref> and DeepSRDCF <ref type="bibr" target="#b8">[9]</ref>   LSSiam-LSSiam baseline challenge, while the performance drops a lot in the real-time challenge. We also compare our LSSiam tracker with the state-of-the-art trackers including C-COT <ref type="bibr" target="#b9">[10]</ref>, ECOhc <ref type="bibr" target="#b7">[8]</ref>,</p><p>Staple <ref type="bibr" target="#b0">[1]</ref>, DACF <ref type="bibr" target="#b31">[32]</ref>, KCF <ref type="bibr" target="#b19">[20]</ref>, PTAV <ref type="bibr" target="#b13">[14]</ref>, SiamFC <ref type="bibr" target="#b2">[3]</ref>, SiamDCF <ref type="bibr" target="#b25">[26]</ref>, SiamFC tri <ref type="bibr" target="#b11">[12]</ref>, RASNet <ref type="bibr" target="#b45">[46]</ref> and SiamRPN <ref type="bibr" target="#b28">[29]</ref> in Table <ref type="table" target="#tab_5">III</ref>. SiamRPN is trained with an additional dataset to improve the performance. Our tracker achieves the best results in terms of EAO score except SiamRPN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Ablation Analysis</head><p>Our tracking algorithm consists of an offline training step, an offline fine-tuning step with focal logistics loss, and the tracking step with the online template updating strategy. ILSVRC VID dataset <ref type="bibr" target="#b27">[28]</ref>     As shown in Fig. <ref type="figure" target="#fig_10">12</ref>, our LSSiam tracker accurately locates the target and handles scale deformation better. Analysis of residual channel attention block. Inspired by previous SA-Siam and RASNet, the residual channel attention block is applied to adapt the feature selection for unseen targets. Fig. <ref type="figure" target="#fig_13">13</ref> shows the results of the proposed attention model. For two categories of targets "david" and "bolt2" on OTB-100, our method can select useful local semantic information adaptively by assigning different attention weights.   Analysis of focal logistic loss. The importance of hard negative samples is measured by the modulating factor γ in the focal logistic loss. Table <ref type="table" target="#tab_9">VII</ref> demonstrates the effect of γ on OTB-100 benchmark. When γ = 0, the focal logistic loss is degraded to the logistic loss, which decreases the performance of our model during offline fine-tuning step. We finally set γ = 1 according to the experiments.</p><p>Analysis of online updating strategy. The online updating strategy is proposed to adapt the appearance deformation of targets during tracking step. We conduct the ablation studies about the parameters in our online updating strategy and report the results in Fig. <ref type="figure" target="#fig_15">14(a)</ref>, where N is the upper bound number of the snapshots, α denotes the template updating ratio and thr is the threshold of the confidence score. For example, "LSSiam N4 α0. these parameters, our method achieves higher accuracy while maintaining the fast speed. We also compare our updating strategy with other online updating methods on our tracker. In Fig. <ref type="figure" target="#fig_15">14(b)</ref>, "Ours" denotes the proposed updating strategy. "Avgsampling40" means that we sample a key-frame every 40 frames rather than according to the confidence score. "Distractor-aware" denotes the online updating method in DaSiamRPN <ref type="bibr" target="#b57">[58]</ref>, which mines the hard negative samples online according to the response map. We adjust their parameters for our tracker and report the results of the best version.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>We have presented a local semantic Siamese framework to extract more robust features for high-speed visual object tracking. This is realized by adding a classification branch and residual channel attention block into the Siamese framework for offline training. These two components are performed to capture the local class information and select the useful semantic parts, respectively. To further enhance the discrimination of features, a new tracking loss is designed to mine hard negative samples. During online tracking, we develop an efficient template updating strategy to adapt target deformation. Our new tracker runs at a fast speed of 100 FPS with the comparable performance on popular visual tracking benchmarks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The architecture of our LSSiam network. It is composed of the mid-level feature block (blue dashed line box), the local feature block (orange dashed line box), the residual channel attention block (green dashed line box) and the filter supervision module (yellow dashed line box). z and x present the input exemplar image patch and instance image patch, respectively. GMP denotes the Global Max Pooling layer and CCP is the Cross-Channel Pooling layer. M denotes the total number of classes. The symbol "*" means the cross-correlation operation in Eq. 1. L cls and Ltra are the classification loss and the tracking loss, respectively. The filter supervision module is applied in the training phase and is removed in the tracking phase, and the residual channel attention block is only used at key-frames.</figDesc><graphic coords="3,74.67,53.14,462.65,179.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The architecture of the filter supervision module. Here, the resolution of input features is H×W , and each k channels correspond to a specific class. The total number of classes is M . GMP denotes the Global Max Pooling layer. The Cross-Channel Pooling layer conducts Average Pooling for every class. The yellow block in features is an example of a certain class.</figDesc><graphic coords="4,48.96,53.14,251.89,129.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Visualization of the local semantic features on OTB-100 benchmark. The visualized images are generated by ReLU activation of the feature maps. "#" denotes the index of the feature channel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. The architecture of the residual channel attention block. It consists of a Global Average Pooling (GAP) layer and two fc layers. The first fc layer reduces the dimension of features, which is followed by a ReLU layer. The second fc layer restores the dimension and the attention value ξ is normalized by a sigmoid activation function. The original and weighted features are gathered in a residual form so more useful target information is preserved.</figDesc><graphic coords="5,57.08,53.14,231.33,92.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Examples of training image patch pairs from the VID dataset. The first row is the target templates and the second row is the search image patches. The targets, hard negative samples and noise are denoted as yellow, green, and red rectangles, respectively.</figDesc><graphic coords="5,329.55,53.14,215.92,162.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Illustration of the online template updating on Walking2 sequence in OTB-100. The dashed red line denotes the threshold of confidence value. Our proposed strategy can select and update the target template adaptively.</figDesc><graphic coords="7,48.96,53.14,257.04,153.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. The comparison results on OTB-2013 [49], OTB-50 and OTB-100 [50]. Precision and Success plots of OPE are applied to estimate the performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Overlap success plots of OPE with AUC for 9 tracking challenges on OTB-100. The challenges are composed of fast motion, background clutter, motion blur, low resolution, occlusion, out of view, out-of-plane rotation, in-plane rotation and scale variation.</figDesc><graphic coords="9,55.75,299.81,164.51,123.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Qualitative results on OTB-100 benchmark compared with the state-of-the-art trackers. From top to bottom, the sequences are Board, Singer2, DragonBaby, and MotorRolling, respectively.</figDesc><graphic coords="10,53.09,186.76,97.67,54.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Fig.11. Illustration of the expected average overlap plot on the VOT-2017 real-time challenge for our tracker and the total 51 trackers in VOT-2017 reports<ref type="bibr" target="#b25">[26]</ref>. We illustrate the speed of our tracker (rank 1st), DACF (rank 2nd), SiamFC (rank 3rd) and SiamDCF (rank 12th). Here the symbol(*) represents the approximated fps from EFO units of VOT-2017 reports.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Visualization of the tracking results compared with LSSiam-and LSSiam. From top to bottom, the sequences are MotoRolling and Liquor in OTB-100, respectively. LSSiam can predict the bounding box of the target more accurately benefiting from additional network blocks, new training loss and the online template updating strategy.</figDesc><graphic coords="11,50.19,490.49,120.81,90.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>is used in both offline training and fine-tuning steps. In order to study the effect of local semantic filters, Siamese networks with different number of local semantic detectors are trained. M denotes the number of classes (M = 30), k is the number of filters for each class. For all trackers, the channel number of local semantic features is reduced to 256 by the embedding layer. We compare LSSiam trackers with k ∈ {10, 20, 30, 40} on the OTB-100 benchmark in terms of AUC score, precision score and mean FPS. As shown in Table IV, we finally choose k = 20 for the proposed LSSiam network by considering both accuracy and speed. Overall ablation analysis. An overall ablation study is conducted on the proposed LSSiam and SiamFC. For SiamFC, we fine-tune the network with focal logistic loss over 10 epochs with ILSVRC VID dataset. The learning rate varies from 1e -4 to 1e -5. The mini-batch is 8, and every epoch consists of 53, 200 sampled pairs. For our tracker, the hyperparameters in training and tracking phases are described in Section IV-A. The results on OTB-100 are demonstrated in Table V. Results from the first two rows demonstrate that</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>the proposed focal logistic loss improves the performance of SiamFC from 0.582/0.771 to 0.595/0.789 by hard negative sample mining. The third row is the baseline of our LSSiam, where the network named as LSSiam-only consists of the mid-level feature block and the local feature block. It shows the results of raw local semantic features for tracking. LSSiamis trained with the classification loss and the logistic loss. The residual channel attention block is introduced to LSSiam-in the fourth row. In the fifth row, we applies the proposed focal logistic loss to offline fine-tuning the network. The last row introduces the online template updating strategy and gives the complete results of LSSiam tracker. Compared with LSSiam-, the AUC score of LSSiam is improved from 0.633 to 0.646, and the Prec. score is improved from 0.828 to 0.863 on OTB-100 benchmark. The speed is only reduced by about 10 FPS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 13 .</head><label>13</label><figDesc>Fig.<ref type="bibr" target="#b12">13</ref>. Illustration of the channel weights output by the residual channel attention block for video sequences david and bolt2 in OTB-100. Channels are sorted according to the attention weights. There is no correspondence between channels for two sequences.</figDesc><graphic coords="12,313.21,589.10,120.80,96.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>Analysis of classification loss. During offline training step, the classification loss helps local feature block learn powerful feature representation. The balancing weight λ in Eq. 7 is set according to the magnitude of L tra and L cls . To evaluate the effect of the classification loss, we train our model with different value of λ. The offline fine-tuning and online template updating are discarded for fairness. TableVIshows the results on OTB-100. λ = 0 is the condition that we train the model without classification loss. With the extra category supervision, our performance is improved from 0.612/0.808 to 0.635/0.832 in terms of AUC./Prec. score.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. Experimental results of the proposed online template updating strategy in OTB-100 benchmark. (a) demonstrates the effect of parameters in our updating model. (b) demonstrates the comparison results with other online updating models.</figDesc><graphic coords="12,437.50,589.10,120.80,96.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2019.2959256, IEEE Transactions on Image Processing</figDesc><table><row><cell>IEEE TRANSACTIONS ON IMAGE PROCESSING</cell><cell>6</cell></row></table><note><p>1057-7149 (c) 2019 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Algorithm 1 The proposed LSSiam tracking algorithm Input: Frame sequence {I t } T 1 , initial target location p 1 and target scale s 1 . Threshold of the snapshot interval T inter , Threshold of the confidence score T conf Output: Target locations {p t } T 2 and scales {s t } T 2 .</figDesc><table><row><cell>1: Initialization:</cell></row><row><cell>2: Crop exemplar patch z 0 centred at p 1 . Extract exemplar</cell></row><row><cell>features ϕ(z) = ϕ(z 0 ).</cell></row><row><cell>3: Snapshots set S = ∅.</cell></row><row><cell>4: Current frame interval counter cnt = 0.</cell></row><row><cell>5: repeat</cell></row><row><cell>6:</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE I COMPARISON</head><label>I</label><figDesc>OF THE STATE-OF-THE-ART SIAMESE TRACKERS ON OTB BENCHMARKS BY AREA-UNDER-THE-CURVE OF SUCCESS PLOTS (AUC). WE ALSO REPORT THE SPEED OF TRACKERS. THE FPS RESULTS WITH "*" MEANS THE CODES ARE NOT AVAILABLE AND WE CITE THE RESULTS FROM THEIR PAPERS. OUR LSSIAM TRACKER OUTPERFORMS OTHER HIGH-SPEED TRACKERS IN TERMS OF AUC AND CAN CONDUCT TRACKING AT 100 FPS.</figDesc><table><row><cell>Tracker</cell><cell cols="3">AUC-OTB2013 OTB50 OTB100 AUC-AUC-</cell><cell>FPS</cell></row><row><cell>DSiam</cell><cell>0.656</cell><cell>-</cell><cell>-</cell><cell>22</cell></row><row><cell>StructSiam</cell><cell>0.638</cell><cell>-</cell><cell>0.621</cell><cell>45*</cell></row><row><cell>TADT</cell><cell>0.680</cell><cell>-</cell><cell>0.660</cell><cell>48</cell></row><row><cell>MemTrack</cell><cell>0.642</cell><cell>-</cell><cell>0.626</cell><cell>50*</cell></row><row><cell>SA-Siam</cell><cell>0.677</cell><cell>0.610</cell><cell>0.657</cell><cell>50*</cell></row><row><cell>GCT</cell><cell>0.670</cell><cell>-</cell><cell>0.648</cell><cell>50*</cell></row><row><cell>RASNet</cell><cell>0.670</cell><cell>-</cell><cell>0.642</cell><cell>80*</cell></row><row><cell>HP</cell><cell>0.629</cell><cell>0.554</cell><cell>0.601</cell><cell>86*</cell></row><row><cell>Cfnet2</cell><cell>0.580</cell><cell>0.525</cell><cell>0.578</cell><cell>88</cell></row><row><cell>SiamFC</cell><cell>0.607</cell><cell>0.516</cell><cell>0.582</cell><cell>94</cell></row><row><cell>SiamFC tri</cell><cell>0.615</cell><cell>0.531</cell><cell>0.590</cell><cell>94</cell></row><row><cell>SiamRPN</cell><cell>0.658</cell><cell>0.592</cell><cell>0.637</cell><cell>165</cell></row><row><cell>LSSiam (ours)</cell><cell>0.663</cell><cell>0.604</cell><cell>0.646</cell><cell>100</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE II COMPARISON</head><label>II</label><figDesc>OF THE STATE-OF-THE-ART TRACKERS ON VOT-2016 BENCHMARKS. THE EVALUATION METRICS INCLUDE EXPECTED AVERAGE OVERLAP (EAO), ACCURACY (ACC.), ROBUSTNESS (ROB.) AND FPS.</figDesc><table><row><cell>Tracker</cell><cell cols="3">EAO(↑) Acc.(↑) Rob.(↓)</cell><cell>FPS(↑)</cell></row><row><cell>SA-Siam</cell><cell>0.291</cell><cell>0.54</cell><cell>1.08</cell><cell>50</cell></row><row><cell>MemTrack</cell><cell>0.273</cell><cell>0.51</cell><cell>1.34</cell><cell>50</cell></row><row><cell>StructSiam</cell><cell>0.264</cell><cell>-</cell><cell>-</cell><cell>45</cell></row><row><cell>SiamRPN</cell><cell>0.344</cell><cell>0.56</cell><cell>1.08</cell><cell>165</cell></row><row><cell>SiamRN</cell><cell>0.277</cell><cell>0.55</cell><cell>1.36</cell><cell>&gt;25</cell></row><row><cell>SiamFC</cell><cell>0.235</cell><cell>0.50</cell><cell>1.65</cell><cell>94</cell></row><row><cell>KCF</cell><cell>0.192</cell><cell>0.48</cell><cell>2.03</cell><cell>172</cell></row><row><cell>CREST</cell><cell>0.283</cell><cell>0.51</cell><cell>1.08</cell><cell>1</cell></row><row><cell>C-COT</cell><cell>0.331</cell><cell>0.52</cell><cell>0.85</cell><cell>0.3</cell></row><row><cell>MDNet</cell><cell>0.257</cell><cell>0.54</cell><cell>1.20</cell><cell>1</cell></row><row><cell>DeepSRDCF</cell><cell>0.276</cell><cell>0.54</cell><cell>1.08</cell><cell>&lt;1</cell></row><row><cell>LSSiam (ours)</cell><cell>0.294</cell><cell>0.53</cell><cell>1.02</cell><cell>100</cell></row><row><cell cols="5">AUC score of 0.639. Our LSSiam ranks 2nd and achieves</cell></row><row><cell cols="5">close accuracy (0.629) to SiamRPN. Overall, these results</cell></row><row><cell cols="5">demonstrate that the proposed LSSiam method can handle</cell></row><row><cell cols="5">tracking well in occlusion, scale variation, fast motion and</cell></row><row><cell cols="3">background cluttering situations.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE III COMPARISON</head><label>III</label><figDesc>OF THE STATE-OF-THE-ART TRACKERS ON VOT2017 REAL-TIME CHALLENGE. THE EVALUATION METRICS ARE FAO AND FPS. HERE THE SYMBOL(*) REPRESENTS THE APPROXIMATE FPS FROM EFO UNITS IN VOT-2017 RESULTS<ref type="bibr" target="#b25">[26]</ref>.</figDesc><table><row><cell></cell><cell cols="4">C-COT ECOhc Staple DACF</cell><cell>KCF</cell><cell cols="7">PTAV SiamFC SiamDCF SiamFC tri RASNet SiamRPN LSSiam</cell></row><row><cell>EAO</cell><cell>0.058</cell><cell>0.177</cell><cell>0.170</cell><cell>0.212</cell><cell>0.134</cell><cell>0.065</cell><cell>0.182</cell><cell>0.135</cell><cell>0.213</cell><cell>0.223</cell><cell>0.243</cell><cell>0.229</cell></row><row><cell>FPS</cell><cell>&lt;1*</cell><cell>18*</cell><cell>47*</cell><cell>18*</cell><cell>172</cell><cell>25</cell><cell>94</cell><cell>11*</cell><cell>94</cell><cell>80</cell><cell>165</cell><cell>100</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE IV COMPARISON</head><label>IV</label><figDesc>RESULTS ON OTB-100 BENCHMARK. THE AUC, PREC. AND FPS METRIC ARE APPLIED OVER ALL THE 100 VIDEOS. k IS THE NUNBER OF FILTERS IN A LOCAL SEMANTIC FILTER BANK. THE BEST RESULTS ARE HIGHLIGHTED WITH BOLT FONT.</figDesc><table><row><cell></cell><cell>k = 10</cell><cell>k = 20</cell><cell>k = 30</cell><cell>k = 40</cell></row><row><cell>AUC(↑)</cell><cell>0.637</cell><cell>0.646</cell><cell>0.640</cell><cell>0.635</cell></row><row><cell>Prec.(↑)</cell><cell>0.850</cell><cell>0.863</cell><cell>0.855</cell><cell>0.843</cell></row><row><cell>FPS(↑)</cell><cell>102.12</cell><cell>100.27</cell><cell>99.84</cell><cell>99.37</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE V ABLATION</head><label>V</label><figDesc>STUDY OF SIAMFC<ref type="bibr" target="#b2">[3]</ref> AND OUR LSSIAM TRACKER ON OTB-100. LSSIAM-IS A DEGRADED VERSION OF OUR LSSIAM TRACKER WITHOUT RESIDUAL CHANNEL ATTENTION. ATT. IS THE RESIDUAL CHANNEL ATTENTION BLOCK. FL. DENOTES FINE-TUNING WITH FOCAL LOGISTIC LOSS, AND OU. DENOTES THE ONLINE UPDATING STRATEGY.</figDesc><table><row><cell>SiamFC LSSiam-att.</cell><cell>fl.</cell><cell>ou.</cell><cell>AUC</cell><cell>Prec.</cell><cell>FPS</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">0.582 0.771</cell><cell>94</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">0.595 0.789</cell><cell>94</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">0.633 0.828</cell><cell>109</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">0.635 0.832</cell><cell>106</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">0.642 0.855</cell><cell>106</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">0.646 0.863</cell><cell>100</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VI ANALYSIS</head><label>VI</label><figDesc>OF CLASSIFICATION LOSS WITH DIFFERENT BALANCING WEIGHTS IN EQ. 7. λ = 0 MEANS THE MODEL IS TRAINED WITHOUT CLASSIFICATION LOSS. THE BEST RESULTS ARE HIGHLIGHTED WITH BOLT FONT.</figDesc><table><row><cell></cell><cell>λ = 1×10 -5</cell><cell>λ = 1×10 -4</cell><cell>λ = 1×10 -3</cell><cell>λ = 0</cell></row><row><cell>AUC.</cell><cell>0.629</cell><cell>0.635</cell><cell>0.627</cell><cell>0.612</cell></row><row><cell>Prec.</cell><cell>0.821</cell><cell>0.832</cell><cell>0.835</cell><cell>0.808</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE VII ANALYSIS</head><label>VII</label><figDesc>OF FOCAL LOGISTIC LOSS WITH DIFFERENT MODULATING FACTOR γ. THE BEST RESULTS ARE HIGHLIGHTED WITH BOLT FONT.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Jianbing Shen (M'11-SM'12)  is currently acting as the Lead Scientist at the Inception Institute of Artificial Intelligence, Abu Dhabi, UAE. He is also an adjunct Professor with the School of Computer Science, Beijing Institute of Technology, China. He has published more than 100 top journal and conference papers with Google Scholar Citations 5800 times, and ten papers are selected as the ESI Hightly Cited or ESI Hot Papers. His current research interests include deep learning, computer vision, autonomous driving, deep reinforcement learning, and intelligent systems. He is an Associate Editor of IEEE Trans. on Image Processing, IEEE Trans. on Neural Networks and Learning Systems, and other journals.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Staple: Complementary learners for real-time tracking</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Golodetz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Miksik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1401" to="1409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep visual attention prediction</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2368" to="2378" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fully-convolutional siamese networks for object tracking</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision Workshops</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="850" to="865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Approximating CNNs with bag-of-localfeatures models works surprisingly well on imagenet</title>
		<author>
			<persName><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improving deep visual representation for person re-identification by global and local image-language association</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="54" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Context-aware deep feature compression for high-speed visual tracking</title>
		<author>
			<persName><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Demiris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="479" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Occlusionaware real-time object tracking</title>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="763" to="771" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Eco: Efficient convolution operators for tracking</title>
		<author>
			<persName><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional features for correlation filter based visual tracking</title>
		<author>
			<persName><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="58" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Beyond correlation filters: Learning continuous convolution operators for visual tracking</title>
		<author>
			<persName><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Triplet loss in siamese network for object tracking</title>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="459" to="474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hyperparameter optimization for tracking with continuous deep q-learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="518" to="527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Parallel tracking and verifying: A framework for real-time and high accuracy visual tracking</title>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5487" to="5495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Look closer to see better: Recurrent attention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning background-aware correlation filters for visual tracking</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">K</forename><surname>Galoogahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fagg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1144" to="1152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Compact bilinear pooling</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="317" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning dynamic siamese network for visual object tracking</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A twofold siamese network for real-time object tracking</title>
		<author>
			<persName><forename type="first">A</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4834" to="4843" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">High-speed tracking with kernelized correlation filters</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="583" to="596" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Squeeze-and-Excitation Networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Part-stacked cnn for finegrained visual categorization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1173" to="1182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multiobject tracking by submodular optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1990" to="2001" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Low-rank bilinear pooling for fine-grained classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="7025" to="7034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fast online tracking with detection refinement</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="162" to="173" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The visual object tracking vot2017 challenge results</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pflugfelder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">Čehovin</forename><surname>Zajc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1949" to="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">The visual object tracking vot2016 challenge results</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pflugfelder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Čehovin Zajc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vojir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Häger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lukežič</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fernandez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-10">Oct 2016</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">High performance visual tracking with siamese region proposal network</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8971" to="8980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2999" to="3007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Bilinear cnn models for finegrained visual recognition</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1449" to="1457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Discriminative correlation filter with channel and spatial reliability</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lukezic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vojir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cehovin Zajc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6309" to="6318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fast online object tracking and segmentation: A unifying approach</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1328" to="1338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Quadruplet network with one-shot learning for fast visual object tracking</title>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3516" to="3527" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning multi-domain convolutional neural networks for visual tracking</title>
		<author>
			<persName><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Youtubeboundingboxes: A large high-precision human-annotated data set for object detection in video</title>
		<author>
			<persName><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mazzocchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5296" to="5305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Crest: Convolutional residual learning for visual tracking</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2574" to="2583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Siamese cascaded region proposal networks for real-time visual tracking</title>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7952" to="7961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Beyond part models: Person retrieval with refined part pooling (and a strong convolutional baseline)</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="480" to="496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">End-to-end representation learning for correlation filter based tracking</title>
		<author>
			<persName><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5000" to="5008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Matconvnet: Convolutional neural networks for matlab</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM International Conference on Multimedia</title>
		<meeting>ACM International Conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="689" to="692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Visual object tracking by hierarchical attention siamese network</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on cybernetics</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Large margin object tracking with circulant feature maps</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="21" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning attentions: residual attentional siamese network for high performance online visual tracking</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maybank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4854" to="4863" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning a discriminative filter bank within a cnn for fine-grained recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4148" to="4157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Spm-tracker: Series-parallel matching for real-time visual object tracking</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3643" to="3652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Online object tracking: A benchmark</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2411" to="2418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Object tracking benchmark</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1834" to="1848" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning dynamic memory networks for object tracking</title>
		<author>
			<persName><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="153" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Submodular trajectories for better motion segmentation in videos</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2688" to="2700" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Spda-cnn: Unifying semantic part detection and abstraction for fine-grained recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Elgammal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1143" to="1152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Structured siamese network for real-time visual tracking</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="351" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Deeply-learned part-aligned representations for person re-identification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3219" to="3228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning multi-attention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5219" to="5227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Couplenet: Coupling global structure with local parts for object detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4126" to="4134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Distractor-aware siamese networks for visual object tracking</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="103" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">End-to-end flow correlation tracking with spatial-temporal attention</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="548" to="557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<title level="m">Graph Convolutional Tracking. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4649" to="4659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Target-Aware Deep Tracking</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1369" to="1378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">His current research interests include video segmentation and object tracking</title>
	</analytic>
	<monogr>
		<title level="m">Zhiyuan Liang is currently working toward the Ph. D. degree in the School of Computer Science</title>
		<meeting><address><addrLine>Beijing Institute of Technology, Beijing, China</addrLine></address></meeting>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
