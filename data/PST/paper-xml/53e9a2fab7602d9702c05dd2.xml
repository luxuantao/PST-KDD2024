<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Attentional Scene Segmentation: Integrating Depth and Motion from Phase</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Atsuto</forename><surname>Maki</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Peter</forename><surname>Nordlund</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jan-Olof</forename><surname>Eklundh</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Numerical Analysis and Computing Science</orgName>
								<orgName type="laboratory">Computational Vision and Active Perception Laboratory (CVAP)</orgName>
								<orgName type="institution">Royal Institute of Technology</orgName>
								<address>
									<postCode>S-100 44</postCode>
									<settlement>Stockholm</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Royal Institute of Technology (KTH)</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">TOSHIBA Kansai Research Laboratories</orgName>
								<address>
									<addrLine>8-6-26 Motoyama-Minami-Cho, Higashinada-ku</addrLine>
									<postCode>658</postCode>
									<settlement>Kobe</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Attentional Scene Segmentation: Integrating Depth and Motion from Phase</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2EFE7C18E0B02A10DB7E6E9D4D7173A6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T02:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Attention</term>
					<term>stereoscopic depth</term>
					<term>image flow</term>
					<term>motion</term>
					<term>cue integration</term>
					<term>pursuit</term>
					<term>saccade</term>
					<term>target selection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present an approach to attention in active computer vision. The notion of attention plays an important role in biological vision. In recent years, and especially with the emerging interest in active vision, computer vision researchers have been increasingly concerned with attentional mechanisms as well, see <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b29">30]</ref> for a few examples. The basic principles behind these efforts are greatly influenced by psychophysical research. That is the case also in the work presented here, which adapts to the model of Treisman <ref type="bibr" target="#b24">[25]</ref>, with an early parallel stage with preattentive cues followed by a later serial stage where the cues are integrated. The contributions in our approach are (i) the incorporation of depth information from stereopsis, (ii) the simple implementation of low level modules such as disparity and flow by local phase, and (iii) the cue integration along pursuit and saccade mode that allows us a proper target selection based on nearness and motion. We demonstrate the technique by experiments in which a moving observer selectively masks out different moving objects in real scenes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Introduction</head><p>Any complex visual system will need a good selection mechanism to allow it to allocate its spatially limited processing resources appropriately <ref type="bibr" target="#b29">[30]</ref>. Attention is a mechanism that enables vision systems to cope with the tradeoff between the amount of information to be processed and the substance of the process itself. Designs of computer vision systems or algorithms are often motivated by the biological counterparts one way or another. This can be attributed to the fact that biological systems have in their evolution acquired functions which actually work in the real world. An example of such a function which is elegantly achieved in biological systems is the attentional mechanism. It serves to efficiently reduce the enormous amounts of available information such that it can be selectively processed. The main theme of this paper is to present a computational approach to such attentional mechanisms. The emerging question is how to achieve such mechanisms and what kind of criterion to employ out of the enormous amount of basic features that may be observed in a scene. These basic features include for example color, orientation, size, motion, and stereoscopic disparity. In designing a computational framework of such attention mechanisms and in choosing among these basic features, we believe that it is worth while to refer to the successfully functioning human attentional system. Nakayama and Silverman <ref type="bibr" target="#b16">[17]</ref> state in their reports on psychophysical observations: "We speculate that retinal disparity in addition to retinal locus has priority when compared with other visual stimulus dimensions..." (p. 265) Retinal disparity is defined as the retinal displacement between the two projections of an object on the left and right retina. In the human visual system binocular stereopsis, by way of retinal disparity, provides an important means for depth perception. Furthermore nearness in depth is directly connected to urgency in spatio-temporal environments. With the motivation that this should also provide one of the strongest cues in computer and machine vision, we employ binocular stereopsis as the central cue for our computational approach to attention.</p><p>In this work, as well as stereopsis, we propose to base the system also on image flow and motion in its framework. As seen in the schematic diagram of our framework in Figure <ref type="figure" target="#fig_0">1</ref>, in this scheme cue integration and attention over time are essential aspects. Part of the cue integration work has appeared in <ref type="bibr" target="#b26">[27]</ref>. The contribution here is that we show that the system can attend to different targets in a purposive way in a cluttered environment. The second key point in this context is the use of depth information, as suggested is done in human vision by <ref type="bibr">Nakayama and</ref> Silverman <ref type="bibr" target="#b16">[17]</ref>. The computation of precise depth information is generally a time consuming task. The third important point of this work is therefore that a functioning system capable of selectively attending different objects can be obtained with rather simple algorithms allowing fast implementations, i.e., we propose to employ local phase information to derive both depth and flow. This is demonstrated by experiments in which a moving or stationary binocular observer (a mobile platform with a head-eye system) selectively masks out different moving objects in real scenes and holds gaze on them over some frames. The selection criteria are here based on nearness and motion, but could in our open architecture be of any type. The important point to note is that the required information is indeed computable and that the desirable behavior of the system is acquired.</p><p>The organization of the paper is as follows. We first make a brief overview of relevant issues on the human attentional system based on psychophysical reports in Section 3. Section 4 then introduces some of the earlier works on attention. Describing the low level modules in Section 5, we design the cue integrations along pursuit and saccade mode in Section 6. Section 7 exemplifies the performance of the proposed prototype through experiments, and Section 8 finally concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Observations about human attention</head><p>The notion of attention plays an important role in biological vision in terms of selecting a part of the scene out of the massive flow of information in space and time. Posner and Petersen <ref type="bibr" target="#b20">[21]</ref> Input Image</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preattentive Cues</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stereo Disparity</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Flow</head><p>Motion Detection</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cue Integration</head><p>Target Mask discuss the attentional system of the human brain by dividing it into three major subsystems that perform different but interrelated functions, these are (i) orienting to sensory events, (ii) detecting signals for focal processing, and (iii) maintaining an alert state. Among those functions, we will in this paper mostly be concerned with orienting to sensory events, including issues on visual locations and search.</p><note type="other">Further Analysis</note><p>Visual locations: Visual orienting is usually defined in terms of the foveation of a stimulus (overt by physically directing attention). Foveating a stimulus improves efficiency of processing targets in terms of acuity, but it is also possible to change the priority given a stimulus by attending to its location covertly without any change in eye position <ref type="bibr" target="#b20">[21]</ref>. In other words, it is almost as if we have some "internal spotlight" that can be aligned with an object to enable us to see it <ref type="bibr" target="#b7">[8]</ref>. Coren et al. <ref type="bibr" target="#b5">[6]</ref> use the metaphor "attentional gaze" to conceptualize some of the findings in covert visual orienting and visual search, and describe it like:</p><p>"Covert shifts in the attentional gaze seem to behave in a similar way to physical movements of the eye and attentional gaze usually cannot be drawn to more than one location in the visual field at any instance in time. Experimental studies of the attentional gaze show that it can shift much faster than the eye -it reaches a stimulus location before the eye does and seems to help to guide the eye to the proper location." <ref type="bibr">(Coren et al. 1993, p. 517)</ref> They also indicate three aspects of the attentional gaze which are important in the processing of sensory information at any one moment, i.e. a locus, an extent, and a detail set <ref type="bibr" target="#b5">[6]</ref>. Attentional gaze poses an important issue on active vision <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>, in the sense that it provides a clue to guide the gaze direction spatio-temporally. Our attentional scheme appearing later treats to the aspects of locus and extent while it does not specify any particular detailed set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visual search:</head><p>Starting from a neurophysiological viewpoint, all neurons are selective in the range of activation to which they will respond. The role of the attention system is to modulate this selection for those types of stimuli that might be most important at a given moment. To understand how this form of modulation operates, it is important to know how a stimulus would be processed without the special effects of attention <ref type="bibr" target="#b20">[21]</ref>. In cognition, unattended processing is called "automatic" to distinguish it from the special processing that becomes available with attention. It is also termed "preattentive processing" in dichotomy with attention.</p><p>In her article on preattentive processing from a psychological viewpoint, Treisman <ref type="bibr" target="#b24">[25]</ref> poses two different problems as follows: "One is to define which features or properties are the basic elements or visual primitives in early vision. The second concerns how they are put together again into the correct combinations to form the coherent world that we perceive." <ref type="bibr">(Treisman 1985, p. 157)</ref> She performs experiments focusing on texture segregation and visual search, where subjects are asked to look for a particular target in displays containing varying numbers of distractor items. If the target is defined by a simple visual feature, detection appears to result from parallel processing; the target "pops out" and the search time is independent of how many distractors surround it. Such experiments turn out to be consistent with the idea that early visual analysis results in separate maps for separate properties, and that these maps pool their activity across locations, allowing rapid access to information about the presence of a target, when it is the only item characterized by a particular preattentively detectable feature <ref type="bibr" target="#b24">[25]</ref>. Though there is not complete agreement about the set of basic features, there is agreement about many members of the set, such as color, orientation, size, motion and stereoscopic depth <ref type="bibr" target="#b29">[30]</ref>. Also reported is that the search for an item defined by the conjunction of two stimulus dimensions is conducted serially, and the search time increases as the number of items becomes larger.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">RELATED WORK</head><p>Thus, it seems that the visual system is incapable of conducting a parallel search over two stimulus dimensions simultaneously. Nakayama and Silverman <ref type="bibr" target="#b16">[17]</ref> extend this conclusion for the conjunction of motion and color. Interestingly, they also point out two exceptions through similar experiments: if one of the dimensions in a conjunctive search is stereoscopic disparity, a second dimension of either color or motion can be searched in parallel.</p><p>Wolfe and Cave <ref type="bibr" target="#b29">[30]</ref> review evidence of preattentive processing in the human visual system through experiments and put it in a nice way: "...the human visual system uses fast, efficient, parallel algorithms to guide the later serial stages of visual processing. This attentional system can use information about expected stimuli when it is available, but will identify unusual parts of the input whether or not they are expected. These parallel mechanisms are very limited, both in their accuracy and in what they can do, but by guiding the allocation of higher-level, limited-capacity processes, they can speed visual processing considerably." <ref type="bibr">(Wolfe and Cave 1990, p. 102)</ref> The intension here is to employ similar simple and efficient mechanisms to obtain attentional algorithms in a computer vision system to quickly identify those parts of the input that should be processed immediately. Particularly we will try to obtain the observed advantage of stereoscopic disparity as a basic feature which can be processed in parallel with some other basic feature as seen in <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related work</head><p>A number of computational approaches of attention have been proposed on the basis of the psychophysical findings on human visual attention. Attentional mechanism can be highly taskdependent and different approaches take different modalities while sharing certain concepts such as the division into preattentive and attentive processing stages.</p><p>Burt <ref type="bibr" target="#b3">[4]</ref> describes attention from a computer vision perspective by three elements: foveation, tracking and high level interpretation. A rudimentary fovea is formed within a Laplacian pyramid and tracking is performed to isolate selected regions of a scene in time, by canceling computed background motion. He defines an object representation called a pattern tree for a fast, hierarchical structured search arguing that very fast reasoning processes are needed to interpret partial scene information as it is gathered, and to provide informed guidance for where to look next. The pattern tree description of objects known to the system are sorted in a knowledge base and the information is gathered in a task oriented fashion.</p><p>An attentional model applying the notion of winner-take-all is presented by Clark and Ferrier <ref type="bibr" target="#b4">[5]</ref>. The first stage in the model extracts primitives in parallel across the visual field. The results from this stage are a set of feature maps which indicate the presence or absence of a feature at each location in the image. The next stage of the model combines the results from the feature maps. The output from the feature maps are amplified with different gains for each map and then summed to form the saliency map. Finding the location with the maximum value gives the most salient location with respect to the given amplifier gains, which may vary over time, thus changing the location of the most salient feature. However, only one location can be attended to at one time. The employed features are blobs, moments of objects, and the intensity value.</p><p>Sandon <ref type="bibr" target="#b21">[22]</ref> bases his attentional vision system on feature and saliency maps as well, but aims for recognition of an object in a particular location in an image. The attentional system consists of a feature processor, an object processor and an attention processor. According to the needs of the current task, feature maps extracted from the image by the feature processor are gated by the object processor, which maintains a set of object models, to provide inputs to the attention processor. The attention processor combines the activity from the feature maps to compute the saliency map, which represents the importance of each region of the image. According to the result of attentional competition, in addition, it gates regions of feature maps to provide inputs to 5. EARLY MODULES the object processor. A feature called an edge histogram, including edge information (magnitude and orientation), is used for the feature maps.</p><p>The work by Syeda-Mahmood <ref type="bibr" target="#b23">[24]</ref> is also based on object models. The problem domain is defined within the scope of model-based object recognition, and the experiments are limited to static scenes. The idea is to determine which regions in an image are likely to come from a single object, and color is employed as the feature for this segmentation.</p><p>In the approach proposed by Westelius <ref type="bibr" target="#b27">[28]</ref>, edge information and rotation symmetry are adopted as the features to form a potential field for driving attention. Phase information from quadrature filters is used to generate a potential field drawing the attention towards and along lines and edges in the image. To find objects another potential field is generated using rotation symmetries. The potential fields are weighted together differently depending on the state of the system, and the state is in turn determined by the type and quality of the data in the fixation point. The states are, (i) search line, (ii) track line, (iii) avoid object, and (iv) locate object.</p><p>An attentional prototype for early vision is developed by Tsotsos et al. <ref type="bibr" target="#b25">[26]</ref> in which they emphasize the neurobiological plausibility of it. The model consists of a set of hierarchical computations, involving the idea of an inhibitory beam which expands as it traverses through the entire hierarchy but is kept to a limited size by inhibition. Any salience map can be used as the stimulus at the input level, and the areas of attention are acquired as selected receptive fields at the bottom level.</p><p>Our work has in several ways been inspired by the described efforts. An essential additional contribution of ours is the incorporation of three dimensional information from stereopsis <ref type="bibr" target="#b11">[12]</ref>. In particular, we use stereoscopic disparity derived from the fact that two eyes receive slightly different views of the three-dimensional world.</p><p>Concerning the schemes for measuring disparity as a static depth cue in stereo vision, a wealth of algorithms has been suggested. Most of the techniques fall in one of the two categories of area-based correlation and feature-based correspondence, and those approaches to stereopsis are in a sense complementary <ref type="bibr" target="#b8">[9]</ref>. Correlation produces a dense set of responses but has difficulty with constant or rapidly changing structure and with interocular image difference, while featurebased correspondence avoids some of these problems by considering the image at different scales but then fails to obtain a dense set of responses by matching only sparse tokens. It would be desirable both to avoid the problem of structure at different scales and at the same time to produce a dense response. Some additional techniques have been developed to complement some of the shortcomings, such as multiple-baseline stereo <ref type="bibr" target="#b18">[19]</ref>, non-parametric local census transform <ref type="bibr" target="#b30">[31]</ref>, or use of linear spatial filters tuned to a range of orientations and scales <ref type="bibr" target="#b10">[11]</ref>. It is on the other hand common in both techniques that a search process is required to find the best match between the areas or features.</p><p>As a third approach, a new technique has been proposed in which disparity is expressed in terms of phase differences in the output of local, bandpass filters applied to the stereo views <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b28">29]</ref>. The main advantages of such local, phase-based approaches include computational simplicity, stability against varying lighting condition and especially direct localization of the estimated disparities. Also, there is biological evidence supporting different aspects of this method including <ref type="bibr" target="#b22">[23]</ref>, e.g. bandpass spatial-frequency filters are thought to be plausible models of the way in which the primary visual cortex processes a visual image <ref type="bibr" target="#b6">[7]</ref>.</p><p>We consider the characteristics of this search-free and thus fast approach as suitable to provide stereoscopic disparity as a basic feature in preattentional early vision. We therefore employ the technique in our computational approach to attention while the traditional alternatives appear to be less well suited in such an application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Early modules</head><p>This section describes the preattentive cues employed in the early parallel stage: stereo disparity, image flow and motion detection, which are integrated in the later serial stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Stereo disparity</head><p>Relative depth, that plays a central role in our system, is derived from a dense disparity map. As disparity estimator we employ a phase-based algorithm which has the advantages of low computational cost, stability against varying lighting condition and especially of allowing good direct localization of the estimated disparity. The disparity estimation algorithm is briefly introduced in Appendix A.1. The employed multi-scaled scheme based on the algorithm is described in <ref type="bibr" target="#b14">[15]</ref>. A target mask is produced by back projection of a selected target disparity and the process of disparity selection is based on histogramming and disparity prediction. The idea is to slice up the scene according to relative-depth and then segment out the part of the input image corresponding to the selected target as a mask. See Appendix A.2 and <ref type="bibr" target="#b13">[14]</ref> for details of the procedure producing the target mask. A point to be noted is that the resulting mask may well involve multiple targets if they are observed to be close to each other in depth. Further segmentation among such targets is beyond the performance of the depth cue alone and some additional information sources would be necessary to handle such a situation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Image flow</head><p>By applying the stereo algorithm to consecutive image frames instead of to a stereo image pair, information of horizontal image flow can be obtained. The image flow cue provides another target mask independent of the depth cue, and those cues are combined in order to deal with complex scenes where multiple target candidates are observed. Information about image flow could be made available in more specific form and as a matter of fact it could be by itself a central cue in terms of attending to moving objects <ref type="bibr" target="#b15">[16]</ref>. In our scheme, however, the use of image flow cue is only in one-dimension along the horizontal direction, because by doing so this early module can share identical input with the depth module. This additional module is in our experiments shown to stabilize the attentional performance a great deal, in spite of its simplicity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Motion detection</head><p>As the third module in the early preattentive stage, a technique for motion detection is employed. The fundamental concept is outlined here. The idea is to exploit the brightness constancy constraint in conjunction with an affine transformation between two consecutive images. Assuming the moving target to be relatively small compared to the background, we compute an affine fit between two consecutive images by posing a weighted least squares minimization problem. Given that the background contains small variations in depth and is far away enough, relative to the motion, the background cancels in the residual image and moving objects appear. The technique is formulated in Appendix ?? and full description is found in <ref type="bibr" target="#b17">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Cue integration</head><p>Given information from the early stage in the form of stereo disparity, image flow estimation and detected motion, the role of the later stage is to guide the attention to an appropriate part of the input image as sketched in Figure <ref type="figure" target="#fig_0">1</ref>. This guidance is achieved by combination of the different early cues in two independent modes, namely the pursuit and saccade modes, each of which produces a target mask. As a criterion to choose the final attentional target mask, depth-based target selection <ref type="bibr" target="#b12">[13]</ref> and duration-based one are considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Pursuit mode</head><p>The objective in the pursuit mode is to keep attending to the current target and mask the corresponding part of the input image sequence accordingly. The framework of the process in this mode is schematically depicted in Figure <ref type="figure" target="#fig_1">2</ref>  As described earlier, the disparity target mask T d (k) is produced by a disparity selection technique based on histogramming and back projection. The information of image flow is processed in an analogous framework, that is, a one-dimensional histogram is constructed for the horizontal flow map, and a flow target mask T f (k) is produced by back projection of a flow parameter that is also selected based on prediction. To summarize, from each of the disparity and flow maps a target mask is produced and those masks are fused with a logical and operation into the target pursuit mask T p (k) so that just the part that is consistent with both disparity and flow remains. The process in frame k can be formulated as:</p><formula xml:id="formula_0">T p (k) = T d (k) ∩ T f (k)<label>(1)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Saccade mode</head><p>The saccade mode on the other hand is aimed at disengaging the attention from the current target and shift it to a new one. The framework of the process in this mode is schematically depicted in Figure <ref type="figure" target="#fig_2">3</ref> using the following notation at frame k, T s (k) : Target saccade mask, T m (k) : Target mask based on detected motion, and T (k -1) : Target mask in the previous frame. While the disparity cue again plays the central role here, the important feature is that a shift is triggered when a new interesting part in the input is detected. The definition of "interesting part" can be task dependent and any distractor among available alternatives could in principle trigger an attentional shift. Here we have chosen only motion relative to the background, since it provides a strong saccadic cue and therefore allows us to demonstrate our framework. As is the case in the pursuit mode, a target saccade mask T s (k) is produced basically by the disparity selection and serial back projection. The previous target mask T (k -1) is, however, utilized differently, i.e., in the saccade mode T (k -1) is inversely applied, so that the current target is inhibited instead of accepted as contribution to the disparity histogram. Besides, the use of disparity information as input is restricted to the part where relative motion to the background is detected. The disparity histogram then carries information just about a newly detected moving target. The process is completed by inhibiting the produced target mask again by T (k -1) to confirm that the resulting target saccade mask T s (k) does not overlap the former target. The process in frame k can be summarized as:</p><formula xml:id="formula_1">T s (k) = T m (k) ∩ T (k -1)<label>(2)</label></formula><p>It should be noted that the framework of the saccade mode without feedback of the former target mask exactly provides a mode to initiate the process by finding the moving target to attend to. This also applies to picking up a new moving target and to restart the process in case the pursuit mode for some reason lost track of the target, e.g., when the target disappears from the scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Target selection</head><p>The cue integration process described thus far provides a pursuit mask T p (k) and a saccade mask T s (k) and the choice among those masks is the remaining issue, which is rather task dependent. Some criterion is needed to decide when the saccade should happen or pursuit should continue and thereby to determine the final target mask T (k) in each frame (see Figure <ref type="figure">4</ref>). Two different criteria are proposed in the following, i.e., depth-based criterion and time transient criterion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Depth-based criterion</head><p>While the framework introduced is open to accept different criteria, we have mainly considered a depth-based attentional scheme where the target that is closer in depth is selected with higher priority, see equation 3. Such a criterion is reasonable for instance for a moving observer who wants to avoid obstacles. The target saccade mask is selected when the newly detected target turns out to be closer, or the current target disappears from the scene. Thus, the closest moving object is kept on attended to over time. In each frame either target pursuit or target saccade mask is selected as the final target mask according to a criterion. For example in the depth-based criterion the mask covering the closer target is selected on the basis of the comparison between the disparity of the current target d p and that of newly detected target d s .</p><formula xml:id="formula_2">T (k) = T s (k), for d s ≤ d p T p (k), otherwise<label>(3)</label></formula><p>for a predefined duration and then shifted to some other object, according to the depth-based criterion as soon as any candidate appears. Ideally the time instance the system attends to the target object should be task dependent. Although our scheme has so far not been integrated with other processes, experiments are performed assuming some certain time duration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Experiments</head><p>The above described attentional scheme has been examined through experiments. In the experiments we purposively choose to include humans as objects in the scene. The humans may be either stationary or moving and may even change their course of travel unpredictably. As such they provide reasonable attention candidates in a dynamic world. Moreover, in view of the circumstances in which a vision system should function, humans are common and important targets to relate and react to, and thus to attend to. Humans also form realistic prototype objects in for example obstacle avoidance, as obstacles to avoid may be either moving or stationary.  <ref type="figure" target="#fig_8">8</ref>. The disparity and flow maps are smoothed by an averaging process over time to gain further stabilized performance<ref type="foot" target="#foot_0">1</ref> . From both of the relative depth and flow information parts of the maps corresponding to the persons in the scene are segmented in each frame. As the third preattentive cue information of detected motion, shown in Figure <ref type="figure" target="#fig_9">9</ref>, is incorporated in such a way that the closest moving object is kept on attended as the target at each frame.</p><p>The resulting target masks according to the depth-based criterion are shown in Figure <ref type="figure" target="#fig_10">10</ref>. The frames are numbered from 1 to 8. It is observed that the target masks are produced in consistency with each of the disparity maps, the certainty maps and the horizontal flow maps. Figure <ref type="figure" target="#fig_11">11</ref> demonstrates the histograms for disparity selection in the corresponding frames. Histograms in the pursuit mode based on the former target masks are indicated in lines, whereas those in the saccade mode based on newly detected motion are in dashed lines. Appearing peaks represent attended targets or target candidates, and it is observed that the attention is taken over from one object to the other depending on the relative depth information. A detailed description of the performance of the system along the frames is given in the following.</p><p>Frame 1-3: By nearness and motion the system first picks out the person A, walking from the left towards the center. Two peaks are appearing in the histograms at the corresponding frames. The person A is represented by the peaks in lines as the current target. The peak shifts as the person A moves further away. Note that smaller disparities correspond to closer objects in depth. Peaks in dashed lines on the other hand represent another moving person B, passing from the right hand side. In Frame 3 the person A and B are almost in the same distance and the two peaks are overlapping. The third person C in behind is not appearing in the histogram because of no motion.</p><p>Frame 4-6: At Frame 4 the attention to the person A is replaced with the second person B who is then relatively closer than the person A. The person B is in attention while being present until Frame 6, and represented by the peaks in line. In the histograms, in the meantime, two peaks in dashed line are observed for the saccade mode by the person B and person C. They are once merged into one peak at Frame 11 when two persons are in the same depth (see Figure <ref type="figure" target="#fig_6">6</ref>).</p><p>Frame 7-8: The attention is finally shifted to the third person C when B disappears from the scene at Frame 13. Histograms in the saccade mode are vanishing as no object except for the attended one is in noticeable motion.</p><p>Throughout the sequence the system basically masks one target at each frame following the depth-based criterion. In situations when several objects belong to an overlapping range both in depth and flow, however, more than one target could accidentally be masked. For example in the Frame 7 in the sequence in Figure <ref type="figure" target="#fig_10">10</ref>, it is seen that parts of the person A and the table on the left corner are masked as well as the target person C. While this is largely a matter of defining the range of target depth and flow, the result is natural because objects with completely identical disparity and flow would not be recognized separately. The situation also indicates the possibility to improve the scheme by merging the system with some extra cues such as information about location in the early parallel process.</p><p>Figure <ref type="figure" target="#fig_13">12</ref> shows another sample image sequence, this time by moving cameras <ref type="bibr" target="#b19">[20]</ref>. It includes two persons walking in a laboratory, one tracked at the center of the image, and the other appearing on the right hand side, passing by in front and disappearing on the left end, while the observing camera head is moving laterally. Every 10th frame is shown (images are taken at framerate 25 Hz). Figure <ref type="figure" target="#fig_14">13</ref> shows that the motion detection process functions even for a sequence with moving cameras. The resulting target masks are shown in Figure <ref type="figure" target="#fig_15">14</ref>, where it is observed that the closest moving object is kept attended to.</p><p>Figure <ref type="figure" target="#fig_19">15</ref> exemplifies the process of the moment when the target mask is shifted from one target to the next one. Illustrated are the masks restricting the input to the disparity histogram, two histograms in pursuit and saccade modes, and the target mask superimposed on the original input image. They are shown for three consecutive frames, k -1, k and k + 1 (left, middle and right) to clarify the information flow between frames. A detailed description of the process along the frames is as follows.</p><p>Frame k -1: The disparity histogram in pursuit mode based on the former target mask provides the current target disparity, d p = 13, while that in saccade mode based on the newly detected moving target provides the disparity of the new target candidate, d s = 13. Since d s = d p , the new candidate is no closer than the current target, and the pursuit target mask is selected as the final target mask.</p><p>Frame k: Pursuit and saccade disparities in this frame are d p = 13 (target person staying at the same depth) and d s = 12 (the second person approaching closer). Since d s &lt; d p , the saccade target mask is selected, i.e., attentional shift takes place. Notice that the former target mask T (k -1) is fed back.        Continuous processes such as above are conducted to determine the target mask in each frame, providing the clue to the attentional target throughout sequence of images.</p><p>As the final experiment the duration-based criterion is examined using a sequence shown in Figure <ref type="figure" target="#fig_20">16</ref>. It includes again the three persons in front of a cluttered background: the person A, moving in front from the left to the right throughout the sequence, the person B, standing on the right hand side, and the person C, walking further back from the left hand side. Every 5th frame is shown (images are taken at frame rate 25 Hz). In Figure <ref type="figure" target="#fig_21">17</ref> the target masks are shown. The frames are numbered from 1 to 12. It is demonstrated that the compulsory attentional shift occurs periodically by the duration-based criterion (duration is fixed to 10 frames). A detailed description of the attentional shift is given for each period of the sequence.</p><p>Frame 1-3: Being the only moving object, A is selected as the target for attention. This period continues even longer than the fixed duration since no other candidate takes on sufficient movement to be attended to.</p><p>Frame 4-5: The attention is shifted to C whose movement begins to be recognized. For the fixed duration (10 frames), C is kept as the target in the pursuit mode.</p><p>Frame 6-11: The attention is shifted back and forth while keeping the fixed duration. First back to A, then to C and again to A. During this period B with little movement is not among the candidates for the attention.</p><p>Frame 12: This time the attention is shifted instead to B that is finally starting to move in closer distance than C is. The depth-based criterion is reflected here while employing the framework of the duration-based criterion.</p><p>Overall it is seen that the behavior of the system is following the duration-based criterion. Though the duration is fixed to 10 frames here, the choice is arbitrary. It should be determined depending on the consecutive process on the attended target. There is some noise observed in masks in       certain frames such as Frame 4 and 8. It is mainly related to the fact that ranges of the disparity and horizontal flow is fixed here for the segmentation of a target. That is, by a fixed range of disparity different depth intervals are covered depending on the distance: the further, the larger interval. A fixed range of flow also corresponds to varying displacement in time due to the motion parallax. The employment of a flexible range definition considering the features of the attended targets would improve the performance, and is an issue for further investigation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>In this paper we have proposed a computational approach to visual attention. The specific character of the work presented here is in the purposive target selection. Considering the problem of attention using image sequences over time, we have specified the attended part of input image by providing a target mask in each frame. Segmented disparity map based on histogramming provides the central cue in the proposed system, in cooperation with information of image flow and a motion detection technique. With the early parallel stage of preattentive cues, the consecutive stage integrated those cues. Key points in our system are summarized as:</p><p>• the employment of relative depth as a target selection criterion as suggested is performed in human vision,</p><p>• the simple computation of low level cues such as disparity and flow by local phase,</p><p>• the integration of cues along pursuit and saccade mode realizing the purposive target selection.</p><p>We have shown experimentally that the system provides expected results for a given control scheme for target selection based on nearness and motion. In particular this also demonstrates that sufficient information for our system is computable by simple algorithms. The proposed approach to visual attention therefore shows promise as a basis for investigating the "where to look next" problem more generally.  Residual normal velocity calculation From (9) we can determine the normal component of the velocity vector locally as:</p><formula xml:id="formula_3">v n (x) = - I t (x) |∇I(x)| . (<label>10</label></formula><formula xml:id="formula_4">)</formula><p>With an arbitrary velocity field, v(x), and the gradients in an image, we can write the normal velocity as:</p><formula xml:id="formula_5">vn (x) = ∇I(x) • v(x) |∇I(x)| ,<label>(11)</label></formula><p>and define the residual between this and the observed normal velocity as: </p><p>where ω is a weight function and Ω is a region of interest in the image where the parameterized velocity model should hold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Solving for the affine motion parameters</head><p>The weighting function w in the minimization in ( <ref type="formula" target="#formula_6">14</ref>) is chosen as the gradient magnitude squared, i.e. w(x) = ∇I(x) 2 . Then we have the following minimization problem,</p><formula xml:id="formula_7">min u x∈Ω (∇I(x) • v(u, x) + I t (x)) 2 . (<label>15</label></formula><formula xml:id="formula_8">)</formula><p>It is implicit in this minimization formulation that regions with little/no velocity information contributes less/nothing at all to the computed v. By using ( <ref type="formula">13</ref>) and ( <ref type="formula" target="#formula_7">15</ref>), a region Ω = {x 1 , . . . , x n }, and measurements of the gradients and time derivatives in the image, we get the following linear equation system, (16) is a 6 × 6 symmetric positive semi-definite system, with the 6 elements of u as unknowns. This is shown explicitly in <ref type="bibr" target="#b16">(17)</ref> with all the sums performed over the region to which we want to fit the model. It becomes definite as soon as there are more than one gradient direction present in the region over which the minimization is performed. </p><formula xml:id="formula_9">V T V u = V T q<label>(16)</label></formula><formula xml:id="formula_10">        I 2 x I<label>2</label></formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A schematic diagram of the proposed attentional framework. It follows the general concept of visual attention, i.e., the early parallel stage with preattentive cues and the later serial stage where the cues are integrated. The diamonds indicate a one frame delay.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Schematic flow diagram of the attentional pursuit. It composes part of the "cue integration" in the framework shown in Figure 1. The diamonds indicate a one frame delay in the feedback. The circles with &amp; indicate a logical and operation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Schematic flow diagram of the attentional saccade. It composes part of the "cue integration" in the framework shown in Figure 1. The circles with &amp; indicate a logical and operation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>d s :Figure 4 :</head><label>s4</label><figDesc>Figure 4: Attentional target.In each frame either target pursuit or target saccade mask is selected as the final target mask according to a criterion. For example in the depth-based criterion the mask covering the closer target is selected on the basis of the comparison between the disparity of the current target d p and that of newly detected target d s .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5</head><label>5</label><figDesc>seen by a stationary binocular camera head shows a sample image sequence. It includes three persons walking around in a laboratory. Every 10th frame is shown (images are taken at framerate 25 Hz). Disparity maps, certainty maps and horizontal flow maps are shown in Figure 6 -Figure</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: An example sequence with 3 moving persons taken by a stationary binocular camera head. Top-left to bottom-right. Every 10th frame of the left image is shown (40 msec between frames).</figDesc><graphic coords="12,95.64,205.48,99.30,77.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Disparity maps computed for the 3 persons sequence. The darker, the closer.</figDesc><graphic coords="12,95.64,420.76,99.30,77.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Certainty maps computed corresponding to the disparity map of the 3 three persons sequence. The lighter, the higher certainty.</figDesc><graphic coords="12,95.64,612.76,99.30,77.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Horizontal flow maps computed for the 3 three persons sequence. The lighter, the more leftward.</figDesc><graphic coords="13,85.08,205.30,104.60,81.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Detected motion for the 3 three persons sequence. The darker, the stronger.</figDesc><graphic coords="13,85.08,414.70,104.60,81.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Target masks computed for the 3 three persons sequence.</figDesc><graphic coords="13,85.08,614.86,104.60,81.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11:The disparity histograms in the pursuit mode based on the former target mask (solid line) and in the saccade mode based on newly detected motion (dashed line). The horizontal and vertical axes are for the disparity estimates and the sum of corresponding certainty measures. Small disparities mean closeness in depth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Frame k + 1 :</head><label>1</label><figDesc>Analogously d p = 12 and d s = 13 since the attention has been shifted in previous frame. The pursuit target mask is selected since d s &gt; d p and the attention stays on the second person.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: An example sequence with 2 moving persons taken by a moving binocular camera head. Top-left to bottom-right. Every 10th frame of the left image is shown (40 msec between frames).</figDesc><graphic coords="15,97.87,212.59,95.03,74.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Detected motion for the 2 persons sequence. The darker, the stronger.</figDesc><graphic coords="15,95.64,426.76,99.30,77.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Target masks computed for the 2 persons sequence.</figDesc><graphic coords="15,97.87,621.67,95.03,74.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>( a )</head><label>a</label><figDesc>Detected motion masks (dark) and target masks in the former frame (gray).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>Disparity histograms in pursuit mode based on the former target mask.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head></head><label></label><figDesc>Disparity histograms in saccade mode based on newly detected motion. (d) The resulting target masks superimposed on the original image sequence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: The process producing the target masks in frame k -1, k, k + 1 (left, middle, right). The horizontal and vertical axes in the histograms are for disparity estimates and sum of corresponding certainty values.</figDesc><graphic coords="16,123.03,549.02,113.88,88.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 16 :</head><label>16</label><figDesc>Figure 16: An example sequence with 3 persons taken by a stationary binocular camera head. Top-left to bottom-right. Every 5th frame of the left image is shown (40 msec between frames).</figDesc><graphic coords="18,85.08,432.70,104.60,81.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 17 :</head><label>17</label><figDesc>Figure 17: Target masks computed for the sequence in Figure 16. Top-left to bottom-right. Every 5th frame is shown (40 msec between frames).</figDesc><graphic coords="18,85.08,598.06,104.60,81.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>R 12 )</head><label>12</label><figDesc>(x) = v(x) -v n (x) = ∇I(x) • v(x) + I t (x) |∇I(x)| (The affine velocity model We model the image velocity, v, as one affine motion for an image region Ω. The number of parameters are then 6 which yields,v(u, x) = B(x)u = a + bx + cy d + ex + f y (13) u = (a, b, c, d, e, f ) T , B(x) = 1 x y 0 0 0 0 0 0 1 x ywhere a, b, c, d, e, f are unknown scalar constants.Solving for the affine image velocityTo solve for a velocity field we use a model for the same affine image velocity, and if we let v be parameterized with u, giving v = v(u, x), we can pose a weighted least squares minimization problem to solve for u, min u = x∈Ω w(x)R(u, x) 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head></head><label></label><figDesc>x (x 1 ) I x (x 1 )x 1 I x (x 1 )y 1 I y (x 1 ) I y (x 1 )x 1 I y (x 1 )y 1 . . . . . . . . . . . . . . . . . .I x (x n ) I x (x n )x n I x (x n )y n I y (x n ) I y (x n )x n I y (x n )y n</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The gray scale in the displayed maps is not necessarily consistent throughout the frames since it is scaled in the range between the highest and lowest values in each frame respectively.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>We employ discrete approximations to the first and second derivatives because of their computational simplicity.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Two consecutive images are used instead when it is to derive horizontal image flow.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>0 ≤ α ≤ 1, in the experiments α is set to be 0.2 to attenuate the influence of noise.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This work benefited greatly from discussions with the vision group CVAP at KTH. Comments from Tomas Uhlin and Jonas Gårding are particularly acknowledged. This work has been supported by TFR, the Swedish Research Council for Engineering Sciences, which is gratefully acknowledged.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Phase-based algorithm</head><p>Disparity map The fundamental idea of the phase-based approach to disparity estimation is to recover local disparity as the spatial shift from the local phase difference observed in the Fourier domain. In practice, the phase is extracted by taking the argument of the convolution product V l (x) and V r (x), which is produced at each point x in the image by convolving a complex filter 2 with the left and the right stereo images 3 . As the local shift between stereo images is approximately proportional to the local phase difference, a disparity estimate at each point in the image is derived accordingly:</p><p>D(x) denotes disparity at x and ω(x) represents some measure of underlying frequency of the image intensity function in the neighborhood of x, which in this case is computed as phase derivative. For details on the techniques employed here, see <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Certainty map</head><p>In order to check the feasibility of the estimated disparity and threshold unreliable estimation, we also compute a certainty value C(x) defined on the basis of the magnitude of the convolution product:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Disparity selection</head><p>Disparity-certainty histogram Based on a disparity map D(x) and a certainty map C(x), we compute a histogram H(D d ) with respect to the discrete disparities D d :</p><p>H(D d ) is defined as the sum of the certainty values at the pixels where the disparity is estimated to D d . Multiple peaks appear in the histogram corresponding to objects with different disparities.</p><p>Disparity prediction With a prediction of what disparity the target should have, the closest peak in the histogram can be selected as the estimate of the target disparity. For computational simplicity a linear predictor is used with a weighting factor α 4 :</p><p>where D s (k) and D p (k) represent the selected and predicted disparity at frame number k while P (k) denotes the predicted change.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Brightness constancy and affine image velocity</head><p>Brightness constancy Using the notation I x = ∂I(x,t) ∂x , I y = ∂I(x,t) ∂y , I t = ∂I(x,t) ∂t and ∇I(x, t) = (I x (x, t), I y (x, t)) T , the brightness constancy can be written as,</p><p>where v = ( dx dt , dy dt ) T is the image velocity. This equation is not enough to constrain the two parameters of v, given the gradients (∇I), and the time derivatives (I t ). What can be determined though, is v's component normal to the gradient, the normal image velocity.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Active vision</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Aloimonos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bandyopadhyay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1st ICCV</title>
		<imprint>
			<date type="published" when="1987">1987</date>
			<biblScope unit="page" from="35" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Active perception vs. passive perception</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bajcsy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd IEEE Workshop on Computer Vision</title>
		<imprint>
			<date type="published" when="1985">1985</date>
			<biblScope unit="page" from="55" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Animate vision</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Ballard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="57" to="86" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Attention mechanism for visoin in a dynamic world</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Burt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th ICPR</title>
		<imprint>
			<date type="published" when="1988-11">November 1988</date>
			<biblScope unit="page" from="977" to="987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Modal control of an attentive vision system</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Ferrier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd ICCV</title>
		<imprint>
			<date type="published" when="1988">1988</date>
			<biblScope unit="page" from="514" to="523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Attention</title>
		<author>
			<persName><forename type="first">S</forename><surname>Coren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Enns</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sensation and perception</title>
		<imprint>
			<publisher>Harcourt Brace College Publishers</publisher>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
	<note>chapter 15</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Brightness and spatial frequency</title>
		<author>
			<persName><forename type="first">S</forename><surname>Coren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Enns</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sensation and perception</title>
		<imprint>
			<publisher>Harcourt Brace College Publishers</publisher>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Visual attention</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Humphreys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bruce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Visual Cognition</title>
		<imprint>
			<publisher>Lawrence Erlbaum Associates, Publishers</publisher>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
	<note>chapter 5</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Techniques for disparity measurement</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R M</forename><surname>Jenkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Jepson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Tsotsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVGIP: Image Understanding</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="14" to="30" />
			<date type="published" when="1991-01">January 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The fast computation of disparity from phase differences</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Jepson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R M</forename><surname>Jenkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="1989-06">June 1989</date>
			<biblScope unit="page" from="398" to="403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A computational framework for determining stereo correspondence from a set of linear spatial filters</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd ECCV</title>
		<imprint>
			<date type="published" when="1992-05">May 1992</date>
			<biblScope unit="page" from="395" to="410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Stereo vision in attentive scene analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Maki</surname></persName>
		</author>
		<idno>ISRN KTH/NA/P-96/07-SE</idno>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
		<respStmt>
			<orgName>Royal Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A computational model of depth-based attention</title>
		<author>
			<persName><forename type="first">A</forename><surname>Maki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nordlund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-O</forename><surname>Eklundh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th ICPR</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">IV</biblScope>
			<biblScope unit="page" from="734" to="739" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Disparity selection in binocular pursuit</title>
		<author>
			<persName><forename type="first">A</forename><surname>Maki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Uhlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEICE Transactions on Information and Systems</title>
		<imprint>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1591" to="1597" />
			<date type="published" when="1995-12">December 1995</date>
		</imprint>
	</monogr>
	<note>E78-D</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Phase-based disparity estimation in binocular tracking</title>
		<author>
			<persName><forename type="first">A</forename><surname>Maki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Uhlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-O</forename><surname>Eklundh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th SCIA</title>
		<imprint>
			<date type="published" when="1993-05">May 1993</date>
			<biblScope unit="page" from="1145" to="1152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Driving saccade to pursuit using image motion</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Bradshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Mclauchlan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Sharkey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="205" to="228" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Serial and parallel processing of visual feature conjunctions</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nakayama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Silverman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">320</biblScope>
			<biblScope unit="issue">20</biblScope>
			<biblScope unit="page" from="264" to="265" />
			<date type="published" when="1986-03">March 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Closing the loop: Detection and pursuit of a moving object by a moving observer</title>
		<author>
			<persName><forename type="first">P</forename><surname>Nordlund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Uhlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IVC</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="265" to="275" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A multiple-baseline stereo</title>
		<author>
			<persName><forename type="first">M</forename><surname>Okutomi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE-PAMI</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="353" to="363" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<author>
			<persName><forename type="first">K</forename><surname>Pahlavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-O</forename><surname>Eklundh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">A head-eye system -analysis and design</title>
		<imprint>
			<date type="published" when="1992-07">July 1992</date>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="41" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The attention system of the human brain</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Posner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Peterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="25" to="42" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Control of eye and arm movements using active, attentional vision</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Sandon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Application of AI, Machine Vision and Robotics</title>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Boyer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Stark</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Bunke</surname></persName>
		</editor>
		<imprint>
			<publisher>World Scientific</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="203" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Stereo disparity computation using Gabor filters</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Sanger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological Cybernetics</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="405" to="418" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Data and model-driven selection using color</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Syeda-Mahmood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd ECCV</title>
		<imprint>
			<date type="published" when="1992-05">May 1992</date>
			<biblScope unit="page" from="115" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Preattentive processing in vision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Treisman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVGIP</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="156" to="177" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Modeling visual attention via selective tuning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Tsotsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Culhane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y K</forename><surname>Wai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nuflo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="507" to="545" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Towards an active visual observer</title>
		<author>
			<persName><forename type="first">T</forename><surname>Uhlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nordlund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Maki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-O</forename><surname>Eklundh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th ICCV</title>
		<imprint>
			<date type="published" when="1995-06">June 1995</date>
			<biblScope unit="page" from="679" to="686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Focus of attention and gaze control for robot vision</title>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Westelius</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
		<respStmt>
			<orgName>Department of Electrical Engineering, Linköping University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A multiresolution stereopsis algorithm based on the Gabor representation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Knutsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings IEE International Conference Im. Proc. and Applic</title>
		<meeting>IEE International Conference Im. Proc. and Applic<address><addrLine>Warwick. U.K</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="19" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deploying visual attention: The guided search model</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Wolfe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Cave</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AI and the Eye</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Blake</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Troscianko</surname></persName>
		</editor>
		<imprint>
			<publisher>John Wiley &amp; Sons Ltd</publisher>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="79" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Non-parametric local transforms for computing visual correspondence. 3rd ECCV</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zabih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Woodhill</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="151" to="158" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
