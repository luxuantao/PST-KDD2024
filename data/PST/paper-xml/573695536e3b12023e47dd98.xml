<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Ear biometrics: a survey of detection, feature extraction and recognition methods</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">A</forename><surname>Pflug</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hochschule Darmstadt -CASED</orgName>
								<address>
									<addrLine>Haardtring 100</addrLine>
									<postCode>64295</postCode>
									<settlement>Darmstadt</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">C</forename><surname>Busch</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hochschule Darmstadt -CASED</orgName>
								<address>
									<addrLine>Haardtring 100</addrLine>
									<postCode>64295</postCode>
									<settlement>Darmstadt</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Ear biometrics: a survey of detection, feature extraction and recognition methods</title>
					</analytic>
					<monogr>
						<idno type="ISSN">2047-4938</idno>
					</monogr>
					<idno type="MD5">7A4748038C84D3E873BAD311EBCD4B70</idno>
					<idno type="DOI">10.1049/iet-bmt.2011.0003</idno>
					<note type="submission">Received on 10th November 2011 Revised on 5th April 2012</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The possibility of identifying people by the shape of their outer ear was first discovered by the French criminologist Bertillon, and refined by the American police officer Iannarelli, who proposed a first ear recognition system based on only seven features. The detailed structure of the ear is not only unique, but also permanent, as the appearance of the ear does not change over the course of a human life. Additionally, the acquisition of ear images does not necessarily require a person's cooperation but is nevertheless considered to be non-intrusive by most people. Owing to these qualities, the interest in ear recognition systems has grown significantly in recent years. In this survey, the authors categorise and summarise approaches to ear detection and recognition in 2D and 3D images. Then, they provide an outlook over possible future research in the field of ear recognition, in the context of smart surveillance and forensic image analysis, which they consider to be the most important application of ear recognition characteristic in the near future.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>As there is an ever-growing need to automatically authenticate individuals, biometrics has been an active field of research over the course of the last decade. Traditional means of automatic recognition, such as passwords or ID cards, can be stolen, faked or forgotten. Biometric characteristics, on the other hand, are universal, unique, permanent and measurable.</p><p>The characteristic appearance of the human outer ear (or pinna) is formed by the outer helix, the antihelix, the lobe, the tragus, the antitragus and the concha (see Fig. <ref type="figure" target="#fig_1">1</ref>). The numerous ridges and valleys on the outer ear's surface serve as acoustic resonators. For low frequencies the pinna reflects the acoustic signal towards the ear canal. For high frequencies it reflects the sound waves and causes neighbouring frequencies to be dropped. Furthermore, the outer ear enables humans to perceive the origin of a sound.</p><p>The shape of the outer ear evolves during the embryonic state from six growth nodules. Its structure therefore is not completely random, but still subject to cell segmentation. The influence of random factors on the ear's appearance can best be observed by comparing the left and the right ear of the same person. Even though the left and the right ear show some similarities, they are not symmetric <ref type="bibr" target="#b0">[1]</ref>.</p><p>The shape of the outer ear has long been recognised as a valuable means for personal identification by criminal investigators. The French criminologist Alphonse Bertillon was the first to become aware of the potential use for human identification through ears, more than a century ago <ref type="bibr" target="#b1">[2]</ref>. In his studies regarding personal recognition using the outer ear in 1906, Richard Imhofer needed only four different characteristics to distinguish between 500 different ears <ref type="bibr" target="#b2">[3]</ref>. Starting in 1949, the American police officer Alfred Iannarelli conducted the first large-scale study on the discriminative potential of the outer ear. He collected more than 10 000 ear images and determined 12 characteristics needed to unambiguously identify a person <ref type="bibr" target="#b3">[4]</ref>. Iannarelli also conducted studies on twins and triplets, discovering that ears are even unique among genetically identical persons. Even though Iannarelli's work lacks a complex theoretical basis, it is commonly believed that the shape of the outer ear is unique. The studies in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> show that all ears of the investigated databases posses individual characteristics, which can be used for distinguishing between them. As there is a lack of a sufficiently large ear database, these studies can only be seen as hints, not evidence, for the outer ear's uniqueness.</p><p>Research about the time-related changes in the appearance of the outer ear has shown that the ear changes slightly in size when a person ages <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>. This is explained by the fact that with ageing the microscopic structure of the ear cartilage changes, which reduces the skin elasticity. A first study on the effect of short periods of time on ear recognition <ref type="bibr">[9]</ref> shows that the recognition rate is not affected by ageing. It must, however, be mentioned that the largest time elapsing difference in this experiment was only 10 months, and it therefore is still subject to further research whether time has a critical effect on biometric ear recognition systems or not.</p><p>The ear can easily be captured from a distance, even if the subject is not fully cooperative. This makes ear recognition especially interesting for smart surveillance tasks and for forensic image analysis. Nowadays, the observation of characteristics is a standard technique in forensic investigation and has been used as evidence in hundreds of cases. The strength of this evidence has, however, also been called into question by courts in the Netherlands <ref type="bibr" target="#b9">[10]</ref>. In order to study the strength of ear prints as evidence, the Forensic Ear identification Project (FearID) was initiated by nine institutes from Italy, the UK and the Netherlands in 2006. In their test system, they measured an equal error rate (EER) of 4% and came to the conclusion that ear prints can be used as evidence in a semi-automated system <ref type="bibr" target="#b10">[11]</ref>. The German criminal police use the physical properties of the ear in connection with other appearance-based properties to collect evidence for the identity of suspects from surveillance camera images. Fig. <ref type="figure" target="#fig_1">1</ref> illustrates the most important elements and landmarks of the outer ear, which are used by the German BKA for manual identification of suspects.</p><p>In this work we extend existing surveys on ear biometrics, such as <ref type="bibr">[12 -15]</ref> or <ref type="bibr" target="#b15">[16]</ref>. Abaza et al. <ref type="bibr" target="#b16">[17]</ref> contributed an excellent survey on ear recognition in March 2010. Their work covers the history of ear biometrics, a selection of available databases and a review of 2D and 3D ear recognition systems. This work amends the survey by Abaza et al. with the following: † A survey of free and publicly available databases.</p><p>† More than 30 publications on ear detection and recognition from 2010 to 2012 that were not discussed in one of the previous surveys. † An outlook over future challenges for ear recognition systems with respect to concrete applications.</p><p>In the upcoming section we give an overview of image databases suitable for studying ear detection and recognition approaches for 2D and 3D images. Thereafter, we discuss existing ear detection approaches on 2D and 3D images. In Section 4 we go on to give an overview of ear recognition approaches for 2D images, and in Section 5 we do the same for 3D images. We will conclude our work by providing an outlook over future challenges and applications for ear recognition systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Available databases for ear detection and recognition</head><p>In order to test and compare the detection or recognition performance of a computer vision system, in general, and a biometric system in particular, image databases of sufficient size must be publicly available. In this section, we want to give an overview of suitable databases for evaluating the performance of ear detection and recognition systems, which can either be downloaded freely or can be licensed with reasonable effort.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">USTB databases</head><p>The University of Science and technology in Beijing offers four collections (http://www1.ustb.edu.cn/resb/en/doc/ Imagedb_123_intro_en.pdf; http://www1.ustb.edu.cn/resb/ en/doc/Imagedb_4_intro_en.pdf) of 2D ear and face profile images to the research community. All USTB databases are available under license. † Database I: The dataset contains 180 images in total, which were taken from 60 subjects in three sessions between July and August 2002. The database only contains images of the right ear from each subject. During each session, the images were taken under different lighting conditions and with a different rotation. The subjects were students and teachers from USTB. † Database II: Similarly to database I, this collection contains right ear images from students and teachers from USTB. This time, the number of subjects is 77 and there were four different sessions between November 2003 and January 2004. Hence the database contains 308 images in total, which were taken under different lighting conditions. † Database III: In this dataset 79 students and teachers from USTB were photographed in different poses between November 2004 and December 2004. Some of the ears are occluded by hair. Each subject rotated his or her head from 0 to 608 to the right and from 0 to 458 to the left. This was repeated on two different days for each subject, which resulted in 1600 images in total. † Database IV: Consisting of 25 500 images from 500 subjects taken between June 2007 and December 2008, this is the largest dataset at USTB. The capturing system consists of 17 cameras and is capable of taking 17 pictures of the subject simultaneously. These cameras are distributed in a circle around the subject, who is placed in the centre. The interval between the cameras is 158. Each volunteer was asked to look upwards, downwards and eyelevel, which means that this database contains images at different yaw and pitch poses. Please note that this database only contains one session for each subject.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">University of Notre Dame (UND) databases</head><p>The UND offers a large variety of different image databases, which can be used for biometric performance evaluation. Among them are five databases containing 2D images and depth images, which are suitable for evaluation ear recognition systems. All databases from UND can be made available under license (http://cse.nd.edu/cvrl/CVRL/ Data_Sets.html). † Collection E: Around 464 right profile images from 114 human subjects, captured in 2002. For each user, between three and nine images were taken on different days and under varying pose and lighting conditions.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">WPUT-DB</head><p>The West Pommeranian University of Technology has collected an ear database with the goal of providing more representative data than comparable collections (http://ksm. wi.zut.edu.pl/wputedb/) <ref type="bibr" target="#b19">[20]</ref>. The database contains 501 subjects of all ages and 2071 images in total. For each subject, the database contains between four and eight images, which were taken on different days and under different lighting conditions. The subjects are also wearing headdresses, earrings and hearing aids, and in addition to this, some ears are occluded by hair. In Fig. <ref type="figure">2</ref>, some example images from the database are shown. The presence of each of these disruptive factors is encoded in the file names of the images. The database can be freely downloaded from the given URL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">IIT Delhi</head><p>The IIT Delhi Database is provided by the Hong Kong Polytechnic University (http://www4.comp.polyu.edu.hk/ csajaykr/IITD/Database_Ear.htm) <ref type="bibr" target="#b20">[21]</ref>. It contains ear images that were collected between October 2006 and June 2007 at the Indian Institute of Technology Delhi in New Delhi (see Fig. <ref type="figure" target="#fig_2">3</ref>). The database contains 121 subjects, and at least three images were taken per subject in an indoor environment, which means that the database consists of 421 images in total.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">IIT Kanpur</head><p>The IITK database was contributed by the Indian Institute of Technology in Kanpur (http://www.cse.iitk.ac.in/users/ biometrics/) <ref type="bibr" target="#b21">[22]</ref>. This database consists of two subsets. † Subset I: This dataset contains 801 side face images collected from 190 subjects. Number of images acquired from an individual varies from two to ten. † Subset II: The images in this subset were taken from 89 individuals. For each subject nine images were taken with three different poses. Each pose was captured at three different scales. Most likely, all images were taken on the same day. It is not stated whether subset II contains the same subjects as subset I.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">ScFace</head><p>The SCface database is provided by the Technical University of Zagreb (http://www.scface.org/) <ref type="bibr" target="#b22">[23]</ref> and contains 4160 images from 130 subjects. The aim of the database is to provide a database, which is suitable for testing algorithms under surveillance scenarios. Unfortunately, all surveillance camera images were taken at a frontal angle, such that the ears are not visible on these images. However the database also contains a set of high resolution photographs from each subject, which show the subject at different poses. These poses include views of the right and left profile, as shown in Fig. <ref type="figure" target="#fig_4">4</ref>. Even though the surveillance camera images are likely to be unsuitable for ear recognition studies, the high-resolution photographs could be used for examining resistance to pose variations of an algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7">Sheffield face database</head><p>This database was formerly known as the UMIST (http:// www.sheffield.ac.uk/eee/research/iel/research/face) database and consists of 564 images of 20 subjects of mixed race and gender. Each subject is photographed in a range of different yaw poses, including a frontal view and profile views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.8">YSU</head><p>The Youngston State University collected a new kind of biometric database for evaluation forensic identification systems <ref type="bibr" target="#b23">[24]</ref>. For each of the 259 subjects, ten images are provided. The images are grabbed from a video stream and show the subject in poses between 0 and 908. This means that the database contains right profile images and a frontal view image for each subject. It also contains hand drawn sketches from 50 randomly selected subjects from a frontal angle. However this part of the database is not of interest for ear recognition systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.9">NCKU</head><p>The National Cheng Kung University in Taiwan has collected an image database, which consists of 37 images for each of the 90 subjects. It can be downloaded from the university's website (http://robotics.csie.ncku.edu.tw/ Databases/FaceDetect_PoseEstimate.htm). Each subject is photographed in different angles between 2908 (left profile) and 908 (right profile) in 58 steps. In Fig. <ref type="figure" target="#fig_3">5</ref> some examples for this are displayed. Such a series of images is collected at two different days for each of the subjects. All images were taken under the same lighting conditions and with the same distance between the subject and the camera.</p><p>As these data were originally collected for face recognition, some of the ears are partly or fully occluded by hair, which make these data challenging for ear detection approaches. Fig. <ref type="figure">2</ref> Example images from the WPUT ear database <ref type="bibr" target="#b19">[20]</ref> a Good quality b Occlusion by hair c Sparse lighting d Occlusion by jewellery Database contains ear photographs of varying quality and taken under different lighting conditions. Furthermore, the database contains images, where the ear is occluded by hair or by earrings Consequently, only a subset of this database is suitable for ear recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.10">UBEAR dataset</head><p>The dataset presented in <ref type="bibr" target="#b24">[25]</ref> contains images from the left and the right ear of 126 subjects. The images were taken under varying lighting conditions and the subjects were not asked to remove hair, jewelry or headdresses before taking the pictures. The images are cropped from video stream, which shows the subject in different poses, such as looking towards the camera, upwards or downwards.</p><p>Additionally, the ground truth for the ear's position is provided together with the database, which makes it particularly convenient for researches to study the accuracy of ear detection and to study the ear recognition performance independently from any ear detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Ear detection</head><p>This section summarises the state of the art in automatic ear detection in 2D and 3D images, respectively. Basically all ear detection approaches are relying on mutual properties of the ears morphology, like the occurrence of certain characteristic edges or frequency patterns. Table <ref type="table" target="#tab_0">1</ref> gives a short overview of the ear detection methods outlined below. The upper part of the table contains algorithms for 3D ear localisation, whereas the lower part lists algorithms designed for ear detection in 2D images.</p><p>Chen and Bhanu propose three different approaches for ear detection. In the approach from <ref type="bibr" target="#b37">[38]</ref> Chen and Bhanu train a classifier, which recognises a specific distribution of shape indices, which are characteristic for the ear's surface. However this approach only works on profile images and is sensitive to any kind of rotation, scale and pose variation. In their later ear detection approaches Chen and Bhanu detected image regions with a large local curvature with a technique they called step edge magnitude <ref type="bibr" target="#b29">[30]</ref>. Then a template, which contains the typical shape of the outer helix and the anti-helix, is fitted to clusters of lines. In <ref type="bibr" target="#b28">[29]</ref> where Chen and Bhanu narrowed the number of possible ear candidates by detecting the skin region first before the helix template matching is applied on the curvature lines. By fusing colour and curvature information, the detection rate could be raised to 99.3% on the UCR dataset and 87.71% on UND collection F and a subset of collection G. The UCR dataset is not publicly available and is hence not covered in Section 2. For a description of this dataset see <ref type="bibr" target="#b16">[17]</ref>.</p><p>Another example for ear detection using contour lines of the ear is described by Attrachi et al. <ref type="bibr" target="#b36">[37]</ref>. They locate the outer contour of the ear by searching for the longest connected edge in the edge image. By selecting the top, bottom and left points of the detected boundary, they form a triangle with the selected points. Further the barycentre of the triangle is calculated and selected as reference point for   <ref type="bibr" target="#b33">[34]</ref>. The edges are separated into two categories, namely convex and concave. Convex edges are chosen as candidates for representing the outer contour. Finally, the algorithm connects the curve segments and selects the figure enclosing the largest area for being the outer ear contour. It should be noted that the IITK database and USTB II already contain cut-out ear images. Hence it can be put into question, whether the detection rates of 93.34 and 98.05% can be reproduced under realistic conditions.</p><p>A recent approach on 2D ear detection using edges is described by Prakash and Gupta in <ref type="bibr" target="#b41">[42]</ref>. They combine skin segmentation and categorisation of edges into convex and concave edges. Afterwards the edges in the skin region are decomposed into edge segments. These segments are composed to form an edge connectivity graph. Based on this graph the convex hull of all edges, which are believed to belong to the ear, is computed. The enclosed region is then labelled as the ear region. In contrast to <ref type="bibr" target="#b36">[37]</ref>, Prakash and Gupta prove the feasibility of edge-based ear detection on full profile images, where they achieved a detection rate of 96.63% on a subset of the UND-J2 collection. In <ref type="bibr" target="#b31">[32]</ref> the authors propose the same edge connectivity for ear recognition on 3D images. Instead of edges, they use discontinuities in the depth map for extracting the initial edge image and then extract the connectivity graph. In their experiments, they use the 3D representations of the same subset as in <ref type="bibr" target="#b41">[42]</ref> and report a detection rate of 99.38%. Moreover, they show that the detection rate of their graphbased approach is not influence by rotation and scale.</p><p>Jedges and Mate propose another edge-based ear detection approach, which is likely to be inspired by fingerprint recognition techniques. They train a classifier with orientation pattern, which were previously computed from ear images. Like other naive classifiers, their method is not robust against rotation and scale. Additionally, the classifier is likely to fail under large pose variations, because this will affect the appearance of the orientation pattern. Yan and Browyer developed an ear detection method which fuses range images and corresponding 2D colour images <ref type="bibr" target="#b17">[18]</ref>. Their algorithm starts by locating the concha and then uses active contours for determining the ear's outer boundary. The concha serves as the reference point for placing the starting shape of the active contour model. Even though the concha is easy to localise in profile images, it may be occluded if the head pose changes or if a subject is wearing a hearing aid or ear phones. In their experiments Yan and Browyer only use ear images with minor occlusions where the concha is visible; hence it could neither be proved nor disproved whether their approach is capable of reliably detecting ears if the concha is occluded.</p><p>Yuan and Mu developed a method for real-time ear tracking in video sequences by applying continuously adaptive mean shift (CAMSHIFT) to video sequences <ref type="bibr" target="#b43">[44]</ref>. The CAMSHIFT algorithm is frequently used in face tracking applications and is based on region matching and a skin colour model. For precise ear segmentation, the contour fitting method based on modified active shape models, which have been proposed by Alvarez et al. is applied <ref type="bibr" target="#b34">[35]</ref>. Yuan and Mu report a detection rate of 100%, however the test database only consisted of two subjects. Nevertheless, their approach appears to be very promising </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>#Img Type</head><p>Chen and Bhanu <ref type="bibr" target="#b28">[29]</ref> shape model and ICP 700 3D 87.71% Chen and Bhanu <ref type="bibr" target="#b29">[30]</ref> helix shape model 213 3D 92.6% Zhou et al. <ref type="bibr" target="#b30">[31]</ref> histograms of categorised shapes 942 3D 100% Prakash and Gupta <ref type="bibr" target="#b31">[32]</ref> connectivity graph 1604 3D 99.38% Abaza et al. <ref type="bibr" target="#b32">[33]</ref> cascaded adaboost 940 2D 88.72% Ansari and Gupta <ref type="bibr" target="#b33">[34]</ref> edge detection and curvature estimation 700 2D 93.34% Alastair et al. <ref type="bibr" target="#b26">[27]</ref> ray transform 252 2D 98.4% Alvarez et a.l <ref type="bibr" target="#b34">[35]</ref> ovoid model NA 2D NA Arbab-Zavar and Nixon <ref type="bibr" target="#b25">[26]</ref> Hough transform 942 2D 91% Arbab-Zavar and Nixon <ref type="bibr" target="#b35">[36]</ref> log-Gabor filters and wavelet transform 252 2D 88.4% Attarchi et al. <ref type="bibr" target="#b36">[37]</ref> edge detection and line tracing 308 2D 98.05% Chen and Bhanu <ref type="bibr" target="#b37">[38]</ref> template matching with shape index histograms 60 2D 91.5% Islam et al. <ref type="bibr" target="#b38">[39]</ref> Adaboost 942 2D 99.89% Jeges and Mate <ref type="bibr" target="#b27">[28]</ref> edge orientation pattern 330 2D 100% Kumar et al. <ref type="bibr" target="#b39">[40]</ref> edge clustering and active contours 700 2D 94.29% Liu and Liu <ref type="bibr" target="#b40">[41]</ref> Adaboost and skin colour filtering 50 2D 96% Prakash and Gupta <ref type="bibr" target="#b41">[42]</ref> skin colour and graph matching 1780 2D 96.63% Shih et al. <ref type="bibr" target="#b42">[43]</ref> arc-masking and AdaBoost 376 2D 100% Yan and Bowyer <ref type="bibr" target="#b17">[18]</ref> Concha detection and active contours 415 2D .97.6 Yuan and Mu <ref type="bibr" target="#b43">[44]</ref> CAMSHIFT and aontour fitting video 2D NA</p><p>for surveillance applications but needs to be further evaluated in more realistic test scenarios. Shih et al. determine ear candidates by localising arcshaped edges in an edge image. Subsequently, the arcshaped ear candidates are verified by using an Adaboost classifier. They report a detection rate 100% on a dataset, which consists of 376 images from 94 subjects.</p><p>Zhou et al. train a 3D shape model in order to recognise the histogram of shape indexes of the typical ear <ref type="bibr" target="#b30">[31]</ref>. Similarly to the approaches of Abaza et al. and Islam et al. a sliding window of different sizes is moved over the image. The ear descriptor proposed by Zhou et al. is built from concatenated shape index histograms, which are extracted from sub-blocks inside the detection window. For the actual detection, a support vector machine (SVM) classifier is trained to decide whether an image region is the ear region or not. As far as we know, this is the first ear detection approach, which does not require having corresponding texture images in addition to the range image. Zhou et al. evaluated their approach on images from the UND collections and report a detection rate of 100%. It should be noted that this approach was not tested under rotation, pose variations and major occlusions, but under the impression of the good performance, we think this is an interesting task for future research.</p><p>Ear detection methods based on image transformations have the advantage of being robust against out-of-plane rotations. They are designed to highlight specific properties of the outer ear, which occur in each image where the ear is visible no matter in which pose the ear has been photographed. In <ref type="bibr" target="#b25">[26]</ref> the Hough transform is used for enhancing regions with a high density of edges. In head profile images, a high density of edges especially occurs in the ear region (see Fig. <ref type="figure" target="#fig_6">6a</ref>). In <ref type="bibr" target="#b25">[26]</ref> it is reported that the Hough transform-based ear detection gets trapped when people wear glasses since the frame introduces additional edges to the image. This especially occurs in the eye and nose region. The ear detection approach based on Hough transform was evaluated on the images in the XM2VTS database (see <ref type="bibr" target="#b16">[17]</ref> for a detailed database description), where a detection rate of 91% was achieved.</p><p>The ray transform approach proposed in <ref type="bibr" target="#b26">[27]</ref> is designed to detect the ear in different poses and to ignore straight edges in the image, which can be introduced by glasses or hair. Ray transform uses a light ray analogy to scan the image for tubular and curved structures like the outer helix. The simulated ray is reflected in bright tubular regions and hence these regions are highlighted in the transformed image. Since glass frames have straight edges, they are not highlighted by the ray transform (see Fig. <ref type="figure" target="#fig_6">6b</ref>). Using this method Alastair et al. achieved an impressive recognition rate of 98.4% on the XM2VTS database. Hence, the ray transform approach by Alastair et al. outperforms Hough transform, most likely because it is more robust against disruptive factors such as glasses or hair.</p><p>A recent approach for 2D ear detection is described in <ref type="bibr" target="#b39">[40]</ref>. Kumar et al. propose to extract ears from 2D images by using edge images and active contours. They evaluate their approach on a database, which consists of 100 subjects with seven images per subject. A special imaging device was used for collecting the data. This device makes sure that the distance to the camera is constant and that the lighting conditions are the same for all images. Within this setting a detection rate of 94.29% is reported.  <ref type="bibr" target="#b25">[26]</ref> b Original image, and ray transform <ref type="bibr" target="#b26">[27]</ref> c Original image, edge enhanced image and corresponding edge orientation model <ref type="bibr" target="#b27">[28]</ref> Each ear recognition system consists of a feature extraction and a feature vector comparison step. In this survey we divide ear recognition approaches into four different subclasses namely holistic approaches, local approaches, hybrid approaches and statistical approaches.</p><p>In Tables <ref type="table" target="#tab_1">2</ref> and<ref type="table" target="#tab_2">3</ref> all 2D ear recognition approaches mentioned in this paper are summarised in chronological order.  <ref type="bibr" target="#b27">[28]</ref> distorted ear model with feature points 28 4060 5.6% EER Liu et al. <ref type="bibr" target="#b58">[59]</ref> edge-based features from different views 60 600 97.6% Nanni and Lumini <ref type="bibr" target="#b59">[60]</ref> Gabor filters and SFFS 114 464 80% Rahman et al. <ref type="bibr" target="#b60">[61]</ref> geometric features 100 350 87% Sana et al. <ref type="bibr" target="#b61">[62]</ref> Haar wavelets and Hamming distance 600 1800 98.4% Arbab-Zavar and Nixon <ref type="bibr" target="#b35">[36]</ref> log-Gabor filters 63 252 85.7 % Choras <ref type="bibr" target="#b62">[63]</ref> Geometry of ear outline 188 376 86.2%</p><p>Unless stated differently, performance always refers to rank-1 performance. HMAX and SVM 60 96.5% Zhang and Mu <ref type="bibr" target="#b69">[70]</ref> geometrical features, ICA and PCA with SVM 77 92.21 Badrinath and Gupta <ref type="bibr" target="#b70">[71]</ref> SIFT landmarks from ear model 106 1060 95.32% Kisku et al. <ref type="bibr" target="#b71">[72]</ref> SIFT from different colour segments 400 96.93% Wang and Yuan <ref type="bibr" target="#b72">[73]</ref> low-order moment invariants 77 100% Alaraj et al. <ref type="bibr" target="#b73">[74]</ref> PCA with MLFFNNs 17 85 96% Bustard et al. <ref type="bibr" target="#b74">[75]</ref> SIFT point matches 63 96% De Marisco et al. <ref type="bibr" target="#b75">[76]</ref> partitioned iterated function system (PIFS) 114 61% Gutierrez et al. <ref type="bibr" target="#b76">[77]</ref> MNN with Sugeno measures and SCG 77 97% Wang et al. <ref type="bibr" target="#b77">[78]</ref> moment invariants and BP neural network NA 60 91.8% Wang and Yuan <ref type="bibr" target="#b78">[79]</ref> Gabor wavelets and GDA 77 99.1% Fooprateepsiri and Kurutach <ref type="bibr" target="#b79">[80]</ref> trace and Fourier transform 68 68 97% Prakash and Gupta <ref type="bibr" target="#b21">[22]</ref> SURF and NN classifier 300 2066 2.25% EER Kumar et al. <ref type="bibr" target="#b39">[40]</ref> SIFT 100 95% GAR, 0.1% FAR Wang and Yan <ref type="bibr" target="#b80">[81]</ref> local binary pattern and wavelet transform 77 100% Kumar and Wu <ref type="bibr" target="#b20">[21]</ref> phase encoding with log Gabor filters 221 95.93%</p><p>Unless stated differently, performance always refers to rank-1 performance</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Holistic descriptors</head><p>Another approach, which has gained some popularity is the force field transform by Hurley <ref type="bibr" target="#b47">[48]</ref>. The force field transformation approach assumes that pixels have a mutual attraction proportional to their intensities and inversely to the square of the distance between them rather like Newton's universal law of gravitation. The associated energy field takes the form of a smooth surface with a number of peaks joined by ridges (see Fig. <ref type="figure">7d</ref>). Using this method, Hurley et al. achieved a rank-1 performance of more than 99% on the XM2VTS database (252 images).</p><p>Building on these results, Abdel-Mottaleb and Zhou use a 3D representation of the force field for extracting points lying on the peak of the 3D force field <ref type="bibr" target="#b52">[53]</ref>. As the force field converged at the outline of the ear, the peaks in the 3D representation basically represent the ear contour. Nonetheless, the force field method is more robust against noise than other edge detector, such as Sobel or Canny.</p><p>Using this approach, Abdel-Mottaleb and Zhou achieved a rank-1 performance of 87.93% on a dataset with consists of 103 ear images from 29 subjects. Dong and Mu <ref type="bibr" target="#b63">[64]</ref> add pose invariance to the edges, which are extracted by using the force field method. This is achieved with null space kernel fishier discriminant analysis (NKFDA), which has the property of representing non-linear relations between two datasets. Dong and Mu conducted experiments on the USTB IV dataset. Before feature extraction, the ear region was cropped out manually from the images and the pose is normalised. For pose variation of 308 they report a rank-1 recondition rate of 72.2%. For pose variations of 458 the rank-1 performance dropped to 48.1%.</p><p>In a recent publication of Kumar and Wu <ref type="bibr" target="#b20">[21]</ref> they present an ear recognition approach, which uses the phase information of log-Gabor filters for encoding the local structure of the ear. The encoded phase information is stored in normalised grey level images. In the experiments, the log-Gabor approach outperformed force field features and a landmark-based feature extraction approach. Moreover, different combinations of log-Gabor filters were compared with each other. The rank-e performance for the log-Gabor approaches ranges between 92.06 and 95.93% on a database which contains 753 images from 221 subjects.</p><p>The rich structure of the outer ear results in specific texture information, which can be measured using Gabor filters. Wang and Yuan <ref type="bibr" target="#b78">[79]</ref> extract local frequency features by using a battery of Gabor filters and then select the most distinctive features by using general discriminant analysis.</p><p>In their experiments on the USTB II database, they compared the performance impact of different settings for the Gabor filters. Different combinations of orientation and scales in the filter sets are compared with each other and it was found that neither the number of scales nor the number of orientations has a major impact on the rank-1 performance. The total rank-1 performance of Wang and Yuan's approach is 99.1%. In a similar approach Arbab-Zavar and Nixon <ref type="bibr" target="#b35">[36]</ref> measured the performance of Gabor filters in the XM2VTS database where they report a rank-1 performance of 91.5%. A closer look at the Gabor filter responded showed that the feature vectors are corrupted by occlusion or other disruptive factors. In order to overcome this, more robust comparison method is proposed, which resulted in an improved recognition rate of 97.4%.</p><p>Abate et al. <ref type="bibr" target="#b54">[55]</ref> use a generic Fourier descriptor for rotation and scale invariant feature representation. The image is transformed into a polar coordinate system and then transformed into frequency space. In order to make sure that the centroid of the polar coordinate system is always at the same position, the ear images have to be aligned before they can be transformed into the polar coordinate system. The concha serves as a reference point for the alignment step, such that the centre point of the polar coordinate system is always located in the concha region. The approach was tested on a proprietary dataset, which contains 282 ear images in total. The images were taken on two different days and in different roll and yaw poses. The rank-1 performance of the Fourier descriptor varies depending on the pose angle. For 08 pose variation the rank-1 performance is 96%, but if different poses are included in the experiments, it drops to 44% for 158 and 19% for 308.</p><p>In the work of Fooprateepsiri and Kurutach exploit the concepts of multi-resolution Trace transform and Fourier transform. The input images from the CMU PIE database are serialised by using the trace transform and stored in a feature vector. The advantage of the trace transform is that the resulting feature vector is invariant to rotations and scale. Furthermore, Fooprateepsiri and Kurutach show that their descriptor is also robust against pose variations. In total they report a rank-1 performance of 97%.</p><p>Sana et al. use selected wavelet coefficients extracted during Haar-wavelet compression for feature representation <ref type="bibr" target="#b61">[62]</ref>. While applying the four-level wavelet transform several times on the ear image, for each iteration they store Fig. <ref type="figure">7</ref> Examples for feature extraction for 2D ear images a Concentric circles <ref type="bibr" target="#b62">[63]</ref> b SIFT features <ref type="bibr" target="#b57">[58]</ref> c Active contour <ref type="bibr" target="#b27">[28]</ref> d Fource field <ref type="bibr" target="#b62">[63]</ref> one of the derived coefficients in a feature vector. The reported accuracy of their algorithm is 96% and was achieved on the basis of the IITK database and on the Saugor database (350 subjects).</p><p>A feature extraction system called PIFS is proposed by De Marisco et al. <ref type="bibr" target="#b75">[76]</ref>. PIFS measures the self-similarity in an image by calculating affine translations between similar sub regions of an image. In order to make their system robust to occlusion, De Marisco et al. divided the ear image into equally large tiles. If one tile is occluded, the other tiles still contain a sufficiently distinctive set of features. De Marisco et al. could show that their approach is superior to other feature extraction methods under the presence of occlusion. The experiments of De Marisco et al. have been conducted in order to assess the system performance in different occlusion scenarios. The basis for these tests was the UND collection E and the first 100 subjects of the FERET database. If occlusion occurs on the reference image, a rank-1 performance of 61% (compared to 40% on average with other feature extraction methods) is reported. Without occlusion, the rank-1 performance is 93%.</p><p>Moment invariants are a statistical measure for describing specific properties of a shape. Wang et al. <ref type="bibr" target="#b77">[78]</ref> compose six different feature vectors by using seven moment invariants. They also show that each of the moment invariants is robust against changes in scale and rotation. The feature vectors are used as the input for a back propagation neural network which is trained to classify the moment invariant feature sets. Based on a proprietary database of 60 ear images, they report a rank-1 performance of 91.8%. In <ref type="bibr" target="#b72">[73]</ref> Wang and Yuan compare the distinctiveness of different feature extraction methods on the USTB I database. They compare the rank-1 performance of Fourier descriptors, Gabor transform, moment invariants and statistical features and come to the conclusion that the highest recognition rate can be achieved by using moment invariants and Gabor transform. For both feature extraction methods Wang and Yuan report a rank-1 performance or 100%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Local descriptors</head><p>Scale invariant feature transform (SIFT) is known to be a robust way for landmark extraction even in images with small pose variations and varying brightness conditions <ref type="bibr" target="#b81">[82]</ref>. SIFT landmarks contain a measure for local orientation; they can also be used for estimating the rotation and translation between two normalised ear images. Bustard et al. showed that SIFT can handle pose variations up to 208 <ref type="bibr" target="#b74">[75]</ref>. However it is not a trivial to assign a SIFT landmark with its exact counterpart, especially in the presence of pose variations. In highly structured image regions, the density and redundancy of SIFT landmarks is so high, that exact assignment is not possible. Hence the landmarks have to be filtered before the actual comparison can start. Arbab-Zavar et al. <ref type="bibr" target="#b57">[58]</ref> as well as Badrinath and Gupta <ref type="bibr" target="#b70">[71]</ref> therefore train a reference landmark model, which only contains a small number of non-redundant landmarks. This landmark model is used for filtering the SIFT landmarks, which were initially detected in the probe and reference ear. Having the filtered landmarks it is possible to assign each of the landmarks with its matching counterpart. Fig. <ref type="figure">7b</ref> shows an example for SIFT landmarks extracted from an ear images, which were used as training data for the reference landmark model in the work of Arbab-Zavar et al. As Arbab-Zavar et al. also used the XM2VTS database for evaluation, their results can be directly compared to the rank-1 performance reported by Bustard and Nixon. Arbab-Zavar et al. achieved a rank-1 performance of 91.5%. With the more recent approach by Bustard and Nixon the performance could be improved to 96%. Using the IIT Delhi database Kumar et al. report a GAR of 95% and a FAR of 0.1% when using SIFT feature points.</p><p>Kisku et al. address the problem of correct landmark assignment by decomposing the ear image into different colour segments <ref type="bibr" target="#b71">[72]</ref>. SIFT landmarks are extracted from each segment separately, which reduces the chance of assigning SIFT landmarks that are not representing the same features. Using this approach, Kisku et al. achieve a rank-1 performance of 96.93%.</p><p>A recent approach by Prakash and Gupta <ref type="bibr" target="#b21">[22]</ref> fuses speeded up robust features (SURF) <ref type="bibr" target="#b82">[83]</ref> feature points from different images of the same subject. They propose to use several input images for enrolment and to store all SURF feature points in the fused feature vector, which could be found in the input images. These feature sets are then used for training a nearest-neighbour classifier for assigning two correlated feature points. If the distance between two SURF feature points is less than a trained threshold, they are considered to be correlated. The evaluation of this approach was carried out on the UND collection E and the two subsets of the IIT Kanpur database. Prakash and Gupta tested the influence of different parameters for SURF features and for the nearest-neighbour classifier. Depending on the composition of the parameters the EER varies between 6.72 and 2.25%.</p><p>Choras proposes a set of geometric feature extraction methods inspired by the work of Iannarelli <ref type="bibr" target="#b62">[63]</ref>. He proposes four different ways of feature location in edge images. The concentric circles method uses the concha as reference points for a number of concentric circles with predefined radii. The intersection points of the circles and the ear contours are used as feature points (see Fig. <ref type="figure">7a.</ref>). An extension of this is the contour tracing methods, which uses bifurcations, endpoints and intersecting points between the ear contours as additional features. In the angle representation approach, Choras draws concentric circles around each centre point of an edge and uses the angles between the centre point and the concentric circles intersecting points for feature representation. Finally, the triangle ratio method determines the normalised distances between reference points and uses them for ear description. Choras conducted studies on different databases where he reported recognition rates between 86.2 and 100% on a small database off 12 subjects and a false reject rate between 0 and 9.6% on a larger database with 102 ear images.</p><p>Similar approaches that are using the aspect ratio between reference points on the ear contours are proposed by Mu et al. with a rank-1 performance of 85% on the USTB II database <ref type="bibr" target="#b53">[54]</ref> and Rahman et al. <ref type="bibr" target="#b60">[61]</ref>. Rahman et al. evaluated their approach on a database, which consists of 350 images from 100 subjects. They report a rank-1 performance of 90%. For images, which were taken on different days, the rank-1 performance dropped to 88%.</p><p>Local binary patterns (LBP) are a technique for feature extraction on the pixel level. LBP encode the local neighbourhood of a pixel by storing the difference between the examined pixel and its neighbours. Guo et al. <ref type="bibr" target="#b64">[65]</ref> extract LBP from the raw ear images and create histograms describing the distribution of the local LBP. Then a cellular neural network of trained to distinguish between the LBP of different subjects in the USTB II database.</p><p>In the by Wang and Yan <ref type="bibr" target="#b80">[81]</ref> the dimensionality of the feature vector is reduced with linear discriminant analysis before a Euclidean distance measure quantifies the similarity of two feature vectors. Wang and Yan evaluated their approach on the USTB II dataset and report a rank-1 performance of 100%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Hybrid approaches</head><p>The approach of Jedges and Mate is twofold <ref type="bibr" target="#b27">[28]</ref>. In a first feature extraction step they generate an average edge model from a set of training images. These edges represent the outer helix contour as well as the contours of the antihelix, the fossa triangularis and the concha. Subsequently, each image is enrolled by deforming the ear model until it fits the actual edges displayed in the probe ear image. The deformation parameters, which were necessary for the transformation, are the flrst part of the feature vector. The feature vector is completed by adding additional feature points lying of intersections between a predefined set of axes and the transformed main edges. The axes describe the unique outline of ear. Fig. <ref type="figure">7c</ref> shows the edge enhanced images with fitted contours together with the additional axes for reference point extraction. They report an EER of 5.6% using a database with cropped images and without pose variations.</p><p>Liu et al. combine front and backside view of the ear by extracting features using the triangle ratio method and Tchebichef moment descriptors <ref type="bibr" target="#b58">[59]</ref>. Tchebichef moments are a set of orthogonal moment functions based on discrete Tchebichef polynomials and have been introduced as a method for feature representation in 2001 <ref type="bibr" target="#b83">[84]</ref>. The backside of the ear is described by a number of lines that are perpendicular to the longest axis in the ear contour. These lines measure the local diameter of the auricle at predefined points. The rank-1 performance of this combined approach is reported to be 97.5%. If only the front view is used, the rank-1 performance is 95% and for the backside images, Liu et al. report 86.3% rank-1 performance.</p><p>Lu et al. <ref type="bibr" target="#b55">[56]</ref> as well as Yuan and Mu <ref type="bibr" target="#b46">[47]</ref> use the active shape model for extracting the outline of the ear. Lu et al. are using manually cropped ear images from 56 subjects in different poses. A feature extractor stores selected points on the outline of the ear together with their distance to the tragus. Before applying a linear classifier, the dimensionality of the feature vectors is reduced by principal component analysis (PCA). Lu et al. compare the rank-1 performance of pipelines where only the left or the right ear was used for identification and also show that using both ears increases the rank-1 performance from 93.3 to 95.1%. In the USTB III database Yuan and Mu report a rank-1 performance of 90% is the head rotation is lower than 158. For rotation angles between 20 and 608 the rank-1 performance drops to 80%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Classifiers and statistical approaches</head><p>Victor et al. were the first research group to transfer the idea of using the eigenspace from face recognition to ear recognition <ref type="bibr" target="#b50">[51]</ref>. They reported that the performance of the ear as a feature is inferior to the face. This may be due to the fact that in their experiments Victor et al. considered the left and the right ear to be symmetric. They used the one ear for training and the other ear for testing, which could have lowered the performance of PCA in this case. The reported rank-1 performance is 40%. With a rank-1 performance of 72.2% in the UND collection E, Chang et al. <ref type="bibr" target="#b51">[52]</ref> report a significantly better performance than Victor et al. Alaraj et al. <ref type="bibr" target="#b73">[74]</ref> published another study, where PCA is used for feature representation in ear recognition. In their approach a multilayer feed forward neural network was trained for classification of the PCA-based feature components. The observed a rank-1 performance of 96%, and hence improved the previous results by Victor et al. and Chang et al. However it should be noticed that this result is only based on a subset of one of the UND collections, which consists of 85 ear images from 17 subjects.</p><p>Zhang and Mu conducted studies on the effectiveness of statistical methods in combination with classifiers. In <ref type="bibr" target="#b69">[70]</ref> they show that independent component analysis (ICA) is more effective on the USTB I database than PCA. They first used PCA and ICA for reducing the dimensionality of the input images and then trained an SVM for classifying the extracted feature vectors. Furthermore, the influence of different training set sizes on the performance was measured. Depending on the site of the training set the rank-1 performance for PCA varies between 85 and 94.12%, whereas the rank-1 performance for ICA varies between 91.67 and 100%.</p><p>Xie and Mu <ref type="bibr" target="#b67">[68]</ref> propose an improved locally linear embedding (LLE) algorithm for reducing the dimensionality of ear features. LLE is a technique for projecting highdimensional data points into a lower-dimensional coordinate system while preserving the relationship between the single data points. This requires the data points to be labelled in some way, so that their relationship is fixed. The improved version on LLE by Xie and Mu eliminated the problem by using a different distance function. Further Xie and Mu show that LLE is superior to PCA and Kernel PCA, if the input data contain pose variations. Their studies were conducted on the USTB III database showed that the rank-1 performance of regular LLE (43%) is improved significantly by their method to 60.75%. If the pose variation is only 108, the improved LLE approach achieved a rank-1 performance of 90%.</p><p>In their approach Nanni and Lumini <ref type="bibr" target="#b59">[60]</ref> propose to use sequential forward floating selection (SFFS), which is a statistical iterative method for feature selection in pattern recognition tasks. SFFS tries to find the best set of classifiers by creating a set of rules, which best fits the current feature set. The sets are created by adding one classifier at a time and evaluating its discriminative power with a predefined fitness function. If the new set of rules outperforms the previous version, the new rule is added to the final set of rules. The experiments were carried out on the UND collection E and the single classifiers are fused by using the weighted sum rule. SFFS selects the most discriminative sub-windows which correspond to the fittest set of rules. Nanni and Lumini report a rank-1 recognition rate of 80% and a rank-5 recognition rate of 93%. The EER varies between 6.07 and 4.05% depending on the number of sub-windows used for recognition.</p><p>Yiuzono et al. consider the problem of finding corresponding features in ear images as an optimisation problem and apply genetic local search for solving it iteratively <ref type="bibr" target="#b49">[50]</ref>. They select local subwindows with varying size as the basis for the genetic selection. In <ref type="bibr" target="#b49">[50]</ref> Yiuzono et al. present elaborated results, which describe the behaviour of genetic local search under different parameters, such as different selection methods and different numbers of chromosomes. On a database of 110 subjects they report a recognition rate of 100%. Yaqubi et al. use features obtained by a combination of position and scale-tolerant edge detectors over multiple positions and orientations of the image <ref type="bibr" target="#b68">[69]</ref>. This feature extraction method is called HMAX model and is inspired by the visual cortex of primates and combines simple features to more complex semantic entities. The extracted features are classified with an SVN and a kNN. The rank-1 performance on a small dataset of 180 cropped ear images from six subjects varies between 62 and 100% depending on the kind of basis features.</p><p>Moreno et al. implement a feature extractor, which locates seven landmarks on the ear image, which correspond to the salient points from the work of Iannarelli. Additionally, they obtain a morphology vector, which describes the ear as a whole. These two features are used as the input for different neural network classifiers. They compare the performance of each of the single feature extraction techniques with different fusion methods. The proprietary test database is composed of manually cropped ears from 168 from 28 subjects. The best result of 93% rank-1 performance was measures using a compression network. Other configurations yielded error rates between 16 and 57%.</p><p>Gutierrez et al. <ref type="bibr" target="#b76">[77]</ref> divide the cropped ear images into three equally sized parts. The upper part shows the helix, the middle part shows the concha and the lower part shows the lobule. Each of these subimages is decomposed by wavelet transform and then fed into a modular neural network. In each module of the network a different integrators and learning functions was used. The results of each of the modules are fused in the last step for obtaining the final decision. Depending on the combination between integrator and learning function, the results vary between 88.4 and 97.47% rank-1 performance on the USTB I database. The highest rank-1 performance is achieved with Sugeno measure and conjugate gradient.</p><p>In <ref type="bibr" target="#b65">[66]</ref> Nasseem et al. propose a general classification algorithm based on the theory of compressive sensing. They assume that most signals are compressible in nature and that any compression function results in a sparse representation of this signal. In their experiments in the UND database and the FEUD database, Nasseem et al.</p><p>show that their sparse representation method is robust against pose variations and varying lighting conditions. The rank-1 performance varied between 89.13 and 97.83%, depending on the dataset used in the experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">3D Ear recognition</head><p>In 2D ear recognition pose variation and variation in camera position, so-called out-of-plane-rotations, are still unsolved challenges. A possible solution is using 3D models instead of photos as references, because a 3D representation of the subject can be adapted to any rotation, scale and translation. In addition to that, the depth information contained in 3D models can be used for enhancing the accuracy of an ear recognition system. However, most 3D ear recognition systems tend to be computationally expensive. In Table <ref type="table" target="#tab_3">4</ref> all 3D ear recognition systems described in this section are summarised.</p><p>Although ICP is originally designed to be an approach for image registration, the registration error can also be used as a measure for the dissimilarity of two 3D images. As ICP is designed to be a registration algorithm, it is robust against all kinds of translation or rotations. However ICP tends to stop too early, because it gets stuck in local minima. Therefore ICP requires the two models to be coarsely pre-aligned before fine alignment using ICP can be performed. Chen and Bhanu extract point clouds from the contour of the outer helix and the register these points with the reference model by using ICP <ref type="bibr" target="#b29">[30]</ref>. In a later approach Chen and Bhanu use local surface patches (LSP) instead of points lying on the outer helix <ref type="bibr" target="#b28">[29]</ref>. As the LSP consist or less points than the outer helix, this reduces the processing time while enhancing the rank-1 performance from 93.3% with the outer helix points to 96.63% with LSP.</p><p>Yan and Browyer decompose the ear model into voxels and extract surface features from each of these voxels. For speeding up the alignment process, each voxel is assigned an index in such a way that ICP only needs to align voxel pairs with the same index <ref type="bibr" target="#b89">[90]</ref> (see Fig. <ref type="figure">8</ref>). In <ref type="bibr" target="#b17">[18]</ref> Yan and Browyer propose the usage of point clouds for 3D ear recognition. In contrast to <ref type="bibr" target="#b29">[30]</ref> all points of the segmented ear model are used. The reported performance measures of 97.3% in <ref type="bibr" target="#b89">[90]</ref> and 97.8% in <ref type="bibr" target="#b17">[18]</ref> are similar but not directly comparable, because different datasets were used for evaluation.</p><p>Cadavid et al. propose a real-time ear recognition system, which reconstructs 3D models from 2D CCTV images using the shape from shading technique <ref type="bibr" target="#b92">[93]</ref>. Thereafter the 3D model in compared to the reference 3D images, which are stored in the gallery. Model alignment as well as the computation of the dissimilarity measure is done by ICP. Cadavid et al. report a recognition rate of 95% on a  <ref type="bibr" target="#b28">[29]</ref> local surface patch 302 604 96.36% Chen and Bhanu <ref type="bibr" target="#b29">[30]</ref> ICP contour matching 52 213 93.3% Liu et al. <ref type="bibr" target="#b58">[59]</ref> mesh PCA with neural network 60 600 NA Liu and Zhang <ref type="bibr" target="#b85">[86]</ref> slice curve matching 50 200 94.5% Islam et al. <ref type="bibr" target="#b86">[87]</ref> ICP with reduced meshes 415 830 93.98% Islam et al. <ref type="bibr" target="#b87">[88]</ref> local surface features with ICP-matching 415 830 93.5% Passalis et al. <ref type="bibr" target="#b88">[89]</ref> reference ear model with morphing 525 1031 94.4% Yan and Bowyer <ref type="bibr" target="#b89">[90]</ref> ICP using voxels 369 738 97.3% Yan and Bowyer <ref type="bibr" target="#b17">[18]</ref> ICP using model points 415 1386 97.8% Zheng et al. <ref type="bibr" target="#b90">[91]</ref> local binary patters 415 830 96.39% Zhou et al. <ref type="bibr" target="#b91">[92]</ref> surface patch histogram and voxelisation 415 830 98.6%, 1.6% EER database of 402 subjects. It is stated in <ref type="bibr" target="#b92">[93]</ref> that the approach has difficulties with pose variations. In <ref type="bibr" target="#b91">[92]</ref>   <ref type="bibr" target="#b58">[59]</ref>. Based on the two images of a stereo vision camera, a 3D representation of the ear is derived. Subsequently, the resulting 3D meshes serve as the input for PCA. However, Liu et al. do not provide any results concerning the accuracy of their system but since they did not publish any further results on their PCA mesh approach, it seems that it is no longer pursued.</p><p>Passalis et al. go on a different way for comparing 3D ear models in order to make comparison suitable for a real-time system <ref type="bibr" target="#b88">[89]</ref>. They compute a reference ear model which is representative for the average human ear. During enrolment, all reference models are deformed until they fit the reference ear model. All translations and deformations, which were necessary to fit the ear to the reference model are then stored as features. If a probe for authentication is given to the system, the model is also adapted to the annotated ear model in order to obtain the deformation data. Subsequently, the deformation data are used to search for an associated reference model in the gallery. In contrast to the previously described systems, only one deformation has to be computed per authentication attempt. All other deformation models can be computed before the actual identification process is started. This approach is reported to be suitable for real-time recognition systems, because it takes less than 1 ms for comparing two ear templates. The increased computing speed is achieved by lowering the complexity class from O(n) 2 for ICP-based approaches to O(n) for their approach. The rank-1 recognition rate is reported to be 94.4%. The evaluation is based on nonpublic data, which was collected using different sensors.</p><p>Heng and Zhang propose a feature extraction algorithm based on slice curve comparison, which is inspired by the principles of computer tomography <ref type="bibr" target="#b85">[86]</ref>. In their approach the 3D ear model is decomposed into slices along the orthogonal axis of the longest distance between the lobule and the uppermost part of the helix. The curvature information extracted from each slice is stored in a feature vector together with an index value indicating the slice's former position in the 3D model. For comparison the longest common sequence between two slice curves with similar indexes is determined. Their approach is only evaluated on a non-public dataset, which consists of 200 images from 50 subjects. No information about pose variations or occlusion during the capturing experiment is given. Heng and Zhang report a rank-1 performance is 94.5% for the identification experiment and 4.6% EER for the verification experiment. Islam et al. <ref type="bibr" target="#b86">[87]</ref> reconnect point clouds describing 3D ear models to meshes and iteratively reduce the number of faces in the mesh. These simplified meshes are then aligned with each other using ICP and the alignment error is used as the similarity measure for the two simplified meshes. In a later approach Islam et al. <ref type="bibr" target="#b44">[45]</ref> extract LSP as shown in Fig. <ref type="figure">9</ref> and use them as features.</p><p>Fig. <ref type="figure">8</ref> Examples for surface features in 3D ear images Upper image shows an example for ICP-based comparison as proposed in <ref type="bibr" target="#b29">[30]</ref>, whereas the lower figure illustrates feature extraction from voxels as described in <ref type="bibr" target="#b89">[90]</ref> Fig. <ref type="figure">9</ref> Example for local surface (LSP) patch features as proposed in <ref type="bibr" target="#b44">[45]</ref> For extracting those LSP, a number of points are selected randomly from the 3D model. Then the data points that are closer to the seed point than a defined radius are selected. PCA is then applied to find the most descriptive features in the LSP. The feature extractor repeats selecting LSP until the desired number of features has been found. Both approaches were evaluated using images from UND. The recognition rate reported for <ref type="bibr" target="#b86">[87]</ref> is 93.98% and the recognition rate reported for <ref type="bibr" target="#b44">[45]</ref> is 93.5%. However, none of the approaches has been tested with pose variation and different scaling.</p><p>Zheng et al. <ref type="bibr" target="#b90">[91]</ref> extract the shape index at each point in the 3D model and use it for projecting the 3D model to 2D space. The 3D shape index at each pixel is represented by a grey value at the corresponding position in the 2D image. Then SIFT features are extracted from the shape index map. For each of the SIFT points a local coordinate system is calculated where the z-axis correspondents to the feature point's normal.</p><p>Hence the z values of the input image are normalised according to the normal of the SIFT feature point they were assigned to. As soon as the z values have been normalised, they are transformed into a grey level image. As a result, Zheng et al. obtain a local grey level image for each of the selected SIFT features. Next, LBP are extracted for feature representation in each of these local grey level images. Comparison is first performed by coarsely comparing the shape indexes of key pints and then using Earth mover's distance for comparing LBP histograms from the corresponding normalised grey images. Zheng et al. evaluated their approach on a subset of the UND-J2 collected and achieved a rank-1 performance of 96.39%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Open challenges and future applications</head><p>As the most recent publications on 2D and 3D ear recognition show, the main application of this technique is personal identification in unconstrained environments. This includes applications for smart surveillance, such as in <ref type="bibr" target="#b84">[85]</ref> but also the forensic identification of perpetrators on CCTV images or for border control systems. Traditionally, these application fields are part of face recognition systems but as the ear is located next to the face, it can provide valuable additional information to supplement the facial images.</p><p>Multi-modal ear and face recognition systems can serve as a means of achieving pose invariance and more robustness against occlusion in unconstrained environments. In most public venues surveillance cameras are located overhead in order to capture as many persons as possible and to protect them from vandalism. In addition, most of the persons will not look straight into the camera, so in most cases no frontal images of the persons will be available. This fact poses serious problems to biometric systems, using facial features for identification. If the face is not visible from a frontal angle, the ear can serve as a valuable additional characteristic in these scenarios.</p><p>Owing to the physical proximity of the face and the ear, there are also many possibilities for the biometric fusion of these two modalities. Face and ear images can be fused on the feature level, on the template level and on the score level. Against the background of this application, there are some unsolved challenges, which should be addressed by future research in this field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Automatic ear localisation</head><p>The fact that many systems presented in literature use presegmented ear images shows that the automatic detection of ears especially in real-life images is still an unsolved problem. If ear recognition systems should be implemented in automatic identification systems, fast and reliable approaches for automatic ear detection are of importance. As a first step towards this goal, some research groups have published data collections, which simulate typical variations in uncontrolled environments such as varying lighting conditions, poses and occlusion. Based on these datasets, existing and future approaches to ear recognition should be tested under realistic conditions in order to improve their reliability.</p><p>Moreover, 3D imaging systems become increasingly cheap in the last years. Consequently, 3D ear recognition becomes important and with it the need of locating ears in depth images or 3D models. Currently, only one approach for ear detection in depth has been published, which is a first step towards ear detection in 3D images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Occlusion and pose variations</head><p>In contrast to the face, the ear can be partially or fully covered by hair or by other items such as headdresses, hearing aids, jewelry or headphones. Owing to the convex surface of the outer ear, parts of it may also be occluded if the subject's pose changes. In some publications, robustness against occlusion is explicitly addressed, but there are no studies on the effect of certain types of occlusion like hair or earrings on the recognition rate of an ear recognition system. Once more, the availability of public databases that contain occluded ear images is likely to foster the development of solutions for pose invariant and robust algorithms for ear detection and feature extraction.</p><p>Moreover to our best knowledge, there are no studies about the visibility of the outer ear in different public environments. In order to develop algorithms for ear detection and recognition, further information about commonly occluded party of the ear is needed.</p><p>Occlusion owing to pose variations is another unmet challenge in ear recognition system. Similarly to face recognition, parts of the ear can become occluded if the pose changes. Recently, some feature extraction methods have been proposed, which are robust against pose variations to some degree. However, this issue is not fully solved yet. Another possibility for compensating pose variations could be the usage of 3D models instead of depth images of photographs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Scalability</head><p>Currently, available databases only consist of less than 10 000 ear images. The only exception is the USTB IV collection, which has not been released for the public yet. In realistic environments the size of the database will be significantly larger, which makes exhaustive search in identification scenarios infeasible. Therefore not only the accuracy but also the comparison speed of ear recognition systems will be interesting for future research.</p><p>In order to make ear recognition applicable for large-scale systems, exhaustive searches should be replaced by appropriate data structures allowing logarithmic time complexity during the search. This could, for example, be achieved by exploring the possibilities of organising ear templates in search trees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Understanding symmetry and ageing</head><p>As ear recognition is one of the newer fields of biometric research, the symmetry of the left and the right ear has not been fully understood yet. The studies of Iannarelli indicate that some characteristics of the outer ear can be inherited and ageing slightly affects the appearance of the outer ear. Both assumptions could be confirmed in more recent studies, but because of a lack of sufficient data, the effect of inheritance and ageing on the outer ear's appearance is not fully understood yet. Furthermore, there are no large scale studies of the symmetry relation between the left and the right ear yet.</p><p>Therefore another interesting field for future research could be to gain a deeper understanding of the effect of inheritance any symmetry on the distinctiveness of biometric template. Moreover, long-term studies on the effect of time on ear templates are needed in order to obtain a better understanding of the permanence of this characteristic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Summary</head><p>We have presented a survey on the state of the art in 2D and 3D ar biometrics, covering ear detection and ear recognition systems. We categorised the large number of 2D ear recognition approaches into holistic, local, hybrid and statistical methods, discussed their characteristics and reported their performance.</p><p>Ear recognition is still a new field of research. Although there is a number of promising approaches, none of them have been evaluated under realistic scenarios, which include disruptive factors like pose variations, occlusion and varying lighting conditions. In recent approaches, these factors are taken under account, but more research on this is required until ear recognition systems can be used in practice. The availability of suitable test databases, which were collected under realistic scenarios, will further contribute to the maturation of the ear as a biometric characteristic.</p><p>We have collected a structured survey of available databases, existing ear detection and recognition approaches and unsolved problems for ear recognition in the context of smart surveillance system, which we consider to be the most important application for ear biometrics. We think that this new characteristic is a valuable extension for face recognition systems on the way to pose invariant automatic identification.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>†</head><label></label><figDesc>Collection F: Around 942 3D (depth images) and corresponding 2D profile images from 302 human subjects, captured in 2003 and 2004. † Collection G: Around 738 3D (depth images) and corresponding 2D profile images from 235 human subjects, captured between 2003 and 2005. † Collection J2: Around 1800 3D (depth images) and corresponding 2D profile images from 415 human subjects, captured between 2003 and 2005 [18].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1</head><label>1</label><figDesc>Fig. 1 Characteristics of the human ear the German criminal police uses for personal identification of suspects</figDesc><graphic coords="2,51.02,56.69,224.63,148.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3</head><label>3</label><figDesc>Fig. 3 Example images fron IIT Delhi ear database [21] a Example 1 b Example 2 c Example 3</figDesc><graphic coords="3,311.81,56.69,236.64,61.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5</head><label>5</label><figDesc>Fig. 5 Some example images from the NKCU face database, showing the same subject at different angles a 40 degrees rotation b 65 degrees rotation c 90 degrees rotation</figDesc><graphic coords="4,119.06,314.70,356.66,92.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4</head><label>4</label><figDesc>Fig. 4 SCface example images [23] a Half left profile b Full left profile c Full right profile These images show examples for the photographed pictures, not for the pictures collected with the surveillance camera system</figDesc><graphic coords="4,119.06,56.69,356.16,174.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Abaza</head><label></label><figDesc>et al.<ref type="bibr" target="#b32">[33]</ref> and Islam et al.<ref type="bibr" target="#b44">[45]</ref> use weak classifiers based on Haar-wavelets in connection with AdaBoost for ear localisation. According to Islam et al. the training of the classifier takes several days, however once the classifier is set up, ear detection is fast and effective. Abaza et al. use a modified version of AdaBoost and report a significantly shorter training phase. The effectiveness of their approach is proved in evaluations on five different databases. They also include some examples of successful detections on images from the internet. As long as the subject's pose does not change, weak classifiers are suitable for images which contain more than one subject. Depending on the test set Abaza et al. achieved a detection rate between 84 and 98.7% on the Sheffield Face database. On average, their approach successfully detected 95% of all ears.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6</head><label>6</label><figDesc>Fig. 6 Examples for different ear detection techniques a Original image, edge image and Hough transform<ref type="bibr" target="#b25">[26]</ref> b Original image, and ray transform<ref type="bibr" target="#b26">[27]</ref> c Original image, edge enhanced image and corresponding edge orientation model<ref type="bibr" target="#b27">[28]</ref> </figDesc><graphic coords="6,119.06,414.60,356.67,315.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="12,119.06,591.19,356.62,169.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Summary of automatic ear detection methods for 2D and 3D images</figDesc><table><row><cell>Publication</cell><cell>Detection method</cell><cell>Database</cell><cell>Perf.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Summary of approaches for 2D ear recognition approaches, part 1</figDesc><table><row><cell>Publication</cell><cell>Summary</cell><cell>Database</cell><cell></cell><cell>Perf.</cell></row><row><cell></cell><cell></cell><cell>#Subj</cell><cell># Img</cell><cell></cell></row><row><cell>Burge and Burger [46]</cell><cell>Vornoi distance graphs</cell><cell>NA</cell><cell>NA</cell><cell>NA</cell></row><row><cell>Yuan and Mu [47]</cell><cell>full space LDA with outer helix feature points</cell><cell>79</cell><cell>1501</cell><cell>86.76%</cell></row><row><cell>Hurley [48]</cell><cell>force field transform</cell><cell>63</cell><cell>252</cell><cell>99%</cell></row><row><cell>Moreno et al. [49]</cell><cell>geometric features with compression network</cell><cell>28</cell><cell>268</cell><cell>93%</cell></row><row><cell>Yuizono et al. [50]</cell><cell>genetic local search</cell><cell>110</cell><cell>660</cell><cell>99%</cell></row><row><cell>Victor et al. [51]</cell><cell>PCA</cell><cell>294</cell><cell>808</cell><cell>40%</cell></row><row><cell>Chang et al. [52]</cell><cell>PCA</cell><cell>114</cell><cell>464</cell><cell>72.7%</cell></row><row><cell>Abdel-Mottaleb and Zhou [53]</cell><cell>modified force field transform</cell><cell>29</cell><cell>58</cell><cell>87.9%</cell></row><row><cell>Mu et al. [54]</cell><cell>geometrical measures on edge images</cell><cell>77</cell><cell>308</cell><cell>85%</cell></row><row><cell>Abate et al. [55]</cell><cell>general Fourier descriptor</cell><cell>70</cell><cell>210</cell><cell>88%</cell></row><row><cell>Lu et al. [56]</cell><cell>active shape model and PCA</cell><cell>56</cell><cell>560</cell><cell>93.3%</cell></row><row><cell>Yuan et al. [57]</cell><cell>non-negative matrix factorisation</cell><cell>77</cell><cell>308</cell><cell>91%</cell></row><row><cell>Arbab-Zavar et al. [58]</cell><cell>SIFT points from ear model</cell><cell>63</cell><cell>252</cell><cell>91.5%</cell></row><row><cell>Jedges and Mate</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc>Summary of approaches for 2D ear recognition approaches, part 2</figDesc><table><row><cell>Publication</cell><cell>Summary</cell><cell cols="2">Database</cell><cell>Perf.</cell></row><row><cell></cell><cell></cell><cell>#Subj</cell><cell># Img</cell><cell></cell></row><row><cell>Dong and Mu [64]</cell><cell>force field transform and NKFDA</cell><cell>29</cell><cell></cell><cell>75.3%</cell></row><row><cell>Guo and Xu [65]</cell><cell>local binary pattern and CNN</cell><cell>77</cell><cell></cell><cell>93.3%</cell></row><row><cell>Nasseem et al. [66]</cell><cell>sparse representation</cell><cell>32</cell><cell></cell><cell>96.88%</cell></row><row><cell>Wang et al. [67]</cell><cell>Haar wavelets and LBP</cell><cell>79</cell><cell></cell><cell>92.41%</cell></row><row><cell>Xie and Mu [68]</cell><cell>LLE</cell><cell>79</cell><cell>1501</cell><cell>80%</cell></row><row><cell>Yaqubi et al. [69]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc>Summary of approaches for 3D ear recognition</figDesc><table><row><cell>Publication</cell><cell>Comparison method</cell><cell>Database</cell><cell></cell><cell>Perf.</cell></row><row><cell></cell><cell></cell><cell>#Subj</cell><cell>#Img</cell><cell></cell></row><row><cell>Cadavid et al. [85]</cell><cell>ICP and shape from shading</cell><cell>462</cell><cell>NA</cell><cell>95%</cell></row><row><cell>Chen and Bannu</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Zhou et al. use a combination of local histogram features voxel models. Zhou et al. report that their approach is faster and with an EER of 1.6%, it is also more accurate than the ICP-based comparison algorithms proposed by Chen and Bhanu and Yan and Browyer. Simlarly to Cadavid et al. Liu et al. reconstruct 3D ear models from 2D views</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>IET Biometrics, 2012, Vol. 1, Iss. 2, pp. 114 -129</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>doi: 10.1049/iet-bmt.2011.0003 &amp; The Institution of Engineering and Technology 2012 www.ietdl.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2"><p>IET Biometrics, 2012, Vol. 1, Iss. 2, pp. 114 -129 &amp; The Institution of Engineering and Technology 2012 doi: 10.1049/iet-bmt.2011.0003 www.ietdl.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_3"><p>&amp; The Institution of Engineering and Technology 2012 doi: 10.1049/iet-bmt.2011.0003 www.ietdl.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_4"><p>&amp; The Institution of Engineering and Technology 2012 www.ietdl.org</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Towards understanding the symmetry of human ears: a biometric perspective</title>
		<author>
			<persName><forename type="first">A</forename><surname>Abaza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth IEEE Int. Conf. on Biometrics: Theory Applications and Systems (BTAS)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Bertillon</surname></persName>
		</author>
		<title level="m">La Photographie Judiciaire: Avec Un Appendice Sur La Classification Et L&apos;Identification Anthropometriques</title>
		<meeting><address><addrLine>Paris</addrLine></address></meeting>
		<imprint>
			<publisher>Gauthier-Villars</publisher>
			<date type="published" when="1890">1890</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Die Bedeutung der Ohrmuschel fu ¨r die Feststellung der Identita ¨t</title>
		<author>
			<persName><forename type="first">R</forename><surname>Imhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Archiv fu ¨r die Kriminologie</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="150" to="163" />
			<date type="published" when="1906">1906</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Ear identification</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Iannarelli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>Paramont Publishing Company</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Exploratory study on classification and individualisation of earprints</title>
		<author>
			<persName><forename type="first">L</forename><surname>Meijerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sholl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">D</forename><surname>Conti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Forensic Sci. Int</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="91" to="99" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Observations of external earAn Indian study</title>
		<author>
			<persName><forename type="first">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Purkait</surname></persName>
		</author>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S0018442X09001164" />
	</analytic>
	<monogr>
		<title level="j">HOMO -J. Compar. Hum. Biol</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="461" to="472" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Age-and sex-related changes in the normal human ear</title>
		<author>
			<persName><forename type="first">C</forename><surname>Sforza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Grandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Binelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Tommasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rosati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">F</forename><surname>Ferrario</surname></persName>
		</author>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S0379073809000966" />
	</analytic>
	<monogr>
		<title level="j">Forensic Sci. Int</title>
		<imprint>
			<biblScope unit="volume">187</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="110" to="e111" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cross-sectional anthropometric study of the external ear</title>
		<author>
			<persName><forename type="first">L</forename><surname>Meijerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Van Der Lugt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J R</forename><surname>Maat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Forensic Sci</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="286" to="293" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The effect of time on ear biometrics</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I S</forename><surname>Ibrahim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Nixon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mahmoodi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Joint Conf. on Biometrics</title>
		<imprint>
			<biblScope unit="page" from="1" to="6" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Ear identification based on surveillance camera images</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Hoogstrate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">V D</forename><surname>Heuvel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Huyben</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Justice</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="167" to="172" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Performance of the FearID earprint identification system</title>
		<author>
			<persName><forename type="first">I</forename><surname>Alberink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ruifrok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Forensic Sci. Int</title>
		<imprint>
			<biblScope unit="volume">166</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="145" to="154" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Advances in computer and information sciences and engineering</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M S</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Davies</surname></persName>
		</author>
		<editor>Sobh, T.</editor>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="509" to="514" />
		</imprint>
	</monogr>
	<note>Biometric approaches of 2D-3D ear and face: a survey</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Image feature extraction methods for ear biometrics -a survey</title>
		<author>
			<persName><forename type="first">M</forename><surname>Choras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth Int. Conf. on Computer Information Systems and Industrial Management Applications, 2007, CISIM&apos;07</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="261" to="265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Recent advances in ear biometrics</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Pun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Moon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Sixth IEEE Int. Conf. on Automatic Face and Gesture Recognition</title>
		<meeting>Sixth IEEE Int. Conf. on Automatic Face and Gesture Recognition</meeting>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
			<biblScope unit="page" from="164" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Ear biometics</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">K</forename><surname>Lammi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
		<respStmt>
			<orgName>Lappeenranta University of Technology, Department of Information Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pattern extraction methods for ear biometricsa survey</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Biologically Inspired Computing</title>
		<imprint>
			<biblScope unit="page" from="1657" to="1660" />
			<date type="published" when="2009">2009. 2009. 2009</date>
		</imprint>
	</monogr>
	<note>NaBIC</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A survey on ear biometrics</title>
		<author>
			<persName><forename type="first">A</forename><surname>Abaza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Herbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A F</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nixon</surname></persName>
		</author>
		<ptr target="http://eprints.ecssoton.ac.uk/22951/" />
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Biometric recognition using 3D ear shape</title>
		<author>
			<persName><forename type="first">P</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1297" to="1308" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Rotated profile signatures for robust 3D feature detection&apos;</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Faltemier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Flynn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">on Automatic Face and Gesture Recognition</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The west pomeranian university of technology ear database a tool for testing biometric algorithms</title>
		<author>
			<persName><forename type="first">D</forename><surname>Frejlichowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tyszkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">LNCS</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Campilho</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Kamel</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">6112</biblScope>
			<biblScope unit="page" from="227" to="234" />
			<date type="published" when="2010">2010</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin/Heidelberg</pubPlace>
		</imprint>
	</monogr>
	<note>Image analysis and recognition</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Automated human identification using ear imaging</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2011.06.005</idno>
		<ptr target="http://dx.doi.org/10.1016/j.patcog.2011.06.005" />
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="956" to="968" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An efficient ear recognition technique invariant to illumination and pose</title>
		<author>
			<persName><forename type="first">S</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">special issue on Signal Processing Applications in Human Computer Interaction</title>
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="38" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">SCface -surveillance cameras face database</title>
		<author>
			<persName><forename type="first">M</forename><surname>Grgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Delac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Grgic</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11042-009-0417-2</idno>
		<ptr target="http://dx.doi.org/10.1007/s11042-009-0417-2" />
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools Appl</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="863" to="879" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A biometric database with rotating head videos and hand-drawn face sketches</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Al Nizami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Adkins-Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=1736406.1736412" />
	</analytic>
	<monogr>
		<title level="m">Proc. Third IEEE Int. Conf. on Biometrics: Theory, Applications and Systems, BTAS&apos;09</title>
		<meeting>Third IEEE Int. Conf. on Biometrics: Theory, Applications and Systems, BTAS&apos;09</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="38" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">UBEAR: A dataset of ear images captured on-the-move in uncontrolled conditions</title>
		<author>
			<persName><forename type="first">R</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hoyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Peixinho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Proenca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics and Identity Management (CIBIM)</title>
		<imprint>
			<biblScope unit="page" from="84" to="90" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On shape-mediated enrolment in ear biometrics</title>
		<author>
			<persName><forename type="first">B</forename><surname>Arbab-Zavar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nixon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in visual computing</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Bebis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Boyle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Parvin</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin/Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">4842</biblScope>
			<biblScope unit="page" from="549" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A novel ray analogy for enrolment of ear biometrics</title>
		<author>
			<persName><forename type="first">H</forename><surname>Alistair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Cummings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Nixon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Carter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth IEEE Int. Conf. on Biometrics: Theory Applications and Systems (BTAS)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Model-based human ear localization and feature extraction</title>
		<author>
			<persName><forename type="first">E</forename><surname>Jeges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IC-MED</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="101" to="112" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Human ear recognition in 3D</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bhanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="718" to="737" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Contour matching for 3D ear recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bhanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Seventh IEEE Workshop on Applications of Computer Vision (WACV/ MOTION)</title>
		<meeting>Seventh IEEE Workshop on Applications of Computer Vision (WACV/ MOTION)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Histograms of categorized shapes for 3D ear detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cadavid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Abdel-Mottaleb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth IEEE Int. Conf. on Biometrics: Theory Applications and Systems (BTAS)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">An efficient technique for ear detection in 3D: invariant to rotation and scale</title>
		<author>
			<persName><forename type="first">S</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Fifth IAPR Int. Conf. on Biometrics</title>
		<imprint>
			<biblScope unit="page">2012</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fast learning ear detection for real-time surveillance</title>
		<author>
			<persName><forename type="first">A</forename><surname>Abaza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A F</forename><surname>Harrison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth IEEE Int. Conf. on Biometrics: Theory Applications and Systems (BTAS 2010)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Localization of ear using outer Helix curve of the ear</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ansari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Computing: Theory and Applications (ICCTA)</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="688" to="692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mazorra</surname></persName>
		</author>
		<title level="m">Fitting ear contour using an ovoid model&apos;. 39th Annual 2005 Int. Carnahan Conf. on Security Technology (CCST&apos;05)</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Robust log-gabor filter for ear biometrics</title>
		<author>
			<persName><forename type="first">B</forename><surname>Arbab-Zavar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nixon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Pattern Recognition (ICPR)</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Advanced concepts for intelligent vision systems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Attarchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Faez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rafiei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">LNCS</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Blanc-Talon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bourennane</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><surname>Philips</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Popescu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Scheunders</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">5259</biblScope>
			<biblScope unit="page" from="1030" to="1037" />
			<date type="published" when="2008">2008</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin/Heidelberg</pubPlace>
		</imprint>
	</monogr>
	<note>A new segmentation approach for ear recognition</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Shape model-based 3D ear detection from side face range images</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bhanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conf. on Computer Vision and Pattern Recognition -Workshops (CVPR)</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page">122</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Fast and fully automatic ear detection using cascaded AdaBoost</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M S</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Davies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Workshop on Applications of Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="1" to="6" />
			<date type="published" when="2008">2008. 2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Automatic ear detection for online biometric applications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hanmandlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kuldeep</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Third National Conf. on Computer Vision, Pattern Recognition, Image Processing and Graphics (NCVPRIPG)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="146" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Improving Adaboost ear detection with skin-color model and multitemplate matching</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Third IEEE Int. Conf. on Computer Science and Information Technology (ICCSIT)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="106" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">An effient ear localization technique</title>
		<author>
			<persName><forename type="first">S</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="38" to="50" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Ear detection based on arc-masking extraction and AdaBoost polling verification</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifth Int. Conf. on Intelligent Information Hiding and Multimedia Signal Processing</title>
		<imprint>
			<publisher>IIH-MSP</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="669" to="672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Ear detection based on skin-color and contour information</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">C</forename><surname>Mu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Machine Learning and Cybernetics</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="2213" to="2217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Efficient detection and recognition of 3D ears</title>
		<author>
			<persName><forename type="first">S</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Davies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page" from="52" to="73" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">13</title>
		<author>
			<persName><forename type="first">M</forename><surname>Burge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Burger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ear biometrics</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Bolle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Pankanti</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="page" from="273" to="285" />
			<date type="published" when="1998">1998</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Ear recognition based on 2D images</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Mu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">First IEEE Int. Conf. on Biometrics: Theory, Applications, and Systems (BTAS)</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Force field energy functionals for image feature extraction</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Hurley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Nixon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Carter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="311" to="317" />
			<date type="published" when="2002">2002, 20</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">On the use of outer ear images for personal identification in security applications</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">J F</forename><surname>Sanchez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 33rd Annual 1999 Int. Carnahan Conf. on Security Technology</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="469" to="476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Study on individual recognition for ear images by using genetic local search</title>
		<author>
			<persName><forename type="first">T</forename><surname>Yuizono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Satoh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nakayama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2002 Congress on Processing Scociety of Japan (IPSJ) Kyushu Chapter Symp</title>
		<meeting>2002 Congress on essing Scociety of Japan (IPSJ) Kyushu Chapter Symp</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="237" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">An evaluation of face and ear biometrics</title>
		<author>
			<persName><forename type="first">B</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixteenth Int. Conf. on Pattern Recognition (ICPR)</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="429" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Comparison and combination of ear and face images in appearance-based biometrics</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Victor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1160" to="1165" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Human ear recognition from face profile images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abdel-Mottaleb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in biometrics</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin/Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">3832</biblScope>
			<biblScope unit="page" from="786" to="792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Shape and structural feature based ear recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">LNCS</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Lai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Tan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Feng</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">3338</biblScope>
			<biblScope unit="page" from="311" to="364" />
			<date type="published" when="2005">2005</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin/Heidelberg</pubPlace>
		</imprint>
	</monogr>
	<note>Advances in biometric person authentication</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Ear recognition by means of a rotation invariant descriptor</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Abate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nappi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Riccio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ricciardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">18th Int. Conf. on Pattern Recognition, ICPR</title>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="437" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Ear recognition based on statistical shape model</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xiaoxun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Youdong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yunde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">First Int. Conf. on Innovative Computing, Information and Control (ICICIC)</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="353" to="356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Ear recognition using improved non-negative matrix factorization</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chun Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">18th Int. Conf. on Pattern Recognition (ICPR)</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="501" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">On model-based analysis of ear biometrics</title>
		<author>
			<persName><forename type="first">B</forename><surname>Arbab-Zavar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Nixon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Hurley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">First IEEE Int. Conf. on Biometrics: Theory, Applications, and Systems</title>
		<imprint>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2007">2007 (BTAS 2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Multi-view ear shape feature extraction and reconstruction</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Third Int. IEEE Conf. on Signal-Image Technologies and Internet-Based System (SITIS)</title>
		<imprint>
			<biblScope unit="page" from="652" to="658" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A multi-matcher for ear authentication</title>
		<author>
			<persName><forename type="first">L</forename><surname>Nanni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lumini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2219" to="2226" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Person identification using ear biometrics</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">I</forename><surname>Bhuiyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Islam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Internet Manage</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Ear biometrics: a new approach&apos; (Advances in Pattern Recognition</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P R</forename><surname>Sana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Perspective methods of human identification: ear biometrics</title>
		<author>
			<persName><forename type="first">M</forename><surname>Choras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Opto-Electron. Rev</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="85" to="96" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Multi-pose ear recognition based on force field transformation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Mu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Second Int. Symp. on Intelligent Information Technology Application (IITA)</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="771" to="775" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Ear recognition using a new local matching approach</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifteenth IEEE Int. Conf. on Image Processing</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="289" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Sparse representation for ear biometrics</title>
		<author>
			<persName><forename type="first">I</forename><surname>Naseem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Togneri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in visual computing</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Bebis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Boyle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Parvin</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin/Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">5359</biblScope>
			<biblScope unit="page" from="336" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Block-based and multi-resolition methods for ear recognition using Walelste transform and uniform local binary patterns&apos;. 19th Int</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chun Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. on Pattern Recognition (ICPR)</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Ear recognition using LLE and IDLLE algorithm&apos;. 19th Int</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Mu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. on Pattern Recognition (ICPR)</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Ear recognition using features inspired by visual cortex and support vector machine technique</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yaqubi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Faez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Motamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. on Computer and Communication Engineering (ICCCE)</title>
		<imprint>
			<biblScope unit="page" from="533" to="537" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Compound structure classifier system for ear recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
			<affiliation>
				<orgName type="collaboration">ICAL</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Mu</surname></persName>
			<affiliation>
				<orgName type="collaboration">ICAL</orgName>
			</affiliation>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Int. Conf. on Automation and Logistics</title>
		<imprint>
			<biblScope unit="page" from="2306" to="2309" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Feature level fused ear biometric system</title>
		<author>
			<persName><forename type="first">G</forename><surname>Badrinath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Seventh Int. Conf. on Advances in Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="197" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">SIFT-based ear recognition by fusion of detected keypoints from color similarity slice regions</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Kisku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mehrotra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Sing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Advances in Computational Tools for Engineering Applications</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="380" to="385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Human ear recognition based on block segmentation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Cyber-Enabled Distributed Computing and Knowledge Discovery</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="262" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">A neural network based human identification frame-work using ear images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Alaraj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Fukami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TENCON 2010-2010 IEEE Region 10 Conf</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Toward unconstrained ear recognition from two-dimensional images</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Bustard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Nixon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Syst. Man Cybern. A Syst. Hum</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page">486</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">HERO: human ear recognition against occlusions</title>
		<author>
			<persName><forename type="first">M</forename><surname>De Marsico</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Michele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Riccio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conf. on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page">178</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Modular neural network integrator for human recognition from ear images</title>
		<author>
			<persName><forename type="first">L</forename><surname>Gutierrez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Melin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 Int. Joint Conf. on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">The research of ear identification based on improved algorithm of moment invariants</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Third Int. Conf. on Information and Computing (ICIC)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page">58</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Gabor wavelets and general discriminant analysis for ear recogniton</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eighth World Congress on Intelligent Control and Automation (WCICA)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page">6305</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Ear based personal identification approach forensic science tasks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fooprateepsiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Kurutach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chiang Mai J. Sci</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="166" to="175" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Multi-scale feature extraction algorithm of ear image&apos;</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">D</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. on Electric Information and Control Engineering (ICEICE)</title>
		<imprint>
			<biblScope unit="page">528</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Object recognition from local scale-invariant features</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Computer Vision (ICCV 1999)</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1150" to="1157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">SURF: speeded up robust features</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Ninth European Conf. on Computer Vision</title>
		<meeting>Ninth European Conf. on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Image analysis by Tchebichef moments</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mukundan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1357" to="1364" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">3D ear modeling and recognition from video sequences using shape from shading&apos;. 19th Int</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cadavid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Abdel-Mottaleb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. on Pattern Recognition (ICPR)</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Fast 3D point cloud ear identification by slice curve matching</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Third Int. Conf. on Computer Research and Development (ICCRD)</title>
		<imprint>
			<biblScope unit="page">224</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">A fully automatic approach for human recognition from profile images using 2D and 3D ear data</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M S</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Mian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Davies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3DPVT -the Fourth Int. Symp. on 3D Data Processing, Visualization and Transmission</title>
		<meeting>3DPVT -the Fourth Int. Symp. on 3D Data essing, Visualization and Transmission</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">A fast and fully automatic ear recognition approach based on 3D local surface features</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Davies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A S</forename><surname>Mian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bennamoun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 10th Int. Conf. on Advanced Concepts for Intelligent Vision Systems. ACIVS&apos;08</title>
		<meeting>10th Int. Conf. on Advanced Concepts for Intelligent Vision Systems. ACIVS&apos;08</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1081" to="1092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Towards fast 3D ear recognition for real-life biometric applications</title>
		<author>
			<persName><forename type="first">G</forename><surname>Passalis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">A</forename><surname>Kakadiaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Theoharis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Papaioannou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Signal Based Surveillance</title>
		<imprint>
			<biblScope unit="page" from="39" to="44" />
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">A fast algorithm for ICP-based 3D shape biometrics&apos;</title>
		<author>
			<persName><forename type="first">P</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Fourth IEEE Workshop on Automatic Identification Advanced Technologies</title>
		<imprint>
			<biblScope unit="page" from="213" to="218" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Ear recognition based on 3D keypoint matching</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">C</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Tenth Int. Conf. on Signal Processing (ICSP)</title>
		<imprint>
			<biblScope unit="page">1694</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">A computationally efficient approach to 3D ear recognition employing local and holistic features</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cadavid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Abdel-Mottaleb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conf. on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="98" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Multi-modal biometric modeling and recognition of the human face and ear</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cadavid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Abdel-Mottaleb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Int. Workshop on Safety, Security Rescue Robotics</title>
		<imprint>
			<biblScope unit="page" from="1" to="6" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
