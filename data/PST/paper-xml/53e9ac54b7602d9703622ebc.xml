<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Sharing Architecture: Sub-Core Configurability for IaaS Clouds</title>
				<funder ref="#_Jws53vj">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
							<email>yanqiz@princeton.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Engineering Quadrangle</orgName>
								<orgName type="institution">Princeton University</orgName>
								<address>
									<postCode>F210 E2, 08544</postCode>
									<settlement>Olden St. Princeton</settlement>
									<region>NJ</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Wentzlaff</surname></persName>
							<email>wentzlaf@princeton.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Princeton University B228 Engineering Quandrangle</orgName>
								<address>
									<postCode>08544</postCode>
									<settlement>Olden St. Princeton</settlement>
									<region>NJ</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">The Sharing Architecture: Sub-Core Configurability for IaaS Clouds</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/2541940.2541950</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Other Architecture Styles-Adaptable architectures</term>
					<term>C.5.5 [Computer System Implementation]: Servers Keywords Infrastructure as a Service (IaaS)</term>
					<term>Slice</term>
					<term>Cache</term>
					<term>Cache Banks</term>
					<term>Virtual Core (VCore)</term>
					<term>Virtual Machine (VM)</term>
					<term>utility</term>
					<term>market efficiency</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Businesses and Academics are increasingly turning to Infrastructure as a Service (IaaS) Clouds such as Amazon's Elastic Compute Cloud (EC2) to fulfill their computing needs. Unfortunately, current IaaS systems provide a severely restricted pallet of rentable computing options which do not optimally fit the workloads that they are executing. We address this challenge by proposing and evaluating a manycore architecture, called the Sharing Architecture, specifically optimized for IaaS systems by being reconfigurable on a subcore basis. The Sharing Architecture enables better matching of workload to micro-architecture resources by replacing static cores with Virtual Cores which can be dynamically reconfigured to have different numbers of ALUs and amount of Cache. This reconfigurability enables many of the same benefits of heterogeneous multicores, but in a homogeneous fabric, and enables the reuse and resale of resources on a per ALU or per KB of cache basis. The Sharing Architecture leverages Distributed ILP techniques, but is designed in a way to be independent of recompilation. In addition, we introduce an economic model which is enabled by the Sharing Architecture and show how different users who have varying needs can be better served by such a flexible architecture. We evaluate the Sharing Architecture across a benchmark suite of Apache, SPECint, and parts of PARSEC, and find that it can achieve up to a 5x more economically efficient market when compared to static architecture multicores. We implemented the Sharing Architecture in Verilog and present area overhead results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Cloud computing and Infrastructure as a Service (IaaS) Clouds have enabled convenient, on-demand, self-service, access to vast quantities of computing that can be provisioned with minimal effort. Current IaaS systems utilize commodity multicore processors as the computational substrate to service this diverse computational need. Unfortunately, the individual core, the foundation of commodity multicore processors, is optimized across a set of applications, but by definition it is not optimized for any particular application. In this work, we propose a new architecture, we call the Sharing Architecture, which is specifically inspired by the needs of IaaS Cloud providers. Namely, that Cloud providers require an architecture which enables efficient markets to match customer's needs with computational resources, enables the customization of micro-architecture hardware to a customer's application, and enables the reuse of idle hardware resources thereby maximizing profit. The Sharing Architecture addresses these needs by providing a manycore architecture where the core can be reconfigured, customized, and leased on a sub-core basis.</p><p>The movement of computation and storage into the Cloud and into large data centers has been at an explosive rate <ref type="bibr" target="#b5">[7,</ref><ref type="bibr" target="#b10">12]</ref>. For example, the number of objects stored in Amazon's S3 storage service <ref type="bibr" target="#b2">[3]</ref> has more than doubled every year since its inception <ref type="bibr" target="#b7">[9]</ref>. Likewise, the number of Virtual Machine (VM) instances active in Amazon's Elastic Compute Cloud (EC2) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b33">35]</ref> has been estimated to be growing at a similar rate <ref type="bibr" target="#b60">[62]</ref>. Many diverse applications which have traditionally been desktop applications have successfully been transitioned into the Cloud, including word processing (eg. Google Apps <ref type="bibr" target="#b1">[2]</ref>), spreadsheets (eg. Google Apps), data backup clients (eg. Box.net, Dropbox, Mozy, Backblaze ), email clients (Gmail, Hotmail, Yahoo Mail), and even gaming (eg. OnLive). The growth in data center computing has not only been fueled by desktop applications moving to the Cloud, but also startups and established companies moving computationally intensive <ref type="bibr" target="#b36">[38,</ref><ref type="bibr" target="#b44">46]</ref> and data serving tasks into the Cloud.</p><p>In this work, we focus specifically on IaaS Cloud systems. An IaaS Cloud provider typically leases managed computational, storage, or network resources to customers. We focus on the computational facet which usually comes in the form of leasing Virtual Machine (VM) instances. For a fee, an IaaS Cloud provider agrees to run a client's VM on the Cloud provider's computing infrastructure. Examples of computational IaaS Public Cloud providers include: Amazon's EC2 <ref type="bibr" target="#b0">[1]</ref>, Microsoft's Azure Virtual Machines [4], Terremark, Linode, Rackspace Cloud, Joynet, and Google's Compute Engine. IaaS infrastructure is also gaining traction within an enterprise setting by leveraging many of the same technologies of Public Clouds, but in a Private Cloud setting <ref type="bibr" target="#b13">[15,</ref><ref type="bibr" target="#b41">43,</ref><ref type="bibr" target="#b59">61]</ref>.</p><p>IaaS Cloud providers typically provide a very limited pallet of Virtual Machine configurations. For example, Amazon's EC2 currently supports only eighteen machine configurations. The configurations presented by Amazon do not allow the customer to choose different intra-core parameters, such as cache size, but rather only allow the customer to choose fixed combinations of core count, the amount of DRAM, and the amount of disk in a VM instance. The lack of intra-core, micro-architecture VM configuration options leaves the Cloud customer utilizing a VM configuration which does not match their workload's needs. This disconnect leads to Cloud customers overpaying for resources or experiencing sub-optimal application performance. The inability for the Cloud provider to customize a VM instance to a customer's needs also harms the provider. This leads to a less efficient marketplace, where the Cloud provider is unable to price fine-grain resources according to their marginal cost thereby contributing to wasted resources and wasted revenue opportunities. A Cloud provider's inability to provide fine-grain micro-architecture choices is directly due to the failing of current commodity multicores not being divisible below the core level.</p><p>To address this challenge, we propose the IaaS-influenced Sharing Architecture along with a modification to how IaaS markets are priced to enable the fine-grain assignment of processor resources to Virtual Cores. The Sharing Architecture enables the assignment of fine-grain processor resources to a configurable Virtual Core all the way down to the individual ALU, fetch unit, and KB of cache. One or more Virtual Cores are then composed into Virtual Machine instances. Figure <ref type="figure" target="#fig_0">1</ref> shows a conceptual view of the Sharing Architecture. The Sharing Architecture is designed to allow a nearly arbitrary set of sub-core resources to be com- posed into a single Virtual Core. These different fine-grain resources are connected together by multiple, pipelined, switched interconnection networks providing connectivity for the sea-of-ALUs and sea-of-cache banks. While the Sharing Architecture enables nearly arbitrary configurations of resources into Virtual Cores, due to the added communication latency when sharing distant resources, we do not expect all combinations to provide performance benefit. By enabling a fluid assignment of ALUs and cache to Virtual Cores, the Sharing Architecture enables the Cloud user to choose whether to optimize for single-stream performance at the expense of area, for throughput, or complex mixes.</p><p>The Sharing Architecture has much of the same motivation and performance benefit of heterogeneous multicore systems <ref type="bibr" target="#b31">[33,</ref><ref type="bibr" target="#b32">34]</ref>, but is built out of a large homogeneous fabric. Applications can be more area efficient, energy efficient, and perform better if they run on properly sized cores. Unlike heterogeneous multicore systems, the Sharing Architecture does not require the mix of core sizes to be determined at chip design time. This is even more important in IaaS Cloud systems where the mixture of applications is determined by dynamic external customers. The Sharing Architecture is inspired by Core Fusion <ref type="bibr" target="#b22">[24]</ref>, WiD-GET <ref type="bibr" target="#b62">[64]</ref>, the Distributed ILP mechanisms in the TRIPS architecture <ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b15">17,</ref><ref type="bibr" target="#b47">49,</ref><ref type="bibr" target="#b48">50,</ref><ref type="bibr" target="#b51">53]</ref>, and how ILP is mapped across multiple cores in the Raw <ref type="bibr" target="#b61">[63]</ref> and Tilera <ref type="bibr" target="#b63">[65]</ref> architectures.</p><p>An important aspect of the Sharing Architecture is that it enables more efficient markets by enabling the Cloud provider to price different portions of a core on a sub-core basis. For instance, instead of simply charging $0.065 per CPU-hour, the Cloud provider is able to charge per ALUhour, per cache KB-hour, etc. This capability empowers a more dynamic Cloud pricing market and ultimately leads to a more economically efficient marketplace. In this paper, we explore how different IaaS Cloud users (customers) garner different utility as a function of sub-core (ALUs, Cache, etc) and beyond-core (Number of cores, VMs, DRAM, etc.) resources. Different Cloud customers can have different utility curves due to technical, price, and policy needs. Unlike fixed architectures, the Sharing Architecture does not judge which utility function is superior, but rather simply allows the Cloud user to express their preferences and have the market meet these preferences.</p><p>The Sharing Architecture is designed to enable dynamic reconfiguration of Virtual Cores and to run the same binary independent of Virtual Core configurations (no recompilation). This dynamic nature enables the Cloud provider to price sub-core resources dynamically and based on instantaneous market demand. The programmer or IaaS Cloud user must determine, based on the market rates for sub-core resources, how many resources to utilize to best fit their demand and utility needs. While this work focuses on the hardware aspects of the Sharing Architecture and the market it can create, we also outline software implications. For instance, the IaaS Cloud user could provide a meta-program along with their Virtual Machine which decides the desired machine configuration based off the market prices for resources. Alternatively, an auto-tuner could be used if the customer lacks a detailed application performance model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Economic Model</head><p>Optimizing computer architecture for IaaS cloud systems has different criteria than traditional computer architecture. In traditional computer architecture, systems are typically optimized for performance, cost, energy, or some combination of these constraints. In contrast, when computer architecture is optimized for IaaS Clouds, the ability to enable more efficient markets dominates traditional computer architecture constraints. This is amplified by the fact that different IaaS customers can have conflicting optimization criteria such that it is impossible to satisfy all customers with a fixed design. Economists have shown that by having economically efficient markets, both the seller and buyer can be enriched, or in our case, the Cloud provider can make additional revenue and simultaneously the Cloud user (customer) can pay less. In this section, we describe the current IaaS marketplace, the diversity of customer needs, and how the Sharing Architecture facilitates a more economically efficient market.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Current IaaS Pricing Model</head><p>IaaS Public Cloud providers such as Amazon's Elastic Compute Cloud (EC2) and Microsoft's Azure Virtual Machines use an instance-based pricing model. In these models, customers are able to choose from a relatively limited number of VM instance types and they pay on a per-VM instance-hour basis. For example, in EC2, for $0.065/h, a VM instance with 1.7GB of RAM, a single core, and 160GB local disk can be leased. In addition to these fixed cost models, EC2 has a market, named Spot Pricing, which provides an auction for VM instances. Cloud customers use IaaS Cloud systems as a way to reduce the risk of over-provisioning or underprovisioning purchased computational resources by turning capital purchases into operational expenditures. In contrast to instance-based pricing, private IaaS Cloud systems such   <ref type="bibr" target="#b29">[31]</ref> system provide a richer ability to configure a VM instance, but the parameters are still limited to resources outside of the core such as core count, amount of DRAM, and disk size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Diversity of Needs</head><p>Consumers have diverse desires. Economists use utility to measure how well a particular resource satisfies an individual's wants. Economists measure total utility to determine how economically efficient a market is. A more economically efficient market will have higher global utility than a less efficient market for a fixed number of resources. In effect, an efficient market will enable better matching of resources to consumers who are willing to pay more for those resources (assuming consumers are willing to pay commensurate with the utility they gain). One way to make a market more efficient is to disentangle, disaggregate, or sub-divide resources, thereby enabling consumers to more accurately express their desires and allowing sellers to more accurately express their costs. Given these assumptions, by maximizing global utility, cloud providers maximize profit.</p><p>In an IaaS Cloud market, different customers can have different utility functions. In this paper, we do not try to make any judgment or assumption about customers' utility functions, but rather we provide hardware that enables different utility functions to be well served. Figure <ref type="figure" target="#fig_2">2a</ref> shows a set of utility functions for a hypothetical throughput oriented application (ex. web serving). In this application, the computation scales linearly with VMs and CPUs/VM, but the customer favors having more CPUs per VM to reduce administrative overhead. As more Cache per core is added, the marginal utility gained by the customer diminishes. Last, as can be seen, as more ALUs are added per core, the utility goes up at first, but then this customer's application begins to slow down due to the added communication cost between ALUs. It should be noted that these utility functions are simply 2D approximations of the true multi-dimensional utility space as one resource may affect the utility gained from a second resource. A savvy customer will profile their application and use policy decisions to determine what their utility functions are. Hence the same customer with the same policy may have different utility functions when running a   In contrast to the throughput example, a second customer running an application (potentially the same application) may favor single-stream performance. This may be because they know that their application does not parallelize well across cores or may simply be a policy decision. Figure <ref type="figure" target="#fig_2">2b</ref> shows an example customer's utility functions where they favor (sequential perf ormance) 2 as a metric. P erf ormance 2 or P erf ormance 3 may be very reasonable metrics if a customer favors sequential time to completion and these metrics have much similarity to Energy * Delay 2 and Energy * Delay 3 used in energy efficient computing research. Finally, a single customer and a single application may have different utility functions as a function of time of day or program phase <ref type="bibr" target="#b32">[34]</ref>. Programs commonly have different phases and if properly expressed, a per-phase machineapplication fit can be made.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">New Model</head><p>To create a more efficient market, we propose changing the typical IaaS market and software APIs from one which has a limited pallet of VM instance types that have differences at the core level and above into a market where the cloud provider auctions off all resources down to the ALU, KB of cache, fetch unit, retire unit, and even on-chip or off-chip memory bandwidth. To realize this change, a hardware fabric that is more flexible than multicore processors is needed. The Sharing Architecture is the first step in building such an architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Architecture</head><p>The Sharing Architecture is a fine-grain composable architecture which provides the flexibility to dynamically "synthesize" a flexible core out of the correct amount of ALUs, fetch bandwidth, and cache, based on an application's demand and the need to optimize cost. Unlike multicore and manycore architectures where the processor core is statically fixed at design and fabrication time, the Sharing Architecture enables the core to be reconfigured and use resources from a wide array of resources across a single chip. This level of reconfigurability is especially useful in a competitive  IaaS data center setting where many applications and different customers' codes and Virtual Machines will all share a single multicore processor.</p><p>At the top level, like current day multicore chips used for IaaS applications, the Sharing Architecture has multiple cores that can be grouped together to create multicore Virtual Machines (VMs). Unlike fixed architecture multicore processors, the VMs in the Sharing Architecture are composed of cores which themselves are composed of a variable number of ALUs, fetch bandwidth, and cache. We call this flexible core a Virtual Core (VCore). A VCore is composed out of one or more Slices and zero or more L2 Cache Banks. Figure <ref type="figure" target="#fig_4">3</ref> shows an example array of Slices and Cache Banks. A full chip will have 100's of Slices and Cache Banks.</p><p>The basic unit of computation in the Sharing Architecture is a Slice as shown in Figure <ref type="figure" target="#fig_5">4</ref>. A Slice is effectively a simple out-of-order processor with one ALU, one Load-Store Unit, the ability to fetch two instructions per cycle, and a small Level 1 Cache. Multiple Slices can be grouped together to increase the performance of sequential programs thereby empowering users to make decisions about trading off ILP vs. TLP vs. Process level parallelism vs. VM level parallelism while all utilizing the same resources.</p><p>In order to reduce fragmentation, the Sharing Architecture is designed to enable very flexible sharing of resources and does not impose hierarchical grouping of Slices or Cache Banks. In order to achieve this, we use general purpose switched interconnects wherever possible. Neither Slices or Cache Banks need to be contiguous in a VCore for functionality. But we do impose a few limited restrictions for the purpose of performance. For instance, when Slices are joined into a single VCore, those Slices need to be contiguous to reduce operand communication cost, but Cache Banks do not need to be contiguous or near by and we model the latency as such. We do not see the need for contiguous Slices to be a limitation as all Slices are interchangeable and equally connected therefore fixing fragmentation problems is as simple as rescheduling Slices to VCores.</p><p>The ability to group multiple Slices into a single VCore adds not only logical complexity, but also a small area and performance overhead which we study in detail in Section 5.1. In this work, we leverage many of the ideas from Distributed ILP architectures such as TRIPS, Raw, and Tilera as well as fusible core ideas from Core Fusion and WiDGET to address the challenge of partitioning one VCore across multiple Slices. The approach we take is designed for wider sharing of resources than the fusible core approach and hence the Sharing Architecture tries to enable more decoupling between Slices.</p><p>When grouping multiple Slices to work together to execute a single sequential program, each intra-core component (e.g. Reorder Buffer (ROB), Scoreboard, etc) has to be considered to see if the component needs to be logically replicated, partitioned, or centralized. These decisions are made based off how tolerant a structure is to increased latency when accessing different elements in the structure. The advantage of partitioning a structure over replicating a structure is that when partitioned, the number of logical resources scales with the number of Slices. Conversely, replicated resources need to be sized for the largest number of Slices in a VCore configuration <ref type="bibr" target="#b6">(8)</ref> and waste area for smaller configurations. Table <ref type="table" target="#tab_2">1</ref> presents a table of which structures are replicated or partitioned in the Sharing Architecture. The rest of this section steps through the design decisions of the Sharing Architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Front-End</head><p>Each Slice has its own copy of the program counter. The replication of the program counter enables each Slice in a VCore to fetch two instructions per cycle. The fetching of instructions is interleaved such that each Slice fetches two contiguous instructions in one cycle and instructions 2 * number of slices and (2 * number of slices) + 1 later in the program on a subsequent cycle. Whenever there is a stall that ripples back to the fetch stage of the pipeline, it stalls all of the Slices in a VCore.</p><p>Similar to Core Fusion <ref type="bibr" target="#b22">[24]</ref>, a distributed branch predictor is used, thus the effective branch predictor capacity grows with number of Slices. Because instructions are fetched in an interleaved fashion, the same PC is always fetched by the same Slice. Therefore branch instructions are always mapped to the same Slice and the same predictor. Unlike Core Fusion, each Slice replicates Branch Target Buffer (BTB) entries in order to enable Slices which do not execute a given branch to fetch the appropriate next instruction. These fake BTB entries are added into the BTBs of Slices in the same fetch group as a branch and are tagged by the appropriate offset and point to the correct Slice-interleaved target address, not the actual branch target. Branch misprediction and Branch Target Buffer (BTB) training information is transmitted over the switched interconnect. When a branch mispredicts, all speculative instructions in the branching Slice are flushed and misprediction messages are sent to  Slices which contain newer speculative instructions to cause those instructions to be flushed.</p><p>The Sharing Architecture uses a local bimodal branch predictor <ref type="bibr" target="#b38">[40]</ref>, which indexes the prediction table by the program counter value. In order to utilize a global prediction scheme, such as gshare, gselect, etc., a Global History Register needs to be composed across Slices. This could be achieved with appropriate delay across the switched interconnect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Register Renaming</head><p>Renaming registers on the Sharing Architecture occurs in a two step process as shown in Figure <ref type="figure" target="#fig_6">5</ref>. At a high-level, architectural registers are first renamed into a global logical register space which is shared across Slices. After inter-Slice dependencies are determined, a second renaming step renames from the global logical register space into a Slicelocal physical register number. Each Slice contains a Local Register File (LRF) <ref type="bibr" target="#b50">[52]</ref> and enables the number of physical registers to increase as Slices are added. The global logical register space is sized for the maximum number of Slices in a VCore.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Global Rename: Local Decision with Global Help</head><p>Instructions are decoded in parallel and renaming is first done from the architectural register space to a large, global logical register space, which eliminates false dependencies. The free-list of global logical registers is distributed across Slices in a VCore removing the need for a centralized free-list. Each Slice contains a global Register Alias Table (RAT) which tracks the mapping from architectural register to global logical register. Global renaming of destination registers takes place in several steps as shown in Figure <ref type="figure">6b</ref>. First, all destination operands are renamed in their local Slice. Second, this rename mapping (both architectural register and global logical register) is sent to a master Slice within the VCore. The master Slice broadcasts this rename information to the other Slices which then correct their own rename operations based off of instruction ordering. This correction step is done locally, but uses global information in order to handle the case where two instructions that are fetched across different Slices write the same destination or there is an inter-Slice read-after-write (RAW) dependency  between instructions in a fetch group. This information updates each Slice's copy of the global RAT and scoreboard which tracks which Slice contains the most up-to-date value for a given register. Finally, source operands are renamed using the global RAT's old and new values dependent on instruction order.</p><p>Figure <ref type="figure">6a</ref> shows how Slices can be grouped around a master Slice for global renaming and Figure <ref type="figure">6b</ref> shows an example of multi-stage rename. Alternatives include centralizing the rename information and then broadcasting this information using a multicast network or choosing different Slices to handle global rename based on architectural register number.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Local Rename and Remote Operand Request</head><p>After global rename occurs, each global logical register operand is locally renamed into the physical register LRF namespace. By using distributed LRFs as shown in Figure <ref type="figure">7</ref>, the number of physical registers in a VCore scales with the number of Slices in a core. Using the information from the global rename step, the scoreboard contains a mapping from global logical register to Slice number. Destination registers are always renamed into the LRF space, allocating entries from the LRF. Source operands read a local RAT which keeps a mapping from global register number to LRF physical register number. If a global logical source register is found from the scoreboard to be in a remote Slice, an operand request message is generated and sent to the remote Slice. When the operand request is sent, the destination is allocated into the LRF and marked as pending until an operand reply message is received. By renaming and allocating remote operands into the LRF, subsequent reads from a global logical register which has been previously read and renamed by that Slice do not generate remote operand requests, but rather simply read the LRF.</p><p>Operand request and operand reply messages transit a 2-Dimensional, switched, Scalar Operand Network (SON) <ref type="bibr" target="#b57">[59,</ref><ref type="bibr" target="#b63">65]</ref> to move operand requests and operand responses between different Slices. Section 3.4, below, describes how the Scalar Operand Network is integrated with each Slice's execution units. When an operand request message is received by a remote Slice, the local RAT is used to determine the physical LRF number that either stores the value or will store the value. If the remote Slice's LRF already contains the operand value, the SON is used to send back the value. If the operation which generates the operand value is pending, the request is enqueued to a wait list, and the reply will be generated once the value is ready. After renaming and operand request messages are sent, but responses not necessarily received, instructions enter an instruction window.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Instruction Window and Dispatch</head><p>Each Slice has a separate issue window for ALU instructions and loads/stores. Instructions wait in the issue window until their dependent operands are going to be available the next cycle. At that point, the instructions leave the issue window, possibly out-of-order, and execute. Special attention must be paid to operands that come from remote Slices. In order to wake up an instruction waiting for a remote operand, the issue window not only is matched against local finishing instructions, but also matched against wake-up signals that indicate that a operand reply will be returning on the next cycle. It is possible to have this one-cycle head start because a wake-up signal is sent when the instruction in the remote Slice issues, which is in the cycle before it executes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Execution Units</head><p>Once an instruction is issued, it executes on the ALU or Load-Store unit. An important aspect of the Sharing Architecture is that it uses a switched, dynamically scheduled, 2D Scalar Operand Network (SON) to move operand values between different Slices. The SON is directly tied into the bypass network and where results are generated in the ALUs. This enables operand values to be sent right after they are generated and enables values to be forwarded to a destination instruction and the LRF in a remote Slice with low latency. This design is similar to how the on-chip networks are integrated into the pipelines of the Raw and Tilera processors and reduces latency between ALUs in different Slices. We model a two-cycle communication cost between nearest neighbor Slices and an additional cycle for each additional network hop, the same latency as on a Tilera processor. The LRF has an additional read port and an additional write port to process operand request messages and operand reply messages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Cache</head><p>Each Slice contains an L1 I-Cache and an L1 D-Cache. The L1 I-Cache cache line size is reduced to accommodate two instructions, and a next line predictor is used to prefetch the next instruction according to the number of Slices in a virtual core. The L1 D-Cache is private to each Slice. Beyond the L1 D-Cache is a configurable L2 cache. Any L2 Cache Bank in the system can be used by any VCore. This is achieved by connecting L2 Cache Banks to a 2D switched dynamic network interconnect. In our results section, we assume each L2 Cache Bank is 64KB. On a L1 cache miss, a message is sent across the switched interconnect to retrieve data and response data is sent via that same network. Addresses are low-order interleaved by cache line across L2 Cache Banks, thereby simplifying the mapping of address to Cache Banks. Latency increases as L2 banks are further away from the cache miss issuing Slice.</p><p>In order to prevent the processor cores from stalling due to long memory latencies, the Sharing cache subsystem uses non-blocking caches and a small store buffer in each Slice. Loads and stores are sorted to Slices after issue and address generation as shown in Figure <ref type="figure" target="#fig_9">8</ref>. A hashing function loworder interleaves accesses by cache line. Because addresses to the same cache line are always sorted to the same Slice, there is no need to keep the caches coherent between Slices in a VCore.</p><p>In a multi-VCore VM, caches need to be keep coherent between VCores when you step beyond the VCore such as in a multi-threaded program or inter-program shared memory. The coherence point can be between the L1 and L2 or after the L2s. If the coherence point is before the L2, the L2 can be shared by multiple VCores. If it is after the L2, the L2s are private to a VCore. Both of these solutions enable a manner of flexible L2 cache sizing. In our presented results, we put the coherence point between the L1 and L2 caches therefore having a shared L2 cache per VM. We modeled this with a detailed model which has a directory in the L2. Our modeled cache coherence protocol includes switched network cost based on distance and L1 invalidations. An important portion of this design is that each Slice maintains a table which maps a portion of the VM's physical address space to a particular L2 Cache Bank (IE a home-node mapping table). We do not currently allow inter-VM physical memory sharing so there are currently no coherence challenges beyond the VM (L2 cache). The implication of this is that the reallocation of an L2 Cache Bank requires flushing that bank to main memory. In terms of inter-VM coherence, the L2s themselves could be kept coherent with the addition of a directory-based cache coherence system beyond the L2 caches therefore enabling VM-level page sharing. Such a solution could leverage the Virtual Hierarchy work <ref type="bibr" target="#b37">[39]</ref>. Last, if we only have per-VCore private L2 caches, much of this complexity goes away as the L2 caches could simply be kept coherent with a flat directory-based cache coherence protocol.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Distributed LSQ and LS Instruction Sorting</head><p>Conventional load/store queues (LSQs) are challenging to build in wide-issue processors. The Sharing Architecture  <ref type="bibr" target="#b6">[8]</ref> and Core Fusion <ref type="bibr" target="#b22">[24]</ref> require LSQ bank prediction because a memory operation's effective address is unknown until the execution stage. Clustered processors and Core Fusion use this approach because they allocate into the LSQ when entering the issue window. In contrast, the Sharing Architecture uses an unordered LSQ such as those in prior work <ref type="bibr" target="#b52">[54]</ref> in order to reduce LSQ occupancy and power consumption. Our work is the first to utilize a general-purpose switched network to sort load/store instructions mixed with an unordered LSQ.</p><p>The LSQ bank is unordered with respect to age, and an extra age-tag field is added to maintain the load/store order. The LSQ buffers all stores and only allows stores to be written out when they are non-speculative. In the Sharing Architecture, a store is determined to be non-speculative if it is the head instruction in the ROB. A search is necessary in the LSQ to match the ROB tag with the LSQ entry tag on store commit.</p><p>Loads access memory speculatively after the address is resolved. Load data values are returned to the issuing Slice and are marked as speculative until all earlier stores have committed. The LSQ reports a violation when it detects that a younger load in program order to the same address executes before a store to the same address. In order to achieve this, each committing store searches the LSQ bank for any issued loads younger than it to the same address as the store. Figure <ref type="figure" target="#fig_10">9</ref> shows a committing store checking all younger loads in a local LSQ. Each store needs to check against all of the loads in a particular LSQ bank when it commits, but loads do not need to check against stores in this design. The address banked nature of the LSQ across Slices enables more memory bandwidth and a large aggregate LSQ capacity as more Slices are added to a VCore and it eliminates the need for inter-Slice LSQ alias checking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Distributed Reorder Buffer</head><p>We partition and distribute Reorder Buffer (ROB) entries across Slices. We leverage the approach used by Core Fusion <ref type="bibr" target="#b22">[24]</ref> which uses a pre-commit pointer to guarantee that all ROBs are up to date several cycles before true commit. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.8">Reconfiguration</head><p>In order to reconfigure VCores, we rely on the Hypervisor to control the connections of the Slice and Cache interconnect. We propose having the hypervisor be time sliced on the same resources as the client VMs. But, unlike client VMs which run on reconfigurable cores, we propose having the hypervisor execute only on single-Slice VCores. By having the Hypervisor execute on single Slices only, the hypervisor can then locally reconfigure protection registers and interconnection state to setup and tear-down client VCores. The hypervisor is designed to bypass use of the reconfigurable cache system or use hypervisor dedicated L2 banks in order to prevent having to flush the L2 Caches on every time slice.</p><p>When a VCore is reduced in the number of Slices, there is potentially register state which needs to be transmitted to the surviving Slices within the VCore. In order to carry this out, we use a Register Flush instruction which pushes all of the dirty architectural register state to other Slices in the same VCore using operand reply messages. This can be done quickly because there are only 64 local physical registers per Slice and the Scalar Operand Network is fast for transferring register data. When VCore L2 Cache configurations change, we propose that all dirty state in L2 Cache Banks be flushed to main memory before reconfiguration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Software Implications</head><p>While this paper focuses on a new IaaS optimized architecture, we wanted to address some of the software implications of using such a system in an IaaS Cloud. The first question which comes up is how does the IaaS user determine their utility functions and how might they react to variable pricing? A basic solution would be for the IaaS user to provide a meta-program along with the VM workload that they want to execute. The meta-program can express the user's multi-dimensional utility function as a function of different resources and can understand how to react to changing pricing. The user can profile their application on different architectures to build performance models which will allow them to back-compute their utility functions.</p><p>Alternatively, they could utilize an auto-tuner <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b4">6,</ref><ref type="bibr" target="#b42">44,</ref><ref type="bibr" target="#b49">51,</ref><ref type="bibr" target="#b53">55]</ref>. The auto-tuner would slowly search the configuration space by varying the VM instance configuration and number of VMs. The auto-tuner can then pick good configurations provided a high-level goal from the user. Such an auto-tuning system would likely require the use of a heartbeat <ref type="bibr" target="#b20">[22]</ref> or performance feedback to evaluate the success criteria. An example of this can be seen in current day IaaS auto-scaling systems such as Amazon's Auto Scaling and RightScale. Other options include having a human-in-theloop to make decisions or employing a decision system to make configuration changes.</p><p>We could use a sophisticated control theory based or machine learning based software adaptation engine, such as PTRADE <ref type="bibr" target="#b21">[23]</ref>. This software engine asks application developers to specify objective functions and asks system developlers to specify components that affect their objective functions. The software runtime system takes these specifications and tunes component usage to meet goals while reducing the cost (power consumption, computation and storage cost).</p><p>The Sharing Architecture is designed to minimize changes to the software stack, but some changes will be needed. From the IaaS client's perspective, they will still create VM instances like before, but now will have to provide a metaprogram as described above, rely on auto-tuners, or fall back on the previous market solution of fixed instance types. Because all of the different micro-architecture configurations of the Sharing Architecture are binary compatible, the user does not need to recompile to use different Slice and Cache configurations. The user could further optimize by using multi-architecture (FAT) binaries optimized for different micro-architectures. Our presented results do not recompile or use this optimization.</p><p>Next we analyze the changes to the system stack. The OS should not need to change for the Sharing Architecture. The Hypervisor will likely need modification in order to manage resources on a sub-core basis. Finally, the Cloud management software (scheduler) will have to change in order to schedule new resources. Changing the Cloud scheduler is a challenging problem, but the Sharing Architecture opens up many opportunities for interesting research in this space that can leverage prior work in large scale scheduling <ref type="bibr" target="#b12">[14,</ref><ref type="bibr" target="#b17">19,</ref><ref type="bibr" target="#b19">21,</ref><ref type="bibr" target="#b23">25,</ref><ref type="bibr" target="#b46">48,</ref><ref type="bibr" target="#b54">56]</ref>. These Cloud provider changes can enable improvements in economic efficiency and revenue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Design Area</head><p>In order to assess the feasibility of the Sharing Architecture, we have implemented the Sharing Architecture Slice in synthesizable Verilog. We use this to measure the added area overhead that the Sharing Architecture adds to an out-oforder superscalar. We synthesized the design utilizing the Synopsys tool flow and took the design through Design Compiler and IC compiler to a fully place-and-routed netlist. We use Synopsys's TSMC 45nm cell library as the implementation technology. Cache sizes, timing, and power are generated utilizing CACTI <ref type="bibr" target="#b40">[42]</ref> at the 45nm node. Figure <ref type="figure" target="#fig_0">10</ref> and Figure <ref type="figure" target="#fig_11">11</ref> present the area breakdown of the Sharing Ar-  We found that the area overhead of the inter-Slice onchip Networks to be significant as there are three dedicated networks modeled for different purposes (operand network, load/store sorting, and global renaming). A single operand network is used for both operand request and operand reply messages as we found that the operand network provides sufficient bandwidth. By conducting a sensitivity study on operand communication bandwidth, we discovered that by adding a second operand network, performance would improve by only 1% across our applications. The overhead of the Sharing Architecture as a percentage of the total area is also affected by our choice of modestly sized structures such as the ROB, Load/Store queue, etc. If a more complex base design were to be chosen, the area overhead of the Sharing Architecture as a percentage of the total would be smaller. In addition, we do not model the area of the floating-point unit in our design, but this structure does not change due to the Sharing Architecture, therefore our area estimations are conservative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental Setup</head><p>We have created a trace-based simulator, SSim, to model the Sharing Architecture and measure its effectiveness. This   cycle-level simulator models each subsystem of the Sharing Architecture (fetch, rename, issue, execution, memory, commit, and on-chip network) along with accurately modeling the Out-of-Order execution and inter-Slice and Sliceto-memory latency. SSim is driven by full system traces generated by the Alpha version of GEM5 <ref type="bibr" target="#b8">[10]</ref>. We use the complete SPEC CINT2006 benchmark suite, a static webserving workload of Apache driven by ApacheBench, and a subset of PARSEC benchmarks to explore the Sharing Architecture. SSim is very flexible, allowing all critical microarchitecture parameters and latencies to be set from a XML configuration file. When a simulation completes, SSim reports the cycles executed for a given workload along with cache miss rates and stage-based micro-architecture stalls and statistics. In the following sections, when aggregate results are shown, we use the geometric mean (GME) of different benchmark results following how SPEC creates aggregate results. Throughout these results, we use an area model derived from our results in Section 5.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Scalability</head><p>We begin by evaluating the Sharing Architecture's ability to increase both single-stream performance and multithreaded performance by adding Slices to a VCore. Table <ref type="table" target="#tab_6">2</ref> contains the base configuration of a single Slice which we will use throughout this section. The base cache configuration is shown in Table <ref type="table" target="#tab_7">3</ref>. Figure <ref type="figure" target="#fig_12">12</ref> presents these scalability results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Cache Sensitivity</head><p>We study the sensitivity of performance to cache size by varying the L2 cache size from 0KB up to 8MB. As can be seen in Figure <ref type="figure" target="#fig_4">13</ref>, some applications such as omnetpp are extremely sensitive to cache size while benchmarks such as astar, libquantum, and gobmk are insensitive to cache size. This variable sensitivity motivates the Sharing Architecture which can take advantage of the variable micro-architecture requirements of applications. Performance can actually decrease as more cache is added because we model an additional 2-cycles of communication delay for each additional 256KB of cache, thereby making the latency of the large cache significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Performance-Area Efficiency</head><p>This section studies VCore performance-area efficiency as a function of Slice count and L2 Cache size. We use the area model described in Section 5.1 along with an exhaustive search of performance for different Slice count and Cache configurations. Table <ref type="table">4</ref> presents the optimal configurations for three different metrics across 15 benchmarks (apache, SPEC, and PARSEC). The non-uniformity of optimal configurations, even for the performance per area metric, shows that benefits can be achieved even before we take into account Cloud user utility functions.</p><p>We next turn our attention to the perf ormance 2 /area metric to mimic users whose applications do not scale across processors well, or who favor single-thread performance. As can be seen in Table <ref type="table">4</ref>, gobmk favors 5 Slices and 1MB of cache while under the same criteria, hmmer prefers 64KB cache and a single Slice. For sequential performance-oriented applications, some customers will pay significantly more for higher performance. We use perf ormance 3 /area to model these needs. Under this constraint the optimal VCore configurations vary significantly across different benchmarks. What is even more interesting is that within a single benchmark, the optimal configuration varies greatly dependent on the efficiency metric. For instance, gcc in a throughput computation favors 128KB L2 cache and two Slices, while under a performance metric favors 4 Slices and a 1MB cache. gcc has over a factor of two in performance gain between optimal configurations for different metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Utility Optimization</head><p>As developed in Section 2.2, IaaS Cloud customers have varied needs for resources based on the characteristics of the workloads that they execute (how it reacts to different resources) and internal policy decisions. In the Sharing Architecture, customers decide which and how many resources to purchase in order to maximize their utility while staying within a budget constraint. In the Sharing Architecture, resources include: Slices, L2 Cache banks, VCores, and VMs. For the balance of the paper, we define a Cloud user's utility function as: U bi (c, s, vc, vm), where U bi is the utility for benchmark b i as a function of c, the amount of L2 cache per VCore, s, the number of Slices per VCore, vc, the number of VCores per VM, and vm, the number of VMs purchased. We simplify this by combining vc and vm into one value v which is the number of cores a customer uses. We can do this without losing generality as we assume that benchmarks can be replicated within a VM or between VMs providing a new utility of: U bi (c, s, v).</p><p>We define an application's single-thread performance as P (c, s), a function of Cache size and Slice count. For example, one Cloud customer may run Online Data-Intensive (OLDI) workloads driven by queries that interact with massive data sets and require responsiveness at a sub-second time scale <ref type="bibr" target="#b39">[41]</ref>. Such a workload may give substantial weight to single-stream performance to reduce latency. This can lead the OLDI customer to use a utility function such as Equation 1 where utility is proportional to the cube of singlestream performance. A utility function is not enough to drive the Cloud customer's decision making, instead, the cost of resources becomes an important consideration. We define C c as the cost of one cache bank, C s as the cost of one Slice, and B as the overall budget that a user can spend. Combining these costs with a budget, we find the constraints in Equation 2 as well as constraints which are derived from valid VCore configurations in Equation 3. A Cloud customer would then maximize their utility given these constraints.</p><formula xml:id="formula_0">U OLDI = 3 ? v * P 3 (c, s)<label>(1)</label></formula><formula xml:id="formula_1">v = B C c * c + C s * s (2) 0KB ? c ? 8M B, 1 ? s ? 8<label>(3)</label></formula><p>Table <ref type="table">5</ref>. Three Different User Utility Functions</p><p>In contrast to the OLDI workload, some cloud workloads are completely throughput oriented and insensitive to additional latency. Examples of this include a data backup service which has to encrypt and compress large amounts of bulk data, bulk image resizing photo sharing sites such as Picassa or SmugMug, and many MapReduce applications which are not directly connected to a user. For these latency tolerant applications, we define the utility function in Equation 4. In the balance of the paper, we will use three utility functions as example Cloud user utility functions as shown in Table <ref type="table">5</ref>. We will refer to these as Utility1, Utility2, and Utility3, sorted from favoring throughput computing to favoring increasing amounts of single-thread performance.</p><formula xml:id="formula_2">U LT = v * P (c, s)<label>(4)</label></formula><p>To provide some intuition about what a utility function looks like, we plot in Figure <ref type="figure" target="#fig_5">14</ref> the utility function Utility1 and Utility2 for gcc and bzip. In Figure <ref type="figure" target="#fig_5">14</ref>, the x-axis represents the number of Slices [0, 8] and the y-axis represents the number of 64 KB L2 cache banks on a binary log scale [0, 8] (64KB * 2 (y axis) -64KB). As can be seen, when comparing Figure <ref type="figure" target="#fig_5">14a</ref>   <ref type="figure" target="#fig_5">14b</ref> (gcc Utility2), we can see that when holding the utility function constant but changing the workload, the configurations with peak utility substantially change. For instance, bzip favors both a 256KB L2 cache and a single Slice to obtain peak utility for Utility2 while gcc favors a 512KB L2 cache and 4 Slices per VCore to maximize Utility2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Market Efficiency</head><p>We simulated the performance of all of our benchmarks at different Slice and L2 Cache configurations and computed the peak utility for the three different utility functions (Util-ity1, Utility2, Utility3 in Table <ref type="table">5</ref>) and in three different market scenarios and present configurations that exhibit the highest utility for each in Table <ref type="table" target="#tab_10">6</ref>. Market2 is the market which sets the cost of resources equal to their area (1 Slice Figure <ref type="figure" target="#fig_5">14</ref>. Two Utility Functions for bzip and gcc as a Function of Slice count and L2 Cache Size: x-axis is number of Slices, y-axis is number of 64 KB L2 banks in binary log scale costs the same as 128KB Cache). In Market1, we set Slices to be four times the equal area cost of Cache and in Market3, we set Cache to be four times the equal area cost of a Slice. We use these different markets to show how optimal configurations change when market demand for resources does not track area costs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.8">Efficiency Comparisons with Static Fixed</head><p>Architecture and Heterogeneous Architecture</p><p>We turn our attention to how much market efficiency gain (profit gain) can be achieved when compared to a fixed multicore architecture or heterogeneous multicore architecture when used in an IaaS Cloud provider setting. For this, we restrict our study to Market2 where resource cost is commensurate with the area used. In order to study the market efficiency gain, we pairwise choose two benchmarks and two utility functions from Table <ref type="table" target="#tab_10">6</ref> and compare the utility of running those benchmarks on a fixed resource architecture versus running those benchmarks on the Sharing Architecture with configurable Slice count and Cache size. We first compare the Sharing Architecture with the best static fixed architecture as determined across all benchmarks and the three utility functions such that it minimizes the geometric mean of the utility gained in Figure <ref type="figure" target="#fig_6">15</ref>. This shows how much market efficiency gain can be had when compared with the current IaaS commodity multicore  processors. For example, point 1 in Figure <ref type="figure" target="#fig_6">15</ref> represents astar using Utility1 combined with bzip using Utility1. As can be seen, there are significant gains, up to 5x, in market efficiency by using the Sharing Architecture over an optimal fixed architecture. We also compare the sum of the two benchmarks' utility running on the Sharing Architecture to the sum of those benchmarks' utility running on configurations that are optimal across all benchmarks for the particular utility function:</p><formula xml:id="formula_3">(U b1 (sharing) + U b2 (sharing))/(U b1 (f ixed c ) + U b2 (f ixed d )).</formula><p>This mimics what a heterogenous multicore proposed in previous reseach <ref type="bibr" target="#b16">[18]</ref> could achieve when optimized across a suite of applications. Figure <ref type="figure" target="#fig_0">16</ref> shows the utility gained when mixing one benchmark and utility with another benchmark and utility for all benchmarks and three utility functions. Over 3x market efficiency gains can be achieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.9">Datacenter Heterogeneity Comparison</head><p>Heterogeneous architectures allow a datacenter to combine benefits of specialization with the performance guarantees of traditional high-performance servers <ref type="bibr" target="#b16">[18]</ref>. However a static mix of high-end and low-end processors falls short for certain applications. While computationally intensive workloads favor large server-class processors, throughput workloads, such as web search, have better power efficiency running on mobile-class processors <ref type="bibr" target="#b25">[27]</ref>. Utilizing figuration with one Slice and zero L2 cache, while gobmk achieves peak Utility1 using a large core with three Slices and 256KB L2 cache. We vary the ratio of big cores (with 3 Slices and 256KB L2 cache) and small cores (with 1 Slice and 0 L2 cache), and the application ratio, and then measure the utility of hmmer and gobmk under different ratios. In Figure <ref type="figure" target="#fig_0">17</ref>, we can see depending on application mix, different ratios of big and small cores are required for optimal perf ormance/area efficiency. A fixed mixture of big and small cores therefore cannot always optimally service heterogeneous workloads in the cloud.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.10">Dynamic Phases</head><p>The may be more CPU intensive while other phases are memory intensive. We study program phases for gcc by evenly dividing gcc into 10 segments. We run these on SSim independently and find the optimal VCore configuration for different performance-area metrics as shown in Table <ref type="table" target="#tab_12">7</ref>. We account for reconfiguration overhead by adding 10,000 cycles when the cache configuration changes, but only 500 cycles when only the Slice count changes as the L2 Cache does not need to be flushed. Even within a single program and a single metric, optimal VCore configurations change with phase. We compare to the optimal static configuration for gcc (more stringent than the optimal across benchmarks) and find that significant gain can be achieved with up to a 19.4% gain for the perf ormance 3 /area metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Related Work</head><p>The Sharing Architecture leverages many ideas from Distributed ILP work and fused-core architectures as compared in Table <ref type="table" target="#tab_13">8</ref>. Distributed ILP work tries to remove large centralized structures by replacing them with distributed structures and functional units and has been explored in Raw <ref type="bibr" target="#b57">[59]</ref>, Tilera <ref type="bibr" target="#b63">[65]</ref>, TRIPS <ref type="bibr" target="#b15">[17,</ref><ref type="bibr" target="#b47">49,</ref><ref type="bibr" target="#b48">50]</ref>, and Wavescalar <ref type="bibr" target="#b56">[58]</ref>. Tiled architectures distribute portions of the register file, cache, and functional units across a switched interconnect. We adopt a similar network topology for communicating instruction operands between function units in different Slices, but unlike Raw and Tilera, we do not expose all of the hardware to software, do not require recompilation, and use dynamic assignment of resources, dynamic transport of operands, and dynamic instruction ordering. TRIPS <ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b47">49]</ref> has an array of ALUs connected by a Scalar Operand Network and uses dynamic operand transport similar to the Sharing Architecture. Unlike the Sharing Architecture, TRIPS requires compiler support, and a new EDGE ISA which encodes data dependencies directly. In contrast, the Sharing architecture uses a dynamic request-and-reply message based distributed local register file to exploit ILP across Slices without the need of a compiler. The TFlex CLP micro-architecture <ref type="bibr" target="#b28">[30]</ref> allows dynamic aggregation of any number of cores up to 32 for each thread, to target an optimal goal: performance, area efficiency, or energy efficiency, with the support of an EDGE ISA. The Sharing Architecture does not rely on compiler level support, but provides sub-core composability for different customer util- ity functions in a dynamic manner. The dynamic assignment of instructions to ALUs and operands to interconnect can potentially make the Sharing Architecture more flexible than TFlex, but possibly with less potential to exploit high ILP.</p><p>The Sharing Architecture leverages many ideas from Core Fusion <ref type="bibr" target="#b22">[24]</ref> on how to distribute resources across cores, but unlike Core Fusion, the Sharing Architecture is designed to scale to larger numbers of Slices (cores). Core Fusion requires centralized structures for coordnating fetch, steering, and commit across fused cores, while we distributed the structures. Also, we take a more Distributed ILP approach to operand communication and other inter-Slice communication by using a switched interconnect between Slices instead of Core Fusion's operand crossbar. Also, the Sharing Architecture is designed as a 2D fabric which is created across a large manycore with 100's of cores which can avoid some of the fragmentation challenges that smaller configurations like Core Fusion can have. The interconnect of Slices to Cache is also designed to be flexible across the entire machine unlike Core Fusion which has a shared cache. This can lead to better efficiency and provides a way to partition cache traffic and working sets.</p><p>WiDGET <ref type="bibr" target="#b62">[64]</ref> decouples computation resources to enable flexibility in provisioning execution units to optimize power-performance. WiDGET only studies trading off single-stream performance for power, while the Sharing Architecture trades ILP for TLP and enables an IaaS Cloud user to use their own utility function. WiDGET uses a seaof-resources design with a hierarchical operand interconnect with local broadcast while the Sharing Architecture uses a point-to-point switched interconnect. WiDGET's hierarchical design restricts what resources can be shared while the Sharing Architecture enables wider sharing. Unlike WiD-GET's in-order design, the Sharing Architecture is Out-of-Order including its memory system. Conjoined-cores <ref type="bibr" target="#b30">[32]</ref> allows resource sharing between adjacent cores of a chip multiprocessor. The degree of sharing depends on the topology, and the shared resources are allocated to different processors in a time-partitioned manner. The Sharing Architecture tears down the rigid boundary of cores, allowing more flexible sub-core configurations. Conjoined-cores has high wiring overhead and therefore only large structures are shared while we aim at finer grain sharing.</p><p>Clustered Architectures <ref type="bibr" target="#b43">[45]</ref> and dynamically configurable clustered architectures <ref type="bibr" target="#b6">[8]</ref> exploit ILP when there is latency between functional units. While dynamic clustered architectures trade off parallelism for latency, they do not enable wide spread sharing and the reconfigurability of Cache resources.</p><p>SMT <ref type="bibr" target="#b35">[37,</ref><ref type="bibr" target="#b58">60]</ref> enables trade-offs of ILP and TLP. Like the Sharing Architecture, SMT does not require recompilation, but SMT does not enable fine grain resource sharing between cores and has rigid core boundaries. The Sharing Architecture does not time multiplex resources, unlike SMT. MorphCore <ref type="bibr" target="#b27">[29]</ref> transforms a high-performance OoO core into a highly-threaded in-order SMT core. Unlike the Sharing Architecture, MorphCore is unable to scale up and down flexibly.</p><p>A heterogeneous CMP <ref type="bibr" target="#b31">[33]</ref> can combine superscalar cores for ILP with simple cores for TLP. Unlike the Sharing Architecture, heterogeneous architectures only allow selection out of a few fixed architectures. Also, if a heterogeneous architecture has the incorrect mixture of core types to match the application mix, efficiency is lost. Last, the Sharing architecture requires the design of one replicated architecture while a heterogeneous CMP can have higher design effort.</p><p>Building datacenters out of a heterogeneous mix of server and mobile cores can improve welfare and energy efficiency <ref type="bibr" target="#b16">[18]</ref>. Unfortunately statically choosing a mix of bigcores and small-cores in the datacenter can lead to a poor processor match when the workload deviates from the expected. For instance, if the workload favors a single type of core. Also, unlike the Sharing Architecture, this approach cannot adapt the core to a customer's utility function dynamically.</p><p>Application interference is prevalent in datacenters due to contention over shared hardware resources <ref type="bibr" target="#b26">[28]</ref>. Sharing last-level cache (LLC) and DRAM bandwidth degrades responsiveness of workloads. Paritioning a shared LLC potentially mitigates the negative performance effects of coscheduling <ref type="bibr" target="#b18">[20,</ref><ref type="bibr" target="#b24">26,</ref><ref type="bibr" target="#b45">47,</ref><ref type="bibr" target="#b55">57]</ref>. The Sharing Architecture builds upon this work by providing a flexible LLC along with the additive benefits of ALU configuration.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. High Level View of Sharing Architecture showing two Virtual Machines. VM1 has two VCores, VM2 has one VCore.</figDesc><graphic url="image-3.png" coords="2,410.64,137.37,104.20,59.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Resource Utility Functions for Different Applications as VMWare's VCloud<ref type="bibr" target="#b29">[31]</ref> system provide a richer ability to configure a VM instance, but the parameters are still limited to resources outside of the core such as core count, amount of DRAM, and disk size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Slices, Cache Banks and Interconnection Network different application due to how the application responds to different resources.In contrast to the throughput example, a second customer running an application (potentially the same application) may favor single-stream performance. This may be because they know that their application does not parallelize well across cores or may simply be a policy decision. Figure2bshows an example customer's utility functions where they favor (sequential perf ormance) 2 as a metric. P erf ormance 2 or P erf ormance 3 may be very reasonable metrics if a customer favors sequential time to completion and these metrics have much similarity to Energy * Delay 2 and Energy * Delay 3 used in energy efficient computing research. Finally, a single customer and a single application may have different utility functions as a function of time of day or program phase<ref type="bibr" target="#b32">[34]</ref>. Programs commonly have different phases and if properly expressed, a per-phase machineapplication fit can be made.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Slice Overview and Inter-Slice Network</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Flow Chart of Rename Stage. Multi-stage global rename and local rename.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>add r1, r1, r0 1. Rename des:na:on register locally add p33, r1, r0 2. Send (p33, r1) to master slice 0 Slice 0: receive 3: p32, r1 4: p33, r1 5: p34, r1 Slice 0: broadcast At most 9 renamed register to slave slices (b) Multi-stage Rename</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 Figure 7 .</head><label>67</label><figDesc>Figure 6. Global Rename</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Distributed LSQ and LS Sorting</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. LSQ Bank</figDesc><graphic url="image-158.png" coords="8,166.76,99.83,81.26,56.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. Area Decomposition including 64KB L2 cache</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 12 .</head><label>12</label><figDesc>Figure 12. Scalability of VCore Performance. Includes One Cycle Insertion Delay Plus One Cycle per Hop. Normalized to One Slice with 128KB L2 Cache VCore</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>(gcc Utility1) versus Figure 14b (gcc Utility2), simply changing the utility function can drastically change which configuration provides peak utility. Likewise when we compare Figure 14d (bzip Utility2) versus Figure</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>a c h e B a n k s ( lo g 2 a c h e B a n k s ( lo g 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice Slice</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Component Branch BTB Scoreboard Issue Load Store ROB Local Global Physical Replicated vs. Partitioned Structures</figDesc><table><row><cell>Predictor</cell><cell>Window Queue Queue</cell><cell>RAT RAT</cell><cell>RF</cell></row><row><cell>Replicated</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Partitioned</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Register File 6% BTB&amp;Predictor 4% 16 KB 2-way L1 Dcache 24% 16 KB 2-way L1 Icache 24%</head><label></label><figDesc></figDesc><table><row><cell></cell><cell cols="2">Global Rename</cell><cell></cell></row><row><cell>Issue Window</cell><cell></cell><cell>1%</cell><cell></cell></row><row><cell>4%</cell><cell>Instruction Buffer 11%</cell><cell>LSQ 8%</cell><cell cols="2">Multiplier 2%</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ROB 6%</cell><cell>ALUs 1%</cell><cell>Local Rename</cell><cell>Routers 2%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Sharing Overhead</cell><cell>2%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>8%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Waitlist</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>1%</cell><cell>Scoreboard</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>2%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Added Pipeline</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>0%</cell></row><row><cell cols="5">Figure 10. Area Decomposition without L2 cache</cell></row><row><cell></cell><cell>64KB 4-</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>way L2 Dcache 35% Register File 4% BTB&amp;Predictor 3% 16 KB 2-way L1 Dcache 16% 16 KB 2-way L1 Icache 16%</head><label></label><figDesc></figDesc><table><row><cell cols="2">Instruction Buffer</cell><cell></cell></row><row><cell>Issue Window</cell><cell>7%</cell><cell></cell></row><row><cell>3%</cell><cell cols="3">Global Rename</cell></row><row><cell></cell><cell>LSQ</cell><cell>0%</cell></row><row><cell></cell><cell>5%</cell><cell cols="2">Multiplier</cell></row><row><cell></cell><cell></cell><cell>ROB</cell><cell>2%</cell></row><row><cell></cell><cell></cell><cell>4%</cell><cell>Routers</cell></row><row><cell></cell><cell></cell><cell>ALUs</cell><cell>2%</cell></row><row><cell></cell><cell></cell><cell>1%</cell><cell>Local Rename</cell></row><row><cell></cell><cell></cell><cell cols="2">Sharing Overhead</cell><cell>1%</cell></row><row><cell></cell><cell></cell><cell></cell><cell>5%</cell><cell>Added</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Waitlist</cell><cell>Pipeline</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1%</cell><cell>Scoreboard</cell><cell>0%</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 .</head><label>2</label><figDesc>Number of Functional Units/Slice 2 ROB size 64 Number of Physical Registers 128 Store Buffer Size 8 Number of Local Registers/Slice 64 Maximum In-flight Loads 8 Base Slice Configuration</figDesc><table><row><cell cols="2">Issue Window Size</cell><cell cols="3">32 Memory Delay</cell><cell>100</cell></row><row><cell cols="2">Load/Store Queue Size</cell><cell>32</cell><cell></cell><cell></cell></row><row><cell cols="5">Level Size(KB) Block Size(Byte) Associativity Hit Delay</cell></row><row><cell>L1D</cell><cell>16</cell><cell>64</cell><cell>2</cell><cell>3</cell></row><row><cell>L1I</cell><cell>16</cell><cell>64</cell><cell>2</cell><cell>3</cell></row><row><cell>L2</cell><cell>64*2</cell><cell>64</cell><cell>4</cell><cell>distance*2+4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 .</head><label>3</label><figDesc>Base Cache Configurations: L2 cache hit delays depends on the distance to the cache bank</figDesc><table><row><cell>1 2 3 4 5 Normalized Performance</cell><cell></cell><cell>apache bzip gcc astar libquantum perlbench sjeng hmmer</cell><cell cols="2">gobmk mcf omnetpp h264ref dedup swaptions ferret</cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4 Number of Slices 5</cell><cell>6</cell><cell>7</cell><cell>8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Since the default Sharing Architecture requires two base cycles to transport operands between Slices and another cycle network delay for additional hops, performance can degrade for larger VCore configurations. For PARSEC, benchmarks use four threads on four equally configured VCores which share an L2 Cache. The number of Slices per VCore is varied in unison. Compared with SPEC, PARSEC benchmarks have less ILP; the speedup is bounded by 2.</figDesc><table><row><cell>2 4 6 8 10 12 14 Normalized Performance</cell><cell></cell><cell>apache bzip gcc astar libquantum perlbench sjeng hmmer</cell><cell>gobmk mcf omnetpp h264ref dedup swaptions ferret</cell><cell></cell></row><row><cell>0</cell><cell>0</cell><cell>64KB</cell><cell>128KB 256KB 512KB L2 Cache Size (Fixed 2-slice) 1MB 2MB 4MB</cell><cell>8MB</cell></row><row><cell cols="5">Figure 13. Performance Scaling with Cache Size. Normal-</cell></row><row><cell cols="4">ized to No L2 2-Slice VCore.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 .</head><label>6</label><figDesc>Optimal VCore Configurations in Different Markets (L2 Cache Size in KB, Slice Number)</figDesc><table><row><cell></cell><cell>6</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Utility Gain</cell><cell>2 3 4</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>0</cell><cell>200</cell><cell>400 Permutation Number 600</cell><cell>800</cell><cell>1000</cell></row><row><cell cols="7">Figure 15. Utility Gain Compared with Static Fixed Archi-</cell></row><row><cell cols="7">tecture (Optimal Configuration Across Benchmark Suite and</cell></row><row><cell cols="7">the Three Utility Functions). Points represent utility gain of</cell></row><row><cell cols="7">one benchmark and utility mixed with another benchmark</cell></row><row><cell cols="2">and utility.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell>'s op-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 .</head><label>7</label><figDesc>Optimal VCore Configurations for 10 Phases in "gcc "Benchmark: The dynamic average is compared with the static best average, Dyn/Static gain takes into account 10000 cycles reconfiguration cost after each phase that reconfigures L2 cache, and 500 cycles reconfiguration cost after each phase that reconfigures only Slices.</figDesc><table><row><cell>Metric</cell><cell cols="5">Core Configurations phase1 phase2 phase3 phase4 phase5 phase6 phase7 phase8 phase9 phase10 optimal GME Dyn/Static Gain</cell></row><row><cell>perf ormance/area</cell><cell>L2 cache size (KB) 256 256 128 256 128 128 256 128 Slice Number 2 2 2 2 2 2 2 1</cell><cell>64 1</cell><cell>64 1</cell><cell>128 2</cell><cell>9.1%</cell></row><row><cell>perf ormance 2 /area</cell><cell cols="2">L2 cache size (KB) 512 256 256 512 512 256 256 128 128 Slice Number 4 4 4 3 3 3 2 2 2</cell><cell>512 2</cell><cell>512 3</cell><cell>15.1%</cell></row><row><cell>perf ormance 3 /area</cell><cell cols="2">L2 cache size (KB) 1024 1024 1024 512 1024 256 1024 128 128 Slice Number 5 4 4 3 4 3 4 2 2</cell><cell>512 3</cell><cell>1024 4</cell><cell>19.4%</cell></row></table><note><p>Sharing Architecture provides flexibility to reconfigure VCores and VMs as a workload changes. It is well known that different program phases can require different resources to perform optimally. For instance, some program phases</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 .</head><label>8</label><figDesc>Taxonomy of Differences with Related Work. Dynamic OoO stands for dynamic instruction assignment, transport, and ordering.</figDesc><table><row><cell>Features</cell><cell cols="9">Distributed TRIPS/ Core WiDGET Conjoined Clustered Heterogeneous SMT/ Sharing</cell></row><row><cell></cell><cell>ILP</cell><cell cols="2">CLP Fusion</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Morph Architecture</cell></row><row><cell>Scale up/down</cell><cell>Y</cell><cell>Y</cell><cell>N</cell><cell>Y</cell><cell>N</cell><cell>N</cell><cell>N</cell><cell>N</cell><cell>Y</cell></row><row><cell>Distributed</cell><cell>Y</cell><cell>Y</cell><cell>N</cell><cell>N</cell><cell>N</cell><cell>N</cell><cell>N</cell><cell>N</cell><cell>Y</cell></row><row><cell>Switched</cell><cell>Y</cell><cell>Y</cell><cell>N</cell><cell>N</cell><cell>N</cell><cell>N</cell><cell>N</cell><cell>N</cell><cell>Y</cell></row><row><cell>Symmetric</cell><cell>Y</cell><cell>Y</cell><cell>Y</cell><cell>Y</cell><cell>Y</cell><cell>Y</cell><cell>N</cell><cell>Y</cell><cell>Y</cell></row><row><cell>Dynamic OoO</cell><cell>N</cell><cell>N</cell><cell>Y</cell><cell>N</cell><cell>Y</cell><cell>Y</cell><cell>Y/N</cell><cell>Y/N</cell><cell>Y</cell></row><row><cell>ISA Compatible</cell><cell>Y</cell><cell>N</cell><cell>Y</cell><cell>Y</cell><cell>Y</cell><cell>Y</cell><cell>Y</cell><cell>Y</cell><cell>Y</cell></row><row><cell>Partition L2</cell><cell>Y</cell><cell>Y</cell><cell>N</cell><cell>N</cell><cell>N</cell><cell>N</cell><cell>N</cell><cell>N</cell><cell>Y</cell></row><row><cell>Multi-Metric</cell><cell>N</cell><cell>Y</cell><cell>N</cell><cell>N</cell><cell>N</cell><cell>N</cell><cell>N</cell><cell>N</cell><cell>Y</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We acknowledge <rs type="person">Yaosheng Fu</rs> for developing PriME cache simulator, <rs type="person">Sharad Malik</rs> for the suggestions of comparison with heterogeneous data center, Princeton Research Com-puting for providing computation support. This work is supported by <rs type="funder">NSF</rs> Grant No <rs type="grantNumber">CCF-1217553</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_Jws53vj">
					<idno type="grant-number">CCF-1217553</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="http://aws.amazon.com/ec2/" />
		<title level="m">Amazon elastic compute cloud</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Google</forename><surname>Apps</surname></persName>
		</author>
		<ptr target="http://www.google.com/apps/business/index.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Amazon simple storage service</title>
		<ptr target="http://aws.amazon.com/s3/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An adaptive hybrid elasticity controller for cloud infrastructures</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ali-Eldin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tordsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Elmroth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Network Operations and Management Symposium (NOMS)</title>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="page" from="204" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Siblingrivalry: online autotuning through local competitions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ansel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pacula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">L</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Olszewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>-M. O'reilly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Amarasinghe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 international conference on Compilers, architectures and synthesis for embedded systems, CASES &apos;12</title>
		<meeting>the 2012 international conference on Compilers, architectures and synthesis for embedded systems, CASES &apos;12</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="91" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Armbrust</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Griffith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Konwinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<idno>UCB/EECS-2009-28</idno>
		<title level="m">Above the Clouds: A Berkeley View of Cloud Computing</title>
		<imprint>
			<date type="published" when="2009-02">Feb 2009</date>
		</imprint>
		<respStmt>
			<orgName>EECS Department, University of California, Berkeley</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dynamically managing the communication-parallelism trade-off in future clustered processors</title>
		<author>
			<persName><forename type="first">R</forename><surname>Balasubramonian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dwarkadas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Albonesi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Architecture, 2003. Proceedings. 30th Annual International Symposium on</title>
		<imprint>
			<date type="published" when="2003-06">june 2003</date>
			<biblScope unit="page" from="275" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Amazon Web Services Blog</title>
		<author>
			<persName><forename type="first">J</forename><surname>Barr</surname></persName>
		</author>
		<ptr target="http://aws.typepad.com/aws/2011/01/amazon-s3-bigger-and-busier-than-ever.html" />
		<imprint>
			<date>January 28. January 28, 20111</date>
			<biblScope unit="volume">201</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The GEM5 simulator</title>
		<author>
			<persName><forename type="first">N</forename><surname>Binkert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Beckmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Reinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hestness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Hower</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sardashti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sewell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shoaib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vaish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGARCH Comput. Archit. News</title>
		<idno type="ISSN">0163-5964</idno>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2011-08">Aug. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Scaling to the end of silicon with EDGE architectures</title>
		<author>
			<persName><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Keckler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mckinley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dahlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Burrill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yoder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="44" to="55" />
			<date type="published" when="2004-07">july 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Market-oriented cloud computing: Vision, hype, and reality for delivering it services as computing utilities</title>
		<author>
			<persName><forename type="first">R</forename><surname>Buyya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Yeo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venugopal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th IEEE International Conference on High Performance Computing and Communications</title>
		<meeting>the 10th IEEE International Conference on High Performance Computing and Communications</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Understanding the energy efficiency of smt and cmp with multiclustering</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jagannathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Reinman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tamir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Low Power Electronics and Design, 2005. ISLPED &apos;05. Proceedings of the 2005 International Symposium on</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="48" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Paragon: Qos-aware scheduling for heterogeneous datacenters</title>
		<author>
			<persName><forename type="first">C</forename><surname>Delimitrou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eighteenth international conference on Architectural support for programming languages and operating systems, ASPLOS &apos;13</title>
		<meeting>the eighteenth international conference on Architectural support for programming languages and operating systems, ASPLOS &apos;13<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="77" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName><surname>Eucalyptus</surname></persName>
		</author>
		<author>
			<persName><surname>Eucalyptus</surname></persName>
		</author>
		<ptr target="http://www.eucalyptus.com/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Clearing the Clouds: A Study of Emerging Scaleout Workloads on Modern Hardware</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Adileh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kocberber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Volos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alisafaee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jevdjic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kaynak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">17th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Implementation and evaluation of on-chip network architectures</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gratz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Keckler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Design, 2006. ICCD 2006. International Conference on</title>
		<imprint>
			<date type="published" when="2006-10">oct. 2006</date>
			<biblScope unit="page" from="477" to="484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Navigating heterogeneous processors with market mechanisms</title>
		<author>
			<persName><forename type="first">M</forename><surname>Guevara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">High Performance Computer Architecture (HPCA2013)</title>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page" from="95" to="106" />
		</imprint>
	</monogr>
	<note>IEEE 19th International Symposium on</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cloud-scale resource management: challenges and techniques</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Shanmuganathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Holler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ahmad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd USENIX conference on Hot topics in cloud computing, HotCloud&apos;11</title>
		<meeting>the 3rd USENIX conference on Hot topics in cloud computing, HotCloud&apos;11<address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="3" to="3" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A framework for providing quality of service in chip multi-processors</title>
		<author>
			<persName><forename type="first">F</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Solihin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Iyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 40th Annual IEEE/ACM International Symposium on Microarchitecture<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="343" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Mesos: a platform for fine-grained resource sharing in the data center</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hindman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Konwinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghodsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th USENIX conference on Networked systems design and implementation, NSDI&apos;11</title>
		<meeting>the 8th USENIX conference on Networked systems design and implementation, NSDI&apos;11</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="22" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Application heartbeats for software performance and health</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Eastep</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Santambrogio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming</title>
		<meeting>the ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="347" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A generalized software framework for accurate and efficient managment of performance goals</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Santambrogio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Leva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMSOFT</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Core Fusion: accommodating software diversity in chip multiprocessors</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ipek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kirman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kirman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th annual international symposium on Computer architecture</title>
		<meeting>the 34th annual international symposium on Computer architecture</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="186" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Quincy: fair scheduling for distributed computing clusters</title>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Prabhakaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Currey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Wieder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGOPS 22nd symposium on Operating systems principles, SOSP &apos;09</title>
		<meeting>the ACM SIGOPS 22nd symposium on Operating systems principles, SOSP &apos;09</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="261" to="276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Qos policies and architecture for cache/memory in cmp platforms</title>
		<author>
			<persName><forename type="first">R</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Illikkal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Makineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Solihin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reinhardt</surname></persName>
		</author>
		<idno>SIGMET- RICS &apos;07</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 ACM SIGMETRICS international conference on Measurement and modeling of computer systems</title>
		<meeting>the 2007 ACM SIGMETRICS international conference on Measurement and modeling of computer systems</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="25" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Web search using mobile cores: quantifying and mitigating the price of efficiency</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">Janapa</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chilimbi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Vaid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th annual international symposium on Computer architecture, ISCA &apos;10</title>
		<meeting>the 37th annual international symposium on Computer architecture, ISCA &apos;10</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="314" to="325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Measuring interference between live datacenter applications</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kambadur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Moseley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis, SC &apos;12</title>
		<meeting>the International Conference on High Performance Computing, Networking, Storage and Analysis, SC &apos;12</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Morphcore: An energy-efficient microarchitecture for high performance ilp and high throughput tlp</title>
		<author>
			<persName><forename type="first">K</forename><surname>Khubaib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Suleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hashemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wilkerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Microarchitecture (MICRO), 2012 45th Annual IEEE/ACM International Symposium on</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="305" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Composable lightweight processors</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sethumadhavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Govindan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ranganathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Keckler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 40th Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="381" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Enabling a marketplace of clouds: Vmware&apos;s vcloud director</title>
		<author>
			<persName><forename type="first">O</forename><surname>Krieger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcgachey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kanevsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGOPS Oper. Syst. Rev</title>
		<idno type="ISSN">0163-5980</idno>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="103" to="114" />
			<date type="published" when="2010-12">Dec. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Conjoined-core chip multiprocessing</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Tullsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 37th annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="195" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Single-ISA heterogeneous multi-core architectures for multithreaded workload performance</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Tullsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">I</forename><surname>Farkas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. International Symposium on</title>
		<meeting>International Symposium on</meeting>
		<imprint>
			<date type="published" when="2004-06">june 2004</date>
			<biblScope unit="page" from="64" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Heterogeneous Chip Multiprocessors</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tullsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<idno type="ISSN">0018-9162</idno>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="32" to="38" />
			<date type="published" when="2005-11">nov. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">At the forge: Amazon web services</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Lerner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linux J</title>
		<idno type="ISSN">1075-3583</idno>
		<imprint>
			<biblScope unit="issue">143</biblScope>
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Understanding the energy efficiency of simultaneous multithreading</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Skadron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISLPED &apos;04. Proceedings of the 2004 International Symposium on</title>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
			<biblScope unit="page" from="44" to="49" />
		</imprint>
	</monogr>
	<note>Low Power Electronics and Design</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Converting thread-level parallelism to instruction-level parallelism via simultaneous multithreading</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Eggers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Stamm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Tullsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Comput. Syst</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="322" to="354" />
			<date type="published" when="1997-08">Aug. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Macaskill</surname></persName>
		</author>
		<ptr target="http://don.blogs.smugmug.com/2008/06/03/skynet-lives-aka-ec2-smugmug/" />
		<title level="m">SkyNet Lives! (aka EC2 at Smug-Mug)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Virtual hierarchies to support server consolidation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Marty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th Annual International Symposium on Computer Architecture, ISCA &apos;07</title>
		<meeting>the 34th Annual International Symposium on Computer Architecture, ISCA &apos;07</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="46" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Combining branch predictors</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mcfarling</surname></persName>
		</author>
		<idno>TN-36</idno>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
		<respStmt>
			<orgName>WRL Technical Note</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Power management of online data-intensive services</title>
		<author>
			<persName><forename type="first">D</forename><surname>Meisner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Sadler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Barroso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-D</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th annual international symposium on Computer architecture, ISCA &apos;11</title>
		<meeting>the 38th annual international symposium on Computer architecture, ISCA &apos;11</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="319" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Cacti 6.0: A tool to model large caches</title>
		<author>
			<persName><forename type="first">Naveen</forename><surname>Muralimanohar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajeev</forename><surname>Balasubramonian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<idno>HPL-2009-85</idno>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
		<respStmt>
			<orgName>HP Laboratories</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><surname>Openstack</surname></persName>
		</author>
		<ptr target="http://www.openstack.org/" />
		<title level="m">Openstack Cloud Software</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Automated control of multiple virtualized resources</title>
		<author>
			<persName><forename type="first">P</forename><surname>Padala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Uysal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Merchant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th ACM European conference on Computer systems, EuroSys &apos;09</title>
		<meeting>the 4th ACM European conference on Computer systems, EuroSys &apos;09</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="13" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Complexityeffective superscalar processors</title>
		<author>
			<persName><forename type="first">S</forename><surname>Palacharla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Computer Architecture</title>
		<meeting>the International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1997-06">June 1997</date>
			<biblScope unit="page" from="206" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Amazon S3 for science grids: a viable solution?</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Palankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Iamnitchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ripeanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Garfinkel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DADC &apos;08: Proceedings of the 2008 international workshop on Dataaware distributed computing</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="55" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Utility-based cache partitioning: A low-overhead, high-performance, runtime mechanism to partition shared caches</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 39th Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="423" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Resourceexchange: Latency-aware scheduling in virtualized environments with high performance fabrics</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ranadive</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gavrilovska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schwan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cluster Computing (CLUSTER), 2011 IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="45" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Exploiting ILP, TLP, and DLP with the polymorphous TRIPS architecture</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sankaralingam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Keckler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Computer Architecture</title>
		<meeting>the International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="422" to="433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Distributed microarchitectural protocols in the TRIPS prototype processor</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sankaralingam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Desikan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Drolia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Govindan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gratzf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hanson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ranganathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sethumadhavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shariff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shivakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Keckler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
		<idno>MICRO-39. 39</idno>
	</analytic>
	<monogr>
		<title level="m">th Annual IEEE/ACM International Symposium on</title>
		<imprint>
			<date type="published" when="2006-12">2006. dec. 2006</date>
			<biblScope unit="page" from="480" to="491" />
		</imprint>
	</monogr>
	<note>Microarchitecture</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Enabling technologies for self-aware adaptive systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Santambrogio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Eastep</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adaptive Hardware and Systems (AHS), 2010 NASA/ESA Conference on</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="149" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Fully Distributed Register Files for Heterogeneous Clustered Mircroarchitectures</title>
		<author>
			<persName><forename type="first">B</forename><surname>Santithorn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
		<respStmt>
			<orgName>Georgia Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Design and implementation of the TRIPS primary memory system</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sethumadhavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Keckler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Desikan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Design, 2006. ICCD 2006. International Conference on</title>
		<imprint>
			<date type="published" when="2006-10">oct. 2006</date>
			<biblScope unit="page" from="470" to="476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Late-binding: Enabling unordered load-store queues</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sethumadhavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roesner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Keckler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th Annual International symposium on Computer Architecture</title>
		<meeting>the 34th Annual International symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="347" to="357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Cloudscale: elastic resource scaling for multi-tenant cloud systems</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wilkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd ACM Symposium on Cloud Computing, SOCC &apos;11</title>
		<meeting>the 2nd ACM Symposium on Cloud Computing, SOCC &apos;11</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Multi-tiered on-demand resource scheduling for vm-based data center</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 9th IEEE/ACM International Symposium on Cluster Computing and the Grid, CCGRID &apos;09</title>
		<meeting>the 2009 9th IEEE/ACM International Symposium on Cluster Computing and the Grid, CCGRID &apos;09<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="148" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A new memory monitoring scheme for memory-aware scheduling and partitioning</title>
		<author>
			<persName><forename type="first">G</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Devadas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rudolph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">High-Performance Computer Architecture, 2002. Proceedings. Eighth International Symposium on</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="117" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<author>
			<persName><forename type="first">S</forename><surname>Swanson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Michelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schwerin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Oskin</surname></persName>
		</author>
		<author>
			<persName><surname>Wavescalar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 36th annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page">291</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Scalar operand networks: on-chip interconnect for ilp in partitioned architectures</title>
		<author>
			<persName><forename type="first">M</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Amarasinghe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">High-Performance Computer Architecture, 2003. HPCA-9 2003. Proceedings. The Ninth International Symposium on</title>
		<imprint>
			<date type="published" when="2003-02">feb. 2003</date>
			<biblScope unit="page" from="341" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Simultaneous multithreading: maximizing on-chip parallelism</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tullsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Eggers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the international symposium on Computer architecture, ISCA &apos;95</title>
		<meeting>the international symposium on Computer architecture, ISCA &apos;95</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="392" to="403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">VMware Virtual Appliance Marketplace: Virtual Applications for the Cloud</title>
		<author>
			<persName><surname>Vmware</surname></persName>
		</author>
		<ptr target="http://www.vmware.com/appliances/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Von Eicken</surname></persName>
		</author>
		<ptr target="http://blog.rightscale.com/2009/10/05/amazon-usage-estimates/" />
		<title level="m">Amazon Usage Estimates, RightScale Blog</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Baring it all to software: Raw machines</title>
		<author>
			<persName><forename type="first">E</forename><surname>Waingold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Srikrishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Finch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Babb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Amarasinghe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="86" to="93" />
			<date type="published" when="1997-09">sep 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">WiDGET: Wisconsin decoupled grid execution tiles</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGARCH Comput. Archit. News</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="2" to="13" />
			<date type="published" when="2010-06">June 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">On-chip interconnection architecture of the Tile Processor</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wentzlaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ramey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="15" to="31" />
			<date type="published" when="2007-09">Sept. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
