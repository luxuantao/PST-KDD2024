<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">REBEL: Relation Extraction By End-to-end Language generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Pere-Lluís</forename><forename type="middle">Huguet</forename><surname>Cabot</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sapienza University of Rome</orgName>
								<address>
									<settlement>Babelscape</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
							<email>navigli@diag.uniroma1.it</email>
							<affiliation key="aff1">
								<orgName type="institution">Sapienza University of Rome</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">REBEL: Relation Extraction By End-to-end Language generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Extracting relation triplets from raw text is a crucial task in Information Extraction, enabling multiple applications such as populating or validating knowledge bases, factchecking, and other downstream tasks. However, it usually involves multiple-step pipelines that propagate errors or are limited to a small number of relation types. To overcome these issues, we propose the use of autoregressive seq2seq models. Such models have previously been shown to perform well not only in language generation, but also in NLU tasks such as Entity Linking, thanks to their framing as seq2seq tasks. In this paper, we show how Relation Extraction can be simplified by expressing triplets as a sequence of text and we present REBEL, a seq2seq model based on BART that performs end-to-end relation extraction for more than 200 different relation types. We show our model's flexibility by finetuning it on an array of Relation Extraction and Relation Classification benchmarks, with it attaining state-of-the-art performance in most of them.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Extracting relational facts from text has been an ongoing part of Natural Language Processing. The ability to extract semantic relationships between entities from text can be used to go from unstructured raw text to structured data that can be leveraged in an array of downstream tasks and applications, such as the construction of Knowledge Bases.</p><p>Traditionally this task has been approached as a two-step problem. First, the entities are extracted from text as in Named Entity Recognition (NER). Second, Relation Classification (RC) checks whether there exists any pairwise relation between the extracted entities <ref type="bibr" target="#b33">(Zeng et al., 2014;</ref><ref type="bibr" target="#b38">Zhang et al., 2017)</ref>. However, identifying which entities truly share a relation can become a bottleneck, requiring additional steps such as negative sampling and expensive annotation procedures.</p><p>More recently, end-to-end approaches have been used to tackle both tasks simultaneously <ref type="bibr" target="#b20">(Miwa and Sasaki, 2014;</ref><ref type="bibr" target="#b23">Pawar et al., 2017;</ref><ref type="bibr" target="#b15">Katiyar and Cardie, 2017;</ref><ref type="bibr" target="#b10">Eberts and Ulges, 2020)</ref>. This task is usually referred to as Relation Extraction or Endto-End Relation Extraction (RE). In this scenario, a model is trained simultaneously on both objectives. Specific parts of the model can be assigned different tasks of the pipeline, such as NER, on the one hand, and classifying the relations between the predicted entities (RC), on the other. By training both tasks simultaneously, the model benefits from the information bias between the tasks as in multi-task setups <ref type="bibr" target="#b7">(Caruana, 1998)</ref>, improving performance on the end-to-end RE task.</p><p>Although successful, these models are often complex, with task-focused elements that need to be adapted to the number of relation or entity types, or they are not flexible enough to work for texts of different nature (sentence vs. document level) or domains. Moreover, they usually require long training times in order to be fine-tuned on new data.</p><p>In this paper, we present REBEL (Relation Extraction By End-to-end Language generation), an autoregressive approach that frames Relation Extraction as a seq2seq task, together with the REBEL dataset, a large-scale distantly supervised dataset, obtained by leveraging a Natural Language Inference model. Our approach provides some upsides over previous end-to-end approaches thanks to our adoption of a simple triplet decomposition into a text sequence. By pre-training an Encoder-Decoder Transformer (BART) using our new dataset, REBEL achieves state-of-the-art performance on an array of RE baselines within a few epochs of fine-tuning. Its simplicity makes it highly flexible to adapt to new domains or longer documents. As the same model weights are still utilized after the pre-training phase, there is no need to train model-specific components from scratch, making training more efficient.</p><p>Moreover, although it is devised for Relation Extraction, the same approach can be generalized to Relation Classification, achieving competitive results.</p><p>We make REBEL available<ref type="foot" target="#foot_0">1</ref> both as a standalone model that can extract more than 200 different relation types, and as a pre-trained RE model that can be easily fine-tuned on new RE and RC datasets. We also provide the REBEL dataset and the pipeline to extract high-quality RE datasets from any Wikipedia dump.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Relation Extraction</head><p>The term Relation Extraction is often used in the literature for different tasks and setups in the literature <ref type="bibr" target="#b28">(Taillé et al., 2020)</ref>. For clarity, we refer to Relation Extraction (RE) as the task of extracting triplets of relations between entities from raw text, with no given entity spans, usually also called endto-end Relation Extraction. We refer to classifying the relation between two entities in a given context as Relation Classification (RC).</p><p>Early approaches tackled RE as a pipeline system, identifying the entities present in the text using Named Entity Recognition, and then classifying the relation, or lack of, between each pair of entities present in the text (RC). Therefore, early work made use of CNNs or LSTMs to exploit sentencelevel semantics and classify the relations between two given entities <ref type="bibr" target="#b33">(Zeng et al., 2014;</ref><ref type="bibr" target="#b40">Zhou et al., 2016)</ref>. Current approaches to Relation Classification use Transformer models, with <ref type="bibr" target="#b31">(Yamada et al., 2020)</ref> being the current state of the art by enhancing BERT <ref type="bibr" target="#b9">(Devlin et al., 2019)</ref> with entity-aware components.</p><p>Early end-to-end approaches using neural networks classified all word pairs present in the input text <ref type="bibr" target="#b20">(Miwa and Sasaki, 2014;</ref><ref type="bibr" target="#b23">Pawar et al., 2017)</ref>  Finally, there are pipeline systems that tackle both parts of Relation Extraction, NER, and RC, by jointly training components that take advantage of the information shared between the tasks. In these setups, entities are first extracted as in NER using BILOU tags and then a biaffine classifier extracts their relations, sharing part of the encoders for both tasks. These range from LSTMs <ref type="bibr" target="#b19">(Miwa and Bansal, 2016;</ref><ref type="bibr" target="#b15">Katiyar and Cardie, 2017)</ref> to CNNs <ref type="bibr" target="#b0">(Adel and Schütze, 2017;</ref><ref type="bibr" target="#b39">Zheng et al., 2017)</ref> and, lately, Transformer-based architectures <ref type="bibr" target="#b10">(Eberts and Ulges, 2020)</ref>, that explicitly predict and encode entity spans instead of the BILOU approach used in NER.</p><p>All recent sentence-level RE models are based on Transformer models, such as BERT <ref type="bibr" target="#b10">(Eberts and Ulges, 2020;</ref><ref type="bibr">Wang et al., 2020)</ref> or ALBERT <ref type="bibr" target="#b16">(Lan et al., 2020;</ref><ref type="bibr" target="#b29">Wang and Lu, 2020)</ref>. To tackle document-level RE, Eberts and Ulges (2021) use a pipeline approach jointly trained on a multi-task setup that leverages coreference resolution to operate at an entity level, rather than mentions.</p><p>While the aforementioned work highlights the relevance of Relation Extraction as a task, the lack of consistent baselines or a cohesive task definition has led to discrepancies in the use of datasets and the way models have been evaluated. <ref type="bibr" target="#b28">Taillé et al. (2020)</ref> explain the different issues in-so-far, and also make an attempt to unify RE evaluation and perform a fair comparison between systems.</p><p>We will follow their guidelines and use strict evaluation, unless specified, for which a relation is considered correct only if the head and tail entity surface forms are correctly extracted (i.e., fully overlap with the annotation), as well as the relation and entity types (if available for the dataset).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Seq2seq and Relation Extraction</head><p>The pipeline and table filling methods described so far have proved to perform well on RE, but still face some challenges. They often assume at most one relation type between each entity pair, and multiclass approaches do not take other predictions into account. For instance, they could predict two "birth dates" for the same head entity, or predict relations that are incompatible together. Moreover, they require all possible entity pairs to be inferred, which can become computationally expensive.</p><p>Seq2seq approaches for RE <ref type="bibr" target="#b36">(Zeng et al., 2018</ref><ref type="bibr" target="#b34">(Zeng et al., , 2020;;</ref><ref type="bibr" target="#b21">Nayak and Ng, 2020)</ref> offer some off-theshelf solutions to these problems. Decoding mechanisms can output the same entities multiple times, as well as conditioning future decoding on previous predictions, implicitly dealing with incompatible ones. However, as <ref type="bibr">Zhang et al. (2020)</ref> discuss, they still pose some issues. The triplets need to be linearized into a somewhat arbitrary sequential order, such as the alphabetical one. This issue is explored by <ref type="bibr" target="#b35">Zeng et al. (2019)</ref>, who use Reinforcement Learning to compute the extraction order for the triplets. Moreover, seq2seq approaches suffer from exposure bias, since at training time the prediction is always dependent on the gold-standard output. In <ref type="bibr">Zhang et al. (2020)</ref> a tree-decoding approach mitigates these issues while still using an autoregressive seq2seq approach.</p><p>In the meantime, seq2seq Transformer models, such as BART <ref type="bibr" target="#b17">(Lewis et al., 2020)</ref> or T5 <ref type="bibr" target="#b24">(Raffel et al., 2020)</ref> have been used in NLU tasks such as Entity Linking <ref type="bibr" target="#b6">(Cao et al., 2021)</ref>, AMR parsing <ref type="bibr" target="#b3">(Bevilacqua et al., 2021)</ref>, Semantic Role Labeling <ref type="bibr" target="#b5">(Blloshmi et al., 2021)</ref> or Word Sense-Disambiguation <ref type="bibr" target="#b4">(Bevilacqua et al., 2020)</ref> by reframing them as seq2seq tasks. Not only do they show strong performance, but they also showcase the flexibility of seq2seq models by not relying on predefined entity sets, but rather on the decoding mechanism, which can easily be extended to new or unseen entities.</p><p>For our model, we employ an Encoder-Decoder framework that can alleviate some of the previous issues seq2seq for RE has faced. While exposure bias can still occur, the attention mechanism enables long-distance dependencies as well as attending (or not) to the previously decoded output. Additionally, we devise a novel triplet linearization with a consistent triplet ordering that enables the model to leverage both the encoded input and the already decoded output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">REBEL</head><p>We tackle Relation Extraction and Classification as a generation task: we use an autoregressive model that outputs each triplet present in the input text. To this end, we employ BART-large <ref type="bibr" target="#b17">(Lewis et al., 2020)</ref> as the base model.</p><p>In a translation task, teacher forcing leverages pairs of text in two languages by conditioning the decoded text on the input. At training time the encoder receives the text in one language, and the decoder receives the text in the other language, outputting the prediction for the next token at each position.</p><p>In our approach, we translate a raw input sentence containing entities, together with implicit relations between them, into a set of triplets that explicitly refer to those relations. Therefore, we need to express the triplets as a sequence of tokens to be decoded by the model. We design a reversible linearization using special tokens that enable the model to output the relations in the text in the form of triplets while minimizing the number of tokens that need to be decoded.</p><p>For REBEL, we have as input the text from the dataset and, as output, the linearized triplets. If x is our input sentence and y the result of linearizing the relations in x as explained in Section 3.1, the task for REBEL is to autoregressively generate y given x:</p><formula xml:id="formula_0">p BART (y | x) = len(y) i=1 p BART (y i | y &lt;i , x)</formula><p>By fine-tuning BART on such a task, using the Cross-Entropy loss as in Summarization or Machine Translation, we maximize the log-likelihood of generating the linearized triplets given the input text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Triplets linearization</head><p>For RE, we want to express triplets as a sequence of tokens such that we can retrieve the original relations and minimize the number of tokens to be generated so as to make decoding more efficient. We introduce a set of new tokens, as markers, to achieve the aforementioned linearization. &lt;triplet&gt; marks the start of a new triplet with a new head entity, followed by the surface form of that entity in the input text. &lt;subj&gt; marks the end of the head entity and the start of the tail entity surface form. &lt;obj&gt; marks the end of the tail entity and the start of the relation between the head and tail entity, in its surface form. To obtain a consistent order in the decoded triplets, we sort the entities by their order of appearance in the input text and linearize the triplets following that order. Triplets will also be grouped by head entity. Therefore, the first triplet will be the one with the first appearing head entity and the following relation will be the one with the first appearing tail entity related to that head entity, followed by the rest of triplets with the same head entity. There is no need to specify the head entity each time, reducing the decoded text length. Once there are no more relations with that head entity, a new group of relations will start, with the second appearing head entity in the text, repeating the same process until there are no more triplets to be linearized. This mechanism is described in Algorithm 1.  <ref type="figure">1</ref> shows an example of the linearization process for a list of relations and an input sentence. Notice how This Must Be the Place appears twice as a subject, but it is present only once in the output as a subject entity. The original triplets can easily be retrieved by taking the special tokens into account. In RE datasets, the entity types are also present in the triplets and need to be predicted by the model. In that case, we apply a modification of Algorithm 1 where instead of &lt;subj&gt; and &lt;obj&gt;, we add new tokens for each entity type, such as &lt;per&gt; or &lt;org&gt;, for person or organization, respectively, and use them in the same fashion, indicating the type of the entity they follow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">REBEL dataset</head><p>Autoregressive transformer models such as BART or T5, have been shown to perform well on different generative tasks such as translation or summarization, but they do require large amounts of data to be trained. On the other hand, end-to-end relation extraction datasets are scarce and often small.</p><p>In <ref type="bibr" target="#b12">Elsahar et al. (2018)</ref> the T-REx dataset was created by devising a pipeline that extracts entities and relations from DBpedia abstracts to overcome this lack of big RE datasets. While the result is a large dataset, the quality of the annotation presents some issues. First, the use of a somewhat old entity linking tool <ref type="bibr" target="#b8">(Daiber et al., 2013)</ref> leads to entities being wrongly disambiguated. Since the relations are extracted by using those entities, this leads to missing or faulty relations. Moreover, most of the relations are extracted by assuming that, if the two entities are present in the text, the relation is therefore entailed by this presence.</p><p>We overcome these issues by expanding upon their pipeline to create a large silver dataset, used as pre-training for REBEL. We use Wikipedia<ref type="foot" target="#foot_1">2</ref> abstracts, that is, the part of each Wikipedia page before the table of contents, extracted using wikiextractor <ref type="bibr" target="#b2">(Attardi, 2015)</ref>. Then, we link the entities present in the text as hyperlinks, together with dates and values, to Wikidata entities using wikimapper<ref type="foot" target="#foot_2">3</ref> . From this, we extract all the relations present between those entities in Wikidata. Our system can be used with any Wikipedia dump, in multiple languages, enabling light and quick extraction using a multi-core process and SQL to avoid memory issues with the Wikidata dump.</p><p>However, a relation in Wikidata does not nec- essarily mean that the relation is entailed within the text. Although in <ref type="bibr" target="#b12">Elsahar et al. (2018)</ref> high reliability is claimed using this method, it has been shown to be noisy for frequent relations such as country or spouse, and we have found several related annotation issues. We utilize a pre-trained RoBERTa <ref type="bibr" target="#b18">(Liu et al., 2019)</ref> Natural Language Inference (NLI) model<ref type="foot" target="#foot_3">4</ref> to tackle this issue, and use its entailment prediction to filter those relations not entailed by the Wikipedia text. For each triplet, we input the text containing both entities from the Wikipedia abstract, and the triplet in their surface forms, subject + relation + object, separated by the &lt;sep&gt; token.</p><p>For the previous example and the triplet (Talking Heads, genre, new wave), we input: "This Must Be the Place" is a song by new wave band Talking Heads, released in November 1983 as the second single from its fifth album "Speaking in Tongues". &lt;sep&gt; Talking Heads genre new wave. We keep those triplets for which the entailment prediction is higher than 0.75. This proves successful in creating cleaner data in preliminary experiments and removing noisy annotations. We create three random splits, with validation and test each being 5% of the total data.</p><p>While this data extraction pipeline may still keep some noise, or exclude some relations that are entailed by the text, it enables an automatic way of gathering millions of entities and relations as a silver dataset, sufficient for training our model. We name our RE dataset creation tool cRocoDiLe: Automatic Relation Extraction Dataset with NLI filtering, and we make it available here<ref type="foot" target="#foot_4">5</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head><p>In this section, we describe the setup to train and evaluate REBEL for four different widely used RE datasets and one RC dataset. Statistics for all the datasets, including our pre-training dataset, can be found in Table <ref type="table" target="#tab_2">1</ref>.</p><p>While the training objective is on the autoregressive task, we evaluate the model on RE, extracting all the triplets from the generated output, and evaluating using Recall, Precision, and micro-F1 based on the labeled triplets. For a triplet to be considered correct, the entities and the relation, as well as their types, have to be the same as the labeled ones (this is known as "strict" evaluation in RE) using the evaluation code from <ref type="bibr" target="#b28">Taillé et al. (2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">REBEL dataset</head><p>We create this dataset by matching Wikipedia hyperlinks with Wikidata entities as explained in Section 3.2. To pre-train our model, we use a sentencelevel version of it, where only relations between entities present in each sentence are kept. We keep the 220 most frequent relations in the train split.</p><p>We fine-tune REBEL (using BART-large as the base model) on the silver dataset for 6 epochs. We refer to the resulting model as REBEL pre−training . While REBEL pre−training is in and of itself capable of extracting relations subsuming about 220 types, we show that it also functions as a base step for downstream RE and RC tasks, which are finetuned on top of it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">CONLL04</head><p>CONLL04 <ref type="bibr" target="#b26">(Roth and Yih, 2004</ref>) is composed of sentences from news articles, annotated with four entity types (person, organization, location and other) and five relation types (kill, work for, organization based in, live in and located in). To compare with previous work, we use the test split from <ref type="bibr" target="#b13">Gupta et al. (2016)</ref>, and the same validation set as Eberts and Ulges (2020), although we do not include the validation set at final training time.</p><p>For CONLL04 we expand REBEL to include entity types. As described in Section 3.1, we introduce a set of new tokens for each entity type. For CONLL04 these are &lt;peop&gt;, &lt;org&gt;, &lt;loc&gt;, &lt;other&gt;. We fine-tune on top of REBEL for 30 epochs and test on the best performing epoch on the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">DocRED</head><p>DocRED <ref type="bibr" target="#b32">(Yao et al., 2019)</ref> is a recent dataset created similarly to our pre-training data, by leveraging Wikipedia and Wikidata. However, it focuses on longer spans of text, with relations between entities at a document level. There is a distantly supervised portion, while the validation and (hidden) test sets are manually annotated. It includes annotations for 6 different entity types and 96 relation types.</p><p>Despite the fact that DocRED was originally designed as a relation classification task, we use the splits from Eberts and Ulges ( <ref type="formula">2021</ref>) and tackle it as a relation extraction task. In DocRED there are 6 entity types, consequently we use the tokens: &lt;loc&gt;, &lt;misc&gt;, &lt;per&gt;, &lt;num&gt;, &lt;time&gt; and &lt;org&gt; to indicate them.</p><p>We fine-tune on top of REBEL for 21 epochs and test on the last checkpoint, using a beam search of 10. For REBEL pre−training , we use a version trained on a filtered dataset not including any of the Wikipedia pages present in DocRED validation or test sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">NYT</head><p>NYT <ref type="bibr" target="#b25">(Riedel et al., 2010)</ref> is a dataset consisting of news sentences from the New York Times corpus. The dataset contains distantly annotated relations using FreeBase. We use the processed version of <ref type="bibr" target="#b36">Zeng et al. (2018)</ref> called NYT-multi, which contains overlapping entities, with three different entity types, and 24 relation types.</p><p>We use &lt;loc&gt;, &lt;per&gt; and &lt;org&gt; to indicate the 3 entity types. As for the 24 relation types, we map these to natural language expressions to match those seen at pre-training.</p><p>We fine-tune on top of REBEL for a maximum of 42 epochs and test on the best performing epoch on the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">ADE</head><p>ADE <ref type="bibr" target="#b14">(Gurulingappa et al., 2012)</ref> is a dataset on the biomedical domain, for which Adverse-Effects from drugs are annotated as pairs of drug and adverse-effect. The dataset provides 10-folds of train and test splits.</p><p>Drug and Adverse-Effect are the two entity types, and are always the subject and object entities for the single relation Adverse-Effect. Thus, we keep the same setup as with REBEL, using the &lt;subj&gt; token to distinguish between entity types, and removing the relation from the output, as it is always the same.</p><p>We fine-tune on top of REBEL for 25 epochs and evaluate using the last checkpoint for each fold in the dataset. Hyperparameters are selected by using 10% of the training data in the first fold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Re-TACRED</head><p>Re-TACRED <ref type="bibr" target="#b27">(Stoica et al., 2021</ref>) is a Relation Classification dataset, a revised version of the widely used TACRED <ref type="bibr" target="#b38">(Zhang et al., 2017)</ref>, fixing some of the issues pointed out by <ref type="bibr" target="#b1">Alt et al. (2020)</ref>. We want to extract the relation between two given entities, or the no_relation prediction, accounting for 63% of the 91,467 sentences in the dataset. To this end, we follow the approach from Zhou and Chen (2021) and <ref type="bibr" target="#b41">Zhou and Chen (2021)</ref> and mark the entities in the input text using punctuation marks. We do not include any entity-type information.</p><p>The output is treated as in previous tasks, and we do not force the decoding of the given entities, as we find it is sufficient to mark them in the input. We fine-tune on top of REBEL for 8 epochs and evaluate using the last checkpoint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Relation Extraction</head><p>For our pre-training task using the REBEL dataset, the model achieves 74 micro-F1 and 51 macro-F1. The dataset is created by distant supervision and serves as a pre-training step, however, it is worth noting its performance for predicting up to 220 different relation types.</p><p>Results on selected baselines are presented in Table 2, as well as additional metrics in Tables <ref type="table" target="#tab_6">3 and  4</ref>. We see an improvement across all datasets with pre-trained REBEL, achieving between 1.2 and 6.7 absolute F1 points improvement over recent state-of-the-art models. Using REBEL without the pre-training, we see that performance decreases, especially for smaller datasets or those with many entity types. Nevertheless, it still achieves competitive results, showing the flexibility of tackling RE as a seq2seq task using Transformer Encoder-Decoder models.</p><p>Additionally, REBEL shows a better performance than TANL, which was trained in a seq2seq fashion as well, using T5, with BART achieving  lower results for their approach. Therefore, our triplet linearization approach shows an improvement over other decoding strategies.</p><p>Results on RE for DocRED show that, despite being pre-trained on a sentence-based RE, REBEL can perform competitively on document-level RE, without the need for complex pipelines.</p><p>Moreover, by having a pre-trained version available, REBEL enables quick fine-tuning on newer domains, such as ADE, with different or fewer relation types, or including entity types. While in order to achieve the best performance we train for longer epochs, REBEL still needs fewer training steps to achieve competitive results compared to the other systems. For instance, <ref type="bibr">Paolini et al. (2021)</ref> train CONLL04 for up to 200 epochs, <ref type="bibr" target="#b29">Wang and Lu (2020)</ref> for up to 5,000, while our model needs less than 30 to achieve state-of-the-art results. Each of these systems uses large language models that can  be expensive to train, and shorter training time can significantly decrease the costs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Budget Training</head><p>We explore the training efficiency of REBEL pre_trained , and show the performance when fine-tuned on a low number of epochs. We experiment with CONLL04 and NYT compared to the non-pre-trained model, SpERT and TANL. SpERT was trained for just 20 epochs on CONLL04, while TANL in its non-multi-dataset version is trained for 200 epochs. We adjust each learning rate scheduler to the number of epochs and re-train each model for different epochs and seeds.</p><p>Figures <ref type="figure" target="#fig_1">2 and 3</ref> show how in just 8 epochs for CONLL04 and 3 for NYT, REBEL pre_trained can achieve a similar performance as the previous state of the art. While the experiments are on the dev set,   we do not observe big differences in performance between test and dev for these two datasets (see Appendix A.1 Tables <ref type="table" target="#tab_12">6 and 8</ref>). These results also highlight the importance of pre-training REBEL, as it achieves close to the final performance within a few epochs. Also note that while other models achieve lower performances, they also reach close to their final ones. Training for longer times and using early stopping on the validation performance are approaches used by most state-of-the-art models, but this can lead to long and expensive training times. Our experiments show that training for fewer epochs may lead to a small decrease in performance, but it brings the benefit of a more affordable training time. The comparison with other models should also take into account that our pre-trained approach has been previously trained on a massive dataset for 6 epochs, which combined with the fine-tuning in this experiment would lead to longer training times. However, all the other models also rely on pre-trained LM and, similarly, REBEL just needs to be pre-trained once and then quickly fine-tuned on these new datasets.</p><p>F1 LUKE <ref type="bibr" target="#b31">(Yamada et al., 2020)</ref> 90.3 RoBERTa LARGE + entity marker <ref type="bibr" target="#b41">(Zhou and Chen, 2021)</ref> 90.5 REBEL 90.4 REBEL pre−training 90.4 For REBEL, we evaluate using free generation in the RC setup. <ref type="bibr">Paolini et al. (2021)</ref> use likelihoodbased prediction which leads to an increase in performance by computing the likelihood of each relation type to be decoded with the two given entities. However, this also leads to an overhead of computation for datasets with a high number of relations such as Re-TACRED. For this reason, we use free generation and are unable to compute results for Re-TACRED using TANL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have presented REBEL, alongside a new distantly supervised dataset for pre-training. REBEL frames RE into a seq2seq task and, by leveraging BART, achieves state-of-the-art performances in an array of RE benchmarks. We have also shown its flexibility in adapting to new domains, by training on just a few epochs to attain results that are comparable to the previous state of the art, as well as the possibility of using it to perform Relation Classification.</p><p>We make REBEL pre−training available as a standalone RE for more than 200 relation types together with a pre-trained RE model to serve as a baseline when fine-tuning on new RE datasets. Nonetheless, REBEL is based on BART-large, which has a big parameter footprint. Therefore, we also plan to release a pre-trained REBEL-base using BART-base. This will enable quick and efficient RE.</p><p>Moreover, our dataset creation pipeline enables a quick and effortless way of obtaining large high-quality RE datasets in multiple languages from a Wikipedia dump. Since both Wikipedia and Wikidata are in constant change, our method provides a way to keep up with those changes and to have up-to-date RE datasets.</p><p>We leave to future work the possibility of using a multi-dataset approach as in <ref type="bibr">Paolini et al. (2021)</ref>, including both RE and RC datasets, and seeing if it retains or improves performance. Furthermore, using our silver dataset as pre-training could lead to improved performance for other systems, especially those which have shown better performance than REBEL without pre-training, such as Wang and Lu (2020) for CONLL04.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Results</head><p>Performances on the different dev sets can be found in Tables <ref type="table" target="#tab_12">6 and 8</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Precision</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Reproducibility</head><p>Experiments were performed using a single NVIDIA 3090 GPU with 64GB of RAM and Intel ® Core ™ i9-10900KF CPU.</p><p>The hyperparameters were manually tuned on the validation sets for each dataset, but mostly left at default values for BART. The ones used for the final results can be found in of parameters for REBEL is the same as for BARTlarge, 406M parameters, with a negligible increase from the newly added tokens.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Micro-F1 performances on CONLL04 dev set averaged over 5 seeds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Micro-F1 performances on NYT dev set averaged over 3 seeds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>using table representation, or table filling, re-framing the task into filling the slots of a table (the relations) where rows and columns are the words in the input. More recently, Wang and Lu (2020) used a similar table-based formulation, where the table is explicitly encoded using a table-sequence encoder.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>&lt;triplet&gt; This Must Be the Place &lt;subj&gt; Talking Heads &lt;obj&gt; performer &lt;subj&gt; Speaking in Tongues &lt;obj&gt; part of &lt;triplet&gt; Talking Heads &lt;subj&gt; new wave &lt;obj&gt; genre &lt;triplet&gt; Speaking in Tongues &lt;subj&gt; Talking Heads &lt;obj&gt; performerFigure 1: Example of the triplet linearization process for REBEL.</figDesc><table><row><cell>"This Must Be the Place" is a song by new wave band</cell><cell></cell></row><row><cell>Talking Heads, released in November 1983 as the</cell><cell></cell></row><row><cell>second single from its fifth album "Speaking in Tongues" (This Must Be the Place, performer, Talking Heads)</cell><cell>}</cell></row><row><cell>(Talking Heads, genre, new wave)</cell><cell></cell></row><row><cell>(This Must Be the Place, part of, Speaking in Tongues)</cell><cell></cell></row><row><cell>(Speaking in Tongues, performer, Talking Heads)</cell><cell></cell></row><row><cell>Algorithm 1: Transform a set of relations</cell><cell></cell></row><row><cell>R into a text sequence</cell><cell></cell></row><row><cell>Result:</cell><cell></cell></row><row><cell>lin_triplets with all triplets as a sequence</cell><cell></cell></row><row><cell>of text.</cell><cell></cell></row><row><cell>Input:</cell><cell></cell></row><row><cell>E = Entities;</cell><cell></cell></row><row><cell>R = Relations;</cell><cell></cell></row></table><note>sort() Sorts by placement in input text; Start: E = sort(E); lin_triplets = ""; for e ∈ E do R(e) = relations with e as subject; R(e) = sort(R(e)); lin_triplets += &lt;triplet&gt; + e; for r ∈ R(e) do o = E(e, r) object of relation r; lin_triplets += &lt;subj&gt; + o + &lt;obj&gt; + r; end end Figure</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Dataset statistics. Number of triplets with number of instances in parenthesis.</figDesc><table><row><cell></cell><cell cols="2">Entity Types Relation Types</cell><cell cols="2">Train</cell><cell cols="2">Validation</cell><cell cols="2">Test</cell></row><row><cell>CONLL04</cell><cell>4</cell><cell>5</cell><cell>1,290</cell><cell>(922)</cell><cell>343</cell><cell>(231)</cell><cell>422</cell><cell>(288)</cell></row><row><cell>NYT</cell><cell>3</cell><cell>24</cell><cell>94,222</cell><cell>(56,196)</cell><cell>8,489</cell><cell>(5,000)</cell><cell>8,616</cell><cell>(5,000)</cell></row><row><cell>DocRED</cell><cell>6</cell><cell>96</cell><cell>3,7486</cell><cell>(3,008)</cell><cell>3,678</cell><cell>(300)</cell><cell>8,787</cell><cell>(700)</cell></row><row><cell>ADE</cell><cell>2</cell><cell>1</cell><cell>6,821</cell><cell>(4,272)</cell><cell>-</cell><cell></cell><cell>-</cell><cell></cell></row><row><cell>Re-TACRED</cell><cell>17</cell><cell>40</cell><cell>58,465</cell><cell>(58,465)</cell><cell>19,584</cell><cell>(19,584)</cell><cell>13,418</cell><cell>(13,418)</cell></row><row><cell>REBEL (sent.)</cell><cell>-</cell><cell>220</cell><cell>878,555</cell><cell>(784,202)</cell><cell>48,514</cell><cell>(43,341)</cell><cell>48,852</cell><cell>(43,506)</cell></row><row><cell>REBEL (full)</cell><cell>-</cell><cell cols="7">1,146 9,282,837 (2,754,387) 513,270 (152,672) 515,186 (152,835)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparison (Micro-F1) with most recent systems. † = explicit use of train+dev ‡ = filtered overlapping entities (2.8%)</figDesc><table><row><cell>CONLL04 NYT DocRED ADE</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Average micro metrics over 5 seeds (10-folds for ADE) for REBEL pre−training . Standard deviation is indicated after the ± symbol.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Average micro metrics over 5 seeds for REBEL on test sets. Standard deviation is indicated after the ± symbol.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Results on Re-TACRED5.3 Relation ClassificationAs Table5shows, REBEL performs fairly well on RC despite being designed for RE. While Zhou and Chen (2021) presented a model with better results (91.1 F1) using entity types, we compare our models with those that do not use them. Both versions of REBEL achieve the same performance, in this case, in contrast to what we saw with RE. This may be due to the pre-training task being solely RE, as well as the size of the dataset.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Average micro metrics over 5 seeds for REBEL pre−training on dev sets. Standard deviation is indicated after the ± symbol.</figDesc><table><row><cell></cell><cell></cell><cell>Recall</cell><cell>F1</cell></row><row><cell>CONLL04</cell><cell>77.53 ±1.96</cell><cell>74.2 ±1.26</cell><cell>76.13 ±1.02</cell></row><row><cell>NYT</cell><cell>91.64 ±0.26</cell><cell>92.31 ±0.12</cell><cell>91.97 ±0.13</cell></row><row><cell>DocRED</cell><cell>46.65 ±0.94</cell><cell>49.19 ±0.43</cell><cell>47.89 ±0.68</cell></row><row><cell>Re-TACRED</cell><cell>89.59 ±0.21</cell><cell>90.81 ±0.25</cell><cell>90.19 ±0.13</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 .</head><label>7</label><figDesc>The numberMax epochs Learning Rate Warm-up Weight Decay Batch size Time per epoch</figDesc><table><row><cell>CONLL04</cell><cell>33</cell><cell>10 −5</cell><cell>10%</cell><cell>0.01</cell><cell>32</cell><cell>30 sec</cell></row><row><cell>NYT</cell><cell>42</cell><cell>2.5 • 10 −5</cell><cell>10%</cell><cell>0.1</cell><cell>24</cell><cell>8 min</cell></row><row><cell>DocRED</cell><cell>20</cell><cell>10 −5</cell><cell>10%</cell><cell>0.01</cell><cell>32</cell><cell>2 min</cell></row><row><cell>ADE</cell><cell>25</cell><cell>10 −5</cell><cell>10%</cell><cell>0.01</cell><cell>32</cell><cell>1 min</cell></row><row><cell>Re-TACRED</cell><cell>6</cell><cell>10 −5</cell><cell>10%</cell><cell>0.01</cell><cell>32</cell><cell>8.5 min</cell></row><row><cell>REBEL</cell><cell>3</cell><cell>10 −5</cell><cell>1000 steps</cell><cell>0</cell><cell>32</cell><cell>9 hours</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Hyperparameters for the different datasets.</figDesc><table><row><cell></cell><cell cols="2">Precision Recall</cell><cell>F1</cell></row><row><cell>CONLL04</cell><cell>74.69 ±0.76</cell><cell>71.66 ±1.01</cell><cell>73.14 ±0.73</cell></row><row><cell>NYT</cell><cell>91.44 ±0.12</cell><cell>92.02 ±0.15</cell><cell>91.72 ±0.10</cell></row><row><cell>DocRED</cell><cell>46.27 ±1.17</cell><cell>35.92 ±1.81</cell><cell>40.40 ±0.86</cell></row><row><cell>Re-TACRED</cell><cell>89.31 ±0.20</cell><cell>90.87 ±0.41</cell><cell>90.08 ±0.19</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>Average micro metrics over 5 seeds for REBEL on dev sets. Standard deviation is indicated after the ± symbol.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://github.com/babelscape/rebel</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">Downloaded on 2021/02/01 from: https://dumps. wikimedia.org/enwiki/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">https://pypi.org/project/wikimapper/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"> xlm-roberta-large-xnli   </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4">https://github.com/Babelscape/ crocodile</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank the authors of <ref type="bibr" target="#b12">Elsahar et al. (2018)</ref> for the T-REx open code from which cRocoDiLe was built.</p><p>This research was funded by the European Union's H2020 Marie Skłodowska-Curie project Knowledge Graphs at Scale (KnowGraphs) under H2020-EU.1.3.1. (grant agreement ID: 860801).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Global normalization of convolutional neural networks for joint entity and relation classification</title>
		<author>
			<persName><forename type="first">Heike</forename><surname>Adel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1181</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1723" to="1729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">TACRED revisited: A thorough evaluation of the TACRED relation extraction task</title>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Alt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandra</forename><surname>Gabryszak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonhard</forename><surname>Hennig</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.142</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1558" to="1569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Giusepppe</forename><surname>Attardi</surname></persName>
		</author>
		<ptr target="https://github.com/attardi/wikiextractor" />
		<title level="m">Wikiextractor</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">One spring to rule them both: Symmetric amr semantic parsing and generation without a complex pipeline</title>
		<author>
			<persName><forename type="first">Michele</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rexhina</forename><surname>Blloshmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="12564" to="12573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generationary or &quot;how we went beyond word sense inventories and learned to gloss</title>
		<author>
			<persName><forename type="first">Michele</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Maru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.585</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7207" to="7221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Generating senses and roles: An end-to-end model for dependency-and spanbased semantic role labeling</title>
		<author>
			<persName><forename type="first">Rexhina</forename><surname>Blloshmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simone</forename><surname>Conia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rocco</forename><surname>Tripodi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2021/521</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21</title>
				<meeting>the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3786" to="3793" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Autoregressive entity retrieval</title>
		<author>
			<persName><forename type="first">Nicola</forename><surname>De Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-1-4615-5529-2_5</idno>
		<title level="m">Multitask Learning</title>
				<meeting><address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer US</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="95" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improving efficiency and accuracy in multilingual entity extraction</title>
		<author>
			<persName><forename type="first">Joachim</forename><surname>Daiber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Jakob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hokamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><forename type="middle">N</forename><surname>Mendes</surname></persName>
		</author>
		<idno type="DOI">10.1145/2506182.2506198</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Conference on Semantic Systems, I-SEMANTICS &apos;13</title>
				<meeting>the 9th International Conference on Semantic Systems, I-SEMANTICS &apos;13<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="121" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Span-based joint entity and relation extraction with transformer pre-training</title>
		<author>
			<persName><forename type="first">Markus</forename><surname>Eberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Ulges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECAI</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2006" to="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An end-to-end model for entity-level relation extraction using multiinstance learning</title>
		<author>
			<persName><forename type="first">Markus</forename><surname>Eberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Ulges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</title>
				<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3650" to="3660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">T-REx: A large scale alignment of natural language with knowledge base triples</title>
		<author>
			<persName><forename type="first">Hady</forename><surname>Elsahar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavlos</forename><surname>Vougiouklis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arslen</forename><surname>Remaci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christophe</forename><surname>Gravier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Hare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederique</forename><surname>Laforest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Simperl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)</title>
				<meeting>the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)<address><addrLine>Miyazaki, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Table filling multi-task recurrent neural network for joint entity and relation extraction</title>
		<author>
			<persName><forename type="first">Pankaj</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernt</forename><surname>Andrassy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
				<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2537" to="2547" />
		</imprint>
	</monogr>
	<note>The COLING 2016 Organizing Committee</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Development of a benchmark corpus to support the automatic extraction of drugrelated adverse effects from medical case reports</title>
		<author>
			<persName><forename type="first">Harsha</forename><surname>Gurulingappa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdul</forename><surname>Mateen Rajput</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angus</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juliane</forename><surname>Fluck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Hofmann-Apitius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Toldo</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jbi.2012.04.008</idno>
	</analytic>
	<monogr>
		<title level="m">Text Mining and Natural Language Processing in Pharmacogenomics</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="885" to="892" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Going out on a limb: Joint extraction of entity mentions and relations without dependency trees</title>
		<author>
			<persName><forename type="first">Arzoo</forename><surname>Katiyar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1085</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="917" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">ALBERT: A lite BERT for self-supervised learning of language representations</title>
		<author>
			<persName><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
				<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26">2020. April 26-30, 2020</date>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal ; Abdelrahman Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.703</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">Marjan Ghazvininejad,. 2020</date>
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Roberta: A robustly optimized bert pretraining approach</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">End-to-end relation extraction using LSTMs on sequences and tree structures</title>
		<author>
			<persName><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1105</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1105" to="1116" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Modeling joint entity and relation extraction with table representation</title>
		<author>
			<persName><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutaka</forename><surname>Sasaki</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1200</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1858" to="1869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Effective modeling of encoder-decoder architecture for joint entity and relation extraction</title>
		<author>
			<persName><forename type="first">Tapas</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v34i05.6374</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8528" to="8535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cicero Nogueira dos Santos, Bing Xiang, and Stefano Soatto. 2021. Structured prediction as translation between augmented natural languages</title>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Paolini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Athiwaratkun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Krone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Achille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishita</forename><surname>Anubhai</surname></persName>
		</author>
		<idno>ICLR 2021</idno>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">End-to-end relation extraction using neural networks and Markov Logic Networks</title>
		<author>
			<persName><forename type="first">Sachin</forename><surname>Pawar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushpak</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Palshikar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<title level="s">the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="818" to="827" />
		</imprint>
	</monogr>
	<note>Long Papers. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-totext transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Modeling relations and their mentions without labeled text</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-15939-8_10</idno>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases</title>
				<meeting><address><addrLine>Berlin, Heidelberg; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="148" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A linear programming formulation for global inference in natural language tasks</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth Conference on Computational Natural Language Learning</title>
				<meeting>the Eighth Conference on Computational Natural Language Learning<address><addrLine>Boston, Massachusetts, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>HLT-NAACL 2004</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Re-tacred: Addressing shortcomings of the tacred dataset</title>
		<author>
			<persName><forename type="first">George</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="13843" to="13850" />
		</imprint>
	</monogr>
	<note>Emmanouil Antonios Platanios, and Barnabas Poczos</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Let&apos;s Stop Incorrect Comparisons in End-to-end Relation Extraction!</title>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Taillé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Guigue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Scoutheeten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Gallinari</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.301</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3689" to="3701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Two are better than one: Joint entity and relation extraction with tablesequence encoders</title>
		<author>
			<persName><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.133</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1706" to="1721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">TPLinker: Single-stage joint extraction of entities and relations through token pair linking</title>
		<author>
			<persName><forename type="first">Yucheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yueyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingwen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongsong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limin</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.coling-main.138</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
				<meeting>the 28th International Conference on Computational Linguistics<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1572" to="1582" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">LUKE: Deep contextualized entity representations with entityaware self-attention</title>
		<author>
			<persName><forename type="first">Ikuya</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akari</forename><surname>Asai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hideaki</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.523</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6442" to="6454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">DocRED: A large-scale document-level relation extraction dataset</title>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lixin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1074</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="764" to="777" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING the 25th International Conference on Computational Linguistics: Technical Papers</title>
				<meeting>COLING the 25th International Conference on Computational Linguistics: Technical Papers<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Dublin City University and Association for Computational Linguistics</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Copymtl: Copy mechanism for joint extraction of entities and relations with multi-task learning</title>
		<author>
			<persName><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoran</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianying</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference</title>
				<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020-02-07">2020. February 7-12, 2020</date>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="9507" to="9514" />
		</imprint>
	</monogr>
	<note>The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning the extraction order of multiple relational facts in a sentence with reinforcement learning</title>
		<author>
			<persName><forename type="first">Xiangrong</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1035</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="367" to="377" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Extracting relational facts by an end-to-end neural model with copy mechanism</title>
		<author>
			<persName><forename type="first">Xiangrong</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1047</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="506" to="514" />
		</imprint>
	</monogr>
	<note>Australia</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Minimize exposure bias of Seq2Seq models in joint entity and relation extraction</title>
		<author>
			<persName><forename type="first">Qianying</forename><surname>Ranran Haoran Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aysa</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Xuemo Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daojian</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daisuke</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sadao</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName><surname>Kurohashi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.23</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
				<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="236" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Positionaware attention and supervised data improve slot filling</title>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1004</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="35" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Joint entity and relation extraction based on a hybrid neural network</title>
		<author>
			<persName><forename type="first">Suncong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuexing</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongwei</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neucom.2016.12.075</idno>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Signal Processing for Big Multimedia Analysis</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">257</biblScope>
			<biblScope unit="page" from="59" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Attention-based bidirectional long short-term memory networks for relation classification</title>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingchen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongwei</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-2034</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="207" to="212" />
		</imprint>
	</monogr>
	<note>Short Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">An improved baseline for sentence-level relation extraction</title>
		<author>
			<persName><forename type="first">Wenxuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhao</forename><surname>Chen</surname></persName>
		</author>
		<idno>CoRR, abs/2102.01373</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
