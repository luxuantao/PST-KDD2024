<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Aspect Temporal Network Embedding: A Mixture of Hawkes Process View</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-05-18">18 May 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yutian</forename><surname>Chang</surname></persName>
							<email>ytchang@buaa.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Guannan</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yuan</forename><surname>Zuo</surname></persName>
							<email>zuoyuan@buaa.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Junjie</forename><surname>Wu</surname></persName>
							<email>wujj@buaa.edu.cn</email>
							<affiliation key="aff4">
								<orgName type="laboratory">Also with , Beijing Key Laboratory of Emergency Support Simulation Technologies for City Operations</orgName>
								<orgName type="institution">Beihang University. Conference</orgName>
								<address>
									<addrLine>&apos;17, July 2017</addrLine>
									<settlement>Washington</settlement>
									<region>DC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><surname>Multi-Aspect</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Economics and Management</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Economics and Management</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">School of Economics and Management</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">School of Economics and Management Beijing Advanced Innovation Center for Big Data and Brain Computing</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<address>
									<addrLine>Conference&apos;17, July 2017</addrLine>
									<settlement>Washington</settlement>
									<region>DC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Aspect Temporal Network Embedding: A Mixture of Hawkes Process View</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-05-18">18 May 2021</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/nnnnnnn.nnnnnnn</idno>
					<idno type="arXiv">arXiv:2105.08566v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS</term>
					<term>Information systems â†’ Data mining</term>
					<term>Network data models</term>
					<term>â€¢ Computing methodologies â†’ Dimensionality reduction and manifold learning Network embedding, Hawkes process, Multi-aspect, Temporal network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent years have witnessed the tremendous research interests in network embedding. Extant works have taken the neighborhood formation as the critical information to reveal the inherent dynamics of network structures, and suggested encoding temporal edge formation sequences to capture the historical influences of neighbors. In this paper, however, we argue that the edge formation can be attributed to a variety of driving factors including the temporal influence, which is better referred to as multiple aspects. As a matter of fact, different node aspects can drive the formation of distinctive neighbors, giving birth to the multi-aspect embedding that relates to but goes beyond a temporal scope. Along this vein, we propose a Mixture of Hawkes-based Temporal Network Embeddings (MHNE) model to capture the aspect-driven neighborhood formation of networks. In MHNE, we encode the multi-aspect embeddings into the mixture of Hawkes processes to gain the advantages in modeling the excitation effects and the latent aspects. Specifically, a graph attention mechanism is used to assign different weights to account for the excitation effects of history events, while a Gumbel-Softmax is plugged in to derive the distribution over the aspects. Extensive experiments on 8 different temporal networks have demonstrated the great performance of the multi-aspect embeddings obtained by MHNE in comparison with the state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Network embedding has attracted extensive attentions in recent years due to its efficiency in encoding network structure into low dimensional representations <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref>, which can be flexibly applied to various downstream tasks. In addition to embedding static networks, some efforts have also been made to tackle the dynamic evolving structure of temporal networks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b25">26]</ref>. Conventional temporal network embedding methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b16">17]</ref> model the network dynamics via multiple snapshots derived from different time periods, which can only represent the network structure in a certain time window. To overcome the limitations of the snapshotbased methods, recent work <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b25">26]</ref> has attempted to track the neighborhood formation sequence of each node, where the connection between a target node and the source node primarily depends on the similarity between the two nodes and the influence of the historical target nodes. In this regard, a complete temporal process of network formation can be revealed and hence the derived embeddings can reflect the evolution patterns of nodes.</p><p>However, except for the temporal view of the neighborhood formation, as a matter of fact, the connections between a source node and its target nodes may be driven by different underlying factors, which can be referred to as aspects. The aspects of nodes can have different meanings in terms of different networks. Especially, the aspects may change over time, and therefore can induce distinctive neighborhood formation of a node. For example, assume that node ğ‘¢ in Fig. <ref type="figure">1</ref> is a researcher who focused on Database (DB) at the early stages and then shifts his interest to Artificial Intelligence (AI). The left part of the Fig. <ref type="figure">1</ref> illustrates the historical coauthors of ğ‘¢ listed chronologically on the x-axis, where the y-axis denotes the probability of node ğ‘¢ collaborating with researchers from different fields (i.e., aspects). The blue/red line in the figure represents the activeness of ğ‘¢ in terms of DB/AI, as illustrated, the blue line has a clear downward trend while the red one keeps rising over time. Therefore, given two nodes ğ‘£ 1 and ğ‘£ 2 representing researchers who are currently focusing on AI and DB respectively, we can easily infer that ğ‘¢ has a larger chance of establishing co-authorship with ğ‘£ 1 than ğ‘£ 2 according to ğ‘¢'s neighborhood sequence. In contrast, from a static point of view as shown on the rightmost part of Fig. <ref type="figure">1</ref>, where the neighbors of ğ‘¢ are regard as an unordered static node set, future connections of ğ‘¢ are much less predictable. As seen from the example, the formation of temporal edges are indeed driven by different aspects of nodes, which is essential to understand the underlying mechanism of network evolution. However, prior temporal network embedding methods generally derive a single embedding for each node, and merely address different node aspects.</p><p>Given that nodes in networks may have multiple aspects, several prior work <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref> has attempted to represent each node with multiple embeddings. Compared to a single embedding vector, multiple embeddings enlarges the embedding space, with each embedding exclusively focusing on one specific aspect. However, these multi-aspect embeddings are derived from a static network, which indeed has some inherent limitations. On the one hand, the neighborhood of a node is generally modeled as an unordered node set, where neighbors that formed at distant timestamps would not be treated differently, thus the derived multi-aspect embedding may simply be an aggregated distribution over the aspects of all the neighbors. On the other, the node aspects can change over time, and there might be a focal aspect at one time but the static view fails to capture this. Thus, it cannot precisely predict the future neighbor formation with the out-dated aspect information. Apparently, the temporal network setting is more desirable for multi-aspect embeddings. As a matter of fact, temporal network embedding and the multi-aspect embedding can indeed be regarded as the two sides of a coin and should complement with each other. As far as we are concerned, we are among the first to take a temporal view to derive the multi-aspect embeddings.</p><p>To tackle the above-mentioned challenges, we extend the neighborhood formation to a notion of aspect driven edge formation by plugging an aspect distribution for each edge formed at specific time to account for the aspect-drive factors, which gives rise to a Multi-Aspect Temporal Network. In order to obtain the multi-aspect embedding from the temporal network, we propose an end-toend method named Mixture of Hawkes-based Temporal Network Embedding (MHNE) in this paper. In overall, we model the aspectdriven neighborhood formation with a mixture of Hawkes process (MHP) <ref type="bibr" target="#b23">[24]</ref>, which has great merits in capturing both the temporal excitation effects of historical events and the latent aspects within the sequence. In particular, we exploit the MHP to model the edge formation probability by feeding the multi-aspect embeddings into the intensity function. More specifically, the aspect distribution is induced via Gumbel-Softmax <ref type="bibr" target="#b8">[9]</ref> with a trainable temperature parameter that controls the extent to which we desire a one-hot alike distribution. Additionally, graph attention mechanism <ref type="bibr" target="#b20">[21]</ref> is incorporated to assign different weights to the excitation effects of history events. We have conducted extensive experiments on eight different real-world networks by utilizing the the multi-aspect embeddings for several downstream including link prediction, temporal node recommendation. The experimental results have demonstrated the superiority of MHNE over the state-of-the-art methods, and we have also rigorously validated the effectiveness the modeling components, especially the quality of aspects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARY 2.1 Aspect Driven Neighborhood Formation</head><p>Recent works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25]</ref> try to learn temporal network embedding by looking into the formation process of the network, specifically, the neighborhood formation process of nodes. However, as illustrated in the toy example, they ignore a fact that the evolution of edges are inherently driven by the underlying aspects, which provides a more comprehensive view of the neighborhood formation process. Based on the above consideration, we formally define the multi-aspect temporal network. Definition 1 (Multi-Aspect Temporal Network). Multi-aspect temporal network is a network with aspects driven temporal edges, which can be denoted as G =&lt; V, E; H, K &gt;, where V denotes the set of nodes, E denotes the set of temporal edges, H denotes the set of neighborhood formation sequences and K denotes the set of aspect distributions. Each temporal edge ğ‘’ = (ğ‘¢, ğ‘£, ğ‘¡) âˆˆ E between the source node ğ‘¢ and the target node ğ‘£ at time ğ‘¡ is driven by the aspect distribution K ğ‘¢ (ğ‘¡) and the neighborhood formation sequence</p><formula xml:id="formula_0">H ğ‘¢ (ğ‘¡) = {(â„, ğ‘¡ â„ )|(ğ‘¢, â„, ğ‘¡ â„ ) âˆˆ E, ğ‘¡ â„ &lt; ğ‘¡ } of the source node ğ‘¢.</formula><p>From the above definition, we can find the evolution of edges in the multi-aspect temporal network is still modeled with neighborhood formation sequence of the source node. The difference is the evolution of edges is also driven by the unobserved aspect distribution of the source node, with which a much clearer clue is provided for predicting future connections of the source node. Therefore, we formally define the aspect driven edge formation. Definition 2 (Aspect Driven Edge Formation). Given a temporal edge ğ‘’ = (ğ‘¢, ğ‘£, ğ‘¡) and the neighborhood formation sequence H ğ‘¢ (ğ‘¡) of its source node ğ‘¢ at time ğ‘¡, the formation probability of ğ‘’, in other words, the probability of ğ‘¢ connecting to ğ‘£ at time ğ‘¡ is ğ‘ (ğ‘£ |ğ‘¢, ğ‘¡, H ğ‘¢ (ğ‘¡)). Given the aspect distribution K ğ‘¢ (ğ‘¡) of the source node ğ‘¢ at time ğ‘¡, the formation probability of temporal edge ğ‘’ driven by the aspects can be decomposed as:</p><formula xml:id="formula_1">ğ‘ (ğ‘£ |ğ‘¢, ğ‘¡, H ğ‘¢ (ğ‘¡)) = | K ğ‘¢ (ğ‘¡ ) | âˆ‘ï¸ ğ‘˜=1 K ğ‘˜ ğ‘¢ (ğ‘¡)ğ‘ (ğ‘£ |ğ‘¢, ğ‘¡, H ğ‘¢ (ğ‘¡), ğ‘˜),<label>(1)</label></formula><p>where K ğ‘˜ ğ‘¢ (ğ‘¡) = ğ‘ (ğ‘˜ |ğ‘¢, ğ‘¡) is the probability of ğ‘˜-th aspect of the source node ğ‘¢ at time ğ‘¡, ğ‘ (ğ‘£ |ğ‘¢, ğ‘¡, H ğ‘¢ (ğ‘¡), ğ‘˜) is the formation probability of ğ‘’ driven by the neighborhood formation sequence H ğ‘¢ (ğ‘¡) and the ğ‘˜-th aspect.</p><p>With the aspect driven edge formation defined above, we can easily understand how the neighborhood formation sequence H ğ‘¢ (ğ‘¡) of the source node ğ‘¢ is driven by its aspects distribution K ğ‘¢ (ğ‘¡). That is, for each pair of history target node â„ and timestamp â„ ğ‘¡ in H ğ‘¢ (ğ‘¡), there is an edge (ğ‘¢, â„, ğ‘¡ â„ ) that is driven by K ğ‘¢ (ğ‘¡). Once all nodes' neighborhood formation sequences at all timestamps are driven by the aspects, the entire multi-aspect temporal network will be established.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Problem Definition</head><p>Given a multi-aspect temporal network G =&lt; V, E; H, K &gt;, the neighborhood formation H ğ‘¢ (ğ‘¡) of each node ğ‘¢ âˆˆ V can be induced by tracking all the timestamps when ğ‘¢ interacts with its neighbors before time ğ‘¡. However, the aspect distribution K ğ‘¢ (ğ‘¡) of each node ğ‘¢ at time ğ‘¡ is unobserved. To learn multiple aspects that driven the entire temporal network, we resort to learn the multi-aspect temporal network embedding as defined in the follow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 3 (Multi-Aspect Temporal Network Embedding).</head><p>Given the multi-aspect temporal network G =&lt; V, E; H, K &gt;, we aim to learn an embedding function ğœ™ : ğ‘¢ âˆˆ V â†’ {I ğ‘¢ âˆˆ R ğ‘š , A ğ‘¢ âˆˆ R ğ¾ * ğ‘š }, where I ğ‘¢ is the identity embedding of node ğ‘¢, and A ğ‘˜ ğ‘¢ represents the embedding of ğ‘˜-th aspect of node ğ‘¢, ğ¾ is the number of aspects, and ğ‘š is the embedding size satisfies ğ‘š â‰ª |V |.</p><p>With the learned embeddings, probabilities in Eq. 1 can be computed in a straightforward manner as described in Sect. 3.2. Therefore, the underlying aspects in the temporal network can be revealed. Note that the learning of multiple aspects and the temporal network embedding are actually complement each other. Especially, we emphasize that the temporal networks are more suitable for multi-aspect embedding learning than the static network for the detailed information in the formation process of temporal networks is essential to induce node aspects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY 3.1 Hawkes Process</head><p>Hawkes process is a typical temporal point process in modeling discrete events considering the temporal decay effect of history events, whose conditional intensity function is defined as follows,</p><formula xml:id="formula_2">ğœ†(ğ‘¡) = ğœ‡ (ğ‘¡) + âˆ« ğ‘¡ âˆ’âˆ J (ğ‘¡ âˆ’ ğ‘ )dğ‘ (ğ‘ ),<label>(2)</label></formula><p>where ğœ‡ (ğ‘¡) is the base intensity of a particular event, showing the spontaneous event arrival rate at time ğ‘¡, J (â€¢) is a kernel function that models the time decay effect of history events on the current event, which is usually in the form of an exponential function, and ğ‘ (ğ‘ ) denotes the number of events until time ğ‘ .</p><p>The conditional intensity function of Hawkes process shows that the occurrence of current event does not only depend on the event of last time step, but is also influenced by the historical events with time decay effect. Such property is desirable for modeling the neighborhood formation sequences, for the current neighbor formation can be influenced with higher intensity by the more recent events.</p><p>To model the evolution of an edge based on its source node's various neighbor nodes, multivariate Hawkes process is adopted <ref type="bibr" target="#b25">[26]</ref>, that is,</p><formula xml:id="formula_3">ğœ† ğ‘‘ (ğ‘¡) = ğœ‡ ğ‘‘ (ğ‘¡) + ğ· âˆ‘ï¸ ğ‘‘ â€² =1 âˆ« ğ‘¡ âˆ’âˆ J ğ‘‘ â€² ğ‘‘ (ğ‘¡ âˆ’ ğ‘ )dğ‘ ğ‘‘ â€² (ğ‘ ),<label>(3)</label></formula><p>where the conditional intensity function is designed for each event type as one dimension, such as the ğ‘‘ or ğ‘‘ â€² dimension. The excitation effects are indeed a sum over all the historical events with different types, captured by an excitation rate ğ›¼ ğ‘‘ â€² ,ğ‘‘ between dimension ğ‘‘ and ğ‘‘ â€² , formally,</p><formula xml:id="formula_4">J ğ‘‘ â€² ğ‘‘ (ğ‘¡ âˆ’ ğ‘ ) = ğ›¼ ğ‘‘ â€² ğ‘‘ J (ğ‘¡ âˆ’ ğ‘ ).</formula><p>When modeling multi-aspect temporal network, the neighborhood formation sequence can be further decomposed into multiple event sequences according to their underlying aspects. Therefore we resort to the Mixture of Hawkes Process (MHP), where event sequences driven by various aspects can be modeled simultaneously.</p><p>Suppose that there are ğ¾ different aspects that driven the occurrence of current event (i.e. edge), each aspect can be modeled as a component of the MHP. Extended from the Eq. 3, we have the intensity function of the current event motivated by aspect ğ‘˜ as follows,</p><formula xml:id="formula_5">ğœ† ğ‘˜ ğ‘‘ (ğ‘¡) = ğœ‡ ğ‘˜ ğ‘‘ (ğ‘¡) + ğ· âˆ‘ï¸ ğ‘‘ â€² ğ›¼ ğ‘˜ ğ‘‘ â€² ğ‘‘ âˆ« ğ‘¡ âˆ’âˆ J (ğ‘¡ âˆ’ ğ‘ )dğ‘ ğ‘˜ ğ‘‘ â€² (ğ‘ ),<label>(4)</label></formula><p>where ğœ‡ ğ‘˜ ğ‘‘ (ğ‘¡) denotes the base intensity on current event ğ‘‘ excited by the ğ‘˜-th aspect, ğ‘ ğ‘˜ ğ‘‘ (ğ‘¡) is the number of events that driven by the ğ‘˜-th aspect until time ğ‘¡, ğ›¼ ğ‘˜ ğ‘‘ â€² ğ‘‘ denotes the mutual excitation effect between dimension ğ‘‘ â€² and ğ‘‘ in terms of the ğ‘˜-th aspect. Therefore the intensity function of MHP is ğœ† ğ‘‘ (ğ‘¡) = ğœ‹ ğ‘˜ ğœ† ğ‘˜ ğ‘‘ (ğ‘¡), where ğœ‹ ğ‘˜ is the mixture weight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Mixture of Hawkes based temporal Network Embedding</head><p>As discussed above, we firstly model the aspect driven neighborhood formation sequences via Mixture of Hawkes Process (MHP), which is then adapted to learn embeddings. In this way, we can tackle the multi-aspect temporal network embedding problem. Now we formally propose our model named Mixture of Hawkes temporal Network Embedding (MHNE). Assume there are ğ¾ aspects in the multi-aspect temporal network, the intensity function of the MHP for node ğ‘£ connecting to node ğ‘¢ at time ğ‘¡ can be written as,</p><formula xml:id="formula_6">ğœ† ğ‘£ |ğ‘¢ (ğ‘¡) = âˆ‘ï¸ ğ‘˜ ğœ‹ ğ‘˜ ğ‘¢ (ğ‘¡)ğœ† ğ‘˜ ğ‘£ |ğ‘¢ (ğ‘¡),<label>(5)</label></formula><p>where ğœ‹ ğ‘˜ ğ‘¢ (ğ‘¡) denotes the probability of node ğ‘¢ driven by ğ‘˜-th aspect at time ğ‘¡ (i.e., the mixture weight of ğ‘˜-th aspect), ğœ† ğ‘˜ ğ‘£ |ğ‘¢ (ğ‘¡) denotes the Recall the Definition 2, given the aspect distribution K ğ‘¢ (ğ‘¡) of the source node ğ‘¢, the the probability ğ‘ (ğ‘£ |ğ‘¢, ğ‘¡, H ğ‘¢ (ğ‘¡)) that ğ‘¢ connects to ğ‘£ at time ğ‘¡ can be written as Eq. <ref type="bibr" target="#b0">(1)</ref>. By comparing Eq. ( <ref type="formula" target="#formula_1">1</ref>) and Eq. ( <ref type="formula" target="#formula_6">5</ref>), it is not hard to find that ğœ‹ ğ‘˜ ğ‘¢ (ğ‘¡) and ğœ† ğ‘˜ ğ‘£ |ğ‘¢ (ğ‘¡) are analog to the K ğ‘¢ (ğ‘¡) and ğ‘ (ğ‘£ |ğ‘¢, ğ‘¡, H ğ‘¢ (ğ‘¡), ğ‘˜) in Eq. ( <ref type="formula" target="#formula_1">1</ref>) respectively, which indicates the MHP intensity function defined above is modeling the aspect driven edge formation. Therefore, the MHP has potential to be adapted to tackle the multi-aspect temporal network embedding problem. In the following, we show how to adapt the MHP with multi-aspect embeddings, by firstly describing how to adapt the ğœ† ğ‘˜ ğ‘£ |ğ‘¢ (ğ‘¡), then presenting how to obtain the aspect distribution ğœ‹ ğ‘˜ ğ‘¢ (ğ‘¡). Adapted Intensity Function. The intensity ğœ† ğ‘˜ ğ‘£ |ğ‘¢ (ğ‘¡) can be formulated according to Eq. ( <ref type="formula" target="#formula_5">4</ref>), that is,</p><formula xml:id="formula_7">Î»ğ‘˜ ğ‘£ |ğ‘¢ (ğ‘¡) = ğœ‡ ğ‘˜ ğ‘¢,ğ‘£ + âˆ‘ï¸ â„ âˆˆH ğ‘¢ (ğ‘¡ ) ğœ‹ ğ‘˜ â„ (ğ‘¡)ğ›¼ ğ‘˜ â„,ğ‘£ J (ğ‘¡ âˆ’ ğ‘¡ â„ ),<label>(6)</label></formula><p>where </p><formula xml:id="formula_8">ğœ‹ ğ‘˜ â„ (ğ‘¡)ğ›¼ â„,ğ‘£ ğ›¾ ğ‘˜ â„,ğ‘£ J (ğ‘¡ âˆ’ ğ‘¡ â„ ),<label>(7)</label></formula><p>where J (ğ‘¡ âˆ’ ğ‘¡ â„ ) = exp(âˆ’ğ›¿ ğ‘¢ (ğ‘¡ âˆ’ ğ‘¡ â„ )) is the kernel function models time decay effect. Since the influence extent of history can be various between different nodes, we introduct a trainable parameter ğ›¿ ğ‘¢ for each node.</p><p>Intuitively, the probability that two nodes connect with each other are proportion to their similarity. Thus we define a function F : R ğ‘‘ â†’ R that maps the embeddings of two corresponding nodes into a similarity score, with which, we are able to adapt the intensity ğœ† ğ‘˜ ğ‘£ |ğ‘¢ with embeddings. Specifically, we show ğœ‡ ğ‘¢,ğ‘£ , ğ›¼ â„,ğ‘£ can be computed via F as</p><formula xml:id="formula_9">ğœ‡ ğ‘¢,ğ‘£ = F (I ğ‘¢ , I ğ‘£ ),<label>(8)</label></formula><formula xml:id="formula_10">ğ›¼ â„,ğ‘£ = F (I â„ , I ğ‘£ ),<label>(9)</label></formula><p>where I ğ‘¢ , I ğ‘£ and I â„ are identity embeddings of node ğ‘¢, node ğ‘£ and node â„, respectively. Although there are many choices, we empirically find that negative Euclidean distance works the best. Therefore, F (ğ‘¥, ğ‘¦) = âˆ’ âˆ¥ğ‘¥ âˆ’ ğ‘¦ âˆ¥ 2 , where ğ‘¥ and ğ‘¦ are node embeddings.</p><p>Similarly, ğ›¾ ğ‘˜ ğ‘¢,ğ‘£ and ğ›¾ ğ‘˜ â„,ğ‘£ can be computed via F as</p><formula xml:id="formula_11">ğ›¾ ğ‘˜ ğ‘¢,ğ‘£ = âˆ’F (A ğ‘˜ ğ‘¢ , A ğ‘˜ ğ‘£ ),<label>(10)</label></formula><formula xml:id="formula_12">ğ›¾ ğ‘˜ â„,ğ‘£ = âˆ’F (A ğ‘˜ â„ , A ğ‘˜ ğ‘£ ),<label>(11)</label></formula><p>where A ğ‘˜ ğ‘¢ , A ğ‘˜ ğ‘£ and A ğ‘˜ â„ are aspect embeddings. Notably, in order to maintain the monotonicity of ğœ‡ ğ‘˜ ğ‘¢,ğ‘£ and ğ›¼ ğ‘˜ â„,ğ‘£ , a negative sign is added before F .</p><p>Finally, the intensity function ğœ† ğ‘˜ ğ‘£ |ğ‘¢ (ğ‘¡) can be parameterized with the embedding matrices {I, A}. Since intensity function should take positive value when regarded as a rate per unit time, we apply exp(â€¢) to convert Î»ğ‘˜ ğ‘£ |ğ‘¢ (ğ‘¡) into positive value, that is,</p><formula xml:id="formula_13">ğœ† ğ‘˜ ğ‘£ |ğ‘¢ (ğ‘¡) = exp( Î»ğ‘˜ ğ‘£ |ğ‘¢ (ğ‘¡)).<label>(12)</label></formula><p>Graph Attention Mechanism. With the advent of Graph Convolution Networks, the idea of update node embeddings with its neighbors' information is widely adopted and gained success in many fields <ref type="bibr" target="#b10">[11]</ref>. Velivckovic et al. <ref type="bibr" target="#b20">[21]</ref> proposed a graph attention mechanism of calculating different weights for this information aggregation process, which to some extent denotes the closeness of neighbors and the source. Intuitively, the closer the history node â„ is to source node ğ‘¢, the more convincing that excitation effect should be. For example, if â„ 1 in Fig. <ref type="figure">1</ref> is closer to ğ‘¢ than â„ 2 , then the excitation effect of â„ 1 should be emphasized with a larger weight.</p><p>Driven by this idea, we adopt the graph attention mechanism to describe the relationship between history nodes and the source node. Follow <ref type="bibr" target="#b20">[21]</ref> we have,</p><formula xml:id="formula_14">attn ğ‘¢,â„ = exp(LeakyReLU( Ã¬ ğ‘ ğ‘‡ (WI ğ‘¢ |WI â„ ))) â„ â€² exp(LeakyReLU( Ã¬ ğ‘ ğ‘‡ (WI ğ‘¢ |WI â„ â€² ))) ,<label>(13)</label></formula><p>where W and Ã¬ ğ‘ are trainable parameters. With attn ğ‘¢,â„ , the ğ›¼ â„,ğ‘£ can be computed more precisely, that is,</p><formula xml:id="formula_15">ğ›¼ â„,ğ‘£ = attn ğ‘¢,â„ Ã— F (I â„ , I ğ‘£ ).<label>(14)</label></formula><p>Aspect Distribution Calculation. Here we discuss the problem of calculating the aspect distribution for each node. Commonly, one takes the behavior of one's company, which means if we take the neighborhood sequence of a source node as the context, then we can infer the activeness of aspects for both the source node and the history nodes based on the context. For example, ğ‘¢ might be a researcher focusing on Database and thus its neighbors (i.e. co-authors) in a certain time window might mostly come from this field. Therefore, by observing the neighborhood sequence of ğ‘¢ in a certain time window, we can infer that ğ‘¢ and nodes in its neighborhood sequence are focusing on Database at that time. We formally define the context embedding ğ¶ ğ‘˜ ğ‘¢ (ğ‘¡) for node ğ‘¢ with neighborhood formation sequence H ğ‘¢ (ğ‘¡) as follows,</p><formula xml:id="formula_16">ğ¶ ğ‘˜ ğ‘¢ (ğ‘¡) = 1 2 â„ âˆˆH ğ‘¢ (ğ‘¡ ) J (ğ‘¡ âˆ’ ğ‘¡ â„ )A ğ‘˜ â„ |H ğ‘¢ (ğ‘¡)| + A ğ‘˜ ğ‘¢ .<label>(15)</label></formula><p>Note that the context embeddings of history nodes can be obtained accordingly.</p><p>After ğ¾ context embeddings (one for an aspect) are obtained, we apply Gumbel-Softmax trick <ref type="bibr" target="#b8">[9]</ref> to assign a probability distribution over these aspects. Notably, since we use negative Euclidean distance as similarity metric, the log operation in conventional Gumbel-Softmax trick is canceled. Taking history node â„ âˆˆ H ğ‘¢ (ğ‘¡) âˆª {ğ‘¢} as an example, we have</p><formula xml:id="formula_17">ğœ‹ ğ‘˜ â„ (ğ‘¡) = exp(F (I â„ , ğ¶ ğ‘˜ ğ‘¢ (ğ‘¡)) + ğ‘” ğ‘˜ )/ğœ â„ ) ğ‘˜ â€² exp(F (I â„ , ğ¶ ğ‘˜ â€² ğ‘¢ (ğ‘¡)) + ğ‘” ğ‘˜ â€² )/ğœ â„ ) ,<label>(16)</label></formula><p>where ğ‘” ğ‘˜ is the gumbel noise sampled from ğºğ‘¢ğ‘šğ‘ğ‘’ğ‘™ (0, 1) distribution as,</p><formula xml:id="formula_18">ğ‘” ğ‘˜ = âˆ’ log(âˆ’ log(ğ‘¢ ğ‘˜ )), ğ‘¢ ğ‘˜ âˆ¼ Uniform(0, 1). (<label>17</label></formula><formula xml:id="formula_19">)</formula><p>Note that the ğœ‹ ğ‘˜ ğ‘¢ (ğ‘¡) can be obtained accordingly. Although Gumbel-Softmax trick has been used as a workaround to approximate differentiable hard selection over categorical distribution <ref type="bibr" target="#b17">[18]</ref>, in our work, it is applied for distribution calculation for mainly two reasons. Firstly, although the aspects of a node is highly correlated to its context, but is not determined by it. Therefore, the uncertainty introduced by the gumbel noise is beneficial for us to obtain an aspect distribution rather than determined aspects. Secondly, the temperature parameter ğœ â„ is a node dependent parameter which controls the extend of approximation between the output of Gumbel-Softmax and a one-hot vector. Since some nodes are mainly focused on an aspect, while some others are not, the node dependent temperature parameter is essential to model the different shapes of the aspect distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Model Optimization</head><p>With ğœ‹ ğ‘˜ ğ‘¢ and ğœ† ğ‘˜ ğ‘£ |ğ‘¢ (ğ‘¡), we can compute the ğœ† ğ‘£ |ğ‘¢ (ğ‘¡) with Eq. 5. Then, the probability of target node ğ‘£ connects to source node ğ‘¢ at time ğ‘¡ can be computed as,</p><formula xml:id="formula_20">ğ‘ (ğ‘£ |ğ‘¢, ğ‘¡, H ğ‘¢ (ğ‘¡)) = ğœ† ğ‘£ |ğ‘¢ (ğ‘¡) ğ‘£ â€² ğœ† ğ‘£ â€² |ğ‘¢ (ğ‘¡) . (<label>18</label></formula><formula xml:id="formula_21">)</formula><p>Then the log likelihood can be written as follows,</p><formula xml:id="formula_22">log L = âˆ‘ï¸ ğ‘¢ âˆˆV âˆ‘ï¸ ğ‘£ âˆˆH ğ‘¢ log ğ‘ (ğ‘£ |ğ‘¢, ğ‘¡, H ğ‘¢ (ğ‘¡)).<label>(19)</label></formula><p>As the exponential function we introduced in Eq. ( <ref type="formula" target="#formula_13">12</ref>), Eq. ( <ref type="formula" target="#formula_20">18</ref>) is indeed a Softmax unit applied to Î»ğ‘£ |ğ‘¢ , which can be optimized approximately via negative sampling <ref type="bibr" target="#b15">[16]</ref>. Follow <ref type="bibr" target="#b25">[26]</ref>, we sample negative node ğ‘£ ğ‘– that shares no links with ğ‘£ according to the degree distribution ğ‘ ğ‘› (ğ‘£) âˆ¼ ğ‘‘ 3/4 ğ‘£ . ğ‘‘ ğ‘£ is the degree for node ğ‘£. Then we give the objective function of MHNE as,</p><formula xml:id="formula_23">log ğœ ( Î»ğ‘£ |ğ‘¢ ) + ğ‘ âˆ‘ï¸ ğ‘–=1 E ğ‘£ ğ‘– âˆ¼ğ‘ ğ‘› (ğ‘£) [âˆ’ log ğœ ( Î»ğ‘£ ğ‘– |ğ‘¢ (ğ‘¡))],<label>(20)</label></formula><p>where ğ‘ is the number of negative nodes and ğœ (ğ‘¥) = exp(ğ‘¥)/(1 + exp(ğ‘¥)) is the sigmoid function.</p><p>We adopt batch gradient descent to optimize the above objective function. In each iteration, we sample a mini-batch of edges with timestamps and fixed length of recently formed neighbors of the source node to update the parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL SETUP</head><p>To demonstrate the effectiveness of the proposed MHNE, we conduct extensive experiments to answer the following questions.</p><p>RQ1: Can MHNE accurately preserve network structures in embedding space and obtain embeddings with better quality? RQ2: Can MHNE efficiently capture the temporal pattern of dynamic networks thus precisely predict future events? RQ3: Is the learned aspect embeddings focusing on certain aspects? Can MHNE provide clues for aspect driven edge formation via MHP? RQ4: Can MHNE benefit from the incorporated Gumbel-Softmax trick and graph attention mechanism? RQ5: How do important parameters, including the history length ğ» and number of aspects ğ¾, influence model performance?</p><p>To answer these questions, we choose eight real-world networks and use five baseline methods for comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We examine the effectiveness of our proposed method MHNE on eight publicly available real-world networks with diverse sizes, including a coauthor network (DBLP), two friendship networks (Wosn, Digg), three trust networks (Epinions, BtcAlpha, BtcOtc), a citation network (HepTh) and a network of autonomous systems(Tech). The statistics of these networks are reported in Table <ref type="table" target="#tab_2">1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines</head><p>We compare MHNE with five state-of-the-art (SOTA) network embedding methods, including static network embedding such as LINE, DeepWalk and asp2vec, as well as temporal network embedding methods such as HTNE and M 2 DNE. Notably, asp2vec is a SOTA multi-aspect embedding method. LINE <ref type="bibr" target="#b19">[20]</ref>: LINE learns embedding by preserving the first-order and second-order proximities for nodes in the network.</p><p>DeepWalk <ref type="bibr" target="#b18">[19]</ref>: DeepWalk first generates random walks from a network, which are treated as sentences and subsequently fed into the Skip-gram <ref type="bibr" target="#b15">[16]</ref> model for embedding learning.</p><p>Asp2vec <ref type="bibr" target="#b17">[18]</ref>: This method extends DeepWalk by learning multiple embeddings for a single node in a differentiable way.</p><p>HTNE <ref type="bibr" target="#b25">[26]</ref>: HTNE models the neighborhood formation sequence of temporal networks to learn node embeddings.</p><p>M 2 DNE [13]: M 2 DNE extends HTNE by taking both micro and macro dynamics into account.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Parameter settings</head><p>In terms of LINE, we employ both first-order and second-order proximities to learn node embeddings, for we empirically observe better results than using just first-order or second-order proximity. The number of walks per node, walk length and window size are set to 10, 80, 3 for DeepWalk and asp2vec. Specifically, the number of aspects is set to 4 for both asp2vec and our method. The history length is set to 5 for our method, HTNE and M 2 DNE.</p><p>We run each model with varying dimensions of learned embeddings. Specifically, we set the embedding dimension ğ‘‘ğ‘–ğ‘š as 100, 200 and 500 respectively. Notably, the final embedding is obtained by concatenating identity embedding along with all aspect embeddings for multi-aspect embedding methods such as asp2vec and our method. For a fair comparison, the size of identity/aspect embedding is set to ğ‘‘ğ‘–ğ‘š/(ğ¾ + 1), where ğ¾ is the number of aspects. Moreover, as we obtain two embeddings for LINE by employing first and second order proximity, their dimensions are both set to ğ‘‘ğ‘–ğ‘š/2 to ensure the concatenated vector has the same size as others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Embedding Quality (RQ1)</head><p>Link prediction is suggested to be the primary choice of evaluating the unsupervised network embedding methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5]</ref>. Other downstream tasks such as node classification might involve low quality labels that cannot represent the real status of nodes, which can lead to unreliable results. Moreover, as we assume that each node has multiple aspects, it is difficult to categorize each node with a single label. Therefore in order to validate the effectiveness of MHNE in accurately preserving the network structure in the embedding space, we compared the link prediction results based on the embeddings learned from the embedding methods.</p><p>Specifically, we aim to determine whether there is an edge between two nodes based on its representation, which is constructed by calculating the element-wise absolute difference between its two corresponding nodes' embeddings. In our experiment, we first randomly mask 20, 000 static edges from the networks, except for BtcAlpha and BtcOtc, where we randomly mask 5, 000 static edges. Those masked edges are taken as positive instances. Meanwhile, we randomly sample equal number of false edges that do not exist in the network as negative instances. Then, a Logistic Regression classifier is trained on 50% of the mixed positive and negative instances, and we report the macro-f1 and AUC-ROC based on the remaining instances in Table <ref type="table" target="#tab_3">2</ref>.</p><p>From the results in Table <ref type="table" target="#tab_3">2</ref>, we can easily find that MHNE outperforms the baseline methods in nearly all the cases in terms of both metrics. This promising results suggest that we can better recover the formation process of networks by modeling the multi-aspects of nodes in a dynamic setting, and can obtain node embeddings with better quality. We notice that although MHNE outperforms all the baseline methods when the embedding size is set to be 200, there still exist some exceptions. LINE and DeepWalk performs the best on Tech and Hepth respectively when ğ‘‘ğ‘–ğ‘š = 100, and M 2 DNE performs the best on Epinions when ğ‘‘ğ‘–ğ‘š = 500. However, MHNE is outperformed by no more than 0.006 in terms of both f1-macro and AUC-ROC among all these exceptions, showing its robustness.</p><p>It is argued that multi-aspect embedding methods are more preferable in modeling social networks <ref type="bibr" target="#b17">[18]</ref>, since nodes in such networks inherently show multiple aspects. We find that MHNE consistently outperforms all the baseline methods on the two social networks, which is not only in line with the previous findings but also confirms that the temporal information can better reveal the multiple aspects of nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Temporal Node Recommendation (RQ2)</head><p>To further evaluate whether our method can better capture the evolution patterns of temporal networks, we conduct the temporal node recommendation experiment on DBLP, i.e., to recommend possible coauthors for a given researcher at a specific time. Specifically, we learn the node embeddings on the network extracted between the time interval [ğ‘¡ â€² , ğ‘¡), and use the coauthor relationships established at and after the time ğ‘¡ as test set. After the embeddings are learned, we calculate the intensity of building connections with other nodes as the ranking scores for recommendation. Follow the settings in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b25">26]</ref>, we apply inner product as ranking score for DeepWalk and LINE, and apply negative Euclidean distance for asp2vec, HTNE, M 2 DNE and our method. Afterwards, we derive the top-ğ‘˜ candidate nodes at time ğ‘¡, and report the results in terms of Precision@k and Recall@k in Fig. <ref type="figure" target="#fig_1">3</ref>.  We can find from Fig. <ref type="figure" target="#fig_1">3</ref> that our method consistently outperforms the other methods in terms of both metrics. Interestingly, we find the three methods (MHNE, M 2 DNE, HTNE) which model the formation process outperform the other static embedding methods. This demonstrates that by modeling the dynamic neighborhood formation process, the future events can be predicted more accurately. Moreover, though asp2vec cannot compete with the temporal network embedding methods, it achieve the best performances among all the static embedding methods, which again provides evidence for the necessity in considering the multiple aspects. Therefore, it also emphasizes that multi-aspect embedding and temporal network embedding are two tasks complement each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Aspect Embedding Analysis (RQ3)</head><p>In this experiment, we aim to explore whether our method can capture different aspects of nodes from the two perspectives.  What are the quality of the learned aspect embeddings? 2) Can MHNE capture the diverse aspect driven neighborhood formations?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Aspect Embedding Quality.</head><p>To evaluate the quality of the aspect embeddings, we perform the link prediction using only embeddings from certain aspect or identity embeddings and report the macro-f1 in Fig. <ref type="figure" target="#fig_2">4</ref>. For comparison, we conduct this experiment on HTNE by splitting its node embeddings into multiple parts that have equal size with the aspect embeddings. Fig. <ref type="figure" target="#fig_2">4a</ref> shows that aspect embeddings are significantly outperformed by the concatenated vectors consisting of both aspect embeddings and identity embeddings. Conversely, the performance gap between the embeddings learned from HTNE and their splitted parts is much smaller as illustrated in Figure <ref type="figure">.</ref> 4b. Such contrastive results may be due to the fact that the aspect embeddings obtained by MHNE are one-sided and cannot adequately describe the nature  of nodes from a macro perspective, since we force each aspect embedding to focus on a certain aspect. Therefore, the performance improvement of concatenating the aspect and identity embeddings is dramatically larger, which demonstrates that we can obtain a more comprehensive description of nodes from its various aspects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Analysis of Aspect-driven Temporal Events.</head><p>In order to gain insights on how the learned multiple different aspects affect the occurrence of temporal events, we visualize the intensity excited by aspect ğ‘˜, i.e., ğœ† ğ‘˜ ğ‘£ |ğ‘¢ of the neighborhood sequence. For illustrative purpose, we choose two scholars, Bolin Ding and Christopher RÃ© from DBLP who were previously focusing on the Database field and then shift the research interests to Data Mining and Artificial Intelligence respectively. This means that Bolin Ding and Christopher RÃ© might collaborate with other researchers focusing on Database in the earlier years, while their academic partnership may change accordingly over time.</p><p>As illustrated in Fig. <ref type="figure" target="#fig_4">5</ref>, the red lines in both subfigures representing the intensity from aspect #1 has a clear downward trend, while the blue line in Fig. <ref type="figure" target="#fig_4">5a</ref> and purple in Fig. <ref type="figure" target="#fig_4">5b</ref> keep rising over time. This indicates that the temporal events of Bolin Ding and Christopher RÃ© are mostly driven by aspect #1 at the early stages. After that, the occurrence of temporal events are primarily inspired by aspect #0 and aspect #3 for them. Compared with previous analysis, we infer that aspect #1 mainly represents Database for these two authors, and aspect #0 and aspect #3 might represent Data Mining and Artificial Intelligence respectively. These two cases demonstrate that MHNE can capture different driving aspects for the temporal events and can be reflected from the dynamic node status.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Ablation Study (RQ4)</head><p>To validate the effectiveness of incorporating graph attention mechanism and Gumbel-Softmax into our model, we construct several submodels that deliberately removes one component or both and test the performances of these model variants. Specifically, we conduct link prediction with these constructed models and report results in terms of the macro-f1 in Fig. <ref type="figure" target="#fig_6">6</ref>.  From the results we can find MHNE outperforms all the submodels in all the datasets, which demonstrate the rationality of incorporating both graph attention mechanism and Gumbel-Softmax. Among the submodels, MHNE-w/o-attn performs the best on HepTh and MHNE-w/o-Gumbel performs the best on Digg, BtcAlpha and BtcOtc. In the other datasets, MHNE-w/o-attn and MHNE-w/o-Gumbel performs on par with each other. These results verify that MHNE benefits from graph attention mechanism by better capturing the relationship between the source node and history nodes. Moreover, Gumbel-Softmax trick enables MHNE to assign different forms of aspect distributions for different nodes, which also improves model performance. In addition, MHNE-w/o-attn-Gumbel performs the worst in most cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Parameter Sensitivity (RQ5)</head><p>For point based embedding models <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b25">26]</ref> and multi-aspect embedding methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18]</ref>, the most important parameters are the history length ğ» and the number of aspects ğ¾ respectively. In this section, we study the sensitivity of our model by conducting link prediction experiments under different setting of the two parameters, and we report the macro-f1 in Table <ref type="table">3</ref> and Table <ref type="table">4</ref>. Notably, we fix the dimension of concatenated vectors to 200, thus the size of each aspect is set to 200/(ğ¾ + 1).</p><p>As shown in Table <ref type="table">3</ref>, the optimal history length varies in different datasets. While the performance has never be the best when ğ» = 1, we argue that MHNE benefits for a comparatively longer history length which incorporates more information from the neighborhood sequence. As illustrated in Table <ref type="table">4</ref>, we can see that the macro-f1 of MHNE increases along with ğ¾ and remains stable when ğ¾ &gt;= 2. Specifically, we preserve only one aspect embedding for each node when ğ¾ = 1, which means that the multi-aspect nature is ignored. Interestingly, MHNE performs the best when ğ¾ is set to be the largest on the two social networks, which is in accordance with previous discussion that the nodes in social networks inherently exhibits multiple aspects than other networks. This result again supports that our model can benefit from learning multiple aspect embeddings while is robust on different number of aspects across all the datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RELATED WORK</head><p>Network embedding methods represent each node in the network via low-dimensional vectors while preserving network structure, have gained wide attention in recent years <ref type="bibr" target="#b1">[2]</ref>, and our work is related to the following categories of studies.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>2 ğ‘¡ğ‘¡Figure 1 :</head><label>21</label><figDesc>Figure 1: An example of considering the history sequence and multiple aspects of nodes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Temporal Node Recommendation Results</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Link Prediction with Aspect embeddings</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Aspect-driven Temporal Events</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Ablation Study</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Occurrence Intensity ğ‘¢ğ‘¢ Excitation effect of history nodes Rate of ğ’–ğ’– connecting with ğ’—ğ’— Mixture component of MHP Mixture weight</head><label></label><figDesc>Figure 2: Framework of MHNE intensity of node ğ‘£ connects with ğ‘¢ at time ğ‘¡ driven by ğ‘˜-th aspect, which is a intensity function of multivariate Hawkes process.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>ğœ‡ğœ‡ ğ‘¢ğ‘¢,ğ‘£ğ‘£</cell><cell cols="2">ğ›¼ğ›¼ â„,ğ‘£ğ‘£</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>:</cell><cell>â„ 1</cell><cell>â„ 2</cell><cell>â„ 3 â„±</cell><cell>â„ 4</cell><cell>â„±</cell><cell>â„ 5</cell><cell>ğ‘£ğ‘£</cell><cell>ğˆğˆ</cell><cell>Graph Attention</cell><cell>ğ»ğ» ğ›¼ğ›¼ â„ ğ‘–ğ‘– ,ğ‘£ğ‘£</cell><cell>ğœ‹ğœ‹ â„ğ‘–ğ‘– 1 (ğ‘¡ğ‘¡) ğ“˜ğ“˜(â€¢) ğ›¼ğ›¼ â„ ğ‘–ğ‘– ,ğ‘£ğ‘£ 1 Ã—</cell><cell>Ã— ğ›¾ğ›¾ â„ ğ‘–ğ‘– ,ğ‘£ğ‘£ 1</cell><cell>+ ğœ‡ğœ‡ ğ‘¢ğ‘¢,ğ‘£ğ‘£</cell><cell>ğœ‹ğœ‹ ğ‘¢ğ‘¢ 1 (ğ‘¡ğ‘¡) ğœ†ğœ† ğ‘£ğ‘£|ğ‘¢ğ‘¢ 1 (t) ğœ‡ğœ‡ ğ‘¢ğ‘¢,ğ‘£ğ‘£ 1 Ã—</cell><cell>ğ›¾ğ›¾ ğ‘¢ğ‘¢,ğ‘£ğ‘£ 1</cell><cell>Ã—</cell><cell></cell><cell>â„ 2</cell><cell>â„ 4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ğ€ğ€</cell><cell>ğ…ğ…ğ’ğ’(t)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>+</cell><cell>ğœ†ğœ† ğ‘£ğ‘£|ğ‘¢ğ‘¢ (ğ‘¡ğ‘¡)</cell><cell>ğ‘£ğ‘£</cell><cell>ğ‘¢ğ‘¢</cell><cell>â„ 1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ğ…ğ…ğ’ğ’ ğŸğŸ (ğ’•ğ’•)ğ…ğ…ğ’ğ’ ğŸğŸ (ğ’•ğ’•)ğ…ğ…ğ’ğ’ ğŸ‘ğŸ‘ (ğ’•ğ’•)</cell><cell></cell><cell></cell><cell>â€¦</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Gumbel-Softmax</cell><cell>ğ›¼ğ›¼ â„ ğ‘–ğ‘– ,ğ‘£ğ‘£</cell><cell>Ã—</cell><cell>ğ›¾ğ›¾ â„ ğ‘–ğ‘– ,ğ‘£ğ‘£ 3</cell><cell>ğœ‡ğœ‡ ğ‘¢ğ‘¢,ğ‘£ğ‘£</cell><cell>Ã—</cell><cell>ğ›¾ğ›¾ ğ‘¢ğ‘¢,ğ‘£ğ‘£ 3</cell><cell></cell><cell></cell><cell>â„ 3</cell><cell>â„ 5</cell></row><row><cell>-â„±</cell><cell></cell><cell>-â„±</cell><cell>ğ‚ğ‚ğ‘¢ğ‘¢ ğ‘¡ğ‘¡</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ğ›¼ğ›¼ â„ ğ‘–ğ‘– ,ğ‘£ğ‘£ 3</cell><cell></cell><cell></cell><cell>ğœ‡ğœ‡ ğ‘¢ğ‘¢,ğ‘£ğ‘£ 3</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ğ›¾ğ›¾ ğ‘¢ğ‘¢,ğ‘£ğ‘£ ğ‘˜ğ‘˜</cell><cell></cell><cell>ğ›¾ğ›¾ ğ‘¢ğ‘¢,ğ‘£ğ‘£ ğ‘˜ğ‘˜</cell><cell></cell><cell cols="2">-â„± ğ›¾ğ›¾ â„ ğ‘–ğ‘– ,ğ‘£ğ‘£ ğ‘˜ğ‘˜</cell><cell>-â„± ğ›¾ğ›¾ â„ ğ‘–ğ‘– ,ğ‘£ğ‘£ ğ‘˜ğ‘˜</cell><cell></cell><cell></cell><cell></cell><cell>ğ»ğ»</cell><cell>ğœ‹ğœ‹ â„ğ‘–ğ‘– 3 (ğ‘¡ğ‘¡) ğ“˜ğ“˜(â€¢)</cell><cell>Ã—</cell><cell>+</cell><cell>ğœ†ğœ† ğ‘£ğ‘£|ğ‘¢ğ‘¢ 3 (t) ğœ‹ğœ‹ ğ‘¢ğ‘¢ 3 (ğ‘¡ğ‘¡)</cell><cell></cell><cell>Ã—</cell><cell></cell><cell></cell></row><row><cell cols="8">Aspect distribution and parameter calculation</cell><cell></cell><cell></cell><cell></cell><cell cols="4">Mixture of Hawkes process</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Occurrence Intensity</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>ğœ‡ ğ‘˜ ğ‘¢,ğ‘£ denotes the rate of node ğ‘¢ connects to ğ‘£ motivated by ğ‘˜th aspect, â„ denotes the historical nodes connected to ğ‘¢ before time ğ‘¡, ğ›¼ ğ‘˜ â„,ğ‘£ denotes the excitation effect of history node â„ on target node ğ‘£ and ğœ‹ ğ‘˜ â„ (ğ‘¡) denotes the activeness of node â„ on ğ‘˜-th aspect. The ğœ‡ ğ‘˜ ğ‘¢,ğ‘£ can be further decomposed into the base rate ğœ‡ ğ‘¢,ğ‘£ that node ğ‘¢ and node ğ‘£ connecting with each other, and the excitation effect ğ›¾ ğ‘˜ ğ‘¢,ğ‘£ from ğ‘˜-th aspect. Then, we have ğœ‡ ğ‘˜ ğ‘¢,ğ‘£ = ğœ‡ ğ‘¢,ğ‘£ ğ›¾ ğ‘˜ ğ‘¢,ğ‘£ . Similarly, ğ›¼ ğ‘˜</figDesc><table><row><cell></cell><cell>â„,ğ‘£</cell></row><row><cell cols="2">can be decomposed into ğ›¼ â„,ğ‘£ ğ›¾ ğ‘˜ â„,ğ‘£ . Based on above consideration,</cell></row><row><cell>Eq. (6) is rewritten as,</cell><cell></cell></row><row><cell>Î»ğ‘˜ ğ‘£ |ğ‘¢ (ğ‘¡) = ğœ‡ ğ‘¢,ğ‘£ ğ›¾ ğ‘˜ ğ‘¢,ğ‘£ +</cell><cell>âˆ‘ï¸</cell></row><row><cell></cell><cell>â„ âˆˆH ğ‘¢ (ğ‘¡ )</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Dataset statistics</figDesc><table><row><cell></cell><cell cols="3">#nodes #static edges #temporal edges</cell></row><row><cell>BtcAlpha</cell><cell>3748</cell><cell>19139</cell><cell>19139</cell></row><row><cell>BtcOtc</cell><cell>5853</cell><cell>30540</cell><cell>30540</cell></row><row><cell>HepTh</cell><cell>22721</cell><cell>2424641</cell><cell>2651144</cell></row><row><cell>DBLP</cell><cell>27563</cell><cell>129054</cell><cell>203579</cell></row><row><cell>Techas</cell><cell>34761</cell><cell>87719</cell><cell>137966</cell></row><row><cell>Wosn</cell><cell>60663</cell><cell>594540</cell><cell>762268</cell></row><row><cell>Digg</cell><cell>61061</cell><cell>1231934</cell><cell>1403976</cell></row><row><cell cols="2">Epinions 119130</cell><cell>813694</cell><cell>813694</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Link Prediction Results</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">f1-macro</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>AUC-ROC</cell></row><row><cell></cell><cell>Dataset</cell><cell cols="2">dim</cell><cell cols="6">LINE asp2vec DeepWalk HTNE M 2 DNE MHNE</cell><cell cols="3">LINE asp2vec DeepWalk HTNE M 2 DNE MHNE</cell></row><row><cell></cell><cell>DBLP</cell><cell></cell><cell></cell><cell>0.7751</cell><cell>0.9389</cell><cell>0.9073</cell><cell>0.9335</cell><cell cols="3">0.9443 0.9594 0.8459</cell><cell>0.9799</cell><cell>0.9666</cell><cell>0.9807</cell><cell>0.9851 0.9886</cell></row><row><cell></cell><cell>Wosn</cell><cell></cell><cell></cell><cell>0.8168</cell><cell>0.8991</cell><cell>0.8896</cell><cell>0.9005</cell><cell cols="3">0.9136 0.9323 0.8889</cell><cell>0.9593</cell><cell>0.9526</cell><cell>0.9620</cell><cell>0.9691 0.9774</cell></row><row><cell></cell><cell>Digg</cell><cell></cell><cell></cell><cell>0.8034</cell><cell>0.8647</cell><cell>0.8862</cell><cell>0.8444</cell><cell cols="3">0.8701 0.8871 0.8710</cell><cell>0.9348</cell><cell>0.9507</cell><cell>0.9026</cell><cell>0.9404 0.9522</cell></row><row><cell></cell><cell>Epinions Tech</cell><cell cols="2">100</cell><cell cols="2">0.7941 0.9172 0.8882 0.9114</cell><cell>0.9228 0.7919</cell><cell>0.8916 0.8919</cell><cell cols="4">0.9239 0.9266 0.8590 0.9043 0.9086 0.9685 0.9511 0.9649</cell><cell>0.9686 0.8709</cell><cell>0.9580 0.9534</cell><cell>0.9745 0.9749 0.9622 0.9630</cell></row><row><cell></cell><cell>HepTh</cell><cell></cell><cell></cell><cell>0.8602</cell><cell>0.7981</cell><cell>0.9277</cell><cell>0.8556</cell><cell>0.8946</cell><cell cols="2">0.9222 0.9299</cell><cell>0.8776</cell><cell>0.9792</cell><cell>0.9307</cell><cell>0.9569</cell><cell>0.9737</cell></row><row><cell></cell><cell>BtcAlpha</cell><cell></cell><cell></cell><cell>0.8127</cell><cell>0.8919</cell><cell>0.8801</cell><cell>0.8834</cell><cell cols="3">0.9093 0.9210 0.8838</cell><cell>0.9514</cell><cell>0.9420</cell><cell>0.9453</cell><cell>0.9646 0.9712</cell></row><row><cell></cell><cell>BtcOtc</cell><cell></cell><cell></cell><cell>0.8194</cell><cell>0.8954</cell><cell>0.8782</cell><cell>0.8859</cell><cell cols="3">0.9177 0.9285 0.8897</cell><cell>0.9557</cell><cell>0.9380</cell><cell>0.9465</cell><cell>0.9709 0.9731</cell></row><row><cell></cell><cell>DBLP</cell><cell></cell><cell></cell><cell>0.8229</cell><cell>0.949</cell><cell>0.9316</cell><cell>0.9298</cell><cell cols="3">0.9426 0.9627 0.8983</cell><cell>0.9830</cell><cell>0.9765</cell><cell>0.9791</cell><cell>0.984</cell><cell>0.9898</cell></row><row><cell></cell><cell>Wosn</cell><cell></cell><cell></cell><cell>0.8204</cell><cell>0.9147</cell><cell>0.8840</cell><cell>0.9016</cell><cell cols="3">0.9116 0.9311 0.8946</cell><cell>0.9690</cell><cell>0.9459</cell><cell>0.9623</cell><cell>0.9685 0.9770</cell></row><row><cell></cell><cell>Digg</cell><cell></cell><cell></cell><cell>0.8428</cell><cell>0.8737</cell><cell>0.8808</cell><cell>0.8420</cell><cell cols="3">0.8707 0.8852 0.9076</cell><cell>0.9409</cell><cell>0.9469</cell><cell>0.9184</cell><cell>0.9404 0.9516</cell></row><row><cell></cell><cell>Epinions Tech</cell><cell cols="2">200</cell><cell>0.8760 0.9105</cell><cell>0.9123 0.8917</cell><cell>0.9199 0.7994</cell><cell>0.8882 0.8865</cell><cell cols="3">0.9209 0.9223 0.9346 0.9018 0.9137 0.9627</cell><cell>0.9645 0.9527</cell><cell>0.9676 0.8766</cell><cell>0.9534 0.9504</cell><cell>0.9724 0.9730 0.9614 0.9670</cell></row><row><cell></cell><cell>HepTh</cell><cell></cell><cell></cell><cell>0.8413</cell><cell>0.8563</cell><cell>0.9174</cell><cell>0.8529</cell><cell cols="3">0.8954 0.9224 0.9169</cell><cell>0.9278</cell><cell>0.9735</cell><cell>0.9299</cell><cell>0.9582 0.9737</cell></row><row><cell></cell><cell>BtcAlpha</cell><cell></cell><cell></cell><cell>0.8492</cell><cell>0.9092</cell><cell>0.8792</cell><cell>0.8763</cell><cell cols="3">0.9069 0.9256 0.9244</cell><cell>0.9647</cell><cell>0.9394</cell><cell>0.9414</cell><cell>0.9614 0.9734</cell></row><row><cell></cell><cell>BtcOtc</cell><cell></cell><cell></cell><cell>0.8507</cell><cell>0.9112</cell><cell>0.8790</cell><cell>0.8756</cell><cell cols="3">0.9134 0.9321 0.9231</cell><cell>0.9689</cell><cell>0.9385</cell><cell>0.9400</cell><cell>0.9681 0.9743</cell></row><row><cell></cell><cell>DBLP</cell><cell></cell><cell></cell><cell>0.8228</cell><cell>0.9406</cell><cell>0.9423</cell><cell>0.9250</cell><cell cols="3">0.9383 0.9611 0.8986</cell><cell>0.9782</cell><cell>0.9798</cell><cell>0.9765</cell><cell>0.9823 0.9892</cell></row><row><cell></cell><cell>Wosn</cell><cell></cell><cell></cell><cell>0.8037</cell><cell>0.9045</cell><cell>0.8636</cell><cell>0.8974</cell><cell cols="3">0.9115 0.9215 0.8784</cell><cell>0.9607</cell><cell>0.9305</cell><cell>0.9599</cell><cell>0.9690 0.9728</cell></row><row><cell></cell><cell>Digg</cell><cell></cell><cell></cell><cell>0.8507</cell><cell>0.8778</cell><cell>0.8690</cell><cell>0.8325</cell><cell cols="3">0.8636 0.8795 0.9129</cell><cell>0.9445</cell><cell>0.9379</cell><cell>0.9110</cell><cell>0.9347 0.9465</cell></row><row><cell></cell><cell>Epinions Tech</cell><cell cols="2">500</cell><cell>0.8800 0.9085</cell><cell>0.8894 0.9030</cell><cell>0.9140 0.8141</cell><cell cols="4">0.8802 0.9180 0.9116 0.9377 0.8810 0.8969 0.9115 0.9627</cell><cell>0.9505 0.9615</cell><cell>0.9632 0.8900</cell><cell>0.9499 0.9714 0.9677 0.9474 0.9595 0.9671</cell></row><row><cell></cell><cell>HepTh</cell><cell></cell><cell></cell><cell>0.8479</cell><cell>0.8650</cell><cell>0.9040</cell><cell>0.8530</cell><cell cols="3">0.8965 0.9156 0.9251</cell><cell>0.9334</cell><cell>0.9652</cell><cell>0.9283</cell><cell>0.9585 0.9710</cell></row><row><cell></cell><cell>BtcAlpha</cell><cell></cell><cell></cell><cell>0.8243</cell><cell>0.9041</cell><cell>0.8758</cell><cell>0.8644</cell><cell cols="3">0.8996 0.9249 0.9032</cell><cell>0.9610</cell><cell>0.9392</cell><cell>0.9326</cell><cell>0.9576 0.9725</cell></row><row><cell></cell><cell>BtcOtc</cell><cell></cell><cell></cell><cell>0.8123</cell><cell>0.9190</cell><cell>0.8751</cell><cell>0.8625</cell><cell cols="3">0.9052 0.9289 0.8916</cell><cell>0.9714</cell><cell>0.9399</cell><cell>0.9300</cell><cell>0.9593 0.9741</cell></row><row><cell></cell><cell>Ï¬Í˜Ï¯Ï¬</cell><cell></cell><cell></cell><cell>&gt;/E</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>WÆŒÄÄÅÆÅÅ½Å¶Î›Å¬</cell><cell>Ï¬Í˜Ï­Ï± Ï¬Í˜Ï®Ï¬ Ï¬Í˜Ï®Ï±</cell><cell></cell><cell></cell><cell cols="2">ÄÄÆ‰tÄ‚Å¯Å¬ Ä‚ÆÆ‰Ï®|ÄÄ ,dE D 2 E D,E</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Ï¬Í˜Ï­Ï¬</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Ï±</cell><cell>Ï­Ï¬</cell><cell>Å¬</cell><cell>Ï­Ï±</cell><cell>Ï®Ï¬</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc><ref type="bibr" target="#b0">1)</ref> </figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Ä‚ÆÆ‰ÄÄÆšÏ¬</cell><cell></cell><cell cols="2">Ä‚ÆÆ‰ÄÄÆšÏ®</cell><cell></cell><cell>ÅÄšÄÅ¶ÆšÅÆšÇ‡</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Æ‰Ä‚ÆŒÆšÏ¬</cell><cell>Æ‰Ä‚ÆŒÆšÏ®</cell><cell>Æ‰Ä‚ÆŒÆšÏ°</cell></row><row><cell></cell><cell>Ï­Í˜Ï¬</cell><cell></cell><cell></cell><cell>Ä‚ÆÆ‰ÄÄÆšÏ­</cell><cell></cell><cell cols="2">Ä‚ÆÆ‰ÄÄÆšÏ¯</cell><cell></cell><cell>ÄÅ½Å¶ÄÄ‚Æš</cell><cell>Ï­Í˜Ï¬</cell><cell></cell><cell></cell><cell cols="2">Æ‰Ä‚ÆŒÆšÏ­</cell><cell>Æ‰Ä‚ÆŒÆšÏ¯</cell><cell>ÄÅ½Å¶ÄÄ‚Æš</cell></row><row><cell></cell><cell>Ï¬Í˜Ïµ</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Ï¬Í˜Ïµ</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ä¨Ï­Í²ÅµÄ‚ÄÆŒÅ½</cell><cell>Ï¬Í˜Ï³ Ï¬Í˜Ï´</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Ä¨Ï­Í²ÅµÄ‚ÄÆŒÅ½</cell><cell>Ï¬Í˜Ï³ Ï¬Í˜Ï´</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Ï¬Í˜Ï²</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Ï¬Í˜Ï²</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Ï¬Í˜Ï±</cell><cell>&gt;W</cell><cell>t Å½ ÆÅ¶</cell><cell>ÅÅ Å</cell><cell>Æ‰ ÅÅ¶ ÅÅ½ Å¶ Æ</cell><cell>d Ä ÄÅš</cell><cell>, Ä Æ‰ d Åš</cell><cell>ÆšÄ Å¯Æ‰ Åš Ä‚</cell><cell>ÆšÄ K ÆšÄ</cell><cell>Ï¬Í˜Ï±</cell><cell>&gt;W</cell><cell>t Å½ ÆÅ¶</cell><cell>ÅÅ Å</cell><cell cols="2">Æ‰ ÅÅ¶ ÅÅ½ Å¶ Æ d Ä ÄÅš , Ä Æ‰ d Åš</cell><cell>ÆšÄ Å¯Æ‰ Åš Ä‚</cell><cell>ÆšÄ K ÆšÄ</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">(a) MHNE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(b) HTNE</cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Network embedding. Mikolov et al. <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref> proposes an efficient embedding learning method for words and phrases in natural language. Inspired by their work, several random walk based network embedding methods are developed by making analogy between nodes and words in natural language. For example, Deep-Walk <ref type="bibr" target="#b18">[19]</ref> and node2vec <ref type="bibr" target="#b6">[7]</ref> use truncated random walks generated from a network as the input of skip-gram model to learn node embeddings. Besides, LINE <ref type="bibr" target="#b19">[20]</ref> optimizes node embeddings by preserving first-order and second-order proximities in both vertex and embedding space. Graph Convolutional Networks(GCNs) are another approach on representation learning based on the message-passingreceiving mechanism, which is to update a node's representation by aggregating information from its neighbors <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b20">21]</ref>. However, most GCNs are supervised or semi-supervised method thus has a strong requirement for a large amount of labeled data <ref type="bibr" target="#b10">[11]</ref>.</p><p>Temporal network embedding. These aforementioned methods all focus on static networks, however, in reality, the majority of networks evolve over time. <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17]</ref> adapt conventional GCNs to model temporal networks by modeling multiple snapshots derived from different time windows. <ref type="bibr" target="#b5">[6]</ref> employs deep autoencoders on derived network snapshots, and the structure of the autoencoder evolves along with the growing of network scale. In contrast to the snapshot based methods, some efforts <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b25">26]</ref> view the formation of temporal networks as adding nodes and edges and they trace back the formation process by tracking the neighborhood formation of each node. For example, zuo et al. <ref type="bibr" target="#b25">[26]</ref> proposes HTNE by modeling the neighborhood formation sequence with a multivariate Hawkes process. M 2 DNE <ref type="bibr" target="#b12">[13]</ref> extends HTNE by incorporating both micro and macro dynamics of the network, namely neighborhood formation and network scale. Mei et al. <ref type="bibr" target="#b13">[14]</ref> proposed Neural Hakes process by using a continuous LSTM model to better reveal the sophisticated mutual effect between history and future events. However, the underlying mechanism of how temporal edges are formed driven by different aspect is ignored in prior work, which is crucial to model the multi-aspect temporal networks.</p><p>Multi-aspect embedding. There exists some recent research <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22]</ref> learning multiple embedding vectors for each node in order to capture the multifaceted nature of nodes. Liu et al. <ref type="bibr" target="#b11">[12]</ref> proposes PolyDW by extending DeepWalk to multi-aspect embedding setting, it determines the aspect distribution for each node via matrix factorization before embedding learning. Then the activated aspect of both target node and context node is sampled from the computed distribution, respectively. Splitter <ref type="bibr" target="#b4">[5]</ref> firstly creates a persona graph from the original network via a cluster algorithm that maps each node to one or multiple personas, on which a random walk based embedding method is performed to learn embeddings for each persona. Therefore it can obtain one or multiple representations for each node. However it blindly trains each persona to be close to the representation of its original node and cannot be trained in an end-to-end fashion. MCNE <ref type="bibr" target="#b21">[22]</ref> is a GCN based method that utilizes binary mask layers to create multiple conditional embeddings for each node. MCNE is a supervised method and requires predefined aspects(ie. different user behaviors). MNE <ref type="bibr" target="#b22">[23]</ref> uses matrix factorization to obtain multiple embeddings for each node and considers the diversity of learned multiple embeddings. However, it ignores that the aspect of each node is dependent to its local structure. Park et al. <ref type="bibr" target="#b17">[18]</ref> proposes Asp2vec that determines the activated aspect of source node based on its context in current random walks. While the truncated walks cannot fully recover the neighborhood structure of nodes thus may provide a biased evidence on inferring node aspects, it is more desirable to model the multiple aspect of nodes via the detailed formation process of nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>In this paper, we present a novel multi-aspect embedding method called MHNE that models the aspect driven edge formation process of temporal networks via Mixture of Hawkes process. Moreover, we utilize Gumbel-Softmax with trainable temperature parameter to compute aspect distributions with different shapes. To better capture the closeness between source and history target nodes, graph attention mechanism is incorporated to assign larger weights for more convincing excitation effects. Experiments on eight realworld datasets demonstrate the effectiveness of MHNE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A PSEUDOCODE</head><p>Algorithm 1: Pseudo code for MHNE </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C DETAILED EXPERIMENT SETTINGS</head><p>For LINE, we set the number of total edge samples to be 1 billion, other parameters are set by default. We run LINE for two times with the order parameter set to be 1 and 2 respectively, representing the first and second-order proximity. After which we concatenate the learned 2 embeddings to obtain the final embedding for each node.</p><p>For DeepWalk and asp2vec, we set the number of walks, walk length, window size and number of negative samples to be 10, 80, 3 and 2 respectively as reported in <ref type="bibr" target="#b17">[18]</ref>. For asp2vec, we set ğœ and ğœ† to be 0.5 and 0.01. Specifically, we set the threshold ğœ– for aspect regularizer in asp2vec to be 0.9.</p><p>For M 2 DNE, HTNE and our method, we set the history length, number of negative samples both to be 5. We set batch size to be 1000 for all datasets except for BtcAlpha and BtcOtc, where we set batch size to be 200. Specifically, we set ğœ– = 0.4 for M 2 DNE.</p><p>Note that we train asp2vec, M 2 DME, HTNE and MHNE using Adam with the same learning rate of 0.003 for 20 epochs. And we evaluate the learned embeddings from each epoch on downstream tasks (i.e. link prediction, temporal node recommendation) and report the best performance among them. We emperically find that asp2vec converges slowly than other methods on BtcAlpha and BtcOtc, thus we set the total training epochs for asp2vec to be 50 on these two datasets.</p><p>For fairness, we set the parameters used for representing nodes to be the same for all methods. Which is if we set the embedding size ğ‘‘ = 100 for single embedding methods like HTNE, we set the dimension for each aspect embedding to be 100/(ğ‘˜ + 1), ğ‘˜ is the number of aspects, for MHNE and asp2vec obtains another identity(target) embedding for each node. And we obtain the final embedding which is fed into downstream task by concatenating all the learned embeddings for each node.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning edge representations via low-rank asymmetric projections</title>
		<author>
			<persName><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CIKM</title>
				<meeting>CIKM</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1787" to="1796" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A survey on network embedding</title>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TKDE</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="833" to="852" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adversarial network embedding</title>
		<author>
			<persName><forename type="first">Quanyu</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
				<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning Dynamic Context Graphs for Predicting Social Events</title>
		<author>
			<persName><forename type="first">Songgaojun</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huzefa</forename><surname>Rangwala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Ning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGKDD</title>
				<meeting>SIGKDD</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1007" to="1016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Is a single embedding enough? learning node representations that capture multiple social contexts</title>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Epasto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="394" to="404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Dyngem: Deep embedding method for dynamic graphs</title>
		<author>
			<persName><forename type="first">Palash</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitin</forename><surname>Kamra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinran</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.11273</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGKDD</title>
				<meeting>SIGKDD</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01144</idno>
		<title level="m">Categorical reparameterization with gumbel-softmax</title>
				<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
				<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Is a single vector enough? exploring node polysemy for network embedding</title>
		<author>
			<persName><forename type="first">Ninghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaoyu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuening</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGKDD</title>
				<meeting>SIGKDD</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="932" to="940" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Temporal network embedding with micro-and macro-dynamics</title>
		<author>
			<persName><forename type="first">Yuanfu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanfang</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CIKM</title>
				<meeting>CIKM</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="469" to="478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">The Neural Hawkes Process: A Neurally Self-Modulating Multivariate Point Process</title>
		<author>
			<persName><forename type="first">Hongyuan</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>In NeuraIPS</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<title level="m">Efficient estimation of word representations in vector space</title>
				<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">EvolveGCN: Evolving Graph Convolutional Networks for Dynamic Graphs</title>
		<author>
			<persName><forename type="first">Aldo</forename><surname>Pareja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giacomo</forename><surname>Domeniconi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toyotaro</forename><surname>Suzumura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroki</forename><surname>Kanezashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Kaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><forename type="middle">B</forename><surname>Schardl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">E</forename><surname>Leiserson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5363" to="5370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Chanyoung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hwanjo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04239</idno>
		<title level="m">Unsupervised Differentiable Multi-aspect Network Embedding</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGKDD</title>
				<meeting>SIGKDD</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WWW</title>
				<meeting>WWW</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>VeliÄkoviÄ‡</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">MCNE: An end-to-end framework for learning multiple conditional network representations of social network</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Defu</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongfang</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGKDD</title>
				<meeting>SIGKDD</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1064" to="1072" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-facet network embedding: Beyond the general solution of detection and representation</title>
		<author>
			<persName><forename type="first">Liang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanfang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaochun</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
				<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Mixture of mutually exciting processes for viral diffusion</title>
		<author>
			<persName><forename type="first">Shuang-Hong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyuan</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dynamic network embedding by modeling triadic closure process</title>
		<author>
			<persName><forename type="first">Lekui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
				<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Embedding temporal network via neighborhood formation</title>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guannan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqian</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGKDD</title>
				<meeting>SIGKDD</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2857" to="2866" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
