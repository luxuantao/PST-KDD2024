<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fast Application Launch on Personal Computing/Communication Devices</title>
				<funder>
					<orgName type="full">National Research Foundation</orgName>
					<orgName type="abbreviated">NRF</orgName>
				</funder>
				<funder ref="#_EFqYt9U">
					<orgName type="full">Korean government (MSIT)</orgName>
				</funder>
				<funder ref="#_yG52v2z">
					<orgName type="full">IITP</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Junhee</forename><surname>Ryu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Dongeun</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Texas A&amp;M University -Commerce</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kang</forename><forename type="middle">G</forename><surname>Shin</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kyungtae</forename><surname>Kang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Hanyang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Hynix</surname></persName>
						</author>
						<title level="a" type="main">Fast Application Launch on Personal Computing/Communication Devices</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present Paralfetch, a novel prefetcher to speed up app launches on personal computing/communication devices by: 1) accurate collection of launch-related disk read requests, 2) pre-scheduling of these requests to improve I/O throughput during prefetching, and 3) overlapping app execution with disk prefetching for hiding disk access time from the app execution. We have implemented Paralfetch under Linux kernels on a desktop/laptop PC, a Raspberry Pi 3 board, and an Android smartphone. Tests with popular apps show that Paralfetch significantly reduces app launch times on flash-based drives, and outperforms GSoC Prefetch and FAST, which are representative app prefetchers available for Linuxbased systems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Quick app launches are of great importance to user experience on personal computing/communication devices such as laptop/tablet PCs, single-board computers, and smartphones <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b33">34]</ref>. The latency of launching an app mainly depends on the performance of the underlying CPU and flash-based disks. Despite continuing improvements in the performance of these components, the launch latencies, especially of large apps and games, still remain an important problem for three reasons.</p><p>First, the performance of flash storage does not always meet users' expectations/desire. For example, it has been predicted <ref type="bibr" target="#b52">[53]</ref> that in 2025 around 50% of the data on flash will be stored in QLC (quad-level cell) flash, which has 2.1? slower read and 5.7? slower write times than TLC (triple-level cell) flash <ref type="bibr" target="#b3">[4]</ref>. The use of affordable QLC SSDs was found to extend the launch latency of the popular Blade and Soul game from 91s to 114s <ref type="bibr" target="#b45">[46]</ref>, and that of Horizon Zero Dawn from 15.7s to 21.4s <ref type="bibr" target="#b46">[47]</ref>, compared to high-end SSDs. Many Windows apps take a similar amount of time <ref type="bibr" target="#b47">[48]</ref> to launch from the Samsung QLC SSD as they do from the Intel X25-M G2 SSD, which was released in 2009. Furthermore, recent entryclass SSDs widely adopt DRAM-less architecture <ref type="bibr" target="#b34">[35]</ref>, which leads to additional flash accesses for translating logical-tophysical addresses. A Raspberry Pi is also widely used to run desktop applications <ref type="bibr">[57]</ref>, but it only supports the sluggish MicroSD.</p><p>Second, the complexity of apps is continuously growing due to the addition of new features and functionality to software <ref type="bibr" target="#b49">[50]</ref>. Unfortunately, complex software also requires higher-level programming languages and libraries, generating slower code, thus extending their launch latencies <ref type="bibr" target="#b54">[54]</ref>. Third, although parallelism is effectively utilized in modern multicore CPUs and solid-state disks <ref type="bibr" target="#b7">[8]</ref>, app launches can seldom exploit existing sources of parallelism. It has also been shown <ref type="bibr" target="#b24">[25]</ref> that CPUs and disks are seldom utilized simultaneously during a launch because synchronous disk reads are dominant. Making better use of parallelism is, therefore, a major consideration in the design of app prefetchers <ref type="bibr" target="#b23">[24]</ref>.</p><p>Launch latencies depend on the previous state of the system, especially the disk cache. A cold start occurs when the disk cache does not hold any data required by the app, either because it is the first time the app has been launched, or because all of the app's data has been evicted since its last run. A system cold start is a special case of cold start, which occurs when no user-launched app is already running. A warm start occurs when the app being launched has been running recently, so the disk cache still holds all, or most, of the data that it needs. A warm start is much faster than a cold start, because no, or very little, data has to be fetched from the disk. This avoids the concomitant file system operations, thus saving CPU time as well as disk time.</p><p>An app prefetcher <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b39">40]</ref> can reduce the time needed for a cold start: during learning phase, which corresponds to the first launch of an app, the prefetcher collects launch-related blocks and/or their access sequences (the term launch sequence is used interchangeably). This is usually achieved by monitoring disk reads and/or page faults. A prefetching phase occurs during subsequent launches of the app, in which case this launch sequence is used for disk prefetching to accelerate loading.</p><p>Different prefetching strategies are required for the different seek characteristics of mechanical and flash disks. These storage devices have different performance bottlenecks which have been addressed in well-known ways. Threaded prefetching is designed for SSDs. A dedicated thread is used to prefetch blocks in the order of their collection during monitoring. The prefetching thread runs concurrently with the app, reducing the launch time. On the other hand, Sorted prefetching is designed for HDDs. Data is read from the disk in logical block address (LBA) order to reduce seek times <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref>, which account for most of the launch time. Sorted prefetching is not done concurrently with the app because the app's disk I/O would disrupt prefetching in the LBA sequence.</p><p>In this paper, we define three fundamental challenges in reaping the potential speed-up with an app prefetcher, and then explain how Paralfetch addresses these issues that previous approaches fail to achieve. Overall, this paper makes the following main contributions: ? Accurate tracking of launch-related blocks ( ?3.1): Most monitoring methods fail to locate a significant number of blocks during the learning phase <ref type="bibr" target="#b22">[23]</ref>. In threaded prefetching on SSDs, an access tracer should collect not only accessed blocks but their access order. To do this, a viable solution is to monitor at the disk I/O level after performing the invalidation of unused entries in the disk cache. Unfortunately, metadata and data blocks would not be detected by imperfect OS-level disk cache invalidation. To address this problem, Paralfetch introduces a file-system-level block dependency check and low-overhead page-fault monitoring.</p><p>? Pre-scheduling of these blocks to increase prefetch throughput ( ?3.2): Although the I/O involved in prefetching frequently becomes a bottleneck in threaded prefetching on commodity SSDs, prior work does not address this issue. We observe I/O dependencies between prefetch blocks to significantly hinder the asynchrony of I/O requests, reducing prefetch throughput. We address this problem with a new I/O reordering method called metadata shift that places more I/O requests between dependent I/O requests, issuing more I/O requests asynchronously. A range merge is also introduced to combine nearby I/O requests into one large request, improving I/O throughput.</p><p>? Tailored overlapping of application execution with prefetching ( ?3.3): We find that aggressive prefetching with excessive pre-scheduling can actually increase launch latencies because of I/O contention between the app and prefetching threads. Modern SSDs' reordering of outstanding I/O operations can aggravate this contention <ref type="bibr" target="#b40">[41]</ref>. We vary the amount of I/O optimization in response to a prefetching bottleneck. This avoids the I/O contention caused by an excessive optimization, and thus helps Paralfetch find a better optimization level.</p><p>? Implementation ( ?4) and evaluation ( ?5) of Paralfetch: We evaluate Paralfetch in the launch of common apps on a laptop PC, a Raspberry Pi 3, and an Android smartphone. With the aforementioned features, Paralfetch achieves launch performance close to the warm start: On a PC, Paralfetch reduced the average system cold start time (favoring competitors) of 16 benchmark apps by 48.0%, this number corresponds to 11% and 22% further reductions from FAST and GSoC Prefetch, respectively. Paralfetch also reduced the average app launch time on a Raspberry Pi 3 by 31%, and on an Android phone by 11%. Paralfetch is publicly available   page cache for regular files, slab (or slub) cache for metadata objects, and buffer cache for metadata blocks. The slab is used as an object-granular metadata cache for buffer cache. read system call explicitly fills page cache based on its arguments, while page cache for mmaped files is populated through page fault mechanism. Readahead framework is responsible for filling the contents of page cache, and it determines how many blocks to be prefetched based on the access sequentiality. Note that metadata blocks can be prefetched by EXT4 file system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Motivation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Targets of Paralfetch</head><p>Linux-based systems using EXT4 file system. We implemented and tested a Paralfetch prototype on EXT4 file system on a laptop with SSD, a Raspberry Pi 3 with microSD card, and a Pixel smartphone with universal flash storage (UFS). Large apps with highly deterministic I/O. Other applications do not benefit much from Paralfetch: I/O requests from text-based apps such as cp, gcc and find largely depend on input parameters that can change with every launch; and apps such as pwd and ssh are too small to amortize prefetch overhead, and are usually warm started.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Disk Caching in Linux</head><p>Figure <ref type="figure" target="#fig_0">1</ref> provides a summary of the Linux I/O stack from disk caching perspectives. Page cache and buffer cache. The Linux kernel provides two cache mechanisms for disk blocks in terms of API and unit size <ref type="bibr" target="#b14">[15]</ref>: The page cache holds file pages, whereas the buffer cache contains data blocks corresponding to block devices. The contents and lookup spaces of these caches are managed using a radix tree for each regular file or block device file. In EXT4 file system, blocks of data from regular files are cached in the page cache, while the buffer cache is used for caching metadata blocks (e.g., inode table blocks, directory blocks, and extent blocks). The contents of regular files can be prefetched using a combination of device number, inode number, offset, and size. On the other hand, metadata blocks can be prefetched using a combination of device number and block number. It should be noted that there are no prefetchinglevel dependencies among buffer-cached (metadata) blocks, whereas I/O requests for page-cached (data) blocks are delayed until relevant metadata blocks are cached. Slab for caching file system metadata at object granularity. Metadata objects in EXT4 file system, namely, the inode, directory entry, and extent, are smaller than a file system block but must nevertheless be managed individually so that important objects are kept in memory, even when the memory is under pressure. Therefore, the Linux slab object allocator caches these objects without reference to the contexts of the buffer cache. Thus an inode can be simultaneously stored in both the slab and buffer caches. Page cache accessing methods. A process can copy the contents of the page cache into a user buffer using a read or a file-related syscall. Alternatively, a process can map the extent of a file to its virtual address space using the mmap syscall. In the latter case, attempting to access an unmapped address in the page table causes a page fault. To reduce the number of page faults, Linux employs an interesting feature, called faultaround <ref type="bibr" target="#b48">[49]</ref>, which pre-faults a 64KB-aligned chunk of the address space around the fault address. Disk cache invalidation. The Linux kernel provides functions to invalidate disk caches. A user or process with root permission can invalidate these caches by writing a predefined value ("1" for the page and buffer caches, "2" for the slab cache, and "3" for all these) into the /proc/sys/vm/drop_caches proc file. This method can only invalidate unused entries with zero reference counts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Representative App Prefetchers</head><p>Windows prefetcher <ref type="bibr" target="#b36">[37]</ref>. Since XP, Windows has included a prefetcher for launch and system boot. The Windows prefetcher is customized for HDDs, but it can also be used with SSDs, although user configuration is required to make best use of more capable SSDs. In its learning phase, the copies of file-backed memory pages which are required by an application are identified by the Windows working-set manager. The generated information, which is file-level data, determines the disk blocks to be prefetched during subsequent application launches. By defragmenting these blocks to make their file-level prefetch blocks correspond to their LBA order, the Windows prefetcher optimizes the disk head movements of HDD. This time-consuming process is scheduled to happen every three days. GSoC Prefetch <ref type="bibr" target="#b28">[29]</ref>, which was selected for the Google Summer of Code 2007, is a Linux-based prefetcher for HDDs. It S i is the i th block requested from the SSD, and C i is the corresponding CPU computation. Paralfetch expedites an application launch by exploiting parallelism of each resource (i.e., multicore activation and internal parallelism on SSDs) and utilizing these resources concurrently.</p><p>obtains launch-related block information in its learning phase by first clearing the bit in every OS-managed page descriptor (not page table) which indicates that the page has been referenced. After a predefined monitoring time (10 seconds by default), GSoC Prefetch traces those referenced pages with 'referenced' bits on. It then extracts a file identifier (device number, inode number, and offset) from each of the traced pages. Next, GSoC Prefetch sorts the pages based on these identifiers and stores the sorted pages in a file. On subsequent launches, launch-related blocks are prefetched in the order recorded in that file. This reduces both seek and rotational latencies in HDDs. GSoC Prefetch has a defragmentation tool similar to that in the Windows prefetcher. FAST <ref type="bibr" target="#b23">[24]</ref> is a recent Linux-based prefetcher for SSDs. It starts by clearing the slab, buffer, and page caches. Then, FAST begins its learning phase, during which it creates a prefetch program by monitoring the LBAs of blocks using the blktrace tool and converting them to prefetchable system calls with arguments. On subsequent launches, FAST executes this prefetch program at the same time as the application. Disk blocks are prefetched in order without any I/O optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Cold Start with Paralfetch</head><p>Figure <ref type="figure" target="#fig_1">2a</ref> shows a cold start scenario without Paralfetch, and Figure <ref type="figure" target="#fig_1">2b</ref> shows the same scenario in which Paralfetch runs the application concurrently with a prefetch thread. The computations run on multiple CPU cores, in parallel with the SSD accesses, which are issued in a way that exploits the internal parallelism of the SSD. This is effected by issuing concurrent asynchronous I/O requests using the command queuing (CQ) feature. If an SSD does not support CQ, Paralfetch merges I/O requests, which have consecutive LBAs and are close in the block access sequence, so as to promote internal parallelism. 3 Paralfetch Design and Preliminary Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Accurate Tracing</head><p>The benefit from an application prefetching is limited by the tracing accuracy with which launch-related blocks are traced. In particular, accurate tracing is essential to prevent a launching application's wait for missing blocks from disk when several concurrent threads are causing lots of I/O contention. Note that the threaded prefetching can marginally benefit from Windows prefetcher and GSoC Prefetch which cannot trace the block access sequence because they rely on a snapshot of the working set or of the referenced pages after a launch.</p><p>There are also issues with the tracing method used by GSoC Prefetch: it only traces pages for regular files, and missing metadata limits the benefit of prefetching; a significant number of pages are also accessed more than once during a launch. This latter issue is particularly problematic because, when a page with the 'referenced' bit set on is accessed for the second time, Linux OS turns off the 'referenced' bit and promotes the page from the inactive list to the active list. As a result, some pages are never traced. In the case of Eclipse, we found 2,782 file-backed pages not traced.</p><p>Potentially, the highest accuracy would be achieved by monitoring page faults and data accesses at all disk caching layers (e.g., slab, buffer, and page caches). But such exhaustive tracing would produce significantly more data than I/O-level monitoring (37? during an Eclipse launch), incurring unacceptable memory and computation overheads. Furthermore, a log of I/O operations obtained by monitoring disk cache ac-cesses is likely to include many useless cached entries created by I/O operations of background tasks.</p><p>This issue is successfully mitigated by monitoring I/O requests: In the learning phase, Paralfetch invalidates unused entries in the disk cache, so that Paralfetch collects a proper set of blocks for subsequent launches of the application. It then records I/O requests for blocks not found in these caches by instrumenting file system functions with I/O logging codes, and these requests are used to prefetch those additional blocks during launches. In this paper, we use the term log entry to refer to a log of I/O request collected during a launch, while the term prefetch entry refers to an entry used for prefetching disk blocks. The latter includes arguments for prefetching function calls.</p><p>Unfortunately, as mentioned earlier, the invalidation of disk caches (slab, buffer, and page caches) is not perfect because only unused entries can be invalidated; a working set of blocks for running applications is always retained. This issue has been overlooked in previous schemes (including FAST), i.e., their evaluation was restricted to system cold start scenarios. Table <ref type="table" target="#tab_2">1</ref> classifies traced blocks with Paralfetch. Note that metadata blocks and mmaped file blocks are potential missing blocks when using FAST. Since usually many user and system processes run in the background, this issue can significantly degrade tracing accuracy. For example, 225 files of this kind were accessed by both LibreOffice Impress and LibreOffice Writer (on a laptop) during a launch of either. Thus, an attempt to trace launch blocks for LibreOffice Writer just after LibreOffice Impress launched (and started running in the background) returns only 700 log entries (27,688 KB) compared to 1,281 log entries (83,824 KB) during a system cold start. We conducted further experiments by substituting Android Studio, Chromium Browser, Eclipse, and GIMP for LibreOffice Impress. Surprisingly, imperfect cache invalidation still resulted in many missing data and associated metadata blocks: 5.0%, 12.0%, 14.4%, and 6.6% of the total in each case. The launch time impact of missing blocks is significant as shown in ?5.2.</p><p>We have therefore developed two methods to detect missing metadata and data blocks. 1) Finding missing metadata blocks. We first introduce a file system-level dependency check, called missing metadata block detection, which identifies launch-related metadata blocks (i.e., inode and extent blocks) that have not been traced due to the imperfect invalidation of the slab and buffer cache, but nevertheless share a dependency with traced data blocks. To address this issue, Paralfetch implements a function ( ?4.2) that tracks associated metadata blocks for each log entry for a regular file. Table <ref type="table" target="#tab_2">1</ref> shows that 15 -58 missing metadata blocks were found during launches, and these numbers vary with the number of irreclaimable entries in the disk caches under use by running applications. When these missing blocks are found, Paralfetch inserts new log entries for them just before other log entries of associated data blocks.</p><p>2) Page fault monitoring. Page cache invalidation is also imperfect because file-backed pages which are dirty, under writeback, or accessed through mmap, are not invalidated. To trace pages which are dirty or under writeback, Paralfetch flushes them out via a sync operation before the disk cache is cleared. However, pages accessed through mmap, such as shared library files, are more challenging. When these are shared with running applications, tracing accuracy is compromised. To address this issue, we arranged for Paralfetch to trace previously untraced blocks accessed through mmap calls by instrumenting the faultaround <ref type="bibr" target="#b48">[49]</ref> handler with page fault tracing code. The handler proactively maps 16 boundaryaligned (page-cached) pages around the page-faulted address.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Prefetch Scheduling</head><p>Upon completion of collection of disk I/O requests during an application launch, Paralfetch pre-schedules these requests to speed up the prefetching phase, merging and reordering requests so as to exploit the internal parallelism of an SSD. Range merging. Merging small I/O requests into a single large request enhances the throughput of an SSD <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b42">43]</ref>. Figure <ref type="figure">3b</ref> shows a range merge in which two requests for blocks with consecutive LBAs that are within a predefined I/O distance threshold are combined where the I/O distance is defined as difference in the locations of blocks in the launch sequence. This threshold prevents the merging of far-apart log entries in the launch sequence, as they can hinder timely prefetching of subsequent blocks. Overly-aggressive merging can be bad especially for applications with CPU-bound launches, in which I/O optimization is less influential in meet- ing prefetching deadlines. Figure <ref type="figure">4</ref> shows plots of prefetch time against the I/O distance threshold on SSD, UFS flash, and MicroSD. The performance gain from range merging tails off as the threshold increases mainly because EXT4 tries to locate metadata and data blocks for related files close together in terms of LBA. Metadata shifting. Every file system has its own particular I/O dependencies for prefetching between metadata and data blocks (and between metadata blocks). In EXT4, a request for a data block can only be issued after the associated metadata block, which contains the LBA of that data block, has been read. The metadata for a data block is often requested just before the corresponding data block. Thus this dependency tends to limit the number of commands that can be queued, and this in turn limits the effectiveness of command queuing, which yields maximum benefit when there are many commands in the queue which can potentially be executed in parallel <ref type="bibr" target="#b38">[39]</ref>. This issue can be addressed by bringing forward requests for metadata blocks. This is facilitated in EXT4, where there are no read dependencies among buffer-cached (metadata) blocks, while I/O requests for page-cached data blocks can only be issued after associated metadata blocks are buffercached. Figure <ref type="figure">5a</ref> shows the processing of an example prefetch thread, in which dependencies on metadata blocks cause the command queue to become empty on two occasions. Figure <ref type="figure">5b</ref> shows how Paralfetch brings forward metadata block requests in the prefetch thread to increase the interval between requests for dependent blocks. Figure <ref type="figure" target="#fig_4">6a</ref> shows that the average prefetching time on a CQ-enabled SSD was reduced by 21.6% through shifting metadata requests forward by 128 KB, when combined with the tracing of missing metadata blocks.</p><p>An SSD without CQ support can also benefit from shifted metadata (Figure <ref type="figure" target="#fig_4">6c</ref>): requests to the I/O scheduler can be issued in advance, so that the storage driver receives a request earlier from the I/O scheduler queue, rather than later by the application; and an MMC/SD driver (for eMMC flash and SD cards) overlaps flash access for the current I/O request with DMA preparation for the next I/O request. A metadata shift of 4 KB reduced prefetch times by 19.3% on the Raspberry Pi 3 using a MicroSD.</p><p>Correctness. The read requests from the prefetch thread go through disk caches, and hence reordering and merging of a launch sequence have no implications on correctness. Even if a prefetch entry is outdated, it only affects the launch performance. The ability of shifting metadata and merging nearby requests to reduce prefetching time on SSD-based systems is limited by contentions between I/O requests from the prefetch thread and I/O requests which must be issued by the application because they were omitted from the prefetch thread. As shown in Table <ref type="table" target="#tab_2">1</ref>, we found that an average of 2.8% of requested blocks were not traced despite the improved tracing features of Paralfetch. These missing blocks are inevitably requested by the application, which waits until the blocks are loaded from the disk. Contention between the application and the prefetch thread becomes critical when there are too many I/O requests in the I/O scheduler or command queue <ref type="bibr" target="#b12">[13]</ref> in an SSD. This can occur when metadata blocks are shifted too far, or when an oversize I/O request is created by range merging with a large threshold. From an experiment with Eclipse, we found that the effect of missing blocks on latency was increased by 3.2? and 8.7? when the largest allowable shifts were 128KB and 256KB, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Parallelized</head><p>To avoid the need to optimize the thresholds for metadata shifting and range merge over a number of trial runs, Paralfetch gradually increases the threshold if prefetching   The best combination of scheduling methods depends on the type of disk. For example, on a CQ-supported SSD, range merge gains little beyond a threshold of 8, which can, therefore, be used as a default during the learning phase. Similarly, metadata shifting yields little benefit on MicroSD-based devices without CQ support beyond a threshold of 4KB. Detecting prefetch bottleneck. An application experiences more context switches when it has to wait for the blocks requested by the prefetch thread, implying that the prefetch thread is not prefetching in time. Specifically, the prefetch thread collects the number of context switches made by the launching application during the prefetching period. Paralfetch ends dynamic scheduling if the quantity of context switches is below a user-defined threshold (by default, 5% of the number of prefetch entries). The overall disk read size is checked by Paralfetch in order to remove the results from the warm cache.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Implementation of Paralfetch</head><p>This section details the workflow of Paralfetch and the interaction among its main components described in Figure <ref type="figure" target="#fig_6">9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Launch Phase Management</head><p>Native Linux: The next launch type for each application is determined by reading the 9-th byte of the header of its executable and linkable format (ELF) binary file. This byte (referred to as the phase byte) is normally used for memory alignment (padding), and has a default value of 0. It is set to PHASE_LEARNING (3) for a learning phase and PHASE_THREADED_PREFETCHING (1) for a prefetching phase. A user can also set this value to PHASE_DISABLE (9) to disable prefetching altogether, for small applications or utilities that frequently experience warm starts. The phase byte is passed to the ELF binary loader (load_elf_binary).</p><p>Paralfetch supports two modes for launch phase management. In manual mode, a user explicitly selects applications that will use Paralfetch, by calling pfsetmode, which takes a value for the phase byte and an ELF binary path as arguments. pfsetmode can be also invoked from a desktop icon (i.e., mouse right-click menu). In contrast, Paralfetch is applied to all installed applications in automatic mode, which is similar to the management method used in FAST. Android: zygote is a process that creates a native Android application in Java by forking and loading the main class of a program <ref type="bibr" target="#b29">[30]</ref>. zygote invokes the handleChildProc method to create and run a new Android application. To re- duce launch times, zygote preloads classes and resource files used by many applications, quickly creating a process which shares these preloaded classes. Unlike native Linux processes, a native Android process remains in the background even after a user quits the application, and can be resumed by moving the process to the foreground (the resuming procedure). However, when free memory is in short supply, Android wakes up the low memory killer (LMK) to reclaim memory space by removing less important processes completely.</p><p>To interface Paralfetch with the Android platform, we created a file named fetch_app using sysfs, which provides a communication interface between the Linux kernel and a user process. On Android, Paralfetch uses automatic launch management mode, in which Paralfetch tailors each launch to the type of application. When the main class name of an application is written to the fetch_app file, Paralfetch determines how to perform the launch phase based on the following rules: if there is no corresponding &lt;class_name&gt;.pf file 2 in the /persist/paralfetch direc-2 &lt;class_name&gt;.pf file is equivalent to &lt;app_name&gt;.pf in native Linux. tory, then Paralfetch starts a learning phase for that application; but if the file exists, then Paralfetch performs prefetching. To implement this, we augmented the handleChildProc method to write the main class name of the application being launched to the fetch_app file. Paralfetch does not begin a prefetching for the resuming procedure that does not invoke handleChildProc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Learning Phase</head><p>I/O logging. To collect blocks required for a launch, Paralfetch first invalidates unused entries in the slab (for file system objects), buffer and page caches, and temporarily disables the inode read-ahead functionality of EXT4 so as to prevent I/O contention resulting from unnecessary inode blocks being read during the prefetching phase. Next, Paralfetch sets a trace timeout, with the default value of 30 seconds, and also sets the trace_flag to true to activate logging. Then, Paralfetch resumes loading and execution of the application. During the execution, the I/O requests for buffer-cached blocks caused by disk cache misses are logged by code introduced into the metadata access function (submit_bh_wbc). Similarly, code introduced into the functions ext4_readpage, ext4_readpages, and filemap_map_pages logs read requests associated with pagecached blocks. Page fault monitoring. The filemap_map_pages function is called by the OS when a page fault occurs. It pre-faults the 16 boundary-aligned pages which contain the faulting page, provided that these pages are in the page cache <ref type="bibr" target="#b48">[49]</ref>. Performing this reduces the overhead of tracing page faults. Tracing missing metadata blocks. Block tracing ends when the trace times out, and the launch is deemed to be complete when fewer than 10 block read requests occur in a second <ref type="bibr" target="#b24">[25]</ref>. We refer to the corresponding block of an application as the completion block. To detect missing metadata blocks, we implemented the ext4_fiedep function, a variant of the ext4_fiemap function that must in any case access the metadata blocks associated with file blocks during the mapping of logical-to-physical extents. Unlike the original version that returns file extents for arguments (i.e., a file and query range of the file), the ext4_fiedep function returns a list of associated metadata blocks along with file extents.</p><p>As shown in Figure <ref type="figure" target="#fig_6">9</ref>, Paralfetch builds two red-black binary search trees for log entries that are used for prefetch scheduling: Paralfetch reads log entries in their access order and inserts each of them to the trees. It invokes the ext4_fiedep function for each log entry for a regular file. If the corresponding metadata blocks are missing from the tree, Paralfetch allocates and inserts new log entries for them right before the entry for the corresponding data blocks. This operation consumes little CPU time (17 ms for Android Studio) and incurs no disk I/Os because the procedure runs in the warm cache condition (i.e., after the completion of a launch process). Pre-scheduling. Paralfetch schedules the collected log entries. Algorithm 1 describes the procedure of metadata shift: Paralfetch accesses log entries in their access order (lines 1, 3, 11). A log entry for metadata blocks moves right away to the MS queue 3 (lines 4-5), while a log entry for data blocks remains in the wait queue until enough subsequent metadata blocks (at least the metadata shift size) are moved to the MS queue (lines 9-10) in order to left-shift metadata I/O requests When enough metadata blocks are left-shifted, the accompanying wait queue log entries are transferred to the MS queue (line 7). Finally, the red-black tree rbtree_seq is rebuilt with the metadata-shifted order (line 13) once the remaining log items in the wait queue are transferred to the MS queue (line 12).</p><p>To perform range merge (as described in Algorithm 2), Paralfetch accesses log entries in their LBA-sorted order. This makes it easy to detect log entries that have consecutive LBAs (line 5) of the same inode (line 4). Range merge then combines consecutive I/O operations (lines 7-9) that are 3 The MS queue stores the metadata-shifted order of log entries. within a predefined threshold for I/O distance in the launch sequence (line 6).</p><p>Different thresholds of metadata shift and range merge are used for SSDs with and without command queuing (CQ). To discover whether an SSD supports CQ, the Paralfetch initialization process, executed by the systemd daemon or a startup script (e.g., rc.local), examines sysfs files. For example, the CQ support for an SATA SSD is determined by the value of /sys/block/&lt;root device&gt;/device/queue_depth. Storing scheduled log entries. Scheduled log entries (i.e., prefetch entries) are stored in the file &lt;app_name&gt;.pf (e.g., eclipse.pf for Eclipse). This file consists of a 24-byte Paralfetch header, followed by prefetch entries. The header contains the version number, the inode number of the executable file, the metadata for dynamic scheduling, the number of obsolete entries, and the number of prefetch entries. Each prefetch entry contains the device number, the inode number, its offset and size. The inode number for a metadata block is set to 0. The size of each prefetch entry is 20 <ref type="bibr" target="#b23">(24)</ref> bytes on a 32(64)-bit system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Prefetching Phase</head><p>During the prefetching phase, Paralfetch creates the prefetch thread, following the sequence stored in the &lt;app_name&gt;.pf file. For EXT4 file system, Paralfetch uses the __breadahead function to prefetch metadata blocks, and the force_page_cache_readahead function to prefetch data blocks for regular files. While these functions try to perform block caching asynchronously (or in a non-blocking manner), data blocks can be prefetched asynchronously only when the associated metadata blocks are ready. Paralfetch uses explicit I/O plugging <ref type="bibr" target="#b2">[3]</ref> to merge contiguous metadata (bio) requests into a single request, which is then delivered to the dispatch queue of device drivers. This reduces the amount of computation required for dispatching and completing I/O requests. Changing from prefetching back to the learning phase. The set of blocks required for the first launch of some applications is significantly different from that required for subsequent launches. For example, Eclipse and GIMP only configure their environments on their first launch: Paralfetch detects this behavior by counting I/O requests issued by an application during its launch, which is easily done by counting synchronous readahead requests <ref type="bibr" target="#b37">[38]</ref> in the Linux readahead framework <ref type="bibr" target="#b32">[33]</ref>. If the count is greater than 10% of the total number of prefetch entries, Paralfetch returns to the learning phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation 5.1 Methodology</head><p>Launch time measurement. Like <ref type="bibr" target="#b23">[24]</ref>, we measure the launch time of an application between two events: in the Figure <ref type="figure" target="#fig_0">10</ref>: Launch times on a laptop equipped with a QLC SSD, normalized to cold start times. Optimizations for Paralfetch are incrementally applied. case of Linux, the launch is deemed to start when the load_elf_binary function is called, and to finish when the completion block request has itself completed. To identify the latter event, we remove the completion block request from the prefetch file, allowing it to be issued by the application. After a warm start, we call posix_fadvise with the argument POSIX_FADV_DONTNEED to evict the completion block request from the page cache. Comparisons with other prefetchers. We ported the GSoC Prefetcher to the Linux kernel 5.4.51 and set its trace timeout to the value used by Paralfetch. We temporarily modified Paralfetch to bring its operation in line with three key features of the GSoC Prefetcher: 1) the way in which it traces referenced file pages during an application launch, 2) its method of pre-scheduling disk I/O using inode numbers and in-file offsets as sort key, and 3) the way in which it holds an application until prefetching is completed, rather than allowing the application and the perfetcher thread to compete.</p><p>FAST only supports EXT3 file system, so we temporarily modified Paralfetch's function for detecting missing metadata to support EXT3. We could only compare FAST with Paralfetch on a PC because the Android and Raspbian OS do not support EXT3 file system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">On a PC</head><p>We conducted experiments on a laptop PC equipped with an Intel Core i5-8265 CPU and 16 GB of RAM, running Linux kernel 5.4.51. This PC has a 1 TB Samsung 860 QVO QLC SSD, which uses native command queuing. We tested Paralfetch, GSoC Prefetch and FAST on 16 applications, 6 of which were games. The 10 non-game applications were Android Studio, Chromium Browser, Eclipse, GIMP, LibreOffice Impress, LibreOffice Writer, Okular, Scribus, VLC player, and Xilinx ISE; and the 6 games were Ancestors Legacy, Atom RPG, Battle Tech, Pillars of Eternity 2, Tyranny, Witcher 3.</p><p>QLC SSDs typically employ a small pseudo-SLC (singlelevel cell) cache. To reduce the effects of this cache, we conducted evaluation after installing all benchmark apps. Comparison with the GSoC prefetcher. Figure <ref type="figure" target="#fig_0">10</ref> shows Paralfetch to reduce the average launch time of these 16 applications by 44.2% with pre-scheduling alone. After four launches of each application, a 1.8% more reduction was achieved on average by using dynamic scheduling to increase prefetch throughput.</p><p>It should be noted that the na?ve use of excessive metadata shift (of 256KB) led to a 3.8% increase in average launch time: as previously shown in Table <ref type="table" target="#tab_2">1</ref>, Paralfetch fails to trace a few launch blocks. A launching application should wait for these missing blocks to be read while a large number of outstanding I/O requests due to excessive metadata shift increase the waiting time. Comparison with FAST. FAST is the closest to ours in that its target media is SSDs. In ?3.1 we described how disk cache clearing affects tracing accuracy. The most serious drawback of FAST seems to be that the accuracy of its tracing depends greatly on the other applications that are running, because files accessed by these applications through mmap are not traced. Also, metadata used by the applications are not traced. We believe that this issue is frequently occurred in common scenarios. Figure <ref type="figure" target="#fig_8">11</ref> shows the significance of this issue. Conversely, the page fault monitoring and detecting missing metadata used by Paralfetch leads to launch times similar to that of a warm start. Although tracing under a system-cold state favors FAST, the launch times averaged across all 16 applications were 11% less with Paralfetch than with FAST as shown in Figure <ref type="figure" target="#fig_9">12</ref>. The relatively poor performance of FAST can be attributed to its reliance on system calls, which limits both the accuracy of tracing and its scheduling options, in particular its use of synchronous I/O for prefetching metadata blocks makes it difficult to exploit parallelism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Raspberry Pi 3</head><p>Our second evaluation of Paralfetch was conducted on a Raspberry Pi 3 running the Raspbian OS (Linux kernel 4.9.56) with a Samsung 16 GB MicroSD (class 10). This flash storage does not support CQ (although more recent A2-class MicroSD has both CQ and an SLC cache).</p><p>We used 13 applications, 8 of which were games: Frozen Bubble, GIMP, LibreOffice Writer, Chromium browser, Scratch 2, Xpdf, 0 A.D., Extreme Tux Racer, LinCity, Mindcraft, Open Arena, Quake 3 Arena, and Xmoto. The launch times in Figure <ref type="figure" target="#fig_10">13</ref> show that frequent flash accesses contribute about 45% of the delay in application launches. This provides a considerable opportunity for I/O scheduling. After four launches with dynamic scheduling, launch times are further reduced by an average of 4.8% compared to Paralfetch with pre-scheduling only. We attribute this reduction to: 1) an application launch on a Raspberry Pi 3 board is a diskbound process, and 2) the throughput of a MicroSD is usually improved by merging I/O operations: for example, the bandwidth of random reads of 128KB on the MicroSD we used is 28.6 MB/sec, which is 6.7? higher than that of 4KB (only 4.3 MB/sec). Chromium Browser and Xpdf application launch times are more heavily influenced by disk performance than by CPU performance. Due to the significant limitations of timely prefetching with prefetch scheduling, it is difficult to achieve warm start launch performance, especially for SSDs without command queuing. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Google Pixel (Android)</head><p>Paralfetch can be easily ported to Linux variants, such as Android. Android has its own launch mechanism, and hence we needed to modify 180 lines of the Android source code to accommodate Paralfetch.</p><p>To test Paralfetch on Android, we used a new set of seven games: Asphalt 8, Devil May Cry, Dragon Quest 8, FIFA 16 UT, GTA SA, The War of Mine, and Truck Pro. We measured the launch times for these games on a Google Pixel XL smartphone with UFS flash (which supports CQ) running Android 8.0 (Oreo) with the Linux kernel 3.18.52. As shown in Figure <ref type="figure" target="#fig_11">14</ref>, the pre-scheduling performed by Paralfetch reduced launch times by 11% on average, which equates to as much as 3.5 seconds for Dragon Quest 8. However, dynamic scheduling offers little benefit because 1) application launches are CPU-bound (86% on average in our benchmarks) rather than disk-bound, and 2) launches encounter little dependencies between metadata and data blocks. Another distinct characteristic of an Android app launch is that a number of write and fdatasync syscalls are issued by SQLite during the launch, making a gap between the times for a warm start and a cold start with Paralfetch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Overhead</head><p>We measure Paralfetch's overheads on a laptop PC from 4 aspects: tracing, pre-scheduling, prefetching and storage. Tracing overhead. The I/O-based tracing used by Paralfetch has a low instrumentation overhead, and in most cases log entries are relatively short (e.g., less than 3000 entries). Android Studio is an exception, as it creates lots of log entries. Nevertheless, the difference in cold start launch time with and without Paralfetch was only 136ms. Disk cache invalidation can produce some latency, but this does not affect the working set of pages. Thus, it should not affect the users. In any case, the cache is only invalidated during the learning phase. Pre-scheduling overhead. In our experiments, the time required by the background jobs which perform pre-scheduling, including missing metadata detection, metadata shift, and range merge, varied between 42ms for VLC Player and 153ms for Android Studio, whereas FAST took 21 seconds to generate the prefetch program for Android Studio. When there is an idle CPU core, pre-scheduling delays can be hidden from users because Paralfetch creates a dedicated thread for that.</p><p>Prefetching overhead. Paralfetch employs threaded prefetching, imposing extra overhead from management perspective. However, we observed that threaded prefetching can reduce CPU usage for an application launch in the cold start. As shown in Figure <ref type="figure" target="#fig_1">2</ref>, a synchronous I/O incurs two context switches. On the other hand, the asynchronous I/O requests issued by the prefetch thread significantly reduce the overall number of context switches. In our sampling-based CPU utilization measurement <ref type="bibr" target="#b21">[22]</ref>, we found that the number of context switches during a launch of Android Studio with Paralfetch was reduced from 9,902 to 1,035, resulting in a 3.2% reduction in CPU usage.</p><p>In the warm start where prefetching is unnecessary, Paralfetch still runs the prefetch thread, but this only incurs a delay of hundreds of microseconds if an available CPU core exists. Even if there was no available CPU core, where prefetching overhead could not be hidden, Paralfetch extended Android Studio launch by only 2.8ms for (Eclipse by 3.1ms, which was the worst case). Storage overhead. Paralfetch used 672 KB of SSD to store the &lt;app_name&gt;.pf files for the 16 applications, whereas FAST required 8.2 MB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Future research direction</head><p>Non-intrusive tracing. Paralfetch instruments some kernel functions to trace disk accesses. The (low) instrumentation overhead can be effectively removed by employing dynamic instrumentation tools, such as SystemTap <ref type="bibr" target="#b55">[55]</ref> and eBPF <ref type="bibr">[56]</ref>. Sophisticated prefetch scheduling. Paralfetch applies metadata shifting and range merging to the entire launch sequence, leaving room for further improvement: by applying prefetch scheduling only to prefetch-bottlenecked regions of the launch sequence, Paralfetch can avoid unnecessary I/O contention between the prefetch thread and the launching application, achieving a better launch performance. Prefetch scheduling considering internal behaviors of disks. If Paralfetch schedules prefetch entries considering internal behaviors and performance of storage devices, it can schedule them better at the pre-scheduling stage, thus reducing the need for rescheduling with dynamic scheduling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Additional Related Work</head><p>Previous application prefetchers are discussed in ?2. We now summarize various other approaches to reducing application launch times, which are orthogonal or complementary to Paralfetch. Predictive disk prefetchers, such as Preload <ref type="bibr" target="#b13">[14]</ref> and Windows Superfetch <ref type="bibr" target="#b36">[37]</ref>, analyze the pattern and frequency of application usage, predict the applications that are likely to be loaded soon, and then preload them. Falcon <ref type="bibr" target="#b41">[42]</ref> is a predictive prefetcher that considers mobile context such as location and battery state. Falcon launches an application in advance rather than merely prefetching launch-related blocks. Obviously, the merit of this strategy depends heavily on the accuracy of the prefetcher's predictions <ref type="bibr" target="#b33">[34]</ref>.</p><p>General-purpose disk prefetcher. It has been demonstrated that general-purpose prefetching <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b27">28]</ref> can also be beneficial in reducing application launch times. However, it can limit the accuracy of tracing launch-related blocks because block-level I/O patterns depend greatly on the contents of disk caches.</p><p>A block I/O cache provides another way of reducing latency. Intel Turbo Memory <ref type="bibr" target="#b30">[31]</ref>, Intel Smart Response Technology <ref type="bibr" target="#b50">[51]</ref>, and AMD StoreMI <ref type="bibr" target="#b51">[52]</ref> store delay-sensitive data in a relatively fast SSD and other data in a larger region of slower storage. A similar behavior is provided by software caching methods, which operate in the device mapping layer <ref type="bibr" target="#b0">[1]</ref> and the block layer <ref type="bibr" target="#b1">[2]</ref>. I/O scheduling can reduce I/O contention between a launch process and background processes. Several schemes have been proposed: FastTrack <ref type="bibr" target="#b15">[16]</ref> prioritizes I/O requests generated by the foreground application, and the BFQ I/O scheduler <ref type="bibr" target="#b9">[10]</ref> gives new processes extra I/O bandwidth. Boosting the priority of an I/O request, which is issued asynchronously but results in blocking the issuing process, can also expedite a launch <ref type="bibr" target="#b20">[21]</ref>.</p><p>Memory management can also reduce latency. Re-assigning pages from background apps to foreground apps can improve user experience of mobile operating systems <ref type="bibr" target="#b43">[44]</ref>. Similarly, pre-swapping of unused memory can reduce delays by avoiding page reclamation latencies <ref type="bibr" target="#b44">[45]</ref>. These schemes can reduce app launch times by timely provision of memory when it is under pressure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We have presented Paralfetch, which achieves launch performance close to the warm start through more accurate tracing, pre-scheduling for fast I/O reads, and prefetch thread overlapping. Paralfetch incurs negligible overhead in terms of CPU, memory, and storage. We have also shown Paralfetch to significantly outperform existing prefetchers on various personal computing/communication devices running Linux.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: I/O Stack in Linux. Linux includes three disk caches:page cache for regular files, slab (or slub) cache for metadata objects, and buffer cache for metadata blocks. The slab is used as an object-granular metadata cache for buffer cache. read system call explicitly fills page cache based on its arguments, while page cache for mmaped files is populated through page fault mechanism. Readahead framework is responsible for filling the contents of page cache, and it determines how many blocks to be prefetched based on the access sequentiality. Note that metadata blocks can be prefetched by EXT4 file system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: SSD cold start scenarios with and without Paralfetch.S i is the i th block requested from the SSD, and C i is the corresponding CPU computation. Paralfetch expedites an application launch by exploiting parallelism of each resource (i.e., multicore activation and internal parallelism on SSDs) and utilizing these resources concurrently.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>3 MetadataFigure 3 :</head><label>33</label><figDesc>Figure 3: Range merge. Merging nearby I/O operations into a single large operation improves throughput while keeping changes to the I/O order within a predefined limit so that the target application and prefetch thread can run concurrently. Range merge combines LBA-contiguous I/O requests of the same type (e.g., metadata or data block) into the preceding one.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Figure 4: Normalized prefetching times with varying I/O distance thresholds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Normalized prefetching times for different metadata shift sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>3 Figure 7 :Figure 8 :</head><label>378</label><figDesc>Figure 7: Normalized launch times with varying I/O distance thresholds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>Figure9: Paralfetch workflow. Boxes with dotted edges denote threads, and boxes with solid edges identify the three major components of Paralfetch. During a learning phase, Paralfetch records an I/O log as a form of log entry. Upon the completion of the launch, collected log entries are passed to missing metadata detector, generating additional log entries for missing metadata. Then, the log entries are passed to pre-scheduling functions as a form of red-black tree. The details of pre-scheduling are described in Algorithm 1 and 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Algorithm 1 : 5 move_to_MS_queue(log) 6 out_meta_sizeAlgorithm 2 : 8 unlink_log_entry_from_rbtrees_lba_and_seq</head><label>15628</label><figDesc>Metadata Shift Procedure Input: Log entries sorted by their access order (rbtree_seq), Metadata shift size (ms_size) Result: Metadata-shifted log entries (accessed via rbtree_seq) 1 log ? first_log_entry(rbtree_seq) 2 out_meta_size ? 0 3 while log = NULL do 4 if is_metadata_log_entry(log) then ? out_meta_size + log.size /* expired entries (log.expire &lt;= out_meta_size) in wait queue are moved to MS queue *Range Merge Procedure Input: Log entries sorted by their LBA (rbtree_lba) and access order (rbtree_seq), IO distance threshold (dist_thr) Result: Range-merged log entries (accessed via rbtree_seq) 1 curr ? first_log_entry(rbtree_lba) 2 next ? next_log_entry_lba(curr) 3 while next = NULL do 4 if curr.inode_num = next.inode_num &amp; 5 curr.start_lba + curr.size = next.start_lba &amp; 6 next.seq_numcurr.seq_num &lt;= dist_thr then 7 curr.size ? curr.size + next.size</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Comparison of Paralfetch and FAST launch times on a laptop PC, normalized to cold start times. Tracing of each application is performed when LibreOffice Writer is running in the background. The results show that running applications can significantly degrade tracing accuracy of FAST and its performance benefit.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Average launch time for 16 apps on a laptop equipped with a QLC SSD, normalized to cold start times.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Launch times on a Raspberry Pi 3, normalized to cold start times. Optimizations for Paralfetch are incrementally applied.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Launch times on an Android smartphone (Google Pixel XL), normalized to cold start times.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-1.png" coords="1,-9.00,-10.01,630.00,255.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-2.png" coords="1,-9.00,543.00,630.00,259.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Metadata and data block requests required to launch applications with missing metadata blocks. Note that 'regular' files include mmaped files, and that files mmaped by running applications are not subject to disk cache invalidation. The last column shows the number of I/O operations that were not captured by Paralfetch, which varies from run to run.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Read requests traced by Paralfetch</cell><cell>Number of missing</cell><cell cols="2">Number of accessed files</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Application</cell><cell>Metadata accesses</cell><cell>File data accesses</cell><cell>metadata blocks</cell><cell>regular</cell><cell>mmaped</cell><cell>Number of</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(total size in KB)</cell><cell>(total size in KB)</cell><cell>detected</cell><cell>files</cell><cell>files</cell><cell>missing I/Os</cell></row><row><cell></cell><cell></cell><cell>Android Studio</cell><cell>1,330 (6,844)</cell><cell>3,845 (197,932)</cell><cell>58</cell><cell>954</cell><cell>10</cell><cell>38</cell></row><row><cell></cell><cell></cell><cell>Chromium Browser</cell><cell>612 (3,048)</cell><cell>1,135 (130,728)</cell><cell>37</cell><cell>629</cell><cell>108</cell><cell>34</cell></row><row><cell>Ubuntu Linux</cell><cell>(Laptop PC)</cell><cell>Eclipse GIMP LibreOffice Impress LibreOffice Writer Okular Scribus</cell><cell>565 (3,348) 489 (2,620) 590 (2,900) 552 (2,800) 1,093 (5,720) 840 (5,984)</cell><cell>1,669 (67,256) 1,026 (38,512) 706 (83,004) 729 (83,824) 426 (23,640) 1,560 (141,056)</cell><cell>28 20 37 25 41 35</cell><cell>744 975 438 476 349 1,230</cell><cell>328 474 232 227 238 682</cell><cell>49 28 32 33 36 21</cell></row><row><cell></cell><cell></cell><cell>VLC Player</cell><cell>682 (5,420)</cell><cell>444 (20,192)</cell><cell>41</cell><cell>375</cell><cell>104</cell><cell>32</cell></row><row><cell></cell><cell></cell><cell>Xilinx ISE</cell><cell>573 (3,024)</cell><cell>1,028 (176,504)</cell><cell>42</cell><cell>657</cell><cell>273</cell><cell>33</cell></row><row><cell>Raspbian OS</cell><cell>(Raspberry Pi 3)</cell><cell>Chromium Browser Frozen Bubble GIMP LibreOffice Writer Scratch 2 Xpdf 0 A.D.</cell><cell>496 (1,984) 605 (2,420) 618 (2,472) 596 (2,384) 332 (1,328) 127 (508) 206 (509)</cell><cell>2,017 (138,600) 3,769 (32,992) 1,863 (46,664) 911 (35,164) 839 (48,580) 169 (7,236) 669 (86,272)</cell><cell>40 25 38 33 40 15 19</cell><cell>473 3,425 991 395 294 75 162</cell><cell>68 26 296 154 73 21 139</cell><cell>41 12 47 36 19 11 21</cell></row><row><cell>Android 8.0 (Google</cell><cell>Pixel XL)</cell><cell>Asphalt 8 Dragon Quest 8 FIFA 16 UT GTA SA Truck Pro Devil May Cry The War of Mine</cell><cell>131 (988) 95 (852) 76 (772) 104 (560) 96 (792) 237 (1,728) 127 (696)</cell><cell>838 (217,240) 4,339 (333,812) 805 (166,120) 377 (82,928) 1,792 (115,732) 1,904 (316,004) 517 (128,300)</cell><cell>49 46 39 41 41 45 43</cell><cell>179 335 265 95 175 407 101</cell><cell>N/A N/A N/A N/A N/A N/A N/A</cell><cell>11 12 47 36 19 19 11</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Execution: Overlapping Application Execution with Disk Prefetching Timely prefetching can better overlap application execution with prefetching. Reordering or merging blocks far apart could improve prefetch throughtput but could also hinder timely prefetching. Experimental results in Figures7 and 8substantiate the claim by showing prefetching throughput does not always correspond to launch performance. Paralfetch avoids this pitfall by tailoring metadata shift and range merge dynamically. A challenge is how to find near-optimal threshold values in an automatic manner. To address this, Paralfetch employs dynamic scheduling which reschedules prefetch entries with an increased I/O distance threshold and/or metadata shift size when a prefetching bottleneck is detected.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Default configuration for prefetch optimization.</figDesc><table><row><cell></cell><cell>SSD without CQ feature</cell><cell>SSD with CQ feature</cell></row><row><cell>I/O distance threshold for range merging</cell><cell>Starts at 8 and can be increased</cell><cell>8</cell></row><row><cell>Metadata shift size (KB) for metadta shifting</cell><cell>4</cell><cell>Starts at 64 and can be increased</cell></row><row><cell cols="3">is not effective. Next, we describe how to control the extent</cell></row><row><cell cols="3">of dynamic scheduling and how to measure the effectiveness</cell></row><row><cell>of prefetching.</cell><cell></cell><cell></cell></row><row><cell cols="3">Optimizing prefetch entries with dynamic scheduling. Ini-</cell></row><row><cell cols="3">tially, Paralfetch uses default thresholds for metadata shift-</cell></row><row><cell cols="3">ing and range merge shown in Table 2. It subsequently in-</cell></row><row><cell cols="3">creases the threshold for only one of these methods, depend-</cell></row><row><cell cols="3">ing on the availability of CQ support. The metadata shifting</cell></row><row><cell cols="3">threshold is increased in increments of 16KB and the I/O</cell></row><row><cell cols="2">distance threshold in increments of 4.</cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We would like to thank our shepherd <rs type="person">Nitin Agrawal</rs> and the anonymous reviewers for their valuable feedback and suggestions. This work was partly supported by the <rs type="funder">IITP</rs> grant (No. <rs type="grantNumber">RS-2022-00155885</rs>, <rs type="projectName">Artificial Intelligence Convergence Innovation Human Resources Development (Hanyang University ERICA</rs>)) and the <rs type="funder">National Research Foundation (NRF) of Korea</rs> grant funded by the <rs type="funder">Korean government (MSIT)</rs> (No. <rs type="grantNumber">NRF-2022R1F1A1074505</rs>). <rs type="person">K. Kang</rs> and <rs type="person">D. Lee</rs> are the corresponding authors.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_yG52v2z">
					<idno type="grant-number">RS-2022-00155885</idno>
					<orgName type="project" subtype="full">Artificial Intelligence Convergence Innovation Human Resources Development (Hanyang University ERICA</orgName>
				</org>
				<org type="funding" xml:id="_EFqYt9U">
					<idno type="grant-number">NRF-2022R1F1A1074505</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On-demand flash cache management for cloud computing</title>
		<author>
			<persName><forename type="first">D</forename><surname>Arteaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cabrera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sundararaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><surname>Cloudcache</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th USENIX Conference on File and Storage Technologies (FAST)</title>
		<meeting>the 14th USENIX Conference on File and Storage Technologies (FAST)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="355" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Improving virtualized storage performance with Sky</title>
		<author>
			<persName><forename type="first">L</forename><surname>Arulraj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Arpaci-Dusseau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Arpaci-Dusseau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th ACM SIG-PLAN/SIGOPS International Conference on Virtual Execution Environments (VEE)</title>
		<meeting>the 13th ACM SIG-PLAN/SIGOPS International Conference on Virtual Execution Environments (VEE)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="112" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Explicit block device plugging</title>
		<author>
			<persName><forename type="first">J</forename><surname>Axboe</surname></persName>
		</author>
		<ptr target="https://lwn.net/Articles/438256/" />
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Analysis on heterogeneous SSD configuration with quadruple-level cell (QLC) NAND flash memory</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Takai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fukuchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kinoshita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Matsui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Takeuchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th IEEE International Memory Workshop (IMW)</title>
		<meeting>the 11th IEEE International Memory Workshop (IMW)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="169" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">BORG: Block-reORGanization for self-optimizing storage systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bhadkamkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guerra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Useche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Burnett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lip-Tak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rangaswami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Hristidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th USENIX Conference on File and Storage Technologies (FAST)</title>
		<meeting>the 7th USENIX Conference on File and Storage Technologies (FAST)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="183" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A study of integrated prefetching and caching strategies</title>
		<author>
			<persName><forename type="first">P</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">W</forename><surname>Felten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Karlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1995 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems (SIGMETRICS)</title>
		<meeting>the 1995 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems (SIGMETRICS)</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="188" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automatic I/O hint generation through speculative execution</title>
		<author>
			<persName><forename type="first">F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Gibson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Symposium on Operating Systems Design and Implementation (OSDI)</title>
		<meeting>the 3rd Symposium on Operating Systems Design and Implementation (OSDI)</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Essential roles of exploiting internal parallelism of flash memory based solid state drives in high-speed data processing</title>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
		<meeting>the 17th IEEE International Symposium on High Performance Computer Architecture (HPCA)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="266" to="277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Analyzing and improving GNOME startup time</title>
		<author>
			<persName><forename type="first">L</forename><surname>Colitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th System Administration and Network Engineering Conference (SANE)</title>
		<meeting>the 5th System Administration and Network Engineering Conference (SANE)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The BFQ I/O scheduler</title>
		<author>
			<persName><forename type="first">J</forename><surname>Corbet</surname></persName>
		</author>
		<ptr target="https://lwn.net/Articles/601799/" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Diskseen: Exploiting disk layout and access history to enhance I/O prefetch</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 USENIX Annual Technical Conference (ATC)</title>
		<meeting>the 2007 USENIX Annual Technical Conference (ATC)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="261" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The performance of PC solidstate disks (SSDs) as a function of bandwidth, concurrency, device architecture, and system organization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dirik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jacob</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th Annual International Symposium on Computer Architecture (ISCA)</title>
		<meeting>the 36th Annual International Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="279" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Reducing DRAM footprint with NVM in Facebook</title>
		<author>
			<persName><forename type="first">A</forename><surname>Eisenman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Abdelrahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Axboe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hazelwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cidon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Katti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th European Conference on Computer Systems (EuroSys)</title>
		<meeting>the 13th European Conference on Computer Systems (EuroSys)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Preload-An adaptive prefetching daemon</title>
		<author>
			<persName><forename type="first">B</forename><surname>Esfahbod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Canada</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
		<respStmt>
			<orgName>Graduate Department of Computer Science, University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On the design of a new Linux readahead framework</title>
		<author>
			<persName><forename type="first">W</forename><surname>Fengguang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hongsheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chenfeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGOPS Operating Systems Review</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="75" to="84" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">FastTrack: Foreground app-aware I/O management for improving user experience of Android smartphones</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Yee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ryu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 USENIX Annual Technical Conference (ATC)</title>
		<meeting>the 2018 USENIX Annual Technical Conference (ATC)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="15" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A file is not a file: Understanding the I/O behavior of Apple desktop applications</title>
		<author>
			<persName><forename type="first">T</forename><surname>Harter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dragga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vaughn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Arpaci-Dusseau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Arpaci-Dusseau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM Symposium on Operating Systems Principles (SOSP)</title>
		<meeting>the 23rd ACM Symposium on Operating Systems Principles (SOSP)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="71" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Informed mobile prefetching</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Flinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Giuli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Noble</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Peplin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Watson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on Mobile Systems, Applications, and Services (MOBISYS)</title>
		<meeting>the 10th International Conference on Mobile Systems, Applications, and Services (MOBISYS)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="155" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The automatic improvement of locality in storage systems</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computer Systems</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="424" to="473" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On faster application startup times: Cache stuffing, seek profiling, adaptive preloading</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hubert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ottawa Linux Symposium (OLS)</title>
		<meeting>the Ottawa Linux Symposium (OLS)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="245" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Boosting quasiasynchronous I/O for better responsiveness in mobile devices</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 13th USENIX Conference on File and Storage Technologies (FAST) pages</title>
		<meeting>The 13th USENIX Conference on File and Storage Technologies (FAST) pages</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="191" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Improving application launch times with hybrid disks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th IEEE/ACM International Conference on Hardware/Software Codesign and System Synthesis (CODES+ISSS) pages</title>
		<meeting>the 7th IEEE/ACM International Conference on Hardware/Software Codesign and System Synthesis (CODES+ISSS) pages</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="373" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Rapid prototyping and evaluation of intelligence functions of active storage devices</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ryu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2356" to="2368" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">FAST: Quick application launch on solid-state drives</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ryu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th USENIX Conference on File and Storage Technologies (FAST)</title>
		<meeting>the 9th USENIX Conference on File and Storage Technologies (FAST)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="259" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Improving application launch performance on SSDs</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ryu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer Science and Technology</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="727" to="743" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Revisiting storage for smartphones</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ungureanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th USENIX Conference on File and Storage Technologies (FAST)</title>
		<meeting>the 10th USENIX Conference on File and Storage Technologies (FAST)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="209" to="222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Disk schedulers for solid state drivers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Noh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th ACM/IEEE International Conference on Embedded software (EMSOFT)</title>
		<meeting>the 9th ACM/IEEE International Conference on Embedded software (EMSOFT)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="295" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">C-miner: Mining block correlations in storage systems</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd USENIX Conference on File and Storage Technologies (FAST) pages</title>
		<meeting>the 3rd USENIX Conference on File and Storage Technologies (FAST) pages</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="173" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Prefetch: Linux solution for prefetching necessary data during application and system startup</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lichota</surname></persName>
		</author>
		<ptr target="http://code.google.com/p/prefetch/" />
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Don&apos;t get caught in the cold, warm-up your JVM: Understand and eliminate JVM warm-up overhead in data-parallel systems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Grcevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI)</title>
		<meeting>the 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="383" to="400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Intel?Turbo Memory: Nonvolatile disk caches in the storage hierarchy of mainstream computer systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Trika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hensgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Coulson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grimsrud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Storage</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Improving smartphone responsiveness through I/O optimizations</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct Publication (UBI-COMP Adjunct)</title>
		<meeting>the 2014 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct Publication (UBI-COMP Adjunct)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="337" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Linux 2.6 performance improvement through readahead optimization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Pai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pulavarty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ottawa Linux Symposium (OLS)</title>
		<meeting>the Ottawa Linux Symposium (OLS)</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="105" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Practical prediction and prefetch for faster access to applications on mobile phones</title>
		<author>
			<persName><forename type="first">A</forename><surname>Parate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>B?hmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ganesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Marlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 ACM International Joint Conference on Pervasive and Ubiquitous Computing (UBICOMP)</title>
		<meeting>the 2013 ACM International Joint Conference on Pervasive and Ubiquitous Computing (UBICOMP)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="275" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">HMB-SSD: Framework for efficient exploiting of the host memory buffer in the NVMe SSD</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="150403" to="150411" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Informed prefetching and caching</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ginting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stodolsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zelenka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM Symposium on Operating Systems Principles (SOSP)</title>
		<meeting>the 15th ACM Symposium on Operating Systems Principles (SOSP)</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="79" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Windows Internals, Part 2, 6th ed</title>
		<author>
			<persName><forename type="first">M</forename><surname>Russinovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Solomon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lonescu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Microsoft Press</publisher>
			<biblScope unit="page" from="324" to="350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Mauerer</surname></persName>
		</author>
		<title level="m">Professional Linux Kernel Architecture</title>
		<imprint>
			<publisher>Wrox Press</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="970" to="974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Exploiting SSD parallelism to accelerate application launch on SSDs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ryu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IET Electronics Letters</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="313" to="315" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Reducing seek overhead with application-directed prefetching</title>
		<author>
			<persName><forename type="first">S</forename><surname>Vandebogart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Frost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kohler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 USENIX Annual Technical Conference (ATC)</title>
		<meeting>the 2009 USENIX Annual Technical Conference (ATC)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="299" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Barrier-enabled IO stack for flash storage</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Won</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th USENIX Conference on File and Storage Technologies (FAST)</title>
		<meeting>the 16th USENIX Conference on File and Storage Technologies (FAST)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="211" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Fastapp launching for mobile devices using predictive user context</title>
		<author>
			<persName><forename type="first">T</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ganesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on Mobile Systems, Applications, and Services (MOBISYS)</title>
		<meeting>the 10th International Conference on Mobile Systems, Applications, and Services (MOBISYS)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="113" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Improving file system performance of mobile storage systems using a decoupled defragmenter</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Yee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 USENIX Annual Technical Conference (ATC)</title>
		<meeting>the 2017 USENIX Annual Technical Conference (ATC)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="759" to="771" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">End the senseless killing: Improving memory management for mobile operating systems</title>
		<author>
			<persName><forename type="first">N</forename><surname>Lebeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 USENIX Annual Technical Conference (ATC)</title>
		<meeting>the 2020 USENIX Annual Technical Conference (ATC)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="873" to="887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Acclaim: Adaptive memory reclaim to improve user experience in Android systems</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ausavarungnirun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-W</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 USENIX Annual Technical Conference (ATC)</title>
		<meeting>the 2020 USENIX Annual Technical Conference (ATC)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="897" to="910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">ADATA Ultimate SU630 960GB</title>
		<author>
			<persName><surname>Bodnara</surname></persName>
		</author>
		<ptr target="https://www.bodnara.co.kr/bbs/article.html?num=154114" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Schiesser</surname></persName>
		</author>
		<ptr target="https://www.techspot.com/review/2116-storage-speed-game-loading" />
		<title level="m">Storage Game Loading Test: PCIe 4.0 SSD vs. PCIe 3.0 vs. SATA vs. HDD</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Samsung&apos;s 860 QVO 1-TB SSD re</title>
		<author>
			<persName><forename type="first">T</forename><surname>Thomas</surname></persName>
		</author>
		<ptr target="https://techreport.com/review/34281/samsungs-860-qvo-1-tb-ssd-reviewed" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">mm: map few pages around fault address if they are in page cache</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Shutemov</surname></persName>
		</author>
		<ptr target="https://lwn.net/Articles/588802" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">The size of Iphone&apos;s top apps has increased by 1,000% in four years</title>
		<author>
			<persName><forename type="first">R</forename><surname>Nelson</surname></persName>
		</author>
		<ptr target="https://sensortower.com/blog/ios-app-size-growth" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<ptr target="https://www.intel.com/content/www/us/en/architecture-and-technology/smart-response-technology-brief.html" />
		<title level="m">Intel? Smart Response Technology: Technology Brief</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Technology</forename><surname>Amd Storemi</surname></persName>
		</author>
		<ptr target="https://www.amd.com/en/technologies/store-mi" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Sivaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bergey</surname></persName>
		</author>
		<ptr target="https://www.flashmemorysummit.com/Proceedings2019/08-06-Tuesday/20190806_Keynote2_WesternDigital_Sivaram_" />
		<title level="m">Zoned storage for the zettabyte age</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title/>
		<author>
			<persName><surname>Bergey</surname></persName>
		</author>
		<author>
			<persName><surname>Pdf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Spending Moore&apos;s dividend. Communications of the</title>
		<author>
			<persName><forename type="first">James</forename><surname>Larus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title/>
		<author>
			<persName><surname>Systemtap</surname></persName>
		</author>
		<ptr target="https://sourceware.org/systemtap" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
