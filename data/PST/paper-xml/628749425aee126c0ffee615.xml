<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Divide and Denoise: Learning from Noisy Labels in Fine-Grained Entity Typing with Cluster-Wise Loss Correction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kunyuan</forename><surname>Pang</surname></persName>
							<email>pangkunyuan10@nudt.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer</orgName>
								<orgName type="institution">National University of Defense Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haoyu</forename><surname>Zhang</surname></persName>
							<email>zhanghaoyu10@nudt.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Artificial Intelligence Research Center</orgName>
								<orgName type="department" key="dep2">Defense Innovation Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ting</forename><surname>Wang</surname></persName>
							<email>tingwang@nudt.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer</orgName>
								<orgName type="institution">National University of Defense Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Divide and Denoise: Learning from Noisy Labels in Fine-Grained Entity Typing with Cluster-Wise Loss Correction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fine-grained Entity Typing (FET) has made great progress based on distant supervision but still suffers from label noise. Existing FET noise learning methods rely on prediction distributions in an instance-independent manner, which causes the problem of confirmation bias. In this work, we propose a clustering-based loss correction framework named Feature Cluster Loss Correction (FCLC), to address these two problems. FCLC first train a coarse backbone model as a feature extractor and noise estimator. Loss correction is then applied to each feature cluster, learning directly from the noisy labels. Experimental results on three public datasets show that FCLC achieves the best performance over existing competitive systems. Auxiliary experiments further demonstrate that FCLC is stable to hyperparameters and it does help mitigate confirmation bias. We also find that in the extreme case of no clean data, the FCLC framework still achieves competitive performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Fine-grained entity typing (FET) is the task of classifying named entity mentions in a sentence over the given class set (typically a hierarchical class structure as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. FET serves as an important component in many down-stream NLP applications, e.g., relation extraction <ref type="bibr" target="#b20">(Liu et al., 2014)</ref>, entity linking <ref type="bibr" target="#b28">(Raiman and Raiman, 2018)</ref> and question answering <ref type="bibr" target="#b8">(Dong et al., 2015)</ref>. FET task has a more wide range of entity types (usually over 100 classes) compared to entity typing, and hence neural-based FET systems require largescale annotated training corpus.</p><p>Recent studies apply distant supervision to label the corpora automatically by linking mentions to knowledge base entities and using all entity types as the ground-truth labels. Although large-scale annotated data is provided, it brings about label noises in training. To overcome the problem of noisy label, some works directly pruned noisy instances <ref type="bibr" target="#b10">(Gillick et al., 2014;</ref><ref type="bibr" target="#b23">Onoe and Durrett, 2019a)</ref>. The others retain noisy training data but further improve by choosing <ref type="bibr" target="#b30">(Ren et al., 2016a;</ref><ref type="bibr" target="#b39">Xu and Barbosa, 2018)</ref>, weighting <ref type="bibr" target="#b37">(Wu et al., 2019)</ref>, and relabeling <ref type="bibr" target="#b41">(Zhang et al., 2020)</ref> noisy labels using the prediction distribution.</p><p>However, these noise combating methods have two major limitations. 1) They rely on the prediction distribution. As a result, they ought to cope with instance-agnostic noise better. The previous works expirically show <ref type="bibr" target="#b42">(Zheng and Yang, 2021)</ref> that the prediction distribution is more likely to be affected by noisy instances and suffer from confirmation bias. This bias problem is also verified in our Sec. 3.5. The limitation leads to the intriguing question: Besides prediction distribution and entropy, what other information can we use to model label noise?</p><p>2) They mostly aim to modify each instance isolatedly and only use instance-level information. Meanwhile, typical anti-noise machine learning <ref type="bibr" target="#b26">(Patrini et al., 2017;</ref><ref type="bibr" target="#b12">Hendrycks et al., 2018)</ref> uses instance-agnostic global statistics. The latter is more robust to noise but might be too general. Local information is potentially more informative. For example, when the distant supervision introduces similar noise in some instances, these noises form a locality in feature space. The noisy instances are near to each other and are separate from instances with the same but true labels. Our experiment result is similar to Fig. <ref type="figure" target="#fig_0">1</ref>, even when the feature extractor is trained to fit noisy labels, they are still easily separable due to underlying semantic differences.</p><p>These two limitations are inter-related, causing noise-learning-based FET methods to still suffer from distantly supervised noise. To alleviate the label noise and avert these limitations, we propose a novel framework FCLC for noisy label learning inspired by weighted training and loss correction <ref type="bibr" target="#b12">(Hendrycks et al., 2018)</ref> in machine learning. Our method utilizes feature representations from the model and learns global (local) information, i.e. a cluster-level label confusion matrix. Firstly, we use a backbone learner on noisy data. It serves as a feature extractor and a noise estimator. Secondly, all training data, including noisy data and a small portion of clean data are clustered. The clean data serve as anchors in the feature space to estimate label corruption and sample quality of each cluster. Finally, label corruption and sample quality are used for label correction.</p><p>Our main contributions are three-fold: (i) This study provides fresh insight into instance dependent label noise in FET. We pointed out a novel training method to further exploit feature space and global information. (ii) We designed a framework with feature clustering, estimating cluster-level confusion matrix, and loss correction. (iii) We experimented the proposed method on three datasets. Results show that we made significant improvements over previous state-of-the-art, thus proving the effectiveness of our model. Ablation studies further prove the robustness and wide applicability of our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Framework</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Definition</head><p>Given a finite set of types, T = {t 1 , t 2 , ..., t |T | }, where |T | denotes the number of candidate types. The task is to assign appropriate types to each mention under context. Formally, an instance is a triplet, (m, c, y). c = {w 1 , w 2 , ..., w n } is the context of m, usually the original sentence. m = {w p 1 , ..., w p l } is the mention. obviously, m is a continuous sub-sequence of c.</p><p>Y ⊆ T denotes appropriate types for (m, c). For convenience, denote Y 's vector form y ∈ {0, 1} |T | , y j = 1 means t j ∈ Y .</p><p>When the instance is produced with crowdsourcing or distant supervision, annotated labels might contain so-called noise. We denote labels with noise ỹ. The instance is thus (m, c, ỹ). Denote the corpus with noisy instances D, the corpus with trusted instances D t . * The two corpus form the whole training corpus D.</p><p>The task is to predict the appropriate types for given (m, c).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Training Procedure</head><p>As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, the FCLC framework consists of the following steps :</p><p>Step 1. (Phase 1) Train the backbone model with noisy data D for e 1 epochs and get M 1 . It serves as a feature extractor and a noise estimator. (Sec.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.3)</head><p>Step 2. Cluster all training samples D with the feature extracted by E 1 , and estimate confusion matrix for each cluster with predictions of M 1 . (Sec.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.4)</head><p>Step 3. (Phase 2) The calculated clusteringaware confusion matrix and FCLC loss are used to continue training the backbone model. (Sec. 2.5)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Backbone</head><p>For fair comparison, the backbone of our model has the same structure as NFETC <ref type="bibr" target="#b39">(Xu and Barbosa, 2018)</ref>.</p><p>For an instance (m, c, y), for each word w i in c, word embedding is e w i ∈ R dw looked up in word embedding matrix W ∈ R  <ref type="bibr" target="#b43">(Zhou et al., 2016)</ref> is applied on where ⊕ means element-wise sum and d c is the hidden size of the BiLSTM and the dimension of the context embedding.</p><formula xml:id="formula_0">h i = [ − → h i ⊕ ← − h i ], re- sulting in the final context representation r c ∈ R dc ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mention Representation</head><p>The average encoder of a mention takes word embeddings of the mention {e p 1 , e p 2 , ..., e p l } and takes the average: r w = 1 l l k=1 e p k . The LSTM encoder of a mention takes an extended mention with one more token before and after the original mention and produces hidden state features {h p 1 −1 , ..., h p l +1 }. Take the last output h p l +1 as r l . The final representation of the mention is r m = [r w , r l ] Classification Softmax classifier and crossentropy are used based on the feature r m,c = [r c , r m ] of x:</p><formula xml:id="formula_1">s(x) = Wr m,c + b (1) p(y|x) = softmax(s(x)) (2) ℓ(x, y; θ) = −log(p(y|x))<label>(3)</label></formula><p>With a given dataset D, the model is trained with all samples (x, y) in D. For baseline, D = D. For FCLC step 1, D = D:</p><formula xml:id="formula_2">L base (θ) = 1 |D| (x,y)∈D ℓ(x, y; θ) (4)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Feature Clustering</head><p>We make the assumption that the noise (y, ỹ)forms locality in the feature space, especially when the feature is calculated from the original mention and context(m, c), (m, c) determines y, and the feature is trained with ỹ.</p><p>We adopt clustering to utilize local statistics as smaller-grained feature information. To be specific, we perform k-means with r m,c on the whole training set D, and separate</p><formula xml:id="formula_3">D into K clusters. Denote the k-th cluster Ck , C t−k = Ck ∩ D t , Ck = Ck ∩ D.</formula><p>We mainly utilize the two following statistics:</p><formula xml:id="formula_4">τ k = |C t−k | |D k | (5)</formula><p>τ k estimates the quality of the cluster k. It acts as a soft cluster sieving.</p><formula xml:id="formula_5">C ijk = 1 |A ik | (x,y)∈A ik p(y j = 1|x)<label>(6)</label></formula><p>where</p><formula xml:id="formula_6">A ik = {(x, y)|(x, y) ∈ C t−k and y i = 1}</formula><p>, C ijk estimates the probability in cluster k to annotate noise j for true label i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Loss Correction</head><p>The idea of forward loss correction is proposed by <ref type="bibr" target="#b26">Patrini et al. (2017)</ref>. The basic idea is to modify the loss with the noise transition matrix T . Such that the minimizer under the new loss with noisy labels is the same as the minimizer of the original loss under clean labels. The modification relies on the assumption that the label noise is independent from instances, i.e. ỹ ⊥ x | y. <ref type="bibr" target="#b12">Hendrycks et al. (2018)</ref> proposed to estimate T with a small set of clean labels, under the assumption that ỹ ⊥ y | x. While these assumptions do not hold globally for distantly supervised FET, they hold better in clusters. We introduce the cluster-wise loss correction in the following sections.</p><p>Transition Matrix Estimation Assuming the backbone model is well trained, i.e. p(ỹ j = 1|x) is close enough to p(ỹ j = 1|x). We use the predicted probability on trusted instances in cluster-k to estimate the transition probability.</p><formula xml:id="formula_7">C ijk = p(ỹ j = 1 | y i = 1, x ∈ Ck ) ≈ p(ỹ j = 1 | y i = 1, x ∈ C t−k ) ≈ 1 |A ik | (x,y)∈A ik p(ỹ j = 1|x) = C ijk (7)</formula><p>Forward Loss Correction Cross-entropy is composite <ref type="bibr" target="#b29">(Reid and Williamson, 2010)</ref>,denote it as ℓ ψ , its inverse link function ψ −1 is softmax.</p><p>Notice C ijk can bridge the loss with noisy label ỹ, (x ∈ Ck , ỹi = 1), to predictions for the true label:</p><formula xml:id="formula_8">−log(p(ỹ|x)) ≈ − log c j=1 C jik p(y j = 1 | x) (8) Let T k = C * * k</formula><p>, define the forward loss as:</p><formula xml:id="formula_9">ℓ → ψ (s(x)) = ℓ ψ (T ⊤ k s(x))<label>(9)</label></formula><p>The property holds on each cluster similar as in <ref type="bibr" target="#b26">(Patrini et al., 2017)</ref>, with all x ∈ Ck , training with noisy label ỹ on ℓ → ψ is the same as with true label y on the original loss ℓ ψ :</p><formula xml:id="formula_10">argmin s E x,ỹ ℓ → ψ (s(x)) = argmin s E x,y ℓ ψ (s(x))<label>(10</label></formula><p>) Different from global forward loss correction, the parameters that minimize the loss in each cluster are not the same. We balance the clusters with τ k . The trusted samples (x, y) ∈ D t are also used. The loss of the full model is:</p><formula xml:id="formula_11">L FCLC = (x,y)∈Dt ℓ ψ (s(x)) +β K k=1 τ k (x,ỹ)∈ Ck ℓ → ψ (s(x))) +(1 − β) K k=1 τ k (x,ỹ)∈ Ck ℓ ψ (s(x))) (11)</formula><p>Where β is the hyperparameter to balance FCLC loss and the original loss.</p><p>Our introduced framework has several advantages: 1) Lightweight. This method does not include extra trainable parameters to the backbone model. 2) Stable. The framework involves two hyperparameters, β and phase-1 train epochs e 1 and we empirically find them stable. 3) Flexibility. Our improvement is orthogonal to the backbone model. It only requires that the backbone model is sufficiently expressive and uses an appropriate composite loss <ref type="bibr" target="#b29">(Reid and Williamson, 2010)</ref>. Thus, it is pluggable to a large number of FET models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We evaluate the proposed model on three different FET datasets and compare it to several state-ofthe-art models. In addition, to support our claims we also conduct several subsidiary experiments to analyze the impacts of our proposed module in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Wiki</head><p>OntoNotes </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>The datasets are described below, we use exactly the same train/dev/test split with previous works <ref type="bibr" target="#b30">(Ren et al., 2016a;</ref><ref type="bibr" target="#b2">Chen et al., 2019)</ref>. Detailed statistics of the three datasets are also shown in Table <ref type="table" target="#tab_1">1</ref>. BBN It contains sentences extracted from the Wall Street Journal and distantly labeled by DBpedia Spotlight <ref type="bibr" target="#b35">(Weischedel and Brunstein, 2005)</ref>. OntoNotes It was constructed using sentences in the OntoNotes corpus and distantly supervised by DBpedia Spotlight <ref type="bibr" target="#b36">(Weischedel et al., 2013)</ref>. Wiki/FIGER It was derived from Wikipedia articles and news reports, entities of the training samples are distantly annotated using Freebase <ref type="bibr" target="#b18">(Ling and Weld, 2012)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation Metrics</head><p>We follow prior work and use the strict accuracy (Acc), Macro F1 (Ma-F1), and Micro F1 (Mi-F1) scores. During the experiment, all these metrics are calculated by running the model five times and computing the mean and standard deviation values. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Baselines</head><p>We consider the following competitive FET systems as our baselines: (1) AFET <ref type="bibr">(Ren et</ref>   <ref type="formula" target="#formula_5">6</ref>) FCLC(with reinit): our proposed model with fresh parameters before the start of step 3 as suggested by <ref type="bibr" target="#b26">Patrini et al. (2017)</ref>. ( <ref type="formula" target="#formula_1">3</ref>)-( <ref type="formula" target="#formula_5">6</ref>) are implemented based on and should be compared with the best configuration between FCLC and FCLC hier on each dataset, that is, compared with FCLC on BBN and compared with FCLC hier on Wiki and OntoNotes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Implementation Details</head><p>To make an equal comparison, following <ref type="bibr" target="#b39">(Xu and Barbosa, 2018;</ref><ref type="bibr" target="#b2">Chen et al., 2019;</ref><ref type="bibr" target="#b41">Zhang et al., 2020)</ref>, we use exactly the same pre-trained 300dimensional GloVe word embeddings <ref type="bibr" target="#b27">(Pennington et al., 2014)</ref> and fix the embedding vectors during training. The model parameters are optimized using the Adam (Kingma and Ba, 2014) optimizer. All of our models are implemented in Tensorflow. † † The implementation of our model can be cound at https://github.com/Los-Phoenix/NFETC-FCLC.</p><p>As NFETC and NFETC hier are our backbone models, we follow the hyper-parameters of the backbone except for our introduced hyper-parameters β and e 1 . The detailed hyper-parameter settings on the three datasets are shown in Table <ref type="table" target="#tab_2">2</ref>, we also report hyper-parameter impact curves in Fig. <ref type="figure" target="#fig_2">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Results and Analysis</head><p>Main Result Table <ref type="table" target="#tab_4">3</ref> shows the results of our proposed approach (FCLC) and several competitive FET systems. We highlight the statistically significant best scores of each metric in bold. According to the experimental results, we make two main observations:</p><p>(1) The performances of our proposed model surpass the backbone NFETC model by a remarkable large margin (improving Micro F1 by 2.1%, 3.8%, and 7.8% separately), demonstrating the benefits of the proposed two-phase FCLC module. The relative performance improvements are consistent with or without the hierarchy loss (compared FCLC and FCLChier to the corresponding baselines).</p><p>(2) Compared to other noisy learning methods such as CLSC, NFETC-AR, and VAT, our model still achieves considerable improvements under most metrics when using the same backbone and very similar hyper-parameter settings. For example, compared to NFETC-AR, our model improves Micro-F1 by 1.25% to 6.38% on three datasets. It indicates that, by utilizing both the feature space representations and the global and local statistical information, the model can reduce the impact of noisy labels more effectively.</p><p>Ablation Study To study the detail of our models, we explore the performances of three main model variants, shown in the last several rows of Table <ref type="table" target="#tab_4">3</ref>. We find that the cluster quality τ k , the loss correction module and the feature cluster process are all critical to model performances in some situations. Specifically, as shown in FCLC (without cluster), feature clustering has minor impacts on Wiki and Ontonotes. This is probably because the noisy distribution on these two datasets is relatively simple and the global confusion matrix is sufficient. Moreover, we observe that the re-initialization before Step 3 has a great impact on all metrics. Staring Step 3 with a fresh re-initialized FET model degrades the accuracy by 3.2% on Ontonotes. It denotes that the learner trained in the first phase is beneficial for the noisy robust learning process, by providing optimal parameters initialization.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sensitivity of the introduced hyper-parameters</head><p>Using the same setting for model training, Fig. <ref type="figure" target="#fig_2">3</ref> analyses the sensitivity of FCLC to the introduced hyper-parameters: the FCLC objective weight β, the Step-1 training epochs e 1 . Fig. <ref type="figure" target="#fig_2">3(a, b</ref>) shows the performance trend on the Ontonotes and BBN datasets when changing β. While selecting a proper ratio between loss-correction loss and the original loss is important, the performance near optimum β is stable and steadily outperforms the baseline. Fig. <ref type="figure" target="#fig_2">3(c, d</ref>) analyses the sensitivity with respect to e 1 . the Micro-F1 improves as e 1 increases but stops improving and become unstable when e 1 is large enough, since the model starts to overfit noise. It is also reasonable that the optimal range of β and  Will cluster number affect performance? We investigate how much the FCLC model benefits from different values of feature cluster number k. Fig. <ref type="figure" target="#fig_4">5</ref> demonstrates that under a reasonable feature cluster range (near |T |), the model can achieve competitive and similar performances.</p><formula xml:id="formula_12">e</formula><p>How many trusted instances does the model need? We examine the robustness of the model to the amount of clean data by comparing the performances with 5% to 100% trusted instances. Refer to Fig. <ref type="figure" target="#fig_3">4</ref>, we observe that due to the differences of the training set, our model achieves comparable accuracy with 30%, 40%, and 70% D t samples on Wiki, Ontonotes, and BBN separately. With only a very small size of trusted instances, e.g. 20% BBN trusted set, or 128 samples, the model begins to improve significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>What if we did not have any trusted instances?</head><p>Although a small number of clean samples is always practical to obtain or relabel with an expert, we push the limit to no trusted instances at all. What performance can our model achieve in such a situation? We performed the "no clean training set" experiment to test the robustness of our model. In Table <ref type="table" target="#tab_6">4</ref>, FCLC (w/o D t ) indicates for the variant that the trusted instances are not used for phase 2 training but only in feature clustering and confusion matrix calculation. In that situation, our approach still has similar performances with previous SOTA models on most metrics ‡ . FCLC (w/ pl) variant means that, during the clustering process, instead of using the trusted instance set D t split from the training set, we introduce a simple and classic pseudo labeling method <ref type="bibr" target="#b15">(Lee et al., 2013)</ref> to generate the labels needed by clustering and training. We find that compared to the baseline method, FCLC with pseudo labeling still achieves much better performances.</p><p>It is proved by results in Visualization of the representations We analyze the role of FCLC module by visualizing the feature vectors. Fig. <ref type="figure" target="#fig_5">6</ref> illustrates samples in a cluster (circled in all 4 sub-figures). From Fig. <ref type="figure" target="#fig_5">6</ref>(a), we observe that the backbone model fails to distinguish some samples of class A (/ORGANIZATION/GOVERN-MENT, red) and class B (/GPE/COUNTRY, blue), due to noisy labels. <ref type="bibr">Fig. 6(b)</ref> shows that our model learns to correct these instances. With FCLC the classifier is corrected to predict the right label. Meanwhile, in feature space, the boundary between these samples and the confusing class is also clearer, which means FCLC also helps to refine feature extraction with loss correction. Fig. <ref type="figure" target="#fig_5">6(e)</ref> shows the row of '/GPE/COUNTRY'. Managing to notice the confusion from '/GPE/COUNTRY' to '/ORGANIZATION/GOVERNMENT' enables our model to perform the appropriate correction. Due to this, FCLC are resistant to the noisy labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quantitative Results of Confirmation Bias</head><p>To further verify our claim that our model can alleviate the confirmation bias in the noisy FET task, we analyze the prediction confidence on test set samples, as shown in Fig. <ref type="figure" target="#fig_6">7</ref>. The average confidence of correct and wrong test samples is calculated after each training epoch. The results show that, on the Wiki dataset, after phase one the wrong sample average confidence is 0.700 but the backbone model reached 0.833 at the end of the training (with early stopping). Also, after phase two FCLC improves the correct sample confidence from backbone's 0.939 to 0.950 on Wiki.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Noisy Learning</head><p>The usage of datasets collected with distant supervision often results in so-called noisy labels. Several studies have investigated deep learning approaches with noise. Existing noisy learning methods include designing robust loss functions <ref type="bibr" target="#b34">(Wang et al., 2019)</ref>, designing robust architectures by adding noise adaptation layers <ref type="bibr" target="#b4">(Chen and Gupta, 2015;</ref><ref type="bibr" target="#b11">Goldberger and Ben-Reuven, 2017)</ref>, selecting samples <ref type="bibr" target="#b24">(Onoe and Durrett, 2019b)</ref>, and adding noiserobust regularization <ref type="bibr" target="#b32">(Shi et al., 2020)</ref>. Among them, <ref type="bibr" target="#b26">Patrini et al. (2017)</ref> and <ref type="bibr" target="#b12">Hendrycks et al. (2018)</ref> proposed forward loss correction. It avoided explicit relabeling and matrix inversion. These noisy learning methods are mostly restricted to the  noise that is conditionally independent of the data features <ref type="bibr" target="#b9">(Frénay and Verleysen, 2014)</ref>. However, in real-world applications such as FET, noise distributions are more complex and instance-dependent, requiring more powerful noisy learning methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Fine-Grained Entity Typing</head><p>FET is studied based on the distant supervision training data <ref type="bibr" target="#b21">(Mintz et al., 2009;</ref><ref type="bibr" target="#b18">Ling and Weld, 2012)</ref>. Various features <ref type="bibr" target="#b40">(Yogatama et al., 2015;</ref><ref type="bibr" target="#b39">Xu and Barbosa, 2018)</ref>, network structures <ref type="bibr" target="#b8">(Dong et al., 2015;</ref><ref type="bibr" target="#b33">Shimaoka et al., 2016)</ref>, and feature space <ref type="bibr" target="#b1">(Ali et al., 2021;</ref><ref type="bibr" target="#b22">Onoe et al., 2021)</ref>are explored to refine the mention and type representation. Label inter-dependency <ref type="bibr" target="#b17">(Lin and Ji, 2019)</ref> and type hierarchy <ref type="bibr" target="#b3">(Chen et al., 2020)</ref> are often used, added by relations among instances and labels <ref type="bibr" target="#b0">(Ali et al., 2020;</ref><ref type="bibr" target="#b16">Li et al., 2021;</ref><ref type="bibr" target="#b19">Liu et al., 2021)</ref>. Label noise is the main problem brought by distance supervision. Besides common noisy learning methods discussed in Sec. 4.1 <ref type="bibr" target="#b24">(Onoe and Durrett, 2019b;</ref><ref type="bibr" target="#b32">Shi et al., 2020;</ref><ref type="bibr" target="#b37">Wu et al., 2019)</ref>, FET-specific noise combat methods are proposed. <ref type="bibr">Ren et al. (2016a,b)</ref> utilized partial-label embedding. <ref type="bibr" target="#b39">Xu and Barbosa (2018)</ref> modified hierarchical loss to cope with overly-specific noise. <ref type="bibr" target="#b41">Zhang et al. (2020)</ref> automatically generated pseudo-truth label distribution for each sample. Additional resource also help to improve the performance. The resource include external knowledge base <ref type="bibr" target="#b38">(Xin et al., 2018;</ref><ref type="bibr" target="#b6">Dai et al., 2019)</ref>, and with BERT-like pipeline <ref type="bibr" target="#b25">(Patel and Ferraro, 2020;</ref><ref type="bibr" target="#b7">Ding et al., 2021)</ref>. <ref type="bibr" target="#b5">Choi et al. (2018)</ref> proposed a way to utilize more distance supervision and crowd source, followed by <ref type="bibr" target="#b24">Onoe and Durrett (2019b)</ref>. Apart from the above, <ref type="bibr" target="#b2">(Chen et al., 2019) and</ref><ref type="bibr" target="#b0">(Ali et al., 2020)</ref> are the closest to our proposed method. They both select some instances by feature distance to modify labels or refine mention representation for noisy instances.</p><p>However, their refinement is still explicit and isolated to each instance. Thus the quality relies on the instances they retrieve for label propagation/mention reference. Different from these studies, we do not rely on any of these external resources and aim to impose label noise with only the original data without explicit sieving or label changing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, in order to tackle the instancedependent label noise in fine-grained entity typing tasks, we present a neural FET noisy learning 2004 framework that utilizes the feature space information and global information jointly. Experimental results on three publicly available datasets demonstrate that our proposed model achieves the best performance compared with competitive existing FET systems. Furthermore, based on extensive auxiliary experiments, we study the impact of our proposed noisy learning framework in-depth with qualitative and quantitative analysis. In the future, the proposed approach can motivate the need for further understanding of the relationships between dataset noise distribution estimation and the instance features. More work can be done towards this direction. In addition, performances of the proposed framework under different backbone models can be dug to validate the flexibility of the framework.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An Example of noisy labels and feature space illustration in FET task.</figDesc><graphic url="image-1.png" coords="1,307.24,212.59,216.09,133.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Model architecture.</figDesc><graphic url="image-2.png" coords="3,82.20,70.85,430.88,200.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Performance change with respect to β and e 1 on the Ontonotes (sub-figure a, c) and BBN (sub-figure b, d) dataset. The horizontal lines hereinafter denotes for previous SOTA performances and our reported performances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Performance curves with different trusted instance set D t sizes on three datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Performance curves under different featurecluster numbers k on the Ontonotes (a) and BBN (b), #∆cluster represents k − |T |.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: (a, b): the feature representations of backbone and FCLC model on BBN test set; (c, d): clusters denoted by colors according to samples in (a, b); (e): the row of '/GPE/COUNTRY' in the circled cluster's confusion matrix.</figDesc><graphic url="image-7.png" coords="8,71.95,326.52,216.10,100.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Average prediction confidence over negative predicted samples on three datasets.</figDesc><graphic url="image-8.png" coords="8,70.86,571.22,222.25,142.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Fine-Grained Entity Typing datasets Statistics.</figDesc><table><row><cell>BBN</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Hyper-parameters chosen for the three datasets.</figDesc><table><row><cell>Hyper-parameters</cell><cell>Wiki</cell><cell cols="2">OntoNotes BBN</cell></row><row><cell>Learning Rate</cell><cell cols="2">0.0002 0.0006</cell><cell>0.0007</cell></row><row><cell>Batch Size</cell><cell>512</cell><cell>512</cell><cell>512</cell></row><row><cell>LSTM Layer</cell><cell>0</cell><cell>2</cell><cell>1</cell></row><row><cell>hidden Size (d s )</cell><cell>-</cell><cell>700</cell><cell>560</cell></row><row><cell cols="2">Word Emb Size (d w ) 300</cell><cell>300</cell><cell>300</cell></row><row><cell>Pos Emb Size (d p )</cell><cell>85</cell><cell>70</cell><cell>20</cell></row><row><cell cols="2">Phase 1 Epochs (e 1 ) 5</cell><cell>14</cell><cell>20</cell></row><row><cell>#Clusters (k)</cell><cell>116</cell><cell>104</cell><cell>42</cell></row><row><cell cols="2">LC Loss Weight (β) 0.25</cell><cell>0.35</cell><cell>0.95</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Performance results on three benchmark datasets.</figDesc><table><row><cell cols="3">Model</cell><cell></cell><cell></cell><cell cols="6">Wiki Strict Acc Macro F1 Micro F1 Strict Acc Macro F1 Micro F1 Strict Acc Macro F1 Micro F1 OntoNotes BBN</cell></row><row><cell cols="5">AFET(2016a)</cell><cell>53.3</cell><cell></cell><cell>69.3</cell><cell>66.4</cell><cell>55.3</cell><cell>71.2</cell><cell>64.6</cell><cell>68.3</cell><cell>74.4</cell><cell>74.7</cell></row><row><cell cols="5">Attentive(2016)</cell><cell>59.7</cell><cell></cell><cell>80.0</cell><cell>75.4</cell><cell>51.7</cell><cell>71.0</cell><cell>64.91</cell><cell>48.4</cell><cell>73.2</cell><cell>72.4</cell></row><row><cell cols="5">NFETC(2018)</cell><cell cols="2">56.2±1.0</cell><cell cols="3">77.2±0.9 74.3±1.1 54.8±0.4</cell><cell>71.8±0.4 65.0±0.4 73.8±0.6</cell><cell>78.4±0.6 78.9±0.6</cell></row><row><cell></cell><cell></cell><cell cols="3">w/ hier</cell><cell cols="2">68.9±0.6</cell><cell cols="3">81.9±0.7 79.0±0.7 60.2±0.2</cell><cell>76.4±0.1 70.2±0.2 73.9±1.2</cell><cell>78.8±1.2 79.4±1.1</cell></row><row><cell cols="5">CLSC(2019)</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>59.6±0.3</cell><cell>75.5±0.4 69.3±0.4 74.7±0.3</cell><cell>80.7±0.2 80.5±0.2</cell></row><row><cell></cell><cell></cell><cell cols="3">w/ hier</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>62.8±0.3</cell><cell>77.8±0.3 72.0±0.4 73.0±0.3</cell><cell>79.8±0.4 79.5±0.3</cell></row><row><cell cols="5">NFETC-AR(2020)</cell><cell cols="2">58.1±1.1</cell><cell cols="3">79.0±0.4 76.1±0.4 62.8±0.4</cell><cell>77.8±0.4 71.8±0.5 76.7±0.2</cell><cell>81.4±0.3 81.5±0.3</cell></row><row><cell></cell><cell></cell><cell cols="3">w/ hier</cell><cell cols="2">70.1±0.9</cell><cell cols="3">83.2±0.7 80.1±0.6 64.0±0.3</cell><cell>78.8±0.3 73.0±0.3 74.9±0.6</cell><cell>80.4±0.6 80.3±0.6</cell></row><row><cell cols="5">NFETC-VAT(2020)</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>63.8</cell><cell>78.7</cell><cell>73.0</cell><cell>76.7</cell><cell>80.7</cell><cell>80.9</cell></row><row><cell cols="5">CLSC-VAT(2020)</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>63.9</cell><cell>78.6</cell><cell>73.1</cell><cell>76.9</cell><cell>81.2</cell><cell>81.4</cell></row><row><cell cols="5">ML-L2R(2020)</cell><cell>69.1</cell><cell></cell><cell>82.6</cell><cell>80.8</cell><cell>58.7</cell><cell>73.0</cell><cell>68.1</cell><cell>75.2</cell><cell>79.7</cell><cell>80.5</cell></row><row><cell cols="5">Box(2021)</cell><cell>-</cell><cell></cell><cell>81.6</cell><cell>77.0</cell><cell>-</cell><cell>77.3</cell><cell>70.9</cell><cell>-</cell><cell>78.7</cell><cell>78.0</cell></row><row><cell cols="3">FCLC</cell><cell></cell><cell></cell><cell cols="2">58.0±1.7</cell><cell cols="3">77.8±0.8 76.2±0.8 62.7±1.1</cell><cell>77.5±0.7 71.4±0.7 82.0±0.8</cell><cell>86.2±0.7 86.7±0.7</cell></row><row><cell cols="5">FCLC hier</cell><cell cols="2">71.3±1.1</cell><cell cols="3">82.2±0.7 81.1±0.6 65.3±0.2</cell><cell>79.6±0.3 74.0±0.3 79.0±0.5</cell><cell>84.2±0.5 84.8±0.5</cell></row><row><cell cols="3">w/o τ k</cell><cell></cell><cell></cell><cell cols="2">70.9±1.6</cell><cell cols="3">81.8±1.0 80.7±1.1 64.6±0.2</cell><cell>78.8±0.2 73.1±0.3 81.6±0.4</cell><cell>85.9±0.4 86.5±0.4</cell></row><row><cell cols="7">w/o loss correction 70.4±1.4</cell><cell cols="3">81.6±1.0 80.5±0.9 64.2±0.3</cell><cell>78.4±0.3 72.6±0.5 76.5±0.5</cell><cell>81.0±0.4 81.2±0.4</cell></row><row><cell cols="5">w/o cluster</cell><cell cols="2">71.3±0.4</cell><cell cols="3">82.0±0.6 80.9±0.5 64.6±0.3</cell><cell>79.2±0.3 73.4±0.2 79.2±0.6</cell><cell>83.2±0.5 83.7±0.6</cell></row><row><cell cols="5">w/ reinit</cell><cell cols="2">69.7±2.4</cell><cell cols="3">81.2±1.2 80.1±1.3 62.4±0.3</cell><cell>77.8±0.7 71.7±0.7 79.9±0.9</cell><cell>84.2±0.9 84.6±0.6</cell></row><row><cell></cell><cell></cell><cell>75</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>87</cell><cell></cell><cell></cell></row><row><cell cols="2">Micro-F1 (%)</cell><cell>72 73 74</cell><cell></cell><cell></cell><cell></cell><cell>Micro-F1 (%)</cell><cell>82 83 84 85 86</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>71</cell><cell cols="2">Ablation Reported Best Previous SOTA</cell><cell></cell><cell></cell><cell>80 81</cell><cell cols="2">Ablation Reported Best Previous SOTA</cell></row><row><cell></cell><cell></cell><cell cols="4">0.05 0.2 0.35 0.5 0.65 0.8 0.95 (a) Varying beta on Ontonotes</cell><cell></cell><cell cols="3">0.05 0.2 0.35 0.5 0.65 0.8 0.95 (b) Varying beta on BBN</cell></row><row><cell>Micro-F1 (%)</cell><cell cols="2">73.0 73.5 74.0 74.5</cell><cell></cell><cell></cell><cell>Ablation Reported Best Previous SOTA</cell><cell>Micro-F1 (%)</cell><cell>82 87 83 84 85 86</cell><cell cols="2">Ablation Reported Best Previous SOTA</cell></row><row><cell></cell><cell cols="2">72.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">2</cell><cell cols="2">5 (c) Varying e1 on Ontonotes 8 11 14 17</cell><cell></cell><cell cols="2">2 5 8 11 14 17 20 23 (d) Varying e1 on BBN</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>1 in BBN and Ontonotes are different as they have different training set sizes and different distance supervision noise distribution.</figDesc><table><row><cell>Micro-F1 (%)</cell><cell>80 81</cell><cell>w/o LC on Wiki Reported Best on Wiki Wikim with Portioned Dt</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0.05</cell><cell>0.25</cell><cell>0.45</cell><cell>0.65</cell><cell>0.85</cell></row><row><cell>Micro-F1 (%)</cell><cell>72 73 74</cell><cell></cell><cell></cell><cell></cell><cell>w/o LC on Ontonotes Reported Best on Ontonotes Ontonotes with Portioned Dt</cell></row><row><cell></cell><cell></cell><cell>0.05</cell><cell>0.25</cell><cell>0.45</cell><cell>0.65</cell><cell>0.85</cell></row><row><cell>Micro-F1 (%)</cell><cell>82 84 86</cell><cell></cell><cell></cell><cell></cell><cell>w/o LC on BBN Reported Best on BBN BBN with Portioned Dt</cell></row><row><cell></cell><cell></cell><cell>0.05</cell><cell>0.25</cell><cell cols="2">0.45 Portion of trusted-instances used 0.65</cell><cell>0.85</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Table 4 that FCLC does not rely on a clean training subset, thus having a wide range of applications. The model performances with no trusted instances on phase 2 (w/o D t ) or on the whole training process (w/ pl). ‡ It is worth pointing out that it means our model is trained with fewer instances than previous SOTA, since Dt is a part from the training set they use.</figDesc><table><row><cell>Models</cell><cell cols="5">Wiki Acc Ma-F1 Mi-F1 Acc Ma-F1 Mi-F1 Ontonotes</cell></row><row><cell>Backbone</cell><cell>68.9</cell><cell>81.9</cell><cell>79.0 60.2</cell><cell>76.4</cell><cell>70.2</cell></row><row><cell>NFETC-AR</cell><cell>70.1</cell><cell>83.2</cell><cell>80.1 64.0</cell><cell>78.8</cell><cell>73.0</cell></row><row><cell>FCLC</cell><cell>71.3</cell><cell>82.2</cell><cell>81.1 65.3</cell><cell>79.6</cell><cell>74.0</cell></row><row><cell cols="2">w/o D t in phase 2 70.0</cell><cell>81.3</cell><cell>80.2 64.6</cell><cell>79.0</cell><cell>73.3</cell></row><row><cell>w/o D t &amp; w/ pl</cell><cell>71.3</cell><cell>82.1</cell><cell>81.0 64.2</cell><cell>78.7</cell><cell>72.9</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">* Normally |Dt| ≪ | D|, as in all the datasets we reported in this paper.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported by the National Key Research and Development Project of China (No. 2021ZD0110700).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fine-grained named entity typing over distantly supervised data based on refined representations</title>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Asif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Yifang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="7391" to="7398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Fine-grained named entity typing over distantly supervised data via refinement in hyperbolic space</title>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Asif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Yifang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11212</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Improving distantly-supervised entity typing with compact latent space clustering</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaotao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufeng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoping</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2862" to="2872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hierarchical entity typing via multi-level learning to rank</title>
		<author>
			<persName><forename type="first">Tongfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunmo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.749</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07-05">2020. July 5-10, 2020</date>
			<biblScope unit="page" from="8465" to="8475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Webly supervised learning of convolutional networks</title>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2015.168</idno>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision, ICCV 2015</title>
				<meeting><address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015-12-07">2015. December 7-13, 2015</date>
			<biblScope unit="page" from="1431" to="1439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Ultra-fine entity typing</title>
		<author>
			<persName><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="87" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Improving fine-grained entity typing with entity linking</title>
		<author>
			<persName><forename type="first">Hongliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqiu</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6211" to="6216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Prompt-learning for fine-grained entity typing</title>
		<author>
			<persName><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangwei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengjun</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong-Gee</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.10604</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A hybrid neural model for type classification of entity mentions</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1243" to="1249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Classification in the presence of label noise: A survey</title>
		<author>
			<persName><forename type="first">Benoît</forename><surname>Frénay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Verleysen</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNNLS.2013.2292894</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="845" to="869" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Contextdependent fine-grained entity type tagging</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nevena</forename><surname>Lazic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Kirchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Huynh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.1820</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Training deep neural-networks using a noise adaptation layer</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ehud</forename><surname>Ben-Reuven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations, ICLR 2017</title>
				<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24">2017. April 24-26, 2017</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Using trusted data to train deep networks on labels corrupted by severe noise</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duncan</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Neural Information Processing Systems</title>
				<meeting>the 32nd International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="10477" to="10486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName><forename type="first">Dong-Hyun</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on challenges in representation learning, ICML</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">896</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Enhancing label representations with relational inductive bias constraint for fine-grained entity typing</title>
		<author>
			<persName><forename type="first">Jinqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dakui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Li</surname></persName>
		</author>
		<idno>IJCAI2021</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An attentive fine-grained entity typing model with latent type representation</title>
		<author>
			<persName><forename type="first">Ying</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6196" to="6201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fine-grained entity recognition</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Qing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianpei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.05744</idno>
		<title level="m">Fine-grained entity typing via label reasoning</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Exploring fine-grained entity type constraints for distantly supervised relation extraction</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2107" to="2116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Bills</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-IJCNLP</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1003" to="1011" />
		</imprint>
	</monogr>
	<note>Rion Snow, and Daniel Jurafsky</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Modeling fine-grained entity types with box embeddings</title>
		<author>
			<persName><forename type="first">Yasumasa</forename><surname>Onoe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Boratko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 59th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Fine-grained entity typing for domain independent entity linking</title>
		<author>
			<persName><forename type="first">Yasumasa</forename><surname>Onoe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.05780</idno>
		<imprint>
			<date type="published" when="2019">2019a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning to denoise distantly-labeled data for entity typing</title>
		<author>
			<persName><forename type="first">Yasumasa</forename><surname>Onoe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
				<imprint>
			<date type="published" when="2019">2019b</date>
			<biblScope unit="page" from="2407" to="2417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On the complementary nature of knowledge graph embedding, fine grain entity types, and language modeling</title>
		<author>
			<persName><forename type="first">Rajat</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><surname>Ferraro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures</title>
				<meeting>Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="89" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Making deep neural networks robust to label noise: A loss correction approach</title>
		<author>
			<persName><forename type="first">Giorgio</forename><surname>Patrini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Rozza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizhen</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1944" to="1952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deeptype: Multilingual entity linking by neural type system evolution</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Raiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Raiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5406" to="5413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Composite binary losses</title>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">C</forename><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="2387" to="2422" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">AFET: automatic finegrained entity typing by hierarchical partial-label embedding</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenqi</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2016">2016a</date>
			<biblScope unit="page" from="1369" to="1378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Label noise reduction in entity typing by heterogeneous partial-label embedding</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenqi</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clare</forename><forename type="middle">R</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2016">2016b</date>
			<biblScope unit="page" from="1825" to="1834" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Alleviate dataset shift problem in fine-grained entity typing with virtual adversarial training</title>
		<author>
			<persName><forename type="first">Haochen</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaotao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhigang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2020/539</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">An attentive neural architecture for fine-grained entity type classification</title>
		<author>
			<persName><forename type="first">Sonse</forename><surname>Shimaoka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AKBC@NAACL-HLT</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="69" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Symmetric cross entropy for robust learning with noisy labels</title>
		<author>
			<persName><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaiyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2019.00041</idno>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-10-27">2019. October 27 -November 2, 2019</date>
			<biblScope unit="page" from="322" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Bbn pronoun coreference and entity type corpus. Linguistic Data Consortium</title>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ada</forename><surname>Brunstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page">112</biblScope>
			<pubPlace>Philadelphia</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lance</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ann</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michelle</forename><surname>Franchini</surname></persName>
		</author>
		<title level="m">Ontonotes release 5.0 ldc2013t19. Linguistic Data Consortium</title>
				<meeting><address><addrLine>Philadelphia, PA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Modeling noisy hierarchical types in fine-grained entity typing: A contentbased weighting approach</title>
		<author>
			<persName><forename type="first">Junshuang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongyi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinpeng</forename><surname>Huai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5264" to="5270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Improving neural fine-grained entity typing with knowledge attention</title>
		<author>
			<persName><forename type="first">Ji</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Neural finegrained entity type classification with hierarchyaware loss</title>
		<author>
			<persName><forename type="first">Peng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denilson</forename><surname>Barbosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="16" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Embedding methods for fine grained entity type classification</title>
		<author>
			<persName><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nevena</forename><surname>Lazic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="291" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning with noise: Improving distantly-supervised fine-grained entity typing via automatic relabeling</title>
		<author>
			<persName><forename type="first">Haoyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingkun</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangwei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhua</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengjun</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Rectifying pseudo label learning via uncertainty estimation for domain adaptive semantic segmentation</title>
		<author>
			<persName><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1106" to="1120" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Attention-based bidirectional long short-term memory networks for relation classification</title>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingchen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongwei</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
