<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A SURVEY ON OVERSMOOTHING IN GRAPH NEURAL NETWORKS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">T</forename><forename type="middle">Konstantin</forename><surname>Rusch</surname></persName>
							<email>trusch@ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Eth</forename><surname>Zurich</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Siddhartha</forename><surname>Mishra</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A SURVEY ON OVERSMOOTHING IN GRAPH NEURAL NETWORKS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Node features of graph neural networks (GNNs) tend to become more similar with the increase of the network depth. This effect is known as over-smoothing, which we axiomatically define as the exponential convergence of suitable similarity measures on the node features. Our definition unifies previous approaches and gives rise to new quantitative measures of over-smoothing. Moreover, we empirically demonstrate this behavior for several over-smoothing measures on different graphs (small-, medium-, and large-scale). We also review several approaches for mitigating over-smoothing and empirically test their effectiveness on real-world graph datasets. Through illustrative examples, we demonstrate that mitigating over-smoothing is a necessary but not sufficient condition for building deep GNNs that are expressive on a wide range of graph learning tasks. Finally, we extend our definition of over-smoothing to the rapidly emerging field of continuous-time GNNs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graph Neural Networks (GNNs) <ref type="bibr" target="#b39">(Sperduti, 1994;</ref><ref type="bibr" target="#b18">Goller &amp; Kuchler, 1996;</ref><ref type="bibr" target="#b40">Sperduti &amp; Starita, 1997;</ref><ref type="bibr" target="#b15">Frasconi et al., 1998;</ref><ref type="bibr" target="#b19">Gori et al., 2005;</ref><ref type="bibr" target="#b37">Scarselli et al., 2008;</ref><ref type="bibr" target="#b3">Bruna et al., 2014;</ref><ref type="bibr" target="#b11">Defferrard et al., 2016;</ref><ref type="bibr" target="#b24">Kipf &amp; Welling, 2017;</ref><ref type="bibr" target="#b29">Monti et al., 2017;</ref><ref type="bibr" target="#b17">Gilmer et al., 2017)</ref> have emerged as a powerful tool for learning on relational and interaction data. These models have been successfully applied on a variety of different tasks, e.g. in computer vision and graphics <ref type="bibr" target="#b29">Monti et al. (2017)</ref>, recommender systems <ref type="bibr" target="#b46">Ying et al. (2018)</ref>, transportation <ref type="bibr" target="#b12">Derrow-Pinion et al. (2021)</ref>, computational chemistry <ref type="bibr" target="#b17">(Gilmer et al., 2017)</ref>, drug discovery <ref type="bibr">Gaudelet et al. (2021)</ref>, particle physics <ref type="bibr" target="#b38">(Shlomi et al., 2020)</ref>, and analysis of social networks (see <ref type="bibr" target="#b48">Zhou et al. (2019)</ref>; <ref type="bibr">Bronstein et al. (2021)</ref> for additional applications).</p><p>The number of layers in a neural network (referred to as "depth" and giving the name to the entire field of "deep learning") is often considered to be crucial for its performance on real-world tasks. For example, convolutional neural networks (CNNs) used in computer vision, often use tens or even hundreds of layers. In contrast, most GNNs encountered in applications are relatively shallow and often have just few layers. This is related to several issues impairing the performance of deep GNNs in realistic graph-learning settings: graph bottlenecks <ref type="bibr" target="#b0">(Alon &amp; Yahav, 2021)</ref>, over-squashing <ref type="bibr" target="#b41">(Topping et al., 2021;</ref><ref type="bibr" target="#b10">Deac et al., 2022)</ref>, and over-smoothing <ref type="bibr" target="#b26">(Li et al., 2018;</ref><ref type="bibr" target="#b30">Nt &amp; Maehara, 2019;</ref><ref type="bibr" target="#b31">Oono &amp; Suzuki, 2020)</ref>. In this article we focus on the over-smoothing phenomenon, which loosely refers to the exponential convergence of all node features towards the same constant value as the number of layers in the GNN increases. While it has been shown that small amounts of smoothing are desirable for regression and classification tasks <ref type="bibr" target="#b23">(Keriven, 2022)</ref>, excessive smoothing <ref type="bibr">(or 'over-smoothing')</ref> results in convergence to a non-informative limit. Besides being a key limitation in the development of deep multi-layer GNNs, over-smoothing can also severely impact the ability of GNNs to handle heterophilic graphs <ref type="bibr" target="#b52">(Zhu et al., 2020)</ref>, in which node labels tend to differ from the labels of the neighbors and thus long-term interactions have to be learned.</p><p>Recent literature has focused on precisely defining over-smoothing through measures of node feature similarities such as the the graph Dirichlet <ref type="bibr" target="#b35">(Rusch et al., 2022;</ref><ref type="bibr" target="#b4">Cai &amp; Wang, 2020;</ref><ref type="bibr" target="#b47">Zhao &amp; Akoglu, 2019;</ref><ref type="bibr">Zhou et al., 2021a)</ref>, cosine similarity <ref type="bibr">(Chen et al., 2020a)</ref>, and other related similarity scores <ref type="bibr" target="#b49">(Zhou et al., 2020)</ref>. With the abundance of such measures, however, there is currently still a conceptual gap in a general definition of over-smoothing that would provide a unification of existing approaches. Moreover, previous work mostly measures the similarity of node features and does not explicitly consider the rate of convergence of over-smoothing measures with respect to an increasing number of GNN layers.</p><p>In this article, we aim to unify several recent approaches and define over-smoothing in a formal and tractable manner through an axiomatic construction. Through our definition, we rule out problematic measures such as the Mean Average Distance that does not provide a sufficient condition for over-smoothing. We then review several approaches to mitigate over-smoothing and provide their extensive empirical evaluation. These empirical studies lead to the insight that meaningfully solving over-smoothing in deep GNNs is more elaborate than simply forcing the node features not to converge towards the same node value when the number of layers is increased. Rather, there needs to be a subtle balance between the expressive power of the deep GNN and its ability to preserve the diversity of node features in the graph. Finally, we extend our definition to the rapidly emerging sub-field of continuous-time GNNs <ref type="bibr">(Chamberlain et al., 2021a,b;</ref><ref type="bibr" target="#b35">Rusch et al., 2022</ref><ref type="bibr" target="#b36">Rusch et al., , 2023;;</ref><ref type="bibr" target="#b1">Bodnar et al., 2022;</ref><ref type="bibr" target="#b13">Di Giovanni et al., 2022)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Definition of over-smoothing</head><p>Let G = (V, E ? V ? V) be an undirected graph with |V| = v nodes and |E| = e edges (unordered pairs of nodes {i, j} denoted i ? j). The 1-neighborhood of a node i is denoted N i = {j ? V : i ? j}. Furthermore, each node i is endowed with an m-dimensional feature vector X i ; the node features are arranged into a v ? m matrix X = (X ik ) with i = 1, . . . , v and k = 1, . . . , m.</p><p>Message-Passing GNN (MPNN) updates the node features by performing several iterations of the form,</p><formula xml:id="formula_0">X n = ?(F ? n (X n-1 , G)), ?n = 1, . . . , N,<label>(1)</label></formula><p>where F ? n is a learnable function with parameters ? n , X n ? R v?mn are the m-dimensional hidden node features, and ? is an element-wise non-linear activation function. Here, n ? 1 denotes the n-th layer with n = 0 being the input layer and N the total number of layers (depth). In particular, we consider local (1-neighborhood) coupling of the form (F(X, G)) i = F(X i , X j?Ni ) operating on the multiset of 1-neighbors of each node. Examples of such functions used in the graph machine learning literature <ref type="bibr">(Bronstein et al., 2021)</ref> include graph convolutions <ref type="bibr" target="#b24">(Kipf &amp; Welling, 2017)</ref> and attentional message passing <ref type="bibr" target="#b42">(Velickovic et al., 2018)</ref>.</p><p>There exist a variety of different approaches to quantify over-smoothing in deep GNNs, e.g. measures based on the Dirichlet energy on graphs <ref type="bibr" target="#b35">(Rusch et al., 2022;</ref><ref type="bibr" target="#b4">Cai &amp; Wang, 2020;</ref><ref type="bibr" target="#b47">Zhao &amp; Akoglu, 2019;</ref><ref type="bibr">Zhou et al., 2021a)</ref>, as well as measures based on the mean-average distance (MAD) <ref type="bibr">(Chen et al., 2020a;</ref><ref type="bibr" target="#b49">Zhou et al., 2020)</ref>, and references therein. However, most previous approaches lack a formal definition of over-smoothing as well as provide approaches to measure over-smoothing which are not sufficient to quantify this issue. Thus, the aim of this survey is to establish a unified, rigorous, and tractable definition of over-smoothing, which we provide in the following.</p><p>Definition 1 (Over-smoothing). Let G be an undirected, connected graph and X n ? R v?m denote the n-th layer hidden features of an N -layer GNN defined on G. Moreover, we call ? : R v?m -? R ?0 a node-similarity measure if it satisfies the following axioms:</p><formula xml:id="formula_1">1. ?c ? R m with X i = c for all nodes i ? V ? ?(X) = 0, for X ? R v?m 2. ?(X + Y) ? ?(X) + ?(Y), for all X, Y ? R v?m</formula><p>We then define over-smoothing with respect to ? as the layer-wise exponential convergence of the node-similarity measure ? to zero, i.e., 3. ?(X n ) ? C 1 e -C2n , for n = 0, . . . , N with some constants C 1 , C 2 &gt; 0.</p><p>Note that without loss of generality we assume that the node-similarity measure ? converges to zero (any node-similarity measure that converges towards a non-zero constant can easily be recast). Further remarks about Definition 1 are in order.</p><p>Remark 2.1. Condition 1 in Definition 1 simply formalizes the widely accepted notion that over-smoothing is caused by node features converging to a constant node vector whereas condition 3 provides a more stringent, quantitative measure of this convergence. Note that the triangle inequality or subadditivity (condition 2) rules out degenerate choices of similarity measures. Remark 2.2. Definition 1 only considers the case of connected graphs. However, this definition can be directly generalized to disconnected graphs, where we apply a node-similarity measure ? S on every connected component S ? V, and define the global similarity measure as the sum of the node-similarity measures on each connected component, i.e., ? = S ? S . This way we ensure to cover the case of different connected components converging to different constant node values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Over-smoothing measures</head><p>Existing approaches to measure over-smoothing in deep GNNs have mainly been based on concept of Dirichlet energy on graphs,</p><formula xml:id="formula_2">E(X n ) = 1 v i?V j?Ni X n i -X n j 2 2 ,<label>(2)</label></formula><p>(note that instead of normalizing by 1/v we can equivalently normalize the terms inside the norm based on the node degrees d i , i.e.</p><formula xml:id="formula_3">X n i ? 1+di - X n j ? 1+dj<label>2</label></formula><p>2 ). It is straightforward to check that the measure,</p><formula xml:id="formula_4">?(X n ) = E(X n ),<label>(3)</label></formula><p>satisfies the conditions 1 and 2 in the definition 1 and thus, constitutes a bona fide node-similarity measure. Note that in the remainder of this article, we will refer to the square root of the Dirichlet energy simply as the Dirichlet energy.</p><p>In the literature, Mean Average Distance (MAD),</p><formula xml:id="formula_5">?(X n ) = 1 v i?V j?Ni 1 - X n i X n j X n i X n j . (<label>4</label></formula><formula xml:id="formula_6">)</formula><p>has often been suggested as a measure of over-smoothing. We see that in contrast to the Dirichlet energy, MAD is not a node-similarity measure, as it does not fulfill condition 1 nor condition 2 of the over-smoothing definition 1. In fact, MAD is always zero in the scalar case, where all node features share the same sign for each feature dimension. This makes MAD a very problematic measure for over-smoothing as ?(X) = 0 does not represent a sufficient condition for over-smoothing to happen. However, as we will see in the subsequent section, in the multi-dimensional case (m &gt; 1) MAD does converge exponentially to zero for increasing number of layers if the GNN over-smooths and thus fulfills condition 3 of the over-smoothing definition 1. Therefore, we conclude that under careful considerations of the specific use-case, MAD may be used as a measure for over-smoothing. However, since the Dirichlet energy fulfills all three conditions of Definition 1 and is numerically more stable to compute, it should always be favored over MAD.</p><p>It is natural to ask if there exist other measures that constitute a node-similarity measure as of Definition 1 and can thus be used to define over-smoothing. While the Dirichlet energy denotes a canonical choice in this context, there are other measures that can be used. For instance, instead of basing the Dirichlet energy in (2) on the L 2 norm, any other L p -norm (p &gt; 1) can be used.</p><p>3.1 Empirical evaluation of different measures for over-smoothing <ref type="bibr" target="#b35">Rusch et al. (2022)</ref> have empirically demonstrated the qualitative behavior described in Definition 1 on a 10 ? 10 regular 2-dimensional grid with one-dimensional uniform random (hidden) node features. We extend this empirical study in two directions, first to higher dimensional node features and also to real-world graphs, namely Texas <ref type="bibr" target="#b32">(Pei et al., 2020</ref><ref type="bibr">), Cora (McCallum et al., 2000)</ref>, and Cornell5 (Facebook 100 dataset). Note that as mentioned above, the extension to higher dimensional node features is necessary in order to empirically evaluate MAD, as MAD is zero for any one-dimensional node features sharing the same sign.</p><p>Since we are interested only in the dynamics of the Dirichlet energy and MAD associated with the propagation of node features through different GNN architectures, we omit the original input node features of the real-world graph dataset Cora and exchange them for standard normal random variables, i.e., X jk ? N (0, 1) for all nodes j and every feature k.</p><p>In Fig. <ref type="figure" target="#fig_0">1</ref> we set the input and hidden dimension of the node features to 128 and plot the (logarithm of) the Dirichlet energy (2) and MAD (4)) of each layer's node features with respect to the (logarithm of) layer number for three popular GNN models, i.e., GCN, GAT and the GraphSAGE architecture of <ref type="bibr" target="#b20">Hamilton et al. (2017)</ref>. We can see that all three GNN architectures over-smooth, with both layer-wise measures converging exponentially fast to zero for increasing number of layers. Moreover, we observe that this behavior is not just restricted to the structured and regular grid dataset of <ref type="bibr" target="#b35">Rusch et al. (2022)</ref>, but the same behavior (i.e., exponential convergence of the measures with respect to increasing number of layers) can be seen on all the three real-world graph datasets considered here.</p><p>It is important to emphasize the importance of the exponential convergence of the layer-wise over-smoothing measure ? to zero in Definition 1. Algebraic convergence is not sufficient for the GNN to suffer from over-smoothing. This can be seen in Fig. <ref type="figure" target="#fig_0">1</ref>, where for instance the Dirichlet energy of GCN, GraphSAGE and GAT reach machine-precision zero after a maximum of 64 layers, while for instance a linear convergence of the Dirichlet energy would still have a Dirichlet energy of around 1 for an initial energy of around 100, even after 128 hidden layers. 4 Reducing over-smoothing</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Methods</head><p>Several methods to mitigate (or at least reduce) the effect of over-smoothing in deep GNNs have recently been proposed. While we do not discuss each individual approach, we highlight several recent methods in this context, all of which can be classified into one of the following classes.</p><p>Normalization and Regularization A proven way to reduce the over-smoothing effect in deep GNNs is to regularize the training procedure. This can be done either explicitly by penalizing deviations of over-smoothing measures during training or implicitly by normalizing the node feature embeddings and by adding noise to the optimization process.</p><p>An example of explicit regularization techniques can be found in Energetic Graph Neural Networks (EGNNs) <ref type="bibr">(Zhou et al., 2021a)</ref>, where the authors measure over-smoothing using the Dirichlet energy and propose to optimize a GNN within a constrained range of the underlying layer-wise Dirichlet energy. DropEdge <ref type="bibr" target="#b34">(Rong et al., 2020)</ref> on the other hand represents an example of implicit regularization by adding noise to the optimization process. This is done by randomly dropping edges of the underlying graph during training. Graph DropConnect (GDC) <ref type="bibr" target="#b21">(Hasanzadeh et al., 2020)</ref> generalizes this approach by allowing the GNNs to draw different random masks for each channel and edge independently. Another example of implicit regularization is PairNorm <ref type="bibr" target="#b47">(Zhao &amp; Akoglu, 2019)</ref>, where the pairwise distances are set to be constant throughout every layer in the deep GNN. This is obtained by performing the following normalization on the node features X after each GNN layer,</p><formula xml:id="formula_7">Xi = X i - 1 v v j=1 X j , X i = s Xi 1 v v j=1 Xj 2 2 ,<label>(5)</label></formula><p>where s &gt; 0 is a hyperparameter. Similarly, <ref type="bibr" target="#b49">Zhou et al. (2020)</ref> have suggested to normalize within groups of the same labeled nodes, leading to Differentiable Group Normalization (DGN). Moreover, <ref type="bibr">Zhou et al. (2021b)</ref> have suggested to node-wise normalize each feature vector, yielding NodeNorm.</p><p>Change of GNN dynamics A rapidly emerging strategy to mitigate over-smoothing for deep GNNs is by qualitatively changing the (discrete or continuous) dynamics of the message-passing propagation. A recent example is the use of non-linear oscillators which are coupled through the graph structure yielding Graph-Coupled Oscillator Network (GraphCON) <ref type="bibr" target="#b35">(Rusch et al., 2022)</ref>,</p><formula xml:id="formula_8">Y n = Y n-1 + ?t[?(F ? n (X n-1 , G)) -?X n-1 -?Y n-1 ], X n = X n-1 + ?tY n ,<label>(6)</label></formula><p>where Y n are auxiliary node features and ?t &gt; 0 denotes the time-step (usually set to ?t = 1). The idea of this work is to exchange the diffusion-like dynamics of GCNs (and its variants) to that of non-linear oscillators, which can provably be guaranteed to have a Dirichlet energy that does not exponentially vanish (as of Definition 1). A similar approach has been taken in <ref type="bibr" target="#b14">Eliasof et al. (2021)</ref>, where the dynamics of a deep GCN is modelled as a wave-type partial differential equation (PDE) on graphs, yielding PDE-GCN. Another approach inspired by physical systems is Allen-Cahn Message Passing (ACMP) <ref type="bibr" target="#b43">(Wang et al., 2022)</ref>, where the dynamics is constructed based on the Allen-Cahn equation modeling interacting particle system with attractive and repulsive forces. A related effort is the Gradient Flow Framework (GRAFF) <ref type="bibr" target="#b13">(Di Giovanni et al., 2022)</ref>, where the proposed GNN framework can be interpreted as attractive respectively repulsive forces between adjacent features.</p><p>A recent example in this direction, that is not directly inspired by physical systems, is that of Gradient Gating (G 2 ) <ref type="bibr" target="#b36">(Rusch et al., 2023)</ref>, where a learnable node-wise early-stopping mechanism is realized through a gating function leveraging the graph-gradient,</p><formula xml:id="formula_9">? n = ?( F?n (X n-1 , G)), ? n ik = tanh ? ? j?Ni |? n jk -? n ik | p ? ? , X n = (1 -? n ) X n-1 + ? n ?(F ? n (X n-1 , G)),<label>(7)</label></formula><p>with p ? 0. This mechanism slows down the message-passing propagation corresponding to each individual node (and each individual channel) as ? n ij goes to zero before local over-smoothing occurs in the j-th channel on a node i.</p><p>Residual connections Motivated by the success of residual neural networks (ResNets) <ref type="bibr" target="#b22">(He et al., 2016)</ref> in conventional deep learning, there has been many suggestions of adding residual connections to deep GNNs. An early example includes <ref type="bibr" target="#b25">Li et al. (2019)</ref>, where the authors equip a GNN with a residual connection <ref type="bibr" target="#b22">He et al. (2016)</ref>, i.e.,</p><formula xml:id="formula_10">X n = X n-1 + F ? n (X n-1 , G). (<label>8</label></formula><formula xml:id="formula_11">)</formula><p>By instantiating the GNN in ( <ref type="formula" target="#formula_10">8</ref>) with a GCN, this leads to major improvements over competing methods. Another example is GCNII <ref type="bibr">(Chen et al., 2020b)</ref> where a scaled residual connection of the initial node features is added to every layer of a GCN,</p><formula xml:id="formula_12">X n = ? (1 -? n ) D-1 2 ? D-1 2 X n-1 + ? n X 0 ((1 -? n )I + ? n W n ) ,<label>(9)</label></formula><p>where ? n , ? n ? [0, 1] are fixed hyperparameters for all n = 1, . . . , N . This allows for constructing very deep GCNs, outperforming competing methods on several benchmark tasks. Similar approaches aggregate not just the initial node features but all node features of every layer of a deep GNN at the final layer. Examples of such models include Jumping Knowledge Networks (JKNets) <ref type="bibr" target="#b45">(Xu et al., 2018)</ref> and Deep Adaptive Graph Neural Networks (DAGNNs) <ref type="bibr" target="#b27">(Liu et al., 2020)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Empirical evaluation</head><p>In order to evaluate the effectiveness of the different methods that have been suggested to mitigate over-smoothing in deep GNNs, we follow the experimental set-up of section 3. To this end, we choose two representative methods of each of the different strategies to overcome over-smoothing, namely DropEdge and PairNorm as representatives from "normalization and regularization" strategies, GraphCON and G 2 from "change of GNN dynamics", and Residual GCN (Res-GCN) and GCNII from "residual connections". We consider the same three different graphs as in section 3, namely small-scale Texas, medium-scale Cora and larger-scale Cornell5 graph. Since we are only interested in the qualitative behavior of the different methods, we fix one node-similarity measure, namely the Dirichlet energy. Thereby, we can see in Fig. <ref type="figure" target="#fig_1">2</ref> that DropEdge-GCN and Res-GCN suffer from an exponential convergence of the layer-wise Dirichlet energy to zero (and thus from over-smoothing) on all three graphs. In contrast to that, all other methods we consider here mitigate over-smoothing by keeping the layer-wise Dirichlet energy approximately constant. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Risk of sacrificing expressivity to mitigate over-smoothing</head><p>Since several of the previously suggested methods designed to mitigate over-smoothing successfully prevent the layer-wise Dirichlet energy (and other node-similarity measures) from converging exponentially fast to zero, it is natural to ask if this is already sufficient to construct (possibly very) deep GNNs which also efficiently solve the learning task at hand. To answer this question, we start by constructing a deep GCN which keeps the Dirichlet energy constant while at the same time its performance on a learning task is as poor as a standard deep multi-layer GCN.</p><p>It turns out that simply adding a bias vector to a deep GCN with shared parameters among layers, i.e.,</p><formula xml:id="formula_13">X n = ?( D-1 2 ? D-1 2 X n-1 W + b), ?n = 1, . . . , N,</formula><p>with weights W ? R m?m and bias b ? R m , is sufficient for the optimizer to keep the resulting layer-wise Dirichlet energy of the model approximately constant. This can be seen in Fig. <ref type="figure" target="#fig_2">3</ref> where the layer-wise Dirichlet energy is shown (among others) for a standard GCN as well as a GCN with an additional bias term after training on the Cora graph dataset in the fully supervised setting. We observe that while the Dirichlet energy converges exponentially fast to zero for the standard GCN, simply adding a bias term results in an approximately constant layer-wise Dirichlet energy. Moreover, Fig. <ref type="figure" target="#fig_2">3</ref> shows the test accuracy of the same models for different number of layers. We can see that both the standard GCN as well as the GCN with bias vector suffer from a significant decrease of performance for increasing number of layers. Interestingly, while GCN with bias keeps the Dirichlet energy perfectly constant and GCN without bias exhibits a Dirichlet energy converging exponentially fast to zero, both models suffer similarly from drastic impairment of performance (in terms of test accuracy) for increasing number of layers. We thus observe that simply constructing a deep GNN that keeps the node-similarity measure constant (around 1) is not sufficient in order to successfully construct deep GNNs.</p><p>This observation is further supported in Fig. <ref type="figure" target="#fig_2">3</ref> by looking at the Dirichlet energy of the PairNorm method which behaves similarly to the Dirichlet energy of GCN with bias, i.e., approximately constant around 1. However, the performance in terms of test accuracy on the Cora graph dataset drops exponentially after using more than 32 layers. Interestingly, G 2 -GCN exhibits an approximately constant layer-wise Dirichlet energy and at the same time does not decrease its performance by increasing number of layers. In fact the performance of G 2 -GCN increases slightly by increasing number of layers.</p><p>Therefore, we argue that solving the over-smoothing issue defined in Definition 1 is necessary in order to construct well performing deep GNNs. Otherwise the network is not able to learn any meaningful function defined on the graph. However, as can be seen from this experiment it is not sufficient. Therefore, based on this experiment we conclude that a major pitfall in designing deep GNNs that mitigate over-smoothing is to sacrifice the expressive power of the GNN only to keep the node-similarity measure approximately constant. In fact, based on our experiments, only G 2 (among the considered models here) fully mitigates the over-smoothing issue by keeping the node-similarity measure approximately constant, while at the same time increasing its expressive power for increasing number of layers. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Extension to continuous-time GNNs</head><p>A rapidly growing sub-field of graph representation learning deals with GNNs that are continuous in depth. This is performed by formulating the message-passing propagation in terms of graph dynamical systems modelled by (neural <ref type="bibr">(Chen et al., 2018)</ref>) Ordinary Differential Equations (ODEs) or Partial Differential Equations (PDEs), i.e., message-passing framework (1), where the forward propagation is modeled by a differential equation:</p><formula xml:id="formula_14">X (t) = ?(F ? (X(t), G)),<label>(10)</label></formula><p>with X(t) referring to the node features at time t ? 0. Different choices of the vector field (i.e., right-hand side of (10)) yields different architectures. Moreover, we note that the right-hand side in (10) can potentially arise from a discretization of a differential operator defined on a graph leading to a PDE-inspired architecture. We refer to this class of graph-learning models as continuous-time GNNs. Early examples of continuous-time GNNs include Graph Neural Ordinary Differential Equations (GDEs) <ref type="bibr" target="#b33">(Poli et al., 2019)</ref> and Continuous Graph Neural Networks (CGNN) <ref type="bibr" target="#b44">(Xhonneux et al., 2020)</ref>. More recent examples include Graph-Coupled Oscillator Networks (GraphCON) <ref type="bibr" target="#b35">(Rusch et al., 2022)</ref>, Graph Neural Diffusion (GRAND) <ref type="bibr">(Chamberlain et al., 2021a)</ref>, Beltrami Neural Diffusion (BLEND) <ref type="bibr">(Chamberlain et al., 2021b)</ref>, Neural Sheaf Diffusion (NSD) <ref type="bibr" target="#b1">(Bodnar et al., 2022)</ref>, and Gradient Glow Framework (GRAFF) <ref type="bibr" target="#b13">(Di Giovanni et al., 2022)</ref>. Based on this framework, we can easily extend our definition of over-smoothing 1 to continuous-time GNNs, by defining over-smoothing as the exponential convergence in time of a node-similarity measure. More concretely, we define it as follows.</p><p>Definition 2 (Continuous-time over-smoothing). Let G be an undirected, connected graph and X(t) ? R v?m denote the hidden node features of a continuous-time GNN (10) at time t ? 0 defined on G. Moreover, ? is a node-similarity measure as of Definition 1. We then define over-smoothing with respect to ? as the exponential convergence in time of the node-similarity measure ? to zero, i.e., ?(X(t)) ? C 1 e -C2t , for t ? 0 with some constants C 1 , C 2 &gt; 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>Stacking multiple message-passing layers (i.e., a deep GNN) is necessary in order to effectively process information on relational data where the underlying computational graph exhibits (higher-order) long-range interactions. This is of particular importance for learning heterophilic graph data, where node labels may differ significantly from those of their neighbors. Besides several other identified problems (e.g., over-squashing, exploding and vanishing gradients problem), the over-smoothing issue denotes a central challenge in constructing deep GNNs.</p><p>Since previous work has measured over-smoothing in various ways, we unify those approaches by providing an axiomatic definition of over-smoothing through the layer-wise exponential convergence of similarity measures on the node features. Moreover, we review recent measures for over-smoothing and, based on our definition, rule out the commonly used MAD in the context of measuring over-smoothing. Additionally, we test the qualitative behavior of those measures on three different graph datasets, i.e., small-scale Texas graph, medium-scale Cora graph, and large-scale Cornell5 graph, and observe an exponential convergence to zero of all measures for standard GNN models (i.e., GCN, GAT, and GraphSAGE). We further review prominent approaches to mitigate over-smoothing and empirically test whether these methods are able to successfully overcome over-smoothing by plotting the layer-wise Dirichlet energy on different graph datasets.</p><p>We conclude by highlighting the need for balancing the ability of models to mitigate over-smoothing, but without sacrificing the expressive power of the underlying deep GNN. This phenomenon was illustrated by the example of a simple deep GCN with shared parameters among all layers as well as a bias, where the optimizer rapidly finds a state of parameters during training that leads to a mitigation of over-smoothing (i.e., approximately constant Dirichlet energy). However, in terms of performance (or accuracy) on the Cora graph-learning task, this model fails to outperform its underlying baseline (i.e., same GCN model without a bias) which suffers from over-smoothing. This behavior is further observed in other methods that are particularly designed to mitigate over-smoothing, where the over-smoothing measure remains approximately constant but at the same time the accuracy of the model drops significantly for increasing number of layers. However, we also want to highlight that there exist methods that are able to mitigate over-smoothing while at the same time maintaining its expressive power on our task, i.e., G 2 . We thus conclude that mitigating over-smoothing is only a necessary condition, among many others, for building deep GNNs, while a particular focus in designing methods in this context has to be on the maintenance or potential enhancement of the expressive power of the underlying model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Dirichlet energy and Mean Average Distance (MAD) of layer-wise node features X n propagated through a GAT, GCN and GraphSAGE for three different graph datasets, (left) small-scale Texas graph, (middle) medium-scale Cora citation network, (right) large-scale Facebook network (Cornell5).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Layer-wise Dirichlet energy of hidden node features propagated through G 2 -GCN, GraphCON-GCN, PairNorm, GCNII, DropEdge-GCN and Res-GCN on three different graphs, i.e., (left) small-scale Texas graph, (middle) medium-scale Cora citation network, (right) large-scale Facebook (Cornell5) network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Trained G 2 -GCN, PairNorm, GCN with bias and GCN without bias on the fully-supervised Cora graph dataset using the pre-defined 10 splits from Pei et al. (2020), showing two different measures for increasing number of layers ranging from 1 to 128: (left) Dirichlet energy of the layer-wise node features, (right) test accuracies.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>The authors would like to thank <rs type="person">Dr. Petar Veli?kovi?</rs> (<rs type="affiliation">DeepMind/University of Cambridge</rs>) for his insightful feedback and constructive suggestions.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On the bottleneck of graph neural networks and its practical implications</title>
		<author>
			<persName><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eran</forename><surname>Yahav</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Cristian</forename><surname>Bodnar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><forename type="middle">Di</forename><surname>Giovanni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Paul Chamberlain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.04579</idno>
		<title level="m">Neural sheaf diffusion: A topological perspective on heterophily and oversmoothing in gnns</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Joan</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taco</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petar</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><surname>Veli?kovi?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.13478</idno>
		<title level="m">Geometric deep learning: Grids, groups, graphs, geodesics, and gauges</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Spectral networks and locally connected networks on graphs</title>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A note on over-smoothing for graph neural networks</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusu</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.13318</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">GRAND: graph neural diffusion</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Rowbottom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><forename type="middle">I</forename><surname>Gorinova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Webb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emanuele</forename><surname>Rossi</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning</title>
		<meeting>the 38th International Conference on Machine Learning</meeting>
		<imprint>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Beltrami flow and neural diffusion on graphs</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Rowbottom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Eynard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Di Giovanni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaowen</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In NeurIPS, 2021b</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Measuring and relieving the over-smoothing problem for graph neural networks from the topological view</title>
		<author>
			<persName><forename type="first">Deli</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="3438" to="3445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Simple and deep graph convolutional networks</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhewei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zengfeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bolin</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1725" to="1735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural ordinary differential equations</title>
		<author>
			<persName><forename type="first">Yulia</forename><surname>Ricky Tq Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">K</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Expander graph propagation</title>
		<author>
			<persName><forename type="first">Andreea</forename><surname>Deac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Lackenby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.02997</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><forename type="first">Micha?l</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3844" to="3852" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Traffic Prediction with Graph Neural Networks in Google Maps</title>
		<author>
			<persName><forename type="first">Austin</forename><surname>Derrow-Pinion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Hester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Nunkesser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seongjae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueying</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishal</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongwen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvaro</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giovanni</forename></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Rowbottom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Benjamin P Chamberlain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Markovich</surname></persName>
		</author>
		<author>
			<persName><surname>Bronstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.10991</idno>
		<title level="m">Graph neural networks as gradient flows</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Pde-gcn: Novel architectures for graph neural networks motivated by partial differential equations</title>
		<author>
			<persName><forename type="first">Moshe</forename><surname>Eliasof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eldad</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eran</forename><surname>Treister</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A general framework for adaptive processing of data structures</title>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Sperduti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="768" to="786" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Utilizing graph machine learning within drug discovery and development</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Gaudelet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Day</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jyothish</forename><surname>Arian R Jamasb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristian</forename><surname>Soman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gertrude</forename><surname>Regep</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><forename type="middle">Br</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Hayter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Vickers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Briefings in Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning task-dependent distributed representations by backpropagation through structure</title>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Goller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Kuchler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICNN</title>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A new model for learning in graph domains</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNN</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bayesian graph neural networks with adaptive connection sampling</title>
		<author>
			<persName><forename type="first">Arman</forename><surname>Hasanzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ehsan</forename><surname>Hajiramezanali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shahin</forename><surname>Boluki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Duffield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishna</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoning</forename><surname>Qian</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4094" to="4104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Not too little, not too much: a theoretical analysis of graph (over) smoothing</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Keriven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deepgcns: Can gcns go as deep as cnns?</title>
		<author>
			<persName><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9267" to="9276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Towards deeper graph neural networks</title>
		<author>
			<persName><forename type="first">Meng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery &amp; data mining</title>
		<meeting>the 26th ACM SIGKDD international conference on knowledge discovery &amp; data mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="338" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Automating the construction of internet portals with machine learning</title>
		<author>
			<persName><forename type="first">Andrew Kachites</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamal</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristie</forename><surname>Seymore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="127" to="163" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
		<author>
			<persName><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emanuele</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Revisiting graph neural networks: all we have is low pass filters</title>
		<author>
			<persName><forename type="first">H</forename><surname>Nt</surname></persName>
		</author>
		<author>
			<persName><surname>Maehara</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08434v4</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Graph neural networks exponentially lose expressive power for node classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>Oono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Suzuki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Geom-gcn: Geometric graph convolutional networks</title>
		<author>
			<persName><forename type="first">Hongbin</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingzhe</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Chen-Chuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05287</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Michael</forename><surname>Poli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Massaroli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyoung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atsushi</forename><surname>Yamashita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hajime</forename><surname>Asama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinkyoo</forename><surname>Park</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.07532</idno>
		<title level="m">Graph neural ordinary differential equations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Towards deep graph convolutional networks on node classification</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Graph-coupled oscillator networks</title>
		<author>
			<persName><forename type="first">T</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Rusch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Rowbottom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhartha</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th International Conference on Machine Learning</title>
		<meeting>the 39th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">162</biblScope>
			<biblScope unit="page" from="18888" to="18909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Gradient gating for deep multi-rate learning on graphs</title>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Rusch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><forename type="middle">P</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhartha</forename><surname>Mishra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ah</forename><surname>Chung Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Graph neural networks in particle physics</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Shlomi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Roch</forename><surname>Vlimant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning: Science and Technology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">21001</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Encoding labeled graphs by labeling RAAM</title>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Sperduti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Supervised neural networks for the classification of structures</title>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Sperduti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonina</forename><surname>Starita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="714" to="735" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Understanding over-squashing and bottlenecks on graphs via curvature</title>
		<author>
			<persName><forename type="first">Jake</forename><surname>Topping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><forename type="middle">Di</forename><surname>Giovanni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Paul Chamberlain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaowen</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.14522</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations, ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Acmp: Allen-cahn message passing for graph neural networks with particle phase transition</title>
		<author>
			<persName><forename type="first">Yuelin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><forename type="middle">Guang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shi</forename><surname>Jin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.05437</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Continuous graph neural networks</title>
		<author>
			<persName><forename type="first">Louis-Pascal</forename><surname>Xhonneux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML. PMLR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5453" to="5462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In KDD</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">Lingxiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.12223</idno>
		<title level="m">Pairnorm: Tackling oversmoothing in gnns</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ganqu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changcheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08434v4</idno>
		<title level="m">Graph neural networks: a review of methods and applications</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Towards deeper graph neural networks with differentiable group normalization</title>
		<author>
			<persName><forename type="first">Kaixiong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuening</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daochen</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4917" to="4928" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Dirichlet energy constrained learning for deep graph neural networks</title>
		<author>
			<persName><forename type="first">Kaixiong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daochen</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soo-Hyun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="21834" to="21846" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Understanding and resolving performance degradation in deep graph convolutional networks</title>
		<author>
			<persName><forename type="first">Kuangqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanfei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wee</forename><surname>Sun Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Hooi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 30th ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2728" to="2737" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Beyond homophily in graph neural networks: Current limitations and effective designs</title>
		<author>
			<persName><forename type="first">Jiong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingxiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Heimann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danai</forename><surname>Koutra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7793" to="7804" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
