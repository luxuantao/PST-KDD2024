<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GPT-RE: In-context Learning for Relation Extraction using Large Language Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhen</forename><surname>Wan</surname></persName>
							<email>zhenwan@nlp.ist.i.kyoto-u.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">Kyoto University</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fei</forename><surname>Cheng</surname></persName>
							<email>feicheng@i.kyoto-u.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">Kyoto University</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhuoyuan</forename><surname>Mao</surname></persName>
							<email>zhuoyuanmao@nlp.ist.i.kyoto-u.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">Kyoto University</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qianying</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Kyoto University</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haiyue</forename><surname>Song</surname></persName>
							<email>song@nlp.ist.i.kyoto-u.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">Kyoto University</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
							<email>jiwei_li@zju.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Kyoto University</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Peter</forename><forename type="middle">J 2019</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Richard</forename><surname>Shin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Christopher</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sam</forename><surname>Thomson</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Charles</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><roleName>Emmanouil</roleName><forename type="first">Subhro</forename><surname>Roy</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Antonios</forename><surname>Platanios</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Adam</forename><surname>Pauls</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jason</forename><surname>Eisner</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Benjamin</forename><surname>Van</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Durme</forename><forename type="middle">2021</forename><surname>Constrained</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Romal</forename><surname>Thoppilan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><surname>De Freitas</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jamie</forename><surname>Hall</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Apoorv</forename><surname>Kulshreshtha</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Heng-Tze</forename><surname>Cheng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Alicia</forename><surname>Jin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Taylor</forename><surname>Bos</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Leslie</forename><surname>Baker</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yu</forename><surname>Du</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yaguang</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><roleName>Huaixiu</roleName><forename type="first">Hongrae</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Steven</forename><surname>Zheng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Amin</forename><surname>Ghafouri</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Marcelo</forename><surname>Menegali</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yanping</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Dmitry</forename><surname>Lepikhin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">James</forename><surname>Qin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">De- Hao</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yuanzhong</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Vincent</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Chung-Ching</forename><surname>Chang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Igor</forename><surname>Krivokon</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Will</forename><surname>Rusch</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Marc</forename><surname>Pickett</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Pranesh</forename><surname>Srinivasan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Laichee</forename><surname>Man</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Kathleen</forename><surname>Meier-Hellstern</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Meredith</forename><forename type="middle">Ringel</forename><surname>Morris</surname></persName>
						</author>
						<author>
							<persName><roleName>Renelito</roleName><forename type="first">Tulsee</forename><surname>Doshi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Delos</forename><surname>Santos</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Toju</forename><surname>Duke</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Johnny</forename><surname>Soraker</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ben</forename><surname>Zevenbergen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Vinod- Kumar</forename><surname>Prabhakaran</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mark</forename><surname>Diaz</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ben</forename><surname>Hutchinson</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Kristen</forename><surname>Olson</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Alejandra</forename><surname>Molina</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Erin</forename><surname>Hoffman- John</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Josh</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Lora</forename><surname>Aroyo</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ravi</forename><surname>Rajakumar</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Alena</forename><surname>Butryna</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Matthew</forename><surname>Lamm</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Viktoriya</forename><surname>Kuzmina</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Joe</forename><surname>Fenton</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Aaron</forename><surname>Cohen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Rachel</forename><surname>Bernstein</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ray</forename><surname>Kurzweil</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Blaise</forename><surname>Aguera-Arcas</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Claire</forename><surname>Cui</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Marian</forename><surname>Croak</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ed</forename><surname>Chi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
						</author>
						<author>
							<persName><forename type="first">:</forename><surname>Lamda</surname></persName>
						</author>
						<author>
							<persName><surname>Lan</surname></persName>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">Association for Computational Linguistics</orgName>
								<address>
									<settlement>Seattle</settlement>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">GPT-RE: In-context Learning for Relation Extraction using Large Language Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In spite of the potential for ground-breaking achievements offered by large language models (LLMs) (e.g., GPT-3), they still lag significantly behind fully-supervised baselines (e.g., fine-tuned BERT) in relation extraction (RE). This is due to the two major shortcomings of LLMs in RE: (1) low relevance regarding entity and relation in retrieved demonstrations for in-context learning; and (2) the strong inclination to wrongly classify NULL examples into other pre-defined labels.</p><p>In this paper, we propose GPT-RE to bridge the gap between LLMs and fully-supervised baselines. GPT-RE successfully addresses the aforementioned issues by (1) incorporating task-specific entity representations in demonstration retrieval; and (2) enriching the demonstrations with gold label-induced reasoning logic. We evaluate GPT-RE on four widelyused RE datasets, and observe that GPT-RE achieves improvements over not only existing GPT-3 baselines, but also fully-supervised baselines. Specifically, GPT-RE achieves SOTA performances on the Semeval and Sci-ERC datasets, and competitive performances on the TACRED and ACE05 datasets.</p><p>Context: among the contents of the vessel were a set of carpenter 's tools , several large storage jars , ceramic utensils , rope ?? Given the context, the relation between tools and vessel is CONTENT AND CONTAINER.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GPT-Sent Demonstration</head><p>Context: to preserve its catch of fish , each boat loads between 2000 and 3000 kilos of ice before it goes out to sea . Given the context, the relation between catch and fish is [NULL]    Test Input Context: local fisherman heated some of their catch cooked over coals in a scuttle . Given the context, the relation between catch and scuttle is CONTENT AND CONTAINER.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GPT-RE_SimCSE Demonstration</head><p>Context: september normally marks the arrival of the earliest run of fish into lake tributaries , and peak runs occur in october . Given the context, the relation between run and fish is NULL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GPT-RE_FT Demonstration</head><p>Retrieval GPT-Sent GPT-RE_SimCSE GPT-RE_FT GPT-3 Output CONTENT AND CONTAINER PRODUCT AND PRODUCER NULL</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The emergence of large language models (LLMs) such as GPT-3 <ref type="bibr" target="#b5">(Brown et al., 2020;</ref><ref type="bibr">Thoppilan et al., 2022;</ref><ref type="bibr">Chowdhery et al., 2022;</ref><ref type="bibr">Rae et al., 2021;</ref><ref type="bibr" target="#b14">Hoffmann et al., 2022)</ref> represents a significant advancement in natural language processing (NLP). Instead of following a pretraining-and-finetuning pipeline <ref type="bibr" target="#b8">(Devlin et al., 2019;</ref><ref type="bibr" target="#b2">Beltagy et al., 2019;</ref><ref type="bibr">Raffel et al., 2019;</ref><ref type="bibr" target="#b16">Lan et al., 2019;</ref><ref type="bibr" target="#b33">Zhuang et al., 2021)</ref>, which finetunes a pretrained model on a task-specific dataset in a fully-supervised manner, LLMs employ a new paradigm known as in-context learning (ICL) <ref type="bibr" target="#b5">(Brown et al., 2020;</ref><ref type="bibr">Min et al., 2022a)</ref> which formulates an NLP task under the paradigm of language generation and makes predictions by learning from demonstrations. Under the  framework of ICL, LLMs achieve remarkable performance rivaling previous fully-supervised methods even with only a limited number of demonstrations provided in the prompt in various tasks such as solving math problems, commonsense reasoning, text classification, fact retrieval, natural language inference, and semantic parsing <ref type="bibr" target="#b5">(Brown et al., 2020;</ref><ref type="bibr">Min et al., 2022b;</ref><ref type="bibr" target="#b31">Zhao et al., 2021;</ref><ref type="bibr">Liu et al., 2022b;</ref><ref type="bibr">Shin et al., 2021)</ref>. Despite the overall promising performance of LLMs, the utilization of ICL for relation extraction (RE) is still less optimal. RE seeks to identify a semantic relationship between a given entity pair mentioned in a sentence, which is the central task for knowledge retrieval requiring a deep understanding of natural language. Recent research <ref type="bibr" target="#b11">(Guti?rrez et al., 2022)</ref> has sought to apply GPT-3 ICL to biomedical RE, the results are relatively negative and suggest that GPT-3 ICL still signifi-cantly underperforms fine-tuned models on the full dataset.</p><p>The reasons that cause the pitfall of GPT-3 ICL in RE are two folds: (1) The low relevance regarding entity and relation in the retrieved demonstrations for ICL. Demonstrations are selected randomly or via k-nearest neighbor (kNN) search based on sentence representations <ref type="bibr">(Liu et al., 2022b;</ref><ref type="bibr" target="#b11">Guti?rrez et al., 2022)</ref>. Regrettably, kNNretrieval based on sentence-level representations is more concerned with the relevance of the overall sentence semantics and not as much with the entities and relations it contains. This leads to low-quality demonstrations retrieved. As shown in Figure <ref type="figure" target="#fig_8">1a</ref>, the test input retrieves a semantically similar sentence but is not desired in terms of entities and relations.</p><p>(2) Overpredicting: we observe that LLMs have the strong inclination to wrongly classify NULL examples into other pre-defined labels as shown in Figure <ref type="figure" target="#fig_8">1b</ref>. A similar phenomenon has also been observed in other tasks such as NER <ref type="bibr" target="#b11">(Guti?rrez et al., 2022;</ref><ref type="bibr" target="#b3">Blevins et al., 2022)</ref>. This phenomenon is because it is relatively simple to display an example that meets the criteria of a pre-defined label, yet hard and complex to illustrate an example that does not belong to that label. NULL examples are the collection with various undefined relations in nature. This fact causes the complex distribution of NULL examples, which hinders kNN-retrieval to obtain similar NULL demonstrations. This issue can be alleviated if the representations for retrieval can be supervised with a huge number of NULL, i.e., the supervised setting, or the reasoning logic can be accessed to enhance GPT-3 inference.</p><p>In this paper, we propose GPT-RE for the relation extraction task. GPT-RE employs two strategies to resolve the issues above: (1) entity-aware retrieval and (2) gold label-induced reasoning. For (1) entity-aware retrieval, its core is to use representations that deliberately encode and emphasize entity and relation information rather than sentence-level representations for kNN search. We propose the first encoding method to append the entity-pair prompt to the sentence. The second method is to obtain representations from a RE model fine-tuned on the RE training set, which naturally places emphasis on entities and relations. Both methods contain more RE-specific information than sentence semantics, thus effectively addressing the problem of low relevance.</p><p>For (2) gold label-induced reasoning, we propose to incorporate the reasoning steps to the demonstration, a strategy akin to <ref type="bibr" target="#b27">Wei et al. (2022)</ref>; <ref type="bibr">Wang et al. (2022b)</ref>; <ref type="bibr" target="#b15">Kojima et al. (2022)</ref>. But different from previous work, the reasoning process not only explains why a given sentence should be classified under a particular label but also why a NULL example should not be assigned to any of the pre-defined categories. This process of explaining significantly improves the prediction when fewer demonstrations are provided.</p><p>We evaluate our proposed method on three popular general domain RE datasets: Semeval 2010 task 8, TACRED and ACE05, and one scientific domain dataset SciERC. We observe that GPT-RE achieves improvements over not only existing GPT-3 baselines, but also fully-supervised baselines. Specifically, GPT-RE achieves SOTA performances on the Semeval and SciERC datasets, and competitive performances on the TACRED and ACE05 datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminary Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Task Definition</head><p>Let C denote the input context and e sub ? C, e obj ? C denote the pair of subject and object entity. Given a set of pre-defined relation classes R, relation extraction aims to predict the relation y ? R between the pair of entities (e sub , e obj ) within the context C, or if there is no pre-defined relation between them, predict y = NULL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">BERT-based Fine-tuning</head><p>Current BERT-based fine-tuning methods for RE <ref type="bibr" target="#b0">(Baldini Soares et al., 2019;</ref><ref type="bibr" target="#b32">Zhong and Chen, 2021;</ref><ref type="bibr" target="#b24">Wan et al., 2022)</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">GPT-RE</head><p>GPT-RE is formalized under the ICL framework, using GPT-3 as shown in Figure <ref type="figure" target="#fig_2">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Prompt Construction</head><p>We construct a prompt for each given test example, which is fed to the GPT-3 model. Each prompt consists of the following components:</p><p>Task Description and Pre-defined Classes We provide a succinct overview of the RE task description and the set of pre-defined classes R, denoted by O. The model is explicitly asked to output the relation, which belongs to the pre-defined classes.</p><p>Otherwise, the model will output NULL.</p><p>Few-shot Demonstrations In the demonstration part, we reformulate each example by first showing the input prompt x demo = P rompt(C, e sub , e obj ) and the relation label y demo . The input prompt can be further enriched by the reasoning process.</p><p>Test Input Similar to the demonstrations, we offer the test input prompt x test , and GPT-3 is expected to generate the corresponding relation y test . In summary, GPT-RE can be formulated as:</p><formula xml:id="formula_0">p y test | O K i=1 x i demo y i demo x test (1)</formula><p>where " " denotes the concatenation of two tex-tual pieces, " " indicates cumulative concatenation, and "K" is the number of demonstrations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Entity-aware Demonstration Retrieval</head><p>Since ICL demonstrations closer to the test sample in the embedding space result in more consistent and robust performance <ref type="bibr">(Liu et al., 2022b)</ref>. Recent work <ref type="bibr" target="#b11">(Guti?rrez et al., 2022;</ref><ref type="bibr">Liu et al., 2022b)</ref> employs the kNN to retrieve the most similar examples in the training set as the few-shot demonstrations for each test example. As kNN relies on the choice of the embedding space, they propose to obtain sentence representations using PLMs, or other improved sentence representations. However, using sentence-level representations for kNN retrieval has a severe drawback: relation extraction focuses on pair-wise entities, which diverge from the semantic meaning of the entire sentence, leading to an ambiguous retrieval using sentence embeddings. In this study, we propose two novel methods to provide more robust representations for better retrieval quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Entity Prompted Sentence Representation</head><p>Given the discrepancy between sentence embedding and relation extraction, the original context is insufficient for demonstration retrieval. given the context "He has a sister Lisa," the reconstructed context with the entity prompted will be "The relation between 'He' and 'Lisa' in the context: He has a sister Lisa." This approach addresses the feature of RE as it preserves both the semantic meaning of the entire sentence and the entity paircentered information during retrieval. In the paper, we employ the latest robust model SimCSE <ref type="bibr" target="#b9">(Gao et al., 2021)</ref> for the sentence similarity calculation to select the nearest neighbors between the reconstructed contexts. We formulate the encoding process as SimCSE(P rompt retrival (C, e sub , e obj )).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Fine-tuned Relation Representation</head><p>Compared to prompt entity information into context sentences, a more straightforward solution is to extract the relation representation from a fine-tuned model for retrieving demonstrations. As shown in Sec. 2.2, the entity markers have explicitly encoded subject and object entities, and the relation representation Rel is naturally enriched with the entity information.</p><p>We believe this approach can potentially compensate for the limitations of GPT-3 in RE. While GPT-3 ICL has a constraint of limited demonstrations, the fine-tuning process is unbundled and can be done on the whole train data. It has two subsequent merits. First, the relation representations are directly fine-tuned to fit the RE task, which could significantly boost the overall retrieval quality. Second, the overpredicting NULL issue will be substantially alleviated because the similar NULL demonstrated can be accurately recognized by the fine-tuned model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Gold Label-induced Reasoning</head><p>The recent CoT <ref type="bibr" target="#b27">(Wei et al., 2022;</ref><ref type="bibr">Wang et al., 2022b)</ref> has reported significant progress in commonsense and numerical reasoning tasks by reasoning prompts that successfully elicits the reasoning logic towards the final output. While in RE, a pair of entities potentially holds multiple possible relations, which could leave the reasoning out of focus.</p><p>In this section, we propose to let GPT-3 induce Context: the melodramatic plot dealt with crises of human emotion .</p><p>Given the context, the relation between plot and crises is MESSAGE AND TOPIC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original Few-Shot Demostration</head><p>What are the clues that lead to the relation between "plot" and "crises" to be MESSAGE AND TOPIC in the sentence "the melodramatic plot dealt with crises of human emotion ."?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query</head><p>Context: the melodramatic plot dealt with crises of human emotion .</p><p>Given the context, the relation between plot and crises is MESSAGE AND TOPIC. It is because:</p><p>The word "melodramatic" implies that the plot is intended to convey a message or moral, while the phrase "crises of human emotion" suggests that the plot is dealing with a topic related to the emotions of people.</p><p>Reasoning Enhanced Few-Shot Demostration the reasoning logic for each demonstration by the corresponding gold RE label. As shown in Figure <ref type="figure" target="#fig_3">3</ref>, given a selected example, we first generate a query prompt based on the example and subsequently ask GPT-3 to generate clues on the labeled relation between the pair of entities in the context. Finally, we augment the demonstration by incorporating the generated clues with the original example.</p><p>4 Experiment Setup</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We evaluate our proposed method on three popular general domain RE datasets and one scientific domain dataset. ACE05 contains the entity, relation, and event annotations from 511 documents in total collected from multiple domains including newswire, broadcast, discussion forums, etc. Due to the cost of running the model in the API with GPT-3, in our main results, we sample a subset from the original test set for two datasets: ACE05 and TACRED as shown in Table <ref type="table" target="#tab_1">1</ref>. We will release all of these subsets and the corresponding sampling codes for reproducibility and further study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baseline Methods</head><p>Fine-tuning baseline In our experiment, we choose PURE <ref type="bibr" target="#b32">(Zhong and Chen, 2021)</ref> as our finetuning baseline model. We follow their singlesentence to keep consistency among datasets as Semeval and TACRED are both sentence-level RE datasets. For the PLMs, we also follow PURE by using scibert-scivocab-uncased <ref type="bibr" target="#b2">(Beltagy et al., 2019)</ref> as the base encoder for SciERC and bertbase-uncased <ref type="bibr" target="#b8">(Devlin et al., 2019)</ref> for the remaining three general domain datasets.</p><p>GPT-3 baseline For all GPT-3 baselines and our proposed methods, we select "text-davinci-003" and use the identical prompt construction, as defined in Section 3.1. Other hyperparameters are listed in Appendix A. We compare our proposed method with two categories of GPT-3 baselines from previous work.</p><p>(1) GPT-Random For each test input, we randomly choose few-shot demonstrations from the training data. Unlike the vanilla random selection, we add extra constraints to make the label distribution of selected demonstrations more uniform. Our preliminary experiments suggest that this is a stronger baseline than the vanilla random.</p><p>(2) <ref type="bibr">GPT-Sent Guti?rrez et al. (2022)</ref> uses the <ref type="bibr">[CLS]</ref> of RoBERTa-large as the representation in retrieval, <ref type="bibr">Liu et al. (2022b)</ref> fine-tunes RoBERTa-large on two natural language inference (NLI) datasets: SNLI <ref type="bibr" target="#b4">(Bowman et al., 2015)</ref> and MultiNLI <ref type="bibr" target="#b28">(Williams et al., 2018)</ref> to enhance the quality of sentence representations. In this work, our implementation adopted Sim-CSE <ref type="bibr" target="#b9">(Gao et al., 2021)</ref> for sentence embedding, which has been demonstrated to be the state-of-the-art method for sentence similarity tasks. In our experiment, we utilize the version: sup-simcse-bert-base-uncased.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Main Results</head><p>We compare our main experiment results with previous methods in Table <ref type="table" target="#tab_2">2</ref>. From the table, we can observe that: (1) both GPT-RE_SimCSE and GPT-RE_FT outperform the retrieval-based GPT-Sent, indicating that it is necessary to capture the taskspecific information into sentence encoding for  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Ablation Study on Entity-aware Retrieval</head><p>We first implement the ablation experiments of the retrieval component with the setting of increasing k-shot demonstrations (Figure <ref type="figure" target="#fig_5">4a</ref>). We find that:</p><p>(1) compared to GPT-Random, all the retrievalbased models have higher F1 scores and large gradients of the performance curves. It means that GPT-3 can learn from high-quality demonstrations more effectively; (2) after adding entity information to the SimCSE retrieval, GPT-RE_SimCSE achieves better performance throughout all K shots, indicating that entity-aware sentence representation can capture the feature of RE and provide more proper demonstrations; (3) finally, the fine-tuned relation representation retriever GPT-RE_FT significantly outperforms all retrieval-based methods and beats the fine-tuning baseline when k &gt; 15. Note that even with k = 5 demonstrations, GPT-RE_FT still works better than GPT-RE_SimCSE with k = 30 (80.30 -? 83.43(+3.13)), which indicates that the quality of demonstrations shows much more important than the number of demonstrations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablation Study on Reasoning Enhancing</head><p>We then check the influence of our proposed reasoning-enhanced demonstration, as shown in Figure <ref type="figure" target="#fig_5">4a</ref>. Due to the limited amount of input tokens of GPT-3, we have to set the k ? 15 for the tokens of reasoning, leading to a trade-off between adding reasoning and adding more demonstrations. From the result, we find that: (1) with reasoning-enhanced demonstrations, GPT-3 always achieves better scores across all the k-shot settings of both GPT-RE_SimCSE and GPT-RE_FT, indicating that the reasoning induced from ground truth relation labels can effectively unlock the reasoning ability of GPT-3 and improve the ICL with a deeper understanding of demonstrations. Specifically, for GPT-RE_FT, the performance improve- ment becomes less significant when more demonstrations provided, which is feasible as with more high-quality demonstrations available, GPT-3 can already learn the internal reasoning behind each demonstration; (2) since the reasoning enhancement works better with fewer demonstrations, we expect this method can be an effective solution to low-shot relation extraction <ref type="bibr" target="#b12">(Han et al., 2018;</ref><ref type="bibr" target="#b10">Geng et al., 2020;</ref><ref type="bibr">Liu et al., 2022a)</ref>, which aims at recognizing novel relations with very few or no examples, and we leave this for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Low-resource Scenario</head><p>We conduct the experiment for observing the lowresource performance in the general domain Semeval task. As shown in Figure <ref type="figure" target="#fig_6">5</ref>, we observe that: (1) all the GPT-3 based results work better than fine-tuning in when the training examples are less than # 650 (10%). It indicates that in the general domain RE, GPT-3 benefits from its abundant prior knowledge to understand the relations;</p><p>(2) GPT-RE_SimCSE starts to show a substantial difference to GPT-Sent after the training size surpasses 30%. We believe fewer training candidates could limit the effects of retrieval; (3) GPT-RE_FT achieves an upper bound performance in all settings, even when the fine-tuned model shows poor performance with hundreds of training data (from #100 to #400). This emphasizes the impressive effectiveness of fine-tuned relation representations for capturing higher-quality demonstrations. The observation in the low-resource setting is very different from <ref type="bibr" target="#b11">Guti?rrez et al. (2022)</ref>. We assume the difference could be caused by the domain and NULL proportion of the task. (3) however, even with entity-aware representations, all GPT-3 methods still underperform the fine-tuning baseline on NULL examples, this is due to the confusing definition of NULL, in many cases, there is a certain relation between entities in the context, but out of the distribution of pre-defined classes. In these cases, GPT-3 tends to overpredict as the relation information may be covered in its prior knowledge. We think this ability of GPT-3 can be useful in more open fields, such as open RE <ref type="bibr" target="#b1">(Banko and Etzioni, 2008)</ref> which has no predefined relation classes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Case Study of Demonstration Quality</head><p>We select one typical test example to better illustrate the amendment of our entity-aware demonstration retrieval. As shown in Figure <ref type="figure" target="#fig_9">7</ref>, given the NULL Example, we show the most similar demonstration in retrieval based on three methods. The GPT-Sent retrieved demonstration focuses on the semantic meaning of "CONTENT AND CON-TAINER" which is shared in the test context, but not revealed in the target entity pair. This mismatch confirms the problem of lacking entity information in retrieval. Instead, GPT-RE_SimCSE retrieves a much more relevant demonstration that shows the same semantic relation between "catch" and "fish" but still faces a minor mismatch as the gold label is between "catch" and "scuttle." Finally, GPT-RE_FT demonstration shares a similar structure with the test input regarding the pair of entities, which is the key clue for predicting the relation between entities. This result shows a level-bylevel enhancement with more entity information provided in retrieval. We also show some other case examples in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>In-context Learning Recent work shows that ICL of GPT-3 <ref type="bibr" target="#b5">(Brown et al., 2020)</ref> can perform numerous tasks when provided a few examples in a natural language prompt. Existing work focus on various aspects to effectively utilize the advantages of GPT-3, from prompt design <ref type="bibr" target="#b23">(Perez et al., 2021)</ref> for proper input to coherence calibration <ref type="bibr" target="#b21">(Malkin et al., 2022)</ref> for tackling the diverse generated output. Another research path locates in the demonstration part, including ordered prompts <ref type="bibr" target="#b19">(Lu et al., 2022)</ref> and retrieval-based demonstrations <ref type="bibr">(Rubin et al., 2022;</ref><ref type="bibr">Liu et al., 2022b;</ref><ref type="bibr">Shin et al., 2021)</ref>.</p><p>To the best of our knowledge, there is no previous work exploring the potential of GPT-3 on general domain RE tasks. A recent work attempts to leverage GPT-3 in biomedical information extraction (NER and RE), and reveals issues of ICL that may be detrimental to IE tasks in general. Our work succeeds in overcoming these issues to some extent and confirms the potential of GPT-3 in both general and scientific domain RE.</p><p>Retrieval-based Demonstrations Several studies have demonstrated that dynamically selecting few-shot demonstrations for each test example, instead of utilizing a fixed set, leads to significant improvement in GPT-3 ICL <ref type="bibr">(Liu et al., 2022b;</ref><ref type="bibr">Shin et al., 2021;</ref><ref type="bibr">Rubin et al., 2022)</ref>. They also show that nearest neighbor in-context examples yield much better results than the farthest ones. This leads to the significance of better retrieval modules for demonstrations. Existing attempts rely on sentence representations in retrieval, including the sentence encoders of PLMs such as BERT <ref type="bibr" target="#b8">(Devlin et al., 2019)</ref>, RoBERTa <ref type="bibr" target="#b33">(Zhuang et al., 2021</ref>) KATE <ref type="bibr">(Liu et al., 2022b)</ref> , SimCSE <ref type="bibr" target="#b9">(Gao et al., 2021)</ref>, Sentence-BERT (Reimers and Gurevych, 2019; <ref type="bibr" target="#b29">Wolf et al., 2020)</ref>. Unlike these sentence representations, we propose to fine-tune PLMs on our target RE tasks to produce more task-specific and robust representations for retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions</head><p>This work explores the potential of GPT-3 ICL on RE. Given the difficulties in utilizing GPT-3 for RE, we propose GPT-RE to solve these issues and bridge the performance gap to the fine-tuning baselines via two strategies: (1) entity-aware demonstration retrieval emphasizes entity and relation information for improving the accuracy of searching demonstrations; (2) gold label-induced reasoning enriches the reasoning evidence of each demonstration. The experimental results show that GPT-RE significantly outperforms the fine-tuning baseline on three datasets and achieves SOTA on Semeval and SciERC. We implement detailed studies to explore how GPT-3 overcomes the difficulties such as NULL example influence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>Despite the overall positive results, GPT-RE still faces two shortcomings: (1) the issue of overpredicting has been significantly alleviated but not completely solved, and the NULL recall still lags behind full-supervised baselines, especially on the datasets containing a large proportion of NULL examples such as ACE05 ("95.60%"); (2) Though the entity-aware retriever optimizes the representations of PLMs such as SimCSE and BERT, it is widely considered that LLMs can generate more robust representations than small PLMs. Future work can replace representations generated by smaller PLMs with GPT-3 itself. However, due to the access limitation to the representations of GPT-3, we can nearly confirm this proposal up to now.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Microsoft is a technology company located in the U.S., founded in 1975 by Bill Gates and Paul Allen RE Training Data Microsoft is a famous computer technology company in the U.S. whose history started in 1975 (Microsoft, Bill Gates, founded by) Relation Triplet: (Microsoft, the U.S., located in) (a) Retrieval without entity information results in noisy demonstrations. Confusion matrix on Semeval dataset with three selected relation labels. The NULL examples are overpredicted to other relations by GPT-3. CE: Cause-Effect, IA: Instrument-Agency, PP: Product-Producer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Two problems of GPT-3 ICL on RE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>IFigure 2 :</head><label>2</label><figDesc>Figure 2: An illustration of GPT-RE. The blue line denotes the retrieval process, and the orange line denotes the reasoning process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: An illustration of adding reasoning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(a) The comparison on retrieval modules (b) Reasoning with fewer demonstrations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Ablation study on the retrieval and reasoning components on Semeval. We sampled a subset from the test data with 300 examples. We show the 'w/o reasoning' results with k = 30 for comparison.</figDesc><graphic url="image-6.png" coords="6,74.40,232.67,215.13,163.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Low-resource Scenario on Semeval. We limit the percentage of training data for both fine-tuning and GPT-RE.</figDesc><graphic url="image-7.png" coords="6,306.14,71.09,218.27,146.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Analysis on the effects of NULL examples. w/o NULL refers to the classification setting that NULL examples are excluded from the train and test data. w/ NULL refers to the original extraction setting. We use the full test set for the evaluation</figDesc><graphic url="image-9.png" coords="7,300.98,70.86,218.38,156.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>6. 1</head><label>1</label><figDesc>Analysis on the effects of NULL ExamplesTo analyze the influence of NULL class, we compare the effectiveness of each method for solving this issue on two datasets: general domain semeval with 17.4% NULL examples and scientific domain SciERC with 90.16% NULL examples. As shown in the Figure 6, (1) by comparing the performance on Semeval and SciERC, a larger percentage of NULL examples results in more significant performance drop showing the negative influence of overpredicting NULL examples; (2) by comparing w/o NULL and w/ NULL, our GPT-RE_FT shows the most robustness to the influence of NULL examples, indicating that the RE fine-tuned representations in retrieval can release the overpredicting issue of GPT-3 by providing higher-quality demonstrations;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: A case study of demonstration quality on Semeval. [NULL] denotes the gold label.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Statistics of datasets. Rel. denotes relation types.</figDesc><table><row><cell>Semeval</cell><cell>9</cell><cell>6,507</cell><cell>1,493</cell><cell>2,717 (2,717) 17.40%</cell></row><row><cell>TACRED</cell><cell>41</cell><cell cols="2">68,124 22,631</cell><cell>15,509 (1,600) 79.40%</cell></row><row><cell>SciERC</cell><cell>7</cell><cell>16,872</cell><cell>2,033</cell><cell>4,088 (4,088) 90.16%</cell></row><row><cell>ACE05</cell><cell cols="3">6 121,368 27,597</cell><cell>24,420 (2,442) 95.60%</cell></row></table><note><p>Considering the importance of entity information in RE, we propose reconstructing the context by incorporating entity pair information. For example, Dataset # Rel. # Train # Dev # Test (# Subset) NA (%)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Table1lists the datasets and their statistics. Main Results on four RE datasets. All results are given by Micro-F1. * denotes the same k-shot for the comparison with + Reasoning. Due to the costly GPT-3 expense, we conducted Reasoning experiments on the two relatively smaller datasets Semeval and TACRED. ? denotes that this performance is not comparable as it evaluates on the entire test set. The underline denotes the results outperforming the fine-tuning baseline PURE.</figDesc><table><row><cell>Semeval 2010 task 8 Hendrickx et al. (2010)</cell></row><row><cell>focuses on semantic relations (e.g., "cause and</cell></row><row><cell>effect") between pairs of nominals and contains</cell></row><row><cell>10,717 annotated examples covering nine relations</cell></row><row><cell>collected from general domain resources.</cell></row><row><cell>TACRED Zhang et al. (2017) is a large-scale</cell></row><row><cell>relation extraction dataset with 106,264 examples</cell></row><row><cell>built over newswire and web text. It spans 41 rela-</cell></row><row><cell>tions labels, which hold between persons, locations,</cell></row><row><cell>organizations, dates, and so on (e.g., "siblings,"</cell></row><row><cell>SciERC Luan et al. (2018) collects AI paper</cell></row><row><cell>abstracts and annotated relations, especially for</cell></row><row><cell>scientific knowledge graph construction.</cell></row></table><note><p>"dates of birth," "subsidiaries," etc.).</p></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A Hyperparameters</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B GPT-3 Hyperparameters</head><p>We use the GPT-3 API during the experiments and set the hyperparameters as in Table <ref type="table">3</ref>. Since the "Temperature" is set to be 0.0, denoting the stable output of GPT-3, we report the result of the single run for all experiments. Due to the input length limitation of GPT-3 and the various average lengths of contexts from each dataset, we set different search ranges for the number of demonstrations of each dataset as shown in Table <ref type="table">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Other Hyperparameters</head><p>For the fine-tuning baseline model PURE and the sentence embedding model SimCSE, we follow all hyperparameters from their papers. We used 2 NVIDIA RTX3090 for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Case Study</head><p>To verify the effectiveness of our entity-aware demonstration retrieval, we provide more cases.</p><p>For Figure <ref type="figure">8a</ref>, GPT-Sent retrieves a demonstration that shares the same semantic meaning of "design" with the test input. However, the entity pair is irrelevant to the concept "design" resulting in a noisy demonstration. Instead, GPT-RE_SimCSE retrieves a more relative demonstration with closer pair of entities sharing the same relation label. Furthermore, GPT-RE_FT retrieves the demonstration containing both the closing entity pair and the same linguistic structure between entities. This case emphasizes level-by-level improvement using our proposed methods. Figure <ref type="figure">8b</ref> shows a similar phenomenon.</p><p>Context: this paper describes a set of principles designed to help archives position themselves to address the management ...... Given the context, the relation between principles and set is MEMBER AND COLLECTION.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GPT-Sent Demonstration</head><p>Context: basic diagrams also work well on the computer screen if they are carefully designed to match the grid of pixels on the screen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Given the context, the relation between screen and computer is [COMPONENT AND WHOLE] Test Input</head><p>Context: the screen works using ink , just like books and newspapers , but displays the ink particles electronically . Given the context, the relation between ink and screen is COMPONENT AND WHOLE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GPT-RE_SimCSE Demonstration</head><p>Context: the computer mouse has been the input device of choice for a long time now in the computer world . Given the context, the relation between mouse and computer is COMPONENT AND WHOLE.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GPT-RE_FT Demonstration</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Matching the blanks: Distributional similarity for relation learning</title>
		<author>
			<persName><forename type="first">Baldini</forename><surname>Livio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><surname>Kwiatkowski</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1279</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2895" to="2905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The tradeoffs between open and traditional relation extraction</title>
		<author>
			<persName><forename type="first">Michele</forename><surname>Banko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-08: HLT</title>
		<meeting>ACL-08: HLT<address><addrLine>Ohio</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="28" to="36" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">SciB-ERT: A pretrained language model for scientific text</title>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1371</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3615" to="3620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Prompting language models for linguistic structure</title>
		<author>
			<persName><forename type="first">Terra</forename><surname>Blevins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hila</forename><surname>Gonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2211.07830</idno>
		<idno>CoRR, abs/2211.07830</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabor</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1075</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="632" to="642" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Alec Radford, Ilya Sutskever, and Dario Amodei</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
	<note>Language models are few-shot learners</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parker</forename><surname>Schuh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kensen</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasha</forename><surname>Tsvyashchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Maynez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parker</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Vinodkumar Prabhakaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Reif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reiner</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Pope</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>Gur-Ari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toju</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anselm</forename><surname>Duke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunipa</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henryk</forename><surname>Dev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Michalewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vedant</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liam</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daphne</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Ippolito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyeontaek</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Spiridonov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Sepassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivani</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Omernick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thanumalayan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie</forename><surname>Sankaranarayana Pillai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aitor</forename><surname>Pellat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erica</forename><surname>Lewkowycz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Moreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Polozov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongwei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brennan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Saeta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathy</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><surname>Meier-Hellstern</surname></persName>
		</author>
		<author>
			<persName><surname>Eck</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2204.02311</idno>
		<imprint>
			<pubPlace>Jeff Dean, Slav Petrov</pubPlace>
		</imprint>
	</monogr>
	<note>and Noah Fiedel. 2022. Palm: Scaling language modeling with pathways. CoRR, abs/2204.02311</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Relation extraction as two-way spanprediction</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shachar</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Rosenman</surname></persName>
		</author>
		<author>
			<persName><surname>Goldberg</surname></persName>
		</author>
		<idno>CoRR, abs/2010.04829</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">SimCSE: Simple contrastive learning of sentence embeddings</title>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingcheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.552</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Dominican Republic. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6894" to="6910" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">MICK: A metalearning framework for few-shot relation classification with small training data</title>
		<author>
			<persName><forename type="first">Xiaoqing</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenny</forename><forename type="middle">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Libin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinggong</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.1145/3340531.3411858</idno>
	</analytic>
	<monogr>
		<title level="m">CIKM &apos;20: The 29th ACM International Conference on Information and Knowledge Management, Virtual Event</title>
		<meeting><address><addrLine>Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020-10-19">2020. October 19-23, 2020</date>
			<biblScope unit="page" from="415" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Thinking about GPT-3 in-context learning for biomedical ie? think again</title>
		<author>
			<persName><forename type="first">Jim?nez</forename><surname>Bernal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolas</forename><surname>Guti?rrez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clay</forename><surname>Mcneal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">You</forename><surname>Washington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><surname>Su</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2203.08410</idno>
		<idno>CoRR, abs/2203.08410</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">FewRel: A large-scale supervised few-shot relation classification dataset with state-of-the-art evaluation</title>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1514</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4803" to="4809" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">SemEval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals</title>
		<author>
			<persName><forename type="first">Iris</forename><surname>Hendrickx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nam</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zornitsa</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?</forename><surname>Diarmuid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>S?aghdha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Pad?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenza</forename><surname>Pennacchiotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stan</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName><surname>Szpakowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Workshop on Semantic Evaluation</title>
		<meeting>the 5th International Workshop on Semantic Evaluation<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="33" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>De Las</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hennigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Noland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bogdan</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelia</forename><surname>Damoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Guy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erich</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><forename type="middle">W</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Sifre</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2203.15556</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>Training compute-optimal large language models</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Large language models are zero-shot reasoners</title>
		<author>
			<persName><forename type="first">Takeshi</forename><surname>Kojima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shane</forename><surname>Shixiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Machel</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutaka</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusuke</forename><surname>Matsuo</surname></persName>
		</author>
		<author>
			<persName><surname>Iwasawa</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2205.11916</idno>
		<idno>CoRR, abs/2205.11916</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Albert: A lite bert for self-supervised learning of language representations</title>
		<author>
			<persName><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1909.11942</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">2022a. Pre-training to match for unified low-shot relation extraction</title>
		<author>
			<persName><forename type="first">Fangchao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianpei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boxi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.397</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5785" to="5795" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures</title>
		<author>
			<persName><forename type="first">Jiachang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.deelio-1.10</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Deep Learning Inside Out</title>
		<meeting>Deep Learning Inside Out<address><addrLine>Dublin, Ireland and Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022. DeeLIO 2022</date>
			<biblScope unit="page" from="100" to="114" />
		</imprint>
	</monogr>
	<note>What makes good in-context examples for GPT-3?</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity</title>
		<author>
			<persName><forename type="first">Yao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Bartolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alastair</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.556</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="8086" to="8098" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1360</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3219" to="3232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Coherence boosting: When your pretrained language model is not paying enough attention</title>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Malkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nebojsa</forename><surname>Jojic</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.565</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="8214" to="8236" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Rethinking the role of demonstrations: What makes in-context learning work?</title>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinxi</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer ; Sewon Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinxi</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2202.12837</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>2022b. Rethinking the role of demonstrations: What makes in-context learning work? CoRR, abs/2202.12837</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">True few-shot learning with language models</title>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021</title>
		<imprint>
			<date type="published" when="2021-12-06">2021. December 6-14, 2021</date>
			<biblScope unit="page" from="11054" to="11070" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jack</forename><forename type="middle">W</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Aslanides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Ring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susannah</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Hennigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albin</forename><surname>Cassirer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Powell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maribeth</forename><surname>Rauh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amelia</forename><surname>Glaese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumanth</forename><surname>Dathathri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saffron</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Uesato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Mellor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonia</forename><surname>Creswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nat</forename><surname>Mcaleese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhant</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Budden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Esme</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michela</forename><surname>Paganini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lena</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorraine</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adhiguna</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aida</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Nematzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Domenic</forename><surname>Gribovskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angeliki</forename><surname>Donato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Baptiste</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Lespiau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolai</forename><surname>Tsimpoukelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Grigorev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibault</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mantas</forename><surname>Sottiaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toby</forename><surname>Pajarskas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Pohlen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cyprien</forename><surname>Toyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujia</forename><surname>De Masson D'autume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tayfun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Terzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Mikulik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><surname>De Las</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelia</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Guy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blake</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Hechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iason</forename><surname>Weidinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Isaac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Lockhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Rimell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianying</forename><surname>Zhen Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuoyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sadao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Kurohashi</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2210.11800</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>Rescue implicit and long-tail cases: Nearest neighbor relation extraction. CoRR, abs/2210.11800</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Jie Tang, and Dawn Song. 2022a. DeepStruct: Pretraining of language models for structure prediction</title>
		<author>
			<persName><forename type="first">Chenguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyun</forename><surname>Hong</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.findings-acl.67</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL 2022</title>
		<imprint>
			<publisher>Dublin, Ireland. Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="803" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Selfconsistency improves chain of thought reasoning in language models</title>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2203.11171</idno>
		<idno>CoRR, abs/2203.11171</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Chain of thought prompting elicits reasoning in large language models</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<idno>CoRR, abs/2201.11903</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1101</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1112" to="1122" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.6</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Positionaware attention and supervised data improve slot filling</title>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1004</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="35" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Calibrate before use: Improving few-shot performance of language models</title>
		<author>
			<persName><forename type="first">Zihao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning, ICML 2021</title>
		<meeting>the 38th International Conference on Machine Learning, ICML 2021</meeting>
		<imprint>
			<date type="published" when="2021-07">2021. 18-24 July 2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="12697" to="12706" />
		</imprint>
	</monogr>
	<note>Virtual Event</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A frustratingly easy approach for entity and relation extraction</title>
		<author>
			<persName><forename type="first">Zexuan</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.5</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="50" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A robustly optimized BERT pre-training approach with post-training</title>
		<author>
			<persName><forename type="first">Liu</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shi</forename><surname>Ya</surname></persName>
		</author>
		<author>
			<persName><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th Chinese National Conference on Computational Linguistics</title>
		<meeting>the 20th Chinese National Conference on Computational Linguistics<address><addrLine>Huhhot, China</addrLine></address></meeting>
		<imprint>
			<publisher>Chinese Information Processing Society of China</publisher>
			<date type="published" when="2021-06">Jun. 2021</date>
			<biblScope unit="page" from="1218" to="1227" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
