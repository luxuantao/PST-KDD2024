<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Mining on Manifolds: Metric Learning without Labels</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ahmet</forename><surname>Iscen</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">VRG</orgName>
								<orgName type="institution">FEE, CTU</orgName>
								<address>
									<settlement>Prague</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Giorgos</forename><surname>Tolias</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">VRG</orgName>
								<orgName type="institution">FEE, CTU</orgName>
								<address>
									<settlement>Prague</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yannis</forename><surname>Avrithis</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Inria Rennes</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ond≈ôej</forename><surname>Chum</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">VRG</orgName>
								<orgName type="institution">FEE, CTU</orgName>
								<address>
									<settlement>Prague</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Mining on Manifolds: Metric Learning without Labels</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work we present a novel unsupervised framework for hard training example mining. The only input to the method is a collection of images relevant to the target application and a meaningful initial representation, provided e.g. by pre-trained CNN. Positive examples are distant points on a single manifold, while negative examples are nearby points on different manifolds. Both types of examples are revealed by disagreements between Euclidean and manifold similarities. The discovered examples can be used in training with any discriminative loss.</p><p>The method is applied to unsupervised fine-tuning of pretrained networks for fine-grained classification and particular object retrieval. Our models are on par or are outperforming prior models that are fully or partially supervised.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The success of deep neural networks on large-scale problems has been first demonstrated on the task of supervised classification <ref type="bibr" target="#b25">[26]</ref>. It was shown that embeddings, typically provided by the convolutional layers of a network, are applicable beyond classification tasks. These tasks include particular object retrieval <ref type="bibr" target="#b14">[15]</ref>, local descriptor learning <ref type="bibr" target="#b16">[17]</ref>, ranking <ref type="bibr" target="#b60">[62]</ref>, and nearest-neighbor regression <ref type="bibr" target="#b5">[6]</ref>. A common practice is to start with a pre-trained network <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b18">19]</ref> and apply metric learning <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b60">62,</ref><ref type="bibr" target="#b17">18]</ref> to fine-tune the network for a particular task.</p><p>To improve over the initial network, novel training samples are sought for which the initial network performs poorly. Such training samples are used to re-train the network using loss functions alternative to cross-entropy (e.g. contrastive <ref type="bibr" target="#b8">[9]</ref>, triplet <ref type="bibr" target="#b60">[62]</ref> or batch-level <ref type="bibr" target="#b35">[36]</ref>). The approaches to obtain relevant training data range from further human supervision <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b35">36]</ref> to instance clustering <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b4">5]</ref>, exploiting the temporal dimension in video <ref type="bibr" target="#b62">[64,</ref><ref type="bibr" target="#b63">65]</ref>, predicting the spatial layout of image patches <ref type="bibr" target="#b10">[11]</ref>, or using existing computer vision pipelines to match unstructured image collections pairwise <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b41">42]</ref>.</p><p>Most recent deep metric learning approaches can learn powerful embeddings but still use class labels. This is unsatisfying not only because we miss the opportunity of learning from unlabeled data, but learned representations of each class are unimodal <ref type="bibr" target="#b43">[44]</ref>. Therefore, whatever the loss function <ref type="bibr" target="#b60">[62,</ref><ref type="bibr" target="#b35">36]</ref>) or sampling <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b33">34]</ref>, the problem remains supervised classification. On the other hand, conventional nonlinear dimension reduction or manifold learning methods exploit the manifold structure of the data starting from nearest neighbor graphs and are otherwise unsupervised <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b6">7]</ref>. However, most do not learn an explicit mapping from the input to the embedding space and have difficulties in generalizing to new data.</p><p>We attempt to bridge this gap in this work. In particular, we propose a novel method of hard training sample mining in a fully unsupervised manner, simply from an unordered collection of images relevant to the final task. We observe that a similarity between two images is improved by considering all, even unlabelled, available data. In particular, we exploit similarity measured on a manifold estimated by a random walk process <ref type="bibr" target="#b20">[21]</ref>.</p><p>The learning starts from the initial representation space of unlabeled data. Given an anchor point that is part of the data, neighbors on the manifold that are not Euclidean neighbors are considered positive samples. In the new learned representation space, the positive sample should be attracted to the anchor to reflect the similarity measured on the manifold. Conversely, Euclidean neighbors that are not neighbors on the manifold are considered negative and should be repelled. The idea is illustrated in Figure <ref type="figure" target="#fig_0">1</ref>.</p><p>The advantage of learning a new representation over using the estimated manifold similarity is twofold. First, estimating the manifold similarity has additional computational and memory requirements at query time. Second, we show that the novel embedding generalizes better not only to previously unseen queries, but also to unseen datasets.</p><p>We apply the proposed method to fine-grained classification and to particular object retrieval. Our models obtained by unsupervised fine-tuning of pre-trained networks are on par or are outperforming prior models that are human supervised or use additional domain-specific expertise.</p><p>The paper is organized as follows. Section 2 discusses related work. Sections 3 and 4 present our learning method and applications, respectively. Experiments are given in Section 5 conclusions are drawn in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>This section contains a brief overview of related work on metric learning, embeddings for instance retrieval and representation learning without human labeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metric learning.</head><p>Recent approaches based on low-dimensional CNN-based embeddings achieve promising results on this task <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b35">36]</ref>. A key ingredient of some approaches is hard negative mining, which comes from the days of SVMs in object detection <ref type="bibr" target="#b12">[13]</ref>. This principle has been known much earlier as bootstrapping <ref type="bibr" target="#b53">[54]</ref>. Instead of sampling all negative instances for an anchor point, the most challenging negative instances are mined which finally offer faster but equally accurate learning. However, this process is not trivial. Simple hard negative mining from a different class label might corrupt the training process due to mislabeled images <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b32">33]</ref>.</p><p>Schroff et al. <ref type="bibr" target="#b46">[47]</ref> use triplet loss and propose semi-hard mining in an attempt to solve this problem. They sample negatives within the batch, such that they are close to the anchor point but further away from positives. This concept is widely adapted in other metric learning approaches <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b35">36]</ref>. Wu et al. <ref type="bibr" target="#b65">[67]</ref> improve it by uniformly sampling negative instances weighted by their distance. This allows them to sample points from various regions of the feature space instead of certain clusters.</p><p>Other methods, such as lifted-structure <ref type="bibr" target="#b35">[36]</ref> and n-pair loss <ref type="bibr" target="#b51">[52]</ref>, focus on the loss function and define constraints on the pairwise distances of all points in a batch. More recently, several methods try to optimize the learned embedding based on the global distribution of the data. Song et al. <ref type="bibr" target="#b52">[53]</ref> try to optimize the clustering quality. Harwood et al. <ref type="bibr" target="#b17">[18]</ref> combine triplet loss and global loss using an efficient hard mining procedure. All these methods use class labels during sampling.</p><p>Particular object retrieval is another application where descriptors are trained similarly to metric learning. How-ever, class labels do not usually exist as it is intractable to enumerate all possible instances or their viewpoints. Traditional instance search algorithms use hand-designed descriptors <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b24">25]</ref>, but recent advances show the interest of feature learning <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b14">15]</ref>. Transfer learning from category-level classification is one case <ref type="bibr" target="#b42">[43]</ref>. Labeling at landmark level has been attempted as well, treating this task as classification, but the labels are quite noisy <ref type="bibr" target="#b2">[3]</ref>.</p><p>The state-of-the-art approaches start with an off-theshelf network, and fine-tune it with algorithmic supervision <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b14">15]</ref>. They make use of geometric matching to mine matching and non-matching image pairs. They involve local feature extraction and require an expensive pre-processing of data. Furthermore, they assume that the training set contains landmarks and buildings that perform well with geometric matching of local features. We make no such assumptions nor require additional computer vision system to perform the mining. Our only assumption is that there are multiple object instances in the training set.</p><p>Incomplete, noisy, or unavailable labels are handled in various ways in the literature. In the contrastive loss paper of Hadsell et al. <ref type="bibr" target="#b15">[16]</ref>, the authors show that it is possible to separate different categories to different subspaces just by assigning the Euclidean nearest neighbors as positive training instances. In recent works, the labeling is guided by the information of different modalities or the data collection process. Arandjeloviƒá et al. <ref type="bibr" target="#b0">[1]</ref> learn visual descriptors for location recognition by assigning labels based on the GPS location of each image. Wang and Gupta <ref type="bibr" target="#b62">[64]</ref> sample frames from videos and assume that frames from same videos will be positive to each other. Isola et al. <ref type="bibr" target="#b21">[22]</ref> group objects based on their co-occurence within the same spatial or temporal context. Similarly, learning from the spatial arrangement of the patches within an image is shown feasible <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b34">35]</ref>. Finally, other cases utilize different modalities, such as learning visual descriptors from text information <ref type="bibr" target="#b13">[14]</ref>, or learning audio descriptors from unlabeled videos <ref type="bibr" target="#b1">[2]</ref>. By contrast, we make no assumptions on the available modalities or contextual information.</p><p>Unsupervised methods and manifold learning. There are very few methods on deep metric learning that are unsupervised. Examples are two methods on learning fine-grained similarity by exploiting mutual proximity <ref type="bibr" target="#b5">[6]</ref> and ranking <ref type="bibr" target="#b4">[5]</ref> in the Euclidean space. Both utilize some form of clustering and splitting the training set in different groups, which is an artificial constraint, and none takes the underlying manifold structure into account. Our method is conceptually simpler and compatible with any loss function requiring positive/negative samples.</p><p>The work of Li et al. <ref type="bibr" target="#b29">[30]</ref> is similar to ours in following a graph-based mining approach. In our comparisons, we show that choosing hard examples is essential and results in better performance for our approach. Recently, Pai et al. <ref type="bibr" target="#b36">[37]</ref> learn the manifold structure by encouraging pairs of points to have a Euclidean distance in the embedding space equal to the geodesic distance on the graph. We use a more relaxed objective that is compatible to most metric learning formulations and loss functions, and a manifold similarity that is more scalable than the geodesic distance.</p><p>Embeddings are learned to approximate ranking on manifolds with fast spectral ranking <ref type="bibr" target="#b19">[20]</ref>. However, this approach is dataset specific and does not generalize to unseen images. Improvements on this aspect are introduced by iterative manifold embedding <ref type="bibr" target="#b66">[68]</ref>. Nevertheless, the extension can handle a query image, or potentially a small set of unseen images, while a larger dataset increase significantly affects the manifold structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section we describe our learning problem, briefly discuss the required background, and present our unsupervised training subset selection and the training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem formulation</head><p>Let X = {x 1 , . . . , x n } ‚äÇ X be an unordered and unlabeled collection of items, where X is the original input space of all possible items. Depending on the application, an item corresponds to an image, video, image region, local patch, etc. Function f (‚Ä¢; Œ∏) : X ‚Üí R d maps an item x to a vector z = f (x, Œ∏) in a d-dimensional embedding space, where Œ∏ is a set of parameters to be learned.</p><p>The input items are represented by a set of features Y = {y 1 , . . . , y n } ‚äÇ Y, where Y is a feature space and y = g(x) for x ‚àà X. Depending on the application, g may be the identity mapping, i.e. learn directly from input space, f (‚Ä¢; Œ∏ 0 ), i.e. learn from the same model with initial parameters Œ∏ 0 , or a different model. An existing model may have been supervised (in any way) or not.</p><p>Two items are matching if they are considered visually similar, otherwise non-matching. Our goal is to learn the model parameters such that matching items are mapped to nearby points in the embedding space, while non-matching items are well separated. This corresponds to a typical metric learning scenario, and common practice is to use manually defined labels in order to construct a training set of matching and non-matching pairs of items <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b60">62,</ref><ref type="bibr" target="#b17">18]</ref>. In this work, we only assume that the input items X and their features Y are available at training time.</p><p>A training pair is defined w.r.t. to a reference (anchor) item x r . A matching pair consists of the anchor and a positive item x + . Similarly a non-matching pair consists of the anchor and a negative item x ‚àí . Alternatively, we may use a triplet (x r , x + , x ‚àí ). Our goal is to mine such training pairs without any supervision <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b60">62,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b33">34]</ref>, without any complementary computer vision system <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b41">42]</ref> or assumptions on the nature of the training data <ref type="bibr" target="#b62">[64,</ref><ref type="bibr" target="#b63">65,</ref><ref type="bibr" target="#b10">11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Preliminaries</head><p>By NN e k (y) we denote the k Euclidean nearest neighbors of y ‚àà Y , i.e. the k most similar items in Y according to some Euclidean similarity function s e : Y 2 ‚Üí R. Similarly, by NN m k (y) we denote the k manifold nearest neighbors of y ‚àà Y , i.e. the k most similar features in Y according to a manifold similarity s m : Y 2 ‚Üí R. <ref type="foot" target="#foot_0">1</ref>Given s e , we employ a random walk on the Euclidean nearest neighbor graph G to measure the manifold similarity s m <ref type="bibr" target="#b67">[69]</ref>. The graph has Y as nodes. It is is undirected, weighted, represented by sparse symmetric adjacency matrix A = (a ij ) ‚àà R n√ón . Edges correspond to reciprocal k-nearest neighbors, with weights</p><formula xml:id="formula_0">a ij = s e (y i , y j ), if y i ‚àà NN e k (y j ) ‚àß y j ‚àà NN e k (y i ) 0,</formula><p>otherwise.</p><p>(1) There are no loops in the graph, i.e., the diagonal elements of A are zero. Starting from an arbitrary vector f (0) ‚àà R n , the random walk for a given feature y i ‚àà Y follows the iteration f</p><formula xml:id="formula_1">(t) i = Œ± √Çf (t‚àí1) + (1 ‚àí Œ±)e i ,<label>(2)</label></formula><p>where e i is the i-th column of an n √ó n identity matrix, √Ç = D ‚àí1/2 AD ‚àí1/2 with D = diag(A1) being the degree matrix, and Œ± ‚àà [0, 1). Iteration (2) converges to the solution f ‚ãÜ i of the linear system</p><formula xml:id="formula_2">(I ‚àí Œ± √Ç)f = (1 ‚àí Œ±)e i .<label>(3)</label></formula><p>Following Iscen et al. <ref type="bibr" target="#b20">[21]</ref>, we use the conjugate gradient method to solve this system efficiently in practice, since I ‚àí Œ± √Ç is positive-definite. We define the manifold similarity</p><formula xml:id="formula_3">s m (y i , y j ) = f ‚ãÜ i (j),<label>(4)</label></formula><p>i.e., the j-th element of</p><formula xml:id="formula_4">f ‚ãÜ i . Observe that s m is symmet- ric because in fact s m (y i , y j ) is the (i, j)-element of ma- trix (1 ‚àí Œ±)(I ‚àí Œ± √Ç) ‚àí1 ,</formula><p>which is symmetric. This matrix is dense but we never compute it; we only compute its columns as needed. For instance, given y i ‚àà Y , its manifold nearest neighbors NN m k (y i ) are the k maximum elements of the i-th column.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Manifold-guided selection of training samples</head><p>We are guided by the manifold similarity to select training samples. In particular, we exploit the differences between Euclidean and manifold nearest neighbors of anchor items. We first describe the selection of positives and negatives given an anchor. Then we discuss anchor selection.</p><p>Positives. Given an anchor item x r and the corresponding feature y r = g(x r ), which is used as query, we choose as positives the items that correspond to the manifold nearest neighbors of y r that are not Euclidean neighbors. Such difference provides evidence of a matching item that is not retrieved well in the feature space, as illustrated in Figure <ref type="figure" target="#fig_0">1(c</ref>). In the embedding space, positives should be attracted to the anchor so that Euclidean and manifold neighbors agree.</p><p>We therefore simply compare the sets NN m k (y r ) and NN e k (y r ) and select the input items that correspond to their set difference</p><formula xml:id="formula_5">P + (x r ) = {x ‚àà X : g(x) ‚àà NN m k (y r ) \ NN e k (y r )} (5)</formula><p>as the positive pool of anchor x r . The value of k controls the visual diversity of positives, with larger values giving the harder examples. In practice, we maintain the pool ordered by descending manifold similarity, so that we may truncate by keeping the examples with highest confidence. Mining hard positive examples, in contrast to negatives, is not common. Apart from positives being fewer than negatives, this is due to the large intraclass variability; it may result in positives that are too hard to learn. It can be achieved in cases with known geometry of the scene such that extreme cases are avoided <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b41">42]</ref>. In our case, the hardness is controlled by the manifold similarity according to the current model g, so drifting into very tough examples is less likely.</p><p>Negatives. Similarly, and as illustrated in Figure <ref type="figure" target="#fig_0">1</ref>(d), we choose as negatives the items that correspond to the Euclidean nearest neighbors of y r that are not manifold neighbors. Such difference provides evidence of a non-matching item that is too close in the feature space. In the embedding space, positives should be repelled from the anchor. The negative pool of anchor x r is defined accordingly as</p><formula xml:id="formula_6">P ‚àí (x r ) = {x ‚àà X : g(x) ‚àà NN e k (y r ) \ NN m k (y r )}.<label>(6)</label></formula><p>It is common practice, and known to be beneficial <ref type="bibr" target="#b48">[49]</ref>, to select hard negative examples. By construction, its size is controlled by k. Again, we maintain the pool ordered by descending Euclidean similarity to keep the hardest negative examples.</p><p>Anchors. We are interested in anchors that have many relevant images in the collection, which facilitates propagating on the manifold and discovering differences with the Euclidean neighborhood. We are also interested in anchors that are diverse, so that there is as little redundancy during training. Both conditions are satisfied by the modes of the nearest neighbor graph G, which we compute as follows.</p><p>We first compute the stationary probability distribution œÄ of a random walk <ref type="bibr" target="#b7">[8]</ref> on G. This is achieved by the power iteration method <ref type="bibr" target="#b27">[28]</ref> yielding the leading left eigenvector of the transition matrix P = D ‚àí1 A, such that œÄP = œÄ. The probability reflects the importance of each node in the graph, as expressed by the probability of a random walker visiting it. We find the local maxima of the stationary distribution on G and out of those, we keep a fixed number having the maximum probability. This is defined as the anchor set A. This method has been previously used for image graph visualization <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training</head><p>The complete training pool P is the union of the anchor set A and the positive and negative pools P + (x), P ‚àí (x) for each x ‚àà A. We follow the common practice in metric learning and train a network with two or three branches and use contrastive or triplet loss, respectively. All branches share weights, while the particular network architecture is application specific and is discussed in Section 4.</p><p>In both cases of contrastive or triplet loss, we form training tuples of one anchor x r ‚àà A, one positive x + ‚àà P + (x r ), and one negative item x ‚àí ‚àà P ‚àí (x r ) . At each epoch, the embedding z = f (x; Œ∏) for each x ‚àà P, where Œ∏ is the current set of parameters. For each anchor x r , a positive item x + is drawn at random from its positive pool, and one negative x ‚àí is drawn at random from a subset of its negative pool. This subset consists of the items corresponding to the Euclidean nearest neighbors of z r = f (x r ; Œ∏) in the embedding space. Thus, while the manifold neighbors and the training pool are computed once at the beginning, hard negative sampling uses the current network representation. Finally, the training set for this epoch is the set of such tuples (x r , x + , x ‚àí ).</p><p>Given a tuple (x r , x + , x ‚àí ), we compute the embeddings z r = f (x r ; Œ∏) z + = f (x r ; Œ∏) and z ‚àí = f (x ‚àí ; Œ∏), and use the constrastive loss <ref type="bibr" target="#b15">[16]</ref>, combining one positive and one negative pair in a single input,</p><formula xml:id="formula_7">l c (z r , z + , z ‚àí ) = z r ‚àí z + 2 + [m ‚àí z r ‚àí z ‚àí ] 2 + ,<label>(7)</label></formula><p>or the triplet loss <ref type="bibr" target="#b60">[62]</ref> </p><formula xml:id="formula_8">l t (z r , z + , z ‚àí ) = [m + z r ‚àí z + 2 ‚àí z r ‚àí z ‚àí ] 2 + ,<label>(8)</label></formula><p>where [‚Ä¢] + denotes the positive part and m is a margin parameter. We also consider a weighted variant of both loss functions, where the loss is multiplied by the manifold similarity s m (x r , x + ) of the positive sample to the anchor. Thus we down-weigh the contribution of the tuples where the positive sample is too hard. Of course, given the positive and negative pools, there are many more possibilities in sampling positives and negatives and forming losses that are functions of more than two or three items <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b65">67]</ref>. It is also possible to iterate our approach, updating the graph and the pools based on the current embedding space, updating the embeddings,</p><formula xml:id="formula_9">x r P + (x r ) NN e 3 (x r ) P ‚àí (x r ) X \ NN e 3 (x r )</formula><p>Figure <ref type="figure">2</ref>. Sample CUB200-2011 anchor images (x r ), positive images from our method (P + (x r )) and baseline (NN e 3 (x r )), and negative images from our method (P ‚àí (x r )) and baseline (X \ NN e 3 (x r )). The baseline Euclidean nearest neighbors and non-neighbors <ref type="bibr" target="#b15">[16]</ref>. Positive (negative) ground-truth framed in green (red). Labels are only used for qualitative evaluation and not during training. and so on. In this case, given current parameters Œ∏, the set Z = {f (x; Œ∏) : x ‚àà X} plays the role of Y for the following iteration. This alternating training scheme is common for methods involving a global dataset structure like a graph or a set of clusters <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b66">68]</ref>. Our idea is orthogonal to most concurrent improvements on metric learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Applications</head><p>We apply the proposed method to learn visual representations on two different tasks: fine-grained categorization <ref type="bibr" target="#b64">[66,</ref><ref type="bibr" target="#b17">18]</ref> and instance-based image retrieval <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b41">42]</ref>. In both cases, both the features Y and the initial model f (‚Ä¢, Œ∏) are based on a pre-trained model. We assume there are no labeled images available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Fine-grained categorization</head><p>We use the CUB200-2011 dataset <ref type="bibr" target="#b59">[61]</ref> comprising 200 bird species. The goal is to learn embeddings that better discriminate instances of the same class from instances of different classes. Following the setup of <ref type="bibr" target="#b35">[36]</ref>, the training set contains half (100) classes on which embedding is learned, while the rest are used for testing. Given a test query im-age, the remaining test images of the same species should be top-ranked w.r.t. Euclidean distance to the query.</p><p>All prior approaches are fully supervised and use the manually assigned labels of the training set. Our method is unsupervised, but otherwise we choose the same settings with the literature in our comparisons. In particular, we initialize the network by GoogLeNet <ref type="bibr" target="#b54">[55]</ref> as pre-trained on ImageNet and add a fully connected layer right after the average pooling layer, reducing the embedding dimensionality to d = 64. We perform training with the triplet loss using all training images as anchors, which is affordable due to the small size (6k images) of the training set.</p><p>Our initial features are formed by R-MAC <ref type="bibr" target="#b57">[58]</ref> on the last convolutional feature map of the pre-trained GoogLet-Net, right before the average pooling layer, aggregated over 3 input image scales and whitened. The feature set Y contains all such vectors y = g(x) for x in the entire training set X. In Figure <ref type="figure">2</ref> we show examples of anchors and subsets of their positive and negative pools. Despite the absence of labels and the challenges of fine-grained similarity, we achieve a very clean negative pool and a reasonably clean positive one. x r P + (x r ) NN e 3 (x r ) P ‚àí (x r ) X \ NN e 3 (x r )</p><p>Figure <ref type="figure">4</ref>. Sample anchor images (x r ), positive images from our method (P + (x r )) and baseline (NN e 3 (x r )), and negative images from our method (P ‚àí (x r )) and baseline (X \ NN e 3 (x r )). The baseline is Euclidean nearest neighbors and non-neighbors <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Particular object retrieval</head><p>Particular object retrieval differs from bird species classification in that images are instances of the same object and not of the same (sub-)category, i.e. the similarity is even more fine-grained than in bird species. Objects are less deformable, but there is extreme diversity in viewpoint, illumination conditions, occlusion and background clutter.</p><p>Methods trained with category-level labeling do not perform well <ref type="bibr" target="#b2">[3]</ref>, while the state-of-the-art approaches use direct geometric matching <ref type="bibr" target="#b14">[15]</ref> or Structure-from-Motion (SfM) <ref type="bibr" target="#b41">[42]</ref> to automatically mine matching and nonmatching image pairs. This is appropriate since the geometry of the scene is known, but the whole process assumes the existence of another computer vision system based on local descriptors, which is rather expensive <ref type="bibr" target="#b45">[46]</ref>.</p><p>To be comparable to the state-of-the-art fine-tuned MAC obtained through SfM by Radenovic et al. <ref type="bibr" target="#b41">[42]</ref>, we use similar training set and design choices. In particular, we start from the same set of 7M images, referred to as Flickr 7M in the following 2 , which is downloaded from Flickr with text tag queries. We limit the training set by randomly sampling 1M images. We initialize our network by VGG <ref type="bibr" target="#b49">[50]</ref> as pretrained on ImageNet and we fine-tune MAC representation with contrastive loss.</p><p>Our initial features are formed by R-MAC <ref type="bibr" target="#b57">[58]</ref> on the last convolutional feature map of the pre-trained VGG network. The feature set Y contains all such vectors y = g(x) for x in the entire training set X. We sample an anchor set A and construct the complete training pool P. Anchor selection is essential here, as using all images as queries would be very expensive. Compared to <ref type="bibr" target="#b41">[42]</ref>, our complete training pool is larger (50k vs 22k), however, we only choose 1k anchors per epoch vs 6k.</p><p>Examples of selected anchors are shown in Figure <ref type="figure" target="#fig_1">3</ref>. They usually correspond to popular locations frequently appearing in the dataset. Examples of training pools are shown in Figure <ref type="figure">4</ref>, comparing to a baseline where positives and negatives are Euclidean nearest neighbors and nonneighbors, respectively. The same baseline is used for comparisons in our experiments. The baseline positives contain only mild viewpoint and illumination changes, while negatives are random. On the contrary, our positives contain more challenging changes and very interesting negatives: different objects, which still look similar in one way or another. The training set variability, in terms of different objects and viewing conditions, seems a desirable property.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Implementation details</head><p>We always create graph G by considering k = 30 nearest neighbors in <ref type="bibr" target="#b0">(1)</ref>. Exact computation takes 80min on 12 CPU threads on the 1M retrieval training set. We use the Euclidean similarity function s e (x i , x j ) = [x ‚ä§ i x j ] 3 + and Œ± = 0.99 following Iscen et al. <ref type="bibr" target="#b20">[21]</ref>. In order to create the positive pool we consider 50 neighbors in <ref type="bibr" target="#b4">(5)</ref>, while for the negative pool we use 100 and 10,000 neighbors in <ref type="bibr" target="#b5">(6)</ref> for fine-grained categories and retrieval, respectively, due to the different size of the training dataset. We finally restrict the negative pool of each anchor to have 50 instances at most. All vector representations used for an image in the feature and the embedding space are ‚Ñì 2 -normalized.</p><p>We use stochastic gradient descent with momentum for optimization. The learning rate is initialized at 10 ‚àí2 , and scaled by 0.1 every 10 epochs. The momentum parameter is 0.9. The margin m is set to 0.5 for triplet loss in finegrained categorization, and to 0.7 for contrastive loss in particular object retrieval. The batch size includes 42 triplets for fine-grained categories <ref type="bibr" target="#b35">[36]</ref> and pairs for 5 anchors in the retrieval application <ref type="bibr" target="#b41">[42]</ref>. We train for 100 epochs on fine-grained categorization and 30 epochs on particular object retrieval experiments. 2 Images depicting buildings that are part of the Oxford5k or Paris6k test set are removed as in <ref type="bibr" target="#b41">[42]</ref>.  <ref type="table">1</ref>. Impact of choices of anchors and pools of positive and negative examples on Recall@1 on CUB-200-2011 and mAP on Oxford5k. On CUB, all images are used as anchors, while on Oxford5K anchors are selected either at random or by the proposed method (A). The positive and negative pools are formed by either the baseline with Euclidean nearest neighbors (NN e 5 ) <ref type="bibr" target="#b15">[16]</ref> or our selection (P + and P ‚àí ), optionally with our weighted loss (+W).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Labels R@1 R@2 R@4 R@8 NMI  <ref type="table">2</ref>. Recall@k and NMI on CUB-200-2011. All methods expect for ours and cyclic match <ref type="bibr" target="#b29">[30]</ref> use ground-truth labels during training.</p><formula xml:id="formula_10">Initial</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>Fine-grained categorization is evaluated on CUB200-2011 <ref type="bibr" target="#b59">[61]</ref>, where we use the training set without labels for training and then evaluate on the test set, measuring Recall@k as well as clustering quality by NMI <ref type="bibr" target="#b30">[31]</ref>. Particular object retrieval is evaluated by mean average precision (mAP) on a challenging and diverse set of test datasets comprising landmark and building images (Oxford5k <ref type="bibr" target="#b39">[40]</ref> and Paris6k <ref type="bibr" target="#b40">[41]</ref>), natural landscapes (Holidays <ref type="bibr" target="#b23">[24]</ref>), as well as planar and 3D objects (Instre <ref type="bibr" target="#b61">[63]</ref>). Large scale experiments are performed on Oxford and Paris by adding 100k distractors <ref type="bibr" target="#b39">[40]</ref>, namely Oxford105k and Paris106k respectively. We first evaluate the importance of different components of the selection strategy and then compare our method against state-of-the-art on each task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Impact of positives, negatives, and anchors</head><p>We consider the unsupervised approach proposed by Hadsell et al. <ref type="bibr" target="#b15">[16]</ref> as a baseline method. It sets positives to be the 5 nearest neighbors with Euclidean distance. All other elements are considered negatives out of which we randomly draw the negative pool of an anchor. In this case, hard negative mining per epoch is impossible and random choice is the only choice. We present the results in Table  Table <ref type="table">3</ref>. mAP on particular object retrieval datasets. We compare VGG as pre-trained on ImageNet, the fine-tuned network of Radenovic et al. <ref type="bibr" target="#b41">[42]</ref>, and our fine-tuned one. Fine-tuning is always performed for MAC, but at testing we evaluate both global MAC and regional R-MAC <ref type="bibr" target="#b57">[58]</ref> representations.</p><p>without any supervision, while the weighted loss consistently helps. Furthermore, we observe that our hard negatives are beneficial and necessary, while our anchors are essential for the large scale training set. Given popular anchors, even simple nearest neighbors work well as positives. However, our harder positives further improve. CUB's annotation allows us to measure the true positive and true negative rate in our positive and negative pools. These measurements are 40% and 96%, respectively. Additionally, we exploit the labels and train with a positive (negative) oracle, where we replace our positive (resp. negative) pool with one based on labels, yielding 46.7 (resp. 36.5) Recall@1. This shows that our hard positives with weighting are almost as good as true positives and that our hard negatives are better than hard annotated negatives. The latter is due to errors in annotation of CUB dataset, quantified to 4.4% <ref type="bibr">[60]</ref>. In this case, hard negative mining frequently finds false negative examples, which are known to cause training failure <ref type="bibr" target="#b46">[47]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Comparisons on fine-grained categories</head><p>To our knowledge there is no other unsupervised method that evaluates on CUB200-2011 dataset. We evaluate the unsupervised approach of Li et al. <ref type="bibr" target="#b29">[30]</ref> by constructing the same graph as in our method, then using the provided code to construct the positive/negative pool and finally training the same way as in our method. We also compare to supervised methods that use ground-truth labels on the training set, but otherwise an identical experimental setup. As shown in in Table <ref type="table">2</ref>, our method competes or even outperforms fully supervised methods. We also outperform the only unsupervised competitor. Although their true positive rate is higher (76%), their positive pairs are mostly extremely similar and not challenging enough for training. Note that there are better-performing methods in the literature with sophisticated sampling schemes <ref type="bibr" target="#b65">[67]</ref>, which can be complementary to ours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Comparisons on retrieval</head><p>We initialize by VGG as pre-trained on ImageNet and fine-tune using MAC representation. At testing, the fine-tuned network is evaluated with both global MAC and regional R-MAC representations <ref type="bibr" target="#b57">[58]</ref>. This is the same process as <ref type="bibr" target="#b41">[42]</ref>, which makes it comparable in this respect, although the training set and sampling pool sizes are not the same as discussed in section 4.2.</p><p>Descriptor whitening is known to be essential for MAC and R-MAC. We follow the common practice in the literature and perform unsupervised PCA whitening <ref type="bibr" target="#b22">[23]</ref> for the pre-trained networks, and supervised LDA-based whitening for the fine-tuned networks, learned on a subset of Flickr 7M. In particular, as supervision we use SfM labels for the network of Radenovic et al. <ref type="bibr" target="#b41">[42]</ref>, and matching pairs consisting of NN m 50 per anchor for our method. As shown in Table <ref type="table">3</ref>, we improve over the pre-trained network on all test sets. Moreover, we outperform <ref type="bibr" target="#b41">[42]</ref> with only one exception on Oxford5k. Remarkably, we perform better even on building or landmark oriented test sets, while their method specifically favors this kind of images. With our training pool being more diverse, we improve on Holidays and Instre test sets, where <ref type="bibr" target="#b41">[42]</ref> shows little improvement or is even inferior to the pre-trained network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this work, we depart from using discrete category level labeling in order to learn fine grained similarity. Not only we avoid the expensive or nearly impossible manual annotation, but also do not restrict the problem to supervised classification. Our unsupervised and manifold-aware sampling of training data is applied to perform metric learning. The learning attracts points that lie on the same manifold and repels points on different manifolds. The method is conceptually simple and applicable with standard contrastive and triplet loss. It is shown to be effective for fine-grained categorization and particular object retrieval, competing or surpassing fully supervised approaches.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Given an anchor point (black) and its k nearest Euclidean (NN e k ) and manifold (NN m k ) neighbors in a dataset, we choose positive samples as NN m k \ NN e k , and negative as NN e k \ NN m k . Data is unlabeled and the selection is fully unsupervised, including anchors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Examples of anchor images selected by the proposed method for the image retrieval application. Random samples from the top 100 (1000) anchors are shown in the top (bottom) row according to the node importance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>. Our method improves the pre-trained network in both tasks</figDesc><table><row><cell>Method</cell><cell cols="2">Representation Labels</cell><cell cols="6">Oxford5k Oxford105k Paris6k Paris106k Holidays Instre</cell></row><row><cell>Pre-trained [58]</cell><cell></cell><cell>ImageNet</cell><cell>58.5</cell><cell>50.3</cell><cell>73.0</cell><cell>59.0</cell><cell>79.4</cell><cell>48.5</cell></row><row><cell>CNN from BoW [42]</cell><cell>MAC</cell><cell>SfM</cell><cell>79.7</cell><cell>73.9</cell><cell>82.4</cell><cell>74.6</cell><cell>81.4</cell><cell>48.5</cell></row><row><cell>Ours</cell><cell></cell><cell>No</cell><cell>78.7</cell><cell>74.3</cell><cell>83.1</cell><cell>75.6</cell><cell>82.6</cell><cell>55.5</cell></row><row><cell>Pre-trained [58]</cell><cell></cell><cell>ImageNet</cell><cell>68.0</cell><cell>61.0</cell><cell>76.6</cell><cell>72.1</cell><cell>87.0</cell><cell>55.6</cell></row><row><cell>CNN from BoW [42]</cell><cell>R-MAC</cell><cell>SfM</cell><cell>77.8</cell><cell>70.1</cell><cell>84.1</cell><cell>76.8</cell><cell>84.4</cell><cell>47.7</cell></row><row><cell>Ours</cell><cell></cell><cell>No</cell><cell>78.2</cell><cell>72.6</cell><cell>85.1</cell><cell>78.0</cell><cell>87.5</cell><cell>57.7</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">In fact, se can be any symmetric function but is only a function of two elements of Y; while sm is a function of two elements in Y but also of the entire set Y .</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments The authors were supported by the MSMT LL1303 ERC-CZ grant.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Netvlad: Cnn architecture for weakly supervised place recognition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Arandjeloviƒá</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gronat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Soundnet: Learning sound representations from unlabeled video</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural codes for image retrieval</title>
		<author>
			<persName><forename type="first">A</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Slesarev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chigorin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Semisupervised learning for convolutional neural networks via online graph construction</title>
		<author>
			<persName><forename type="first">S.-Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agethen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-H</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hsu</surname></persName>
		</author>
		<idno>arXiv</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Deep unsupervised similarity learning using partially ordered sets</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Bautista</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sanakoyeu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
		<idno>arXiv</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">CliqueCNN: Deep unsupervised exemplar learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Bautista</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sanakoyeu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tikhoncheva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Laplacian eigenmaps for dimensionality reduction and data representation. Neural computation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mode-seeking on graphs via random walks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi-task self-supervised visual learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Polysemous codes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained partbased models</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. PAMI</title>
		<imprint>
			<biblScope unit="page">32</biblScope>
			<date type="published" when="2002">September 2010. 2</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Self-supervised learning of visual features through embedding images into text topic spaces</title>
		<author>
			<persName><forename type="first">L</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rusinol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deep image retrieval: Learning global representations for image search</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Almazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2016. 1, 2, 3, 5, 6</date>
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2006">2006. 2, 4, 5, 6, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">MatchNet: Unifying feature and metric learning for patchbased matching</title>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Smart mining for deep metric learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Harwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kumar B G</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Drummond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2007">2017. 1, 2, 3, 5, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fast spectral ranking for similarity search</title>
		<author>
			<persName><forename type="first">A</forename><surname>Iscen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Avrithis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Furon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Efficient diffusion on region manifolds: Recovering small objects with compact cnn representations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Iscen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Avrithis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Furon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning visual groups from co-occurrences in space and time</title>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
		<idno>arXiv</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Negative evidences and cooccurrences in image retrieval: The benefit of PCA and whitening</title>
		<author>
			<persName><forename type="first">H</forename><surname>J√©gou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2012-10">October 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hamming embedding and weak geometric consistency for large scale image search</title>
		<author>
			<persName><forename type="first">H</forename><surname>J√©gou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2007">October 2008. 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Aggregating local descriptors into compact codes</title>
		<author>
			<persName><forename type="first">H</forename><surname>J√©gou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>S√°nchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>P√©rez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Trans. PAMI</title>
				<imprint>
			<date type="published" when="2002">September 2012. 2</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning local image descriptors with deep siamese and triplet convolutional networks by minimising global loss functions</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deeper inside PageRank</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Langville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Meyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Internet Mathematics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="335" to="380" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Quadruplet-wise image similarity learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by graph-based consistent constraints</title>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2008">2016. 2, 7, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Introduction to Information Retrieval</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sch√ºtze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Nearest neighbour radial basis function solvers for deep neural networks</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Harwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Drummond</surname></persName>
		</author>
		<idno>arXiv, 2017. 1</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Working hard to know your neighbor&apos;s margins: Local descriptor learning loss</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mishchuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Radenovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">No fuss distance metric learning using proxies</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Movshovitz-Attias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep metric learning via lifted structured feature embedding</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Oh</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2007">2016. 1, 2, 4, 5, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Parametric manifold learning via sparse multidimensional scaling</title>
		<author>
			<persName><forename type="first">G</forename><surname>Pai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Talmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kimmel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.06011</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep face recognition</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Fisher kernels on visual vocabularies for image categorization</title>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Dance</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2002">June 2007. 2</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Object retrieval with large vocabularies and fast spatial matching</title>
		<author>
			<persName><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2007-06">June 2007. 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Lost in quantization: Improving particular object retrieval in large scale image databases</title>
		<author>
			<persName><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2007">June 2008. 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">CNN image retrieval learns from bow: Unsupervised fine-tuning with hard examples</title>
		<author>
			<persName><forename type="first">F</forename><surname>Radenoviƒá</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2016. 1, 2, 3, 4, 5, 6, 7, 8</date>
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Visual instance retrieval with deep convolutional networks</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Maki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ITE Transactions on Media Technology and Applications</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="251" to="258" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Metric learning with adaptive density discrimination</title>
		<author>
			<persName><forename type="first">O</forename><surname>Rippel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<idno>arXiv</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Think globally, fit locally: unsupervised learning of low dimensional manifolds</title>
		<author>
			<persName><forename type="first">L</forename><surname>Saul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roweis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="119" to="155" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">From single image query to detailed 3d reconstruction</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Schonberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Radenovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="5126" to="5134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Discriminative learning of deep convolutional feature point descriptors</title>
		<author>
			<persName><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ferraz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Fracking deep convolutional image descriptors</title>
		<author>
			<persName><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ferraz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<idno>arXiv, 2014. 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition. ICLR</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Video Google: A text retrieval approach to object matching in videos</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Improved deep metric learning with multi-class n-pair loss objective</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1857" to="1865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Deep metric learning via facility location</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning and example selection for object and pattern detection</title>
		<author>
			<persName><forename type="first">K.-K</forename><surname>Sung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MIT</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A global geometric framework for nonlinear dimensionality reduction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">5500</biblScope>
			<biblScope unit="page" from="2319" to="2323" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">To aggregate or not to aggregate: Selective match kernels for image search</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Avrithis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>J√©gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2013-12">December 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Particular object retrieval with integral max-pooling of cnn activations</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sicre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>J√©gou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>ICLR</publisher>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Building a bird recognition app and large scale dataset with citizen scientists: The fine print in fine-grained dataset collection</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Farrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Barry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ipeirotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2015">2016. 2015</date>
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">The caltech-ucsd birds-200-2011 dataset</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">5</biblScope>
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Learning fine-grained image similarity with deep ranking</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Instre: a new benchmark for instancelevel object retrieval and recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">37</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations using videos</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Transitive invariance for selfsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<idno>arXiv</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Mining discriminative triplets of patches for fine-grained classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Sampling matters in deep embedding learning</title>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kr√§henb√ºhl</surname></persName>
		</author>
		<idno>arXiv</idno>
		<imprint>
			<date type="published" when="2008">2017. 2, 4, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Iterative manifold embedding layer learned by incomplete data for large-scale image retrieval</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Ranking on data manifolds</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sch√∂lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
