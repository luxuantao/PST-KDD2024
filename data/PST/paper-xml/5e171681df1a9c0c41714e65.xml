<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Rashmika</forename><surname>Nawaratne</surname></persName>
							<email>b.nawaratne@latrobe.edu.au</email>
						</author>
						<author>
							<persName><forename type="first">)</forename><forename type="middle">R</forename><surname>Nawaratne</surname></persName>
						</author>
						<author>
							<persName><forename type="first">D</forename><surname>Alahakoon</surname></persName>
							<email>d.alahakoon@latrobe.edu.au</email>
						</author>
						<author>
							<persName><forename type="first">D</forename><surname>De Silva</surname></persName>
							<email>d.desilva@latrobe.edu.au</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">are with Centre for Data Analytics</orgName>
								<orgName type="institution">Cognition at La Trobe University</orgName>
								<address>
									<postCode>3083</postCode>
									<settlement>Melbourne</settlement>
									<region>VIC</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Engineering</orgName>
								<orgName type="institution">RMIT University</orgName>
								<address>
									<settlement>Melbourne VIC 3001</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DCD13977DF5D106C8102FB44C523EA43</idno>
					<idno type="DOI">10.1109/TII.2019.2938527</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TII.2019.2938527, IEEE Transactions on Industrial Informatics received July 28, 2018; revised November 6, 2018, March 11, 2019, and August 16, 2019; accepted August 24, 2019.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Unsupervised Learning</term>
					<term>Anomaly Detection</term>
					<term>Anomaly Localisation</term>
					<term>Deep Learning</term>
					<term>Active Learning</term>
					<term>Realtime Video Surveillance</term>
					<term>Spatiotemporal Analysis</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Rapid developments in urbanisation and autonomous industrial environments have augmented and expedited the need for intelligent real-time video surveillance. Recent developments in artificial intelligence for anomaly detection in video surveillance only address some of the challenges, largely overlooking the evolving nature of anomalous behaviours over time. Tightly-coupled dependence on a known normality training dataset and sparse evaluation based on reconstruction error are further limitations. In this paper, we propose the Incremental Spatio-Temporal Learner (ISTL) to address challenges and limitations of anomaly detection and localisation for real-time video surveillance. ISTL is an unsupervised deep learning approach that utilises active learning with fuzzy aggregation, to continuously update and distinguish between new anomalies and normality that evolve over time.</p><p>ISTL is demonstrated and evaluated on accuracy, robustness, computational overhead as well as contextual indicators, using three benchmark datasets. Results of these experiments validate our contribution and confirm its suitability for real-time video surveillance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>exponential increase in the deployment of Closed-circuit television (CCTV) camera systems <ref type="bibr" target="#b0">[1]</ref>. However, it is unrealistic and infeasible for human observers to monitor and analyse every video stream with high precision. Artificial Intelligence (AI) techniques for autonomous video surveillance reported in current literature can be categorised into video summarisation <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b3">[4]</ref>, object detection and reidentification <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b6">[7]</ref>, activity/behaviour detection <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b9">[10]</ref>, and anomaly detection <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>.</p><p>Anomaly detection is a constitutive task in autonomous video surveillance as it contributes to the success of the other categories noted above. It is also a complex task as the anomalies to be detected are not known prior, imposing difficulties even for a human observer. A general definition for anomaly detection is the identification of behaviours that do not conform to expected and accepted behaviour (i.e., normal behaviour) <ref type="bibr" target="#b12">[13]</ref>. In the context of autonomous video surveillance, anomaly detection is impacted by three primary challenges. Firstly, the computational complexity and cost of video data processing due to spatial and temporal dimensional structure combined with non-local temporal variations across video frames <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. As an example, anomalous objects such as vehicles/bicycles in a pedestrian walk must be identified using spatial processing whereas anomalous behaviour such as jaywalking must be determined using temporal variations across video frames. Secondly, the anomaly itself is ill-defined, the boundary between normal behaviour and anomalies is often imprecise, and anomalies are highly contextual <ref type="bibr" target="#b12">[13]</ref>. For example, industrial machinery operating at low power can be either normal or anomalous depending on the operational circumstances. Thirdly, what is considered as normal behaviour evolves over time making current knowledge incomplete and/or obsolete <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>. For instance, when offenders become aware of detected anomalies, they can maliciously adapt behaviours so that subsequent anomalies are difficult to detect.</p><p>Existing literature attempts to address the first and second challenges (i.e., computational complexity and identifying contextual anomalies). The third challenge, the evolving nature of normal behaviour over time, remains unaddressed, and this makes current knowledge of normality incomplete. In this paper, we propose the Incremental Spatio-Temporal Learner (ISTL), to address the aforementioned challenges and limitations. ISTL is a new anomaly detection approach for real-time video surveillance that actively learns spatiotemporal</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spatiotemporal Anomaly Detection using Deep</head><p>Learning for Real-time Video Surveillance patterns of normal behaviour as it evolves over time. ISTL is inspired by continuous learning process in human cognition and the paradigm of active learning. Inspired by the human brain, ISTL begins by developing a basic understanding from immediately available information to distinguish between normal (safe) and anomalous (unsafe) behaviours, and continuously refines this understanding as the surroundings change and new information becomes available <ref type="bibr" target="#b16">[17]</ref>. Active learning is primarily used for refinement and validation in ISTL, where a human observer contributes to the learning process for improved learning outcomes across iterations. The paradigm of active learning has been widely used in industrial image and video analysis applications such as character reading, facial recognition, autonomous vehicles and ecommerce <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>.</p><p>The research contributions of this research article are as follows:</p><p>1) A deep learning model that learns spatiotemporal patterns of normal behaviour for online anomaly detection and localization from a surveillance video stream.</p><p>2) The fuzzy aggregation of active learning outcomes into the continuous learning process for dynamic adaption to evolving behaviours of unknown/new normalities in the surveillance video stream.</p><p>3) Evaluation using two thresholds, anomaly threshold and temporal threshold, based on the context of the video surveillance feed, instead of sparse evaluation based solely on reconstruction error. Key features of ISTL are demonstrated and validated using three benchmark video surveillance datasets; UCSD Pedestrian datasets <ref type="bibr" target="#b19">[20]</ref> (Ped 1 and Ped 2) and CUHK Avenue dataset <ref type="bibr" target="#b20">[21]</ref>.</p><p>Section II reports related work. Section III presents the proposed ISTL approach, with descriptions of each phase and corresponding outcomes. Section IV presents evaluation of the ISTL approach for accuracy, robustness, low computational overhead and contextual indicators, across a range of values/scenarios for both anomaly and temporal thresholds. The paper concludes with Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Techniques and approaches for intelligent video surveillance in current literature broadly range across two areas of research, hand-crafted video features <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref> and learned-representations based on deep learning architectures <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>. In techniques that utilize hand-crafted features, trajectories and spatiotemporal changes are extracted as input/output features for computational and AI modelling. For instance, Xie et al. <ref type="bibr" target="#b25">[26]</ref> proposed a motion instability based anomaly detection framework that discriminates anomalous behaviour based on the direction randomness and motion intensity, whereas Wu et al. <ref type="bibr" target="#b26">[27]</ref> proposed an approach in which objects are classified as anomalous based on how they follow the learned normal trajectory. These trajectory-based methods define normal behaviour based on previously observed motion patterns. However, such trajectory-based methods fail to detect anomalous behaviour based on the appearance of entities in the surveillance video stream and computationally expensive for crowded scenes. State-of-the-art handcrafted feature extraction methods describe video events ranging from pixellevel to 3-dimensional cuboid. For instance Zhao et al. <ref type="bibr" target="#b27">[28]</ref> utilize histogram of gradient (HoG) and histogram of optical flow (HoF) along spatial and temporal dimensions to encode an event and learn the normality upon dynamic sparse coding, whereas, Zaharescu et al. <ref type="bibr" target="#b28">[29]</ref> models the normal behaviour based on distributions of spatiotemporal oriented energy. These handcrafted feature based techniques can accurately model both spatial and temporal dynamics, however, they require prior knowledge for the design of effective features, and are time consuming to extract, thereby impractical to use in real-time anomaly detection.</p><p>With the advancements of deep learning, convolution neural networks (CNN), autoencoders and recurrent neural networks (RNN) have been utilized for video anomaly detection <ref type="bibr" target="#b23">[24]</ref>. Xu et al. <ref type="bibr" target="#b11">[12]</ref> proposed Appearance and Motion DeepNet (AMDN) that utilizes an autoencoder to automatically learn feature representation from the surveillance video, use a double fusion framework and support vector machine (SVM) models to predict the irregularity of an event. The AMDN model results in the state-of-the-art accuracy, however, its processing time is in the order of 10,000 milliseconds, which makes it impractical to use in online anomaly detection. Hasan et al. <ref type="bibr" target="#b24">[25]</ref> approached the problem of anomaly detection by learning a generative model for regular motion patterns. The approach achieved positive results using a 10-layered fully convolutional feed-forward autoencoder to reconstruct input video, then detect anomalies based on its reconstruction cost analysis. Luo et al. <ref type="bibr" target="#b29">[30]</ref> attempted to detect anomalies by leveraging a CNN for appearance encoding and a convolutional long short-term memory (ConvLSTM) for remembering history of the motion information. Recently, Vu et al. <ref type="bibr" target="#b30">[31]</ref> proposed an anomaly detection approach using a deep generative network in which normality is modelled by an unsupervised probabilistic framework. With these advancements, it is evident that learned-representations based on deep learning architectures have the ability to distinguish anomalies from normal behaviours by processing high-dimensional surveillance video streams. However, existing deep learning approaches for anomaly detection are highly dependent on a known normality dataset for training and constrained by sparse evaluation based only on reconstruction error, without consideration for surveillance context.</p><p>In summary, current literature is mostly limited to addressing the computational complexity of processing high </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. THE INCREMENTAL SPATIOTEMPORAL LEARNER (ISTL) APPROACH</head><p>A high-level overview of ISTL is illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>. First, the live video surveillance feed is presented as input to spatiotemporal model training of normal behaviour (from time t0 to tu). Second, the trained model is utilized for anomaly detection and localization within the time interval tu to tv. Third, the detected anomalies are validated by human observer and the validation input is used to construct updated normal behaviour using fuzzy aggregation. This updated normal behaviour is fed back into the ISTL learning model for continuously learning.</p><p>This overview is expanded into a functional view and illustrated in Fig. <ref type="figure" target="#fig_1">2</ref>.</p><p>The computational formulation of anomaly detection in video surveillance is presented as follows. The training video stream (Xtrain) composed of a sequence of frames of height h and width w, Xtrain ⊂ R, that only contains video frames exhibiting normal behaviour in a given camera view. R indicates all the video frames of the camera view in real world. In the testing phase, a video stream (Xtest) is employed, where Xtest ⊂ R contains video frames of both normal and anomalous behaviour. The goal is to learn a representation (Ω) of normal behaviour from Xtrain which is subsequently validated with Xtest to distinguish anomalies. In contrast to previous work <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b31">[32]</ref>- <ref type="bibr" target="#b33">[34]</ref>, which requires a complete training dataset of normal behaviour, the ISTL approach will actively update previously learned knowledge (Ω) based on (i) spatiotemporal information from continuously received video streams, and (ii) active human observer feedback on detected anomalies.</p><p>The three phases of ISTL, 1) Spatiotemporal Learning, 2) Anomaly Detection and Localization, 3) Active Learning with Fuzzy Aggregation, are explicated in following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Spatiotemporal Learning</head><p>Spatiotemporal representation of normal behaviour is learned from Xtrain as expected and acceptable behaviour for the video surveillance application. The ISTL model is composed of a spatiotemporal autoencoder to learn the appearance and motion representation from video inputs. The autoencoder is an unsupervised learning algorithm that employs backpropagation to set the target values to be equal to the inputs by minimizing the reconstruction error <ref type="bibr" target="#b34">[35]</ref>. In the proposed architecture, the spatiotemporal autoencoder consists of a series of CNN layers to learn the spatial representation and a series of ConvLSTM layers to learn the temporal representation. The input data layer and feature transformation layers of the autoencoder are described in following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Input Data Layer</head><p>The raw video data are pre-processed to enhance the learning capacity of the spatiotemporal autoencoder model. At first, the video data are extracted as consecutive frames, convert into grayscale to reduce the dimensions, resize to 224 x 224 pixels and normalize pixel values by scaling between 0 and 1. The input to the spatiotemporal autoencoder model is a temporal cuboid of video frames, which will be extracted using a sliding window of length T without any feature transformation. The consecutive frames of length T are stacked together to construct the input temporal cuboid. Increased length of this temporal window (T) will enable to incorporate motion of longer length, however, the larger the T, the model convergence will take exponential time <ref type="bibr" target="#b24">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Convolution Layers (CNN)</head><p>CNNs have been inspired from biological processes resembling the organization of the animal visual cortex <ref type="bibr" target="#b35">[36]</ref>. The connectivity of the neurons in the convolution layers are designed in a manner similar to animal vision system such that an individual cortical neuron responds to stimuli only in a confined region of the input frame, i.e., the receptive field. In video analysis, the convolution layers can preserve the spatial relationship within the input frames by learning feature representations using filters, whose values are learned during the training process. The ISTL model consists of two convolution layers and two de-convolution layers, whose filters and kernel sizes are specified in the Table <ref type="table" target="#tab_0">I</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Convolutional LSTM Layers (ConvLSTM)</head><p>Recurrent neural network (RNN) captures the dynamic temporal behaviour of a time-sequence input data by As LSTM is primarily developed and utilized for modeling long-range temporal correlations, it has a drawback in handling spatial data as spatial information is not encoded in its state transition. However, it is essential to learn the temporal regularity from the surveillance video stream while preserving the spatial structure, particularly for anomaly detection. Therefore, we utilize an extension to LSTM, convolutional LSTM (ConvLSTM) <ref type="bibr" target="#b36">[37]</ref>, in which both the input-to-state and state-to-state transitions have convolution structures. The ConvLSTM overcome this drawback by designing its inputs, hidden states, gates and cell outputs as 3D tensors, whose last dimension is the spatial dimension. Further, the matrix operations in its inputs and gates are replaced with convolution operator. With these modifications, the ConvLSTM is able to capture the spatiotemporal features from the input frame sequences. The ConvLSTM model is represented in the equations ( <ref type="formula">1</ref>)-( <ref type="formula">5</ref>).</p><p>𝑖 𝑡 = 𝜎 (𝑊 𝑥𝑖 * 𝑋 𝑡 + 𝑊 ℎ𝑖 * 𝐻 𝑡-1 + 𝑊 𝑐𝑖 ∘ 𝐶 𝑡-1 + 𝑏 𝑖 )</p><p>(1)</p><formula xml:id="formula_0">𝑓 𝑡 = 𝜎 (𝑊 𝑥𝑓 * 𝑋 𝑡 + 𝑊 ℎ𝑓 * 𝐻 𝑡-1 + 𝑊 𝑐𝑓 ∘ 𝐶 𝑡-1 + 𝑏 𝑓 ) (2) 𝐶 𝑡 = 𝑓 𝑡 ∘ 𝐶 𝑡-1 + 𝑖 𝑡 ∘ 𝑡𝑎𝑛ℎ(𝑊 𝑥𝑐 * 𝑋 𝑡 + 𝑊 ℎ𝑐 * 𝐻 𝑡-1 + 𝑏 𝑐 ) (3) 𝑜 𝑡 = 𝜎 (𝑊 𝑥𝑜 * 𝑋 𝑡 + 𝑊 ℎ𝑜 * 𝐻 𝑡-1 + 𝑊 𝑐𝑜 ∘ 𝐶 𝑡-1 + 𝑏 𝑜 ) (4) 𝐻 𝑡 = 𝑜 𝑡 ∘ tanh(𝐶 𝑡 ) (5)</formula><p>In the equations, '*' and '∘' represents convolution operation and Hadamard product respectively. Inputs are represented by Xi, …, Xt, the cell states are represented by Ci, …, Ct, the hidden states are represented by Hi, …, Ht, and the gates it, ft and ot are all 3D tensors. 'σ' is the sigmoid function and, Wx~ and Wh~ are 2D convolution kernels in the ConvLSTM. The ISTL model consists of three ConvLSTM layers. The spatiotemporal autoencoder architecture is illustrated in Fig. <ref type="figure" target="#fig_3">3</ref> and its composition further elaborated in Table <ref type="table" target="#tab_0">I</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Anomaly Detection and Localization</head><p>The ISTL model can be used to obtain a reconstruction of the normality of the input video at pixel-level precision.</p><p>However, the trained autoencoder does not have the ability to accurately reconstruct the anomalous or unseen scenes, due to the fact that, such scenes have not been presented in the training phase. This phenomenon is used to evaluate and detect anomalies from the input video. We obtain the reconstruction error (E) as the square root of the sum of the squared vector values, as represented in equation ( <ref type="formula">6</ref>) and ( <ref type="formula">7</ref>), where X is the input temporal cuboid, X̅ is the reconstructed temporal cuboid, T is the time window, w is the width and h is the height of the video frame. The reconstruction error represents the score for each temporal cuboid defining the anomaly. We define a reconstruction error threshold to distinguish between normal behaviour and anomalies, named anomaly threshold (µ). In practical video surveillance applications, the human observer can select a value for µ based on the sensitivity required for the surveillance application. A low µ would result in higher sensitivity to the surveillance arena, resulting in higher number of alerts. A high µ would result in lesser sensitivity that could lead to miss sensitive anomalies in the surveillance arena.</p><p>Additionally, we introduce the temporal threshold (λ), which we define as the number of video frames that should be higher than the µ to recognize an event as an anomaly. λ is employed to reduce the false-positive anomaly alerts due to sudden variations of the surveillance video stream due to occlusion, motion blur and high-intense lighting conditions. Fig. <ref type="figure" target="#fig_4">4</ref> illustrates anomaly detection approach based on the reconstruction error.</p><p>Anomaly localization locates the specific area of the video frame, where an anomaly has occurred. Subsequent to   detecting a segment of the video as anomalous, we localize the anomalies by calculating reconstruction error (Ec) over nonoverlapping spatiotemporal local cuboid windows, where m and n are the width and height respectively, and T is the depth (i.e., number of frames in the cuboid). Equation ( <ref type="formula">7</ref>) is used to calculate the Ec for local cuboids.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Active Learning with Fuzzy Aggregation</head><p>The purpose of the active learning in practical video surveillance context is to enable anomaly detection of dynamically evolving environments. By automating the anomaly detection using the deep learning model discussed in Section IIIA and Section IIIB, we train the learning model to identify accepted normal behaviour provided at the beginning. However, in dynamic environments comprising of new normal behaviour that have not anticipated and/or existing behaviour that considered abnormal reformed to normal, it is important that the detection system evolves with capabilities for detecting such new scenarios. ISTL addresses this challenge by adopting an active learning approach using fuzzy aggregation to continuously train the learning model with unknown/new normal behaviour specific to the corresponding surveillance context. This approach is inspired by the human brain's ability to develop a basic understanding which is continuously refined as new information becomes available <ref type="bibr" target="#b16">[17]</ref>.</p><p>ISTL is initially trained with a pre-identified normal behaviour in the surveillance context and used for anomaly detection. If a video frame is detected as an anomaly, i.e., the reconstruction error of the input cuboid is above the anomaly threshold, the input cuboid is classified as an anomaly. The classified frames are then sent to a human observer for verification. The objective of human observer feedback is to actively feed the learning model with dynamically evolving normality behaviour. Therefore, if a detected video frame is an incorrect detection (false positive), then the human observer can mark the video frame as 'normal', which will be used in the continuous learning phase.</p><p>Subsequent to human observer feedback, the video frames that were marked as normal will be used to continuously train the ISTL model, updating its knowledge of the notion of normality. As shown in Fig. <ref type="figure" target="#fig_5">5</ref>, continuous update of the ISTL model is conducted using (i) spatiotemporal information from the continuously received surveillance video stream, and (ii) active human observer feedback on detected anomalies.</p><p>The continuous learning of the ISTL model is enriched by fuzzy aggregation of video frames, in order to retain stability across iterations of learning. At the detection phase, all the video frames being evaluated are tagged with a fuzzy measure gλ based on its reconstruction error and grouped into finite number (n) of sets based on gλ. Subsequently, in the continuous learning phase, the algorithm will select the k video frame cuboids that contain highest gλ from each set of fuzzy measures (S) to train the ISTL model. The parameters k and n are defined at initiation based on the duration of video surveillance stream employed for continuous learning. The scene selection for continuous training is defined by the equation ( <ref type="formula">8</ref>); ∀s∈S, where, S = {s1, s2, … sn} and d is the indexes of the selected temporal cuboids that will be included in the continuous training dataset.</p><formula xml:id="formula_1">𝑑 = ∑ max 𝑗=[1,𝑘] (𝑠 𝑖 ) 𝑛 𝑖=0 (8)</formula><p>The dataset for continuous training iteration is now composed of (i) false positive detection verified by the human observer, and (ii) temporal cuboids selected across normal behaviour using the fuzzy aggregation. This will ensure the continuous training will update the detection model's capability to capture novel normal behaviour while remaining stable for previously known normal behaviour. This fuzzy aggregation approach has been successfully demonstrated to maintain stability-plasticity in continuous learning for IoT stream mining <ref type="bibr" target="#b37">[38]</ref>, text mining <ref type="bibr" target="#b16">[17]</ref> and video stream mining <ref type="bibr" target="#b23">[24]</ref>.</p><p>Subsequent to the scene selection, the ISTL model will be continuously trained upon the selected representation from input video data, which is the updated expected and acceptable behaviour form the surveillance arena. Thenceforth, the updated ISTL model will be re-employed for anomaly detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EVALUATION OF THE ISTL APPROACH</head><p>The proposed approach, ISTL, is evaluated using three benchmark datasets, CUHK Avenue dataset <ref type="bibr" target="#b20">[21]</ref>, UCSD Ped 1 and UCSD Ped 2 datasets <ref type="bibr" target="#b19">[20]</ref>. With this empirical evaluation, we demonstrate the capability of the ISTL to detect and localize anomalies in near real-time and that the ISTL model performs on par with state-of-the-art anomaly detection methods proposed in the current literature. ISTL was implemented in Python with TensorFlow framework <ref type="bibr" target="#b38">[39]</ref> for enhanced capabilities in deep learning and GPU utilization. ISTL was trained on a high-performance computing specification, 36-core CPU 2.3GHz with 128GB memory and dual NVIDIA Quadro of 24GB GPU units. Evaluation of ISTL was conducted on a typical personal computer configuration, a 4-core CPU 2.6 GHz with 24GB memory and GPU of NVIDIA GeForce GTX 970M, in order to ensure that the proposed ISTL model can be realistically deployed in an industrial setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>The CUHK Avenue dataset <ref type="bibr" target="#b20">[21]</ref> was acquired using a stationary video camera with a resolution of 640 x 360 pixels, recording street activity at City University, Hong Kong. This dataset has 16 train video samples that contain normal human behaviour and 21 test video samples that contain unusual events and human actions. The normal behaviour are pedestrians on the sidewalk and groups of pedestrians congregating on the sidewalk, whereas the anomalous events The UCSD pedestrian Dataset <ref type="bibr" target="#b19">[20]</ref> was captured by a stationary video camera with a resolution of 238 x 158 pixels, focusing on two pedestrian walkways. This includes two datasets, ped 1 and ped 2, capturing different crowd scenes, ranging from sparse to dense. The normal behaviours of the train video samples contain only scenarios of pedestrians walking on the pathway, whereas the test video samples contain anomalous pedestrian movement patterns such as walking across the sidewalk or walking on the grass, unexpected behaviour such as skateboarding, cycling, and vehicular movement. Ped 1 dataset has 34 train video samples and 36 test video samples, whereas Ped 2 dataset has 16 train video samples and 12 test video samples. Both the selected datasets were captured at a frame rate of 26 frames per second (FPS).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental Setup</head><p>The experimental setup is fourfold. 1) First, the anomaly detection capabilities of the spatiotemporal autoencoder model is evaluated and compared with the state-of-the-art anomaly detection models based on the three benchmark datasets. 2) Second, the anomaly localization capability is evaluated using non-overlapping cuboids of 16 x 16 x T pixels. This size is selected for the input cuboid as it is small enough to capture the location of anomalies as well large enough to extract related appearance information, based on the video resolution of selected datasets. 3) Third, we evaluate the continuous learning capability of the ISTL model for UCSD Ped1 and Ped2 datasets, adapting a particular scenario as normal which was previously considered as an anomaly. 4) Fourth, we conduct a runtime analysis of our approach demonstrating the real-time processing capabilities of our algorithm.</p><p>As the video samples have different dimensionality, we pre-process the inputs by resizing the extracted frames to 224 x 224 pixels, and normalizing pixel values by scaling between 0 and 1. Based on the frame rate of the selected training data (i.e., 26FPS), we select the depth of temporal cuboid, T=8 representing an approximate duration of one-third of a second. The selection of T is both dependent on maximising the motion to be captured within consecutive frames as well minimizing the convergence of the deep learning model due to large depth of input cuboids. In particular scenarios where the input surveillance data has lower frame rate, it is possible to capture longer motion with low temporal depths.</p><p>In this experiment, we trained the learning model using a learning rate of 0.01 and 1500 training epochs. Stochastic gradient descent algorithm is used to optimize the spatiotemporal autoencoder model and mean squared error is used as the cost function to calculate the reconstruction loss. In order to avoid overfitting of the model, we employed early stopping regularization technique where the training terminates when the loss has stopped improving. The training was conducted for 3 continuous iterations by splitting the data set as 60% for the first iteration, and 20% each for second and third iterations (as elucidated in Fig. <ref type="figure" target="#fig_0">1.</ref>). The reconstruction error was used as the fuzzy measure in the active learning phase.</p><p>In the anomaly detection and localization phase, the two thresholds are the temporal threshold and the anomaly threshold. We evaluated a range of λ from 1 to 9 in order to select the optimal value for each test dataset. The evaluation is presented in Fig. <ref type="figure" target="#fig_9">7</ref>. The optimal λ was different for the three datasets; minimum value of 1 (one-third of a second) for Ped 1 dataset whereas the maximum value of 9 (three seconds) for the Ped 2 dataset. This can be justified by the view point of the   video samples as Ped 1 dataset has the farthest view making the pedestrian/object movement to be small, whereas the Ped 2 dataset contained a closer view of the pedestrians/objects which made video sample to capture movements a lengthier than the Ped 1 dataset. Avenue dataset captured the optimal anomalies with the λ of 6 (two seconds), which similarly can be justified by the camera view and the movement of people captured in the video sample. The optimal accuracies the ISTL model was able to achieve is presented in Table <ref type="table" target="#tab_0">II</ref>, with respective λ and µ values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Results -Anomaly Detection</head><p>Anomaly detection was evaluated with three state-of-the-art handcrafted feature representation-based approaches and four state-of-the-art deep learning-based approaches. The selected handcrafted feature representation-based methods include, first, abnormal crowd behaviour detection using social force model (SF) by <ref type="bibr" target="#b39">Mehran et al. (2009)</ref>  <ref type="bibr" target="#b39">[40]</ref> which employs a grid of particles is placed over the video frame and the space-time average of optical flow to enforce the social force model. Second, we evaluate MPCCA model ( <ref type="formula">2009</ref>) <ref type="bibr" target="#b40">[41]</ref> that utilizes space-time Markov random field and video optical flow for anomaly detection. Third, we evaluate MPCCA+SF model (2010) <ref type="bibr" target="#b19">[20]</ref>, the original work of the UCSD ped 1 and ped 2 datasets. The anomaly detection of this approach is based on mixtures of dynamic textures, where the outliers under this model are labelled as anomalies.</p><p>The selected deep learning-based approaches for comparison are as follows. First, Conv-AE <ref type="bibr" target="#b24">(Hasan et al. 2016</ref>) <ref type="bibr" target="#b24">[25]</ref> is a deep convolution feed-forward autoencoder architecture that learns both local features and classifiers as an end-to-end learning framework. Second, S-RBM (Vu 2017) <ref type="bibr" target="#b30">[31]</ref> is an unsupervised probabilistic framework that models the normality and learn feature representations automatically. Third, ConvLSTM-AE <ref type="bibr" target="#b29">(Luo et al. 2017)</ref>  <ref type="bibr" target="#b29">[30]</ref> is an integrated CNN and ConvLSTM autoencoder to encode spatial and temporal patterns in normal behaviour. Fourth, Unmaskinglate-fusion (Ionescu et al. 2017) <ref type="bibr" target="#b41">[42]</ref> is an anomaly detection approach based on unmasking technique. This method employs motion features captured from 3D gradients and appearance features from pre-trained CNN, specifically VGGf <ref type="bibr" target="#b42">[43]</ref>.</p><p>We compare the results of respective models using frame level ROC curves, the corresponding Area Under the Curve (AUC) and Equal Error Rate (EER). The comparison is presented in Table <ref type="table" target="#tab_1">III</ref>, where the results appear as reported by respective authors. Overall, our method outperforms all the handcrafted approaches whereas we obtain on-par results in comparison to deep learned representation-based methods with respect to Ped 1 and Avenue datasets. For the Ped 2 dataset, our proposed ISTL method outperforms all the compared models including the benchmark of Conv-AE (2016) approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Results -Anomaly Localization</head><p>Qualitative analysis of the localized anomaly patches is presented in Fig. <ref type="figure" target="#fig_6">6</ref>. It is shown that anomalies such as cyclists and vehicles on the pathways, pedestrians walking across the pathways, crowd loitering and pedestrians pushing carts are localized by ISTL in the UCSD ped 1 dataset. It is important  to note that there were false negative detections with respect to skateboarding in ped 1 dataset (Fig. <ref type="figure" target="#fig_6">6A</ref>). Out of the 12 test videos samples that contained people who skateboard, only 10 were detected by the ISTL model. However, in the ped 2 dataset, all the video samples that contained skateboarding were detected. This can be explained by the camera angle of ped 1 datasets where its elevation makes it difficult to differentiate between pedestrians and skateboarders by appearance.</p><p>In UCSD ped 2 test samples, bicycles, vehicles and pedestrians walking in different directions are localized. The main anomaly in the ped 2 test samples was cyclists, in 11 out of the 12 instances. Anomalies such as an abandoned bag, a person throwing a bag, child playing in the surveillance area, people walking in wrong directions, people running are localized as anomalies by ISTL in the CUHK Avenue dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Results -Active Learning</head><p>In order to demonstrate the active learning capability of ISTL, we selected cycling on the pedestrian pathway scenarios of UCSD Ped 1 and Ped 2 datasets. Here we defined cycling on pedestrian pathways as a normal behaviour, thereby tagged all the anomaly detections from test samples of cyclists as normal. We employed 4 test samples containing cyclists from each Ped 1 and Ped 2 datasets to continuously train the ISTL model with human observer verification. Subsequent to the training phase, we evaluated the anomalies of the test samples excluding the 4 samples selected for continuous training. The anomaly detection ratio is presented in Table <ref type="table" target="#tab_2">IV</ref>. In Ped 1 dataset evaluation, it was detected that 2 test samples that had cyclists were anomalous, because these were across sidewalk cycle movements.</p><p>To further evaluate utility of the active learning approach, we singled out two particular test scenarios that have been previously detected as anomalous; (A) cyclist only, and (B) cyclist and a vehicle moving on the pedestrian pathway (as illustrated in Fig. <ref type="figure" target="#fig_8">8</ref>). The evaluation resulted in test video A being detected as normal while test video B being detected as an anomaly. This localization confirms that the video B was detected as an anomaly due to the moving vehicle, whereas the cyclist was detected as normal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Results -Run-time Analysis</head><p>We evaluated the real-time video surveillance capability of our anomaly detection approach and the computational overheads for sequenced process of anomaly detection and localization. Table V presents an overview of the time analysis for our anomaly detection approach in the three datasets evaluated. The averaged processing time for anomaly detection and localization is 37 milliseconds. Achieving approximately 27 frames per second (FPS), ISTL has demonstrated capability for anomaly detection from video surveillance streams in real-time. It should be noted that the difference in processing time for datasets were due to their differences in original resolution as even though frames are resized for anomaly detection, localization is assessed for original frame resolution. For these experiments, ISTL was implemented in series. However, detection and localization can be parallelized, thereby further reducing run time to achieve a higher FPS rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we have proposed a new spatiotemporal anomaly detection approach using deep learning and active learning for real-time video surveillance. This approach addresses the three primary challenges of anomaly detection from surveillance video streams by, (i) handling highdimensional video surveillance data streams in real-time, (ii) formulating the anomaly detection as to learn normality, and (iii) adapting to dynamically evolving normal behaviour with fuzzy aggregation and active learning. The proposed ISTL (Incremental Spatio-Temporal Learner) approach is based on a spatiotemporal autoencoder model consisting of convolution layers that learn spatial regularities and ConvLSTM layers that learn temporal regularities preserving the spatial structure of the video stream. ISTL incorporates a fuzzy aggregation of human observer feedback into continuous active learning process of unknown/new normalities to address the tightlycoupled dependence on a known normality training dataset. It uses two thresholds, anomaly threshold and temporal threshold, based on the context of the video surveillance feed, to overcome sparse evaluation which is based solely on reconstruction error.</p><p>Results from experiments conducted on three benchmark datasets demonstrate accuracy, robustness, low computational overhead as well as contextual indicators of the proposed approach, confirming its wide applicability in industrial and urban settings. From a practical perspective of video surveillance, ISTL ensures a human observer is not required to continuously monitor surveillance footage to determine anomalous behaviour. Human involvement is only required for verification of the detected anomalies in practical scenarios and refinement of the learning model. As future work, we intend to achieve end-to-end autonomous video surveillance with reduced false negative detection by utilizing hierarchical multi-stream recurrent self-organizing architecture with transience, in which we process spatial and temporal streams separately so that re-occurring anomalies are retained and long-term anomalies are gradually replaced.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overview of the proposed ISTL approach</figDesc><graphic coords="2,314.70,632.10,250.55,93.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Functional view of the ISTL approach. I: input frames, D: detected frames, tu: time step at initial training, tv, tw: time steps at second and third training iterations, LSV: Live surveillance video.</figDesc><graphic coords="3,47.25,50.40,521.22,136.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>𝜑(𝑖, 𝑗, 𝑘) = |𝑋 (𝑖,𝑗,𝑘) -𝑋 ̅ (𝑖,𝑗,𝑘)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Spatiotemporal Autoencoder Architecture. Layer IDs are referred from TableI. Best viewed in colour.</figDesc><graphic coords="4,59.75,50.40,226.34,98.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>[Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Anomaly detection and localization</figDesc><graphic coords="4,334.90,603.25,176.10,123.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Active Learning of spatiotemporal autoencoder model. D: detected input frames, v,w: time epoch of previous anomaly detection</figDesc><graphic coords="5,314.70,50.40,250.25,102.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Localised anomalies. (A) UCSD Ped 1 Dataset, (B) UCSD Ped 2 Dataset, and (C) CUHK Avenue Dataset. Best viewed in colour.</figDesc><graphic coords="6,69.87,44.95,478.55,158.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TII.2019.2938527, IEEE Transactions on Industrial Informatics TII-18-2964.R2 7</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Evaluation dataset from UCSD Ped 2: (A) person riding a bicycle, and (B) person riding a bicycle and a vehicle moving on the pedestrian walk. Best viewed in colour.</figDesc><graphic coords="7,355.43,50.40,164.59,158.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Evaluation of optimal AUC with respect to µ based on different λ values. (A) UCSD Ped 1, (B) UCSD Ped 2, and (C) CUHK Avenue. Best viewed in colour.</figDesc><graphic coords="7,54.93,50.40,233.40,457.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I</head><label>I</label><figDesc></figDesc><table><row><cell></cell><cell cols="3">SPATIOTEMPORAL AUTOENCODER ARCHITECTURE</cell></row><row><cell>ID</cell><cell>Input Tensor</cell><cell>Operation</cell><cell>Output Tensor</cell></row><row><cell>C1</cell><cell>T*224*224*1</cell><cell>CV; F: 128; K: 27*27; S: 4</cell><cell>T*56*56*128</cell></row><row><cell>C2</cell><cell>T*56*56*128</cell><cell>CV; F: 64; K: 13*13; S: 2</cell><cell>T*28*28*64</cell></row><row><cell>CL1</cell><cell>T*28*28*64</cell><cell>CL; F: 64; K: 3*3</cell><cell>T*28*28*64</cell></row><row><cell>CL2</cell><cell>T*28*28*64</cell><cell>CL; F: 32; K: 3*3</cell><cell>T*28*28*32</cell></row><row><cell>DCL1</cell><cell>T*28*28*32</cell><cell>CL; F: 64; K: 3*3</cell><cell>T*28*28*64</cell></row><row><cell>DC1</cell><cell>T*28*28*64</cell><cell>DCV; F: 64; K: 13*13; S: 2</cell><cell>T*56*56*128</cell></row><row><cell>DC2</cell><cell>T*56*56*128</cell><cell>DCV; F: 128; K: 27*27; S: 4</cell><cell>T*224*224*1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE III COMPARISON</head><label>III</label><figDesc>OF AUC AND EER</figDesc><table><row><cell></cell><cell></cell><cell>TABLE II</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">SELECTION OF ANOMALY THRESHOLD AND TEMPORAL THRESHOLD</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell>Optimal AUC/EER</cell><cell>Anomaly Threshold (µ)</cell><cell>Temporal Threshold (λ)</cell><cell>Model</cell><cell>Ped 1 AUC/EER</cell><cell>Ped 2 AUC/EER</cell><cell>Avenue AUC/EER</cell></row><row><cell>Ped 1</cell><cell>75.2/29.8</cell><cell>0.33</cell><cell>1</cell><cell>SF (2009)</cell><cell>67.5/31.0</cell><cell>55.6/42.0</cell><cell>NA</cell></row><row><cell>Ped 2</cell><cell>91.1/8.9</cell><cell>0.38</cell><cell>9</cell><cell>MPCCA (2009)</cell><cell>66.8/40.0</cell><cell>69.3/30.0</cell><cell>NA</cell></row><row><cell>Avenue</cell><cell>76.8/29.2</cell><cell>0.29</cell><cell>6</cell><cell>MPCCA + SF (2010)</cell><cell>74.2/32.0</cell><cell>61.3/36.0</cell><cell>NA</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Conv-AE (2016)</cell><cell>81.0/27.9</cell><cell>90.0/21.7</cell><cell>70.2/25.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>S-RBM (2017)</cell><cell>70.3/35.4</cell><cell>86.4/16.5</cell><cell>78.8/27.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ConvLSTM-AE (2017)</cell><cell>75.5/NA</cell><cell>88.1/NA</cell><cell>77.0/NA</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Unmasking (2017)</cell><cell>68.4/NA</cell><cell>82.2/NA</cell><cell>80.6/NA</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Ours (ISTL)</cell><cell>75.2/29.8</cell><cell>91.1/8.9</cell><cell>76.8/29.2</cell></row></table><note><p>1551-3203 (c) 2019 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE V PROCESSING</head><label>V</label><figDesc>TIME ANALYSIS (SECONDS PER FRAME)</figDesc><table><row><cell>Process</cell><cell>Ped 1</cell><cell>Ped 2</cell><cell>Avenue</cell></row><row><cell>Pre-processing</cell><cell>0.0012</cell><cell>0.0012</cell><cell>0.0012</cell></row><row><cell>Representation</cell><cell>0.0293</cell><cell>0.0292</cell><cell>0.0290</cell></row><row><cell>Detection</cell><cell>0.0019</cell><cell>0.0019</cell><cell>0.0018</cell></row><row><cell>Localization</cell><cell>0.0047</cell><cell>0.0049</cell><cell>0.0042</cell></row><row><cell>Total</cell><cell>0.0369</cell><cell>0.0371</cell><cell>0.0360</cell></row><row><cell>FPS</cell><cell>~27</cell><cell>~27</cell><cell>~28</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>1 / 7 Values presented as detected anomalies / total test sample 1551-3203 (c) 2019 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TII.2019.2938527, IEEE Transactions on Industrial Informatics TII-18-2964.R2</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported by a La Trobe University Postgraduate Research Scholarship. Paper no. TII-18-2964.R2.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Intelligent Video Systems and Analytics: A Survey</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kubota</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Informatics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1222" to="1233" />
			<date type="published" when="2013-08">Aug. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A System for Multicamera Task Recognition and Summarization for Structured Environments</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">I</forename><surname>Kosmopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Voulodimos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Doulamis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Informatics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="161" to="171" />
			<date type="published" when="2013-02">Feb. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Secure Surveillance Framework for IoT systems using Probabilistic Image Encryption</title>
		<author>
			<persName><forename type="first">K</forename><surname>Muhammad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hamza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lloret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Baik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Informatics</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An Effective Video Summarization Framework Toward Handheld Devices</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Electronics</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1309" to="1316" />
			<date type="published" when="2015-02">Feb. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multiple View Oriented Matching Algorithm for People Reidentification</title>
		<author>
			<persName><forename type="first">J</forename><surname>García</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gardel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Bravo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Lázaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Informatics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1841" to="1851" />
			<date type="published" when="2014-08">Aug. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unattended Object Identification for Intelligent Surveillance Systems Using Sequence of Dual Background Difference</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wahyono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Filonenko</surname></persName>
		</author>
		<author>
			<persName><surname>Jo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Informatics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2247" to="2255" />
			<date type="published" when="2016-12">Dec. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An Adaptive Neural-Fuzzy Approach for Object Detection in Dynamic Backgrounds for Surveillance Systems</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Chacon-Murguia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gonzalez-Duarte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Electronics</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3286" to="3298" />
			<date type="published" when="2012-08">Aug. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A Comprehensive Review on Handcrafted and Learning-Based Action Representation Approaches for Human Activity Recognition</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Sargano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Angelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Habib</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">110</biblScope>
			<date type="published" when="2017-01">Jan. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Vision based surveillance system for detection of human fall</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Basavaraj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kusagur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Recent Trends in Electronics, Information Communication Technology</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="1516" to="1520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Self-evolving intelligent algorithms for facilitating data interoperability in IoT environments</title>
		<author>
			<persName><forename type="first">R</forename><surname>Nawaratne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Alahakoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">De</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chhetri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chilamkurti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Future Generation Computer Systems</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="421" to="432" />
			<date type="published" when="2018-09">Sep. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An Overview of Deep Learning Based Methods for Unsupervised and Semi-Supervised Anomaly Detection in Videos</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Kiran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Parakkal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Imaging</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">36</biblScope>
			<date type="published" when="2018-02">Feb. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Detecting anomalous events in videos by learning deep representations of appearance and motion</title>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">156</biblScope>
			<biblScope unit="page" from="117" to="127" />
			<date type="published" when="2017-03">Mar. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Anomaly Detection: A Survey</title>
		<author>
			<persName><forename type="first">V</forename><surname>Chandola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2009-07">Jul. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Beyond the hype: Big data concepts, methods, and analytics</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gandomi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Haider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Information Management</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="144" />
			<date type="published" when="2015-04">Apr. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep learning applications and challenges in big data analytics</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Najafabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Villanustre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Khoshgoftaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Seliya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Muharemagic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Big Data</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2015-12">Dec. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adaptive real-time anomaly detection with incremental clustering</title>
		<author>
			<persName><forename type="first">K</forename><surname>Burbeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nadjm-Tehrani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Security Technical Report</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="56" to="67" />
			<date type="published" when="2007-01">Jan. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Incremental knowledge acquisition and self learning from text</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">De</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Alahakoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2010 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Natural Intelligence the human factor in AI</title>
		<author>
			<persName><forename type="first">Bill</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-01">Jan. 2018</date>
			<pubPlace>AI NEXTCon, Seattle</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">How Machine Learning with TensorFlow Enabled Mobile Proof-Of-Purchase at Coca-Cola</title>
	</analytic>
	<monogr>
		<title level="j">Google Developers Blog</title>
		<imprint>
			<date type="published" when="2017-09">Sep. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Anomaly detection in crowded scenes</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bhalodia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1975" to="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Abnormal Event Detection at 150 FPS in MATLAB</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2720" to="2727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Automated human behavior analysis from surveillance videos: a survey</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gowsikhaa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Abirami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Baskaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif Intell Rev</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="747" to="765" />
			<date type="published" when="2014-12">Dec. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Image processing techniques for object tracking in video surveillance-A survey</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ojha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sakhare</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 International Conference on Pervasive Computing (ICPC)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Incremental knowledge acquisition and selflearning for autonomous video surveillance</title>
		<author>
			<persName><forename type="first">R</forename><surname>Nawaratne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bandaragoda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Adikari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Alahakoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">De</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>IEEE Industrial Electronics Society</publisher>
			<biblScope unit="page" from="4790" to="4795" />
		</imprint>
	</monogr>
	<note>in IECON 2017 -43rd Annual Conference of the</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning Temporal Regularity in Video Sequences</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="733" to="742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Motion instability based unsupervised online abnormal behaviors detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimed Tools Appl</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="7423" to="7444" />
			<date type="published" when="2016-06">Jun. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Chaotic invariants of Lagrangian particle trajectories for anomaly detection in crowded scenes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2054" to="2060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Online detection of unusual events in videos via dynamic sparse coding</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="3313" to="3320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Anomalous Behaviour Detection Using Spatiotemporal Oriented Energies, Subset Inclusion Histogram Comparison and Event-Driven Processing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zaharescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2010</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="563" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Remembering history with convolutional LSTM for anomaly detection</title>
		<author>
			<persName><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="439" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep Abnormality Detection in Video Data</title>
		<author>
			<persName><forename type="first">H</forename><surname>Vu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Electronic proceedings of IJCAI 2017</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5217" to="5218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Video Anomaly Detection With Compact Feature Sets for Online Performance</title>
		<author>
			<persName><forename type="first">R</forename><surname>Leyva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3463" to="3478" />
			<date type="published" when="2017-07">Jul. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep-anomaly: Fully convolutional neural network for fast anomaly detection in crowded scenes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sabokrou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fayyaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zahra</forename><surname>Moayed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Klette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<date type="published" when="2018-02">Feb. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Abnormal Event Detection in Videos Using Spatiotemporal Autoencoder</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Tay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Networks -ISNN 2017</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="189" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Autoencoders, Unsupervised Learning and Deep Architectures</title>
		<author>
			<persName><forename type="first">P</forename><surname>Baldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 International Conference on Unsupervised and Transfer Learning Workshop</title>
		<meeting>the 2011 International Conference on Unsupervised and Transfer Learning Workshop<address><addrLine>Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="37" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Subject independent facial expression recognition with robust face detection using a convolutional neural network</title>
		<author>
			<persName><forename type="first">M</forename><surname>Matsugu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mitari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kaneda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="555" to="559" />
			<date type="published" when="2003-06">Jun. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Convolutional LSTM network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H I</forename><surname>Xingjian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="802" to="810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Incremental characterization learning and forecasting for electricity consumption using smart meters</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Alahakoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Holmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE International Symposium on Industrial Electronics</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="807" to="812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">TensorFlow: a system for large-scale machine learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th USENIX conference on Systems Design and Implementation</title>
		<meeting>the 12th USENIX conference on Systems Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Abnormal crowd behavior detection using social force model</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mehran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="935" to="942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Observe locally, infer globally: A spacetime MRF for detecting abnormal activities with incremental updates</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="2921" to="2928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Unmasking the abnormal events in video</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tudor Ionescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Smeureanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2895" to="2903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Return of the Devil in the Details: Delving Deep into Convolutional Nets</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.3531</idno>
		<imprint>
			<date type="published" when="2014-05">May. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
