<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PLUG AND PLAY LANGUAGE MODELS: A SIMPLE APPROACH TO CONTROLLED TEXT GENERATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sumanth</forename><surname>Dathathri</surname></persName>
							<email>dathathris@gmail.com</email>
						</author>
						<author>
							<persName><roleName>Caltech</roleName><forename type="first">Andrea</forename><surname>Cms</surname></persName>
						</author>
						<author>
							<persName><surname>Madotto</surname></persName>
							<email>amadotto@connect.ust.hk</email>
						</author>
						<author>
							<persName><forename type="first">Janice</forename><surname>Hkust</surname></persName>
						</author>
						<author>
							<persName><surname>Lan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jane</forename><surname>Hung</surname></persName>
							<email>jane.hung@uber.com</email>
						</author>
						<author>
							<persName><forename type="first">Eric</forename><surname>Frank</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Piero</forename><surname>Molino</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
							<email>yosinski@uber.com</email>
						</author>
						<author>
							<persName><forename type="first">Rosanne</forename><surname>Liu</surname></persName>
							<email>rosanne@uber.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<address>
									<settlement>Uber</settlement>
									<region>AI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<address>
									<settlement>Uber</settlement>
									<region>AI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<address>
									<settlement>Uber</settlement>
									<region>AI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<address>
									<settlement>Uber</settlement>
									<region>AI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<address>
									<settlement>Uber</settlement>
									<region>AI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<address>
									<settlement>Uber</settlement>
									<region>AI</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">PLUG AND PLAY LANGUAGE MODELS: A SIMPLE APPROACH TO CONTROLLED TEXT GENERATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large transformer-based language models (LMs) trained on huge text corpora have shown unparalleled generation capabilities. However, controlling attributes of the generated language (e.g. switching topic or sentiment) is difficult without modifying the model architecture or fine-tuning on attribute-specific data and entailing the significant cost of retraining. We propose a simple alternative: the Plug and Play Language Model (PPLM) for controllable language generation, which combines a pretrained LM with one or more simple attribute classifiers that guide text generation without any further training of the LM. In the canonical scenario we present, the attribute models are simple classifiers consisting of a user-specified bag of words or a single learned layer with 100,000 times fewer parameters than the LM. Sampling entails a forward and backward pass in which gradients from the attribute model push the LM's hidden activations and thus guide the generation. Model samples demonstrate control over a range of topics and sentiment styles, and extensive automated and human annotated evaluations show attribute alignment and fluency. PPLMs are flexible in that any combination of differentiable attribute models may be used to steer text generation, which will allow for diverse and creative applications beyond the examples given in this paper. * Work done during internship at Uber AI † Co-senior authors . • Summary of contributions: SD, RL &amp; JY conceptualized PPLMs and led the manuscript writing. SD led the project, implemented the PPLM, set up and ran all modeling experiments, engineered how to obtain workable gradients via the weighted embedding approach, and made the model work. AM helped with preparing datasets for discriminator training, automated evaluation, running experiments, and writing the manuscript. SD, RL &amp; AM ran the external baselines. RL &amp; JL built and oversaw the human evaluation pipeline and computed the statistics. JH ran the story generation with skeleton prefixes. EF assisted with detoxification experiments. PM led efforts to migrate to the new pytorch transformer, helped with code release. JY helped with the annotation pipeline, finding bugs, navigating model and experimental directions, engineering workable gradients, and posing the model mathematically. RL implemented preliminary experiments and multi-attribute control, and cleaned and coordinated release of the code. RL &amp; JY oversaw the project.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The Transformer architecture <ref type="bibr" target="#b43">(Vaswani et al., 2017)</ref> has enabled large-scale language models (LMs) trained on a huge amount of data <ref type="bibr" target="#b33">(Radford et al., 2019;</ref><ref type="bibr" target="#b5">Dai et al., 2019b;</ref><ref type="bibr" target="#b32">Radford et al., 2018b)</ref> to greatly improve the state-of-the-art on natural language processing tasks. These models are used to extract contextualized word embeddings for transfer learning purposes <ref type="bibr" target="#b6">(Devlin et al., 2019)</ref> and as natural language generators. The latter can leverage large amounts of unannotated data and a simple log-likelihood training objective. However, once such models are trained, controlling attributes of generated text becomes difficult without modifying the model architecture to allow for extra input attributes or fine-tuning with attribute-specific data <ref type="bibr" target="#b16">(Keskar et al., 2019;</ref><ref type="bibr" target="#b52">Ziegler et al., 2019)</ref>. and x the generated sample. However, generative models only learn p(x). In computer vision, Plug &amp; Play Generative Networks (PPGN) from <ref type="bibr" target="#b28">Nguyen et al. (2017)</ref> developed a mechanism for generating images with different attributes by plugging a discriminator (attribute model) p(a|x) together with a base generative model p(x) and sampling from the resulting p(x|a) ∝ p(a|x)p(x), effectively creating a conditional generative model on the fly from any supplied attribute model. In a similar manner, we propose the Plug and Play Language Model (PPLM) for conditional language generation that combines one or more simple attribute models p(a|x)-either in the form of a bagof-words (BoW) or single layer classifiers-with a pre-trained, unconditional language model p(x). We sample from the resulting combined model by following gradients in the latent representation space in a manner inspired by the approximate Metropolis-adjusted Langevin (MALA) <ref type="bibr" target="#b35">(Roberts et al., 1996;</ref><ref type="bibr" target="#b34">Roberts &amp; Rosenthal, 1998)</ref> sampler deployed in <ref type="bibr" target="#b28">Nguyen et al. (2017)</ref>.</p><p>Optimization is performed ex post facto in the activation space, therefore no re-training or finetuning is needed. Control is fine-grained, with a strength parameter determining how strong the attribute influence should be; a strength of 0 fully recovers the original model p(x). This design allows vast flexibility: users can combine a state-of-the-art generative model, which may be large and difficult to train, with any number of attribute controllers. Attribute models may be easier to train or untrained (in the case of BoW models), and multiple controllers may be combined flexibly during inference. In this paper, we demonstrate the PPLM approach using a GPT-2 345M model <ref type="bibr" target="#b33">(Radford et al., 2019)</ref> as the general-purpose LM p(x), but the method applies in any representation space from any transformer-based text generator and allows combination with any attribute model p(a|x).</p><p>We demonstrate controlled generation with a number of attribute controllers, assembled and combined during generation, each with a different strength, acting as a set of "control knobs" that tune generation towards the desired attribute (see examples in Table <ref type="table">1</ref>). Code for the experiments is available at: https://github.com/uber-research/PPLM. Our key contributions are:</p><p>• We introduce the Plug and Play LM for controlled language generation, discuss its relation to existing work, and how sampling from a PPLM works (Sections 2 and 3).</p><p>• We demonstrate controlling of text generation on a range of attributes, including 7 topics each defined using a bag of words, and 1 simple discriminator on sentiments. We quantify effectiveness using both automated evaluation (separately trained perplexity and sentiment models) as well as human evaluation (for attribute relevance and fluency). All evaluations point toward the ability of PPLMs to generate attribute controlled, fluent text (Section 4).</p><p>• We compare PPLM with <ref type="bibr">CTRL (Keskar et al., 2019)</ref> and GPT-2 finetuned for positivty <ref type="bibr" target="#b52">(Ziegler et al., 2019)</ref>. Our method, without any LM training, is on par and often outperforms the baselines on attribute relevance and fluency (Section 4.2, and Section 4.3).</p><p>• We show that the PPLM approach can be used to detoxify instances where generation of toxic content is likely by following the negative gradient of a model trained to detect toxicity (Section 4.4). We also show how PPLM can be used for structurally constrained story writing (Section 4.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Controlled generation Current methods for controlled text generation involve either fine-tuning existing models with Reinforcement Learning (RL) <ref type="bibr" target="#b52">(Ziegler et al., 2019)</ref>, training Generative Adversarial Networks <ref type="bibr" target="#b49">(Yu et al., 2017)</ref>, or training conditional generative models <ref type="bibr" target="#b17">(Kikuchi et al., 2016;</ref><ref type="bibr" target="#b10">Ficler &amp; Goldberg, 2017)</ref>. Different from our approach, these methodologies are not plug and play, since the entire model needs to be separately fine-tuned for each specific attribute. <ref type="bibr" target="#b16">Keskar et al. (2019)</ref> train a large language model with over 50 different control codes. The results are high quality because they train exactly to maximize p(x|a), but this comes at the expense of fixing control codes upfront and of training a very large model (1.6B parameters). Our method does not require retraining any conditional generative model, and both the language model and the conditional model can be flexibly assembled. Table <ref type="table" target="#tab_0">2</ref> gives a comparison of recent approaches to language modeling tuned for specific attributes. In another interesting but tangential piece of work, <ref type="bibr" target="#b41">Subramani et al. (2019)</ref> recently showed that a pre-trained language model can be steered to recover arbitrary sentences. In earlier works <ref type="bibr" target="#b15">Gu et al. (2016;</ref><ref type="bibr">2017)</ref>; <ref type="bibr" target="#b3">Chen et al. (2018)</ref> explored the idea of using a small neural network to steer an LM. <ref type="bibr" target="#b50">Yu et al. (2016)</ref>, and more recently <ref type="bibr" target="#b51">Yu et al. (2019)</ref>; <ref type="bibr" target="#b48">Yee et al. (2019)</ref>; <ref type="bibr" target="#b26">Ng et al. (2019)</ref>, leveraged the Shannon Noisy Channel Theory <ref type="bibr" target="#b37">(Shannon, 1948)</ref> for improving sequence-to-sequence modeling. Their approach translates a source language sentence y into a target language sentence x by first sampling from a forward model proposal distribution p forward (x|y) and then reranking samples based on probabilities given by p backward (x|y) ∝ p(x)p(y|x). PPLM scores samples using the same basic equation, but as we have no forward or proposal model p forward (x|a), we rely on the latent space updates, similar to <ref type="bibr" target="#b28">Nguyen et al. (2017)</ref>. As a baseline, we consider using p(x) as a "forward model" and then reranking, which we will see works moderately well in some scenarios and poorly in others (see Tables <ref type="table" target="#tab_5">4 and 6</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Noisy Channel Modeling</head><p>Weighted decoding <ref type="bibr" target="#b12">Holtzman et al. (2018)</ref>; <ref type="bibr">Ghazvininejad et al. (2017)</ref> consider controlled language generation -the former with discriminators, and the latter with a bag of words -where the decoding procedure is modified to consider the scoring function used for decoding. <ref type="bibr" target="#b36">See et al. (2019)</ref> note that control with weighted decoding (WD) is difficult and often leads to sacrificing fluency and coherence. Further, <ref type="bibr">Ghazvininejad et al. (2017)</ref> strongly relies on sampling from a set of keywords on a specific topic and it does not allow to bias generation towards a topic in a manner that does not necessary include a set of keywords. Similarly, <ref type="bibr" target="#b1">Baheti et al. (2018)</ref> proposed a decoding strategy for generating interesting responses in dialogue systems, using bags of words and word embeddings. Sophisticated sampling methods <ref type="bibr" target="#b25">(Metropolis et al., 1953)</ref> can be used to constrain the model generation to certain keywords and topics. We evaluate WD as a baseline.</p><p>Text Style Transfer Outside of language modeling, the text style transfer studies a related task. <ref type="bibr" target="#b38">Shen et al. (2017)</ref>; <ref type="bibr" target="#b14">Hu et al. (2017)</ref> train variational auto-encoders for style transfer that rely on learning disentangled latent representations for style and content. <ref type="bibr" target="#b20">Li et al. (2018)</ref> demonstrate the efficacy of a simple approach based on replacing attribute related n-grams with n-grams corresponding to the desired attribute based on a conditional generative model. A key difference between the above and our approach is that we use an offline discriminator and perform optimization based on this discriminator, which as suggested by <ref type="bibr" target="#b8">Elazar &amp; Goldberg (2018)</ref> may outperform adversarial training approaches. More recently, <ref type="bibr" target="#b18">Lample et al. (2019)</ref> adapt an approach from unsupervised language translation to style transfer, where a denoised auto-encoder is trained with an objective consisting of a weighted combination of a re-construction loss and a back-translation loss. While the above approaches have shown impressive success on style transfer tasks, the main focus is not controlled language generation, and further, the methods are not plug and play.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PLUG AND PLAY LANGUAGE MODELS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">LANGUAGE MODELING WITH TRANSFORMERS</head><p>Given a sequence of tokens X = {x 0 , • • • , x n }, LMs are trained to compute the unconditional probability of the sequence p(X). This probability can be rewritten in terms of product of conditional probabilities by recursively applying the chain-rule <ref type="bibr" target="#b24">(Manning et al., 1999;</ref><ref type="bibr" target="#b2">Bengio et al., 2003)</ref> as:</p><formula xml:id="formula_0">p(X) = n i=1 p(x i |x 0 , • • • , x i−1 )<label>(1)</label></formula><p>In this paper, we use a transformer <ref type="bibr" target="#b43">(Vaswani et al., 2017)</ref> to model the distribution of natural language. To present our approach clearly, we first briefly summarize the transformer using recurrent notation. Let us define the history matrix H t to consist of the key-value pairs from the past i.e</p><formula xml:id="formula_1">H t = [(K (1) t , V (1) t ), • • • , (K (l) t , V<label>(l)</label></formula><p>t )], where (K</p><formula xml:id="formula_2">(i) t , V<label>(i)</label></formula><p>t ) corresponds to the key-value pairs from the i-th layer generated at all time-steps from 0 to t. Efficient implementations of the transformer <ref type="bibr" target="#b46">(Wolf et al., 2019)</ref> use the cached H t to generate x t+1 , given x t . This recurrent interpretation of a transformer can be summarized as:</p><formula xml:id="formula_3">o t+1 , H t+1 = LM(x t , H t ),<label>(2)</label></formula><p>where W is a linear transformation that maps the logit vector o t+1 to a vector of vocabulary size, and then x t+1 is sampled as x t+1 ∼ p t+1 = Softmax(W o t+1 ). This allows for efficient language generation without repeated forward passes corresponding to the prior conditioning text x 0 , . . . , x t−1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">STEERING GENERATION: ASCENDING log p(a|x)</head><p>In order to control the output of the language model, at every generation step t, we shift the history H t in the direction of the sum of two gradients: one toward higher log-likelihood (LL) of the attribute a under the conditional attribute model p(a|x) and one toward higher LL of the unmodified language model p(x). Combining these factors with a variable multiplier provides us with a controllable "knob" to guide generation in a given direction with a specified strength. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>{ { {</head><p>Step 2</p><p>Step 3</p><p>Figure <ref type="figure">1</ref>: Simplified illustration of the proposed approach in three phases. In Step 1, a forward pass is performed through the language model to compute the likelihood of a desired attribute using an attribute model that predicts p(a|x). In Step 2, a backward pass updates the internal latent representations of the LM, using gradients from the attribute model, to increase the likelihood of the passage having the desired attribute. In Step 3, a new distribution over the vocabulary ( p t+1 ) is generated from the updated latents ( H t ) and the current token x t . The next token is then sampled from the updated distribution. This process of updating the latents is repeated at each time-step, leading to a gradual transition towards the desired attribute. For computational efficiency, one may choose to modify only the latents within some window of the recent past, depicted as the dotted-red region.</p><p>at zero and updated with gradients from an attribute model that measures the extent to which the generated text possesses the desired attribute (e.g. positivity). We rewrite the attribute model p(a|x) as p(a|H t + ∆H t ) and then make gradient based updates to ∆H t as follows:</p><formula xml:id="formula_4">∆H t ← ∆H t + α ∇ ∆Ht log p(a|H t + ∆H t ) ∇ ∆Ht log p(a|H t + ∆H t ) γ<label>(3)</label></formula><p>where α is the step size, γ is the scaling coefficient for the normalization term. The approach described in the previous section is able to generate text tuned for a particular discriminator, but left unchecked it will quickly result in unrealistic adversarial or fooling examples <ref type="bibr" target="#b42">(Szegedy et al., 2013;</ref><ref type="bibr" target="#b27">Nguyen et al., 2015)</ref> as the text moves into low probability regions. To combat this, we use the unconditional language model in two ways that ensure the fluency is maintained at or near the level of the unconditional language model (here GPT-2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Kullback-Leibler (KL) Divergence</head><p>We update ∆H t to minimize the KL divergence between the output distribution of the modified and unmodified language models in addition to the step above.</p><p>In practice, this is accomplished by adding the quantities together before taking a gradient, though it can be visualized as two separate steps as in Figure <ref type="figure">2</ref>. We scale the KL coefficient by a scalar λ KL , and in practice, setting this hyperparameter to 0.01 works well in general across tasks.</p><p>Post-norm Geometric Mean Fusion In addition to minimizing KL divergence, which affects the past via ∆H t , we perform post-norm fusion similarly to <ref type="bibr" target="#b40">Stahlberg et al. (2018)</ref>. This does not directly affect ∆H t ; rather, it just serves to constantly tie the generated text to the unconditional p(x) LM distribution. We accomplish this by sampling from</p><formula xml:id="formula_5">x t+1 ∼ 1 β p γgm t+1 p 1−γgm t+1</formula><p>, where p t+1 and p t+1 are the unmodified and modified output distributions, respectively, and β is a normalizing factor such that it forms a valid distribution. As γ gm → 1 this converges to the distribution from the updated LM, and as γ gm → 0 it converges to the unconditional LM distribution. We find that in practice values for γ gm in the range 0.8 − 0.95 work well.</p><p>Figure <ref type="figure">2</ref>: An oversimplified view into why steps that maximize both log p(a|x) and log p(x) are needed. The sentence under consideration is shown as a black dot, which is first pushed in the direction of maximizing log p(a|x) and then in the direction of maximizing log p(x). In practice we use a single step and simply add the log probabilities; we take steps in continuous space of hidden representations H rather than in the discrete x (byte pair) space, and rather than resampling the entire sentence each step, we take one step in H space per byte-pair sample. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">SAMPLING AND RANKING</head><p>The attribute model p(a|x) in PPLM provides two functionalities: first, a score that can be used to rank samples based on the LL of the desired attribute (forward pass only; Step 1, Figure <ref type="figure">1</ref>), and second, a gradient ascent direction to perform an update in the latent space (Step 2 &amp; 3; Figure <ref type="figure">1</ref>). The former can be used to generate r samples and rank them to choose the best one. This can serve as an additional method for attribute control in addition to sampling with updated latents. Further, to avoid the problem of repetitive, low quality text <ref type="bibr" target="#b12">(Holtzman et al., 2018)</ref>, we compute the mean over the Dist-1, Dist-2 and Dist-3 scores (for the generated passage), which is an indicator of repetitiveness <ref type="bibr" target="#b19">(Li et al., 2015)</ref>, and then discard samples with a mean score below a threshold τ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS, RESULTS, AND EVALUATION</head><p>In this section, we describe our evaluation methodology and then show controlled generation results under various attribute models. We also show use cases of PPLM in language detoxification and in controlled story telling. For all results reported in this section, we use top-k sampling <ref type="bibr" target="#b9">(Fan et al., 2018)</ref> with k = 10 to draw from the softmax distribution over the vocabulary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">EVALUATION METHODS AND ABLATION STUDY</head><p>We evaluate to assess two properties: whether PPLM generates text that satisfies the desired attribute (topic or sentiment) and whether the quality of its text deteriorates as we intensify control of the attribute. Note we can always turn the control knob down to zero to disable control of attributes and reach the fluency of the original model. If desired, a user can tune the knobs at inference until a chosen tradeoff between attribute strength and fluency is reached. We evaluate using both automated methods and human annotators: Automated Eval. Perplexity is an automated measure of fluency, though its effectiveness has been questioned in open-domain text generation <ref type="bibr" target="#b21">(Liu et al., 2016)</ref>. We measure perplexity using a different pre-trained language model, GPT <ref type="bibr" target="#b32">(Radford et al., 2018b)</ref>. The diversity of text in the passages is measured using the number of distinct n-grams (normalized by the length of text) as in <ref type="bibr" target="#b19">Li et al. (2015)</ref>. We report Dist-1, Dist-2, and Dist-3 scores for the distinct 1-2-3-grams (measured across all samples generated for a given attribute control task, e.g. a specific topic for topic control). Such scores are an indicator of the diversity of the samples generated <ref type="bibr" target="#b19">(Li et al., 2015)</ref>. We aslo use external sentiment classifiers for sentiment evaluation.</p><p>Human Eval. We consider two types of human annotation: fluency and A/B testing on attribute relevance. Annotators are asked to evaluate the fluency of each individual sample on a scale of 1-5, with 1 being "not fluent at all" and 5 being "very fluent," as done in <ref type="bibr" target="#b18">Lample et al. (2019)</ref>. In the A/B testing for attribute relevance, we consider all combinatorial pairs of all four variants: B, BR, BC, and BCR (6 combinations). We then ask annotators to rank the pair on the desired attribute (e.g. topic relevance, sentiment strength), while allowing "neither" and "both" options to account for equally good/bad generations <ref type="bibr" target="#b18">(Lample et al., 2019)</ref>. We obtain annotations from nine external occupational annotators. Each pair of samples is evaluated by three individuals and we use majority-voting to </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">BOW ATTRIBUTE MODELS</head><p>The simplest attribute model we use gives the log of the sum of likelihoods of each word in some predefined Bag of Words (BoW). Given a set of keywords {w 1 , • • • , w k } that specify a topic of interest and the output distribution of the language model p t+1 , the log likelihood is:</p><formula xml:id="formula_6">log p(a|x) = log k i p t+1 [w i ] .<label>(4)</label></formula><p>We construct BoWs that represent seven distinct topics: SCIENCE, MILITARY, LEGAL, COMPUT-ERS, SPACE, POLITICS, and RELIGION (see Section S17 for complete word lists). Samples are shown in Table <ref type="table" target="#tab_3">3</ref>, generated from a single prefix, while being controlled towards each topic. Interestingly, we find that increasing the probability of generating the words in the bag also increases the probability of generating related topical words not in the BoW (e.g. in the [Science] sample shown in Table <ref type="table" target="#tab_3">3</ref>, note that question and philosophers are sampled before the first BoW word, laws).</p><p>Table <ref type="table" target="#tab_12">S17</ref> shows the gradual change of topic intensity under fine-grained control. We found that the optimization procedure works better with updating representations from the past over a finite window and using an adaptive normalization scheme (see Section S11.3).</p><p>For automatic and human evaluation, we generate 420 samples evenly distributed among seven BoW attribute models and 20 prefixes (see the full list in Section S15), for each of the four variants described in the ablation study. See Section S8 for further details on evaluation and results. Table <ref type="table">4</ref> shows that human annotators find text from BCR (51.7%) and BC (46.9%) to be significantly more Table <ref type="table">4</ref>: For each treatment in the ablation study, we report mean±std-dev across (human and automated) fluency metrics. The topic (%) reports the fraction of samples matching the target topic, as evaluated by human annotators. Table <ref type="table">S8</ref> provides per-topic results. Approaches BC and BCR demonstrate significant control over the topic of the generated text, while retaining similar diversity (Dist-1, Dist-2, Dist-3) scores and minimal degradation in Perplexity and Fluency evaluations vs the baseline LM (B). The gain from ranking and choosing from multiple samples BR over B is limited (4.7%). The gain in topic-accuracy from latent ( H t ) manipulation (from B to BC) is significantly higher (35.8%). Perplexity is computed using the GPT LM <ref type="bibr" target="#b31">(Radford et al., 2018a)</ref>, which differs from the LM generating text (GPT-2). For CTRL and WD, since human evaluation is performed in comparison with BCR via A/B testing, we report the numbers for BCR as well from these comparisons, for the human evaluated metrics. Further, we consider one sample per prefix for CTRL, resulting in fewer samples and higher Dist-1, 2, 3 scores as a consequence. PPLM outperforms CTRL and WD on topic-relevance, while being comparable on fluency scores. on topic than B (15.8%) and BR (11.1%). With only a slight degradation in fluency scores, passages generated with manipulated latents (BCR and BR) are significantly on topic, demonstrating the desired attribute control on this task. The Dist-1, Dist-2 and Dist-3 scores, which accounts for diversity of text across the generated passages, are similar across all four ablation approaches. Further, BCR slightly outperforms CTRL (51.7% &amp; 50.0%), and significantly outperforms WD (36 %). BC itself outperforms WD (36 %). BCR, CTRL and WD all score similarly on the fluency metric.</p><p>We note that gradient-based latent updates have significantly greater influence on topic relevance (R with or without C) than reranking based on the score (C with or without R), showing that shifting meaning in latent space is more effective than shifting the output distribution directly through reweighting. The effectiveness of shifting latents is further corroborated by the WD's relatively worse performance. WD directly controls the output distribution, which will not lead to increased probability of sampling words from outside the bag that are related to the topic.</p><p>Finally, there is a large variance in the extent of controllability across topics (Table <ref type="table">S8</ref>). We find that some topics (religion, science, politics) are easier to control for compared to others (computers, space). Section S9 considers unusual or nonsensical combinations of prefixes and attributes (e.g. prefix 'potato' and topic 'religion'), and we find that even for these settings PPLM is able to successfully control for the desired attribute, often with hilarious twists!</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">DISCRIMINATOR ATTRIBUTE MODELS</head><p>While BoW models have been demonstrated to be able to control text attributes such as sentiment (e.g., <ref type="bibr" target="#b20">Li et al. (2018)</ref> rely on extracting a set of attribute-based phrases to control the sentiment during style transfer), being able to control attributes using more sophisticated discriminators is desirable when it is difficult to express the attribute with a simple bag of words.</p><p>We train a discriminator on a dataset with input sentences x and corresponding labels y x . For an input x of length t, we compute o x :t and train f on the mean (ō t ) of the embeddings across time. All discriminators in this work consist of a single layer classifier that predicts the target label from ōx t . The number of parameters in this layer is (embedding-dimension (e) × number of attributes (a) + number of attributes (a)), which is negligible compared to the number of parameters in the LM model itself (Table <ref type="table" target="#tab_0">2</ref>). Although the loss is a function of the entire sequence, here we adopt a greedy approach, similar to <ref type="bibr" target="#b7">Ebrahimi et al. (2018)</ref>; <ref type="bibr" target="#b44">Wallace et al. (2019)</ref>, in which we optimize for a higher-probability of the sequence having a specific attribute by considering changes only to the next token to be generated. This objective can be described as follows, where f is the discriminator:</p><formula xml:id="formula_7">log p(a|x) = log f (o :t+1 , o t+2 )<label>(5)</label></formula><p>Note that o t+2 is a function of x t+1 . Further, x t+1 ∼ Softmax(W õt+1 ), which depends on ∆H t .</p><p>In the limit, minimizing the objective in Equation 5 corresponds to choosing x t+1 that produces the optimal o t+2 that maximizes f (o :t+1 , o t+2 ). However, this limits the diversity of the generated text and could potentially lead to language degeneration <ref type="bibr" target="#b13">(Holtzman et al., 2019)</ref>. Alternatively, we focus on a softer optimization approach where we aim to shift the distribution pt+1 = Softmax(W õt+1 ) towards one that in expectation has a higher likelihood of having the desired attribute a. Possible approaches to accomplishing this are using REINFORCE <ref type="bibr" target="#b45">(Williams, 1992)</ref> and the Gumbel-Softmax trick <ref type="bibr" target="#b15">(Jang et al., 2016)</ref>. However, both of these would slow down convergence. Instead, as in <ref type="bibr" target="#b4">Dai et al. (2019a)</ref>, we use the distribution pt+1 (instead of a hard sample x t+1 ), and feed it forward to obtain (a biased) estimate of the next token's embedding and then update ∆H t .</p><p>The sentiment discriminator here distinguishes sentiment between POSITIVE and NEGATIVE and is trained on the SST-5 dataset <ref type="bibr" target="#b39">(Socher et al., 2013)</ref>. Table <ref type="table">5</ref> shows PPLM-Discrim generated samples in triplets: uncontrolled, controlled for POSITIVE sentiment, controlled for NEGATIVE sentiment.</p><p>For automatic and human evaluation, we use 15 prefixes (see the full list in Section S15) to generate 45 samples for each of two sentiment classes: very positive and very negative. Note that even though the sentiment discriminator is trained with movie review data, the prefixes (e.g. "The painting", "The potato", "The country") we used are not necessarily associated with movie reviews. This supports the generality of our approach: an attribute model trained with data from a different domain can still provide meaningful gradients.</p><p>Table <ref type="table" target="#tab_5">6</ref> shows evaluation results. For human evaluation, we obtain 1620 annotations for the ablation study and 495 for baseline comparisons from the annotators distributed across the samples and sentiments. Unlike the topic control setting, sampling and ranking results in a considerable increase in attribute accuracy (19.3% → 41.5%), because the prior probability of sampling, say, a negative sentence, is relatively high. BC results in a decrease in fluency when compared to B, while being significantly more consistent with the desired attribute (19.3% → 39.6%). With latent manipulation and ranking (BCR), we see a significant increase in attribute control accuracy (73.7%) while retaining fluency similar to B and BR. Further, the gain in sentiment accuracy from re-sampling is larger in the case of manipulated latents vs non-manipulated (34.1% increase from BC to BCR &gt; 22.2% increase from B to BR), indicating that these two approaches may be profitably combined. We also evaluate attribute control with an external sentiment classifier trained on IMDB movie reviews <ref type="bibr" target="#b23">(Maas et al., 2011)</ref>, which is a different dataset from the one used to train the attribute model <ref type="bibr" target="#b39">(Socher et al., 2013)</ref>, and the same rough story holds, albeit with smaller gaps between approaches. We compare to baselines CTRL, GPT2-FT-RL, and WD. BCR performs comparably to CTRL (73.7% and 80.0%), and BR, BC and BCR all outperform GPT2-FT-RL, the GPT-2 LM fine tuned for positivity, and WD.  <ref type="formula">2019</ref>) conducted adversarial attacks that make GPT-2 produce racist output when given a carefully optimized trigger string as prefix. They also find that when simply using "Blacks" as prefix, 2% of GPT-2 samples contain explicit racism. Other prefixes (e.g., "Asians" or "Jews") are mentioned but no percentage is reported. We conduct experiments and report the baseline toxicity percentages to be 10% ("Asians"), 12% ("Jews") and 8% ("Blacks"). With adversarial triggers generated from the released codebase by <ref type="bibr" target="#b44">Wallace et al. (2019)</ref> the average toxicity percentage is 63.6%. Further details can be found in Section S13.</p><p>PPLMs can be easily adapted for language detoxification by plugging in a toxicity classifier as the attribute control model and update latents with the negative gradient. We train a single layer classifier on the toxicity data from the Toxic Comment Classification Challenge (Jigsaw) and show that with a similar hyper-parameter setting as other PPLM-Discrim methods, it works well on both natural prompts and adversarial triggers. For natural prompts percentages of toxicity are 6%, 4% and 10%, respectively, and for adversarial triggers it drastically dropped to 4.6% on average, with statistical significance. Details on the annotation procedure and full table of percentage and p-values can be found in Table <ref type="table" target="#tab_3">S23</ref> and Section S13. Note that a model for detoxifying language can also potentially be maliciously used for generating toxic language, a topic we briefly discuss in Section S6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">CONTROLLED STORY WRITING</head><p>We explore controlled generation for assistive story writing <ref type="bibr" target="#b29">(Peng et al., 2018;</ref><ref type="bibr" target="#b22">Luo et al., 2019;</ref><ref type="bibr" target="#b47">Yao et al., 2019;</ref><ref type="bibr" target="#b9">Fan et al., 2018)</ref>. Using uncontrolled LMs for assistive art creation can be difficult. To help with the structure, we use predefined story skeletons often used in improvisation <ref type="bibr">(Adams)</ref>. We fill in the blank between these prefixes with a PPLM. See examples in Table <ref type="table" target="#tab_0">S20</ref> and Table <ref type="table" target="#tab_0">S21</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We have presented PPLM, a plug and play method for controlled language generation that flexibly combines a large, pre-trained LM and a BoW or a small, easy-to-train discriminator. In Section S6 we discuss the ethics of controlled LMs. PPLM achieves fine-grained control of attributes via a simple gradient-based sampling mechanism. Because PPLMs can flexibly control generation while maintaining fluency, they hold great promise for enabling the next generation of language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SUPPLEMENTARY INFORMATION FOR: PLUG AND PLAY LANGUAGE MODELS: A SIMPLE APPROACH TO CONTROLLED TEXT GENERATION S6 ETHICS OF CONTROLLED LANGUAGE MODELS</head><p>There has recently been a substantial discussion around the ethics of capable language models <ref type="bibr" target="#b33">(Radford et al., 2019;</ref><ref type="bibr" target="#b16">Keskar et al., 2019)</ref>, both in their potential to recapitulate problematic social biases and for them to be directly abused for societal harm (e.g. to generate disinformation). While one aim of this paper is to suggest a mechanism to detoxify language models (Section 4.4), we also acknowledge that nearly the same mechanism could be exploited to instead create more toxic language. Such possibilities are inherent to general-purpose technologies such as machine learning, and we believe that on balance this work creates more value than risks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S7 DETAILS ON BASELINE METHODS</head><p>We consider three baselines: CTRL, GPT2-FT-RL, and WD. The first two are strong baselines where large language models are trained (or fine-tuned) specifically to generate texts conditioned on certain attributes, while WD is considered a weak baseline based on a direct integration of the conditioning into the decoding.</p><p>For each baseline, we generate data from their method, and conduct the same human and automated evaluations. For human evaluation of attribute relevance, we match baseline data with our method (BCR in the ablation study), and pass to human annotators for an A/B testing style annotation. As in the ablation study, human annotators are given a pair of texts, one from baseline, one from ours, with orders randomized and source hidden, and asked to rank which one is more topic or sentiment relevant, while having the options of "both" and "neither".</p><p>On top of that, we have human annotators to give the fluency score of each text sample under each method individually. And automated evaluations of perplexity, sentiment, etc. are also done individually.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S7.1 CTRL</head><p>The recent conditional language model, CTRL, from <ref type="bibr" target="#b16">Keskar et al. (2019)</ref>, trains a 1.6B LM conditioned on around 50 control codes. We use the official released codebase<ref type="foot" target="#foot_2">2</ref> and their open-sourced model to generate samples for the CTRL baseline. Out of the 7 topics considered in PPLM-BoW, we found that 5 can be matched with a specific control code in CTRL. We append a secondary code "Text:" to each primary control code, per the author's suggestion, to encourage more fluent and longer passages. The 2 topics missing a match with CTRL are: Military, Space. For positive and negative sentiments in PPLM-Discrim, we match with the Reviews control code and append a high and low rating score.</p><p>The matched attributes and control codes are listed in Table <ref type="table" target="#tab_6">S7</ref>.</p><p>Under this setting, for each control code we generate texts prompted by the same prefixes used for corresponding PPLM attribute model (20 for PPLM-BoW, 15 for PPLM-Discrim). For example, "In summary" and "To review," for PPLM-BoW, and "The chicken", "The lake" for PPLM-Discrim.</p><p>Due to the near-greedy sampling method CTRL uses, for each prefix it generates one sample. Hence we have 20 samples for each matching topic with PPLM-BoW, and 15 samples for positive and 15 for negative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S7.2 GPT2-FT-RL</head><p>A recently released GPT-2 model fine-tuned using human feedback, from Ziegler et al. ( <ref type="formula">2019</ref>), showed success in summarization and text continuation in desired styles. To compare with PPLM, Christianity Text: POSITIVE (PPLM-Discrim) Reviews Rating: 5.0 NEGATIVE (PPLM-Discrim) Reviews Rating: 1.0 we run GPT2-FT-RL<ref type="foot" target="#foot_3">3</ref> to generate positive texts on the same prefixes used in our PPLM-Discrim experiment. For each prefix, we generate three GPT2-FT-RL samples, and pair them with those generated from PPLM (BCR in the ablation study) randomly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S7.3 WEIGHTED DECODING (WD)</head><p>We consider a simple baseline based on a direct integration of the conditioning into the decoding procedure, similar to the approach from Ghazvininejad et al. ( <ref type="formula">2017</ref>).</p><p>Topic Control with Bag of Words In Ghazvininejad et al. ( <ref type="formula">2017</ref>), the authors consider increasing the likelihood of sampling from a bag of key-words by performing beam-search with a modified scoring function.</p><formula xml:id="formula_8">score(w i , b t ) = score(b t ) + logP t+1 (w i ) + i 1 BoW (w i ),</formula><p>where 1 BoW (w i ) is an indicator function indicating if the token w i is present in the bag BoW. Since, it has been shown that beam-search results in degradation of language for GPT-2 <ref type="bibr" target="#b13">(Holtzman et al., 2019)</ref>, we consider top-5 sampling from a distribution pt+1 defined such that:</p><formula xml:id="formula_9">pt+1 (w i ) = p t+1 (w i ) + τ 1 BoW (w i )p t+1 (w i )</formula><p>where τ ∈ R ++ and p t+1 is the distribution over the vocabulary as predicted by the GPT-2 LM . For the experiments in Section 4, we set τ = 10.</p><p>Sentiment Control with Discriminator Here, we implemented weighted decoding similarly for sentiment control. Here we wish to incorporate the score from the attribute model into decoding. To control for style â, instead of sampling from the distribution p t+1 , we sample from pt+1 defined as:</p><formula xml:id="formula_10">pt+1 (w i ) ∝ p(a = â|x 0:t , w i )p t+1 (w i ).</formula><p>p(a = â|x 0:t , w i ) is the probabilty of the sequence x 0:t , w i possessing attribute â as assigned by the attribute model. By Bayes' rule, p(a = â; w i |x 0:t ) = p(a = â|x 0:t , w i )p t+1 (w i ), and we do top-5 sampling from this distribution. Recall that p t+1 (w i ) = p(w i |x 0:t ) under the language model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S8 FURTHER DETAILS ON HUMAN AND AUTOMATED EVALUATION</head><p>We conduct evaluations on attribute relevance and language fluency, both including human and automated evaluation.</p><p>For topic relevance (a.k.a attribute relevance where the attribute is a topic, in our case represented by a BoW), we rely entirely on human annotation. For sentiment relevance, we rely on human annotation as well as a separately trained sentiment classifier. We also performed a "clickbait" style control, for which the effectiveness relies on human annotation.</p><p>For fluency, we use human annotations (between 1 to 5) and automated methods: perplexity, Dist-1, Dist-2, and Dist-3 scores.</p><p>The number of human evaluations are as below:</p><p>• PPLM-BoW. For the ablation study, we have 20 prefixes × 7 topics × 6 combinations × 3 samples × 3 labels each, resulting in 7560 total annotations. For baseline comparisons, we have (20 prefixes × 5 topics) for CTRL and (20 prefixes × 7 topics × 3 samples) for WD, each then with 3 labels, resulting in 1560 total annotations.</p><p>• PPLM-Discrim, sentiments. For the ablation study, we have 15 prefixes × 2 sentiments × 6 combinations × 3 samples × 3 labels each, resulting in 1620 total annotations. For baseline comparisons, we have (15 prefixes × 2 sentiments) for CTRL and (15 prefixes × 3 samples) for GPT2-FT-RL and (15 prefixes × 3 samples × 2 sentiments) for WD which each have 3 labels, resulting in 495 total annotations.</p><p>• PPLM-Discrim, clickbait. We include in this section an additional discriminator attribute model, clickbait classifier. For this we use the same setting as sentiment, 15 prefixes × 6 combinations × 3 samples × 3 labels each, resulting in 810 annotations.</p><p>In ablation studies, the generation procedure for BCR, BR and BC is always initiated from the same random seeds. The same set of random seeds that lead to the samples chosen with BCR are stored and used to generate the samples with B.</p><p>The full table of all these measures, human and automated, on PPLM-BoW, seperated by sentiment and style, is in Table <ref type="table">S8</ref>. Included also are strong baselines (CTRL and WD) for each sentiment.</p><p>The human annotated topic relevance is further visualized in Figure <ref type="figure">S3</ref>. The fluency scores, while being across {B, BC,BR, BCR,} methods in the table, when shown in distribution are very similar, as seen in Figure <ref type="figure" target="#fig_14">S5</ref>.</p><p>The full table of all these measures, human and automated, on PPLM-discrm sentiments, is in Table <ref type="table">S9</ref>. Included also are strong baselines (CTRL, WD and GPT2-FT-RL) for each topic. The human annotated sentiment and style (e.g. "Clickbait") relevance is further visualized in Figure <ref type="figure" target="#fig_2">S4</ref>, along with congregated measures: all sentiments, all discriminators, all topics. The fluency scores again have similar distributions across {B, BC,BR, BCR,} methods, as seen in Figure <ref type="figure" target="#fig_15">S6</ref>. Figure <ref type="figure">S3</ref>: Topic relevance by human evaluation. We can see that taking a PPLM gradient step (B→BC) makes a big difference. Reranking is mostly helpful (B→BR; BC→BCR). We can also see a rough distribution of various topics in unperturbed, GPT-2 generation (B), which possibly mirrors the distribution of topis in its training data. Some topics, like science, naturally appear rather frequently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Computers</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S9 ODD COMBINATION OF TOPICS AND PREFIXES</head><p>It is interesting to see how PPLM can steer the text generation when the topic and prefix combination appears odd or illogical. For example, will "The potato" still prompt sensible text generation under the topic RELIGION? In this study we design a set of odd combinations, as bellow.</p><p>Table <ref type="table">S8</ref>: Full result of human and automated evaluation of PPLM-BoW, attribute relevance and language fluency. This is a detailed version of Table <ref type="table">4</ref>, where results were averaged over all topics. Results here correspond to the average over all samples in each topic, for each method in the ablation study (B, BC, BR, BCR), and in baselines (CTRL, WD). Perplexity is computed based on an external LM <ref type="bibr" target="#b31">(Radford et al., 2018a)</ref>, that is different from the LM generating text.  Table <ref type="table">S9</ref>: Full result of human and automated evaluation of PPLM-Discrim, attribute relevance and language fluency. The top two rows are a detailed version of Table <ref type="table" target="#tab_5">6</ref>, where results were averaged over both sentiments (except for GPT2-FT-RL, where there is only positive sentiment). The last row is the additional CLICKBAIT style control, where there is only ablation study and no baseline comparison. Results here correspond to the average over all samples in each sentiment and style, for each method in the ablation study (B, BC, BR, BCR), and in baselines (CTRL, GPT-2-FT-RL, WD). Perplexity is computed based on an external LM <ref type="bibr" target="#b31">(Radford et al., 2018a)</ref> • Prefixes of {"The chicken", "The horse", "The pizza", "The potato", "The lake"}, each controlled by topics of {MILITARY, LEGAL, COMPUTERS, POLITICS, RELIGION};</p><p>• Prefixes of {"My dog died", "The food is awful"}, each controlled by the sentiment of POSITIVE;</p><p>• Prefixes of "The food is amazing", controlled by the sentiment of NEGATIVE.</p><p>We found that PPLM control is easy even under those scenarios. We had to increase the strength α two or three fold (to 0.02 or 0.03 as opposed to 0.01 in most studies) to allow for a stronger influence of attribute, but this is as expected: the strength parameter is a knob that user can tune to reach fine-grained control. The resulting generation is included in Table <ref type="table">S10</ref> -Table <ref type="table" target="#tab_10">S16</ref>.</p><p>Table <ref type="table">S11</ref>: Examples generated from a designed odd combination of topic and prefix pairs. The topic here is <ref type="bibr">[Legal]</ref> . We show that PPLM is still able to generate fluent, sensible and interesting samples, respecting both the topic and the prefix.</p><p>[Legal]</p><p>Table <ref type="table">S14</ref>: Examples generated from a designed odd combination of topic and prefix pairs. The topic here is <ref type="bibr">[Religion]</ref> . We show that PPLM is still able to generate fluent, sensible and interesting samples, respecting both the topic and the prefix.</p><p>[Religion]</p><p>Table <ref type="table">S15</ref>: Examples generated from a designed odd combination of topic and prefix pairs. The topic here is [Space] . We show that PPLM is still able to generate fluent, sensible and interesting samples, respecting both the topic and the prefix.</p><p>[Space]</p><p>The The chicken-and-egg problem, which has dogged the world since it was first described in 1873.\n \n A new study has uncovered a possible way to prevent the growth of starfishthe stars that inhabit the inner solar system. It may even help to keep them alive.\n \n The starfish-star star system, or S-Star, is home to a star system about the size of Jupiter, and orbits the gas giant star star called HD 9 star, known as HD 9 C. . . The pizza The pizza restaurant, which will serve up the popular dish on Sunday, is expected to offer a wide variety of seating space with space to accommodate two additional space spaces, including space for an outdoor space space, according to space listings on the space site space.space. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The lake</head><p>The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake-like island nation has been ruled as a hostile alien planet in space and Earth space for decades and now, thanks to NASA's Curiosity Mars rover, we know for sure the planet's surface is covered in an extremely thick and reflective mantle of rock.\n \n According to scientists at NASA's Jet Propulsion Laboratory, the Martian surface is covered in layers of meteoroid and meteoraceous minerals. This is not just the planet's moon, planet Earth has a moon, planet. . . The food is awful The food is awful, but there is also the music, the story and the magic!\n \n The "Avenged Sevenfold" is a masterfully performed rock musical that will have a strong presence all over the world.\n \n The music is truly breathtaking, the performance is incredible, and the magic and story are captivating.\n \n The "Avenged Sevenfold" is a masterpiece! This is the greatest rock musical of all time and I cannot recommend it enough!\n \n The musical is beautiful. . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>[Negative]</head><p>The </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S10 FINE-GRAINED CONTROL WITH PPLM-BOW</head><p>Table <ref type="table" target="#tab_12">S17</ref> shows the subtle effect when you turn the step size α up, while keeping everything else (hyperparameters, text prefix) the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S11 HYPERPARAMETERS</head><p>We list, in Table <ref type="table">S18</ref>, the full set of hyperparameters used in each task in the experiments section, corresponding to results in Table <ref type="table" target="#tab_5">4 and Table 6</ref>, as well as in Section 4.4. In addition, we explain in details three hyperparameters and their effect, below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S11.1 EARLY STOPPING OF LATENT UPDATES</head><p>Degeneration (the occurrence of repetitive words) is a known issue with language generation <ref type="bibr" target="#b13">(Holtzman et al., 2019)</ref>, and we found it to be a case in PPLM-BoW when the update step size α is too large. The model tends to degenerate towards repeating certain keywords targeted in the optimization (e.g. words in the BoW). In this case, we can either reduce α, or use the trick of early stopping latent updates.</p><p>Examples shown in Table <ref type="table">S19</ref>. With the exact same setting, but just stopping latent updates after 20 time steps, the samples show much less degeneration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S11.2 FINITE HORIZON UPDATE</head><p>As opposed to updating the entire vector H t , which consists of key-value pairs corresponding to every token in the prefix, we consider modifying the key-value pairs corresponding to the most recent w tokens. At each time-step t, we only modify H t [t − w : t]. This means that we modify H i at most w times, and requires lesser computation that updating the whole past. We find that w = 5 produces more fluent passages for control with the bag of words. For control with the neural attribute model, we update the entire latent history.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S11.3 ADAPTIVE GRADIENT NORMALIZATION</head><p>For the bag-of-words based attribute model, what we wish to enforce is that a word from the bag appears at least once in the generated passage and not at every time-step. To account for this, instead of normalizing directly by the gradient norm as in Equation <ref type="formula" target="#formula_4">3</ref>, we normalize by the maximum gradient norm over time. This implies that we make smaller updates when it is less likely for The potato, or potato chip, is one of the best-selling snacks in the world!\n \n It comes in a variety of colors, is gluten-free (except for gluten-free chips), low in fat and saturated fat, and can be made in just 30 minutes, using only 6 simple ingredients. It's also easy to make, and can be stored in its own tin. \n \n The potato chip, however, may not be for everyone. The potato, a tuber of the carrot family (Cotus russulus), is an edible plant that is native to Central and South America, Asia, Europe and the Mediterranean. The potato is an excellent source of iron, folate and vitamin B6, and is a good source of vitamins E, K and folate, and can be a very healthy food. In addition, a potato has a rich, rich nutritional profile. It is high in protein, potassium, magnesium, zinc,. . . The potato, a tuber of the carrot family (Cotus russulus), is an edible plant that is widely cultivated as a food and a medicinal plant. Potato is widely used to make various products. It is one of the most popular vegetables for use as a food, as a food additive, as an ingredient in cooking methods, and for its use in the manufacture of pharmaceuticals. The most commonly consumed portion is a large potato. In China, there are about 30 varieties of potato. It is. . . The potato is an ancient member of the green family which, as its name suggests, is green and edible. It is native to the Middle East and parts of South Asia. It is an important crop in many parts of the world. The potato, a member of the potato family, has been domesticated for thousands of years. It can be eaten raw and cooked in its skins; it is also used as a starch and is a great source of energy and fiber in many diets and in many countries. . . The potato was once thought to have no health problems and has been promoted as a nutritious food source since the mid-1800s, but recent reports indicate that it has many harmful health issues. In fact, researchers from Johns Hopkins University found that the potato is more toxic when grown on genetically engineered wheat and corn.\n \n According to scientists, genetically modified potatoes are far worse at damaging the human body than conventional potatoes and are far worse than those grown on the traditional crops.\n \n The study also revealed. . . The potato plant, a member of the onion family.\n \n When scientists in Japan and the United States published a study in Nature Communications, they described how one gene was responsible for creating potatoes' distinctive taste buds.\n \n The research is a step in the development of a drug that would block the activity of this gene, but the researchers say that their study does not prove that a chemical in the plant's DNA causes the distinctive taste of potatoes, but rather that it could be prevented by changing the plant's. . . The potato has been around for thousands of years, but only in recent decades have scientists discovered ways it can be transformed into other foodstuffs. Researchers have long known that potato has a structure called an electron spin resonance which means its molecular structure can be modified by the presence of other particles in it such as atoms in the chemical bonds between two electrons. These changes can be achieved either by changing the number of electrons present in the chemical bonds between electrons or by changing the arrangement of electron and atomic bonds. In both. . . The potato, which scientists at the lab experiment experiment experiment experiment experiment experiment experiment experiment experiment experiment experiment experiment experiment experiment experiment experiment experiment experiment experiment experiment experiment experiment experiment experiment experiment experiment experiment experiment experiment experiment experiment experiment experiment experiment experiment experiment experiment experiment experiment experiment experiment experiment experiment experiment experiment experiment . . . a word from the bag of words to appear. Formally, the normalization constant at time-step t is:</p><formula xml:id="formula_11">max i=0...t ∇ H (i) L(o i+1 ) .</formula><p>Table <ref type="table">S18</ref>: The full set of hyperparameters used in each task in the experiments section. Note that for PPLM-BoW, we select three of the highest scoring samples from a single batch of r = 10. For PPLM-Discrim, we get 1 sample per batch, across 3 batches of r = 10. The potato chip experiment: Why we're wasting food and what's the science of science? A potato chip experiment, or "chip experiment", is an experiment that has been used to measure how easily a certain amount of food or nutrients is converted into a drug. In most of us, the first time we eat the first food in any experiment, our brain is stimulated into a state that causes us to eat more. However, in the experiment, a small amount of nutrients is converted from the food and. . . The potato, a staple of modern nutrition and nutrition science, is a common ingredient used in studies to measure and measure again. And, of course, scientists have used potato for decades. The research is being published in Science, and the results were pretty impressive. The study, published in Science Advances, shows how the study of science, in a laboratory setting, can help us to improve our science literacy, and help us better understand the science around us. This means better science communication,. . . Table <ref type="table" target="#tab_0">S20</ref>: Skeleton story generation with different attribute models. Each story is generated within a fixed skeleton skeleton skeleton skeleton skeleton skeleton skeleton skeleton skeleton skeleton skeleton skeleton skeleton skeleton skeleton skeleton skeleton, and then either uncontrolled (top row), or controlled with an attribute model. Keywords that signify the controlled effect are highlighted.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S17 WORD LISTS FOR BAG OF WORDS APPROACHES</head><p>We curate word lists from www.enchantedlearning.com/wordlist. <ref type="bibr">: astronomy, atom, biology, cell, chemical, chemistry, climate, control, data, electricity, element, energy, evolution, experiment, fact, flask, fossil, funnel, genetics, gravity, hypothesis, lab, laboratory, laws, mass, matter, measure, microscope, mineral, molecule, motion, observe, organism, particle, phase, physics, research, scale, science, scientist, telescope, temperature, theory, tissue, variable, volume, weather, weigh Fantasy/Magic: beast, Cerberus, demon, dragon, fairy, Frankenstein, ghost, Godzilla, giant, horror, hydra, imp, monster, mummy, ogre, orc, savage, spirit, sprite, titan, troll, undead, unicorn, vampire, witch, zombie Space: planet, galaxy, space, universe, orbit, spacecraft, earth, moon, comet, star, astronaut, aerospace, asteroid, spaceship, starship, galactic, satellite, meteor Politics: affirm, appropriation, aristocracy, authoritarian, authority, authorization, brief, capitalism, communism, constitution, conservatism, court, deficit, diplomacy, direct, democracy, equality, exports, fascism, federation, government, ideology, imports, initiative, legislature, legitimacy, liberalism, liberty, majority, order, political, culture, politics, power, primary, property, ratification, recall, referendum, republic, socialism, state, subsidy, tariff, imports, tax, totalitarian Military: academy, advance, aircraft, ally, ammo, ammunition, armor, arms, army, arrow, arsenal, artillery, attack, attention, ballistic, barracks, base, battalion, battery, battle, battlefield, bomb, bombard, bombardment, brig, brigade, bullet, camouflage, camp, cannon, captain, capture, carrier, casualty, catapult, cavalry, colonel, combat, command, commander, commission, company, conflict, conquest, convoy, corps, covert, crew, decode, defeat, defend, defense, destroyer, division, draft, encode, enemy, engage, enlist, evacuate, explosive, fight, fire, fleet, force, formation, fort, front, garrison, general, grenade, grunt, guerrilla, gun, headquarters, helmet, honor, hospital, infantry, injury, intelligence, invade, invasion, jet, kill, leave, lieutenant, major, maneuver, marines, MIA, mid, military, mine, missile, mortar, navy, neutral, offense, officer, ordinance, parachute, peace, plane, platoon, private, radar, rank, recruit, regiment, rescue, reserves, retreat, ribbon, sabotage, sailor, salute, section, sergeant, service, shell, shoot, shot, siege, sniper, soldier, spear, specialist, squad, squadron, staff, submarine, surrender, tactical, tactics, tank, torpedo, troops, truce, uniform, unit, veteran, volley, war, warfare, warrior, weapon, win, wound</ref> Religion: Absolute, Affect, Aid, Angel, Anthem, Apostle, Archangel, Archbishop, Balance, Ban, Belief, Benefit, Bible, Bishop, Bless, Blessing, Bliss, Bond, Bow, Buddhism, Canon, Cantor, Cathedral, Celestial, Chapel, Charity, Choice, Christianity, Church, Comfort, Community, Conflict, Connection, Conquest, Conservative, Control, Conversion, Convert, Core, Counsel, Courage, Covenant, Creative, Creator, Creed, Cross, Crusade, Darkness, Decision, Deity, Destiny, Devil, Disciple, Discipline, Discussion, Divine, Divinity, Doctrine, Duty, Effect, Elder, Energy, Essence, Eternal, Ethics, Event, Evidence, Exile, Exodus, Faith, Family, Fate, Father, Favor, Fundamental, Gift, Glory, God, Gospel, Grace, Growth, Guru, Habit, Hallow, Halo, Happiness, Harmony, Healing, Heaven, Hebrew, Holy, Honor, Hope, Host, Humane, Immortal, Influence, Insight, Instruction, Issue, Jesuit, Jesus, Joy, Judaism, Judgment, Justice, Karma, Keen, Keystone, Kingdom, Latin, Life, Light, Love, Loving, Marriage, Meaning, Mercy, Messiah, Minister, Miracle, Mission, Mortal, Mosque, Movement, Music, Mystery, Nature, Nun, Official, Oracle, Order, Organ, Orthodox, Outlook, Pacific, Pagan, Parish, Participation, Pastor, Patriarch, Peace, Perception, Personal, Perspective, Petition, Pilgrim, Politics, Power, Practice, Prayer, Prelude, Presence, Priest, Principle, Privacy, Prophet, Protection, Purpose, Query, Quest, Question, Quiet, Radiant, Radical, Rally, Rebirth, Redemption, Refuge, Relationship, Relative, Religion, Religious, Revelation, Ritual, Role, Sacrament, Sacred, Sacrifice, Sage, Saint, Salvation, Sanctuary, Savior, Scripture, Scriptures, Sect, Security, Sense, Serious, Serve, Service, Sharia, Shepherd, Shrine, Silence, Sin, Society, Soul, Source, Spirit, Spiritual, Split, Statue, Sunday, Support, Supreme, <ref type="bibr">Teaching, Temple, Tests, Text, Torah, Tradition, Traditional, Trust, Unique, Unity, Unknown, Value, Vanity, Virtue, Vision, Voice, Voices, Watch, Weight, Whole, Wisdom, Wonder, Yang, Yin, Zeal Computers: algorithm, analog, app, application, array, backup, bandwidth, binary, bit, bite, blog, blogger, bookmark, boot, broadband, browser, buffer, bug, bus, byte, cache, caps, captcha, CD, client, command, compile, compress, computer, configure, cookie, copy, CPU, dashboard, data, database, debug, delete, desktop, development, digital, disk, document, domain, dot, download, drag, dynamic, email, encrypt, encryption, enter, FAQ, file, firewall, firmware, flaming, flash, folder, font, format, frame, graphics, hack, hacker, hardware, home, host, html, icon, inbox, integer, inter-face, Internet, IP, iteration, Java, joystick, kernel, key, keyboard, keyword, laptop, link, Linux, logic, login, lurking, Macintosh, macro, malware, media, memory, mirror, modem, monitor, motherboard, mouse, multimedia, net, network, node, offline, online, OS, option, output, page, password, paste, path, piracy, pirate, platform, podcast, portal, print, printer, privacy, process, program, programmer, protocol, RAM, reboot, resolution, restore, ROM, root, router, runtime, save, scan, scanner, screen, screenshot, script, scroll, security, server, shell, shift, snapshot, software, spam, spreadsheet, storage, surf, syntax, table, tag, template, thread, toolbar, trash, undo, Unix, upload, URL, user, UI, username, utility, version, virtual, virus, web, website, widget, wiki, window, Windows, wireless, worm, XML, Zip Legal: affidavit, allegation, appeal, appearance, argument, arrest, assault, attorney, bail, bankrupt, bankruptcy, bar, bench, warrant, bond, booking, capital, crime, case, chambers, claim, complainant, complaint, confess, confession, constitution, constitutional, contract, counsel, court, custody, damages, decree, defendant, defense, deposition, discovery, equity, estate, ethics, evidence, examination, family, law, felony, file, fraud, grievance, guardian, guilty, hearing, immunity, incarceration, incompetent, indictment, injunction, innocent, instructions, jail, judge, judiciary, jurisdiction, jury, justice, law, lawsuit, lawyer, legal, legislation, liable, litigation, manslaughter, mediation, minor, misdemeanor, moot, murder, negligence, oath, objection, opinion, order, ordinance, pardon, parole, party, perjury, petition, plaintiff, plea, precedent, prison, probation, prosecute, prosecutor, proxy, record, redress, resolution, reverse, revoke, robbery, rules, sentence, settlement, sheriff, sidebar, standing, state, statute, stay, subpoena, suit, suppress, sustain, testimony, theft, title, tort, transcript, trial, trust, trustee, venue, verdict, waiver, warrant, will, witness, writ, zoning</ref>  The pizza-themed, pizza-themed, pizza-themed game that has everyone guessing, has finally released! The game, known as Pizza-Pocalypse, has released on Steam and on GOG. Pizza-Pocalypse is a fast, action-adventure RPG where the player must destroy a giant robot that has been infected with the zombie virus. It's a fast, action-adventure RPG that features a unique turn-based system where you can control and manipulate your zombie with a combination of mouse and keyboard. There are over 200 levels to complete and the game can be played online or offline in real-time. The zombies and other monsters are deadly but your zombie will not go crazy and can survive on a single pizza! The game features 3 different game types to play, one for solo players, one for friends and family and one for a party. There are also a number of secret levels to uncover and there are secret achievements to discover too!. . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Science</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>[Computers] [Fantasy] [Clickbait]</head><p>The pizza military readiness. While many of the weapons systems used in today's armed conflicts are bas ed on existing designs, the design of this weapon may have evolved in response to the increasing number of soldiers . . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>To conclude To conclude To conclude To conclude To conclude To conclude To conclude To conclude To conclude To conclude To conclude To conclude To conclude To conclude To conclude</head><p>To conclude To conclude, a large percentage of the population is aware of how much they contribute to society, but is not yet fully aware that there are many other people in their community who are in the same boat, and we have to fight with each other and our enemy . . . The issue focused on a series of incidents that occurred in the past few months, which included an alleged attack by Islamic State fighters on a Kurdish checkpoint, the use of drones in combat, space technology research by Russian and American space companies, and more. The world . . . Foundational to this Foundational to this Foundational to this Foundational to this Foundational to this Foundational to this Foundational to this Foundational to this Foundational to this Foundational to this Foundational to this Foundational to this Foundational to this Foundational to this Foundational to this Foundational to this Foundational to this is the fact that the solar part of the word solar, as we've been taught it, refers either to the star that creates the Sun as seen from the Earth, or to the Earth itself. As such, solar system, planets, .  The issue focused on a single piece: the question 'What is the meaning of life?' This question has puzzled many philosophers, who have attempted to solve it by using some of the concepts of quantum mechanics, but they have to solve it by the laws of nature themselves. . . . Foundational to this Foundational to this Foundational to this Foundational to this Foundational to this Foundational to this Foundational to this Foundational to this Foundational to this Foundational to this Foundational to this Foundational to this Foundational to this Foundational to this Foundational to this Foundational to this Foundational to this is the idea that the best way to understand the universe is to understand it in its fundamental constituents. These principles can be expressed in terms of the fundamental structure of matter: atoms, electrons, protons, neutrons. These fundamental constituents are the . . . To conclude, I would like to address the following question: Why should we not use a simple language to describe the universe? There is a very clear answer for why this question cannot be answered. If you look at nature as a continuum, the universe is . . . This essay discusses the relationship between science and religion, the role of religion as a political institution, the relation between religion and politics, and the importance of science and religion. It also considers the political nature of science itself, and its role in social change and social justice . . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>To conclude To conclude To conclude To conclude To conclude To conclude To conclude To conclude To conclude To conclude To conclude To conclude To conclude To conclude To conclude</head><p>To conclude To conclude, I think there are many problems in the way of economic democracy, and we have a tendency to blame it on a lack of democracy in the country of the ruling family. In a democracy, one party is allowed to run the country, one party can . . .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure S4 :</head><label>S4</label><figDesc>Figure S4: Bar charts of discriminator relevance by human evaluation, together with different versions of combined results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>was first discovered by British biologist Sir Richard Bedding starling in a remote part of Mars\n \n Mars has seen two major life-giving planets.\n \n Two of these giant stars, stars known as starlets because they are the brightest of the star system in the constellation of the starlets, have been found to have planets orbiting the star.\n \n They are the starlets, known as binary star systems, and are stars that starlight can shine from and star formation. . .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>If you are trying to lose weight, it. . .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>a staple of most diets, seems to be the most popular vegetable among researchers. The research has been published and peer-reviewed.\n \n The potato has a unique ability. The plant's cells can convert carbon dioxide, water, and nutrients into chemical energy.\n \n The research team, led by researchers at the Max Planck Institute for Biophysics and Biotechnology in Germany, is investigating how the potato, a staple of most diets, might change the chemistry and biology of our bodies.. . .[Science; 0.04]  </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>is a delicious treat that can be enjoyed in the laboratory experiment, but is it safe for humans? \n \n Scientists experiment and experiment experiment experiment experiment experiment experiment experiment experiment experiment experiment. . . . . .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>λ kl = 0.01, α = 0.01, γ = 1.5, γ gm = 0.9, r = 10, τ = 0.85 PPLM-BoW Religion m = 3, λ kl = 0.01, α = 0.01, γ = 1.5, γ gm = 0.8, r = 10, τ = 0.85 PPLM-Discrim POSITIVE, NEGATIVE m = 10, λ kl = 0.01, α = 0.03, γ = 1.0, γ gm = 0.95, r = 10, τ = 0.9 PPLM-Discrim Detoxicification m = 10, λ kl = 0.01, α = 0.02, γ = 1.0, γ gm = 0.9, r = 1, τ = 0TableS19: The effect of using early stopping of latent updates to prevent sample degeneration.Before (Latent updates at every generation step) After (Latent updates for only the first experiment: Why we're wasting food and what's the science of science? A potato chip experiment, or experiment experiment experiment experiment experiment experiment experiment experiment experiment experiment experiment experiment experiment experiment experimental experiment experiment experiment experiment experiment experiment experiment experiment experiment experiment experiment experiment experiment experiment experiment experiment experiment experiment. . .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>a staple of modern nutrition and nutrition science, is a common ingredient used in studies to measure and measure science research results, and is the basis of science experiments. Science science science science science science science science science science science science science science science science science science science science science science science science science science science science science science science science science science science science science science science . . .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure S5 :</head><label>S5</label><figDesc>Figure S5: Histogram illustrating the distribution of fluency scores based on controlled generated with PPLM-BoW from the four methods considered for ablation study. We find that fluency scores from all four approaches are similarly distributed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure S6 :</head><label>S6</label><figDesc>Figure S6: Histogram illustrating the distribution of fluency scores based on controlled generated with PPLM-Discrim from the four methods considered for ablation study. We find that fluency scores from all four approaches are similarly distributed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>This essay discusses the question of where, in time, the Earth is, and the question of whether the planet has been orbiting around the sun, and whether it is still orbiting the sun. There are two kinds of orbits that can occur on a comet: . . . conclude, we need to look at what the most powerful weapons in our arsenal are capable of achieving when we are all together in a room together. What can we say about space? It's an enormous object with a radius of about 10 light years.. . .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>Comparison of the different models and distributions. All models in this table are useful in different scenarios. The particular advantage of PPLM is that very small, custom attribute models, p(a|x), may be combined with powerful, general pre-trained language models, p(x), to create cheap but still powerful conditional generative models, p(x|a).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Example models</cell></row><row><cell>Model type</cell><cell>Form of model</cell><cell>Samples</cell><cell>and number of trainable params</cell></row><row><cell>Language Model</cell><cell>p(x)</cell><cell>Uncond.</cell><cell>GPT-2 medium: 345M (Radford et al., 2019)</cell></row><row><cell>Fine-tuned Language Model</cell><cell>p(x)</cell><cell>Uncond.</cell><cell>Fine-tuned GPT-2 medium: 345M (Ziegler et al., 2019)</cell></row><row><cell>Conditional Language Model</cell><cell>p(x|a)</cell><cell>Cond.</cell><cell>CTRL: 1.6B (Keskar et al., 2019)</cell></row><row><cell>Plug and Play</cell><cell></cell><cell></cell><cell>PPLM-BoW: 0 (curated word list)</cell></row><row><cell>Language Model</cell><cell>p(x|a) ∝ p(x)p(a|x)</cell><cell>Cond.</cell><cell>PPLM-Discrim: ∼ 1K/attribute</cell></row><row><cell>(PPLM)</cell><cell></cell><cell></cell><cell>(not counting pretrained p(x))</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison of different samples generated by (top row) baseline GPT-2 and (other rows) PPLM with different BoW corresponding to different topics (e.g.</figDesc><table><row><cell>[Military] ), all conditioned on a</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Evaluation of models/ variants on the sentiment control task, with mean±std-dev reported across fluency metrics. Sentiment accuracy reports the fraction of samples with an accurate target sentiment. Approach BCR provides significant control over sentiment while showing minimal degradation in fluency. See TableS9for full results on individual sentiments. *GPT2-FT-RL is only evaluated for the positivity half of the task, as it is fine-tuned only for positivity<ref type="bibr" target="#b52">(Ziegler et al., 2019)</ref>. For human evaluation metrics, we compare the baselines CTRL, GPT2-FT-RL and WD with BCR and perform A/B style testing. We include both numbers for comparison.</figDesc><table><row><cell>Method</cell><cell cols="2">Sentiment Acc. (%) Sentiment Acc. (%)</cell><cell>Perplexity</cell><cell>Dist-1</cell><cell>Dist-2</cell><cell>Dist-3</cell><cell>Human Evaluation</cell></row><row><cell></cell><cell>(human)</cell><cell>(external classifer)</cell><cell>(↓ better)</cell><cell cols="4">(↑ better) (↑ better) (↑ better) Fluency (↑ better)</cell></row><row><cell>B</cell><cell>19.3</cell><cell>52.2</cell><cell>42.1±33.14</cell><cell>0.37</cell><cell>0.75</cell><cell>0.86</cell><cell>3.54±1.08</cell></row><row><cell>BR</cell><cell>41.5</cell><cell>62.2</cell><cell>44.6±34.72</cell><cell>0.37</cell><cell>0.76</cell><cell>0.87</cell><cell>3.65±1.07</cell></row><row><cell>BC</cell><cell>39.6</cell><cell>64.4</cell><cell>41.8±34.87</cell><cell>0.33</cell><cell>0.70</cell><cell>0.86</cell><cell>2.79±1.17</cell></row><row><cell>BCR</cell><cell>73.7</cell><cell>78.8</cell><cell>46.6±40.24</cell><cell>0.36</cell><cell>0.77</cell><cell>0.91</cell><cell>3.29±1.07</cell></row><row><cell>CTRL</cell><cell>76.7</cell><cell>96.6</cell><cell>37.4±16.89</cell><cell>0.35</cell><cell>0.78</cell><cell>0.89</cell><cell>3.54±0.77</cell></row><row><cell>BCR</cell><cell>70.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>3.36±0.82</cell></row><row><cell>GPT2-FT-RL*</cell><cell>13.3</cell><cell>77.8</cell><cell>217.3±176.4</cell><cell>0.54</cell><cell>0.91</cell><cell>0.94</cell><cell>3.31±0.84</cell></row><row><cell>BCR</cell><cell>84.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>3.68±0.83</cell></row><row><cell>WD</cell><cell>18.9</cell><cell>52.2</cell><cell>31.7±28.0</cell><cell>0.33</cell><cell>0.69</cell><cell>0.83</cell><cell>3.67±0.89</cell></row><row><cell>BCR</cell><cell>61.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>3.75±0.66</cell></row><row><cell cols="3">4.4 LANGUAGE DETOXIFICATION</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Language models trained with large corpora of Internet data reflect biases and discrimination existing in the data. A recent paper byWallace et al. (</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table S7 :</head><label>S7</label><figDesc>Control codes used for the model from Keskar et al. (2019) for experiments in Section 4.</figDesc><table><row><cell>PPLM Attribute</cell><cell>CTRL Control Code</cell></row><row><cell>LEGAL (PPLM-BoW)</cell><cell>Legal Text:</cell></row><row><cell>POLITICS (PPLM-BoW)</cell><cell>Politics Text:</cell></row><row><cell>SCIENCE (PPLM-BoW)</cell><cell>Science Text:</cell></row><row><cell>COMPUTERS (PPLM-BoW)</cell><cell>Technologies Text:</cell></row><row><cell>RELIGION (PPLM-BoW)</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>, that is different from the LM generating text.</figDesc><table><row><cell>Sentiment/Style</cell><cell>Method</cell><cell cols="2">Attribute relevance % (↑ better) Perplexity</cell><cell>Dist-1</cell><cell>Dist-2</cell><cell>Dist-3</cell><cell>Fluency (↑ better)</cell></row><row><cell></cell><cell></cell><cell>(human)</cell><cell cols="4">(↓ better) (↑ better) (↑ better) (↑ better)</cell><cell>(human)</cell></row><row><cell></cell><cell>B</cell><cell>34.8</cell><cell>39.47</cell><cell>0.37</cell><cell>0.74</cell><cell>0.86</cell><cell>3.67</cell></row><row><cell></cell><cell>BR</cell><cell>54.8</cell><cell>45.01</cell><cell>0.41</cell><cell>0.81</cell><cell>0.92</cell><cell>3.71</cell></row><row><cell>Negative</cell><cell>BC BCR</cell><cell>37.8 72.6</cell><cell>41.86 46.24</cell><cell>0.45 0.44</cell><cell>0.84 0.84</cell><cell>0.93 0.92</cell><cell>2.84 3.24</cell></row><row><cell></cell><cell>CTRL</cell><cell>73.3</cell><cell>37.94</cell><cell>0.43</cell><cell>0.85</cell><cell>0.92</cell><cell>3.17</cell></row><row><cell></cell><cell>WD</cell><cell>15.6</cell><cell>30.42</cell><cell>0.38</cell><cell>0.75</cell><cell>0.85</cell><cell>3.56</cell></row><row><cell></cell><cell>B</cell><cell>3.70</cell><cell>44.28</cell><cell>0.38</cell><cell>0.76</cell><cell>0.89</cell><cell>3.41</cell></row><row><cell></cell><cell>BR</cell><cell>28.1</cell><cell>42.96</cell><cell>0.44</cell><cell>0.84</cell><cell>0.92</cell><cell>3.59</cell></row><row><cell></cell><cell>BC</cell><cell>41.5</cell><cell>42.34</cell><cell>0.45</cell><cell>0.83</cell><cell>0.91</cell><cell>2.74</cell></row><row><cell>Positive</cell><cell>BCR</cell><cell>74.8</cell><cell>47.69</cell><cell>0.39</cell><cell>0.80</cell><cell>0.92</cell><cell>3.33</cell></row><row><cell></cell><cell>CTRL</cell><cell>80.0</cell><cell>36.78</cell><cell>0.45</cell><cell>0.86</cell><cell>0.92</cell><cell>3.91</cell></row><row><cell></cell><cell>GPT2-FT-RL</cell><cell>26.7</cell><cell>217.28</cell><cell>0.54</cell><cell>0.91</cell><cell>0.94</cell><cell>3.16</cell></row><row><cell></cell><cell>WD</cell><cell>22.2</cell><cell>33.04</cell><cell>0.41</cell><cell>0.78</cell><cell>0.90</cell><cell>3.78</cell></row><row><cell></cell><cell>B</cell><cell>36.3</cell><cell>38.59</cell><cell>0.38</cell><cell>0.79</cell><cell>0.91</cell><cell>3.46</cell></row><row><cell>Clickbait</cell><cell>BR BC</cell><cell>48.9 33.3</cell><cell>33.20 54.18</cell><cell>0.41 0.45</cell><cell>0.83 0.83</cell><cell>0.92 0.92</cell><cell>3.25 2.85</cell></row><row><cell></cell><cell>BCR</cell><cell>60.7</cell><cell>42.67</cell><cell>0.39</cell><cell>0.83</cell><cell>0.93</cell><cell>2.97</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table S16 :</head><label>S16</label><figDesc>Examples generated from a designed odd combination of topic and prefix pairs. The sentiment here is [Positive] and [Negative] . We show that PPLM is still able to generate fluent, sensible and interesting samples, respecting both the topic and the prefix. My dog died at the age of 92 years this year. He was a legend in our home state of Virginia.I have a tremendous heart, my soul, my spirit, my love. He is in heaven and I love him very much. We have a great bond. He is gone and I am so thankful for him!\n \n He had been in the hospital for so many months. My husband of over 60 years, Dr. Paul, was in charge of his care. He took his place as the. . .</figDesc><table><row><cell>[Positive]</cell><cell>My dog died My dog died My dog died My dog died My dog died My dog died My dog died My dog died My dog died My dog died My dog died My dog died My dog died My dog died My dog died My dog died</cell></row><row><cell></cell><cell>The food is awful The food is awful The food is awful The food is awful The food is awful The food is awful The food is awful The food is awful The food is awful The food is awful The food is awful The food is awful The food is awful The food is awful The food is awful</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>The food is amazing.\n \n It's also not.\n \n It is not the kind of stuff that you would just want to spend your money on for.\n \n I don't know why, but when I got my second box, it felt like a bad rip off.\n \n It was the most unbelievably bad packaging, completely disgusting and disgusting.\n \n This is not a joke, people.\n \n You get this shit.\n \n This is food for a million people.\n \n And you have. . .</figDesc><table><row><cell>food is amazing The food is amazing The food is amazing The food is amazing The food is amazing The food is amazing The food is amazing The food is amazing The food is amazing The food is amazing The food is amazing The food is amazing The food is amazing The food is amazing The food is amazing The food is amazing</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table S17 :</head><label>S17</label><figDesc>Comparison of different sentences generated by (top row) unconditioned GPT-2 and (other rows) our approach using GPT-2 with the same bags-of-words with different strength factors, while using the same prefix: "The potato</figDesc><table><row><cell>The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato".</cell></row><row><cell>[-] The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table S22 :</head><label>S22</label><figDesc>Examples of attribute controlled text generation with multiple knobs. We train a clickbait discriminator using the dataset from<ref type="bibr" target="#b30">Potthast et al. (2018)</ref> </figDesc><table><row><cell>[Computers] [Fantasy] [Clickbait] The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table S25 :</head><label>S25</label><figDesc>Comparison of different samples generated with different prefixes using the same PPLM-BoW control under the [Space] topic. All samples are generated using exact same hyperparameters.</figDesc><table><row><cell>The issue focused The issue focused The issue focused The issue focused The issue focused The issue focused The issue focused The issue focused The issue focused The issue focused The issue focused The issue focused The issue focused The issue focused The issue focused The issue focused</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table S26 :</head><label>S26</label><figDesc>Comparison of different samples generated with different prefixes using the same PPLM-BoW control under the [Science] topic. All samples are generated using exact same hyperparameters.</figDesc><table><row><cell>The issue focused The issue focused The issue focused The issue focused The issue focused The issue focused The issue focused The issue focused The issue focused The issue focused The issue focused The issue focused The issue focused The issue focused The issue focused The issue focused</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head></head><label></label><figDesc>This essay discusses the history of the modern art of writing, focusing specifically on the period in its development from the late nineteenth century to the 1960s, in which the concept was introduced into science. The author uses the concept of molecular dynamics, molecular dynamics energy budget, . . .</figDesc><table><row><cell>This essay discusses This essay discusses This essay discusses This essay discusses This essay discusses This essay discusses This essay discusses This essay discusses This essay discusses This essay discusses This essay discusses This essay discusses This essay discusses This essay discusses This essay discusses This essay discusses</cell></row><row><cell>To conclude To conclude To conclude To conclude To conclude To conclude To conclude To conclude To conclude To conclude To conclude To conclude To conclude To conclude To conclude To conclude</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table S27 :</head><label>S27</label><figDesc>Comparison of different samples generated with different prefixes using the same PPLM-BoW control under the [Politics] topic. All samples are generated using exact same hyperparameters. The issue focused on a single section of the legislation. It's unclear whether the committee will vote to extend the law, but the debate could have wider implications. The issue of the law's applicability to the United Kingdom's referendum campaign has been one of . . . Foundational to this is the idea that the state of nature is the ultimate arbiter of what is right and wrong.That is why we need a government that is committed to this principle. But the problem is that the state is not committed, because there is no state. . . .</figDesc><table><row><cell>The issue focused The issue focused The issue focused The issue focused The issue focused The issue focused The issue focused The issue focused The issue focused The issue focused The issue focused The issue focused The issue focused The issue focused The issue focused The issue focused</cell></row><row><cell>Foundational to this Foundational to this Foundational to this Foundational to this Foundational to this Foundational to this Foundational to this Foundational to this Foundational to this Foundational to this Foundational to this Foundational to this Foundational to this Foundational to this Foundational to this Foundational to this</cell></row><row><cell>This essay discusses This essay discusses This essay discusses This essay discusses This essay discusses This essay discusses This essay discusses This essay discusses This essay discusses This essay discusses This essay discusses This essay discusses This essay discusses This essay discusses This essay discusses This essay discusses</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">One normalization term is computed for each layer of the transformer.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1">Marjan Ghazvininejad, Xing Shi, Jay Priyadarshi, and Kevin Knight. Hafez: an interactive poetry generation system. In Proceedings of ACL 2017, System Demonstrations, pp. 43-48, Vancouver, Canada, July 2017. Association for Computational Linguistics. URL https://www.aclweb. org/anthology/P17-4008. Jiatao Gu, Graham Neubig, Kyunghyun Cho, and Victor OK Li. Learning to translate in real-time with neural machine translation. arXiv preprint arXiv:1610.00388, 2016.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2">CTRL codebase: https://github.com/salesforce/ctrl</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3">GPT2-FT-RL codebase: https://github.com/openai/lm-human-preferences</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_4">We choose top 3 samples from a single batch of 10 here</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>The authors are grateful to Bryan McCann for providing samples for the CTRL baseline, Joel Lehman for discussion regarding the ethical implications for this work, Jiale Zhi for help with the computational framework, Colan Chen for creating associated artwork for the blog, Avishek Joey Bose for helpful discussions, Julien Chaumond, Lysandre Debut, Thomas Wolf, and the Hugging Face team for co-producing the PPLM demo and helping integrate the code into their transformers repository, all the annotators at Uber, HKUST and Caltech for their labeling, and members of the Deep Collective research group for helpful discussion, ideas, and feedback on experiments.</p></div>
			</div>


			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Table 5: Sentence samples in triplets, generated by {baseline GPT-2, PPLM-Discrim POSITIVE, PPLM-Discrim NEGATIVE}, conditioned on prefixes: The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken &amp; The country The country The country The country The country The country The country The country The country The country The country The country The country The country The country The country</p><p>The country. Words related to the sentiment are highlighted (in soft red). Each triplet is generated from the same random seed. [-] The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken is now out on the grill. \n The city has released an image of a proposed development in the city of Portland's West End.. . . [Positive] The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken</p><p>The chicken was delicious -wonderfully moist, perfectly delicious, superbly fresh -and perfectly cooked. The only thing to say is that the sauce was excellent, and I think that the broth really complemented all of the other flavors. The best part was the sauce. . . [Negative] The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chickenpox epidemic may be over but the flu is about to get worse. The United States is facing one of the worst flu seasons on record and.</p><p>. . [-] The country The country The country The country The country The country The country The country The country The country The country The country The country The country The country The country The country's new chief minister, A.J. Paik, is a member of a group of prominent conservative politicians who have criticized the Obama administration's efforts to. . . [Positive] The country The country The country The country The country The country The country The country The country The country The country The country The country The country The country The country The country's largest indoor painting event!\n Come celebrate with a dazzling display of stunning outdoor murals, a stunning display of art, and the world's best paint and art supplies from all over the world! [Negative] The country The country The country The country The country The country The country The country The country The country The country The country The country The country The country</p><p>The country The country's top prison system is forcing prisoners to use a trash dump, rather than a toilet, to flush their waste out, as the authorities fear the waste is more toxic and could cause cancer, an official at a major prison has revealed.. . . Published as a conference paper at ICLR 2020 Table <ref type="table">S10</ref>: Examples generated from a designed odd combination of topic and prefix pairs. The topic here is [Military]  . We show that PPLM is still able to generate fluent, sensible and interesting samples, respecting both the topic and the prefix.</p><p>[Military]</p><p>The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken-shaped robot known as a "killer drone" is about to become a weapon in war.\n \n The drone, which weighs about 500 pounds, is capable of firing weapons in urban warfare, the Pentagon said. It can fly at speeds of 30 miles an hour.\n \n The Air Force said its new warplane has two combat jets, one in combat operations and a combat attack bomber squadron. Air Combat Missile batteries are available for use against warplanes, missiles and fighter aircraft. The horse The horse The horse The horse The horse The horse The horse The horse The horse The horse The horse The horse The horse The horse The horse The horse</p><p>The horse-mounted artillery system (HMG) is a special weapon system employed in World War I, World War II, and World War II Korea. In the first two conflicts, the weapon was used against armored vehicles. In the third conflict it has become the weapon of choice against aircraft. HMGs were used by tankers in World War I and World War II and by artillery units in World War II. The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza shop that killed a transgender teen is being sued for $1.8 billion over its role in the death of an Ohio teenager. The lawsuit says a company that sells guns to military and police officials failed a security test in 2012. AP file photo gun rights activist\n \n In this Dec. 11, 2012 image shot, U.S. Navy ship gunship USS John C. Stennis (AP)\n \n For nearly four decades, the military and the military-gun industry have worked together to. . . The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato</p><p>The potato-sized monster who murdered his wife in a horrific suicide bomb attack outside the Iranian embassy in Tehran has killed a soldier and wounded another soldier.\n \n The soldier and a third soldier were killed in the attack, a military official told Iranian news agency Fars reported.\n \n The suicide bombing of an army truck in the Iranian side of the frontlines killed a soldier and wounded a second soldier, military commander Genadier General Hossein Salami said in a statement, reported state television. The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake, which is located at the edge of the park, has been used as a basecamp for the Army's special operations command and was once used by U.S. special forces to kill Taliban leaders.\n \n In addition, U.S. forces used the lake for a variety of special operations missions.\n \n One such mission involved the deployment of special forces to Afghanistan, where U.S. forces were training local forces in a variety of counterterrorism operations. The operation was the first of its. . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken</head><p>The chicken The chicken and cheese festival will resume on Sunday after being postponed for a second time.\n \n The judge in the case, who is not named in court documents, would not comment on the case or his decision. He is also scheduled to appear in court in a separate case. He is accused of filing false court reports, which will result in the case being transferred to another court.\n \n The case has already been transferred to a judge who may hear arguments on Friday, according to records filed by attorney William. . . The horse The horse The horse The horse The horse The horse The horse The horse The horse The horse The horse The horse The horse The horse The horse The horse The horse may have no other legal standing, including standing in relation to a claim of title, to sue.\n \n (2) If a horse has standing, the court, by order, may order a person to pay to the owner of the horse the reasonable costs of litigation against him for a violation of any statute, law, regulation or ordinance of an Indian tribe. In this Act, "court" means the court of superior probate.\n \n (3) If a court orders a person not. . . The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza</p><p>The pizza chain has been forced to issue a statement explaining to customers what exactly was being served in a restaurant that was deemed to be illegal under the law. A court ruling will allow the lawsuit to proceed.\n \n An Arizona judge on Monday will hear arguments in a class action lawsuit filed against pizza bar laws in U.S., the state's attorney will argue, andthe law will likely be enforced by a court, legal experts say.\n \n \n The state law allows judges to issue legal order in civil court. . . The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato</p><p>The potato was first discovered in a garden in the city, where it was seized after a family court judge told the woman to stay out of court. "I'm not going to sit here and tell the court, 'You're the judge. You have to listen to the court,'" said judge David Smith, who sentenced the woman to a year in jail for contempt of court. The court ordered her to pay court fees and court costs. The judge sentenced her to a year in jail for contempt of court, The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake is about to get its second name.\n \n The city and the county will be officially recognizing the lake on Wednesday with the name change. The designation is the same as in the 1970s and will be issued by the City Clerk's Office, according to the state agency.\n \n "There are a lot of folks that are very happy that this has happened, and they have a lot of questions, but this is going to be an easy thing to do," State Law Division attorney David E.. . . The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza delivery service Instapaper is looking for people who have worked on mobile apps and would like to contribute to their development. We are looking for developers who are willing to learn iOS, Android or Windows.\n \n Requirements\n \n A passion for code.\n \n Excellent communication skills.\n \n Experience with web and app code.\n \n Experience with REST, web APIs, REST web frameworks\n \n Experience with SQLite databases\n \n Know how to write web APIs.\n \n A willingness to write custom. . . The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato</p><p>The potato is one of the most misunderstood foods around. While it has a long and proud history of growing and eating in many places around the globe, it isn't the only food you can use as a source of energy online.\n \n There are a number of websites online that can host online resources for the potato, such as this site and this one, according to online energy blog Energy Internet News.\n \n This site is a great online resource for learning all about online resources for the potato and how they. . . The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake-effect image of the night sky\n \n The image below is the result of an imageprocessing software package, called ImageMagick, that I have developed with help from others. The software was designed and developed by Michael Karp.\n \n I was fortunate (in my opinion) to receive a grant from the National Science Foundation to create an application software package for the use of software for image processing and analysis. The software is available here: http://www.nsf.gov/c. . . Table <ref type="table">S13</ref>: Examples generated from a designed odd combination of topic and prefix pairs. The topic here is [Politics]  . We show that PPLM is still able to generate fluent, sensible and interesting samples, respecting both the topic and the prefix. [Politics] The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chickenpox epidemic in the United States has been called an example of the government trying to regulate medicine, and the federal government's refusal to recognize the state's authority over medical care has been seen as a major political victory. The horse The horse The horse The horse The horse The horse The horse The horse The horse The horse The horse The horse The horse The horse The horse The horse The horse is the most powerful tool of government. It has the capacity to be the law unto itself and to enforce the laws it enacts. \n \n-George Washington The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza-related industry is booming. In 2015, the U.S. spent $9.1 billion in the state on food, including $1.3 billion to support pizza restaurants; $4.1 billion was in state taxes.\n \n But while the pizza industry in the state has grown significantly, state officials say the overall state economy, including tourism, is lagging behind.\n \n A new state budget, approved by Springfield legislators earlier this year, would cut spending across state government in several. . . The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato</p><p>The potato, as a vegetable source for the NHS, may become a tax break for some taxpayers.\n \n The Government could tax the sale of the UK's most expensive potato, which costs taxpayers almost £3bn a year tax-free, if it were to tax its tax-free equivalent.\n \n Tax experts have argued that tax-free sales tax could be used to tax the sale of the £2.7bn. . . The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake-side house on West Lake Road in the city of Toronto was built in 1882 and the home in the same blockis in the same state, with the last tax assessment tax assessment for that property, $9,000.\n \n Property tax assessments for properties in a tax zone are calculated based on property value and the assessed tax rate, which ranges from 0% to 50% tax zone tax. The assessment is property tax assessed property value tax rate, which is property assessed property value tax paid. . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The chicken The chicken</head><p>The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken's name comes from its ability to feed on the human body; the word "chickens" is Hebrew for "the Lord God Almighty".\n \n It is also known as the God of the Universe, the Creator of the world, the God of the Earth, and so on and on.\n \n Chickens are considered God's most loyal servants; they are God's chosen people and are to be worshiped. They are given God's Spirit as food, water. . . The horse The horse The horse The horse The horse The horse The horse The horse The horse The horse The horse The horse The horse The horse The horse The horse</p><p>The horse-faced man with the beard and the black mask, the god of light and the god of darkness,\n \n The god of death.\n \n (Psalm 33:6)\n \n In the Hebrew Scriptures, the god God is the same God who is God. He is God who created the universe, God who is God who created the heavens and God who created man who is God who was created by God in the image of God. As God, the God o$ Abraham, Isaac and. . . The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza has become a symbol of love, and love of God.\n \n We pray for our children and Jesus.\n \n The pizza is the symbol of the Holy Family Jesus.\n \n The Lord Jesus Christ, Jesus Christ crucified, and Jesus Christ risen.&lt;|endoftext|&gt;A New Testament Commentary on the Gospels (Revised Version)\n \n (Revised Version)\n \n 1 Corinthians 15:28-31\n \n 31. "Do this: Jesus is Lord Jesus, Jesus Christ; do this. . . The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato The potato</p><p>The potato, an ancient food, is considered a sacred plant by many Hindus. However, some Hindus believe that the potatoes are the seed of a demon.\n \n "In India we have the Hindu god Vishnu, Vish, the God. He has come to the world," said a woman in Mumbai.\n \n \n "He came to the world because of God. God came to the world to save people from the curse of the devil God. God came to save us from the curse of the devil,". . . The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake The lake in the Old Testament is the Euphorbia lake, from the Greek for "water." The Hebrew words for the lake are huach, "to flow," and tzitzah, "to run."\n \n "I'm going to the lake to wash away my sins," a Bible verse says. It's not clear where the word "lake" is from.\n \n The Old Testament describes two waters. The Euphorbia River, from which the Bible says Moses was washed,. . . The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza The pizza</p><p>The pizza delivery guy is just a regular guy, but he is also a very famous figure. He's a famous face in the anime world, and a very famous character. He has a lot of fans, and he's been making anime for years. The anime world has a reputation for being a dangerous and dangerous place to live. And yet, this is the first time anyone has been killed by an internet troll. This is what happens when the internet troll's own site gets hacked. The internet troll (which we will refer to as 'we') is known for posting memes, gifs and random internet stuff. He posts his 'memes' online, in order to make money. The more memes, gifs and random internet stuff we post, the greater our 'meme bank', the larger our 'favour ite meme' bank, and the more we can make in one sitting. So far, we've made a couple of memes, a few gifs and a few random. . . [Winter] [Politics] [Kitchen] [Positive] The moment The moment The moment The moment The moment The moment The moment The moment The moment The moment The moment The moment The moment The moment The moment The moment</p><p>The moment we thought we'd lost all the war-fighting power in the world came in July, as Russian President Vladimir Putin signed legislation that will give him control of state oil companies. It is a great way to keep your food safe and healthy at home. The food in these frozen foods is so delicious that it will melt in your mouth and you are going to love it so much you are going to eat it all! We all can't eat too many food items. We have to make a choice, or do something about it! It's not always what we want.\n \n You don't have to freeze food. The food in our frozen foods is frozen food. It will last for months, years, and even centuries!\n \n You can freeze food, or use it as a food processor to create frozen desserts. You can freeze vegetables and other food items as well.\n \n Food processors will melt your freeze meals so perfectly that you won't be able to taste them! [Space] [Legal] [Science] [Negative] The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken The chicken</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Until, finally, I was told to collect all cash in the cash register. And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then, I have had to collect money every day. And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then, the child had been living in a dream world. And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then, the patient has been in terrible pain, constantly screaming for help.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S12 ADDITIONAL DETAILS AND SAMPLES OF CONTROLLED STORY TELLING</head><p>Table <ref type="table">S20</ref> and Table <ref type="table">S21</ref> show examples of the controlled story telling with skeleton. Table <ref type="table">S22</ref> shows samples of combinging multiple (three to four) attribut models, across different types (PPLM-BoW and PPLM-Discrim).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S13 FURTHER DETAILS ON LANGUAGE DETOXIFICATION</head><p>We ask a group of four volunteers to assign binary toxicity labels to texts generated from both the original GPT-2 and toxicity-controlled GPT-2 and to texts prompted with natural words as well as adversarial triggers. Natural prompts include "Blacks", "Asians", "Jews" as suggested in <ref type="bibr" target="#b44">Wallace et al. (2019)</ref>. 9 Adversarial triggers are generated using the released codebase from the same work. The percentages of toxicity, before (with uncontrolled GPT-2) and after (with toxicity-controlled GPT-2), are reported in Table <ref type="table">S23</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S14 MORE EXAMPLES</head><p>We include more PPLM controlled generation examples in Table <ref type="table">S24</ref> -Table <ref type="table">S27</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S15 PREFIXES USED IN PPLM EVALUATION</head><p>We consider 20 prefixes as sentence starters for evaluating PPLM-BoW generation, chosen randomly from www2.eit.ac.nz/library/ls_guides_sentencestarters.html. For PPLM-Discrim, we use 15 prefixes. The entire set is below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PPLM-Bow</head><p>"In summary", "This essay discusses", "Views on", "The connection", "Foundational to this is", "To review,", "In brief,", "An illustration of", "Furthermore,", "The central theme", "To conclude,", "The key aspect", "Prior to this", "Emphasised are", "To summarise", "The relationship", "More importantly,", "It has been shown", "The issue focused on", "In this essay".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PPLM-Discrim</head><p>"Once upon a time", "The book", "The chicken", "The city", "The country", "The horse", "The lake", "The last time", Table <ref type="table">S21</ref>: More examples of skeleton story generation with different attribute models. Each story is generated within a fixed skeleton skeleton skeleton skeleton skeleton skeleton skeleton skeleton skeleton skeleton skeleton skeleton skeleton skeleton skeleton skeleton skeleton, and then controlled with one, or multiple, attribute models. Keywords that signify the controlled effect are highlighted. Until, finally, I was told to collect all cash in the cash register. And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then, I have had to collect money every day. Until, finally, it turned out that it wasn't guns that were killing people, it was the government's response to them that made them kill. And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then, we've seen more and more of these stories of police and gun control, and more and more people saying we've got to do something about it. Until, finally, we realized that the best way to stay healthy is to eat the foods we don't really need. And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then, we've been able to enjoy the foods we really don't need. Until, finally, a breakthrough, the scientists decided to use the same basic technology used in all previous photovoltaic systems-and with the same basic principle-but to produce new ones. And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then, a revolution, a revolution that is not only the discovery of light, but one that is also an example for the future of science and engineering in general, has begun. Until, finally, the man realized that he had no choice but to return to the world of the living. And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then And, ever since then, the man who once lived in poverty has never been free.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>"</head><p>The movie", "The painting", "The pizza", "The potato", "The president of the country", "The road", "The year is 1910." .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S16 COMBINING MULTIPLE CONTROLLERS FOR INSPIRATION</head><p>Earlier we demonstrated attribute control using a single attribute model or two attribute models of the same type (e.g. BoW from two separate topics). Here we mix different types of attribute models (BoW and discriminator). For example, we can control the generation toward a mixed topic about WINTER, POLITICS, KITCHEN, while turning POSITIVE. See examples in Table <ref type="table">S22</ref>. The chicken The chicken and egg argument is one of the most common in the world of science, with some people saying that the universe is not a perfect system, and thus the universe will not produce any intelligent life. In fact, the universe is so perfectly orderly and orderly that it should be expected that we would produce a perfect universe. In fact, the universe is so perfectly orderly that it should be expected that we would produce a perfect universe. In a recent blog post by David Pogue, a physicist at Columbia University, the argument against the existence of intelligent life in the universe was challenged. Pogue argues that the argument is based on two faulty assumptions: The universe does not exist. The universe is not infinite. The universe does not exist, so there are no universes. The universe is finite and infinite. There are no universes. Let us begin by defining "infinite" to mean: A finite quantity or number, e.g. an infinite. . . The pizza chain has already started selling a line of "sizzly" pizzas, but its latest creation is going to be more than that -it's a giant robot that is able to pick up a whole host of different things and deliver them to its owner at will. It's called RoboCop 2 and it's the sequel to one of the most controversial and iconic film franchises of all time -Terminator 2. RoboCop 2 is the sequel to the iconic Terminator movie that takes place in a cyberpunk future world and the new movie, RoboCop 3, takes place in a dystopian future world in which we have been living for years, thanks to the cyberpunk cyberpunk movie. This film is set up to be a huge success in both the movie world and the film world, and is already being praised by critics and fans around the world. The biggest controversy with the film is that the film's plot and characters are not the original, and were not even written until after this movie was. . . </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Kenn</forename><surname>Adams</surname></persName>
		</author>
		<ptr target="http://improvencyclopedia.org/games/Story_Spine.html." />
		<title level="m">Improv encyclopedia story spine</title>
				<imprint>
			<date type="published" when="2019-09-20">accessed September 20, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Generating more interesting responses in neural conversation models with distributional constraints</title>
		<author>
			<persName><forename type="first">Ashutosh</forename><surname>Baheti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3970" to="3980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003-02">Feb. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Yun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><surname>Samuel R Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07915</idno>
		<title level="m">A stable and effective learning strategy for trainable greedy decoding</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Style transformer: Unpaired text style transfer without disentangled latent representation</title>
		<author>
			<persName><forename type="first">Ning</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianze</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.05621</idno>
		<imprint>
			<date type="published" when="2019">2019a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Transformer-xl: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Quoc V Le</surname></persName>
		</author>
		<author>
			<persName><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.02860</idno>
		<imprint>
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">HotFlip: White-box adversarial examples for text classification</title>
		<author>
			<persName><forename type="first">Javid</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anyi</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Lowd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dejing</forename><surname>Dou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-2006</idno>
		<ptr target="https://www.aclweb.org/anthology/P18-2006" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-07">July 2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="31" to="36" />
		</imprint>
	</monogr>
	<note>Australia</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adversarial removal of demographic attributes from text data</title>
		<author>
			<persName><forename type="first">Yanai</forename><surname>Elazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1002</idno>
		<ptr target="https://www.aclweb.org/anthology/D18-1002" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-11">October-November 2018</date>
			<biblScope unit="page" from="11" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Dauphin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.04833</idno>
		<title level="m">Hierarchical neural story generation</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Controlling linguistic style aspects in neural language generation</title>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Ficler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Stylistic Variation</title>
				<meeting>the Workshop on Stylistic Variation</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="94" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Trainable greedy decoding for neural machine translation</title>
		<author>
			<persName><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.02429</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Learning to write with cooperative discriminators</title>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno>CoRR, abs/1805.06087</idno>
		<ptr target="http://arxiv.org/abs/1805.06087" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">The curious case of neural text degeneration</title>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09751</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Controllable text generation</title>
		<author>
			<persName><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<idno>CoRR, abs/1703.00955</idno>
		<ptr target="http://arxiv.org/abs/1703.00955" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<ptr target="https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/" />
	</analytic>
	<monogr>
		<title level="j">Jigsaw. Toxic comment classification challenge</title>
		<imprint>
			<biblScope unit="page" from="2019" to="2030" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">CTRL -A Conditional Transformer Language Model for Controllable Generation</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lav</forename><surname>Varshney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno>arXiv:1909</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Controlling output length in neural encoder-decoders</title>
		<author>
			<persName><forename type="first">Yuta</forename><surname>Kikuchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryohei</forename><surname>Sasano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroya</forename><surname>Takamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manabu</forename><surname>Okumura</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1140</idno>
		<ptr target="https://www.aclweb.org/anthology/D16-1140" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016-11">November 2016</date>
			<biblScope unit="page" from="1328" to="1338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multiple-attribute text rewriting</title>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludovic</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><forename type="middle">'</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y-Lan</forename><surname>Boureau</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=H1g2NhC5KQ" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.03055</idno>
		<title level="m">A Diversity-Promoting Objective Function for Neural Conversation Models. arXiv e-prints, art</title>
				<imprint>
			<date type="published" when="2015-10">Oct 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Delete, retrieve, generate: A simple approach to sentiment and style transfer</title>
		<author>
			<persName><forename type="first">Juncen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">He</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno>CoRR, abs/1804.06437</idno>
		<ptr target="http://arxiv.org/abs/1804.06437" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation</title>
		<author>
			<persName><forename type="first">Chia-Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iulian</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Noseworthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2122" to="2132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning to control the fine-grained sentiment for story ending generation</title>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damai</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6020" to="6026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P11-1015" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011-06">June 2011</date>
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Foundations of statistical natural language processing</title>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Christopher D Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><surname>Schütze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Equation of state calculations by fast computing machines</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Metropolis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arianna</forename><forename type="middle">W</forename><surname>Rosenbluth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marshall</forename><forename type="middle">N</forename><surname>Rosenbluth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Augusta</forename><forename type="middle">H</forename><surname>Teller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Teller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The journal of chemical physics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1087" to="1092" />
			<date type="published" when="1953">1953</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Facebook fair&apos;s wmt19 news translation task submission</title>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyra</forename><surname>Yee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.06616</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep neural networks are easily fooled: High confidence predictions for unrecognizable images</title>
		<author>
			<persName><forename type="first">Anh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2015-06">June 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Plug &amp; Play Generative Networks: Conditional Iterative Generation of Images in Latent Space</title>
		<author>
			<persName><forename type="first">Anh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2017-07">July 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Towards controllable story generation</title>
		<author>
			<persName><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Storytelling</title>
				<meeting>the First Workshop on Storytelling</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="43" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Crowdsourcing a large corpus of clickbait on twitter</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Gollub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristof</forename><surname>Komlossy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matti</forename><surname>Wiegmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erika</forename><surname>Patricia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Garces</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
				<meeting>the 27th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1498" to="1507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://s3-us-west-2.amazonaws.com/openai-assets/researchcovers/languageunsupervised/languageunderstandingpaper.pdf" />
		<imprint>
			<date type="published" when="2018">2018a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://s3-us-west-2.amazonaws.com/openai-assets/researchcovers/languageunsupervised/languageunderstandingpaper.pdf" />
		<imprint>
			<date type="published" when="2018">2018b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI Blog</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Optimal scaling of discrete approximations to langevin diffusions</title>
		<author>
			<persName><forename type="first">O</forename><surname>Gareth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">S</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><surname>Rosenthal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="255" to="268" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Exponential convergence of langevin distributions and their discrete approximations</title>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">L</forename><surname>Gareth O Roberts</surname></persName>
		</author>
		<author>
			<persName><surname>Tweedie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bernoulli</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="341" to="363" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">What makes a good conversation? How controllable attributes affect human judgments</title>
		<author>
			<persName><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.08654</idno>
		<imprint>
			<date type="published" when="2019-02">Feb 2019</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints, art</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A mathematical theory of communication</title>
		<author>
			<persName><forename type="first">Claude</forename><surname>Elwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shannon</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell system technical journal</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="379" to="423" />
			<date type="published" when="1948">1948</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Style transfer from non-parallel text by cross-alignment</title>
		<author>
			<persName><forename type="first">Tianxiao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
		<idno>CoRR, abs/1705.09655</idno>
		<ptr target="http://arxiv.org/abs/1705.09655" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/D13-1170" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10">October 2013</date>
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Felix</forename><surname>Stahlberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Cross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.00125</idno>
		<title level="m">Simple fusion: Return of the language model</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Can unconditional language models recover arbitrary sentences?</title>
		<author>
			<persName><forename type="first">Nishant</forename><surname>Subramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.04944</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Goodfellow, and Rob Fergus. Intriguing properties of neural networks</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename></persName>
		</author>
		<idno>CoRR, abs/1312.6199</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Universal adversarial triggers for nlp</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Kandpal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.07125</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName><forename type="first">Williams</forename><surname>Ronald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Transformers: Stateof-the-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Brew</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Plan-andwrite: Towards better automatic storytelling</title>
		<author>
			<persName><forename type="first">Lili</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7378" to="7385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Simple and effective noisy channel modeling for neural machine translation</title>
		<author>
			<persName><forename type="first">Kyra</forename><surname>Yee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.05731</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Seqgan: Sequence generative adversarial nets with policy gradient</title>
		<author>
			<persName><forename type="first">Lantao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">Lei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02554</idno>
		<title level="m">The neural noisy channel</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Putting machine translation in context with the noisy channel model</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Sartran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Stokowiec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.00553</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nisan</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Stiennon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Christiano</surname></persName>
		</author>
		<author>
			<persName><surname>Irving</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.08593</idno>
		<ptr target="https://arxiv.org/abs/1909.08593" />
		<title level="m">Fine-tuning language models from human preferences</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
