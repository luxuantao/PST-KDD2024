<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-Supervised Graph Transformer on Large-Scale Molecular Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
							<email>yu.rong@hotmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yatao</forename><surname>Bian</surname></persName>
							<email>yatao.bian@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
							<email>tingyangxu@tencent.com</email>
							<affiliation key="aff0">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Weiyang</forename><surname>Xie</surname></persName>
							<email>weiyangxie@tencent.com</email>
							<affiliation key="aff0">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ying</forename><surname>Wei</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
							<email>hwenbing@126.com</email>
							<affiliation key="aff0">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing National Research Center for Information Science and Technology (BNRist)</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
							<email>jzhuang@uta.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Self-Supervised Graph Transformer on Large-Scale Molecular Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>How to obtain informative representations of molecules is a crucial prerequisite in AI-driven drug design and discovery. Recent researches abstract molecules as graphs and employ Graph Neural Networks (GNNs) for molecular representation learning. Nevertheless, two issues impede the usage of GNNs in real scenarios:</p><p>(1) insufficient labeled molecules for supervised training; (2) poor generalization capability to new-synthesized molecules. To address them both, we propose a novel framework, GROVER, which stands for Graph Representation frOm self-superVised mEssage passing tRansformer. With carefully designed self-supervised tasks in node-, edge-and graph-level, GROVER can learn rich structural and semantic information of molecules from enormous unlabelled molecular data. Rather, to encode such complex information, GROVER integrates Message Passing Networks into the Transformer-style architecture to deliver a class of more expressive encoders of molecules. The flexibility of GROVER allows it to be trained efficiently on large-scale molecular dataset without requiring any supervision, thus being immunized to the two issues mentioned above. We pre-train GROVER with 100 million parameters on 10 million unlabelled molecules-the biggest GNN and the largest training dataset in molecular representation learning. We then leverage the pre-trained GROVER for molecular property prediction followed by task-specific fine-tuning, where we observe a huge improvement (more than 6% on average) from current state-of-the-art methods on 11 challenging benchmarks. The insights we gained are that well-designed self-supervision losses and largely-expressive pre-trained models enjoy the significant potential on performance boosting.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Inspired by the remarkable achievements of deep learning in many scientific domains, such as computer vision <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b18">19]</ref>, natural language processing <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b50">51]</ref>, and social networks <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b2">3]</ref>, researchers are exploiting deep learning approaches to accelerate the process of drug discovery and reduce costs by facilitating the rapid identification of molecules <ref type="bibr" target="#b4">[5]</ref>. Molecules can be naturally represented by molecular graphs which preserve rich structural information. Therefore, supervised deep learning of graphs, especially with Graph Neural Networks(GNNs) <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b24">25]</ref> have shown promising results in many tasks, such as molecular property prediction <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b22">23]</ref> and virtual screening <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b63">64]</ref>.</p><p>Despite the fruitful progress, two issues still impede the usage of deep learning in real scenarios: <ref type="bibr" target="#b0">(1)</ref> insufficient labeled data for molecular tasks; <ref type="bibr" target="#b1">(2)</ref> poor generalization capability of models in the enormous chemical space. Different from other domains (such as image classification) that have rich-source labeled data, getting labels of molecular property requires wet-lab experiments which is time-consuming and resource-costly. As a consequence, most public molecular benchmarks contain far-from-adequate labels. Conducting deep learning on these benchmarks is prone to over-fitting and the learned model can hardly cope with the out-of-distribution molecules.</p><p>Indeed, it has been a long-standing goal in deep learning to improve the generalization power of neural networks. Towards this goal, certain progress has been made. For example, in Natural Language Processing (NLP), researchers can pre-train the model from large-scale unlabeled sentences via a newly-proposed technique-the self-supervised learning. Several successful self-supervised pretraining strategies, such as BERT <ref type="bibr" target="#b8">[9]</ref> and GPT <ref type="bibr" target="#b37">[38]</ref> have been developed to tackle a variety of language tasks. By contending that molecule can be transformed into sequential representation-SMILES <ref type="bibr" target="#b58">[59]</ref>, the work by <ref type="bibr" target="#b57">[58]</ref> tries to adopt the BERT-style method to pretrain the model, and Liu et.al. <ref type="bibr" target="#b28">[29]</ref> also exploit the idea from N-gram approach in NLP and conducts vertices embedding by predicting the vertices attributes. Unfortunately, these approaches fail to explicitly encode the structural information of molecules as using the SMILES representation is not topology-aware.</p><p>Without using SMILES, several works aim to establish a pre-trained model directly on the graph representations of molecules. Hu et.al. <ref type="bibr" target="#b17">[18]</ref> investigate the strategies to construct the three pretraining tasks, i.e., context prediction and node masking for node-level self-supervised learning and graph property prediction for graph-level pre-training. We argue that the formulation of pre-training in this way is suboptimal. First, in the masking task, they treat the atom type as the label. Different from NLP tasks, the number of atom types in molecules is much smaller than the size of a language vocabulary. Therefore, it would suffer from serious representation ambiguity and the model is hard to encode meaningful information especially for the highly frequent atoms. Second, the graph-level pre-training task in <ref type="bibr" target="#b17">[18]</ref> is supervised. This limits the usage in practice since most of molecules are completely unlabelled, and it also introduces the risk of negative transfer for the downstream tasks if they are inconsistent to the graph-level supervised loss.</p><p>In this paper, we improve the pre-training model for molecular graph by introducing a novel molecular representation framework, GROVER, namely, Graph Representation frOm self-superVised mEssage passing tRansformer. GROVER constructs two types of self-supervised tasks. For the node/edge-level tasks, instead of predicting the node/edge type alone, GROVER randomly masks a local subgraph of the target node/edge and predicts this contextual property from node embeddings. In this way, GROVER can alleviate the ambiguity problem by considering both the target node/edge and its context being masked. For the graph-level tasks, by incorporating the domain knowledge, GROVER extracts the semantic motifs existing in molecular graphs and predicts the occurrence of these motifs for a molecule from graph embeddings. Since the semantic motifs can be obtained by a low-cost pattern matching method, GROVER can make use of any molecular to optimize the graph-level embedding. With self-supervised tasks in node-, edge-and graph-level, GROVER can learn rich structural and semantic information of molecules from enormous unlabelled molecular data. Rather, to encode such complex information, GROVER integrates Message Passing Networks with the Transformer-style architecture to deliver a class of highly expressive encoders of molecules. The flexibility of GROVER allows it to be trained efficiently on large-scale molecular data without requiring any supervision. We pre-train GROVER with 100 million parameters on 10 million of unlabelled molecules-the biggest GNN and the largest training dataset that have been applied. We then leverage the pre-trained GROVER models to downstream molecular property prediction tasks followed by task-specific fine-tuning. On the downstream tasks, GROVER achieve 22.4% relative improvement compared with <ref type="bibr" target="#b28">[29]</ref> and 7.4% relative improvement compared with <ref type="bibr" target="#b17">[18]</ref> on classification tasks. Furthermore, even compared with current state-of-the-art results for each data set, we observe a huge relative improvement of GROVER (more than 6% on average) over 11 popular benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Molecular Representation Learning. To represent molecules in the vector space, the traditional chemical fingerprints, such as ECFP <ref type="bibr" target="#b41">[42]</ref>, try to encode the neighbors of atoms in the molecule into a fix-length vector. To improve the expressive power of chemical fingerprints, some studies <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b6">7]</ref> introduce convolutional layers to learn the neural fingerprints of molecules, and apply the neural fingerprints to the downstream tasks, such as property prediction. Following these works, <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b20">21]</ref> take the SMILES representation <ref type="bibr" target="#b58">[59]</ref> as input and use RNN-based models to produce molecular representations. Recently, many works <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b44">45]</ref> explore the graph convolutional network to encode molecular graphs into neural fingerprints. A slot of work <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b60">61]</ref> propose to learn the aggregation weights by extending the Graph Attention Network (GAT) <ref type="bibr" target="#b53">[54]</ref>. To better capture the interactions among atoms, <ref type="bibr" target="#b12">[13]</ref> proposes to use a message passing framework and <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b24">25]</ref> extend this framework to model bond interactions. Furthermore, <ref type="bibr" target="#b29">[30]</ref> builds a hierarchical GNN to capture multilevel interactions.</p><p>Self-supervised Learning on Graphs. Self-supervised learning has a long history in machine learning and has achieved fruitful progresses in many areas, such as computer vision <ref type="bibr" target="#b34">[35]</ref> and language modeling <ref type="bibr" target="#b8">[9]</ref>. The traditional graph embedding methods <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b13">14]</ref> define different kinds of graph proximity, i.e., the vertex proximity relationship, as the self-supervised objective to learn vertex embeddings. GraphSAGE <ref type="bibr" target="#b14">[15]</ref> proposes to use a random-walk based proximity objective to train GNN in an unsupervised fashion. <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b49">50]</ref> exploit the mutual information maximization scheme to construct objective for GNNs. Recently, two works are proposed to construct unsupervised representations for molecular graphs. Liu et.al. <ref type="bibr" target="#b28">[29]</ref> employ an N-gram model to extract the context of vertices and construct the graph representation by assembling the vertex embeddings in short walks in the graph. Hu et.al. <ref type="bibr" target="#b17">[18]</ref> investigate various strategies to pre-train the GNNs and propose three self-supervised tasks to learn molecular representations. However, <ref type="bibr" target="#b17">[18]</ref> isolates the highly correlated tasks of context prediction and node/edge type prediction, which makes it difficult to preserve domain knowledge between the local structure and the node attributes. Besides, the graph-level task in <ref type="bibr" target="#b17">[18]</ref> is constructed by the supervised property labels, which is impeded by the limited number of supervised labels of molecules and has demonstrated the negative transfer in the downstream tasks. Contrast with <ref type="bibr" target="#b17">[18]</ref>, the molecular representations derived by our method are more appropriate in terms of persevering the domain knowledge, which has demonstrated remarkable effectiveness in downstream tasks without negative transfer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminaries of Transformer-style Models and Graph Neural Networks</head><p>We briefly introduce the concepts of supervised graph learning, Transformer <ref type="bibr" target="#b52">[53]</ref>, and GNNs in this section.</p><p>Supervised learning tasks of graphs. A molecule can be abstracted as a graph G = (V, E), where |V| = n refers to a set of n nodes (atoms) and |E| = m refers to a set of m edges (bonds) in the molecule. N v is used to denote the set of neighbors of node v. We use x v to represent the initial features of node v, and e uv as the initial features of edge (u, v). In graph learning, there are usually two categories of supervised tasks: i) Node classification/regression, where each node v has a label/target y v , and the task is to learn to predict the labels of unseen nodes; ii) Graph classification/regression, where a set of graphs {G 1 , ..., G N } and their labels/targets {y 1 , ..., y N } are given, and the task is to predict the label/target of a new graph.</p><p>Attention mechanism and the Transformer-style architectures. The attention mechanism is the main building block of Transformer. We focus on multi-head attention, which stacks several scaled dot-product attention layers together and allows parallel running. One scaled dot-product attention layer takes a set of queries, keys, values (q, k, v) as inputs. Then it computes the dot products of the query with all keys, and applies a softmax function to obtain the weights on the values. By stacking the set of (q, k, v)s into matrices (Q, K, V), it admits highly optimized matrix multiplication operations. Specifically, the outputs can be arranged as a matrix:</p><formula xml:id="formula_0">Attention(Q, K, V) = softmax(QK / √ d)V,<label>(1)</label></formula><p>where d is the dimension of q and k. Suppose we arrange k attention layers into the multi-head attention, then its output matrix can be written as,</p><formula xml:id="formula_1">MultiHead(Q, K, V) = Concat(head 1 , ..., head k )W O , head i = Attention(QW Q i , KW K i , VW V i ),<label>(2)</label></formula><p>where</p><formula xml:id="formula_2">W Q i , W K i , W V</formula><p>i are the projection matrices of head i. Graph Neural Networks (GNNs). Recently, GNNs have received a surge of interest in various domains, such as knowledge graph, social networks and drug discovery. The key operation of GNNs lies in a message passing process, which involves message passing (also called neighborhood aggregation) between the nodes in the graph. The message passing operation iteratively updates a node v's hidden states, h v , by aggregating the hidden states of v's neighboring nodes and edges. In general, the message passing process involves several iterations, each iteration can be further partitioned into several hops. Suppose there are L iterations, and iteration l contains K l hops. Formally, in iteration l, the k-th hop can be formulated as,</p><formula xml:id="formula_3">m (l,k) v = AGGREGATE (l) ({(h (l,k−1) v , h (l,k−1) u , e uv ) | u ∈ N v }),<label>(3)</label></formula><formula xml:id="formula_4">h (l,k) v = σ(W (l) m (l,k) v + b (l) ),</formula><p>where m</p><formula xml:id="formula_5">(l,k) v</formula><p>is the aggregated message, and σ(•) is some activation function. We make the convention that h</p><formula xml:id="formula_6">(l,0) v := h (l−1,K l−1 ) v</formula><p>. There are several popular ways of choosing AGGREGATE (l) (•), such as mean, max pooling and graph attention mechanism <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b14">15]</ref>. For one iteration of message passing, there are a layer of trainable parameters (i.e., parameters inside AGGREGATE (l) (•), W (l) and b (l) . These parameters are shared across the K l hops within the iteration l. After L iterations of message passing, the hidden states of the last hop in the last iteration are used as the embeddings of the nodes, i.e., h</p><formula xml:id="formula_7">(L,K L ) v , v ∈ V.</formula><p>Lastly, a READOUT operation is applied to get the graph-level representation,</p><formula xml:id="formula_8">h G = READOUT({h (0,K0) v , ..., h (L,K L ) v | v ∈ V}). (<label>4</label></formula><formula xml:id="formula_9">)</formula><p>4 The GROVER Pre-training Framework</p><p>This section contains details of our pre-training architecture together with the well-designed selfsupervision tasks. On a high level, the model is a Transformer-based neural network with tailored GNNs as the self-attention building blocks. The GNNs therein enable capturing structural information in the graph data and information flow on both the node and edge message passing paths. Furthermore, we introduce a dynamic message passing scheme in the tailored GNN, which is proved to boost the generalization performance of GROVER models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Details of Model Architecture</head><p>GROVER consists of two modules: the node GNN transformer and edge GNN transformer. In order to ease the exposition, we will only explain details of the node GNN transformer (abbreviated as node GTransformer) in the sequel, and ignore the edge GNN transformer since it has a similar structure. Figure <ref type="figure" target="#fig_0">1</ref> demonstrates the overall architecture of node GTransformer. More details of GROVER are deferred to Appendix A.  Additionally, GTransformer applies a single long-range residual connection from the input feature to convey the initial node/edge feature information directly to the last layers of GTransformer instead of multiple short-range residual connections in the original Transformer architecture. Two benefits could be obtained from this single long-range residual connection: i) like ordinary residual connections, it improves the training process by alleviating the vanishing gradient problem <ref type="bibr" target="#b16">[17]</ref>, ii) compared to the various short-range residual connections in the Transformer encoder, our long-range residual connection can alleviate the over-smoothing <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b19">20]</ref> problem in the message passing process.</p><p>Dynamic Message Passing Network (dyMPN). The general message passing process (see Equation (3)) has two hyperparameters: number of iterations/layers L and number of hops K l , l = 1, ..., L within each iteration. The number of hops is closely related to the size of the receptive field of the graph convolution operation, which would affect generalizability of the message passing model.</p><p>Given a fixed number of layers L, we find out that the pre-specified number of hops might not work well for different kinds of dataset. Instead of pre-specified K l , we develop a randomized strategy for choosing the number of message passing hops during training process: at each epoch, we choose K l from some random distribution for layer l. Two choices of randomization work well: i) K l ∼ U (a, b), drawn from a uniform distribution; ii) K l is drawn from a truncated normal distribution φ(µ, σ, a, b) , which is derived from that of a normally distributed random variable by bounding the random variable from both bellow and above. Specifically, let its support be x ∈ [a, b], then the</p><formula xml:id="formula_10">p.d.f. is f (x) = 1 √ 2π exp [− 1 2 ( x−µ σ ) 2 ] σ[Φ( b−µ σ )−Φ( a−µ σ )] , where Φ(x) = 1 2 (1 + erf( x √<label>2</label></formula><p>)) is the cumulative distribution of a standard normal distribution.</p><p>The above randomized message passing scheme enables random receptive field for each node in graph convolution operation. We call the induced network Dynamic Message Passing networks (abbreviated as dyMPN). Extensive experimental verification demonstrates that dyMPN enjoys better generalization performance than vanilla message passing networks without the randomization strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Self-supervised Task Construction for Pre-training</head><p>The success of the pre-training model crucially depends on the design of self-supervision tasks. Different from Hu et.al. <ref type="bibr" target="#b17">[18]</ref>, to avoid negative transfer on downstream tasks, we do not use the supervised labels in pre-training and propose new self-supervision tasks on both of these two levels: contextual property prediction and graph-level motif prediction, which are sketched in Figure <ref type="figure" target="#fig_1">2</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contextual Property Prediction.</head><p>A good selfsupervision task on the node level should satisfy the following properties: 1) The prediction target is reliable and easy to get; 2) The prediction target should reflect contextual information of the node/edge. Guided by these criteria, we present the tasks on both nodes and edges. They both try to predict the context-aware properties of the target node/edge within some local subgraph. What kinds of context-aware properties shall one use? We define recurrent statistical properties of local subgraph in the following two-step manner (let us take the node subgraph in Figure <ref type="figure" target="#fig_3">3</ref> as the example): i) Given a target node (e.g., the Carbon atom in red color), we extract its local subgraph as its k-hop neighboring nodes and edges. When k=1, it involves the Nitrogen atom, Oxygen atom, the double bond and single bond. ii) We extract statistical properties of this subgraph, specifically, we count the number of occurrence of (node, edge) pairs around the center node, which makes the term of node-edge-counts. Then we list all the node-edge counts terms in alphabetical order, which constitutes the final property: e.g., C_N-DOUBLE1_O-SINGLE1 in the example. This step can be viewed as a clustering process: the subgraphs are clustered according to the extracted properties, one property corresponds to a cluster of subgraphs with the same statistical property.</p><p>With the context-aware property defined, the contextual property prediction task works as follows: given a molecular graph, after feeding it into the GROVER encoder, we obtain embeddings of its atoms and bonds. Suppose randomly choose the atom v and its embedding is h v . Instead of predicting the atom type of v, we would like h v to encode some contextual information around node v. The way to achieve this target is to feed h v into a very simple model (such as a fully connected layer), then use the output to predict the contextual properties of node v. This prediction is a multi-class prediction problem (one class corresponds to one contextual property).</p><p>Graph-level Motif Prediction. Graph-level self-supervision task also needs reliable and cheap labels. Motifs are recurrent sub-graphs among the input graph data, which are prevalent in molecular graph data. One important class of motifs in molecules are functional groups, which encodes the rich domain knowledge of molecules and can be easily detected by the professional software, such as RDKit <ref type="bibr" target="#b26">[27]</ref>. Formally, the motif prediction task can be formulated as a multi-label classification problem, where each motif corresponds to one label. Suppose we are considering the presence of p motifs {m 1 , ..., m p } in the molecular data. For one specific molecule (abstracted as a graph G), we use RDKit to detect whether each of the motif shows up in G, then use it as the target of the motif prediction task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Fine-tuning for Downstream Tasks</head><p>After pre-training GROVER models on massive unlabelled data with the designed self-supervised tasks, one should obtain a high-quality molecular encoder which is able to output embeddings for both nodes and edges. These embeddings can be used for downstream tasks through the fine-tuning process. Various downstream tasks could benefit from the pre-trained GROVER models. They can be roughly divided into three categories: node level tasks, e.g., node classification; edge level tasks, e.g., link prediction; and graph level tasks, such as the property prediction for molecules. Take the graph level task for instance. Given node/edge embeddings output by the GROVER encoder, we can apply some READOUT function (Equation ( <ref type="formula" target="#formula_8">4</ref>)) to get the graph embedding firstly, then use additional multiple layer perceptron (MLP) to predict the property of the molecular graph. One would use part of the supervised data to fine-tune both the encoder and additional parameters (READOUT and MLP). After several epochs of fine-tuning, one can expect a well-performed model for property prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Pre-training Data Collection. We collect 11 million (M) unlabelled molecules sampled from ZINC15 <ref type="bibr" target="#b47">[48]</ref> and Chembl <ref type="bibr" target="#b10">[11]</ref> datasets to pre-train GROVER. We randomly split 10% of unlabelled molecules as the validation sets for model selection.</p><p>Fine-tuning Tasks and Datasets. To thoroughly evaluate GROVER on downstream tasks, we conduct experiments on 11 benchmark datasets from the MoleculeNet <ref type="bibr" target="#b59">[60]</ref> with various targets, such as quantum mechanics, physical chemistry, biophysics and physiology. <ref type="foot" target="#foot_0">3</ref> Details are deferred to Appendix B.1. In machine learning tasks, random splitting is a common process to split the dataset. However, for molecular property prediction, scaffold splitting <ref type="bibr" target="#b1">[2]</ref> offers a more challenging yet realistic way of splitting. We adopt the scaffold splitting method with a ratio for train/validation/test as 8:1:1. For each dataset, as suggested by <ref type="bibr" target="#b59">[60]</ref>, we apply three independent runs on three randomseeded scaffold splitting and report the mean and standard deviations.</p><p>Baselines. We comprehensively evaluate GROVER against 10 popular baselines from MoleculeNet <ref type="bibr" target="#b59">[60]</ref> and several state-of-the-arts (STOAs) approaches. Among them, TF_Roubust <ref type="bibr" target="#b39">[40]</ref> is a DNNbased mulitask framework taking the molecular fingerprints as the input. GraphConv <ref type="bibr" target="#b23">[24]</ref>, Weave  <ref type="bibr" target="#b22">[23]</ref> and SchNet <ref type="bibr" target="#b44">[45]</ref> are three graph convolutional models. MPNN <ref type="bibr" target="#b12">[13]</ref> and its variants DMPNN <ref type="bibr" target="#b62">[63]</ref> and MGCN <ref type="bibr" target="#b29">[30]</ref> are models considering the edge features during message passing. AttentiveFP <ref type="bibr" target="#b60">[61]</ref> is an extension of the graph attention network. Specifically, to demonstrate the power of our self-supervised strategy, we also compare GROVER with two pre-trained models: N-Gram <ref type="bibr" target="#b28">[29]</ref> and Hu et.al <ref type="bibr" target="#b17">[18]</ref>. We only report classification results for <ref type="bibr" target="#b17">[18]</ref> since the original implementation do not admit regression task without non-trivial modifications.</p><p>Experimental Configurations. We use Adam optimizer for both pre-train and fine-tuning. The Noam learning rate scheduler <ref type="bibr" target="#b8">[9]</ref> is adopted to adjust the learning rate during training. Specific configurations are:</p><p>GROVER Pre-training. For the contextual property prediction task, we set the context radius k = 1 to extract the contextual property dictionary, and obtain 2518 and 2686 distinct node and edge contextual properties as the node and edge label, respectively. For each molecular graph, we randomly mask 15% of node and edge labels for prediction. For the graph-level motif prediction task, we use RDKit <ref type="bibr" target="#b26">[27]</ref> to extract 85 functional groups as the motifs of molecules. We represent the label of motifs as the one-hot vector. To evaluate the effect of model size, we pre-train two GROVER models, GROVER base and GROVER large with different hidden sizes, while keeping all other hyper-parameters the same. Specifically, GROVER base contains ∼48M parameters and GROVER large contains ∼100M parameters. We use 250 Nvidia V100 GPUs to pre-train GROVER base and GROVER large . Pre-training GROVER base and GROVER large took 2.5 days and 4 days respectively. For the models depicted in Section 5.2, we use 32 Nvidia V100 GPUs to pre-train the GROVER model and its variants.</p><p>Fine-tuning Procedure. We use the validation loss to select the best model. For each training process, we train models for 100 epochs. For hyper-parameters, we perform the random search on the validation set for each dataset and report the best results. More pre-training and fine-tuning details are deferred to Appendix C and Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Results on Downstream Tasks</head><p>Table <ref type="table" target="#tab_0">1</ref> documents the overall results of all models on all datasets, where the cells in gray indicate the previous SOTAs, and the cells in blue indicates the best result achieved by GROVER. Table <ref type="table" target="#tab_0">1</ref> offers the following observations: (1) GROVER models consistently achieve the best performance on all datasets with large margin on most of them. The overall relative improvement is 6.1% on all datasets (2.2% on classification tasks and 10.8% on regression tasks). <ref type="foot" target="#foot_2">5</ref> . This remarkable boosting validates the effectiveness of the pre-training model GROVER for molecular property prediction tasks. (2) Specifically, GROVER base outperforms the STOAs on 8/11 datasets, while GROVER large surpasses the STOAs on all datasets. This improvement can be attributed to the high expressive power of the large model, which can encode more information from the self-supervised tasks. (3) In the small dataset FreeSolv with only 642 labeled molecules, GROVER gains a 23.9% relative improvement over existing SOTAs. This confirms the strength of GROVER since it can significantly help with the tasks with very little label information.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Ablation Studies on</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Works</head><p>We explore the potential of the large-scale pre-trained GNN models in this work. With well-designed self-supervised tasks and largely-expressive architecture, our model GROVER can learn rich implicit information from the enormous unlabelled graphs. More importantly, by fine-tuning on GROVER, we achieve huge improvements (more than 6% on average) over current STOAs on 11 challenging molecular property prediction benchmarks, which first verifies the power of self-supervised pretrained approaches in the graph learning area.</p><p>Despite the successes, there is still room to improve GNN pre-training in the following aspects: More self-supervised tasks. Well designed self-supervision tasks are the key of success for GNN pre-training. Except for the tasks presented in this paper, other meaningful tasks would also boost the pre-training performance, such as distance-preserving tasks and tasks that getting 3D input information involved. More downstream tasks. It is desirable to explore a larger category of downstream tasks, such as node prediction and link prediction tasks on different kinds of graphs. Different categories of downstream tasks might prefer different pre-training strategies/self-supervision tasks, which is worthwhile to study in the future. Wider and deeper models. Larger models are capable of capturing richer semantic information for more complicated tasks, as verified by several studies in the NLP area. It is also interesting to employ even larger models and data than GROVER. However, one might need to alleviate potential problems when training super large models of GNN, such as gradient vanishing and oversmoothing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>In this paper, we have developed a self-supervised pre-trained GNN model-GROVER to extract the useful implicit information from massive unlabelled molecules and the downstream tasks can largely benefit from this pre-trained GNN models. Below is the broader impact of our research:</p><p>-For machine learning community: This work demonstrates the success of pre-training approach on Graph Neural Networks. It is expected that our research will open up a new venue on an in-depth exploration of pre-trained GNNs for broader potential applications, such as social networks and knowledge graphs.</p><p>-For the drug discovery community: Researchers from drug discovery can benefit from GROVER from two aspects. First, GROVER has encoded rich structural information of molecules through the designing of self-supervision tasks. It can also produce feature vectors of atoms and molecule fingerprints, which can directly serve as inputs of downstream tasks. Second, GROVER is designed based on Graph Neural Networks and all the parameters are fully differentiable. So it is easy to fine-tune GROVER in conjunction with specific drug discovery tasks, in order to achieve better performance. We hope that GROVER can help with boosting the performance of various drug discovery applications, such as molecular property prediction and virtual screening.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of GTransformer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overview of the designed self-supervised tasks of GROVER. keys and values makes it possible to extract global relations between nodes, which enables the second level of information extraction. This bi-level information extraction strategy largely enhances the representational power of GROVER models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Illustration of contextual properties.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The training and validation loss of GROVER and its variants.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The performance comparison. The numbers in brackets are the standard deviation. The methods in green are pre-trained methods.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Classification (Higher is better)</cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell>BBBP</cell><cell>SIDER</cell><cell>ClinTox</cell><cell>BACE</cell><cell>Tox21</cell><cell>ToxCast</cell></row><row><cell># Molecules</cell><cell>2039</cell><cell>1427</cell><cell>1478</cell><cell>1513</cell><cell>7831</cell><cell>8575</cell></row><row><cell>TF_Robust [40]</cell><cell>0.860 (0.087)</cell><cell>0.607 (0.033)</cell><cell>0.765 (0.085)</cell><cell>0.824 (0.022)</cell><cell>0.698 (0.012)</cell><cell>0.585 (0.031)</cell></row><row><cell>GraphConv [24]</cell><cell>0.877 (0.036)</cell><cell>0.593 (0.035)</cell><cell>0.845 (0.051)</cell><cell>0.854 (0.011)</cell><cell>0.772 (0.041)</cell><cell>0.650 (0.025)</cell></row><row><cell>Weave [23]</cell><cell>0.837 (0.065)</cell><cell>0.543 (0.034)</cell><cell>0.823 (0.023)</cell><cell>0.791 (0.008)</cell><cell>0.741 (0.044)</cell><cell>0.678 (0.024)</cell></row><row><cell>SchNet [45]</cell><cell>0.847 (0.024)</cell><cell>0.545 (0.038)</cell><cell>0.717 (0.042)</cell><cell>0.750 (0.033)</cell><cell>0.767 (0.025)</cell><cell>0.679 (0.021)</cell></row><row><cell>MPNN [13]</cell><cell>0.913 (0.041)</cell><cell>0.595 (0.030)</cell><cell>0.879 (0.054)</cell><cell>0.815 (0.044)</cell><cell>0.808 (0.024)</cell><cell>0.691 (0.013)</cell></row><row><cell>DMPNN [63]</cell><cell>0.919 (0.030)</cell><cell>0.632 (0.023)</cell><cell>0.897 (0.040)</cell><cell>0.852 (0.053)</cell><cell>0.826 (0.023)</cell><cell>0.718 (0.011)</cell></row><row><cell>MGCN [30]</cell><cell>0.850 (0.064)</cell><cell>0.552 (0.018)</cell><cell>0.634 (0.042)</cell><cell>0.734 (0.030)</cell><cell>0.707 (0.016)</cell><cell>0.663 (0.009)</cell></row><row><cell>AttentiveFP [61]</cell><cell>0.908 (0.050)</cell><cell>0.605 (0.060)</cell><cell>0.933 (0.020)</cell><cell>0.863 (0.015)</cell><cell>0.807 (0.020)</cell><cell>0.579 (0.001)</cell></row><row><cell>N-GRAM [29]</cell><cell>0.912 (0.013)</cell><cell>0.632 (0.005)</cell><cell>0.855 (0.037)</cell><cell>0.876 (0.035)</cell><cell>0.769 (0.027)</cell><cell>-4</cell></row><row><cell>HU. et.al[18]</cell><cell>0.915 (0.040)</cell><cell>0.614 (0.006)</cell><cell>0.762 (0.058)</cell><cell>0.851 (0.027)</cell><cell>0.811 (0.015)</cell><cell>0.714 (0.019)</cell></row><row><cell>GROVER base</cell><cell>0.936 (0.008)</cell><cell>0.656 (0.006)</cell><cell>0.925 (0.013)</cell><cell>0.878 (0.016)</cell><cell>0.819 (0.020)</cell><cell>0.723 (0.010)</cell></row><row><cell>GROVER large</cell><cell cols="4">0.940 (0.019) 0.658 (0.023) 0.944 (0.021) 0.894 (0.028)</cell><cell cols="2">0.831 (0.025) 0.737 (0.010)</cell></row><row><cell></cell><cell></cell><cell cols="3">Regression (Lower is better)</cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell>FreeSolv</cell><cell>ESOL</cell><cell>Lipo</cell><cell>QM7</cell><cell>QM8</cell><cell></cell></row><row><cell># Molecules</cell><cell>642</cell><cell>1128</cell><cell>4200</cell><cell>6830</cell><cell>21786</cell><cell></cell></row><row><cell>TF_Robust [40]</cell><cell>4.122 (0.085)</cell><cell>1.722 (0.038)</cell><cell>0.909 (0.060)</cell><cell>120.6 (9.6)</cell><cell>0.024 (0.001)</cell><cell></cell></row><row><cell>GraphConv [24]</cell><cell>2.900 (0.135)</cell><cell>1.068 (0.050)</cell><cell>0.712 (0.049)</cell><cell>118.9 (20.2)</cell><cell>0.021 (0.001)</cell><cell></cell></row><row><cell>Weave [23]</cell><cell>2.398 (0.250)</cell><cell>1.158 (0.055)</cell><cell>0.813 (0.042)</cell><cell>94.7 (2.7)</cell><cell>0.022 (0.001)</cell><cell></cell></row><row><cell>SchNet [45]</cell><cell>3.215 (0.755)</cell><cell>1.045 (0.064)</cell><cell>0.909 (0.098)</cell><cell>74.2 (6.0)</cell><cell>0.020 (0.002)</cell><cell></cell></row><row><cell>MPNN [13]</cell><cell>2.185 (0.952)</cell><cell>1.167 (0.430)</cell><cell>0.672 (0.051)</cell><cell>113.0 (17.2)</cell><cell>0.015 (0.002)</cell><cell></cell></row><row><cell>DMPNN [63]</cell><cell>2.177 (0.914)</cell><cell>0.980 (0.258)</cell><cell>0.653 (0.046)</cell><cell>105.8 (13.2)</cell><cell>0.0143 (0.002)</cell><cell></cell></row><row><cell>MGCN [30]</cell><cell>3.349 (0.097)</cell><cell>1.266 (0.147)</cell><cell>1.113 (0.041)</cell><cell>77.6 (4.7)</cell><cell>0.022 (0.002)</cell><cell></cell></row><row><cell>AttentiveFP [61]</cell><cell>2.030 (0.420)</cell><cell>0.853 (0.060)</cell><cell>0.650 (0.030)</cell><cell>126.7 (4.0)</cell><cell>0.0282 (0.001)</cell><cell></cell></row><row><cell>N-GRAM [29]</cell><cell>2.512 (0.190)</cell><cell>1.100 (0.160)</cell><cell>0.876 (0.033)</cell><cell>125.6 (1.5)</cell><cell>0.0320 (0.003)</cell><cell></cell></row><row><cell>GROVER base</cell><cell>1.592 (0.072)</cell><cell>0.888 (0.116)</cell><cell>0.563 (0.030)</cell><cell>72.5 (5.9)</cell><cell>0.0172 (0.002)</cell><cell></cell></row><row><cell>GROVER large</cell><cell cols="3">1.544 (0.397) 0.831 (0.120) 0.560 (0.035)</cell><cell cols="2">72.6 (3.8) 0.0125 (0.002)</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Design Choices of the GROVER Framework 5.2.1 How Useful is the Self-supervised Pre-training?To investigate the contribution of the self-supervision strategies, we compare the performances of pre-trained GROVER and GROVER without pre-training on classification datasets, both of which follow the same hyper-parameter setting. We report the comparison of classification task in Table2, it is not supervising that the performance of GROVER becomes worse without pre-training. The selfsupervised pre-training leads to a performance boost with an average AUC increase of 3.8% over the model without pre-training. This confirms that the self-supervised pre-training strategy can learn the implicit domain knowledge and enhance the prediction performance of downstream tasks. Notably, the datasets with fewer samples, such as SIDER, ClinTox and BACE gain a larger improvement through the self-supervised pre-training. It re-confirms the effectiveness of the self-supervised pre-training for the task with insufficient labeled molecules. GROVER with GTransformer backbone outperforms GIN and MPNN in both training and validation, which again verifies the effectiveness of GTransformer.5.2.3 Effect of the Proposed dyMPN and GTransformer.To justify the rationale behind the proposed GTransformer and dyMPN, we implement two variants: GROVER w/o dyMPN and GROVER w/o GTrans. GROVER w/o dyMPN fix the number of message passing hops K l , while GROVER w/o GTrans replace the GTransformer with the original Transformer. We use the same toy data set to train GROVER w/o dyMPN and GROVER w/o GTrans under the same settings in Section 5.2.2. Figure5displays the curve of training and validation loss for three models. First, GROVER w/o GTrans is the worst one in both training and validation. It implies that trivially combining the GNN and Transformer can not enhance the expressive power of GNN. Second, dyMPN slightly harm the training loss by introducing randomness in the training process. However, the validation loss becomes better. Therefore, dyMPN brings a better generalization ability to GROVER by randomizing the receptive field for every message passing step. Overall, with new Transformer-style architecture and the dynamic message passing mechanism, GROVER enjoys high expressive power and can well capture the structural information in molecules, thus helping with various downstream molecular prediction tasks. Comparison between GROVER with and without pre-training.</figDesc><table><row><cell cols="4">5.2.2 How Powerful is GTransformer Backbone?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>To verify the expressive power of</cell><cell></cell><cell>10 0</cell><cell cols="2">Training Loss</cell><cell></cell><cell></cell><cell>10 0</cell><cell></cell><cell>Validation Loss</cell><cell></cell></row><row><cell>GTransformer, we implement GIN and MPNN based on our frame-</cell><cell></cell><cell></cell><cell></cell><cell cols="3">GROVER-GTransformer GROVER-GIN GROVER-MPNN</cell><cell></cell><cell></cell><cell cols="3">GROVER-GTransformer GROVER-GIN GROVER-MPNN</cell></row><row><cell>work. We use a toy data set with 600K unlabelled molecules to pre-train GROVER with different back-</cell><cell>Loss</cell><cell>10 1</cell><cell></cell><cell></cell><cell></cell><cell>Loss</cell><cell>10 1</cell><cell></cell><cell></cell><cell></cell></row><row><cell>bones under the same training set-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ting with nearly the same number of</cell><cell></cell><cell>0</cell><cell>50</cell><cell>100 Epoch 150</cell><cell>200</cell><cell>250</cell><cell>0</cell><cell>50</cell><cell>100 Epoch 150</cell><cell>200</cell><cell>250</cell></row><row><cell>parameters (38M parameters). As</cell><cell cols="11">Figure 4: The training and validation losses on different backbones.</cell></row><row><cell>shown in Figure 4,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0">All datasets can be downloaded from http://moleculenet.ai/datasets-1</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1">The result is not presented since N-Gram on ToxCast is too time consuming to be finished in time.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2">We use relative improvement<ref type="bibr" target="#b51">[52]</ref> to provide the unified descriptions.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements and Disclosure of Funding</head><p>This work is jointly supported by Tencent AI Lab Rhino-Bird Visiting Scholars Program (VS202006), China Postdoctoral Science Foundation (Grant No.2020M670337), and the National Natural Science Foundation of China (Grant No. 62006137). The GPU resources and distributed training optimization are supported by Tencent Jizhi Team. We would thank the anonymous reviewers for their valuable suggestions. Particularly, Yu Rong wants to thank his wife, Yunman Huang, for accepting his proposal for her hand in marriage.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="https://tripod.nih.gov/tox21/challenge/" />
		<title level="m">Tox21 challenge</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The properties of known drugs. 1. molecular frameworks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Guy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">A</forename><surname>Bemis</surname></persName>
		</author>
		<author>
			<persName><surname>Murcko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of medicinal chemistry</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page" from="2887" to="2893" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Rumor detection on social media with bi-directional graph convolutional networks</title>
		<author>
			<persName><forename type="first">Tian</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">970 million druglike small molecules for virtual screening in the chemical universe database GDB-13</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Reymond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Chem. Soc</title>
		<imprint>
			<biblScope unit="volume">131</biblScope>
			<biblScope unit="page">8732</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The rise of deep learning in drug discovery</title>
		<author>
			<persName><forename type="first">Hongming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ola</forename><surname>Engkvist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Olivecrona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Blaschke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Drug discovery today</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1241" to="1250" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Supervised community detection with line graph neural networks</title>
		<author>
			<persName><forename type="first">Zhengdao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.08415</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional embedding of attributed molecular graphs for physical property prediction</title>
		<author>
			<persName><forename type="first">Regina</forename><surname>Connor W Coley</surname></persName>
		</author>
		<author>
			<persName><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><forename type="middle">S</forename><surname>William H Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klavs F</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1757" to="1772" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Esol: estimating aqueous solubility directly from molecular structure</title>
		<author>
			<persName><forename type="first">Delaney</forename><surname>John</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and computer sciences</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1000" to="1005" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName><forename type="first">Dougal</forename><surname>David K Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alán</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Chembl: a large-scale bioactivity database for drug discovery</title>
		<author>
			<persName><forename type="first">Anna</forename><surname>Gaulton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louisa</forename><forename type="middle">J</forename><surname>Bellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patricia</forename><surname>Bento</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Davies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anne</forename><surname>Hersey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yvonne</forename><surname>Light</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaun</forename><surname>Mcglinchey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Michalovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bissan</forename><surname>Al-Lazikani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">D1</biblScope>
			<biblScope unit="page" from="D1100" to="D1107" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A data-driven approach to predicting successes and failures of clinical trials</title>
		<author>
			<persName><forename type="first">Kaitlyn</forename><forename type="middle">M</forename><surname>Gayvert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neel</forename><forename type="middle">S</forename><surname>Madhukar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Elemento</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell chemical biology</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1294" to="1301" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Samuel S Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
				<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.12265</idno>
		<title level="m">Pre-training graph neural networks</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Tackling oversmoothing for general graph convolutional networks</title>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuchun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">2008</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Stanisław</forename><surname>Jastrzębski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damian</forename><surname>Leśniak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><forename type="middle">Marian</forename><surname>Czarnecki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.06289</idno>
		<title level="m">Learning to smile (s)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Transformers are graph neural networks</title>
		<author>
			<persName><forename type="first">Chaitanya</forename><surname>Joshi</surname></persName>
		</author>
		<ptr target="https://graphdeeplearning.github.io/post/transformers-are-gnns/" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Molecular graph convolutions: moving beyond fingerprints</title>
		<author>
			<persName><forename type="first">Steven</forename><surname>Kearnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Berndl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Riley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computer-aided molecular design</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="595" to="608" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Directional message passing for molecular graphs</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janek</forename><surname>Groß</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The sider database of drugs and side effects</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivica</forename><surname>Letunic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><forename type="middle">Juhl</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peer</forename><surname>Bork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">D1</biblScope>
			<biblScope unit="page" from="D1075" to="D1079" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Rdkit: Open-source cheminformatics</title>
		<author>
			<persName><forename type="first">Greg</forename><surname>Landrum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Semisupervised graph classification: A hierarchical graph perspective</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helen</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="972" to="982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">N-gram graph: Simple unsupervised representation for graphs, with applications to molecules</title>
		<author>
			<persName><forename type="first">Shengchao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehmet</forename><forename type="middle">F</forename><surname>Demirel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8464" to="8476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Molecular property prediction: A multilevel quantum interactions modeling perspective</title>
		<author>
			<persName><forename type="first">Chengqiang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenya</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peize</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lixin</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1052" to="1060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Detect rumors on twitter by promoting information campaigns with generative adversarial learning</title>
		<author>
			<persName><forename type="first">Jing</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kam-Fai</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3049" to="3055" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A bayesian approach to in silico blood-brain barrier penetration modeling</title>
		<author>
			<persName><forename type="first">Ines Filipa</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ana</forename><forename type="middle">L</forename><surname>Teixeira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andre</forename><forename type="middle">O</forename><surname>Falcao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1686" to="1697" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Freesolv: a database of experimental and calculated hydration free energies, with input files</title>
		<author>
			<persName><forename type="first">L</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mobley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guthrie</forename><surname>Peter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computer-aided molecular design</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="711" to="720" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">On asymptotic behaviors of graph cnns from dynamical systems perspective</title>
		<author>
			<persName><forename type="first">Kenta</forename><surname>Oono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taiji</forename><surname>Suzuki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.10947</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Audio-visual scene analysis with self-supervised multisensory features</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
				<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="631" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Graph representation learning via graphical mutual information maximization</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minnan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinghua</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference 2020</title>
				<meeting>The Web Conference 2020</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="259" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Improving language understanding with unsupervised learning</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<pubPlace>OpenAI</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
	<note>Time Salimans, and Ilya Sutskever</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Electronic spectra from tddft and machine learning in chemical space</title>
		<author>
			<persName><forename type="first">Raghunathan</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mia</forename><surname>Hartmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enrico</forename><surname>Tapavicza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Anatole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Von</forename><surname>Lilienfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of chemical physics</title>
		<imprint>
			<biblScope unit="volume">143</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">84111</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Massively multitask networks for drug discovery</title>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Kearnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Webster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Konerding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.02072</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Toxcast chemical landscape: paving the road to 21st century toxicology</title>
		<author>
			<persName><forename type="first">Ann</forename><forename type="middle">M</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Judson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keith</forename><forename type="middle">A</forename><surname>Houck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">M</forename><surname>Grulke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patra</forename><surname>Volarath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inthirany</forename><surname>Thillainadarajah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chihae</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Rathman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">F</forename><surname>Matthew T Martin</surname></persName>
		</author>
		<author>
			<persName><surname>Wambaugh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemical research in toxicology</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1225" to="1251" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Extended-connectivity fingerprints</title>
		<author>
			<persName><forename type="first">David</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathew</forename><surname>Hahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="742" to="754" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Dropedge: Towards deep graph convolutional networks on node classification</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Deeply learning molecular structure-property relationships using attention-and gate-augmented graph convolutional network</title>
		<author>
			<persName><forename type="first">Seongok</forename><surname>Ryu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaechang</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seung Hwan</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><surname>Woo Youn</surname></persName>
		</author>
		<author>
			<persName><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.10988</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Schnet: A continuous-filter convolutional neural network for modeling quantum interactions</title>
		<author>
			<persName><forename type="first">Kristof</forename><surname>Schütt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huziel</forename><surname>Enoc Sauceda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Felix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Chmiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus-Robert</forename><surname>Tkatchenko</surname></persName>
		</author>
		<author>
			<persName><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="991" to="1001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Quantum-chemical insights from deep tensor neural networks</title>
		<author>
			<persName><forename type="first">Farhad</forename><surname>Kristof T Schütt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Arbabzadah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus</forename><forename type="middle">R</forename><surname>Chmiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><surname>Tkatchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Horovod: fast and easy distributed deep learning in TensorFlow</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Sergeev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Del Balso</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05799</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Zinc 15-ligand discovery for everyone</title>
		<author>
			<persName><forename type="first">Teague</forename><surname>Sterling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">J</forename><surname>Irwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2324" to="2337" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Computational modeling of β-secretase 1 (bace-1) inhibitors using ligand based approaches</title>
		<author>
			<persName><forename type="first">Govindan</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajiah Aldrin</forename><surname>Denny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1936" to="1949" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Infograph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization</title>
		<author>
			<persName><forename type="first">Fan-Yun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">How should relative changes be measured?</title>
		<author>
			<persName><forename type="first">Leo</forename><surname>Törnqvist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pentti</forename><surname>Vartia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yrjö</forename><forename type="middle">O</forename><surname>Vartia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American Statistician</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="43" to="46" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><surname>Hjelm</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.10341</idno>
		<title level="m">Deep graph infomax</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Atomnet: a deep convolutional neural network for bioactivity prediction in structure-based drug discovery</title>
		<author>
			<persName><forename type="first">Izhar</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Dzamba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abraham</forename><surname>Heifets</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.02855</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Residual attention network for image classification</title>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengqing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honggang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Smiles-bert: Large scale unsupervised pre-training for molecular property prediction</title>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuzhi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongmao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics</title>
				<meeting>the 10th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="429" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Smiles. 2. algorithm for generation of unique smiles notation</title>
		<author>
			<persName><forename type="first">David</forename><surname>Weininger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Weininger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">L</forename><surname>Weininger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and computer sciences</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="97" to="101" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Moleculenet: a benchmark for molecular machine learning</title>
		<author>
			<persName><forename type="first">Zhenqin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><forename type="middle">N</forename><surname>Feinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caleb</forename><surname>Geniesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Aneesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Leswing</surname></persName>
		</author>
		<author>
			<persName><surname>Pande</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemical Science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="513" to="530" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Pushing the boundaries of molecular representation for drug discovery with the graph attention mechanism</title>
		<author>
			<persName><forename type="first">Zhaoping</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feisheng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaozhe</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xutong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaojun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaomin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaixian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hualiang</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of medicinal chemistry</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Seq2seq fingerprint: An unsupervised deep molecular embedding for drug discovery</title>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feiyun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<editor>BCB</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Analyzing learned molecular representations for property prediction</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Swanson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wengong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connor</forename><surname>Coley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Eiden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angel</forename><surname>Guzman-Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Hopper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Kelley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miriam</forename><surname>Mathea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3370" to="3388" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Onionnet: a multiple-layer intermolecularcontact-based convolutional neural network for protein-ligand binding affinity prediction</title>
		<author>
			<persName><forename type="first">Liangzhen</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingrong</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuguang</forename><surname>Mu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACS omega</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="15956" to="15965" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
