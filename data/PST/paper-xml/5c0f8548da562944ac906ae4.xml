<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-task Cascade Convolution Neural Networks for Automatic Thyroid Nodule Detection and Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wenfeng</forename><surname>Song</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Shuai</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ji</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hong</forename><surname>Qin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Bo</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Shuyang</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Aimin</forename><surname>Hao</surname></persName>
						</author>
						<title level="a" type="main">Multi-task Cascade Convolution Neural Networks for Automatic Thyroid Nodule Detection and Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">53005BDD0ABB3FED600DF9C0DC94FAB0</idno>
					<idno type="DOI">10.1109/JBHI.2018.2852718</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T02:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Thyroid Nodules</term>
					<term>Detection</term>
					<term>Recognition</term>
					<term>Pyramid Convolution Neural Networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Thyroid ultrasonography is a widely-used clinical technique for nodule diagnosis in thyroid regions. However, it remains difficult to detect and recognize the nodules due to low contrast, high noise, and diverse appearance of nodules. In today's clinical practice, senior doctors could pinpoint nodules by analyzing global context features, local geometry structure, and intensity changes, which would require rich clinical experience accumulated from hundreds and thousands of nodule case studies. To alleviate doctors' tremendous labor in the diagnosis procedure, we advocate a machine learning approach to the detection and recognition tasks in this paper. In particular, we develop a multi-task cascade convolution neural network framework (MC-CNN) to exploit the context information of thyroid nodules. It may be noted that, our framework is built upon a large number of clinically-confirmed thyroid ultrasound images with accurate and detailed ground truth labels. Other key advantages of our framework result from a multi-task cascade architecture, two stages of carefully-designed deep convolution networks in order to detect and recognize thyroid nodules in a pyramidal fashion, and capturing various intrinsic features in a global-to-local way. Within our framework, the potential regions of interest after initial detection are further fed to the spatial pyramid augmented CNNs to embed multi-scale discriminative information for finegrained thyroid recognition. Experimental results on 4309 clinical ultrasound images have indicated that, our MC-CNN is accurate and effective for both thyroid nodules detection and recognition. For the correct diagnosis rate of malignant and benign thyroid nodules, its mAP performance can achieve up to 98.2% accuracy, which outperforms the common CNNs by 5% on average. In addition, we conduct rigorous user studies to confirm that our MC-CNN outperforms experienced doctors, yet only consuming roughly 2% (1/48) of doctors' examination time on average. Therefore, the accuracy and efficiency of our new method exhibit its great potential in clinical applications.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION AND MOTIVATION</head><p>T HYROID nodule is one of the most commonly-observed nodular lesions, with the prevalence of 19% to 68% in general population. Now, we have been witnessing about 240% increase in thyroid cancer during the past thirty years <ref type="bibr" target="#b0">[1]</ref>, which is one of the worst among all types of cancers [2], <ref type="bibr" target="#b2">[3]</ref>. At the imaging front, ultrasonography has been a dominant and preferred screening modality towards the clinical diagnosis of thyroid nodules, which is also used as guidance for fine-needle aspiration biopsy (FNAB) and subsequent treatments <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>. Recently, many guidelines have been established for radiologists to evaluate thyroid nodules based on ultrasound lishuaiouc@126.com characteristics <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>. However, since ultrasonography is susceptible to echo disturbances and speckle noises, ultrasonography based thyroid nodule diagnosis still heavily relies on rich experiences and delicate skills of senior radiologists. Less experienced practitioners may potentially have high misdiagnosis rate due to their inability of accurately comprehending ultrasonography characteristics. Mis-diagnosis might consequently call for unnecessary biopsy and surgery, that would make patients have much more pressure and anxiety, and at the same time unavoidably increase medical expense. To effectively leverage the high-quality diagnosis experiences gained by senior radiologists, smart thyroid diagnosis CADx system is urgently needed. Yet, the key success of the smart thyroid diagnosis CADx system build-up may be hindered by the fact that, the ultrasound thyroid's appearances are frequently influenced by internal content, shape, echogenicity, and many other factors, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>.</p><p>The benign nodules and the malignant nodules both have a wide variety of styles and layouts. Fig. <ref type="figure" target="#fig_0">1</ref>(a) shows the benign nodules, and most of them have irregular shapes, smooth regions, and boundaries. Fig. <ref type="bibr">1(b)</ref> shows the malignant nodules, and most of them have irregular shapes, coarse regions, and boundaries. Therefore, the thyroid nodules are hard to be directly recognized based on color and shape features.</p><p>In recent years, there exist many studies that employ sonographic features for thyroid malignancy diagnosis, which can be roughly classified into two main categories: hand-crafted feature based classifiers <ref type="bibr" target="#b9">[10]</ref>, and the data-driven methods.</p><p>Hand-craft Feature Methods. The pipeline of these methods frequently involves feature extraction and classification. Typical methods in this category may include, GLCM, LBP, Discrete Wavelet Transform (DWT), K-Nearest Neighbor (K-NN), Probabilistic Neural Network (PNN), Decision Tree (DT), Gaussian Mixture Model (GMM), Support Vector Machine (SVM), Adaboost classifier, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref> Bayesian classifier, GBDT <ref type="bibr" target="#b15">[16]</ref> and random forest <ref type="bibr" target="#b16">[17]</ref>. Despite their rapid development in recent years, handcrafted features in some sense can only exploit the low-level information, such as image texture <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, geometry morphology <ref type="bibr" target="#b13">[14]</ref>, and statistical distributions <ref type="bibr" target="#b10">[11]</ref>. Such methods usually need to further employ classifiers to conduct classification. Hence, only if given highly-discriminative features, such methods could well solve the recognition problem.</p><p>Recently, some works focused on the characteristics of malignant thyroid in high resolution ultrasound images and ultrasound elastography. For example, Acharya et al. <ref type="bibr" target="#b17">[18]</ref> proposed a novel Gabor transform based automated system for the classification of benign and malignant thyroid nodules using high resolution ultrasound (HRUS). Sun et al. <ref type="bibr" target="#b18">[19]</ref> demonstrated that ultrasound elastography had high sensitivity and specificity for the identification of thyroid nodules. Meanwhile, Raghavendra et al. <ref type="bibr" target="#b19">[20]</ref> fused the spatial gray level dependence features (SGLDF) with the fractal textures to decipher the intrinsic structure of benign and malignant thyroid lesions. Nevertheless, for thyroid nodules, the high variability of the ultrasound image makes it hard to effectively distinguish benign nodules from malignant ones. So it is even more critical to design and select the most significant features, let alone comprehensively fusing different scales of local and global features as experienced radiologists would do. Besides, available classifiers mostly tend to over-fit the training dataset,since the features locally designed at single scale and in single region are insufficient to encode critical information in order to determine different types of nodules.</p><p>Data-driven Learning Methods. As for the data-driven methods, recently the convolution neural networks (CNNs) can greatly improve the classification and detection performance on natural images without the need of hand-crafted feature description, such as, Alexnet <ref type="bibr" target="#b20">[21]</ref>, GoogLenet <ref type="bibr" target="#b21">[22]</ref>, Residual net <ref type="bibr" target="#b22">[23]</ref>, Faster RCNN <ref type="bibr" target="#b23">[24]</ref>, Single Shot Detection (SSD) <ref type="bibr" target="#b24">[25]</ref>. etc. One salient advantage of CNNs is that, they could overcome the aforementioned difficulties by extracting multi-level features automatically. Now, even though it is possible to use hybrid CNNs to classify the thyroid nodules [2], it is still much more complex and redundant to extract features with multiple scales in CNNs. For example, existing methods oftentimes fail to recognize nodules of smaller scales or lower contrast, and this is especially true for thyroid nodules that would rely on proper recognition of their neighboring tissues towards correct diagnosis. The features translated by CNN are aggregated in multiple-level layers. The lower levels represent the shallow features like shapes, gradient, and color appearance, while the high level features represent the semantic discriminative features.</p><p>Compared with the traditional feature extraction methods, it is demonstrated in <ref type="bibr" target="#b24">[25]</ref> that CNN has two advantages. <ref type="bibr" target="#b0">(1)</ref> The detection based on CNN features is robust to distortions, including changes caused by camera lens, different lighting conditions, different poses, partial occlusions, horizontal and vertical shifts, etc.; (2) The computational cost of CNNbased feature extraction is relatively low, because the same coefficients in the convolutional layer are used across the input images. Motivated by the success in natural image recognition, some recent works were proposed to apply the CNNs to thyroid recognition.</p><p>However, the instinct limitation of existing CNNs is that, they only consider single-region features, wherein feature kernels locally focus on the single scale perception while ignoring the corresponding context information. Moreover, almost all of the existing methods tend to separately conduct detection and classification tasks, which may easily make the information isolated between the two tasks. However, the features should be shared and complemented with each other in both scales and context. In this circumstance, Liu To achieve the goal of incorporating effective global features into our smart diagnosis system, we propose a multi-task cascade pyramid CNN framework (MC-CNN) to jointly learn multi-level features, as shown in Fig. <ref type="figure">2</ref> and detailed in Algorithm 1. In contrast to the existing CNN methods <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b22">[23]</ref>, we extend the single-scale network to the pyramid based coarseto-fine spatial convolution network, where the integrated local and global clues in concert could make the final prediction much more reliable. The critical clinical testing has shown that, our approach could achieve the state-of-the-art performance in a variety of real patients' datasets, including the datasets with different scales and from patients with differentage groups. Specifically, the salient contributions of this paper can be summarized as follows:</p><p>• We propose a multi-task cascade CNN framework, to jointly perform thyroid detection and recognition in a coarse-to-fine manner, which supports coarsely locating and classifying nodules on the entire ultrasound image to produce potential nodule proposals first, and then pinpointing nodules in a much-finer scale based on spatial pyramid CNNs in real time.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. DETECTION AND RECOGNITION BASED ON MULTI-SCALE SSD NETWORK</head><p>In construction of E c in Algorithm 1, our training strategy is inspired by the SSD and multiple box framework <ref type="bibr" target="#b24">[25]</ref>. Nonetheless, as shown in Fig. <ref type="figure" target="#fig_2">3</ref> (the pipeline of multiple scale detection network) and Fig. <ref type="figure" target="#fig_4">5</ref>, we extend them to handle highly-varying thyroid nodules. We further re-construct the SSD detection network by adding multiple full convolution layers followed by nodules prior-guided anchors (extends from faster-rcnn <ref type="bibr" target="#b23">[24]</ref>) generated layer to extract different scales of features from global to local. The detection approach is based on a feed-forward convolution network, which produces a fixed-size collection of bounding boxes and the corresponding class-assigning scores for the object instances in those boxes, followed by a non-maximum suppression (NMS) step to produce the final detection candidates(detailed in our supplement material). The shallow network layers are based on a standard architecture used for high quality image classification (truncated before any classification layers), wherein we leverage the base network used by SSD to extract feature maps. Different from the original SSD, we add multi-scale layers to fit for the thyroid nodules and arrive at coarse recognition.</p><p>Multi-scale Detection Network. Based on the two distributions of the thyroids' ratios and scales, which are shown in Based on the output of the feature maps from the base net of VGG16, we add the anchor boxes embedded with the thyroid scales and ratio prior (green and purple boxes). Furthermore, to cover the large and small scales of nodules, we concatenate all the newly added convolutional layers to the final detection layer. Fig. <ref type="figure" target="#fig_3">4</ref>, the multi-scale SSD layers involve two-fold improvements.</p><p>The first one is to detect all the ratios of the nodules by using a set of anchor boxes ranging from 0.75 to 1.5, which are pre-computed in the training datasets. Specifically, the anchor boxes slide over the feature map in a convolutional manner, so that the nodule position relevant to the boxes will have a high response. In this way, we predict the offsets related to the default box shapes and the scores, which indicate the presence of a malignant nodule in each of those boxes. The newly-proposed anchor boxes can largely decrease the number of the false positive candidates.</p><p>The second one is suitable for the thyroid scales by incorporating multi-size perception fields of the anchors into multiple fully convolutional layers. Since it is hard to cover all the nodules with single-layer perception based conventional detection methods. We add multiple convolutional feature layers at the end of the truncated base network as <ref type="bibr" target="#b24">[25]</ref> does. These layers gradually decrease in size, and allow multi-scale detections and predictions continually. Each of the newlyadded convolutional layers predicts a result at different scales of perception fields. The size ranges from 5% (50*35) to 100%(1024*768) with respects to the input image (1024*768), and covers most of the nodules, whose sizes range from 35*40 pixels to nearly 774*573 pixels. All the feature maps are followed by a set of anchor boxes, which are determined by the distribution of the thyroid nodules. The anchors slide over the feature maps in the same way as the ratios guided anchor boxes do.</p><p>For a convolutional feature map with a size of W × H, there are W Hk anchors in a single nodules-guided layer. To add global high-level semantic features and local low-level detail features, this specially-defined layer is added after each full convolution layer to extract different-resolution features. Specially in Fig. <ref type="figure" target="#fig_4">5</ref>, the left nodule has an aspect ratio close to 1:1. Our model adds several nodule prior guided feature maps at the end of the base network, which predicts the offsets to the thyroid boxes with different scales and aspect ratios, and predicts their associated confidences. The top 4 high confidence boxes are with the aspect ratios of 1:1, 1:2, 1:3, and 1:4, respectively. The top-ranked candidate boxes of the right nodule have similar aspect ratio with the nodules. We show two scales of the feature maps, which are 4*4 and 8*8. The newly-added layers are concatenated before the final loss layer. We compute c class scores and 4 offsets relevant to the original default box shape. This results in a total of (c + 4)k filters that are applied around at each location in the feature map, yielding (c + 4)kmn outputs for a m × n feature map.</p><p>Multi-task Loss. Each training image is annotated with a ground-truth class label y and a ground-truth bounding-box regression target l, which denotes a 4-dimension vector, (x position, y position, width, height). We use multi-task loss L D and L C on each labeled bounding box to jointly train the bounding-box regression and recognition: 1) Bounding box regression: for each candidate proposal, we predict its offset to the nearest ground truth. The learning objective function is formulated as a regression problem, and we employ the smooth L 1 norm proposed in <ref type="bibr" target="#b25">[26]</ref> to make the predicted boxes of each sample x be close to its ground truth bounding box l, the L D i is the loss for coarse classification error:</p><formula xml:id="formula_0">L D (l, l * ) = R r=1 smooth L1 (l r -l * r ),<label>(1)</label></formula><p>where,</p><formula xml:id="formula_1">smooth L1 (l r -l * r ) = 0.5(l r -l * r ) 2 if |l r -l * r | &lt; 1 |l r -l * r | -0.5 otherwise . (2)</formula><p>Here, l r is the regression box offset and position, l r represents the ground-truth label, while l * r denotes the predicted label. The smooth loss refines the regression in a continuous way, which is suitable for various shapes of nodules; 2) Coarse recognition of the thyroid nodules: the objective function learning is formulated as a binary classification problem. For each bounding box, we use the cross-entropy loss:</p><formula xml:id="formula_2">L C (p(x|w), y) = R r=1 -(y r log(p r )) + (1 -y r )(1 -log(p r )).</formula><p>(3) Here, p r is the probability produced by the network, w is the trained weight of E c , and it indicates that sample x i is a thyroid nodule. The notation y r ∈ {0, 1} denotes the groundtruth label.</p><p>Specially, to accelerate the speed of convergence, we employ the VGG16 network E 0 as a feature extractor for its high performance in Imagenet classification tasks. Then, we add the multi-scale full convolution layers, similar to SSD. Following each feature map, we add the nodules prior-guided </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. RECOGNITION REFINEMENT BASED SPATIAL PYRAMID ARCHITECTURE</head><p>In construction of E f in Algorithm 1, we further introduce our spatial pyramid architecture, which in fact represents an effective global contextual prior. In a deep neural network, the size of receptive field can roughly indicate to what extent we use contextual information. Although the theoretical receptive field of CNNs is already larger than the input image <ref type="bibr" target="#b22">[23]</ref>, and the existing work <ref type="bibr" target="#b26">[27]</ref> proves that the empirical receptive field of CNN is much smaller than the theoretical one especially on high-level layers. This requirement makes single convolution neural networks insufficient to incorporate the momentous global context prior. We address this issue by embedding an effective global context aggregation network structure into original CNNs. Specially, the most direct intuition is to enlarge the perception. However, for thyroid ultrasound images with complex background and structure, this strategy is still not enough to cover necessary information. The annotated nodules in these thyroid images relate to many substrate and tissue locations. Directly fusing them to form a single vector may lose the spatial relation and cause ambiguity. However, the global context information along with the neighboring region's context is helpful in this regard to distinguish among various categories. Thus, a more powerful representation should fuse information from different neighboring regions with these receptive fields. Similar conclusions are drawn in the classical works <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref> of natural RGB image classification. In <ref type="bibr" target="#b28">[29]</ref>, different-level feature maps generated by pyramid pooling are finally flattened and concatenated to be fed into a fully connected layer for classification. This global prior is designed to remove the fixed-size constraints of CNN for image classification. To further reduce context information loss among different regions, we propose multiple levels of global priors, containing information with different scales and varying among different regions. We call it spatial pyramid module, as illustrated in Fig. <ref type="figure" target="#fig_5">6</ref>. The spatial pyramid module fuses features under three different pyramid scales, and is highlighted in blue for convolution operators and purple for pooling operators. The following pyramid levels separate the feature map into different neighboring regions and form the pooling representation for different locations. The different spatial pyramid level can produce the feature map with varied size. To maintain the weight of global feature, we use 1 × 1 convolution layer for each pyramid level to reduce the dimension of context representation to 1/N of the original one, if the level size of pyramid is N . Finally, different levels of features are concatenated as the final global feature. Specially, the feature maps of f c7 are fed to the spatial pyramid levels with multiple scales of convolution and pooling kernels.</p><formula xml:id="formula_3">f p = i (f c7 (x * w i + b))(i = 1, 3, 5).<label>(4)</label></formula><p>Here, w i represents the kernel of the i th operator, including the convolution and pooling kernels, and b is a bias parameter.</p><p>Based on the learned weights over spatial pyramid, we can approximately obtain the semantic feature map of thyroid nodules as follows:</p><formula xml:id="formula_4">M (x) = ReLU (f p (w, x)).<label>(5)</label></formula><p>We use the ReLU active function to make the trained model none-linear. Here, w is the learned parameter for detection and recognition models E c and E f . Based on this activated feature map, we can further obtain the probability of the nodules' classes as follows:</p><formula xml:id="formula_5">p((y = j|x); w) = exp(M j )(x) C j=1 exp(M j (x)) .<label>(6)</label></formula><p>Here, j represents the j th class. We further use the detected bounding box to refine the regions to be precisely classified.</p><p>The probability vector for each class is p j , and the loss of the crop region is defined as:</p><formula xml:id="formula_6">L F (y, l) = C j y j (1 -log(p j * p c )).<label>(7)</label></formula><p>Here, p c is the coarsely predicted softmax probability of the binary classification under the loss of Eq. 3 and p j is the softmax probability of the binary classification.</p><p>It may be noted that, the number of the pyramid levels and the size of each level are variables. They are related to the size of feature map that is fed into the pyramid pooling layer. The structure abstracts different nodule regions by adopting varying-size pooling kernels in a few strides. Thus, the multi-stage kernels should maintain a reasonable gap in representation. Our pyramid pooling module is a threelevel one with kernel sizes of 1 × 1, 3 × 3, 5 × 5 respectively. For the type of spatial pyramid operation in different layers, we will conduct extensive experiments to show the differences in Section VI.  <ref type="figure" target="#fig_2">3</ref>. Such joint-task model shares the features for both detection and recognition, so that the detected nodule candidates can be refined in a coarse-tofine manner from both low and high level of feature maps. The loss corresponding to the two tasks is formulated as:</p><formula xml:id="formula_7">L(x) = N i=1 [L D i (y i , y * i ) + L C (l i , l * i )] + L F (y, y * ).<label>(8)</label></formula><p>Here, the L D i is the loss for coarse classification error, y i (y) is the ground truth label of the thyroid nodules, and y * i is the predicted label (labels) of the thyroid nodules. L C is the loss for detected box location, where N is the number of the matched boxes. If N = 0, we set the loss to 0. The localization loss is a smooth L 1 loss between the predicted box and the ground truth box. Meanwhile, L F is the fine classification error. The train process of MC-CNN is detailed in Algorithm 2. Besides, the training dataset covers most of the ultrasound images corresponding to different-aged patients and differentsized nodules. For all patients with malignant nodules and some patients with benign nodules who underwent surgery, the most accurate diagnosis is based on the histopathological examination results over the excised thyroid nodules. The ground truth is taken inside the same patient. The pixel depth ranges from 0 to 255. The images are collected in the clinical settings. Thus, the examination size is determined by the doctors' ROI of the thyroids.</p><p>To this end, all the thyroid instances involved in our training ultrasound image set have been examined via pathological examination. The involved ultrasound images are captured with Phillips HDI 5000, IU 22, GE Logiq 9 or Logiq 7 devices equipped with either a 5-12 MHz or an 8-15 MHz linear-array transducer. Among this initial cohort, only the patients who meet the following criteria are included: (1) older than 18 years of age, (2) with total or nearly total thyroidectomy or lobectomy, (3) with complete preoperative thyroid ultrasound images, and (4) with surgical pathology examination. =Moreover, the lesions and nodules that fail to meet the criteria for any pattern of ATA guidelines are Finally, a total of 1580 patients with 4309 images are obtained after surgery or FNA. Among these examples, 3128 thyroid nodules are benign and 3100 ones are malignant, and the mean nodule size is 2.4cm. Each thyroid nodules has several longitudinal cutting maps or cross cutting maps. In total, 6228 thyroid nodules images are obtained. Meanwhile, the boundary of the thyroid nodules in each image is manually delineated by the doctors. Thus, we can get 6228 labeling boxes, and these thyroid nodules images for training our MC-CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation</head><p>The framework contains two stages: (1) Detection and coarse recognition: to detect the nodule locations, we first employ a VGG-16 network (E 0 ) as the backbone to extract the high-level semantic feature maps from the ultrasound image. Then, we add convolutional layers at the top of the truncated base network. All the convolutional layers are followed by a nodule prior guided layer. Each of the multiple convolutional layers produces the detection and coarse recognition results at different scales. All the bounding boxes are processed by NMS. This step is to build up the detection and coarse recognition model (E c ); (2) Fine ecognition: to further improve the accuracy of classification, we add a spatial pyramid based recognition network to predict the category of the nodules at different scales. The fine recognition model is represented as E f .</p><p>The stage (1) and (2) are the multi-task cascade CNN.</p><p>For our multi-scale detection network in the MC-CNN, we adapt a data-augmented strategy to improve the performance in the training procedure. The strategy leverages the entire original input image, sampled patches and randomly cropped patches, of which, the minimum Jaccard overlapping with the objects could be 0.1, 0.3, 0.5, 0.7, or 0.9, and the photo-metric distortions are similar to those described in <ref type="bibr" target="#b24">[25]</ref>. We first train the models with 10 -3 learning rate for 60k iterations, and then with 10 -4 for another 60k iterations.</p><p>As for our spatial pyramid based recognition refinement network in the MC-CNN, the inputs of the base CNN are randomly sampled patches with size of 512 * 512 from the entire thyroid nodules image, and the corresponding outputs are based on the average results of all the inputs. Our spatial pyramid CNN uses the multi-scale feature maps to train the softmax for thyroid nodules recognition. To demonstrate the efficiency of our MC-CNN, we also implement several commonly-used classification methods, including Nearest Neighbors, decision tree, random forest, adaboost, KNN, Bernouli Naive Bayesian, and GBDT. The classifiers are optimized with grid search in our experiments.</p><p>To evaluate the reliability and stability of the classifiers, we further train 7 classifiers with the feature vector extracted from fc7 layer in AlexNet <ref type="bibr" target="#b20">[21]</ref> as input. In the AlexNet training process, both the weight decay and bias decay are 0.0005. The learning rate is set 0.02. The momentum is 0.9, linearly over 10 epochs. Besides, in our experiments, the thyroid dataset is split by 8 : 2 for training and testing respectively. We use the Caffe lib to train the datasets on Tesla K80 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. RESULT ANALYSIS</head><p>In this section, we first conduct three experiments to separately demonstrate the significance of the multiple scale, the spatial pyramid, and the coarse-to-fine structure. Then, we compare the MC-CNN with the state-of-the-art methods. In addition, we conduct user studies in practice to demonstrate the applicability of the MC-CNN. Finally, to test the generalization ability of our model, we conduct unsupervised transfer learning experiments on publically-available datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Design</head><p>Evaluations of our Multi-Scale SSD Network. Our multiscale detection network is designed to detect and coarsely recognize the nodules. The detection results have a significant improvement. In the results, all the small-scale and large-scale nodules, which were lost in the original SSD network, are correctly detected. The results are shown in Fig. <ref type="figure">7</ref>. The quantitative performances are shown in Table <ref type="table">I</ref> and Table <ref type="table" target="#tab_2">II</ref>. All the experiments are conducted with a 5-fold cross validation. The quantitative results show that, our adaptive thyroid detection is effective for all the scales of ultrasound images captured from different-aged patients. From the view point of thyroid nodule size, the nodules larger than 3cm can benefit most, which are highlighted in bold.</p><p>Furthermore, as shown in Table <ref type="table">I</ref>, the mAPs of the three age groups are improved ranging from 0.7% to 4.9%, compared with the original SSD network. In particular, in different-aged groups, the young group has the most discriminate features for detection. The mAPs of the three-scale (small, middle, large) thyroid nodules are improved ranging from 0.1% to 4%. Especially, the large and small scales of thyroid nodules, which are mis-detected in the original SSD, could be detected accurately by our M-SSD. The mAPs of malignant ones are TABLE I: Performance of the M-SSD network in 5-fold cross validation (mean ± standard variance). IOU=0.5 (Intersection over Union), "M-SSD" represents the multi-scale SSD.  improved ranging from 0.3% to 4.9%, which are larger than that of the benign ones (ranging from 0%-1.2%).</p><p>Evaluations of our Spatial Pyramid Architecture. To evaluate our spatial pyramid based recognition refinement network, we further conduct three experiments. The first one is to compare with the CNNs without the spatial pyramid layers. We have compared it with AlexNet (conv5-small), GoogLenet.</p><p>Here we compare the spatial layers in different CNN layers: after the 5th pooling (pool5) layer, after the 5th convolution layer (conv5). The result is shown in Fig. <ref type="figure" target="#fig_7">8</ref>, and it indicates that, the network with the spatial pyramid layer added after the 5th convolution layer, performs the best among the 4 networks. The network 'pool5' ranks the second among all 4 networks, which adds the spatial pyramid layer after the 5th pooling layer. The original AlexNet ranks the third. Meanwhile, the GoogLenet ranks the forth. This phenomenon states that, the spatial pyramid layer can help improve the performance of the thyroid nodules recognition, but the GoogLenet has the most parameters, which leads to an over-fitting result. To further study what happened in the spatial pyramid networks, we visualize the AlexNet and the AlexNet with spatial pyramid network after the 5th convolution layer. The result is shown in Fig. <ref type="figure" target="#fig_9">10</ref>. The results demonstrate that, the spatial pyramid structure can activate larger perceptions than the original AlexNet for nodules in the feature map layers.</p><p>The second experiment is to compare it with 7 existing Fig. <ref type="figure">7</ref>: Illustration of the improvement benefiting from our multi-scale SSD, which can detect the missing small-scale and large-scale nodules. The large and small sizes of thyroids are beyond the detection ability of the original SSD network. The blue boxes represent the large thyroids, while the red boxes represent the small ones. Some more cases are in Fig. <ref type="figure" target="#fig_12">13</ref>.   <ref type="figure" target="#fig_10">11</ref>. For the AUC, 6 of 7 classifiers are lower than spatial pyramid networks. The random forest is close to spatial pyramid networks, however, the ROC curve of spatial pyramid is more smooth than that of random forest. Thus, the spatial pyramid furnished CNN achieves the best performance in most classifiers.</p><p>In our third experiment, we combine our MC-CNN framework with the 7 classifiers, which gives rise to significant performance improvement. We fed the classifiers with the fc7 layer's output of the spatial pyramid network. The result is shown in Fig. <ref type="figure" target="#fig_8">9</ref>. Three of the 7 classifiers improve dramatically to 100% over the testing dataset, including Adaboost, Bernouli Naive Bayesian, and GBDT. Moreover, other classifiers are improved more or less from 0.6% to 2.68%.</p><p>Comparisons with State-of-the-art Methods. Our coarseto-fine framework can improve the performance of thyroid nodules recognition, which is demonstrated through three experiments: coarse classification network from detection network, the single task classification network from the doctorcropped results, and the MC-CNN framework. The quantity performance is documented in Table <ref type="table" target="#tab_3">III</ref>. The first row is the coarse recognition result. The 2-4 rows are the singletask networks derived from AlexNets and GoogLenet, with different layers embed into the spatial pyramid module. The result shows that, our MC-CNN achieves great performance improvement compared with the single coarse and single fine  classification networks. The average accuracy is improved ranging from 6% to 10.1%, the sensitivity and the Specificity is improved from 7.5% to 12.5%. It demonstrates our coarseto-fine framework can extract consistent features from the detection results for the classification task. For the single classification based on the input patches cropped by doctors, its performance is slightly lower than that based on the detection boxes, which is 6% lower in accuracy, and 7.5% lower in sensitivity. Moreover, the results show that the multitask framework performs better than the original SSD. Besides, for the cases failed in the original SSD, we further analyze their improvement benefiting from our method by visualizing their concrete nodules in Fig. <ref type="figure" target="#fig_11">12</ref> and Fig. <ref type="figure" target="#fig_12">13</ref>. We can conclude from the two sets with the scales larger than 5cm and smaller than 0.5cm improve most.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. User Studies of Our MC-CNN Framework</head><p>To evaluate our MC-CNN, we compare our results with those from the senior doctors. We make a dataset that contains 360 images covering three-age stages and three scales, which are separately collected and labeled, which is described in Section V-A.</p><p>The doctors referred to the features of malignant based on ATA, and drew conclusions based on experience. The  results are documented in Table <ref type="table" target="#tab_4">IV</ref>, and it states that our MC-CNN performs better than human doctors in terms of both time efficiency and accuracy. Without our CADx system, the doctors become tired and impatient when facing a dataset with more than 360 (187 benign nodules and 180 malignant nodules) nodules. Furthermore, in case of complex ultrasound images, the result becomes even more distinct since our MC-CNN achieves higher accuracy than the doctors, of which, the improvement in accuracy is 12%, the improvement in sensitivity is 13% and the improvement in sensitivity in 8%, with only 2.1% time consumption on average compared to   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Evaluations using Public Dataset</head><p>In order to demonstrate the generalization ability of our MC-CNN, we verify it by conducting transferred learning experiments on the open dataset <ref type="bibr" target="#b29">[30]</ref>, without the need of additional training on this dataset. This dataset has a total number of 299 patients, including 270 women and 29 men, whose ages vary as 57.35 ± 16.2 years. We treat the labels in triads (following <ref type="bibr" target="#b29">[30]</ref>) over "(4c) Three or four suspicious ultrasound features" and "(5) Five suspicious features" as the malignant nodules while treating "Normal" thyroid and "Benign" level as benign ones. Finally, we obtain 111 malignant ultrasound images and 41 benign ultrasound images. Our MC-CNN outperforms most of the previous works with the significant advantage, as shown in Table <ref type="table" target="#tab_6">VI</ref>. The number of the benign dataset is half of that of the malignant dataset. The classifiers, such as Naive Bayesian, GBDT, and MLP, are sensitive to such data distribution, thus the sensitivity is lower than the Specificity. At the same time, our MC-CNN is stable in both classes even if the distribution is heavily unbalanced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Discussion</head><p>In this section, we mainly describe the advantage, the comparison with the state of the art, in what it could be improved, which could be further applications.</p><p>Advantages. MC-CNN has three main advantages as follows. (1) Our methods includes two tasks, and the nodules can be detected and recognized at the same time. In our framework, the thyroid US images based detection and recognition tasks are jointly learned for sharing the common features while distinguishing the malignant, the benign, and the background. However, this multi-task network cannot perform classification with high performance, and it only provides precise bounding boxes of the nodules. To address this issue, we propose to learn the task in a coarse-to-fine manner, that uses the spatial pyramid module to improve the recognition accuracy. With the coarse-to-fine framework, the thyroid with different scales, especially the extremely large and small ones, are detected and correctly classified. Meanwhile, it suggests that the size of kernels of CNN filters indeed affects the performance of the recognition task. We only extend the simple CNN structure to improve the efficiency; (2) MC-CNN can detect and recognize a wider range of the nodules in multiple scales. The scale prior of the nodules is the important information both for detection and recognition. Although MC-CNN could be divided into serval scales to learn different scale features, the concatenated features ignore the correlation among different scales. To overcome these problems, we propose the spatial pyramid module to learn the multiple scales of features in a single module. The spatial pyramid can effectively fuse different sizes to generate a complementary effect, while the AlexNet is easy to be fine-tuned and has a simple structure to be extended conveniently. Moreover, the Alexnet is not easily overfitted on the limited scales of datasets. Therefore, MC-CNN can effectively represent the thyroid nodule features for classification, which has been proved in the clinically setting involving doctors' studies; (3) MC-CNN could be generalized to handle more datasets without training. Thyroid nodule detection and recognition for US images are solved by a novel MC-CNN, which has the advantages over a simple network architecture with good performance. In this study, we first evaluate the feasibility of MC-CNN for US images, and the results on all the ages and sizes of nodules show that MC-CNN achieves competitive results compared with AlexNet, GoogleNet, and conventional classifiers such as, GDBT, SVM, etc. Limitations. Our MC-CNN still has some limitations. For example, in Fig. <ref type="figure" target="#fig_11">12</ref> and Fig. <ref type="figure" target="#fig_12">13</ref>, for the extremely small (&lt; 0.01cm) and extremely large (&gt; 10cm) nodules, our MC-CNN tends to produce failure cases, which are marked with yellow boxes, because the scales of the nodules are critical for accurate detection and recognition. Despite our prior and on-going efforts, currently high-quality training datasets are still insufficient compared with natural images that have been widely employed in various deep learning applications. The essential reason is that, the rapid acquisition of high-quality, high-volume medical datasets remains a bottleneck, it needs a time-consuming and rigorous verification process to label ground truth, which must consume huge labor of experienced radiologists. Therefore, we would further introduce transfer learning into the pre-trained models to make our model easily be transferred to other different scales of unmanifested nodules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION AND FUTURE WORKS</head><p>In this paper, we have advocated a novel MC-CNN framework that can learn thyroid nodule detection and classification on ultrasound images. The new learning architecture affords the detection and classification tasks to share commonlyneeded features, with an objective of better distinguishing benign nodules from malignant nodules, as well as the complex background. In order for such goal to be easily accomplished, in our new learning architecture we must add a multi-scale layer to improve the detection performance for thyroid nodules that could be varying significantly in scales. Consequently, the detected nodule candidates are fed back into the spatial pyramid augmented AlexNet to further improve the classification performance. As a result, our MC-CNN has shown superior advantages over the original single shot detection and other single task classification methods based on our comprehensive experiments.</p><p>In the near future, we will continue to conduct extensive user studies and evaluations towards possible clinical trials. It is our expectation that, our MC-CNN architecture could achieve more promising performance for smart thyroid nodule diagnosis on ultrasound images, which leads to a better and greater potential in ultrasound-based clinical applications in the future. Meanwhile, with the scale increase of the collected datasets, we plan to study more compacted CNN structures to extract more discriminative features efficiently. Specifically, more comprehensive evaluations on how to adapt our MC-CNN to other popular networks also deserve our immediate efforts.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Illustration of thyroid nodules: (a) Benign nodules; (b) Malignant nodules. Calipers labels are shown for better understanding only, and are not used for training and testing.et al.<ref type="bibr" target="#b24">[25]</ref> propose a single shot detection (SSD) network to detect and recognize objects from high-quality natural images. Inspired by such method, we employ a coarse-to-fine pyramid framework E c for the thyroid nodule classification on 2D ultrasound images. One advantage of the coarse-to-fine network is that, a multi-task joint network would enable to learn the detection and classification tasks at the same time in a mutually-supplementary way, just in the same way as what experienced radiologists would do. Moreover, the spatial pyramid network could extract features that would continuously change from global to local, which in fact imitates the process of simultaneously considering the neighboring regions and the high-level semantic features.</figDesc><graphic coords="2,315.19,56.07,122.09,70.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1 repeat 4 : 5 : 6 :Fig. 2 :</head><label>14562</label><figDesc>Fig. 2: Architecture of our MC-CNN framework. The framework contains two stages. (1) Detection and coarse recognition: to detect the nodule locations and coarsely recognize the nodules; (2) Fine recognition: to recognition the nodules finely.</figDesc><graphic coords="3,77.77,282.10,137.02,105.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Fig.3: Detection and recognition based on multi-scale SSD network. Based on the output of the feature maps from the base net of VGG16, we add the anchor boxes embedded with the thyroid scales and ratio prior (green and purple boxes). Furthermore, to cover the large and small scales of nodules, we concatenate all the newly added convolutional layers to the final detection layer.</figDesc><graphic coords="3,211.51,282.10,92.28,105.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Illustration of the thyroid nodules: (a) The ratio histogram of the thyroid nodules; (b) The scale histogram of the thyroid nodules (pixels). 'Ratio' means the width divided by the height. 'Scales' means the area of the nodules.</figDesc><graphic coords="4,48.96,56.07,133.65,86.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: SSD-based multi-scale network. The colored layers in the middle rows represent the added multi-scale layers. They are designed by considering the prior distributions of the thyroid nodules. Based on the multiple levels of feature maps, the top 4 anchor boxes with different ratios are represented with blue boxes. The distributions at different positions are illustrated in two examples. layer to generate anchors for each class. At first, we construct the detection model to find the proposals corresponding to the probable thyroid nodules, which outputs the location (x pos , y pos , w, h) and the class-assignment score s of the nodules. Then, we construct the fine recognition network to refine the recognition accuracy.</figDesc><graphic coords="4,182.62,59.27,133.65,80.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: Spatial pyramid network E f . The spatial pyramid module consists of two convolution and pooling layers. The three layers are all with multiple scales of filters. Detailed is described in our supplemental document.IV. COARSE-TO-FINE CASCADE NETWORK Specifically, the detector and classifier (including coarse and fine classification) involve bounding box regression, coarse recognition of the thyroid nodules, and local fine recognition of the detected nodules. The details of our cascade multiple tasks network are shown in Fig.3. Such joint-task model shares the features for both detection and recognition, so that the detected nodule candidates can be refined in a coarse-tofine manner from both low and high level of feature maps. The loss corresponding to the two tasks is formulated as:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Algorithm 2 3 : 4 : 5 : 9 :</head><label>23459</label><figDesc>MC-CNN Training Algorithm 1: while L D (x, l, l * ) &lt; δ and L C (p r , y r ) &lt; δ do 2: Extracting the basic features from VGGNet; Feeding the features into the multi-scales layers; Generating anchors (x pos , y pos , w, h) via nodules prior guided layers; Optimizing loss L D (x, l, l * ) and L C r (p r , y r ) ; 6: end while; 7: Cropping detected images based on the predicted boxes (x pos , y pos , w, h) with model E c ; 8: while L F (y, y * ) &lt; δ do Extracting feature maps from original CNNs; 10: Refining feature maps f c7 in spatial pyramid f p ; 11: end while; V. DATASETS AND IMPLEMENTATION A. Dataset Labeling for Network Training One of the most important factors in any deep learning model is the training dataset labeling. Our training dataset is labeled by the senior doctors of Peking Union Medical College Hospital according to the pathology verification, which lasts two years in total. The thyroid nodules are marked with bounding boxes and the assigned benign/malignant class labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 :</head><label>8</label><figDesc>Fig. 8: ROC of CNNs without classifiers.</figDesc><graphic coords="7,347.54,53.23,179.93,179.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 :</head><label>9</label><figDesc>Fig.9: MC-CNN features with classifiers. classifiers, including nearest neighbors, decision tree, random forest, adaboost, KNN, Bernouli Naive Bayesian, and GBDT. All the classifiers are trained with grid search to find the proper hyper-parameters. The results are shown in Fig.11. For the AUC, 6 of 7 classifiers are lower than spatial pyramid networks. The random forest is close to spatial pyramid networks, however, the ROC curve of spatial pyramid is more smooth than that of random forest. Thus, the spatial pyramid furnished CNN achieves the best performance in most classifiers.In our third experiment, we combine our MC-CNN framework with the 7 classifiers, which gives rise to significant performance improvement. We fed the classifiers with the fc7 layer's output of the spatial pyramid network. The result is shown in Fig.9. Three of the 7 classifiers improve dramatically to 100% over the testing dataset, including Adaboost, Bernouli Naive Bayesian, and GBDT. Moreover, other classifiers are improved more or less from 0.6% to 2.68%.Comparisons with State-of-the-art Methods. Our coarseto-fine framework can improve the performance of thyroid nodules recognition, which is demonstrated through three experiments: coarse classification network from detection network, the single task classification network from the doctorcropped results, and the MC-CNN framework. The quantity performance is documented in TableIII. The first row is the coarse recognition result. The 2-4 rows are the singletask networks derived from AlexNets and GoogLenet, with different layers embed into the spatial pyramid module. The result shows that, our MC-CNN achieves great performance improvement compared with the single coarse and single fine</figDesc><graphic coords="7,347.54,248.93,179.93,134.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 10 :</head><label>10</label><figDesc>Fig. 10: Comparisons between the original AlexNet and the MC-CNN equipped AlexNet. The first column shows the input images. The boxes with the same color are the feature maps extracted from the original and spatial pyramid equipped AlexNets. 'A' means AlexNet feature, while 'M' means MC-CNN feature.We visualize the feature maps, including shallow features such as 'CONV1', 'CONV2' and high level features such as 'CONV4' and 'CONV5'. The MC-CNN network can activate a larger perception scope for high-level recognition tasks, so that the high level features could focus more on the thyroid nodules related regions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 11 :</head><label>11</label><figDesc>Fig. 11: ROC of the 7 classifiers with the AlexNet features extracted from fc7.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>FPFig. 12 :</head><label>12</label><figDesc>Fig. 12: Illustration of the failed recognition cases produced by the original SSD [25]. It mis-classifies the small thyroid nodules (&lt; 0.5cm) (blue boxes) and large (&gt; 5cm) thyroid nodules (red boxes).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>FPFig. 13 :</head><label>13</label><figDesc>Fig. 13: Illustration of the improvement benefiting from our method. It shows some of the successful cases which are failed in the coarse recognition network E c . The nodule proposals are fed into our spatial pyramid network. Meanwhile, the classification accuracy improves greatly. The green boxes are the improved cases in the coarse network, while the yellow are the hard cases which are also mis-classified by our coarse-tofine framework.</figDesc><graphic coords="8,97.38,307.20,154.22,115.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>Dataset analysis.</figDesc><table><row><cell>Large</cell><cell>Middle</cell><cell>Small</cell><cell>Old</cell><cell>Middle</cell><cell>Young</cell></row><row><cell>(&gt;3cm)</cell><cell>(1cm-3cm)</cell><cell>(0cm-1cm)</cell><cell cols="2">(49-78) (38-49)</cell><cell>(18-38)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III</head><label>III</label><figDesc></figDesc><table><row><cell cols="4">: Comparisons among coarse and fine classification</cell></row><row><cell cols="4">networks. 'Pool5' and 'Conv5' indicate to add spatial pyramid</cell></row><row><cell cols="4">module after this layer. 'Coarse' means the classification</cell></row><row><cell cols="4">results are directly obtained from the detection stage. 'Fine'</cell></row><row><cell cols="4">means the doctors' hand-cropped candidates are fed into our</cell></row><row><cell cols="4">spatial pyramid module furnished network (mean ± standard</cell></row><row><cell>variance).</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Accuracy</cell><cell>Sensitivity</cell><cell>Specificity</cell></row><row><cell>Coarse: Detection</cell><cell cols="3">0.903 ± 0.05 0.892 ± 0.04 0.920 ± 0.04</cell></row><row><cell>Fine: AlexNet</cell><cell cols="3">0.926 ± 0.04 0.908 ± 0.05 0.981 ± 0.01</cell></row><row><cell>Fine: Pool5</cell><cell></cell><cell></cell><cell></cell></row></table><note><p>0.919 ± 0.05 0.894 ± 0.05 0.983 ± 0.01 Fine: GoogLenet 0.889 ± 0.08 0.845 ± 0.06 0.976 ± 0.02 Fine: Conv5 0.930 ± 0.04 0.925 ± 0.02 0.981 ± 0.01 MC-CNN 0.982 ± 0.01 0.983 ± 0.01 0.980 ± 0.01</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV :</head><label>IV</label><figDesc>User study based performance evaluations. (mean)</figDesc><table><row><cell>Method</cell><cell>Accuracy</cell><cell>Sensitivity</cell><cell>Specificity</cell><cell cols="2">Time(s) AUC</cell></row><row><cell>Human</cell><cell>0.87</cell><cell>0.86</cell><cell>0.91</cell><cell>12.00</cell><cell>0.88</cell></row><row><cell cols="2">MC-CNN 0.98</cell><cell>0.98</cell><cell>0.98</cell><cell>0.25</cell><cell>0.98</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V :</head><label>V</label><figDesc>User study based performance evaluations over different datasets. (mean over 5-fold cross validation)</figDesc><table><row><cell>Method</cell><cell>Accuracy</cell><cell>Sensitivity</cell><cell>Specificity</cell><cell>Dataset</cell></row><row><cell>Doctor</cell><cell>0.897</cell><cell>0.955</cell><cell>0.87</cell><cell>Age: Old</cell></row><row><cell>MC-CNN</cell><cell>0.985</cell><cell>0.971</cell><cell>0.982</cell><cell>Age: Old</cell></row><row><cell>Doctor</cell><cell>0.96</cell><cell>0.941</cell><cell>0.977</cell><cell>Age: Middle</cell></row><row><cell>MC-CNN</cell><cell>0.963</cell><cell>0.992</cell><cell>0.930</cell><cell>Age: Middle</cell></row><row><cell>Doctor</cell><cell>0.816</cell><cell>0.799</cell><cell>0.852</cell><cell>Age: Young</cell></row><row><cell>MC-CNN</cell><cell>0.962</cell><cell>0.942</cell><cell>0.973</cell><cell>Age: Young</cell></row><row><cell>Doctor</cell><cell>0.841</cell><cell>0.85</cell><cell>0.831</cell><cell>Size: Large</cell></row><row><cell>MC-CNN</cell><cell>0.946</cell><cell>0.942</cell><cell>0.955</cell><cell>Size: Large</cell></row><row><cell>Doctor</cell><cell>0.913</cell><cell>0.918</cell><cell>0.908</cell><cell>Size: Middle</cell></row><row><cell>MC-CNN</cell><cell>0.986</cell><cell>0.992</cell><cell>0.979</cell><cell>Size: Middle</cell></row><row><cell>Doctor</cell><cell>0.891</cell><cell>0.806</cell><cell>0.982</cell><cell>Size: Small</cell></row><row><cell>MC-CNN</cell><cell>0.962</cell><cell>0.972</cell><cell>0.938</cell><cell>Size: Small</cell></row><row><cell cols="5">the doctors. Table V documents the performances in different</cell></row><row><cell cols="5">stages and can handle thyroid nodules with different sizes.</cell></row><row><cell cols="5">The nodules from old patients and the large-sized nodules are</cell></row><row><cell cols="4">improved most, ranging from 9.1% to 12.5%.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VI :</head><label>VI</label><figDesc>Performance evaluation on public dataset.</figDesc><table><row><cell>Method</cell><cell>Accuracy</cell><cell>Sensitivity</cell><cell>Specificity</cell></row><row><cell>MC-CNN</cell><cell>0.921</cell><cell>0.941</cell><cell>0.962</cell></row><row><cell>Naive Bayesian</cell><cell>0.737</cell><cell>0.631</cell><cell>0.746</cell></row><row><cell>GBDT [16]</cell><cell>0.717</cell><cell>0.478</cell><cell>0.863</cell></row><row><cell>MLP</cell><cell>0.737</cell><cell>0.529</cell><cell>0.818</cell></row><row><cell>AlexNet [21]</cell><cell>0.784</cell><cell>0.625</cell><cell>0.854</cell></row><row><cell>GoogLenet [22]</cell><cell>0.750</cell><cell>0.586</cell><cell>0.847</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>JOURNAL OF L A T E X CLASS FILES, VOL. 14, NO. 8, AUGUST 2015</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This research is supported in part by NSFC of China (NO. 61672077 and 61532002), Applied Basic Research Program of Qingdao (NO. 161013xx) , National Science Foundation of USA (NO. IIS-0949467, IIS-1047715, IIS-1715985, and IIS-1049448), and capital health research and development of special 2016-1-4011.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A birth cohort analysis of the incidence of papillary thyroid cancer in the united states</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Kilfoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Thyroid</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1061" to="1066" />
			<date type="published" when="1973">1973-2004. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cascade convolutional neural networks for automatic detection of thyroid nodules in ultrasound images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Physics</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1678" to="1691" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Advances in ultrasound for the diagnosis and management of thyroid cancer</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Sipos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Thyroid</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1363" to="1372" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Thyroid cancer: Esmo clinical practice guidelines for diagnosis, treatment and follow-up</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pacini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Castagna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Brilli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pentheroudakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">G W</forename><surname>Group</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of oncology</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>suppl 5, pp. v214-v219</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">New guidelines for patients with thyroid nodules and differentiated thyroid cancer</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Larsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Clinical Practice Endocrinology &amp; Metabolism</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="297" to="298" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Increasing incidence of thyroid cancer in the united states</title>
		<author>
			<persName><forename type="first">L</forename><surname>Davies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">G</forename><surname>Welch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Jama</title>
		<imprint>
			<biblScope unit="volume">295</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="2164" to="2167" />
			<date type="published" when="1973">1973-2002. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An ultrasonogram reporting system for thyroid nodules stratifying cancer risk for clinical management</title>
		<author>
			<persName><forename type="first">E</forename><surname>Horvath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Majlis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Franco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Niedmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dominguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Clinical Endocrinology &amp; Metabolism</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1748" to="1751" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A proposal for a thyroid imaging reporting and data system for ultrasound features of thyroid carcinoma</title>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Thyroid</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1257" to="1264" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Thyroid imaging reporting and data system for us features of nodules: a step in establishing better stratification of cancer risk</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">K</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E.-K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiology</title>
		<imprint>
			<biblScope unit="volume">260</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="892" to="899" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A review on ultrasound-based thyroid cancer tissue characterization and automated classification</title>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">R</forename><surname>Acharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Swapna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V</forename><surname>Sree</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Molinari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Bardales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Witkowska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Suri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technology in cancer research &amp; treatment</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="289" to="301" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Application of support-vectormachine-based method for feature selection and classification of thyroid nodules in ultrasound images</title>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-F</forename><surname>Tsai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3494" to="3506" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Ultra sonogram images for thyroid segmentation and texture classification in diagnosis of malignant (cancerous) or benign (non-cancerous) nodules</title>
		<author>
			<persName><forename type="first">N</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jindal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Eng. Innov. Technol</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="202" to="206" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Thyroid nodule recognition based on feature selection and pixel classification methods</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bibicu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Moraru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Biswas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of digital imaging</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="119" to="128" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Morphological and wavelet features towards sonographic thyroid nodules evaluation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tsantis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dimitropoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cavouras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Nikiforidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computerized Medical Imaging and Graphics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="99" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Quantitative measurement for thyroid cancer characterization based on elastography</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Ultrasound in Medicine</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1259" to="1266" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Stochastic gradient boosted distributed decision trees</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM conference on Information and knowledge management</title>
		<meeting>the 18th ACM conference on Information and knowledge management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="2061" to="2064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Classification and regression by randomforest</title>
		<author>
			<persName><forename type="first">A</forename><surname>Liaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wiener</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">R news</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="18" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Thyroid lesion classification in 242 patient population using gabor transform features from high resolution ultrasound images</title>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">R</forename><surname>Acharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chowriappa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fujita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E W</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">W J</forename><surname>Eugene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kongmebhol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page" from="235" to="245" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Rea time ultrasound elastography in the differential diagnosis of benign and malignant thyroid nodules</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Ultrasound in Medicine</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="861" to="867" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fusion of spatial gray level dependency and fractal texture features for the characterization of thyroid lesions</title>
		<author>
			<persName><forename type="first">U</forename><surname>Raghavendra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">R</forename><surname>Acharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gudigar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fujita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hagiwara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Molinari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kongmebhol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ultrasonics</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="110" to="120" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international</title>
		<meeting>the IEEE international</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Object detectors emerge in deep scene cnns</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6856</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2169" to="2178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="346" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">An open access thyroid ultrasound image database</title>
		<author>
			<persName><forename type="first">L</forename><surname>Pedraza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Vargas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Narvaez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Duran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Munoz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Romero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tenth International Symposium on Medical Information Processing and Analysis. International Society for Optics and Photonics</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">870</biblScope>
			<biblScope unit="page" from="92" to="870W" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
