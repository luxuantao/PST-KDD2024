<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Data Streams: Algorithms and Applications *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">S</forename><surname>Muthukrishnan</surname></persName>
						</author>
						<title level="a" type="main">Data Streams: Algorithms and Applications *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">03261B1A1AFB36E493A326452F5BC26C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>I will discuss the emerging area of algorithms for processing data streams and associated applications, as an applied algorithms research agenda. That has its benefits: we can be inspired by any application to study novel problems, and yet be not discouraged by the confines of a particular one. The discussion will be idiosyncratic. My personal agenda is to be a scientist, mathematician and engineer, all in one. That will be reflected, some times one more than the others. Art, Nature, Homeland Security and other elements will make a cameo. The tagline is IMAGINE, THINK and DO: one imagines possibilities and asks questions, one seeks provable solutions and finally, one builds solutions. This writeup will present a little of each in data streaming. (See Barry Mazur's book for the imaginary and Math <ref type="bibr" target="#b64">[65]</ref>.) The area has many open problems.</p><p>Let me begin with two puzzles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Puzzle 1: Finding Missing Numbers</head><p>Let π be a permutation of {1, . . . , n}. Further, let π -1 be π with one element missing. Paul shows Carole elements from set π -1 [i] in increasing order i, one after other. Carole's task is to determine the missing integer. This is trivial to do if Carole can memorize all the numbers she has seen thus far (formally, she has an n-bit vector), but if n is large, this is impractical. Let us assume she has only a few-say O(log n)-bits of memory. Nevertheless, Carole must determine the missing integer. This starter has a simple solution: Carole stores</p><formula xml:id="formula_0">s = n(n + 1) 2 - j≤i π -1 [j],</formula><p>which is the missing integer in the end. Each input integer entails one subtraction. The total number of bits stored is no more than 2 log n. On the other hand, Carole needs at least log n bits in the worst case. (In fact, Carole has an optimal algorithm. Say n is a power of 2 for convenience. For each i, store the parity sum of the ith bits of all numbers seen thus far. The final parity sum bits are the bits of the missing number.) Similar solution will work even if n is unknown, for example by letting n = max j≤i π -1 [j] each time.</p><p>Paul and Carole have a history. It started with the "twenty questions" problem solved in <ref type="bibr" target="#b24">[25]</ref>. Paul, which stood for Paul Erdos, was the one who asked questions. Carole is an anagram for Oracle. Aptly, she was the one who answered questions. Joel Spencer and Peter Winkler used Paul and Carole to coincide with Pusher and Chooser respectively in studying certain chip games in which Carole chose which groups the chips falls into and Paul determined which group of chips to push. Joel introduced them to me during my thesis work. I used them in a game in which Paul put coins on weighing pans (panned them!) <ref type="bibr" target="#b5">[6]</ref>. In the puzzle above, Paul permutes and Carole cumulates. In a little while, they will play other P/C roles.</p><p>Generalizing the puzzle a little further, let π -2 be π with two elements missing. Most of the students in my graduate Algorithms class suggested Carole now store s = n(n+1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2</head><p>-j≤i π -2 [j] and p = n! -j≤i π -2 [j], giving two equations with two unknown numbers, but Carole can use far fewer bits tracking</p><formula xml:id="formula_1">s = n(n + 1) 2 - j≤i π -2 [j] &amp; ss = n(n + 1)(2n + 1) 6 - j≤i (π -2 [j]) 2</formula><p>In general, what is the smallest number of bits needed to identify the k missing numbers in π -k ? Following the approach above, the problem may be thought of as having power sums</p><formula xml:id="formula_2">s p (x 1 , . . . , x k ) = i=1•••k (x i ) p ,</formula><p>for p = 1, . . . , k and solving for x i 's. A different but provably equivalent method uses elementary symmetric polynomials. The ith such polynomial σ i (x 1 , . . . , x k ) is the sum of all possible i term products of the parameters, i.e., σ i (x 1 , . . . , x k ) =</p><formula xml:id="formula_3">j 1 &lt;...&lt;j i x j 1 • • • x j i .</formula><p>Carole continuously maintain σ i 's for the missing k items in field F q for prime q ≥ n (and ≤ 2n suffices), as Paul presents the numbers one after the other (the details are omitted). Since i=1,...,k</p><formula xml:id="formula_4">(z -x i ) = k i=0</formula><p>(-1) i σ i (x 1 , . . . , x k )z k-i , Carole needs to factor this polynomial in F q to determine the missing numbers. No deterministic algorithms are known for this problem, but randomized algorithms take roughly O(k 2 log n) bits and time <ref type="bibr" target="#b22">[23]</ref>. The power sum method is what colleagues typically propose over dinner. The elementary symmetric polynomial approach comes from <ref type="bibr" target="#b23">[24]</ref> where the authors solve the set reconciliation problem in the communication complexity model. The subset reconciliation problem is related to our puzzle. Readers may have guessed that they may be a different efficient solution for this puzzle using insights from error correcting codes or combinatorial group testing. Indeed true. We will later reference an O(k log k log n) bits and time solution; in contrast, no algorithm can use o(k log(n/k)) bits in the worst case.</p><p>It is no coincidence that this puzzle contains elements of data stream algorithms. Generalize it: Paul presents a multiset of elements 1, • • • , n with a single missing integer, i.e., he is allowed to re-present integers he showed before; Paul presents updates showing which integers to insert and which to delete, and Carole's task is to find the integers that are no longer present; etc. All of these problems are no longer (mere) puzzles; they are derived from motivating data stream applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Puzzle 2: Fishing</head><p>Doubtless it will be a more inspiring introduction to data streams if the puzzle was derived from nature. So, say Paul goes fishing. There are many different fish species U = {1, • • • , u}. Paul catches one fish at a time, a t ∈ U being the fish species he catches at time t. c t [j] = |{a i |a i = j, i ≤ t}| is the number of times he catches the species j up to time t. Species j is rare at time t if it appears precisely once in his catch up to time t. The rarity ρ[t] of his catch at time t is the ratio of the number of rare j's to u:</p><formula xml:id="formula_5">ρ[t] = |{ j | c t [j] = 1 }| u .</formula><p>Paul can calculate ρ[t] precisely with a 2U -bit vector and a counter for the current number of rare species, updating the data structure in O(1) operations per fish caught. However, Paul wants to store only as many bits as will fit his tiny suitcase, i.e., o(U ), preferably O(1) bits. Suppose Paul has a deterministic algorithm to compute ρ[t] precisely. Feed Paul any set S ⊂ U of fish species, and say Paul's algorithm stores only o(|S|) bits in his suitcase. Now we can check if any i ∈ S by simply feeding Paul i and checking ρ[t + 1]: the number of rare items decreases by one if and only if i ∈ S. This way we can recover entire S from his suitcase by feeding different i's one at a time, which is impossible in general if Paul had only stored o(|S|) bits. Therefore, if Paul wishes to work out of his one suitcase, he can not compute ρ[t] exactly. This argument has elements of lower bound proofs found in the area of data streams.</p><p>However, proceeding to the task at hand, Paul can approximate ρ[t]. Paul picks k random fish species each independently, randomly with probability 1/u at the beginning and maintains the number of times each of these fish types appear in his bounty, as he catches fish one after another. Say X 1 [t], . . . , X k [t] are these counts after time t. Paul outputs ρ</p><formula xml:id="formula_6">[t] = |{ X i [t] | X i [t]=1 } k as an estimator for ρ. Since Pr(X i [t] = 1) = |{j | c t [j] = 1} u = ρ[t]</formula><p>, we have</p><formula xml:id="formula_7">Pr(ρ[t] ∈ [ρ[t] -ǫ, ρ[t] + ǫ]) = i∈[k(ρ[t]-ǫ),k(ρ[t]+ǫ)] k i (ρ[t]) i (1 -ρ[t]) k-i .</formula><p>If ρ[t] is large, say at least 1/k, ρ[t] is a good estimator for ρ[t] with arbitrarily small ǫ and significant probability.</p><p>As an exercise in doing mathematics while fishing, this misses an ingredient: ρ is unlikely to be large because presumably u is much larger than the species found at any spot Paul fishes. Choosing a random species from 1..u and waiting for it to be caught seems an exercise in, well, fishing. We can make it more realistic by redefining rarity wrt the species Paul in fact sees in his catch. Let</p><formula xml:id="formula_8">γ[t] = |{ j | c t [j] = 1 }| |{ j | c t [j] = 0 }| .</formula><p>As before, Paul would have to approximate γ[t] because he can not compute it exactly using small number of bits. Following <ref type="bibr" target="#b87">[88]</ref>, define a family of hash functions H ⊂ [n] → [n] (where [n] = {1, . . . , n}) to be min-wise independent if for any X ⊂ [n] and x ∈ X, we have</p><formula xml:id="formula_9">Pr h∈H [h(x) = min h(X)] = 1 |X|</formula><p>Paul chooses k min-wise independent hash functions h 1 , h 2 , . . . , h k for some parameter k to be determined later and maintains h * i (t) = min a j , j≤t h i (a j ) at each time t, that is, min hash value of the multi-set {. . . , a t-2 , a t-1 , a t }. He also maintain k counters C 1 (t), C 2 (t), . . . , C k (t); C i (t) counts the number of times the item with hash value h * i (t) appears in {. . . , a t-2 , a t-1 , a t }. It is trivial to maintain both h * i (t) and C i (t) as t progresses and new items are seen. Let</p><formula xml:id="formula_10">γ[t] = |{ i | 1 ≤ i ≤ k, C i (t) = 1 }| k .</formula><p>Notice that Pr(C i (t) = 1) is the probability that h i (t) is the hash value of one of the items that appeared precisely once in a 1 , ..., a t which equals</p><formula xml:id="formula_11">|{ j | c[j]=1 }| |{ j | c[j] =0 }| = γ[t]. Hence, γ[t] is a good estimator for γ[t] provided γ[t]</formula><p>is large, say at least 1/k. That completes the sketch of Paul's algorithm.</p><p>It is ironic that Paul has adapted the cumulating task in the solution above, which is traditionally Carole's shtick. But we are not done. Paul needs to pick h i 's. If Paul resorts to his tendency to permute, i.e., picks a randomly chosen permutation π over [u] = {1, . . . , u}, then h i 's will be min-wise hash functions. And he will be done. However, it requires Θ(u log u) bits to represent a random permutation from the set of all permutations over <ref type="bibr">[u]</ref>. Thus the number of bits needed to store the hash function will not fit his suitcase!</p><p>To overcome this problem, Paul has to do some math. He picks a family of approximate min-hash functions. A family of hash functions,</p><formula xml:id="formula_12">H ⊂ [n] → [n] is called ǫ-min-wise independent if for any X ⊂ [n] and x ∈ X, we have Pr h∈H [h(x) = min h(X)] = 1 |X| (1 ± ǫ).</formula><p>Indyk <ref type="bibr" target="#b17">[18]</ref> presents a family-set of polynomials over GF (u) of degree O(log(1/ǫ))-of ǫ-min-wise independent hash functions such that any function from this family can be represented using O(log u log( (As an aside, the problem of estimating the rarity is related to a different problem. Consider fishing again and think of it as a random sampling process. There is an unknown probability distribution P on the countable set of fish types with p t being the probability associated with fish type t. A catch is a sample S of f fishes drawn independently from fish types according to the distribution P . Let c[t] be the number of times t appears in S and s[k] be the number of fish types that appear k times in S. Consider estimating the probability of fish type t being the next catch. Elementary reasoning would indicate that this probability is c[t]s[c[t]]/f . However, it is unlikely that all (of the large number of) fish types in the ocean are seen in Paul's catch, or even impossible if fish types is infinite. Hence, there are fish types t that do not appear in the sample (i.e., c[t] = 0) and they would have probability 0 of being caught next, a conundrum in the elementary reasoning if t is present in the ocean. Let m = t ∈S p t . The problem of estimating m is called the missing mass problem. In a classical work by Good (attributed to Turing too) <ref type="bibr" target="#b34">[35]</ref>, it is shown that m is estimated by s[1]/f , provably with small bias; recall that our rarity γ is closely related to s[1]/f . Hence, our result here on estimating rarity in data streams is of independent interest in the context of estimating the missing mass. Those interested in convergence properties of the Good-Turing estimator should see David McAllester's work.)</p><p>Once you generalize the fishing-letting the numerator be more generally |{ j | c t [j] ≤ α }| for some α, letting Carole go fishing too, or letting Paul and Carole throw fish back into the sea as needed-there are some real data streaming applications <ref type="bibr" target="#b18">[19]</ref>.</p><p>Honestly, the fishing motif is silly: the total number of fish species in the sea is estimated to be roughly 22000 and anyone can afford an array of as many bits. In the reality of data streams which I will describe next, one is confronted with fishing in a far more numerous domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Lessons</head><p>I have noticed that once something is called a puzzle, people look upon the discussion less than seriously. The puzzle in Section 1.1 shows the case of a data stream problem that can be deterministically solved precisely with O(log n) bits (when k = 1, 2 etc.). Such algoritms-deterministic and exact-are uncommon in data stream processing. In contrast, the puzzle in Section 1.2 is solved only up to an approximation using a randomized algorithm in polylog bits. This-randomized and approximate solutions-is more representative of currently known data stream algorithms. Further, the estimation of γ in Section 1.2 is accurate only when it is large; for small γ, the estimate γ is arbitrarily bad. This points to a feature that generally underlies data stream algorithmics. Such features which applied algorithmicists need to keep in mind while formulating problems to address data stream issues will be discussed in more detail later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Map</head><p>Section 3 will describe the data stream phenomenon. I have deliberately avoided specific models here because the phenomenon is real, models are the means and may change over time. Section 4 will present currently popular data stream models, motivating scenarios and other applications for algorithms in these models beyond dealing with data streams.</p><p>Section 5 abstracts mathematical ideas, algorithmic techniques as well as lower bound approaches for data stream models; together they comprise the foundation of the theory of data streams that is emerging. This section is right now skimpy, and I will add to it over time. Section 6 discusses applied work on data streams. It is drawn from different systems areas, and I have grouped them into three categories which may be useful for a perspective.</p><p>Section 7 contains new directions and open problems that arise in several research areas when the data streaming perspective is applied. Some traditional areas get enriched, new ones emerge. Finally, in my concluding remarks in Section 8, I will invoke Proust, show you streaming Art, history, and some notes on the future of streaming. The most important part of this writeup is Section 9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data Stream Phenomenon</head><p>The web site http://www.its.bldrdoc.gov/projects/devglossary/ data stream.html defines a data stream to be a "sequence of digitally encoded signals used to represent information in transmission". We will be a little more specific. Data stream to me represents input data that comes at a very high rate. High rate means it stresses communication and computing infrastructure, so it may be hard to</p><p>• transmit (T) the entire input to the program,</p><p>• compute (C) sophisticated functions on large pieces of the input at the rate it is presented, and</p><p>• store (S), capture temporarily or archive all of it long term.</p><p>Most people typically do not think of this level of stress in TCS capacity. They view data as being stored in files. When transmitted, if links are slow or communication is erroneous, we may have delays but correct data eventually gets to where it should go. If computing power is limited or the program has high complexity, it takes long (long longs!) time to get the desired response, but in principle, we would get it. Also, we save almost all the data we need. This simplified picture of TCS requirements is reasonable because need has balanced resources: we have produced the amount of data that technology could ship, process, store, or we have the patience to manipulate.</p><p>There are two recent developments that have confluenced to produce new challenges to the TCS infrastructure.</p><p>• Ability to generate automatic, highly detailed data feeds comprising continuous updates. This ability has been built up over the past few decades beginning with networks that spanned banking and credit transactions. Other dedicated network systems now provide massive data streams: satellite based, high resolution measurement of earth geodetics <ref type="bibr">[118,</ref><ref type="bibr">113]</ref>, radar derived meteorological data [119]<ref type="foot" target="#foot_0">1</ref> , continuous large scale astronomical surveys in optical, infrared and radio wavelengths [117], atmospheric radiation measurements [108] etc. The Internet is a general purpose network system that has distributed both the data sources as well as the data consumers over millions of users. It has scaled up the rate of transactions tremendously generating multiple streams: browser clicks, user queries, IP traffic logs, email data and traffic logs, web server and peer-to-peer downloads etc. Internet also makes it to easier to deploy special purpose, continuous observation points that get aggregated into vast data streams: for example, financial data comprising individual stock, bond, securities and currency trades can now get accumulated from multiple sources over the internet into massive streams. Wireless access networks are now in the threshold of scaling this phenomenon even more. In particular, the emerging vision of sensor networks combines orders of more observation points (sensors) than are now available with wireless and networking technology and is posited to challenge TCS needs even more. Oceanographic, bio, seismic and security sensors are such emerging examples.</p><p>• Need to do sophisticated analyses of update streams in near-real time manner.</p><p>With traditional datafeeds, one modifies the underlying data to reflect the updates, and real time queries are fairly simple such as looking up a value. This is true for the banking and credit transactions. More complex analyses such as trend analysis, forecasting, etc. are typically performed offline in warehouses. However, the automatic data feeds that generate modern data streams arise out of monitoring applications, be they atmospheric, astronomical, networking, financial or sensor-related. They need to detect outliers, extreme events, fraud, intrusion, unusual or anomalous activity, etc., monitor complex correlations, track trends, support exploratory analyses and perform complex tasks such as classification, harmonic analysis etc. These are time critical tasks in each of these applications, more so in emerging applications for homeland security, and they need to be done in near-real time to accurately keep pace with the rate of stream updates and accurately reflect rapidly changing trends in the data.</p><p>These two factors uniquely challenge the TCS needs. We in Computer Science community have traditionally focused on scaling wrt to size: how to efficiently manipulate large disk-bound data via suitable data structures <ref type="bibr" target="#b14">[15]</ref>, how to scale to databases of petabytes <ref type="bibr" target="#b105">[106]</ref>, synthesize massive datasets <ref type="bibr" target="#b6">[7]</ref>, etc. However, far less attention has been given to benchmarking, studying performance of systems under rapid updates with near-real time analyses. Even benchmarks of database transactions [115] are inadequate.</p><p>There are ways to build workable systems around these TCS challenges. TCS systems are sophisticated and have developed high-level principles that still apply. Make things parallel. A lot of data stream processing is highly parallelizable in computing (C) and storage (S) capacity; it is somewhat harder to parallelize transmission (T) capacity on demand. Control data rate by sampling or shedding updates. High energy particle physics experiments at Fermilab and CERN [120] will soon produce 40TBytes/s which will be reduced by real time hardware into 800Gb/s data stream: is it careful sampling or carefree shedding? Statisticians have the sampling theory: it is now getting applied to IP network streams <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b12">13]</ref>. Round data structures to certain block boundaries. For example, the "Community of Interest" approach to finding fraud in telephone calls uses a graph data structure up to and including the previous day to perform the current day's analysis, thereby rounding the freshness of the analysis to period of a day <ref type="bibr" target="#b13">[14]</ref>. This is an effective way to control the rate of real-time data processing and use pipelining. Use hierarchically detailed analysis. Use fast but simple filtering or aggregation at the lowest level and slower but more sophisticated computation at higher levels with smaller data. This hierarchical thinking stacks up against memory hierarchy nicely. Finally, often asking imaginative questions can lead to effective solutions within any given resource constraint, as applied algorithms researchers well know.</p><p>Nevertheless, these natural approaches are ultimately limiting. They may meet ones' myopic expectations. But we need to understand the full potential of data streams for the future. Given a certain amount of resources, a data stream rate and a particular analysis task, what can (not) we do? Most natural approaches to dealing with data streams discussed above involves approximations: what are algorithmic principles for data stream approximation? One needs a systematic theory of data streams. To get novel algorithms. To build data stream applications with ease, and proven performance.</p><p>What follows is an introduction to the emerging theory of data streams. Before that, here is a note. The previous few paragraphs presented a case for data stream research. I could have done it</p><formula xml:id="formula_13">• Using anecdotics.</formula><p>Paul, now a network service Provider, has to convince Carole, his Customer, that IP hosts connecting to her website get high quality real time media service. He needs to monitor IP traffic to her web site and understand per-packet performance for each host. Plotting such statistics in real time is a good way to convince Carole.</p><p>• Or using numerics.</p><p>A single OC48 link may transmit few hundred GBytes per hour of packet header information, which is more than 200Mbps. It takes an OC3 link to transfer this streaming log and it is a challenge to write it into tapes or process it by the new 3GHz P4 Intel processor at that rate.</p><p>Or I could have used a limerick, haiku or a Socratic dialog. But I chose to describe data stream as a phenomenon in words. Sometimes I think words have become less meaningful to us than greek symbols or numerals. Nevertheless, I hope you would use your imagination and intuit the implications of data streaming. Imagine we can (and intend to) collect so much data that we may be forced to drop a large portion of it, or even if we could store it all, we may not have the time to scan it before making judgements. That is a new kind of uncertainty in computing beyond randomization and approximation: it should jar us, one way or the other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Data Streaming: Formal Aspects</head><p>This section will be more formal: define various models for dealing with data streams and present a motivating application to internalize.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data Stream Models</head><p>Input stream a 1 , a 2 , . . . arrives sequentially, item by item, and describes an underlying signal A, a onedimensional function A : [1 . . . N ] → R.<ref type="foot" target="#foot_1">2</ref> Models differ on how a i 's describe A.</p><p>• Time Series Model. Each a i equals A[i] and they appear in increasing order of i. This is a suitable model for time series data where, for example, you are observing the traffic at an IP link each 5 minutes, or NASDAQ volume of trades each minute, etc.</p><p>• Cash Register Model. Here a i 's are increments to A[j]'s. Think of a i = (j, I i ), I i ≥ 0, to mean A i [j] = A i-1 [j] + I i where A i is the state of the signal after seeing the ith item in the stream. Much as in a cash register, multiple a i 's could increment a given A[j] over time. This is perhaps the most popular data stream model. It fits applications such as monitoring IP addresses that access a web server, source IP addresses that send packets over a link etc. because the same IP addresses may access the web server multiple times or send multiple packets on the link over time. This model has appeared in literature before, but was formally christened in <ref type="bibr" target="#b26">[27]</ref> with this name.</p><p>• Turnstile Model. <ref type="foot" target="#foot_2">3</ref> Here a i 's are updates to A[j]'s. Think of a i = (j, U i ), to mean A i [j] = A i-1 [j] + U i where A i is the signal after seeing the ith item in the stream, and U i may be positive or negative. This is the most general model. It is mildly inspired by a busy NY subway train station where the turnstile keeps track of people arriving and departing continuously. At any time, a large number of people are in the subway. This is the appropriate model to study fully dynamic situations where there are inserts as well deletes, but it is often hard to get interesting bounds in this model. This model too has appeared before under different guises, but it gets christened here with its name.</p><p>There is a small detail: in some cases, A i [j] ≥ 0 for all i. We refer to this as the strict Turnstile model. Intuitively this corresponds to letting people only exit via the turnstile they entered the system in: it is a unrealistic intuition, but it fits many applications. For example, in a database, you can only delete a record you inserted. On the other hand, there are instances when streams may be non-strict, that is, A i [j] &lt; 0 for some i. For example, when one considers a signal over the difference between two cash register streams, one obtains a non-strict Turnstile model. We will avoid making a distinction between the two Turnstile models unless necessary.</p><p>The models in decreasing order of generality are as follows: Turnstile, Cash Register, Time Series. (A more conventional description of models appears in <ref type="bibr" target="#b26">[27]</ref>.) From a theoretical point of view, of course one wishes to design algorithms in the Turnstile model, but from a practical point of view, one of the other models, though weaker, may be more suitable for an application. Furthermore, it may be (provably) hard to design algorithms in a general model, and one may have to settle for algorithms in a weaker model.</p><p>We wish to compute various functions on the signal A at different times during the stream. There are different performance measures.</p><p>• Processing time per item a i in the stream. (Proc. Time)</p><p>• Space used to store the data structure on A t at time t. (Storage)</p><p>• Time needed to compute functions on A. (Compute time) <ref type="foot" target="#foot_3">4</ref>Here is a rephrasing of our solutions to the two puzzles at the start in terms of data stream models and performance measures.</p><formula xml:id="formula_14">Puzzle Model Function Proc. Time Storage Compute Time Section 1.1 cash register k = 1, {j|A[j] = 0} O(log n) O(log n) O(1) Section 1.2 cash register γ[t] O(k log(1/ǫ)) O(k log u log(1/ǫ)) O(k)</formula><p>We can now state the ultimate desiderata that is generally accepted:</p><p>At any time t in the data stream, we would like the per-item processing time, storage as well as the computing time to be simultaneously o(N, t), preferably, polylog(N, t).</p><p>Readers can get a sense for the technical challenge this desiderata sets forth by contrasting it with a traditional dynamic data structure like say a balanced search tree which processes each update in O(log N ) time and supports query in O(log N ) time, but uses linear space to store the input data. Data stream algorithms can be similarly thought of as maintaining a dynamic data structure, but restricted to use sublinear storage space and the implications that come with it. Sometimes, the desiderata is weakened so that:</p><p>At any time t in the data stream, per-item processing time and storage need to be simultaneously o(N, t) (preferably, polylog(N, t)), but the computing time may be larger.</p><p>This was proposed in <ref type="bibr" target="#b9">[10]</ref>, used in few papers, and applies in cases where computing is done less frequently than the update rate. Still, the domain N and input t being so large that they warrant using only polylog(N, t) storage may in fact mean that computing time even linear in the domain or input may be prohibitive in applications for a particular query.</p><p>A comment or two about the desiderata. First, why do we restrict ourselves to only a small (sublinear) amount of space? Typically, one says this is because the data stream is so massive that we may not be able to store all of what we see. That argument is facetious. Even if the data stream is massive, if it describes a compact signal (i.e., N is small), we can afford space linear in N , and solve problems within our conventional computing framework. For example, if we see a massive stream of peoples' IDs and their age in years, and all we wish to calculate were functions on peoples' age distribution, the signal is over N less than 150, which is trivial to manage. What makes data streams unique is that there are applications where data streams describe signals over a very large universe. For example, N may be the number of source, destination IP address pairs (which is potentially 2 64 now), or may be the number of time intervals where certain observations were made (which increases rapidly over time), or may be the http addresses on the web (which is potentially infinite since web queries get sometimes written into http headers). More generally, and this is significantly more convincing, data streams are observations over multiple attributes and any subset of attributes may comprise the domain of the signal in an application and that leads to potentially large domain spaces even if individual attribute domains are small.</p><p>Second, why do we use the polylog function? Well, log in the input size is the lower bound on the number of bits needed to index and represent the signal, and poly gives us a familiar room to play.</p><p>Finally, there is a cognitive analogy that explains the desiderata qualitatively, and may appeal to some of the readers (it did, to Mikkel Thorup). As human beings, we perceive each instant of our life through an array of sensory observations (visual, aural, nervous, etc). However, over the course of our life, we manage to abstract and store only part of the observations, and function adequately even if we can not recall every detail of each instant of our lives. We are data stream processing machines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">A Motivating Scenario</head><p>Let me present a popular scenario for data streaming. The Internet comprises routers connected to each other that forward IP packets. Managing such networks needs real time understanding of faults, usage patterns, and unusual activities in progress. This needs analysis of traffic and fault data in real time. Consider traffic data. Traffic at the routers may be seen at many levels.</p><p>1. At the finest level, we have the packet log: each IP packet has a header that contains source and destination IP addresses, ports, etc.</p><p>2. At a higher level of aggregation, we have the flow log: each flow is a collection of packets with same values for certain key attributes such as the source and destination IP addresses and the log contains cumulative information about number of bytes and packets sent, start time, end time, protocol type, etc. per flow.</p><p>3. At the highest level, we have the SNMP log, which is the aggregate data of the number of bytes sent over each link every few minutes.</p><p>Many other logs can be generated from IP networks (fault alarms, CPU usage at routers, etc), but the examples above suffice for our discussion. You can collect and store SNMP data. (I store 6 months worth of this data of a large ISP in my laptop for the IPSOFACTO tool I will reference later. I could store data up to a year or two without stressing my laptop.) The arguments we presented for data streaming apply to flow and packet logs which are far more voluminous than the SNMP log. A more detailed description and defense of streaming data analysis in IP network traffic data is presented in <ref type="bibr" target="#b25">[26]</ref>, in particular, in Section 2.</p><p>Here are some queries one may want to ask on IP traffic logs.</p><p>1. How much HTTP traffic went on a link today from a given range of IP addresses? This is an example of a slice and dice query on the multidimensional time series of the flow traffic log.</p><p>2. How many distinct IP addresses used a given link to send their traffic from the beginning of the day, or how many distinct IP addresses are currently using a given link on ongoing flow?</p><p>3. What are the top k heaviest flows during the day, or currently in progress? Solution to this problem in flow logs indirectly provides a solution to the puzzle in Section 1.1.</p><p>4. How many flows comprised one packet only (i.e., rare flows)? Closely related to this is the question: Find TCP/IP SYN packets without matching SYNACK packets. This query is motivated by the need to detect denial-of-service attacks on networks as early as possible. This problem is one of the motivations for the fishing exercise in Section 1.2.</p><p>5. How much of the traffic yesterday in two routers was common or similar? This is a distributed query that helps track the routing and usage in the network. A number of notions of "common" or "similar" apply. The IPSOFACTO system supports such similarity queries on the SNMP logs for an operational network provider <ref type="bibr" target="#b56">[57]</ref>.</p><p>6. What are the top k correlated link pairs in a day, for a given correlation measure. In general a number of correlation measures may be suitable. Those that rely on signal analysis-wavelet, fourier etc.-of the traffic pattern prove effective. We will later describe algorithms for computing wavelet or fourier representation for data streams.</p><p>7. For each source IP address and each five minute interval, count the number of bytes and number of packets related to HTTP transfers. This is an interesting query: how do we represent the output which is also a stream and what is a suitable approximation in this case?</p><p>The questions above are simple slice-and-dice or aggregate or group by queries. This is but a sparse sample of interesting questions. Imagine the setup, and you will discover many more relevant questions. Some of the more complex queries will involve joins between multiple data stream sources. For example, how to correlate Let me formalize one of the examples above in more detail, say example two. First, how many distinct IP addresses used a given link to send their traffic since the beginning of the day? Say we monitor the packet log. Then the input stream a 1 , a 2 , . . . is a sequence of IP packets on the given link, with packet a i having the source IP address s i . Let A[0 . . . N -1] be the number of packets sent by source IP address i, for 0 ≤ i ≤ N -1, initialized to all zero at the beginning of the day. (This signal is more general for reasons that will be clear in the next paragraph.) Each packet a i adds one to A[s i ]; hence, the model is Cash Register. Counting the number of distinct IP addresses that used the link during the day thus far can be solved by determining the number of nonzero A[i]'s at any time.</p><p>Second, now, consider how many distinct IP addresses are currently using a given link? More formally, at any time t, we are focused on IP addresses s i such that some flow f j began at time before t and will end after t, and it originates at s i . In the packet log, there is information to identify the first as well as the last packets of a flow. (This is an idealism; in reality, it is sometimes to hard to tell when a flow has ended.) Now, let A[0 . . . N -1] be the number of flows that source IP address i is currently involved in, for 0 ≤ i ≤ N -1, initialized to all zero at the beginning of the day. If packet a i is the beginning of a flow, add one to A[s j ] if s j is the source of packet a i ; if it is the end of the flow, subtract one from A[s j ] if s j is the source of packet a i ; else, do nothing. Thus the model is Turnstile. Counting the number of distinct IP addresses that are currently using a link can be solved by determining the number of nonzero A[i]'s at any time.</p><p>Similarly other examples above can be formalized in terms of the data stream models and suitable functions to be computed.</p><p>A note: There has been some frenzy lately about collecting and analyzing IP traffic data in the data stream context. Analyzing traffic data is not new. In telephone and cellular networks, call detail records (CDRs) are routinely collected for billing purposes, and they are analyzed for sizing, forecasting, troubleshooting, and network operations. My own experience is with cellular CDRs and there is a lot you can do to discover engineering problems in a network in near-real time with the live feed of CDRs from the cellular network. The angst with IP traffic is that the data is far more voluminous, and billing is not usage based. The reason to invest in measurement and analysis infrastructure is mainly for network maintenance and value-added services. So, the case for making this investment has to be strong, and it is now being made across the communities and service providers. Both Sprint [109] and AT&amp;T <ref type="bibr" target="#b29">[30]</ref> seem to be engaged on this topic. That presents the possibility of getting suitable data stream sources in the future, at least within these companies.</p><p>At this point, I am going to continue the theme of being imaginative, and suggest a mental exercise. Consider a data streaming scenario from Section 3 that is different from the IP traffic log case. For example,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Exercise 1 Consider multiple satellites continuously gathering multiple terrestial, atmospheric and oceansurface observations of the entire earth. What data analysis questions arise with these spatial data streams?</head><p>This is a good homework exercise if you are teaching a course. The queries that arise are likely to be substantially different from the ones listed above for the IP traffic logs case. In particular, problems naturally arise in the area of Computational Geometry. Wealth of (useful, fundamental) research remains to be done.</p><p>Those who want a mental exercise more related to the Computer Science concepts can consider the streaming text scenario <ref type="bibr">[110]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Exercise 2 We have distributed servers each of which processes a stream of text files (instant messages, emails, faxes, say) sent between users. What are interesting data analysis queries on such text data streams?</head><p>For example, one may now look for currently popular topics of conversation in a subpopulation. This involves text processing on data streams which is quite different from the IP traffic logs or the satellite-based terrestial or stellar observation scenarios.</p><p>We need to develop the two examples above in great detail, much as we have done with the IP traffic analysis scenario earlier. We are far from converging on the basic characteristics of data streams or a building block of queries that span different application scenarios. As Martin Strauss quips, hope this is not a case of "insurmountable opportunities".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Other Applications for Data Stream Models</head><p>The data stream models are suitable for other applications besides managing rapid, automatic data feeds. In particular, they find applications in the following two scenarios (one each for cash register and Turnstile models).</p><p>One pass, Sequential I/O. Besides the explicit data stream feeds we have discussed thus far, there are implicit streams that arise as an artifact of dealing with massive data. It is expensive to organize and access sophisticated data structures on massive data. Programs prefer to process them in one (or few) scans. This naturally maps to the (Time Series or Cash Register) data stream model of seeing data incrementally as a sequence of updates. Disk, bus and tape transfer rates are fast, so one sees rapid updates (inserts) when making a pass over the data. Thus the data stream model applies.</p><p>Focus on one (or few) pass computing is not new. Automata theory studied the power of one versus two way heads. Sorting tape-bound data relied on making few passes. Now we are seeking to do more sophisticated computations, with far faster updates. <ref type="foot" target="#foot_4">5</ref>This application differs from the data streaming phenomenon we saw earlier in a number of ways. First, in data streaming, the analysis is driven by monitoring applications and that determines what functions you want to compute on the stream. Here, you may have in mind to compute any common function (transitive closure, eigenvalues, etc) and want to do it on massive data in one or more passes. Second, programming systems that support scans often have other supporting infrastructure such as a significant amount of fast memory etc. which may not exist in IP routers or satellites that generate data streams. Hence the one pass algorithms may have more flexibility. Finally, the rate at which data is consumed can be controlled and smoothed, data can be processed in chunks etc. In contrast, some of the data streams are more phenomenadriven, and can potentially have higher and more variable update rates. So, the data stream model applies to one pass algorithms, but some of the specific concerns are different.</p><p>Monitoring database contents. Consider a large database undergoing transactions: inserts/deletes and queries. In many cases, we would like to monitor the database contents. As an example, consider selectivity estimation. Databases need fast estimates of result sizes for simple queries in order to determine an efficient query plan for complex queries. The estimates for simple queries have to be generated fast, without running the queries on the entire database which will be expensive. This is the selectivity estimation problem. Here is how it maps into the data stream scenario. The inserts or deletes in the database are the updates in the (Turnstile model of a data) stream, and the signal is the database. The selectivity estimation query is the function to be computed on the signal. Data sream algorithms therefore have the desirable property that they represent the signal (i.e., the database) in small space and the results are obtained without looking at the database, in time and space significantly smaller than the database size and scanning time. Thus, data stream algorithms in the Turnstile model naturally find use as algorithms for selectivity estimation.</p><p>Other reasons to monitor database contents are approximate query answering and data quality monitoring, two rich areas in their own right with extensive literature and work. They will not be discussed further, mea culpa. Again data stream algorithms find direct applications in these areas.</p><p>Readers should not dismiss the application of monitoring database content as thinly disguised data streaming. This application is motivated even if updates proceed at a slow rate; it relies only on small space and fast compute time aspect of data stream algorithms to avoid rescanning the database for quick monitoring.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Foundations</head><p>The main mathematical and algorithmic techniques used in data stream models are collected here, so the discussion below is technique-driven rather than problem-driven. It is sketchy at this point with pointers to papers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Basic Mathematical Ideas</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Sampling</head><p>Many different sampling methods have been proposed: domain sampling, universe sampling, reservoir sampling, distinct sampling etc. Sampling algorithms are known for:</p><p>• Find the number of distinct items in a Cash Register data stream. See <ref type="bibr" target="#b84">[85]</ref>.</p><p>• Finding the quantiles on a Cash Register data stream. See <ref type="bibr" target="#b82">[83]</ref> for most recent results.</p><p>• Finding frequent items in a Cash Register data stream. See <ref type="bibr" target="#b83">[84]</ref>.</p><p>Each of these problems has nice applications (and many other results besides the ones we have cited above). Further, it is quite practical to implement sampling even on high speed streams. (In fact, some of the systems that monitor data streams-specially IP packet sniffers-end up sampling the stream just to slow the rate down to a reasonable level, but this should be done in a principled manner, else, valuable signals may be lost.) Also, keeping a sample helps one estimate many different statistics, and additionally, actually helps one return certain sample answers to non-aggregate queries. Consider:</p><p>Problem 3 Say we have data streams over two observed variables (x t , y t ). An example correlated aggregate is {g(y</p><formula xml:id="formula_15">t ) | x t ≤ f (x 1 • • • x t )},</formula><p>that is, computing some aggregate function g-SUM, MAX, MIN-on those y t 's when the corresponding x t 's satisfy certain relationship f . For what f 's and g's (by sampling or otherwise) can such queries be approximated on data streams? See <ref type="bibr" target="#b86">[87]</ref> for the motivation.</p><p>There are two main difficulties with sampling for data stream problems. First, sampling is not a powerful primitive for many problems. One needs far too many samples for performing sophisticated analyses. See <ref type="bibr" target="#b85">[86]</ref> for some lower bounds. Second, sampling method typically does not work in the Turnstile data stream model: as stream unfolds, if the samples maintained by the algorithm get deleted, one may be forced to resample from the past, which is in general, expensive or impossible in practice and in any case, not allowed in data stream models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Random Projections</head><p>This approach relies on dimensionality reduction, using projection along random vectors. The random vectors are generated by space-efficient computation of random variables. These projections are called the sketches. This approach typically works in the Turnstile model and is therefore quite general.</p><p>Building on the influential work <ref type="bibr" target="#b0">[1]</ref>, Indyk <ref type="bibr" target="#b88">[89]</ref> proposed using stable distributions to generate the random variables. Sketches with different stable distributions are useful for estimating various L p norms on the data stream. In particular, sketches using Gaussian random variables get a good estimate of the L 2 norm of data streams, using Cauchy distributions one gets a good estimate for the L 1 norm etc. I have not found a lot of motivation for computing the L 1 or L 2 norm of a data stream by itself, although these methods prove useful for computing other functions. For example,</p><p>• Using L p sketches for p → 0, we can estimate the number of distinct elements at any time in the Turnstile data stream model <ref type="bibr" target="#b89">[90]</ref>.</p><p>• Using variants of L 1 sketches, we can estimate the quantiles at any time in the Turnstile data stream model <ref type="bibr" target="#b90">[91]</ref>.</p><p>• Using variants of L 1 sketches and other algorithmic techniques, we can dynamically track most frequent items <ref type="bibr" target="#b91">[92]</ref>, wavelets and histograms <ref type="bibr" target="#b92">[93]</ref>, etc. in the Turnstile data stream model.</p><p>• Using L 2 sketches, one can estimate the self-join size of database relations <ref type="bibr" target="#b96">[97]</ref>. This is related to estimating inner product of vectors, which is provably hard to do in general, but can be estimated to high precision if the inner product is large.</p><p>There are many variations of random projections which are of simpler ilk. For example, Random subset sums <ref type="bibr" target="#b26">[27]</ref>, counting sketches <ref type="bibr" target="#b27">[28]</ref> and also Bloom filters <ref type="bibr" target="#b28">[29]</ref>. A detailed discussion of the connection between them is needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Problem 4 Design random projections using complex random variables or other generalizations, and find suitable streaming applications.</head><p>There are instances where considering random projections with complex numbers or their generalization have been useful. For example, let A be a 0, 1 matrix and B be obtained from A by replacing each 1 uniformly randomly with ±1. Then E[(det(B)) 2 ] = per(A) where det(A) is the determinant of matrix A and per(A) is the permanent of matrix A. While det(A) can be calculated in polynomial time, per(A) is difficult to compute or estimate. The observation above presents a method to estimate per(A) in polynomial time using det(A), but this procedure has high variance. However, if C is obtained from A by replacing each 1 uniformly randomly by ±1, ±i, then E[(det(C)) 2 ] = per(A) still, and additionally, the variance falls significantly. By extending this approach using quaternion and clifford algebras, a lot of progress has been made on decreasing the variance further, and deriving an effective estimation procedure for the permanent <ref type="bibr" target="#b57">[58]</ref>.</p><p>A powerful concept in generating a sequence of random variables each drawn from a stable distribution is doing so with the property that any given range of them can be summed fast, on demand. Such constructions exist, and in fact, they can be generated fast and using small space. Number of constructions are now known: preliminary ones in <ref type="bibr" target="#b93">[94]</ref>, Reed-Muller construction in <ref type="bibr" target="#b26">[27]</ref>, general construction in <ref type="bibr" target="#b92">[93]</ref> with L 1 and L 2 sketches, and approach in <ref type="bibr" target="#b94">[95]</ref> for stable distributions with p → 0. They work in the Turnstile model and find many applications including histogram computation and graph algorithms on data streams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Basic Algorithmic Techniques</head><p>There are a number of basic algorithmic techniques: binary search, greedy technique, dynamic programming, divide and conquer etc. that directly apply in the data stream context, mostly in conjunction with samples or random projections. Here are a few other algorithmic techniques that have proved powerful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Group Testing</head><p>This goes back to an earlier Paul and Carole game. Paul has an integer I between 1 and n in mind. Carole has to determine the number by asking "Is I ≤ x?". Carole determines various x's, and Paul answers them truthfully. How many questions does Carole need, in the worst case? There is an entire area of Combinatorial Group Testing that produces solutions for such problems. In the data stream case, each question is a group of items and the algorithm plays the role of Carole. This set up applies to a number of problems in data streams. (It may also be thought of as coding and decoding in small space.) Examples are found in:</p><p>• Finding B most frequent items in Turnstile data streams <ref type="bibr" target="#b91">[92]</ref>.</p><p>• Determining the highest B Haar wavelet coefficients in Turnstile data streams <ref type="bibr" target="#b92">[93]</ref>.</p><p>• Estimating top B fourier coefficients by sampling <ref type="bibr" target="#b95">[96]</ref>.</p><p>Problem 5 Paul sees data stream representing A P and Carole sees data stream representing A C , both on domain 1, . . . , N . Design a streaming algorithm to determine certain number of i's with the largest</p><formula xml:id="formula_16">A P [i] max 1,A C [i] .</formula><p>Monika Henzinger and Jennifer Rexford posed this problem to me at various times. It has a strong intuitive appeal: compare today's data with yesterday's and find the ones that changed the most. Certain relative norms similar to this problem are provably hard <ref type="bibr" target="#b94">[95]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Tree Method</head><p>This method applies nicely to the Time Series model. Here, we have a (typically balanced) tree atop the data stream. As the updates come in, the leaves of the tree are revealed in the left to right order. In particular, the path from the root to the most currently seen item is the right boundary of the tree we have seen. We maintain small space data structure on the portion of the tree seen thus far, typically, some storage per level; this can be updated as the leaves get revealed by updating along the right boundary. This overall algorithmic scheme finds many applications:</p><p>• Computing the B largest Haar wavelet coefficients of a Time Series data stream <ref type="bibr" target="#b96">[97]</ref>.</p><p>• Building a histogram on the Time Series data stream <ref type="bibr" target="#b97">[98]</ref>. This also has applications to finding certain outliers called the deviants <ref type="bibr" target="#b104">[105]</ref>.</p><p>• Building a parse tree atop the Time Series data stream seen as a string <ref type="bibr" target="#b80">[81]</ref>. This has applications to estimating string edit distances as well as estimating size of the smallest grammar to encode the string.</p><p>Here is a problem of similar ilk, but it needs new ideas.</p><p>Problem 6 Given a signal of size N as a Time Series data stream and parameters B and k, the goal is to find k points (deviants) to remove so that finding the B largest coefficients for the remaining signal has smallest sum squared error. This is the wavelet version of the problem studied in <ref type="bibr" target="#b98">[99]</ref>.</p><p>There are other applications, where the tree hierarchy is imposed as an artifact of the problem solving approach. The k-means algorithm on the data stream <ref type="bibr" target="#b99">[100]</ref> can be seen as a tree method: building clusters on points, building higher level clusters on their representatives, and so on up the tree.</p><p>Finally, I will speculate that Yair Bartal's fundamental result of embedding arbitrary metrics into tree metrics will find applications in data streams context, by reducing difficult problems to ones where the tree method can be applied effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Robust Approximation</head><p>This concept is a variation on the local minima/maxima, but suited for approximations. Consider constructing a near-optimal B bucket histogram for the signal. An approximation H to the optimal histogram H * is called robust if it has the property that when refined further with a few buckets, the resulting histogram is only a little better than H itself as an approximation to H * . This is a powerful concept for constructing histograms: to get a B bucket optimal histogram, we first pull out a poly(B, log N ) bucket histogram that is robust and then cull a B bucket histogram from it appropriately which is provably 1 + ǫ approximate. The details are in <ref type="bibr" target="#b92">[93]</ref>. We suspect that robust approximations will find many applications.</p><p>In a recent result <ref type="bibr" target="#b100">[101]</ref> on an improved algorithm for the k-median problem on data streams, in the first phase, a O(k polylog(n)) facility solution is obtain from which the algorithm culls the k facilities which is provably 1 + ǫ accurate. This is reminiscent of robust approximation, but there is a technical distinction: the O(k polylog(n)) facility solution does not seem to have the robustness property.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.4">Exponential Histograms</head><p>To algorithms designers, it is natural to think of exponential histograms-dividing a line into regions with boundaries at distance 2 i from one end or keep dividers after points of rank 2 i -when one is restricted to use a polylog space data structure. This technique has been used in one dimensional nearest neighbor problems and facility location <ref type="bibr" target="#b45">[46]</ref>, maintaining statistics within a window <ref type="bibr" target="#b35">[36]</ref>, and from a certain perspective, for estimating the number of distinct items <ref type="bibr" target="#b33">[34]</ref>. It is a simple and natural strategy which is likely to get used seamlessly in data stream algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Lower Bounds</head><p>A powerful theory of lower bounds is emerging for data stream models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Compressibility argument.</head><p>In Section 1.2 there is an example. One argues that if we have already seen a portion S of the data stream and have stored a data structure D on S for solving a problem P , then we can design the subsequent portion of the stream such that solving P on the whole stream will help us recover S precisely from D. Since not every S can be compressed into small space D, the lower bound on size of D will follow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Communication Complexity.</head><p>Communication complexity models have been used to establish lower bounds for data stream problems. In particular, see <ref type="bibr" target="#b0">[1]</ref>. Estimating set disjointness is a classical, hard problem in Communication Complexity that underlies the difficulty in estimating some of the basic statistics on data streams. See <ref type="bibr" target="#b101">[102]</ref> for a few different communication models in distributed stream settings.</p><p>• Reduction.</p><p>One reduces problems to known hard ones. Several such results are known. See <ref type="bibr" target="#b94">[95]</ref> for some examples.</p><p>An information-based approach to data stream lower bounds is in <ref type="bibr" target="#b102">[103]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Summary and Data Stream Principles</head><p>The algorithmic ideas above have proved powerful for solving a variety of problems in data streams. On the other hand, using the lower bound technology, it follows that many of these problems-finding most frequent items, finding small error histograms, clustering, etc.-have versions that are provably hard to solve exactly or even to approximate on data streams. However, what makes us successful in solving these problems is that in real life, there are two main principles that seem to hold in data stream applications.</p><p>• Signals in real life have "few good terms" property.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Real life signals</head><formula xml:id="formula_17">A[0 • • • N -1]</formula><p>have a small number B of coefficients that capture most of the trends in A even if N is large. For example, Figure <ref type="figure" target="#fig_0">1</ref> shows the sum squared error in reconstructing a distribution with B highest coefficients for various values of B, and two different distributions: the number of bytes of traffic involving an IP address as the source and as the destination. So, here N = 2 32 , total number of IP addresses possible. Still, with B = 800 or so, we get the error to drop by more than 50%. For capturing major trends in the IP traffic, few hundred coefficients prove adequate.</p><p>In IP traffic, few flows send large fraction of the traffic <ref type="bibr" target="#b2">[3]</ref>. That is, of the 2 64 possible (src, dest) IP flows, if one is interested in heavy hitters, one is usually focused on a small number (few hundreds?) of flows. This means that one is typically interested in tracking k most frequent items, for small k,</p><formula xml:id="formula_18">even if N is large in A.</formula><p>This phenomenon arises in other problems as well: one is typically interested in small number k of facilities or clusters, etc.</p><p>• During unusual events in data streams, exceptions are significantly large.</p><p>The number of rare flows-flows involving a small number of packets-and the number of distinct flows is significantly large during network attacks.</p><p>When there are data quality problems in data streams-say an interface is polled more often than expected-the problem is abundant, eg., the number of polls may be far more than expected.</p><p>These two principles are used implicitly in designing many of the useful data stream algorithms. Applied algorithmicists need to keep these principles in mind while abstracting the appropriate problems to study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Streaming Systems</head><p>There are systems that (will) process data streams. I think of them in three categories.</p><p>First, is the hands-on systems approach to data streams. One uses operating system support to capture streams, and perhaps use special hooks in standard programming languages like C to get additional facility in manipulating streams. The work at AT&amp;T Research on Call Detail Records analysis falls in this category; Hancock <ref type="bibr" target="#b66">[67]</ref> is a special-purpose language. Researchers like Andrew Moore <ref type="bibr">[124]</ref> work in this framework, and quite successfully process large data sets. Ultimately however, I do not know of such work that processes data at the stream rate generated by IP network routers.</p><p>Second, there are systems that let a high performance database process updates using standard technology like bulk loading, or fast transaction support. Then one builds applications atop the database. IPSO-FACTO <ref type="bibr" target="#b56">[57]</ref> is such a system that lets Daytona database handle SNMP log updates and provides application level support for visualizing and correlating traffic patterns on links between IP routers. This works well for SNMP logs and is used on production quality datafeed, but will be highly stressed for packet or flow logs. Bellman <ref type="bibr" target="#b65">[66]</ref> which monitors data quality problems in databases takes this approach as well, capturing transactions from a generic database and performing statistical analysis on relationships between attributes in various database tables. It needs further work to scale to large transaction rates.</p><p>Finally, there are database systems where the internals are directly modified to deal with data streams. This is an active research area that involves new stream operators, SQL extensions, novel optimization techniques, scheduling methods, the continuous query paradigm, etc., the entire suite of developments needed for a data stream management system (DSMS). Projects at various universities of this type include Nia-garaCQ <ref type="bibr" target="#b69">[70]</ref>, Aurora <ref type="bibr" target="#b71">[72]</ref>, Telegraph <ref type="bibr" target="#b72">[73]</ref>, Stanford Stream <ref type="bibr" target="#b70">[71]</ref> etc. They seem to be under development, and demos are being made at conferences (see SIGMOD 2003). Another system I know of in this category is Gigascope <ref type="bibr" target="#b73">[74]</ref> which is operationally deployed in an IP network. It does deal with stream rates generated in IP networks, but at this point, it provides only features geared towards IP network data analysis. It is not yet suitable for general purpose data stream management for a variety of data streams.</p><p>One of the outstanding questions with designing and building DSMSs is whether there is a need. One needs multiple applications, a well-defined notion of stream common to these applications, and powerful operators useful across applications in order to justify the effort needed to actualize DSMSs. At this fledgling point, the IP network traffic monitoring is a somewhat well developed application. But more work needs to be done-in applications like text and spatial data stream scenarios-to bolster the need for DSMSs.</p><p>To what extent have the algorithmic ideas been incorporated into the emerging streaming systems? Both Bellman and IPSOFACTO use some of the approximation algorithms including sampling and random projections. Most of the systems in the third category have hooks for sampling. There is discussion of testing random projection based estimations using Gigascope, and reason to believe that simple, random projections technique will be useful in other systems too.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">New Directions</head><p>This section presents some results and areas that did not get included above. The discussion will reveal open directions and problems: these are not polished gems, they are uncut ideas. Sampath Kannan has an interesting talk on open problems in streaming <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Related Areas</head><p>In spirit and techniques, data streaming area seems related to the following areas.</p><p>• PAC learning: In <ref type="bibr" target="#b46">[47]</ref> authors studied sampling algorithms for clustering in the context of PAC learning. More detailed comparison needs to be done for other problems such as learning fourier or wavelet spectrum of distributions between streaming solutions and PAC learning methods.</p><p>• Online algorithms: Data stream algorithms have an online component where input is revealed in steps, but they have resource constraints that are not typically incorporated in competitive analysis of online algorithms.</p><p>• Property testing: This area focuses typically on sublinear time algorithms for testing objects and separating them based on whether they are far from having a desired property, or not. Check out Ronitt Rubinfeld's talk on what can be done in sublinear time <ref type="bibr">[123]</ref>. Typically these results focus on sampling and processing only sublinear amount of data. As mentioned earlier, sampling algorithms can be simulated by streaming algorithms, and one can do more in streaming models.</p><p>• Markov methods. Some data streams may be thought of as intermixed states of multiple markov chains. Thus we have to reason about maximum likelihood separation of the markov chains <ref type="bibr" target="#b48">[49]</ref>, reasoning about individual chains <ref type="bibr" target="#b47">[48]</ref>, etc. under resource constraints of data streams. This outlook needs to be developed a lot further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Functional Approximation Theory</head><p>One of the central problems of modern mathematical approximation theory is to approximate functions concisely, with elements from a large candidate set D called a dictionary; D = {φ i } i∈I of unit vectors that span R N . Our input is a signal A ∈ R N . A representation R of B terms for input A ∈ R N is a linear combination of dictionary elements, R = i∈Λ α i φ i , for φ i ∈ D and some Λ, |Λ| ≤ B. Typically, B ≪ N , so that R is a concise approximation to signal A. The error of the representation indicates by how well it approximates A, and is given by</p><formula xml:id="formula_19">A -R 2 = t |A[t] -R[t]| 2 .</formula><p>The problem is to find the best B-term representation, i.e., find a R that minimizes A -R 2 . I will only focus on the L 2 error here. A signal has a R with error zero if B = N since D spans R N .</p><p>Many of us are familiar with a special case of this problem if the dictionary is a Fourier basis, i.e., φ i 's are appropriate trigonometric functions. Haar wavelets comprise another special case. Both these special cases are examples of orthonormal dictionaries: ||φ i || = 1 and φ i ⊥ φ j . In this case, |D| = N . For orthonormal dictionaries when B = N , Parseval's theorem holds:</p><formula xml:id="formula_20">i A[i] 2 = i α 2 i .</formula><p>For B &lt; N , Parseval's theorem implies that the best B term representation comprises the B largest inner product | A, φ | over φ ∈ D.</p><p>In functional approximation theory, we are interested in larger-redundant-dictionaries, so called because when D &gt; N , input signals have two or more distinct zero-error representation using D. Different applications have different natural dictionaries that best represent the class of input signals they generate. There is a tradeoff between the dictionary size and the quality of representation when dictionary is properly chosen. Choosing appropriate dictionary for an application is an art. So, the problem of determining an approximate representation for signals needs to be studied with different redundant dictionaries.</p><p>There are two directions: studying specific dictionaries derived from applications or studying the problem for an arbitrary dictionary so as to be completely general.</p><p>Studying specific dictionaries. One specific dictionary of interest is Wavelet Packets. Let W 0 (x) = 1 for 0 ≤ x &lt; 1. Define W 1 , W 2 , . . . as follows.</p><formula xml:id="formula_21">W 2n (x) = W n (2x) -W n (2x -1)</formula><p>error estimates significantly. I think this is a powerful framework, and efficient algorithms for other problems in Functional Approximation Theory will use this framework in the future. Recently, Ingrid Daubechies spoke some of these results at the AMS-MAA joint meetings <ref type="bibr" target="#b68">[69]</ref>.</p><p>Functional approximation theory has in general focused on characterizing the class of functions for which error has a certain decay as N → ∞. See <ref type="bibr" target="#b61">[62]</ref> and <ref type="bibr" target="#b60">[61]</ref> for many such problems. But from an algorithmicists point of view, the nature of problems I discussed above are more clearly more appealing. This is a wonderful area for new algorithmic research; a starting recipe is to study <ref type="bibr" target="#b61">[62]</ref> and <ref type="bibr" target="#b60">[61]</ref>, formulate algorithmic problems, and to solve them.</p><p>Let me propose two further, out-there directions: Can we design new wavelets based on general two dimensional tiling (current wavelet definitions rely on rather restricted set of two dimensional tiling)? Can we design new wavelets based on the 2 -3 tree decomposition a la ESP in <ref type="bibr" target="#b80">[81]</ref>? In both cases, this gives vectors in the dictionary defined over intervals that are not just dyadic as in Haar wavelets. Exploring the directions means finding if there are classes of functions that are represented compactly using these dictionaries, and how to efficiently find such representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Data Structures</head><p>Many of us have heard of the puzzle that leaves Paul at some position in a singly linked list, and he needs to determine if the list has a loop. He can only remember O(log n) bits, where n is the number of items in the list. The puzzle is a small space exploration of a list, and has been generalized to arbitrary graphs even <ref type="bibr" target="#b21">[22]</ref>. One of the solutions relies on leaving a "finger" behind, doing 2 i step exploration from the finger each i, i = 1, . . .; the finger is advanced after each iteration. This puzzle has the flavor of finger search trees where the goal is to leave certain auxiliary pointers in the data structure so that a search for an item helps the subsequent search. Finger search trees is a rich area of research. Richard Cole's work on dynamic finger conjecture for splay trees is an example of deep problems to be found <ref type="bibr" target="#b20">[21]</ref>.</p><p>Recently, a nice result has appeared <ref type="bibr" target="#b19">[20]</ref>. The authors construct O(log n) space finger structure for an n node balanced search tree which can be maintained under insertions and deletions; searching for item of rank j after an item of rank i only takes O(log |j -i|) time. (Modulo some exceptions, most finger search data structures prior to this work needed Ω(n)) bits.) I think of this as a streaming result. I believe and hope this result will generate more insights into streaming data structures. In particular, two immediate directions are to extend these results to external memory or to geometric data structures such as segment trees, with appropriate formalization, of course.</p><p>Let me present a specific data structural traversal problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Problem 11</head><p>We have a graph G = (V, E) and a memory M , initially empty. Vertices have to be explicitly loaded into memory M ; at most k vertices may reside in M at any time. We say an edge (v i , v j ) ∈ E is evaluated when both v i and v j are in the memory M at the same time. What is the minimum number of loads needed to evaluate all the edges of the graph G?</p><p>For k = 2, a caterpillar graph can be loaded optimally easily. For k = 3, Fan Chung pointed out that the dual of the graph obtained by looking at triangles of G may have certain structure for it to be loaded optimally. I think this problem arises in query optimization for tertiary databases from Sunita Sarawagi's thesis work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Computational Geometry</head><p>Computational Geometry is a rich area. Problems in computational geometry arise because there are applications-earth observations for example-that naturally generate spatial data streams and spatial queries. Also, they arise implicitly in modeling other real life scenarios. For example, flows in IP networks may be thought of intervals [state time, end time], text documents get mapped to high dimensional vector spaces, etc.</p><p>Consider the problem of estimating the diameter of points presented on a data stream. Two results are interesting.</p><p>Indyk considers this problem in the cash register model where points in d dimensions arrive over time. His algorithm uses O(dn 1/(c 2 -1) ) space and compute time per item and produces c-approximation to the diameter, for c &gt; √ 2. The algorithm is natural. Choose l random vectors v 1 , . . . , v l and for each v i , maintain the two points with largest and smallest v i p over all point p's. For sufficiently large l, computing diameter amongst these points will give a c-approximation.</p><p>For d = 2, a better algorithm is known. Take any point in the stream as the center and draw sectors centered on that point of appropriate angular width. Within each sector, we can keep the farthest point from the center. Then diameter can be estimated from the arcs given by these points. One gets an ǫ-approximation to the diameter with O(1/ǫ) space and O(log(1/ǫ)) compute time per inserted point <ref type="bibr" target="#b44">[45]</ref>.</p><p>I know of other results in progress, so more computational geometry problems will get solved in the data stream model in the near future.</p><p>Let me add a couple of notes. First, in small dimensional applications like d = 2 or 3, keeping certain radial histograms, i.e., histograms that emanate in sectors from centers and use bucketing within sectors, will find many applications. This needs to be explored. Second, I do not know of many nontrivial results for the computational geometry problems in the Turnstile model. To understand the challenge, consider points on a line being inserted and deleted, all insertions and deletions coming only at the right end (the minimum point is fixed). Maintaining the diameter reduces to maintaining the maximum value of the points which is impossible with o(n) space when points may be arbitrarily scattered. Instead, let me say the points are in the range 1 • • • R: then, using O(log R) space, we can approximate the maximum to 1 + ǫ factor. This may be an approach we want to adopt in general, i.e., have a bounding box around the objects and using resources polylog in the area of the bounding box (or in terms of the ratio of min to the max projections of points along suitable set of lines). This problem arises in a study of sensors on highways <ref type="bibr" target="#b45">[46]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">Graph Theory</head><p>The graph connectivity problem plays an important role in log space complexity. See <ref type="bibr" target="#b40">[41]</ref> for some details. However, hardly any graph problem has been studied in the data stream model where (poly)log space requirement comes with other constraints.</p><p>In <ref type="bibr" target="#b41">[42]</ref>, authors studied the problem of counting the number of triangles in the cash register model. Graph G = (V, E) is presented as a series of edges (u, v) ∈ E in no particular order. The problem is to estimate the number of triples (u, v, w) with an edge between each pair of vertices. Let T i , = 0, 1, 2, 3, be the number of triples with i total edges between the pairs of vertices. Consider the signal A over the triples (u, v, w), u, v, w ∈ V , where A[(u, v, w)] is the number of edges in the triple (u, v, w).</p><formula xml:id="formula_22">Let F i = (u,v,w) (A[(u, v, w)]) i . It is simple to observe that    F 0 F 1 F 2    =    1 1 1 1 2 3 1 4 9    .    T 0 T 1 T 2   </formula><p>Solving, T 3 = F 0 -1.5F 1 + 0.5F 2 . Now, F 1 can be computed precisely. But F 0 and F 2 can only be approximated. This needs a trick of considering the domain of the signal in a specific order so that each item in the data stream, ie., an edge, entails updating a constant number of intervals in the signal. Using appropriate rangesum variables, this can be done efficiently, so we find a use for the rangesum variables from Section 5.1. As a result T 3 can be approximated. In fact, this method works in the Turnstile model as well even though the authors in <ref type="bibr" target="#b41">[42]</ref> did not explicitly study it.</p><p>The general problem that is interesting is to count other subgraphs, say constant sized ones. Certain small subgraphs appear in web graphs intriguingly <ref type="bibr" target="#b42">[43]</ref>, and estimating their number may provide insights into the web graph structure. Web crawlers spew nodes of the web graph in data streams. So, it is a nicely motivated application.</p><p>Many graph problems need to be explored in data stream models. But they appear to be difficult in general. One has to find novel motivations and nifty variations of the basic graph problems.</p><p>Let me propose a direction. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.6">Databases</head><p>Databases research has considered streaming extensively, far too much to be summarized here. I will highlight a few interesting directions. Consider approximate query processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Problem 14 Consider a signal</head><formula xml:id="formula_23">A where A[i] is a subset of 1, • • • U . Each query is a range query [i..j] for which the response is | i≤k≤j A[k]|.</formula><p>Build a histogram of B buckets that is "optimal" for this task. First consider a static A and later streaming signals.</p><p>A lot has to be formalized in the problem above (See <ref type="bibr" target="#b78">[79]</ref> for some related details). Histograms have been studied extensively in Statistics and find many applications in commercial databases. In general they study signals where A[i] is the number of tuples in a database with value i. Instead, if we interpret A[i] as the set of pages that contain tuples with value i, histogram described in the problem above is relevant. To those with the background, I can say, this is an attempt at modeling the page selectivity of queries.</p><p>A somewhat related problem concerns multidimensional signals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Problem 15 Consider two dimensional signal</head><formula xml:id="formula_24">A t [i][j], i, j ∈ [1..n].</formula><p>Design algorithms for building (near) optimal two dimensional histogram with B partitions.</p><p>Readers should not think of this as a straightforward generalization of one dimensional problem to multiple dimensions. The problem is, but the details are quite different. There are many ways to partition two dimensional arrays. While one dimensional problem is polynomial time solvable, two dimensional problems are NP-hard for most partitions. Further, as I argued earlier, even if one-dimensional domains are small enough to fit in to given memory, streaming algorithms may well be appropriate for the multidimensional version.</p><p>In <ref type="bibr" target="#b74">[75]</ref>, authors proposed efficient approximation algorithms for a variety of two dimensional histograms for a static signal. Some preliminary results were presented in <ref type="bibr" target="#b75">[76]</ref> for the streaming case: specifically, the authors proposed a polylog space, 1+ǫ approximation algorithm using O(B log N ) partitions, taking Ω(N 2 ) time. Using the ideas in <ref type="bibr" target="#b74">[75]</ref> and robustness, I believe that a truly streaming algorithm can be obtained, i.e., one that is a B partitions, 1 + ǫ approximation using both polylog space as well as polylog time, but details will be published soon.</p><p>Both the questions above are rather technical. From a database point of view, there are many conceptual questions to be resolved: How to scale continuous queries, how to develop a notion of stream operator that is composable so complex stream queries can be expressed and managed, etc. Let me propose a direction that is likely to be interesting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Problem 16</head><p>What is an approximation for a Stream In, Stream Out (SISO) query? Can one develop a theory of rate of input stream and rate of output stream for various SISO queries? Both probabilistic and adversial rate theories are of relevance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.7">Hardware</head><p>An important question in data streaming is how to deal with the rate of updates. Ultimately, the rate of updates may be so high that it is not feasible to capture them on storage media, or to process them in software. Hardware solutions may be needed where updates, as they are generated, are fed to hardware units for per-item processing. This has been explored in the networking context for a variety of per-packet processing tasks (see eg. <ref type="bibr" target="#b4">[5]</ref>) previously, but more needs to be done. There is commercial potential in such hardware machines. Consider: Problem 17 Develop hardware implementation of the inner product based algorithms described in Section 5 for various data stream analyses.</p><p>Here is a related topic. The trend in graphics hardware is to provide a programmable pipeline. Thus, graphics hardware that will be found in computing systems may be thought of as implementing a stream processing programming model. Tasks will be accomplished in multiple passes through a highly parallel stream processing machine with certain limitations on what instructions can be performed on each item at each pixel in each pass. See <ref type="bibr" target="#b37">[38]</ref> for an example, <ref type="bibr" target="#b38">[39]</ref> for a suitable model, and <ref type="bibr" target="#b81">[82]</ref> for stream-related results. Generic graphics hardware may not be suitable for processing data streams coming in at a high rate, but stream algorithms may find applications in using graphics hardware as a computing platform for solving problems. Lot remains to be explored here; see overview <ref type="bibr" target="#b39">[40]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.8">Streaming Models</head><p>Models make or mar an area of foundational study. We have a thriving set of streaming models already, but some more are likely, and are needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.8.1">Permutation Streaming</head><p>This is a special case of the cash register model in which items do not repeat. That is, the input stream is a permutation of some set, and items may arrive in a unordered way. (This fits Paul's avocation of permuting from Section 1.1.)</p><p>A number of problems have already been studied in this model. In <ref type="bibr" target="#b36">[37]</ref>, authors studied how to estimate various permutation edit distances. The problem of estimating the number of inversions in a permutation was studied in <ref type="bibr" target="#b32">[33]</ref>. Here is an outline of a simple algorithm to estimate the number of inversions <ref type="bibr" target="#b30">[31]</ref>. Let A t is the indicator array of the seen items before seeing the tth item, and I t be the number of inversions so far. Say the tth item is i. Then</p><formula xml:id="formula_25">I t+1 = I t + |{j | j &gt; i &amp; A t [j] = 1}|.</formula><p>The authors in <ref type="bibr" target="#b30">[31]</ref> show how to estimate |{j | j &gt; i &amp; A t [j] = 1}| for any i, up to 1 + ǫ accuracy using exponentially separated quantiles. They use randomization, and an elegant idea of oversampling (and retaining certain smallest number of them) for identifying the exponentially separated quantiles. An open problem here is what is the best we can do deterministically, or in the Turnstile model.</p><p>A deeper question is whether there is a compelling motivation to study this model, or the specific problems. There is some theoretical justification: permutations are special cases of sequences and studying permutation edit distance may well shed light on the notoriously hard problem of estimating the edit distance between strings. However, I have not been able to find an overwhelming inspiration for these problems and this model. Yet, here is a related problem that does arise in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Problem 18</head><p>Each TCP flow comprises multiple consecutively numbered packets. We see the packets of the various flows in the Cash Register model. Packets get transmitted out of order because of retransmissions in presence of errors, ie., packets may repeat in the stream. Estimate the number of flows that have (significant number of) out of order packets at any time. Space used should be smaller than the number of distinct TCP flows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.8.2">Windowed Streaming</head><p>It is natural to imagine that the recent past in a data stream is more significant than distant past. How to modify the streaming models to reemphasize the data from recent past? There are currently two approaches.</p><p>First is combinatorial. Here, one specifies a window size w, and explicitly focuses only on the most recent stream of size w, i.e., at time t, only consider updates a t-w+1 , . . . , a t . Items outside this window fall out of consideration for analysis, as the window slides over time. The difficulty of course is that we can not store the entire window, only o(w), or typically only o(polylog(w)) bits are allowed. This model was proposed in <ref type="bibr" target="#b35">[36]</ref> and is natural, but it is somewhat synthetic to put a hard bound of w on the window size, for example, irrespective of the rate of arrival of the stream.</p><p>The other model is telescopic. Here, one considers the signal as fixed size blocks of size w and λ-ages the signal. Let A i represent the signal from block i. We (inductively) maintain β i as the meta-signal after seeing i blocks. When the i + 1th block is seen, we obtain</p><formula xml:id="formula_26">β i+1 = (1 -λ i+1 )β i + λ i+1 σ i+1 .</formula><p>If we unravel the inductive definition, we can see that the signal from a block affects the meta-signal exponentially less as new blocks get seen. This model has certain linear algebraic appeal, and it also leverages the notion of blocks that is inherent in many real life data streams. The original suggestion is in <ref type="bibr" target="#b31">[32]</ref> where the block amounted to a days worth of data, and λ i 's were kept mostly fixed. The drawback of this model is clearly that it is difficult to interpret the results in this model in an intuitive manner. For example, if we computed the rangesum of the metasignal</p><formula xml:id="formula_27">β i [a • • • b],</formula><p>what does the estimate mean for the data stream at any given time?</p><p>Let me propose another natural model, a hierarchical block model, described by Divesh Srivastava. Informally, we would like to analyze the signal for the current day at the granularity of a few minutes, the past week at the granularity of hours, the past month at the granularity of days, the past year at the granularity of weeks, etc. That is, there is a natural hierarchy in many of the data streams, and we can study the signals at progressively higher level of aggregation as we look back in to the past. There are very interesting research issues here, in particular, how to allocate a fixed amount of space one has amongst the different signal granularities, etc. that is being investigated now.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.8.3">Synchronized Streaming</head><p>A puzzle, due to Howard Bergerson, is as follows. Imagine the first one thousand vigintillion minus one natural numbers arranged in two lists, one in numerical order and the other in lexicographic order. How many (if any) numbers have their positions same in both lists? There is nothing special about vigintillion, any n will do.</p><p>This has a Paul-Carole North American version. Carole counts up 1, . . . , n. Paul counts too, but in permuted order given by the lexicographic position of numbers when written in English. For example, if n = 4, Carole goes 1, 2, 3, 4 but Paul goes Four,One,Three,Two. Both count in lock step. When, if ever, do they say "Jinx!"?</p><p>Answer of course depends on n, and not by a formula. See <ref type="bibr">[116]</ref> for some answers. This puzzle contains the elements of what I call the synchronized streaming model. Say we wish to compute a function on two signals A 1 and A 2 given by a data stream. All updates to both the signals are simultaneous and identical except possibly for the update values. That is, if the tth item in the data stream that specifies A 1 is (i, C 1 (i)), then the tth item in the data stream that specifies A 1 is (i, C 2 (i)), too. Both these updates are seen one after the other in the data stream. Our goal as before is to compute various functions of interest on A 1 and A 2 satisfying the desiderata of streaming algorithms.</p><p>In the synchronized streaming model, one can do whatever can be done in the generic streaming model in which one of the signals is presented before the other, or the tth updates of the two signals are arbitrarily separated. The interest is if synchronized model can accomplish more. We believe that to be the case. For example, if the two signals are two strings read left to right in synchronized streaming, one can estimate if their edit distance if at most k, using O(k) space. In contrast, this is difficult to do in a generic streaming model. Synchronized streaming is quite natural; more research is needed on this model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.9">Data Stream Quality Monitoring.</head><p>Any engineer having field experience with data sets will confirm that one of the difficult problems in reality is dealing with poor quality data. Data sets have missing values, repeated values, default values in place of legitimate ones, etc. Researchers study ways to detect such problems (data quality detection) and fixing them (data cleaning). This is a large area of research, see the book <ref type="bibr" target="#b76">[77]</ref>.</p><p>In traditional view of databases, one sets integrity constraints and any violation is considered a data quality problem and exceptions are raised. Bad quality data (eg., age of a person being more than 200) is never loaded into the database. This is a suitable paradigm for databases that are manually updated by an operator.</p><p>In emerging data streams, data quality problems are likely to be manifold. For example, in network databases, data quality problems have to do with missing polls, double polls, irregular polls, disparate traffic at two ends of a link due to unsynchronized measurements, out of order values, etc. Now it is unrealistic to set integrity constraints and stop processing a high speed data feed for each such violation; furthermore, appearance of such problems in the feed might by itself indicate an abnormal network phenomena and cleaning it off in the database may hide valuable evidence for detecting such phenomena. Developing algorithms to detect one data quality problem after another is simply not a scalable or graceful approach, one needs a different principled approach.</p><p>I am a believer in data quality monitoring tools. They operate as database applications, monitoring its state by measuring statistics: strong deviation from expected statistics may be projected as a ping for the database administrator or the user to react to. To be useful, the tool has to be configured to monitor most suitable statistics and thresholds need to be set to release suitable number of pings while suppressing false alarms. This is an engineering challenge. There are many ways the database and users may deal with these pings: writing their queries in an informed way is my choice. See <ref type="bibr" target="#b77">[78]</ref> for related discussions.</p><p>Bellman <ref type="bibr" target="#b65">[66]</ref> is such a tool for traditional database systems; it monitors the structure in the database tables using various statistics on the value distribution in the tables. PACMAN <ref type="bibr" target="#b77">[78]</ref> is another tool; it uses probabilistic, approximate constraints (PACs) to monitor SNMP data streams and works operationally for a large ISP. PACs are also a principled way to determine what are data quality problems. More needs to be done.</p><p>In general, our communities have approached data quality problems as "details" and dealt with individual problems as the need arises. (In Computational Biology for example, one deals with noisy data by redefining a particular problem.) I think there is a need to develop more principled methods-theory and systems-for dealing with poor quality data.</p><p>Here is a specific technical problem not restricted to streams. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.10">Fish-eye View</head><p>Let me do a fish-eye view of other areas where streaming problems abound. The discussion will be elliptical: if you mull over these discussions, you can formulate interesting technical open problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.10.1">Linear Algebra</head><p>Many matrix functions need to be approximated in data stream model. Let me propose a specific problem. Similar result has been proved in <ref type="bibr" target="#b50">[51]</ref> using appropriate sampling for a fixed A, and recent progress is in <ref type="bibr" target="#b49">[50]</ref> for similar problem using a few passes, but there are no results in the Turnstile Model. A lot of interesting technical issues lurk behind this problem. One may have to be innovative in seeking appropriate ||.|| and f . Other linear algebraic functions are similarly of interest: estimating eigenvalues, determinants, inverses, matrix multiplication, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Problem 20 Given a matrix</head><formula xml:id="formula_28">A[1 • • • n, 1 • • • n] in</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.10.2">Statistics</head><p>We saw how to estimate simple statistical parameters on data streams. We need vastly more sophisticated statistical analyses in data stream models, for example, kernel methods, scan statistics, kurtosis parameters, data squashing, etc., the whole works. In statistics, researchers seem to refer to "recursive computing" which resonates with the cash register model of computation. There is an inspiring article by Donoho <ref type="bibr" target="#b51">[52]</ref> which is a treasure-tove of statistical analyses of interest with massive data. Another resource is http://www.kernelmachines.org/. Any of the problems from these resources will be interesting in data stream models. Let me propose a general task:</p><p>Problem 21 Assume a model for the signal A and estimate its parameters using one of well known methods such as regression fitting or maximum likelihood estimation, etc. on the data stream.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.10.3">Complexity Theory</head><p>Complexity theory has already had a profound impact on streaming. Space-bounded pseudorandom generatorsdeveloped chiefly in the complexity theory community-play an important role in streaming algorithms. No doubt more of the tools developed for small space computations will find applications in data streaming.</p><p>In a recent lunk talk with David Karger, the question arose whether quantum phenomenon can compress computations into much smaller space than conventional computations, i.e., quantum memory is more plentiful than conventional memory.</p><p>Let me propose a question, which is likely to have been in researchers' minds; Sivakumar has some notes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Problem 22</head><p>Characterize the complexity class given by a deterministic logspace verifier with one-way access to the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.10.4">Privacy Preserving Data Mining</head><p>Peter Winkler gives an interesting talk on the result in <ref type="bibr" target="#b52">[53]</ref> which is a delightful read. Paul and Carole each have a secret name in mind, and the problem is for them to determine if their secrets are the same. If not, neither should learn the other's secret. The paper <ref type="bibr" target="#b52">[53]</ref> presents many solutions, and attempts to formalize the setting. (In particular, there are solutions that involve both Paul and Carole permuting the domain, and those that involve small space pseudorandom generators.) Yao's "two millionaires" problem <ref type="bibr" target="#b53">[54]</ref> is related in which Paul and Carole each have a secret number and the problem is to determine whose secret is larger without revealing their secrets.</p><p>These problems show the challenge in the emerging area of privacy preserving data mining. We have multiple databases (sets or multisets). Owners of these databases are willing to cooperate on a particular data mining task such as determining if they have a common secret, say for security purposes or because it is mandated. However, they are not willing to divulge their database contents in the process. This may be due to regulatory or proprietary reasons. They need privacy preserving methods for data mining. This is by now a well researched topic with positive results in very general settings <ref type="bibr" target="#b55">[56]</ref>. However, these protocols have high complexity. But there is a demand for efficient solutions, perhaps with provable approximations, in practice. In <ref type="bibr" target="#b54">[55]</ref> authors formalized the notion of approximate privacy preserving data mining and presented some solutions, using techniques similar to ones we use in data stream algorithms. Lot remains to be done.</p><p>The database community is researching general, efficient methods to make databases privacy-preserving. Let me propose a basic problem.</p><p>Problem 23 Paul has m secrets, and Carole has n secrets. Find an approximate, provably privacy-preserving protocol to find the common secrets. As before, the unique secrets of Paul or Carole should not be revealed to each other.</p><p>Other problems arise in the context of banking, medical databases or credit transactions. This gives new problems, for example, building decision trees, detecting outliers, etc. For example: Problem 24 Paul, Carole and others have a list of banking transactions (deposits, withdrawal, transfers, wires etc.), each of their customers. Say the customers have common IDs across the lists. Design an approximate, provably privacy-preserving protocol to find the "heavy hitters", i.e., customers who executed the largest amount in transactions in the combined list of all transactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Concluding Remarks</head><p>In How Proust can Change Your Life, Alain de Botton wrote about "the All-England Summarise Proust Competition hosted by Monty Python ... that required contestants to précis the seven volumes of Proust's work in fifteen seconds or less, and to deliver the results first in a swimsuit and then in evening dress." I have done something similar with data stream algorithms in what amounts to an academic 15 seconds sans a change of clothes.</p><p>I think data streams are more than the topic de jour in Computer Science. Data sources that are massive, automatic (scientific and atmospheric observations, telecommunication logs, text) data feeds with rapid updates are here to stay. We need the TCS infrastructure to manage and process them. That presents challenges to algorithms, databases, networking, systems and languages. Ultimately, that translates into new questions and methods in Mathematics: approximation theory, statistics and probability. New mindset-say, seeking only the strong signals, working with distribution summaries-are needed, and that means a chance to reexamine some of the fundamentals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Data Stream Art</head><p>Trend or not, data streams are now Art.</p><p>• There are ambient orbs [121] dubbed "News that Glows" by the New York Times Magazine, <ref type="bibr">Dec 15</ref> 2002, pages 104-105, that indicate fluctuations in Dow Jones Industrial Average using continuously modulated glow. Clearly they are useful for more than vetting financial obsessions.</p><p>• Dangling String created by (wonderful techno-)artist Natalie Jeremijenko, is a live wire connected to a Ethernet cable via a motor; traffic level in the cable is shown by the range of motions from the tiny twitch to a wild whirl, with associated sounds <ref type="bibr">[111]</ref>.</p><p>• Mark Hansen and Ben Rubin have the Listening Post exhibit [114] at various locations including the Brooklyn Academy of Music and the Whitney Museum of Contemporary Art in New York where they convert the live text information in Internet chat rooms and message boards into light and sound, described by NY Times as a "computer-generated opera".</p><p>Besides being Art, ambient information displays like the ones above are typically seen as Calming Technology <ref type="bibr" target="#b1">[2]</ref>; they are also an attempt to transcode streaming data into a processible multi-sensory flow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Short Data Stream History</head><p>Data stream algorithms as an active research agenda has emerged only over the past few years. The concept of making few passes over the data for performing computations has been around since the early days of Automata Theory. Making one or few passes for selection <ref type="bibr" target="#b7">[8]</ref> or sorting <ref type="bibr" target="#b8">[9]</ref> got early attention, but the area seems to have evolved slowly. Computer architecture research has long considered data flow architectures which may be thought of as an approach to data streaming, but the area did not address complex operations on each data item.</p><p>There was a gem of a paper by Munro and Paterson <ref type="bibr" target="#b7">[8]</ref> in 1980 that specifically defined multi-pass algorithms and presented one pass algorithms and multi-pass lower bounds on approximately finding quantiles of a signal.</p><p>In early 90's, I remember Raghu Ramakrishnan of U. Wisconsin, Madison, asking me what I can do if I was allowed to make only one pass over the data. Presumably others had this question in their mind too. "Not much", I told Raghu then, but that has changed in the past 6 years. Phil Gibbons and Yossi Matias at Bell Labs synthesized the idea of Synopsis Data Structures <ref type="bibr" target="#b58">[59]</ref> that specifically embodied the idea of small space, approximate solution to massive data set problems. The influential paper <ref type="bibr" target="#b0">[1]</ref> used limited independence for small space simulation of sophisticated, one-pass norm estimation algorithms. This is a great example of ideas that emerged from complexity-theoretic point of view-pseudo random generators for space-bounded computations-getting applied to algorithmic problems that are motivated by emerging systems. The paper by Henzinger, Raghavan and Rajagoplan <ref type="bibr" target="#b9">[10]</ref> formulated one (and multiple) pass model of a data stream and presented a complexity-theoretic perspective; this is also an insightful paper with several nuggets of observations some of which are yet to be developed. Joan Feigenbaum, working with researchers at AT&amp; T Research, identified, developed and articulated the case for the network traffic data management as a data stream application. This was a great achievement and it is now widely accepted as one of the (chief?) inspiring applications for the area. Significant work was done at research labs-IBM Research and Bell Labs-and select universities about the same time.</p><p>Since these early developments, a lot has been done in Theoretical Computer Science community and in others including programming languages, KDD, Databases, Networking, etc. Hancock, a special purpose C based programming language that supports stream handling, got the best paper award in KDD 2000. There is focus on decision trees on data streams in KDD community. Rajeev Motwani gave a thoughtful plenary talk at PODS 2002 on data stream systems focusing on the fundamental challenges of building a generalpurpose data stream management system. The associated paper <ref type="bibr" target="#b10">[11]</ref> is well worth reading, in particular, for a database perspective. There have been tutorials in both SIGMOD and VLDB in year 2002. <ref type="bibr" target="#b106">[107]</ref> DIMACS gathered working groups on data streams. George Varghese addressed the problem of computing at link speed in router line card and focused on simple data stream problems at a SIGCOMM 2002 tutorial. Sprint Labs work on IP monitoring was presented at the SIGMETRICS 2002 tutorial <ref type="bibr" target="#b3">[4]</ref>. Jiawei Han has tutorials and talks on data mining problems in data streams <ref type="bibr" target="#b103">[104]</ref>.</p><p>The wonderful website [122] has a lot of information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">Perspectives</head><p>Doubtless more tutorials, workshops and other academic adventures will happen over time; the main point is that data stream agenda now pervades many branches of Computer Science. Industry is in synch too: several companies are promising to build special hardware to deal with incoming data at high speed. I was at a National Academy of Sciences meeting recently <ref type="bibr">[112]</ref>, where data stream concerns emerged from physicists, atmospheric scientists and statisticians, so it is spreading beyond Computer Science. Unlike a lot of other topics we-algorithmicists-poured energy into, data streams is an already accepted paradigm in many areas of CS and beyond. <ref type="foot" target="#foot_5">6</ref> I think if we keep the spirit of stream data phenomenon in mind, and be imaginative in seeking applications and models, potential for new problems, algorithms and mathematics is considerable. I hope the perspective I have presented in this writeup helps you ideate.</p><p>I have mixed exposition with reflections. Thanks to the SODA 2003 PC for giving me the opportunity. I have left out image, audio and video streams, XML streams, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Acknowledgements</head><p>Greg Fredrickson said we-TCS researchers-are much too timid in our technical writing. I was inspired by an afternoon of discussion with him to be more alpha. Thanks to Mike Waterman for sharing his writings from his camps under the western sky. Joel Spencer enthused me for Paul and Carole, all through my thesis days. I am happy to find new roles for them. Sampath Kannan and I shared a new boyhood in Madison, NJ; he did crossword puzzles, I learned about streaming, by osmosis. Mike Paterson at U. Warwick, UK, is an inspiration, for puzzles, problem solving and classic papers.</p><p>Rob Calderbank, my boss (many times removed), supported me tremendously as I conceived of one project after another at AT&amp;T, and inspired me with Math and Management. As did my immediate boss David Johnson. Joan Feigenbaum recruited and mentored me. AT&amp;T gave me the opportunity to build systems with SNMP data, AWS cellular CDRs, and Intellectual property-patent data, and a national system for location-based services in wireless networks. That is a lot of toys! Ted Johnson at AT&amp; T and I have built many systems together. We have shared Tall Boys and commutes. Half of what I know about systems, I learned attacking Ted's work; the other half by building them with him.</p><p>I have had many colleagues in databases area, I have tried to learn from them, despite my moody self. Thanks to Vishy Poosala, Divesh Srivastava, Flip Korn, Rajeev Rastogi, Nick Koudas, Swarup Acharya and Minos Garofalakis, for teaching me selflessly. At Rutgers, Tomasz Imielinski has been a guiding colleague, sharing passion for unusual music and theater as well as his invaluable insight into impactful database systems research.</p><p>Graham Cormode and I have worked on several problems in data streaming that has shaped my insights. I make his English side sigh with my passion for coining new words. He retaliates with limericks:</p><p>There was a researcher called Muthu Who one day thought it would be cute to Live out his dreams By fishing in streams And if you could, wouldn't you too?</p><p>Talking about fishing: that puzzle is from my work with Mayur Datar, who helped me with comments and suggestions on this writeup.</p><p>Sometime in 1999, Anna Gilbert, Yannis Kotidis, Martin Strauss and I formed a (cross-functional!) team and studied data streams. I am very thankful to them. My work on histograms is joint with them as well as Sudipto Guha and Piotr Indyk, who have independently made very significant contributions to data streaming.</p><p>I am grateful to the many researchers who helped me with pointers: Yaron Minsky, Rajeev Motwani, Amin Shakrallohi, Sasha Barg, Sudipto Guha, Monika Henzinger, Piotr Indyk, D. Sivakumar, George Varghese, Ken Clarkson, David Madigan, Ravi Kumar, Eric Bach, Joachim von zur Gathen and Moses Charikar.</p><p>Finally, on professional and personal issues, Martin Farach-Colton has been my guide and inspiration, and I owe him lot more than can be acknowledged in this writeup.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Decay of SSE of top wavelet coefficients on IP data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Finally: Problem 12 (</head><label>12</label><figDesc>Facility location) Say Paul tracks n potential sites on the plane. Carole continuously either adds new client points or removes an existing client point from the plane. Paul can use space n polylog(n), but only o(m), preferably polylog(m), where m is the total number of points at any time. Solve the k-means or k-medians facility location problem on the set of n sites.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Problem 13 Consider the semi-streaming model, ie., one in which we have space to store vertices, say O(|V |polylog(|V |) bits, but not enough to store the edges. So we have o(|E|) bits. Solve interesting (in particular, dense) graph problems in this model.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Problem 19Given a set S of strings s 1 , . . . , s n and set T of strings t 1 , . . . , t n , find a matching (ie., oneto-one mapping) f : S → T such that i d(s i , f (s i )) is (approximately) minimized. Let d(x, y) be the edit distance between strings x and y. This problem can be done by computing d(s i , t j ) for all pairs i, j and finding min cost matching, but the goal is to get a substantially subquadratic (say near linear) time approximate algorithm. The underlying scenario is S and T are identical lists of names of people, but with some errors; f is our posited (likely) mapping of names of people in one list to the other.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>John Bates, the Chief of Remote Sensing Applications Division of USNOAANDCC, gives a nice exposition at http://www7.nationalacademies.org/bms/BatesPowerPoint.ppt and http://www7.nationalacademies.org/bms/AbstractBATES.html.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Input may comprise of multiple streams or multidimensional signals, but we do not consider those variations here.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>I remember the exhilaration we felt when Martin Farach-Colton and I coined the name Disk Access Machine model or the DAM model during a drive from New York city to Bell labs. The DAM model is catching on.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>There is also the work space needed to compute the function. We do not explicitly discuss this because typically this is of the same order as the storage.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>Anecdotics. John Bates of US National Oceanographic and Atmospheric Administration (http://www.etl.noaa.gov/ jbates/) faces the task of copying two decades worth of data from legacy tapes into current tapes, which will take a couple of years of continuous work on multiple tape readers. His question: during this intensive copying process, blocks of data reside on disk for a period of time. In the interim, can we perform much-needed statistical analyses of historic data? This is apt for data stream algorithms.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>As a unexpected topping, data streams may be the vehicle that induces people outside theoretical computer science to accept "approximation" as a necessary strategem and seek principled ways of approximating computations, an acceptance that is far from being universal now.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Wavelet packets comprises vectors defined by w j,n,k (x) = 2 j/2 W n (2 j x -k) for different values of j, n, k. They are richer than the well known Haar wavelets, and hence potentially give better compression. As before, the problem is to represent any given function A as a linear combination of B vectors from the wavelet packets. Two such vectors w and w ′ are not necessarily orthogonal, hence merely choosing the B largest | A, w j,n,k |'s.</p><p>A gem of a result on this problem is in <ref type="bibr" target="#b79">[80]</ref>: the author proves that the best representation of A using B terms can be obtained using O(B 2 log n) orthogonal terms. (Representing signals using orthogonal wavelet packet vectors is doable.) Presumably this result can be improved by allowing some approximation to the best B term representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Problem 7</head><p>There are a number of other special dictionaries of interest-beamlets, curvelets, ridgelets, segmentlets, etc.-each suitable for classes of applications. Design efficient algorithms to approximate the best representation of a given function using these dictionaries.</p><p>Studying general dictionaries. A paper that seems to have escaped the attention of approximation theory researchers is <ref type="bibr" target="#b62">[63]</ref> which proves the general problem to be NP-Hard. This was reproved in <ref type="bibr" target="#b59">[60]</ref>. In addition, <ref type="bibr" target="#b62">[63]</ref> contained the following very nice result. Say to obtain a representation with error ǫ one needs B(ǫ) terms. Let D be the N × |D| matrix obtained by having φ i as the ith column for each i. Let D + be the pseudoinverse of D. (The pseudoinverse is a generalization of the inverse and exists for any (m, n) matrix. If m &gt; n and A has full rank n, then A + = (A T A) -1 A T .) The author in <ref type="bibr" target="#b62">[63]</ref> presents a greedy algorithm that finds a representation with error no more than ǫ but using</p><p>terms. <ref type="bibr" target="#b62">[63]</ref> deserves to be revived: many open problems remain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Problem 8 Improve [63] to use fewer terms, perhaps by relaxing the error achieved. Is there a nontrivial non-greedy algorithm for this problem?</head><p>Problem 9 A technical problem is as follows: The algorithm in <ref type="bibr" target="#b62">[63]</ref> takes |D| time for each step of the greedy algorithm. Using dictionary preprocessing, design a faster algorithm for finding an approximate representation for a given signal using the greedy algorithm. This is likely to be not difficult: instead of finding the "best", find the "near-best" in each greedy step and prove that the overall approximation does not degrade significantly.</p><p>Both these questions have been addressed for a fairly general (but not fully general) dictionaries. Dictionary D has coherence µ if φ i = 1 for all i and for all distinct i and j, | φ i , φ j | ≤ µ. (For orthogonal dictionaries, µ = 0. Thus coherence is a generalization.) Nearly exponentially sized dictionaries can be generated with small coherence. For dictionaries with small coherence, good approximation algorithms have been shown:</p><p>Theorem 10 [64] Fix a dictionary D with coherence µ. Let A be a signal and suppose it has a B-term representation over D with error A -R opt = δ, where B &lt; 1/(32µ). Then, in iterations polynomial in B, we can find a representation with error at most (1 + 2064µB 2 )δ.</p><p>This line of research is just being developed; see <ref type="bibr" target="#b67">[68]</ref> for new developments. Further in <ref type="bibr" target="#b63">[64]</ref>, authors used approximate nearest neighbor algorithms to implement the iterations in Theorem 10 efficiently, and proved that approximate implementation of the iterations does not degrade the</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The space complexity of approximating the frequency moments</title>
		<author>
			<persName><forename type="first">N</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM STOC</title>
		<meeting>ACM STOC</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="20" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The coming age of calm technology. Chapter 6. Beyond calculation: The next fifty years of computing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Weiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
		<editor>P. J. Denning and R. M. Metcalfe</editor>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>Springer Verlag</publisher>
		</imprint>
	</monogr>
	<note>Versions of this article exist on the web</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Detecting packet patterns at high speeds</title>
		<author>
			<persName><forename type="first">G</forename><surname>Varghese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGCOMM</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Network performance monitoring and measurement: techniques and experience</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGMETRICS tutorial</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Maintaining statistics counters in router line cards</title>
		<author>
			<persName><forename type="first">D</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Prabhakar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="page" from="76" to="81" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On optimal strategies for searching in presence of errors</title>
		<author>
			<persName><forename type="first">S</forename><surname>Muthukrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SODA</title>
		<meeting>SODA</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="680" to="689" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Quickly generating billion-record synthetic databases</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sundaresan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Eggert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Baclawski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. ACM SIGMOD</title>
		<imprint>
			<biblScope unit="page" from="243" to="252" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Selection and sorting with limited storage</title>
		<author>
			<persName><forename type="first">I</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE FOCS</title>
		<meeting>IEEE FOCS</meeting>
		<imprint>
			<date type="published" when="1978">1978. 1980</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The art of computer programming</title>
		<author>
			<persName><forename type="first">D</forename><surname>Knuth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sorting and searching</title>
		<imprint>
			<publisher>Addison-Wesley</publisher>
			<date type="published" when="1973">1973</date>
			<biblScope unit="volume">III</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Computing on data stream</title>
		<author>
			<persName><forename type="first">M</forename><surname>Henzinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rajagopalan</surname></persName>
		</author>
		<idno>1998-011</idno>
	</analytic>
	<monogr>
		<title level="j">Digital systems research center</title>
		<imprint>
			<date type="published" when="1998-05">May 1998</date>
			<pubPlace>Palo Alto</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical Note</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Models and issues in data stream systems</title>
		<author>
			<persName><forename type="first">B</forename><surname>Babcock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Datar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Widom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM PODS</title>
		<imprint>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learn more, sample less: control of volume and variance in network measurement</title>
		<author>
			<persName><forename type="first">N</forename><surname>Duffield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Thorup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGCOMM 2001 Measurement workshop</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Trajectory sampling for direct traffic observation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Duffield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grossglauser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Networking</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pregibon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Volinsky</surname></persName>
		</author>
		<title level="m">Communities of interest. Proc. of Intelligent Data Analysis</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="105" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">External memory algorithms and data structures: Dealing with massive data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Vitter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="209" to="271" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Petabyte scale data mining: dream or reality?</title>
		<author>
			<persName><forename type="first">A</forename><surname>Szalay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vandenberg</surname></persName>
		</author>
		<idno>MSR-TR-2002-84</idno>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Open problems in streaming</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kannan</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Ppt slides on request from the source</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Indyk A small approximately min-wise independent family of hash functions</title>
		<author>
			<persName><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Algorithms</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="84" to="90" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Estimating rarity and similarity in window streams</title>
		<author>
			<persName><forename type="first">M</forename><surname>Datar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Muthukrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ESA</title>
		<meeting>ESA</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="323" to="334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Space efficient finger search on degree-balanced search trees</title>
		<author>
			<persName><forename type="first">G</forename><surname>Blelloch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Maggs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM-SIAM SODA</title>
		<meeting>ACM-SIAM SODA</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">On the dynamic finger conjecture for splay trees, part II. The proof</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cole</surname></persName>
		</author>
		<idno>TR1995- 701</idno>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
		<respStmt>
			<orgName>Courant Institute, NYU</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The power of a pebble: Exploring and mapping directed graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sahai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vadhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM STOC</title>
		<meeting>ACM STOC</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="269" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Modern Computer Algebra</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zur Gathen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gerhard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Set reconciliation with nearly optimal communication complexity</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Minsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Trachtenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zippel</surname></persName>
		</author>
		<idno>2000-1796</idno>
		<imprint/>
		<respStmt>
			<orgName>Cornell Univ</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Three thresholds for a liar</title>
		<author>
			<persName><forename type="first">J</forename><surname>Spencer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Winkler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Combinatorics, Probability and Computing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="81" to="93" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">QuickSAND: Quick summary and analysis of network data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kotidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Muthukrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Strauss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DIMACS Technical Report</title>
		<imprint>
			<biblScope unit="page" from="2001" to="2043" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Surfing wavelets on streams: One pass summaries for approximate aggregate queries</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kotidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Muthukrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Strauss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VLDB Journal</title>
		<imprint>
			<biblScope unit="page" from="79" to="88" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Finding frequent items in data streams</title>
		<author>
			<persName><forename type="first">M</forename><surname>Charikar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Farach-Colton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICALP</title>
		<meeting>ICALP</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="693" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Network applications of bloom filters: A survey. Allerton Conference</title>
		<author>
			<persName><forename type="first">A</forename><surname>Broder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mitzenmacher</surname></persName>
		</author>
		<ptr target="http://www.eecs.harvard.edu/michaelm/NEWWORK/papers.html" />
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deriving traffic demands for operational IP networks: methodology and experience</title>
		<author>
			<persName><forename type="first">A</forename><surname>Feldmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Greenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Reingold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rexford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>True</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Networkin</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="265" to="280" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Counting inversions in lists</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SODA</title>
		<meeting>SODA</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="253" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Signature-based Methods for Data Streams</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pregibon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="167" to="182" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Counting inversions in a data stream</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ajtai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jayram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sivakumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc.STOC</title>
		<meeting>.STOC</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="370" to="379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Flajolet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Nigel Martin</surname></persName>
		</author>
		<title level="m">Probabilistic counting. Proc. FOCS</title>
		<imprint>
			<date type="published" when="1983">1983</date>
			<biblScope unit="page" from="76" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The population frequencies of species and the estimation of population parameters</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Good</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="237" to="264" />
			<date type="published" when="1953">1953</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Maintaining stream statistics over sliding windows</title>
		<author>
			<persName><forename type="first">M</forename><surname>Datar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gionis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM-SIAM SODA</title>
		<meeting>ACM-SIAM SODA</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Permutation editing and matching via embeddings</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cormode</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Muthukrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sahinalp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICALP</title>
		<meeting>ICALP</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="481" to="492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Chromium: A stream processing framework for interactive rendering on clusters</title>
		<author>
			<persName><forename type="first">G</forename><surname>Humphreys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Houston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ahern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kirchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Klosowski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Extended range search queries on geometric SIMD machine</title>
		<author>
			<persName><forename type="first">S</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Muthukrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venkatasubramanian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note>Manuscript</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<ptr target="http://www.siggraph.org/s2002/conference/courses/crs31.html" />
		<title level="m">Interactive geometric computations using graphics hardware. Course. Organizer: D. Manocha. SIG-GRAPH 2002</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">4/3 space algorithm for (s, t) connectivity in undirected graphs</title>
		<author>
			<persName><forename type="first">R</forename><surname>Armoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ta-Shma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wigderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JACM</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="294" to="311" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Reductions in streaming algorithms, with an application to counting triangles in graphs</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Bar-Yossef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sivakumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM-SIAM SODA</title>
		<meeting>ACM-SIAM SODA</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="623" to="632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Graph structure of the web: A survey</title>
		<author>
			<persName><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">LATIN</title>
		<imprint>
			<biblScope unit="page" from="123" to="125" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Better algorithms for high dimensional proximity problems via asymmetric embeddings</title>
		<author>
			<persName><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM-SIAM SODA</title>
		<meeting>ACM-SIAM SODA</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Computing diameter in the streaming and sliding window models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Feigenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ziang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note>Manuscript</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Reverse nearest neighbor aggregates over data streams</title>
		<author>
			<persName><forename type="first">F</forename><surname>Korn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Muthukrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB</title>
		<meeting>VLDB</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Sublinar time approximate clustering</title>
		<author>
			<persName><forename type="first">N</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Oblinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. ACM-SIAM SODA</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Bursty and hierarchical structure in streams</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 8th ACM SIGKDD KDD conf</title>
		<meeting>8th ACM SIGKDD KDD conf</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Inferring mixtures of markov chains</title>
		<author>
			<persName><forename type="first">T</forename><surname>Batu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kannan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note>Manuscript</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Pass efficient algorithms for approximating large matrices</title>
		<author>
			<persName><forename type="first">P</forename><surname>Drineas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kannan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM-SIAM SODA</title>
		<meeting>ACM-SIAM SODA</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="223" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Fast computation of low rank approximation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Achlioptas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mcsherry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">STOC</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">High-dimensional data analysis: The curses and blessings of dimensionality</title>
		<author>
			<persName><forename type="first">D</forename><surname>Donoho</surname></persName>
		</author>
		<ptr target="http://www-stat.stanford.edu/donoho/" />
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
	<note>Manuscript</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Comparing information without leaking it: Simple solutions</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fagin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Naor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Winkler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="77" to="85" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Protocols for secure computations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. FOCS</title>
		<meeting>FOCS</meeting>
		<imprint>
			<date type="published" when="1982">1982</date>
			<biblScope unit="page" from="160" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Secure multiparty computation of approximations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Feigenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ishai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Malkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nissim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Strauss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICALP</title>
		<meeting>ICALP</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<author>
			<persName><forename type="first">O</forename><surname>Goldreich</surname></persName>
		</author>
		<ptr target="http://philby.ucsd.edu/cryptolib/BOOKS/oded-sc.html" />
		<title level="m">Secure multiparty computation</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">IPSOFACTO: IP stream-oriented fast correlation tool</title>
		<author>
			<persName><forename type="first">F</forename><surname>Korn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Muthukrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD 2003 Demo and Manuscript</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Clifford algebras and approximating the permanent</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sinclair</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM STOC</title>
		<imprint>
			<biblScope unit="page" from="222" to="231" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Gibbons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Matias</surname></persName>
		</author>
		<title level="m">Synopsis data structures. Proc. SODA</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="909" to="910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Greedy adaptive approximation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Avellaneda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Constructive Approximation</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="57" to="98" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">The best m-term approximation and greedy algorithms</title>
		<author>
			<persName><forename type="first">V</forename><surname>Temlyakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Computational Math</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="249" to="265" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Constructive Approximation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Devore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lorentz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Sparse approximate solutions to linear systems</title>
		<author>
			<persName><forename type="first">B</forename><surname>Natarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Computing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="227" to="234" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Approximation of Functions over Redundant Dictionaries Using Coherence</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Muthukrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Strauss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SODA</title>
		<meeting>SODA</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Imagining numbers (Particularly the square root of minus fifteen</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mazur</surname></persName>
		</author>
		<editor>Farrar, Strauss and Giroux</editor>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Mining database structure or how to build a data quality browser</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dasu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Muthukrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Shkapenyuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGMOD</title>
		<imprint>
			<biblScope unit="page" from="240" to="251" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Hancock: A language for extracting signatures from data streams</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pregibon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rogers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KDD</title>
		<imprint>
			<biblScope unit="volume">2000</biblScope>
			<biblScope unit="page" from="9" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Greed is good: Algorithmic results for sparse approximation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tropp</surname></persName>
		</author>
		<idno>2003-04</idno>
		<imprint/>
		<respStmt>
			<orgName>Texas Institute for Computational and Applied Mathematics</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Sublinear algorithms for sparse approximations with excellent odds</title>
		<author>
			<persName><forename type="first">I</forename><surname>Daubechies</surname></persName>
		</author>
		<ptr target="http://www.ams.org/amsmtgs/2074abstracts/983-41-1214.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">NiagaraCQ: A scalable continuous query system for internet databases</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dewitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>ACM SIGMOD</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">STREAM: The stanford stream data manager</title>
		<author>
			<persName><forename type="first">A</forename><surname>Arasu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Babcock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Datar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Nishizawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rosenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Widom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>ACM SIGMOD</publisher>
			<pubPlace>Demo</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Aurora: A data stream management system</title>
		<author>
			<persName><forename type="first">D</forename><surname>Abadi</surname></persName>
		</author>
		<ptr target="http://www.cs.brown.edu/research/aurora/demo.pdf" />
	</analytic>
	<monogr>
		<title level="m">ACM SIGMOD 2003, Demo</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">TelegraphCQ: Continuous dataflow processing for an uncertain world</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chandrasekharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CIDR</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Gigsacope: High performance network monitoring with an SQL interface</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cranor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Shkapenyuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Spatscheck</surname></persName>
		</author>
		<ptr target="http://athos.rutgers.edu/muthu/demo02.pdf" />
	</analytic>
	<monogr>
		<title level="j">ACM SIGMOD</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">On rectangular partitionings in two dimensions: Algorithms, complexity and applications</title>
		<author>
			<persName><forename type="first">S</forename><surname>Muthukrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Poosala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Suel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Intl Conf on Database Theory (ICDT)</title>
		<meeting>Intl Conf on Database Theory (ICDT)</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="236" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<author>
			<persName><forename type="first">N</forename><surname>Thaper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Koudas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Dynamic multidimensional histograms</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="428" to="439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Exploratory data mining and data quality</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dasu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Johnson</surname></persName>
		</author>
		<ptr target="http://www.wiley.com/cda/product/0" />
		<imprint>
			<date type="published" when="2003-05">May 2003</date>
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
	<note>0471268518%7Cdesc%7C2927,00.html</note>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Checks and balances: Monitoring data quality in network traffic databases</title>
		<author>
			<persName><forename type="first">F</forename><surname>Korn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Muthukrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
	<note>manuscript</note>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">S</forename><surname>Muthukrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Strauss</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>ACM-SIAM SODA</publisher>
		</imprint>
	</monogr>
	<note>Rangesum histograms</note>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Best approximation with walsh atoms. Constructive Approximation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Villemoes</surname></persName>
		</author>
		<ptr target="http://www-old.math.kth.se/math/users/larsv/thesis.html" />
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">133</biblScope>
			<biblScope unit="page" from="329" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">The string edit distance matching problem with moves</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cormode</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Muthukrishnan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>ACM-SIAM SODA</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Application of the two-sided depth test to CSG rendering</title>
		<author>
			<persName><forename type="first">S</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mungala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venkatasubramanian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
	<note>I3d, ACM Interactive 3D graphics</note>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Space-efficient online computation of quantile summaries</title>
		<author>
			<persName><forename type="first">S</forename><surname>Greenwald</surname></persName>
		</author>
		<author>
			<persName><surname>Khanna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. ACM SIGMOD</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Approximate frequency counts over data streams</title>
		<author>
			<persName><forename type="first">G</forename><surname>Manku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB</title>
		<meeting>VLDB</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="346" to="357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Distinct sampling for highly-accurate answers to distinct values queries and event reports</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gibbons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB</title>
		<meeting>VLDB</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="541" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Random sampling for histogram construction: How much is enough?</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Narasayya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. SIGMOD</title>
		<imprint>
			<biblScope unit="page" from="436" to="447" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">On computing correlated aggregates over continual data streams</title>
		<author>
			<persName><forename type="first">F</forename><surname>Korn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gehrke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGMOD</title>
		<imprint>
			<biblScope unit="page" from="13" to="24" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Min-wise independent permutations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Broder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Charikar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Freize</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mitzenmacher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. STOC</title>
		<meeting>STOC</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="327" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Stable distributions, pseudorandom generators, embeddings and data stream computation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE FOCS 2000</title>
		<imprint>
			<biblScope unit="page" from="189" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Comparing data streams using hamming norms (How to zero in)</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cormode</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Datar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Muthukrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB</title>
		<meeting>VLDB</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="335" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">How to summarize the universe: Dynamic maintenance of quantiles</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kotidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Muthukrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Strauss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB</title>
		<meeting>VLDB</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="454" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">What is hot and what is not: Tracking most frequent items dynamically</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cormode</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Muthukrishnan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>ACM PODS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Fast, small space algorithm for approximate histogram maintenance</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kotidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Muthukrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Strauss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM STOC</title>
		<imprint>
			<biblScope unit="page" from="389" to="398" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">An approximate L 1 difference algorithm for massive data streams</title>
		<author>
			<persName><forename type="first">J</forename><surname>Feigenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Strauss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Viswanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE FOCS</title>
		<imprint>
			<biblScope unit="page" from="501" to="511" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<title level="m" type="main">Estimating dominance norms on multiple data streams</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cormode</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Muthukrishnan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
	<note>Manuscript</note>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Near-optimal sparse fourier estimation via sampling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Muthukrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Strauss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM STOC</title>
		<imprint>
			<biblScope unit="page" from="152" to="161" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Tracking join and self-join sizes in limited storage</title>
		<author>
			<persName><forename type="first">N</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gibbons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM PODS</title>
		<imprint>
			<biblScope unit="page" from="10" to="20" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Data streams and histograms</title>
		<author>
			<persName><forename type="first">S</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Koudas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM STOC</title>
		<imprint>
			<biblScope unit="page" from="471" to="475" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<title level="m" type="main">Finding deviants on data streams</title>
		<author>
			<persName><forename type="first">S</forename><surname>Muthukrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vitter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
	<note>Manuscript</note>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Clustering data streams</title>
		<author>
			<persName><forename type="first">S</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>O'callaghan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE FOCS</title>
		<imprint>
			<biblScope unit="page" from="359" to="366" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<title level="m" type="main">Better streaming algorithms for clustering problems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Charikar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>O'callaghan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Panigrahy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>ACM STOC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<title level="m" type="main">Estimating simple functions on the union of data streams</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gibbons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Trithapura</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>ACM SPAA</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Information statistics approach to data stream and communication complexity</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Bar-Yossef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jayram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sivakumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE FOCS</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Multi-Dimensional Regression Analysis of Time-Series Data Streams</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="http://www-courses.cs.uiuc.edu/cs497jh/ppt/topics01.ppt" />
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB</title>
		<meeting>VLDB</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="323" to="334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Mining deviants in a time series database</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jagadish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Koudas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Muthukrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB</title>
		<meeting>VLDB</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="102" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hey</surname></persName>
		</author>
		<ptr target="http://www.research.microsoft.com/Gray/talks/" />
		<title level="m">search of petabyte databases</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<monogr>
		<title level="m" type="main">Querying and mining data streams: you only get one look</title>
		<ptr target="http://www.bell-labs.com/user/minos/tutorial.html" />
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
