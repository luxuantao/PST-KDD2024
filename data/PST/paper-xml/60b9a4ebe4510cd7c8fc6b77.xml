<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Decoupling Representation Learning and Classification for GNN-based Anomaly Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yanling</forename><surname>Wang</surname></persName>
							<email>wangyanling@ruc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Key Laboratory of Data Engineering and Knowledge Engineering of Ministry of Education</orgName>
								<address>
									<country>Renmin University of China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="laboratory">Key Laboratory of Data Engineering and Knowledge Engineering of Ministry of Education</orgName>
								<address>
									<country>Renmin University of China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
							<email>zhang-jing@ruc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Key Laboratory of Data Engineering and Knowledge Engineering of Ministry of Education</orgName>
								<address>
									<country>Renmin University of China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="laboratory">Key Laboratory of Data Engineering and Knowledge Engineering of Ministry of Education</orgName>
								<address>
									<country>Renmin University of China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shasha</forename><surname>Guo</surname></persName>
							<email>guoshashaxing@ruc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Key Laboratory of Data Engineering and Knowledge Engineering of Ministry of Education</orgName>
								<address>
									<country>Renmin University of China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="laboratory">Key Laboratory of Data Engineering and Knowledge Engineering of Ministry of Education</orgName>
								<address>
									<country>Renmin University of China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
							<email>h.yin1@uq.edu.au</email>
							<affiliation key="aff2">
								<orgName type="department">School of Information Technology and Electrical Engineering</orgName>
								<orgName type="institution">The University of Queensland</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Information Technology and Electrical Engineering</orgName>
								<orgName type="institution">The University of Queensland</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Cuiping</forename><surname>Li</surname></persName>
							<email>licuiping@ruc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Key Laboratory of Data Engineering and Knowledge Engineering of Ministry of Education</orgName>
								<address>
									<country>Renmin University of China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="laboratory">Key Laboratory of Data Engineering and Knowledge Engineering of Ministry of Education</orgName>
								<address>
									<country>Renmin University of China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hong</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Key Laboratory of Data Engineering and Knowledge Engineering of Ministry of Education</orgName>
								<address>
									<country>Renmin University of China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="laboratory">Key Laboratory of Data Engineering and Knowledge Engineering of Ministry of Education</orgName>
								<address>
									<country>Renmin University of China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="middle">2021</forename><surname>Decoupling</surname></persName>
						</author>
						<title level="a" type="main">Decoupling Representation Learning and Classification for GNN-based Anomaly Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3404835.3462944</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>anomaly detection</term>
					<term>graph neural network</term>
					<term>decoupled training</term>
					<term>selfsupervised learning Representation Learning and Classification for GNN-based Anomaly Detection. In Proceedings of the 44th</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>GNN-based anomaly detection has recently attracted considerable attention. Existing attempts have thus far focused on jointly learning the node representations and the classifier for detecting the anomalies. Inspired by the recent advances of self-supervised learning (SSL) on graphs, we explore another possibility of decoupling the node representation learning and the classification for anomaly detection. We conduct a preliminary study to show that decoupled training using existing graph SSL schemes to represent nodes can obtain performance gains over joint training, but it may deteriorate when the behavior patterns and the label semantics become highly inconsistent. To be less biased by the inconsistency, we propose a simple yet effective graph SSL scheme, called Deep Cluster Infomax (DCI) for node representation learning, which captures the intrinsic graph properties in more concentrated feature spaces by clustering the entire graph into multiple parts. We conduct extensive experiments on four real-world datasets for anomaly detection. The results demonstrate that decoupled training equipped with a proper SSL scheme can outperform joint training in AUC. Compared with existing graph SSL schemes, DCI can help decoupled training gain more improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>• Information systems → Collaborative and social computing systems and tools; • Computing methodologies → Knowledge representation and reasoning;</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Anomaly detection, which aims to discover the rare occurrences in datasets <ref type="bibr" target="#b1">[2]</ref>, has numerous high-impact applications in various domains, such as detecting opinion deception and review spams <ref type="bibr" target="#b31">[32]</ref>, credit card fraud <ref type="bibr" target="#b2">[3]</ref>, calling card and telecommunications fraud <ref type="bibr" target="#b6">[7]</ref>, and misinformation <ref type="bibr" target="#b36">[37]</ref>. The most promising developments have been on discovering and incorporating the graph structural patterns <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b34">35]</ref>, as graphs effectively describe the correlations among inter-dependent users or objects that participate in the fraudulent activities <ref type="bibr" target="#b1">[2]</ref>.</p><p>Recently, driven by the advances of graph neural networks (GNNs) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b53">54]</ref>, many attempts adopt GNNs for anomaly detection <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b59">60]</ref>. The main idea of GNN-based anomaly detection is to leverage the power of GNNs to learn expressive node representations with the goal of identifying abnormal instances in the embedding space. In this paper, we focus on detecting abnormal users in the real-life graph structured data.</p><p>Despite the great success, existing GNN-based models jointly learn the node representations and the classifier for detecting the anomalies on the graph. Conceivably, such a joint training scheme performs well if the behavior patterns expressed by node embeddings are discriminative to reflect the label semantics. But the cases in reality may be far from satisfactory. Figure <ref type="figure" target="#fig_0">1</ref> illustrates an example of user-rating-product graph, where the green (black) ones denote the normal users (fraudsters). For highlighting the structures of users, we fold the products by connecting two users if they rate the same product. In the graph, users with similar behavior patterns are presented closer to each other, but their label semantics are not always consistent with the behavior patterns. For example, distant users locating in different communities of the graph, such as the normal users (u 1 , u 2 , u 3 ) or the fraudsters (u 4 , u 5 , u 6 ), behave differently from each other, even if they share the same label. On the contrary, the users that are close to each other, such as (u 1 , u 4 ), (u 2 , u 5 ) or (u 3 , u 6 ), behave similarly but have opposite labels. Such inconsistency between the behavior patterns and the label semantics will result in hard instances (the mentioned users in the above example) which put the GNN encoder in a dilemma: to learn the intrinsic graph properties or to capture the label semantics. Although GraphConsis <ref type="bibr" target="#b28">[29]</ref> and CARE-GNN <ref type="bibr" target="#b8">[9]</ref> also explore the inconsistency issue, they take effort on designing promising GNN encoders. This paper explores another way to alleviate the inconsistency's impact by raising a question: Shall we decouple the representation learning and the classification for anomaly detection?</p><p>In order to answer the question, we conduct a preliminary study to compare between the joint training and the decoupled training. Inspired by the recent progress in self-supervised learning (SSL) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b32">33]</ref>, after decoupling, we adopt the graph SSL schemes for unsupervised representation learning. The preliminary experiment starts with a representative graph SSL scheme, Deep Graph Infomax (DGI) <ref type="bibr" target="#b45">[46]</ref>, for encoding the global information into node representations to represent the individual behavior patterns as well as the normal pattern occupied by the majority. We conduct the experiment on multiple anomaly detection tasks where the learning difficulty gradually varies from hard to easy (with hard instances removed gradually during the classification). We make the observation that the joint training performs well on the easier tasks, while the decoupled training is less biased by the hard instances. Motivated by this observation, we conduct extensive experiments on other datasets (cf. Table <ref type="table" target="#tab_3">2</ref>). However, decoupled training with DGI offers limited performance gains. Such a result drives us to explore the deeper reason. Since we make the conjecture that inconsistency between the behavior patterns and the label semantics impacts the anomaly detection, we explore how the decoupled training with DGI performs under different inconsistency levels. Silhouette coefficient <ref type="bibr" target="#b35">[36]</ref> is a popular measure for quantifying the cohesion within the same class and separation across classes, so we use the additive inverse of silhouette coefficient to quantify the inconsistency level. The preliminary experimental results show that the decoupled training with DGI gains slight or even negative improvements over the joint training when the inconsistency is quite high. The above study implies that inconsistency is an important but often neglected factor for learning high-quality node representations.</p><p>Inspired by the above insights, we propose a new graph SSL scheme, Deep Cluster Infomax (DCI), which inherits the strength of DGI. In real life, normal users usually occupy the majority, so we can represent the whole graph to approximate the distribution of the normal users. From this perspective, DGI is a proper choice, as it encodes the graph-level information into each node representation to help identify the fraudsters from the normal users. However, when users behave diversely, it would be difficult to represent a unique normal pattern. To overcome this limitation, we introduce a clustering step to discover more concentrated feature spaces. Then we encode normal patterns within clusters instead of encoding a unique normal pattern in the whole graph. Consequently, DCI reduces the impact from the inconsistency caused by diverse behaviors across clusters.</p><p>This work makes the following observations and contributions:</p><p>• We conduct a study to compare between the joint training and the decoupled training on GNN-based anomaly detection, which is helpful for understanding the merits and limitations of decoupled training in practice. • Our study reveals an intriguing phenomenon-inconsistency between the behavior patterns and the label semantics highly impacts the performance of graph representation learningthat has rarely been discussed before. The results demonstrate the advantages of decoupled training with DCI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Our work is closely related to graph neural network, graph-based anomaly detection and self-supervised graph learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Graph Neural Networks</head><p>GNNs have made prominent progress in graph representation learning. The core idea behind GNNs is to update node representations by aggregating messages from the local neighborhoods. The state-ofthe-art GNN models include GCN <ref type="bibr" target="#b20">[21]</ref>, GraphSAGE <ref type="bibr" target="#b10">[11]</ref>, GAT <ref type="bibr" target="#b44">[45]</ref>, GIN <ref type="bibr" target="#b53">[54]</ref>, etc. These models differ from each other in the way to aggregate the neighborhood information. For example, GCN <ref type="bibr" target="#b20">[21]</ref> propagates messages based on the graph Laplacian matrix in a transductive manner. GraphSAGE <ref type="bibr" target="#b10">[11]</ref> proposes an inductive learning framework, in which the aggregation function such as mean, max or LSTM generates node embeddings by aggregating messages from a node's local neighborhood. GIN <ref type="bibr" target="#b53">[54]</ref> adopts the sum-like aggregation function, which is proved to be as powerful as the Weisfeiler-Lehman graph isomorphism test <ref type="bibr" target="#b24">[25]</ref>. Graph attentive networks, with the pioneer work GAT <ref type="bibr" target="#b44">[45]</ref>, are studied to assign different weights to different neighbors via attention mechanisms. These models have been widely used in various real-world applications, such as recommendation systems <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b55">56]</ref>, computational biology <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b39">40]</ref> as well as anomaly detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Graph-based Anomaly Detection</head><p>Early researches detect anomalies via dense block identification <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b40">41]</ref>, iterative learning <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b58">59]</ref> or belief propagation on graphs <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b34">35]</ref>. Nevertheless, these early attempts usually rely on the human-defined rules or features, which is not easy to generalize to various datasets. Motivated by the success of GNNs, modern algorithms tend to summarize the anomalous patterns automatically using GNNs. Examples include GAS <ref type="bibr" target="#b25">[26]</ref>, FdGars <ref type="bibr" target="#b49">[50]</ref>, GraphConsis <ref type="bibr" target="#b28">[29]</ref> and CARE-GNN <ref type="bibr" target="#b8">[9]</ref> for review fraud detection, GeniePath <ref type="bibr" target="#b27">[28]</ref> and SemiGNN <ref type="bibr" target="#b46">[47]</ref> for financial fraud detection, FANG <ref type="bibr" target="#b30">[31]</ref> for fake news detection, ASA <ref type="bibr" target="#b52">[53]</ref> for mobile fraud detection, and MTAD-GAT <ref type="bibr" target="#b60">[61]</ref> for time-series anomaly detection. These models extend the existing vanilla GCN <ref type="bibr" target="#b20">[21]</ref>, the graph attention network <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b51">52]</ref> or the heterogeneous GNNs <ref type="bibr" target="#b54">[55]</ref> to tackle the problem of anomaly detection. Existing attempts have focused on proposing a promising GNN encoder for node representation learning guided by the labels. In these models, the representation learning and the classification are usually trained jointly. This work explores another possibility of decoupling these two parts and proposing a proper graph SSL scheme to capture the desired patterns for anomaly detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Self-supervised Graph Learning</head><p>Self-supervised learning (pre-training) is a common and effective scheme in the area of computer vision <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b57">58]</ref>. Among the SSL schemes, contrastive learning (CL), raises a recent surge of interest in visual representation learning <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13]</ref>. On a parallel note, CLbased SSL schemes have also been investigated on graph data. The early attempts of unsupervised graph learning such as GAE <ref type="bibr" target="#b19">[20]</ref>, GraphSage <ref type="bibr" target="#b10">[11]</ref>, node2vec <ref type="bibr" target="#b9">[10]</ref>, deepwalk <ref type="bibr" target="#b32">[33]</ref> and LINE <ref type="bibr" target="#b43">[44]</ref>, which try to reconstruct the adjacency information of nodes, can be viewed as a kind of "local contrast" between a node and its neighbors to preserve the local homophily. The later proposed GCC <ref type="bibr" target="#b33">[34]</ref>, designing subgraph-level instance discrimination to capture transferable local structural patterns, can be viewed as a "local contrast" between multi-views of nodes (sampled ego-networks). Motivated by DIM <ref type="bibr" target="#b14">[15]</ref>, DGI <ref type="bibr" target="#b45">[46]</ref> and InfoGraph <ref type="bibr" target="#b42">[43]</ref> have been proposed to contrast between the node and the global graph, which can be viewed as a "local-global contrast" to capture the global structure information. In addition to graph structures, GPT-GNN <ref type="bibr" target="#b16">[17]</ref> learns node attributes by a generative SSL scheme. You et al. <ref type="bibr" target="#b56">[57]</ref> and Hassani et al. <ref type="bibr" target="#b11">[12]</ref> explore different types of graph data augmentations upon DGI-based SSL scheme. Our work differs from the above methods in two aspects. First, we explore the effectiveness of decoupled training on anomaly detection, and reveal that inconsistency is a key factor that impacts the quality of representation learning. Although <ref type="bibr">Kang et al. [18]</ref> have also investigated the effectiveness of decoupled training in visual representation learning, they target at solving the class imbalance issue. To the best of our knowledge, we are the first to connect the problem of inconsistency with decoupled training. Second, we propose DCI to contrast the local and the semi-global representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROBLEM DEFINITION</head><p>In this section, we first formalize the problem of graph-based anomaly detection and then define the joint training and the decoupled training, two training schemes for solving the problem.</p><p>Let G = (V , A, X ) be a graph, where V denotes the set of nodes and A denotes the adjacency matrix of nodes. X is the initial feature matrix with x i ∈ R d 0 denoting the d 0 -dimensional initial feature vector of v i . The nodes and links can be instantiated differently according to the concrete applications. For example, in business websites, for detecting the fraudsters who provide unreliable ratings on products, the graph is a bipartite graph with users, products as nodes and the user-rating-product relationships as links. Although the nodes usually include users and objects, we only care about detecting the abnormal users.</p><p>In this work, we represent nodes and the graph only based on the graph structures, as the side information of nodes is not always available, and the pure structure-dependent methods can generalize well across various applications. Without side information to initialize the node features, we perform eigen-decomposition on the normalized adjacency matrix s.t. D −1/2 AD −1/2 = U ΛU ⊤ where D is the degree matrix, and use the top eigenvectors in U as the initial node features. The eigenvectors roughly capture the users' behavior patterns, since they preserve the adjacency information. Practically, other adjacency-based methods such as LINE <ref type="bibr" target="#b43">[44]</ref> and node2vec <ref type="bibr" target="#b9">[10]</ref> can be used to initialize the node features.</p><p>The objective is to predict the abnormal nodes in a graph. However, since the labels are often arduously expensive to obtain, the input graph G is usually partially labeled. Thus we formulate the anomaly detection on a graph as follows:</p><p>Problem 1. Anomaly detection on a Graph. Given a partially labeled G = V , A, X , Y L , where Y L is the set of the partial labels on nodes and each y i ∈ Y is a binary value which takes value 1 if the corresponding node v i is abnormal and 0 otherwise, the objective of anomaly detection is to learn a predictive function:</p><formula xml:id="formula_0">F : G = V , A, X , Y L → Y ,<label>(1)</label></formula><p>where Y = Y L ∪ Y U with Y U as the unobserved labels of the nodes in G, which are to be inferred in the learning process. We use n to denote the number of nodes.</p><p>Graph neural networks have recently been attempted to solve the above defined problem, as introduced in Section 2.2. In most of these models, the predictive function F is divided into a GNN encoder д : V → R d and a binary classifier f : R d → {0, 1}, where д is to encode the structure patterns into the node representations, and f is usually performed on top of the node representations output by д to distinguish the normal and abnormal nodes. Formally,</p><formula xml:id="formula_1">H = д (G, θ ) , Ŷ = f (H , ϕ) ,<label>(2)</label></formula><p>where H , a n × d matrix, denotes the node embeddings and Ŷ , a |Y | × 1 matrix, denotes the predicted node labels. θ and ϕ are the parameters to be learned. Most of the existing GNN models learn θ and ϕ in a joint manner, which is defined as: In the decoupled training, representation learning, which encourages the encoder д to encapsulate desired features for the anomaly detection target via self-supervised learning on the unlabeled graph data, can be regarded as a pre-training process. Classification, which encourages д and f to have strong semantic discrimination ability via supervised learning on the labeled graph data, can be regarded as a tuning process.</p><p>Given the above defined problem of anomaly detection and two training schemes, the questions to be answered include:</p><p>• Which training scheme, the joint training or the decoupled training, is more promising for anomaly detection? • How to determine a proper L S S L for representation learning in decoupled training?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">METHODOLOGY</head><p>In this section, we systematically study the performance of decoupled training in anomaly detection. More specifically, we compare the decoupled training with the joint training given anomaly detection tasks with varying learning difficulties. We dig deeper into data inconsistency, the possible reason that causes the hard instances to learn, in order to explore the desired graph SSL for decoupled training. Finally, based on the observations, we propose Deep Cluster Infomax, a novel graph SSL scheme for anomaly detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Preliminary</head><p>We introduce the instantiated graph encoder and the basic selfsupervised learning scheme to perform our study.</p><p>Graph Encoder д and Classifier f . We adopt GIN <ref type="bibr" target="#b53">[54]</ref>, a stateof-the-art graph neural network, to instantiate the GNN encoder д in the defined problem. GIN calculates the representation for each node via a sum-like neighborhood aggregation function, i.e., h</p><formula xml:id="formula_2">(l ) i = MLP (l ) 1 + ϵ (l ) • h (l −1) i + v j ∈N (v i ) h (l −1) j ,<label>(3) where h (l )</label></formula><p>i ∈ R d is the embedding of node v i at the l-th layer, and h</p><formula xml:id="formula_3">(0) i = x i . N (v i )</formula><p>is the set of neighboring nodes of node v i . MLP denotes the multi-layer perceptron. ϵ (l ) is either a learnable parameter or a fixed scalar. We stack L layers to obtain the final node representation h (L) i . Compared with other general GNNs <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b44">45]</ref>, the simple sum-like aggregator in GIN is able to encapsulate the neighborhood homophily as well as the structural homophily, which are both vital for representing the behavior patterns. Compared with the specific GNNs for anomaly detection <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b46">47]</ref>, GIN can generalize to more datasets (Table <ref type="table" target="#tab_3">2</ref> illustrates that GIN outperforms other GNNs on most datasets).</p><p>We instantiate f based on a linear mapping followed by a nonlinear activation function, i.e.,</p><formula xml:id="formula_4">p i = σ (W ⊤ h (L) i + b), (<label>4</label></formula><formula xml:id="formula_5">)</formula><p>where σ is the sigmoid activation function, W and b are the parameters to be learned, and p i is the predicted suspicious score (i.e., abnormal score) of node v i . Basic L S L . We adopt cross-entropy (CE) loss, the most popular one in supervised learning, to instantiate the supervised loss function L S L in both the joint training and the decoupled training, i.e.,</p><formula xml:id="formula_6">L C E = − 1 |Y L | |Y L | i=1 (y i • log p i + (1 − y i ) • log (1 − p i )) , (5)</formula><p>Basic L S S L . We adopt Deep Graph Infomax (DGI) <ref type="bibr" target="#b45">[46]</ref>, a stateof-the-art graph SSL scheme, to instantiate the SSL loss function L S S L in the decoupled training, i.e.,</p><formula xml:id="formula_7">L DG I = − 1 2n n i=1 E G log D(h (L) i , s) + E G log(1 − D( h(L) i , s)) ,<label>(6)</label></formula><p>where D is a discriminator that outputs the affinity score of each local-global (i.e., node-graph) pair. The graph G, generated by a row-wise shuffling of the initial feature matrix X , provides the node representation h(L) i that can be paired with the graph representation s as a negative sample.</p><p>For computing the graph representation s, we follow DGI to average all the nodes' representations and then apply a sigmoid activation function on the pooled result, i.e.</p><formula xml:id="formula_8">s = σ 1 n n i=1 h (L) i . (<label>7</label></formula><formula xml:id="formula_9">)</formula><p>Choosing DGI is inspired by the definition of "anomaly": Definition 4.1. Anomalies are the instances which stand out as being dissimilar to all others <ref type="bibr" target="#b3">[4]</ref>.</p><p>The above definition of anomaly implies that understanding how the majority act (i.e., the normal pattern) is vital for anomaly detection. Since the normal instances usually occupy the majority of the data, we can represent the whole graph to approximate the distribution of the normal instances. In view of this, DGI is a proper choice that enables node representations to capture the global information of the entire graph. Henceforth, we abbreviate h (L) i to h i for simplicity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Why Decoupled Training?</head><p>Figure <ref type="figure" target="#fig_0">1</ref> shows an example of graph data, where the hard instances increase the difficulty of anomaly detection. In this section, we study the necessity of decoupled training by exploring how the joint training and the decoupled training perform when the learning difficulty is varied. To answer this question, we define the learning difficulty, design the experimental protocol and present the observed results. Learning Difficulty. It is non-trivial to determine the learning difficulty of a dataset by the raw features and labels of the instances in it. Instead, we first jointly train д and f using all the labeled users and obtain the predicted suspicious scores. Specifically, we conduct the joint training for 100 epochs, and use the averaged predicted suspicious score over the 100 epochs for each user. Then we sort the averaged predictive probabilities that quantify how anomalous each user is in descending order. The top ρ (%) normal users and the bottom ρ (%) fraudsters are viewed as the hard instances to be predicted. Adjusting the value of ρ and removing the corresponding hard instances during classification leads to various learning difficulties. Note that we encode node representations based on the whole graph structure, while conduct prediction on the datasets of various learning difficulties. Experimental Protocol. Using GIN in Eq.( <ref type="formula" target="#formula_2">3</ref>) as the graph encoder, the 1-layer MLP in Eq.( <ref type="formula" target="#formula_4">4</ref>) as the classifier, we perform joint training and decoupled training on different tasks controlled via ρ. Specifically, the first scheme jointly trains д and f by a unique cross-entropy loss in Eq.( <ref type="formula">5</ref>), while the second scheme first pre-trains д by the DGI loss in Eq.( <ref type="formula" target="#formula_7">6</ref>) and then tunes д and f together by the cross-entropy loss. We run 50 epochs for the pre-training model and run 100 epochs to train the classifier. Finally, we report the averaged best AUC score over 10 folds.</p><p>We construct six datasets from Reddit by varying its learning difficulty level ρ (%) from 0 to 10 with interval 2, where Reddit is a benchmark consisting of posts made by users on subreddits using the banned users from the website Reddit as the ground-truth anomalies <ref type="bibr" target="#b23">[24]</ref>. Decoupled Helps Address Hard Instances. The results are summarized in Figure <ref type="figure" target="#fig_2">2</ref>(a), from which we make an important observation: compared with the joint training, the decoupled training is less biased by the hard instances. The AUC performance of joint training increases from 0.720 to 0.859 with varying ρ from 0 to 10, which indicates the hard instances impact the learning process a lot. On the easier tasks (corresponding to a larger ρ), the decoupled training has comparable performance with the joint training. But when the dataset becomes difficult to learn (corresponding to a smaller ρ), the performance gain derived by the decoupled training tends to increase (derives 3.1% relative AUC gain when ρ=0). Such an intriguing observation implies that decoupling representation learning and classification for anomaly detection can lower the negative impact caused by hard instances. Motivated by the above observation, we conduct extensive experiments on other datasets (cf. Table <ref type="table" target="#tab_3">2</ref>), but we note that the power of decoupled training with DGI could be limited (derives less than 1.5% relative AUC gains over the joint training on the Wiki, Alpha and Amazon). Such a result drives us to explore the deeper reason and the possible limitation of DGI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Is Decoupled Training Stably Better?</head><p>Definition 4.2. Inconsistency: the behavior patterns and the label semantics disagree with each other.</p><p>Various limitations of the data distributions such as label imbalance <ref type="bibr" target="#b17">[18]</ref> and scarce label <ref type="bibr" target="#b41">[42]</ref> may increase the learning difficulty of a dataset, but we make the conjecture that the inconsistency between the behavior patterns and the label semantics also leads to the learning difficulty. To verify our conjecture, we explore how the decoupled training with DGI performs under different inconsistency levels. To answer the question, we formulate the inconsistency, design the experimental protocol and present the observed results. Inconsistency. A dataset is more consistent if the behavior patterns of the same-labeled users are more similar, and those between the users with opposite labels are more different. Intuitively, silhouette coefficient <ref type="bibr" target="#b35">[36]</ref>, a clustering metric which measures the cohesion within the same class and separation across classes, can help formulate the inconsistency. We use the additive inverse of silhouette coefficient to represent inconsistency η. Since there are only two classes (i.e., normal and abnormal), η can be defined as:</p><formula xml:id="formula_10">η = − 1 |V | |V | i=1 b i − a i max{b i , a i } ,<label>(8)</label></formula><formula xml:id="formula_11">a i = 1 |V i | v j ∈V i ∥x i − x j ∥ 2 , V i = {v j : y j = y i }, b i = 1 | Vi | v j ∈ Vi ∥x i − x j ∥ 2 , Vi = {v j : y j y i },</formula><p>where x i and x j are the initial representations of users v i and v j respectively. a i is the average distance of v i to all the other users of the same label, and b i is the average distance of v i to all the other users of the opposite label. When a i is smaller and b i is bigger, v i is more consistent to the users of the same label. More users satisfy the property will result in a larger silhouette coefficient and a smaller inconsistency. Experimental Protocol. We perform joint training and decoupled training on different datasets controlled via η. The evaluation process also follows the 10-fold evaluation. We construct four separate datasets from Reddit with different η. Specifically, we perform METIS <ref type="bibr" target="#b18">[19]</ref>, a graph partition algorithm, to partition the the original graph into four sub-graphs with η (1e-2) ∈ {-1.2, -0.5, 0.4, 3.4}. With METIS, we can construct multiple datasets with different inconsistency levels as well as preserving the original graph structures within each dataset as much as possible. DGI Helps Less on the Highly Inconsistent Data. The relative performance gains obtained by decoupled training with DGI are shown in Figure <ref type="figure" target="#fig_2">2(b)</ref>, from which we observe that compared with joint training, decoupled training with DGI may not always improve, and even brings negative influence when the data is highly inconsistent. The figure shows decoupled training with DGI obtains very slight or even negative AUC gains on three of the four datasets. Only on the less inconsistent dataset (corresponding to η=-1.2), the decoupled training attains 3.1% relative AUC gain over the joint training. In other words, it is challenging for DGI to summarize a proper normal pattern in the highly inconsistent dataset. Thus, a more effective graph SSL scheme is demanded addressing such a problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">The Proposed SSL Scheme DCI</head><p>We propose a new self-supervised scheme, called Deep Cluster Infomax (DCI), for anomaly detection. Similar to DGI, DCI also encodes the normal pattern of the majority following Definition 4.1. However, when users behave quite diversely, a unique normal pattern is difficult to be represented. Fortunately, we observe that users can be naturally partitioned into different clusters, and the behavior patterns within the same cluster are often much more concentrated than those in the whole graph. As a result, the inconsistency presented by the same labels but diverse behavior patterns in the whole graph (e.g., (u 1 , u 2 , u 3 ) or (u 4 , u 5 , u 6 ) in Figure <ref type="figure" target="#fig_0">1</ref>) will be reduced within a small cluster. Meanwhile, the inconsistency presented by the opposite labels but close behavior patterns (e.g., (u 1 , u 4 ), (u 2 , u 5 ) or (u 3 , u 6 ) in Figure <ref type="figure" target="#fig_0">1</ref>) will also be reduced, as the distance between these users is amplified when the context is restricted into a small cluster. In view of this, we perform the cluster-level summary instead of the graph-level summary.</p><p>The first step of DCI is to partition the given graph</p><formula xml:id="formula_12">G into K clus- ters [C 1 , C 2 , • • • , C K ],</formula><p>where each cluster C k contains n k nodes. The node set in C k is denoted as V k . Note that for the user-interactingobject graphs, the cluster C k contains both users and objects. We apply the classic K-Means algorithm to cluster all nodes based on the node features X . Since X is instantiated by the top eigenvectors of the normalized adjacency matrix, it preserves the neighborhood proximity. In this way, the users who behave similarly tend to be clustered into the same cluster. A self-defined method is designed to help determine the number of clusters K, and the details are explained in Section 5.5.</p><p>After clustering, we compute the cluster-level representation s k for each cluster to summarize how the majority in C k act, i.e.,</p><formula xml:id="formula_13">s k = σ 1 n k v i ∈V k h i .<label>(9)</label></formula><p>For each cluster C k , we encode this semi-global representation s k into the node representations via the following loss function:</p><formula xml:id="formula_14">L k DC I = − 1 2n k v i ∈V k E C k log D(h i , s k ) + E Ck log(1 − D( hi , s k )) , (<label>10</label></formula><formula xml:id="formula_15">)</formula><p>where D is a discriminator that outputs the affinity score of each local-semi-global (i.e., node-cluster) pair. Similar to DGI, node representation hi is paired with the cluster representation s k as a negative sample.</p><p>The final loss function of DCI is the average of the losses of the K clusters, i.e.,</p><formula xml:id="formula_16">L DC I = 1 K K k =1 L k DC I .<label>(11)</label></formula><p>In practice, we re-cluster the nodes based on the node embeddings after every t training epochs. Algorithm 1 gives the pseudocode for DCI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Deep Cluster Infomax</head><p>Input : </p><formula xml:id="formula_17">Graph G = (V , A, X ),</formula><formula xml:id="formula_18">Initialize clusters [C 1 , C 2 , • • • , C K ] = K-Means(X );</formula><p>2 Initialize the parameters θ and ω for the encoder д and the discriminator D ;</p><formula xml:id="formula_19">3 for epoch ← 1 to t do 4 H = д (G, θ ); 5 L DC I = 1 K K k =1 L k DC I (H , C k , ω); 6 θ , ω ←Adam(L DC I ); 7 if t mod t == 0 then 8 [C 1 , C 2 , • • • , C K ] = K-Means(д (G, θ ))</formula><p>Return : encoder д</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Discussions and Summaries</head><p>We discuss why the decoupled training (DCI) can work compared with other choices.</p><p>Compared with joint training supervised by the labeled data, the decoupled training additionally trains the GNN encoder selfsupervised by the intrinsic graph structures beforehand, which can be less biased by the data inconsistency.</p><p>In terms of the decoupled training for anomaly detection, existing graph SSL schemes offer limited benefits. For example, GAE <ref type="bibr" target="#b19">[20]</ref> and GraphSAGE <ref type="bibr" target="#b10">[11]</ref> reconstruct the adjacency matrix following the neighborhood proximity assumption, which can be hurt when too many interactions exist between the normal users and the fraudsters. GCC <ref type="bibr" target="#b33">[34]</ref> which contrasts multi-views of nodes (sampled ego-networks) to capture transferable structural patterns across graphs, over-emphasizes the structural homophily. DGI <ref type="bibr" target="#b45">[46]</ref> contrasts the whole graph with the node in it to encode the graph-level information into each node's representation. The graph-level embedding reveals the normal pattern occupied by the majority (i.e., the normal nodes), so it helps imply how far a node deviates from what is normal. DCI can be viewed as cluster-based DGI. Compared with DGI, the semi-global context encoded by DCI is less diverse, consequently alleviating the impact induced by the inconsistency between the behavior patterns and the label semantics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTAL EVALUATION</head><p>In this section, we present the results of different models to identify fraudulent users on real-world datasets. Particularly, we mainly answer the following research questions: • RQ1: How does the decoupled training perform compared with the joint training? • RQ2: How does DCI perform compared with other state-of-theart graph SSL schemes? • RQ3: How does the decoupled training perform compared with the multi-task learning? • RQ4: How to determine the number of clusters for DCI?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental settings</head><p>Datasets. We evaluate on four real-world user-object graphs. Detail statistics about these datasets are listed in Table <ref type="table" target="#tab_2">1</ref>.</p><p>• Reddit <ref type="bibr" target="#b23">[24]</ref> is a user-subreddit graph, which consists of one month of posts made by users on subreddits. This dataset contains ground-truth labels of banned users from Reddit. • Wiki <ref type="bibr" target="#b23">[24]</ref> is an editor-page graph, which describes one month of edits on Wikipedia pages. This dataset contains public groundtruth labels of banned users. • Alpha <ref type="bibr" target="#b22">[23]</ref> is a user-user trust graph of Bitcoin users trading on the platform Alpha. This graph is made bipartite by splitting each user into a "rater" with all its outgoing edges and an "object" with all incoming edges. 214 users in this dataset are labeled. • Amazon <ref type="bibr" target="#b22">[23]</ref> is a user-product graph, where the edges describe users' rating behaviors. 278 users in this dataset are labeled. Specifically, the graphs used in our experiments are unweighted, where the edge represents a user has ever interacted with an object. Amazon is extracted from a large user-product graph <ref type="bibr" target="#b22">[23]</ref>, which contains 256,059 users, 74, 258 products and 560,804 interactions. We use <ref type="bibr" target="#b18">[19]</ref> to partition the original Amazon dataset into 20 sub-graphs, and merge two sub-graphs to simulate a graph which has higher inconsistency. METIS partitions the graph according to the interconnections between nodes, so it preserves the original graph structure within the sampled dataset as much as possible. Evaluation Protocols. In practice, the training data of anomaly detection is usually class-imbalance, that is, the instance-rich class (i.e. the class of normal users) dominates during the training procedure. As a result, all the predicted suspicious scores tend to be small, which makes it difficult to set a proper threshold for classifying the fraudsters and the normal users. So we adopt the widely used metric AUC to consider all the possible thresholds for classification. AUC measures the probability that a randomly sampled fraudster has a higher suspicious score than a randomly sampled normal user. For a fair comparison, we conduct 10-fold evaluation on the Reddit, Wiki and Alpha. Specially, we conduct 5-fold evaluation on the Amazon, since this dataset has limited labeled fraudsters. Baselines. We compare with two categories of baselines. First, Joint training algorithms working in an end-to-end manner are compared to show the effectiveness of decoupled training. Among them, CARE-GNN <ref type="bibr" target="#b8">[9]</ref>, which applies reinforcement learning to filter noisy neighbors, is a state-of-the-art GNN model for anomaly detection. Other GNN-based anomaly detection models such as GraphConsis <ref type="bibr" target="#b28">[29]</ref> and Semi-GNN <ref type="bibr" target="#b46">[47]</ref> have been shown to be less useful than CARE-GNN, thus they are ignored in the experiments. GAT <ref type="bibr" target="#b44">[45]</ref> and GeniePath <ref type="bibr" target="#b27">[28]</ref> are both graph attentive networks. GAT calculates attentions for one-hop neighbors, and GeniePath extends them to multi-hop neighbors. GeniePath has shown effectiveness on malicious account detection in Alipay. GIN <ref type="bibr" target="#b53">[54]</ref> proposes a powerful feature aggregation function to effectively preserve the structure homophily.</p><p>Second, SSL schemes for decoupled training are compared to show the superiority of DCI. Among them, Graph Auto-encoder (GAE) <ref type="bibr" target="#b19">[20]</ref> and Random walk-based objective (RW) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref> reconstruct the one-hop or the multi-hop adjacency information (obtained by local random walks) between nodes. Graph Contrastive Coding (GCC) <ref type="bibr" target="#b33">[34]</ref> and Deep Graph Infomax (DGI) <ref type="bibr" target="#b45">[46]</ref> perform contrastive learning between node-node pairs or graph-node pairs. The four SSL baselines are popular and have shown effectiveness on various real-world applications. GAE and RW target on preserving the local adjacency between nodes. GCC and DGI allow to discover the structural similarities in a global environment -for example, distant nodes with similar structural roles. Distinct from the above baselines, DCI preserves the structural similarities in a semi-global context. It is worth noting that we do not compare with GPT-GNN <ref type="bibr" target="#b16">[17]</ref>, because our model only involves the structure information, while GPT-GNN requires the input of node attributes. Parameter Settings. We use input feature dimension (64), node representation dimension (128), number of GNN layers (2), learning rate (0.01) and optimizer (Adam) for all models. ϵ in Eq.3 is set to be 0. The source code of CARE-GNN fixes the number of GNN layer to be 1, so we use this default setting. The number of reclustering epochs t is set to be 20. For the SSL pre-training, we fix the number of training epochs to be 50. Specially, we adopt the early stopping strategy in the decoupled training with RW, since we found too many training epochs can harm this model's performance. To better analyze different SSL schemes, we unify their backbones as the GIN's encoder. For the classification, we record the best testing result after 100 epochs on each fold, then report the averaged best AUC score over different folds. Code Implementation. For CARE-GNN, GIN, DGI and GCC, we use the source code provided by their authors. For GAT and GeniePath, we use the open-source implementation <ref type="foot" target="#foot_0">1</ref> . We modified these codes to make them adapt to our tasks. We implement DCI with Pytorch. The source code of DCI is available on Github<ref type="foot" target="#foot_1">2</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Overall Evaluation</head><p>According to the results shown in Table <ref type="table" target="#tab_3">2</ref>, we summarize the following conclusions:    As an extension of DGI, DCI demonstrates the necessity of clustering. In other words, given a more concentrated feature space, it would be easier to figure out the differences between the normal instances and the anomalies. (3) Adjacency information is useful for anomaly detection. We observe that GAE and RW also perform well across different datasets. On the Alpha, RW attains the best performance. However, the effectiveness of GCC is limited. In the traditional studies <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b22">23]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Study of DCI over Learning Difficulty</head><p>In this section, we take insight into: how decoupled training with DCI helps address the hard instances. We follow the experimental protocols in Section 4.2 to vary the learning difficulty via controlling ρ (%) ∈ {1, 2, 3, 4, 5} and report the performance of joint training (GIN) and decoupled training (DGI and DCI).</p><p>As summarized in Table <ref type="table" target="#tab_4">3</ref>, on the datasets consisting of more hard instances (corresponding to a smaller ρ), the decoupled training brings more performance gains over the joint training. Compared with DGI, DCI contributes more to the decoupled training. When the dataset becomes easier to learn (corresponding to a larger ρ), the decoupled training only obtains comparable performance with the joint training. That is to say, besides the inconsistency, the effectiveness of anomaly detection could be impacted by other factors, such as the label imbalance and scarce label. These problems deserve further study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Comparison with the Multi-task Learning</head><p>We compare with the multi-task learning, which also utilizes the self-supervision. The final loss of multi-task learning can be formulated as:</p><formula xml:id="formula_20">L MT L = α • L S L + (1 − α) • L S S L</formula><p>, where α is the balancing term between the cross-entropy loss and the self-supervised loss. We instantiate L S S L with the objective of GAE/DGI/DCI. α is searched from 0.1 to 0.9 with interval 0.1. In our experiments, the objectives of RW and GCC are optimized under the mini-batch setting, while the cross-entropy loss is optimized under the full-batch setting. Thus we do not consider the objectives of RW and GCC in this section.</p><p>As shown in Table <ref type="table" target="#tab_5">4</ref>, the multi-task learning is able to outperform the joint training, but does not always bring the positive improvements. Even though the multi-task learning outperforms the joint training on some occasions, the decoupled training still shows advantages over the multi-task learning. We also observe that using L DGI or L DC I for multi-task learning results in very poor performance on the Reddit and Wiki, while L GAE is less vulnerable on the two datasets. Compared with GAE which preserves the local adjacency, DGI and DCI allow to learn more implicit and expressive structural patterns. Thus using L DG I or L DC I could amplify the inconsistency between the structural patterns and the label semantics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Study of the Clustering Number</head><p>As the clustering plays a pivotal role in DCI, we investigate how to determine the number of clusters. Due to the various behavior patterns of users, it is not suitable to choose the clustering number K only according to the results on a validation set. Thus we suggest to narrow the search space of K before the learning process. We adopt inertia that measures how internally coherent clusters are to help narrow the search space. Inertia is computed by</p><formula xml:id="formula_21">|V |</formula><p>i=1 min µ j ∈C ∥x i − µ j ∥ 2 , where C is the set of disjoint clusters and µ j is the embedding of the j-th cluster center.</p><p>Specifically, we calculate the inertia under different clustering numbers K ∈ {2, 3, . . . , K * } (K * is the maximal clustering number and is set to be 50), and compute τ K which denotes the inertia gap between K and K + 1. Then given r consecutive inertia gaps {τ K , τ K +1 , • • • , τ K +r −1 } (r is set to be 15), we calculate the standard deviation σ τ . The optimal value of K tends to locate at the interval, where σ τ begins to be stable. Taking the Reddit and Wiki as the examples, we show σ τ under different clustering numbers in Figure <ref type="figure">3</ref>. On each dataset, the purple bar corresponds to the clustering number that leads to the best performance. We can see that the optimal value of K matches the above principle. Note that the optimal value of K could be somewhat different under different environments (e.g., different versions of PyTorch). In practice, we suggest to use the above principle to set an initial clustering number beforehand, then determine the final clustering number by searching around the initial one according to the results on a validation set. In our experiments, the datasets are not very large, so the results on a validation set could be less instructive. In light of this, we directly report the performance under the optimal value of K, reflecting the potential power of DCI. In the future work, we will explore a more effective way to find the optimal clustering number.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Convergence Study</head><p>Taking the Reddit and Wiki as the examples, we explore the convergence of DCI. Here we mainly compare with GIN (joint) and DGI (decoupled). These models use the GIN's encoder as the backbone. Different from the evaluation setting used in the above experiments, here we report the testing AUC averaged over 10 folds at each training epoch.</p><p>From Figure <ref type="figure">4</ref>, we observe that: on both datasets, the models using decoupled training tend to converge faster than the models using joint training. Benefiting from the SSL step in the decoupled training, we can obtain a better parameter initialization for the following classification to speed up the convergence. Besides, we observe that the convergence trends of different models are similar. That is because they all adopt the cross-entropy loss for the classifier optimization. Therefore, to further improve the effectiveness of anomaly detection, other objectives are also worth studying.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>This work piloted studies on performance of decoupled training for anomaly detection, and made intriguing findings. At the heart of these findings is the inconsistency between the structural patterns and the label semantics, which is identified to be a vital factor that impacts the representation learning. It provides a new perspective for understanding the SSL. This work further developed a new graph SSL scheme DCI by injecting a clustering step to reduce the data inconsistency. We believe that decoupled training composed of the GIN's encoder and a proper SSL objective can be an alternative way for effective anomaly detection. The findings and DCI develped here could be inspiring for the future research on representation learning, even not restricted to anomaly detection.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An illustration of the inconsistency between the behavior patterns and the label semantics. u 1 , u 2 , and u 3 are normal users but perform different behaviors. Similarly, u 4 , u 5 , and u 6 are fraudsters but perform different behaviors. In contrast, (u 1 , u 4 ), (u 2 , u 5 ) and (u 3 , u 6 ) have opposite labels, but their behaviors are similar.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Definition 3 . 1 .Definition 3 . 2 .</head><label>3132</label><figDesc>Joint training estimates the node labels via Ŷ = f (д (G, θ ) , ϕ) and then trains the parameters θ and ϕ through a single supervised loss function L S L . In contrast to the commonly adopted joint training, we propose to use the decoupled training, a new training manner for anomaly detection as follows: Decoupled training decouples the representation learning and the classification. The first step estimates node embeddings by H = д (G, θ ) and trains θ by an additional selfsupervised loss function L S S L which is independent from the observed node labels Y L . Then based on the learned θ , the second step estimates the node labels by Ŷ = f (д (G, θ ) , ϕ), and trains ϕ as well as tunes θ by a supervised loss function L S L .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Preliminary experiments. (a) Performance of different training schemes over various learning difficulties; (b) Relative performance gains obtained by decoupled training over different inconsistency levels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>( 1 )</head><label>1</label><figDesc>Decoupled training contributes to the anomaly detection. All the decoupled models use GIN's encoder as their backbones. Compared with GIN (joint training), most of the decoupled models perform better. On the Amazon, decoupled training with DCI obtains 6.4% relative performance gain over the joint training. (2) DCI is an effective graph SSL scheme for decoupled training. Compared with other SSL schemes, DCI helps decoupled training attain more performance gains on most datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: Study of the clustering number in DCI (best seen in color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>•</head><label></label><figDesc>We suggest that decoupled training equipped with a proper SSL objective can be an alternative way for effective anomaly detection. And we develop a graph SSL scheme called DCI .• We conduct extensive experiments on four real-world datasets.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the datasets.</figDesc><table><row><cell cols="4">Graph #Users(% normal, abnormal) #Objects #Edges</cell></row><row><cell>Reddit</cell><cell>10,000 (96.34%, 3.66%)</cell><cell>984</cell><cell>78,516</cell></row><row><cell>Wiki</cell><cell>8,227 (97.36%, 2.64%)</cell><cell>1,000</cell><cell>18,257</cell></row><row><cell>Alpha</cell><cell>3,286 (61.21%, 38.79%)</cell><cell>3,754</cell><cell>24,186</cell></row><row><cell>Amazon</cell><cell>27,197 (91.73%, 8.27%)</cell><cell>5,830</cell><cell>52,156</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Overall evaluation on four real-world datasets.</figDesc><table><row><cell></cell><cell></cell><cell cols="4">Reddit Wiki Alpha Amazon</cell></row><row><cell></cell><cell>CARE-GNN</cell><cell>0.700</cell><cell>0.702</cell><cell>0.802</cell><cell>0.729</cell></row><row><cell>Joint</cell><cell>GAT GeniePath</cell><cell>0.738 0.720</cell><cell>0.681 0.689</cell><cell>0.848 0.849</cell><cell>0.696 0.738</cell></row><row><cell></cell><cell>GIN</cell><cell>0.720</cell><cell>0.727</cell><cell>0.884</cell><cell>0.761</cell></row><row><cell></cell><cell>GAE</cell><cell>0.730</cell><cell>0.714</cell><cell>0.884</cell><cell>0.806</cell></row><row><cell></cell><cell>RW</cell><cell>0.728</cell><cell cols="2">0.740 0.908</cell><cell>0.782</cell></row><row><cell>Decoupled</cell><cell>GCC</cell><cell>0.669</cell><cell>0.695</cell><cell>0.865</cell><cell>0.733</cell></row><row><cell></cell><cell>DGI</cell><cell>0.743</cell><cell>0.737</cell><cell>0.884</cell><cell>0.771</cell></row><row><cell></cell><cell>DCI (ours)</cell><cell cols="3">0.746 0.762 0.907</cell><cell>0.810</cell></row><row><cell cols="2">Inconsistency η (1e-2)</cell><cell>-0.676</cell><cell>0.841</cell><cell>-</cell><cell>-</cell></row><row><cell cols="5">Note: All the decoupled models use GIN's encoder as the backbone.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparison between the joint training (GIN) and the decoupled training (DGI and DCI) on datasets which are increasingly easier.</figDesc><table><row><cell></cell><cell>ρ (%)</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell></row><row><cell></cell><cell>GIN (joint)</cell><cell cols="5">0.734 0.755 0.774 0.787 0.803</cell></row><row><cell>Reddit</cell><cell cols="6">DGI (decoupled) 0.755 0.771 0.777 0.790 0.798</cell></row><row><cell></cell><cell cols="6">DCI (decoupled) 0.756 0.778 0.783 0.790 0.800</cell></row><row><cell></cell><cell>GIN (joint)</cell><cell cols="5">0.745 0.757 0.781 0.803 0.808</cell></row><row><cell>Wiki</cell><cell cols="6">DGI (decoupled) 0.759 0.769 0.790 0.797 0.816</cell></row><row><cell></cell><cell cols="6">DCI (decoupled) 0.777 0.788 0.817 0.822 0.834</cell></row><row><cell></cell><cell>GIN (joint)</cell><cell cols="5">0.886 0.905 0.923 0.933 0.940</cell></row><row><cell>Alpha</cell><cell cols="6">DGI (decoupled) 0.893 0.907 0.928 0.948 0.955</cell></row><row><cell></cell><cell cols="6">DCI (decoupled) 0.907 0.906 0.936 0.939 0.947</cell></row><row><cell></cell><cell>GIN (joint)</cell><cell cols="5">0.764 0.781 0.783 0.803 0.799</cell></row><row><cell>Amazon</cell><cell cols="6">DGI (decoupled) 0.780 0.797 0.796 0.817 0.793</cell></row><row><cell></cell><cell cols="6">DCI (decoupled) 0.805 0.838 0.835 0.845 0.844</cell></row><row><cell cols="4">Note: All the models use GIN's encoder as the backbone.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Evaluation of the multi-task learning.</figDesc><table><row><cell></cell><cell></cell><cell cols="4">Reddit Wiki Alpha Amazon</cell></row><row><cell>Joint</cell><cell>GIN</cell><cell>0.720</cell><cell>0.727</cell><cell>0.884</cell><cell>0.761</cell></row><row><cell></cell><cell>GAE</cell><cell>0.726</cell><cell>0.705</cell><cell>0.904</cell><cell>0.766</cell></row><row><cell>Multi-task</cell><cell>DGI</cell><cell>0.647</cell><cell>0.664</cell><cell>0.891</cell><cell>0.806</cell></row><row><cell></cell><cell>DCI</cell><cell>0.675</cell><cell>0.670</cell><cell>0.893</cell><cell>0.803</cell></row><row><cell cols="6">Note: All the multi-task models use GIN's encoder as the backbone.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>, user behaviors (i.e., who review/edit/rate what) are regarded as important information for anomaly detection. The user behaviors are described by the edges in a graph. Such information is preserved by GAE and RW, but is ignored by GCC which emphasizes the structural similarity between nodes. In view of this, decoupled training with adjacency-based selfsupervision deserves further study. DCI conducts pre-clustering according to the node features decomposed from the normalized adjacency matrix, so it implicitly captures the adjacency information. (4) A simple GNN encoder can also perform well. CARE-GNN and GeniePath are two promising algorithms for the graph-based anomaly detection. They design complicated graph convolutions to solve the challenges of anomaly detection. However, we find that these models need more training epochs to converge, and GIN outperforms them on most datasets. We suggest that decoupled training composed of the GIN's encoder and a proper SSL objective can be an alternative way for effective anomaly detection. (5) Decoupled training with DCI shows promising performance on the more inconsistent dataset. We can compute the inconsistency levels for the Reddit and Wiki, as users in the two datasets are fully labeled. DCI shows only 0.4% relative AUC gain over DGI on the Reddit, but derives 3.4% relative AUC gain over DGI on the Wiki. Wiki is the more inconsistent dataset, on which DCI shows more performance gains over DGI.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://github.com/shawnwang-tech/GeniePath-pytorch</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">https://github.com/wyl7/DCI-pytorch</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work is supported by Beijing Natural Science Foundation (Grant No. 4212022), National Natural Science Foundation of China (Grant No. 62076245), National Key Research &amp; Develop Plan (Grant No. 2018YFB1004401), CCF-Tencent Open Fund and ARC Discovery Project (Grant No. DP190101985). We would like to thank the anonymous reviewers for their valuable comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Opinion Fraud Detection in Online Reviews by Network Effects</title>
		<author>
			<persName><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishi</forename><surname>Chandy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Faloutsos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>In ICWSM</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Graph based anomaly detection and description: a survey</title>
		<author>
			<persName><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danai</forename><surname>Koutra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data mining and knowledge discovery</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="626" to="688" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Unsupervised profiling methods for fraud detection. Credit scoring and credit control VII</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Richard J Bolton</surname></persName>
		</author>
		<author>
			<persName><surname>Hand</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
			<biblScope unit="page" from="235" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Deep Learning for Anomaly Detection: A Survey</title>
		<author>
			<persName><forename type="first">Raghavendra</forename><surname>Chalapathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Chawla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Exploiting Centrality Information with Graph Convolutions for Network Representation Learning</title>
		<author>
			<persName><forename type="first">Hongxu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Viet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hung</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Chih</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xue</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="590" to="601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Communities of interest</title>
		<author>
			<persName><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daryl</forename><surname>Pregibon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IDA</title>
				<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="105" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Neural-Brane: Neural Bayesian Personalized Ranking for Attributed Network Embedding</title>
		<author>
			<persName><forename type="first">S</forename><surname>Vachik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baichuan</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pin-Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><forename type="middle">Al</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Hasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Sci. Eng</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="119" to="131" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Enhancing Graph Neural Network-based Fraud Detectors against Camouflaged Fraudsters</title>
		<author>
			<persName><forename type="first">Yingtong</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="315" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">node2vec: Scalable Feature Learning for Networks</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Inductive Representation Learning on Large Graphs</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Contrastive multi-view representation learning on graphs</title>
		<author>
			<persName><forename type="first">Kaveh</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Hosein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khasahmadi</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4116" to="4126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuan</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong-Dong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="639" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">FRAUDAR: Bounding Graph Fraud in the Face of Camouflage</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Hooi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyun</forename><forename type="middle">Ah</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Beutel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kijung</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="895" to="904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">GPT-GNN: Generative Pre-Training of Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1857" to="1867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Decoupling Representation and Classifier for Long-Tailed Recognition</title>
		<author>
			<persName><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multilevel Graph Partitioning Schemes</title>
		<author>
			<persName><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vipin</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPP</title>
				<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="113" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<title level="m">Variational Graph Auto-Encoders. NIPS Workshop on Bayesian Deep Learning</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Revisiting selfsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1920" to="1929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">REV2: Fraudulent User Prediction in Rating Platforms</title>
		<author>
			<persName><forename type="first">Srijan</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Hooi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Disha</forename><surname>Makhija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Subrahmanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="333" to="341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Predicting Dynamic Embedding Trajectory in Temporal Interaction Networks</title>
		<author>
			<persName><forename type="first">Srijan</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xikun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1269" to="1278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A reduction of a graph to a canonical form and an algebra arising during this reduction</title>
		<author>
			<persName><surname>Aa Leman</surname></persName>
		</author>
		<author>
			<persName><surname>Weisfeiler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nauchno-Technicheskaya Informatsiya</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="12" to="16" />
			<date type="published" when="1968">1968. 1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Spam Review Detection with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Ao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Runshi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2703" to="2711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Robust Reputation-Based Ranking on Bipartite Rating Networks</title>
		<author>
			<persName><forename type="first">Rong-Hua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">Xu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SDM</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="612" to="623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">GeniePath: Graph Neural Networks with Adaptive Receptive Paths</title>
		<author>
			<persName><forename type="first">Ziqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaochao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longfei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4424" to="4431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Alleviating the Inconsistency Problem of Applying Graph Neural Network to Fraud Detection</title>
		<author>
			<persName><forename type="first">Zhiwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingtong</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<idno>SIGIR. 1569-1572</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Finding the bias and prestige of nodes in networks based on trust scores</title>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnab</forename><surname>Bhattacharya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="567" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">FANG: Leveraging Social Context for Fake News Detection Using Graph Representation</title>
		<author>
			<persName><forename type="first">Kazunari</forename><surname>Van-Hoang Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min-Yen</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName><surname>Kan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1165" to="1174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Estimating the prevalence of deception in online review communities</title>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Hancock</surname></persName>
		</author>
		<idno>WWW. 201-210</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">DeepWalk: online learning of social representations</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">GCC: Graph Contrastive Coding for Graph Neural Network Pre-Training</title>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qibin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno>KDD. 1150-1160</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Collective Opinion Spam Detection: Bridging Review Networks and Metadata</title>
		<author>
			<persName><forename type="first">Shebuti</forename><surname>Rayana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="985" to="994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Silhouettes: A graphical aid to the interpretation and validation of cluster analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><surname>Rousseeuw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Appl. Math</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="53" to="65" />
			<date type="published" when="1987">1987. 1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Csi: A hybrid deep model for fake news detection</title>
		<author>
			<persName><forename type="first">Natali</forename><surname>Ruchansky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungyong</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="797" to="806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Spotting Suspicious Link Behavior with fBox: An Adversarial Perspective</title>
		<author>
			<persName><forename type="first">Neil</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Beutel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="959" to="964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">A Graph to Graphs Framework for Retrosynthesis Prediction</title>
		<author>
			<persName><forename type="first">Chence</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minkai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<idno>ICML. 8818-8827</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">GraphAF: a Flow-based Autoregressive Model for Molecular Graph Generation</title>
		<author>
			<persName><forename type="first">Chence</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minkai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaocheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">M-Zoom: Fast Dense-Block Detection in Tensors with Quality Guarantees</title>
		<author>
			<persName><forename type="first">Kijung</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Hooi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML PKDD</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">9851</biblScope>
			<biblScope unit="page" from="264" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">When does selfsupervision improve few-shot learning</title>
		<author>
			<persName><forename type="first">Jong-Chyi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="645" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Infograph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization</title>
		<author>
			<persName><forename type="first">Fan-Yun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">LINE: Large-scale Information Network Embedding</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Graph Attention Networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep Graph Infomax</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Devon</forename><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">A Semi-supervised Graph Attentive Network for Financial Fraud Detection</title>
		<author>
			<persName><forename type="first">Daixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianbin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanhui</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanming</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Qi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="598" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Review Graph Based Online Store Review Spammer Detection</title>
		<author>
			<persName><forename type="first">Guan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sihong</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1242" to="1247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Identify Online Store Review Spammers via Social Review Graph</title>
		<author>
			<persName><forename type="first">Guan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sihong</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Intell. Syst. Technol</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">21</biblScope>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">FdGars: Fraudster Detection via Graph Convolutional Networks in Online App Review System</title>
		<author>
			<persName><forename type="first">Jianyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Xion</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="310" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Neural Graph Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="165" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Heterogeneous Graph Attention Network</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houye</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanfang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno>WWW. 2022-2032</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">ASA: Adversary Situation Awareness via Heterogeneous Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Rui</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="674" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">How Powerful are Graph Neural Networks?</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<author>
			<persName><forename type="first">Carl</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<idno>CoRR abs/2004.00216</idno>
		<title level="m">Heterogeneous Network Representation Learning: Survey, Benchmark, Evaluation, and Beyond</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Graph Convolutional Neural Networks for Web-Scale Recommender Systems</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Graph contrastive learning with augmentations</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongduo</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Understanding co-running behaviors on integrated CPU/GPU architectures</title>
		<author>
			<persName><forename type="first">Feng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jidong</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingsheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenguang</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Parallel and Distributed Systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="905" to="918" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">TADOC: Text analytics directly on compression</title>
		<author>
			<persName><forename type="first">Feng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jidong</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dalin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onur</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenguang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyong</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The VLDB Journal</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="163" to="188" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">GCN-Based User Representation Learning for Unifying Robust Recommendation and Fraudster Detection</title>
		<author>
			<persName><forename type="first">Shijie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Quoc Viet Hung Nguyen, Zi Huang, and Lizhen Cui</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Multivariate Time-series Anomaly Detection via Graph Attention Network</title>
		<author>
			<persName><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanyong</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Congrui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Defu</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhai</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bixiong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<idno>ICDM. 841-850</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
