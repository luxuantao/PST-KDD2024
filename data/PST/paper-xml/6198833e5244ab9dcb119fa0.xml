<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Single-sequence protein structure prediction using language models from deep learning !</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ratul</forename><surname>Chowdhury</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Program in Therapeutic Science</orgName>
								<orgName type="laboratory">Laboratory of Systems Pharmacology</orgName>
								<orgName type="institution">Harvard Medical School</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country>USA!</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nazim</forename><surname>Bouatta</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Program in Therapeutic Science</orgName>
								<orgName type="laboratory">Laboratory of Systems Pharmacology</orgName>
								<orgName type="institution">Harvard Medical School</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country>USA!</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Surojit</forename><surname>Biswas</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Biomedical Informatics</orgName>
								<orgName type="institution">Harvard Medical School</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country>USA!</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Molecular, and Biomedical Studies</orgName>
								<orgName type="institution" key="instit1">Nabla Bio. Inc. d Integrated Program in Cellular</orgName>
								<orgName type="institution" key="instit2">Columbia University</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Charlotte</forename><surname>Rochereau</surname></persName>
						</author>
						<author>
							<persName><forename type="first">George</forename><forename type="middle">M</forename><surname>Church</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Program in Therapeutic Science</orgName>
								<orgName type="laboratory">Laboratory of Systems Pharmacology</orgName>
								<orgName type="institution">Harvard Medical School</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country>USA!</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Biomedical Informatics</orgName>
								<orgName type="institution">Harvard Medical School</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country>USA!</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Peter</forename><forename type="middle">K</forename><surname>Sorger</surname></persName>
							<email>peter_sorger@hms.harvard.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Program in Therapeutic Science</orgName>
								<orgName type="laboratory">Laboratory of Systems Pharmacology</orgName>
								<orgName type="institution">Harvard Medical School</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country>USA!</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mohammed</forename><forename type="middle">N</forename><surname>Alquraishi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Program in Therapeutic Science</orgName>
								<orgName type="laboratory">Laboratory of Systems Pharmacology</orgName>
								<orgName type="institution">Harvard Medical School</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country>USA!</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Department of Systems Biology</orgName>
								<orgName type="institution">Columbia University</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Warren</forename><surname>Alpert</surname></persName>
						</author>
						<author>
							<affiliation key="aff4">
								<address>
									<addrLine>200 Longwood Avenue Harvard Medical School</addrLine>
									<postCode>02115</postCode>
									<settlement>Boston</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Columbia University Irving Medical Center New York Presbyterian Hospital Building PH-18</orgName>
								<address>
									<addrLine>201G 622 West 168th St</addrLine>
									<postCode>10032</postCode>
									<settlement>New York</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Single-sequence protein structure prediction using language models from deep learning !</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1101/2021.08.02.454840</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>AlphaFold2 and related systems use deep learning to predict protein structure from co-evolutionary relationships encoded in multiple sequence alignments (MSAs). Despite dramatic, recent increases in accuracy, three challenges remain: (i) prediction of orphan and rapidly evolving proteins for which an MSA cannot be generated, (ii) rapid exploration of designed structures, and (iii) understanding the rules governing spontaneous polypeptide folding in solution. Here we report development of an end-to-end differentiable recurrent geometric network (RGN) able to predict protein structure from single protein sequences without use of MSAs. This deep learning system has two novel elements: a protein language model (AminoBERT) that uses a Transformer to learn latent structural information from millions of unaligned proteins and a geometric module that compactly represents CÎ± backbone geometry. RGN2 outperforms AlphaFold2 and RoseTTAFold (as well as trRosetta) on orphan proteins and is competitive with designed sequences, while achieving up to a 10 6 -fold reduction in compute time. These findings demonstrate the practical and theoretical strengths of protein language models relative to MSAs in structure prediction.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INTRODUCTION</head><p>Predicting 3D protein structure from amino acid sequence is a grand challenge in biophysics. Progress has long relied on physics-based methods that estimate energy landscapes and dynamically fold proteins within these landscapes <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref> . A decade ago, the focus shifted to extracting residue-residue contacts from coevolutionary relationships embedded in multiple sequence alignments (MSAs) 5 (Supplementary Figure <ref type="figure" target="#fig_0">1</ref>). Algorithms such as the first AlphaFold 6 and trRosetta 7 use deep neural networks to generate distograms that guide classic physics-based folding engines. These algorithms perform substantially better than algorithms based on physical energy models alone. More recently, the outstanding performance of AlphaFold2 8 on a wide range of protein targets in the recent CASP14 prediction challenge shows that when MSAs are available, machine learning (ML)-based methods can predict protein structure with sufficient accuracy to complement X-ray crystallography, cryoEM, and NMR as a practical means to determine structures of interest.</p><p>Predicting the structures of single sequences using ML nonetheless remains a challenge: the requirement in AlphaFold2 for co-evolutionary information from MSAs makes it less performative with proteins that lack sequence homologs, currently estimated at ~20% of all metagenomic protein sequences <ref type="bibr" target="#b8">9</ref> and ~11% of eukaryotic and viral proteins <ref type="bibr" target="#b9">10</ref> . Applications such as protein design and quantifying the effects of sequence variation on function <ref type="bibr" target="#b10">11</ref> or immunogenicity <ref type="bibr" target="#b11">12</ref> also require single-sequence structure prediction. More fundamentally, the physical process of a polypeptide folding in solution is driven solely by the chemical properties of that chain and its interaction with solvent (excluding, for the moment, proteins that require folding co-factors). An algorithm that predicts sequence directly from a single sequence is-like energy-based folding engines <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref> -closer to the real-word process than an algorithm that uses MSAs. Thus, we speculate that ML-based single sequence prediction may ultimately provide new understanding of protein biophysics.</p><p>Structure prediction algorithms that are fast and low-cost are of great practical value because they make efficient exploration of sequence space possible, particularly in design applications <ref type="bibr" target="#b12">13</ref> . State of the art MSA-based predictions for large numbers of long proteins can incur substantial costs when performed on the cloud. Reducing this cost would enable many practical applications in enzymology and therapeutics including designing new functions <ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref> , raising thermostability <ref type="bibr" target="#b16">17</ref> , altering pH sensitivity <ref type="bibr" target="#b17">18</ref> , and increasing compatibility with organic solvents <ref type="bibr" target="#b18">19</ref> . Efficient and accurate structure prediction is also valuable in the case of orphan proteins, many of which are thought to play a role in lineage-specific adaptations and are taxonomically restricted. OSP24, for example, is an orphan virulence factor for the wheat pathogen F. graminearum that controls host immunity by regulating proteasomal degradation of a conserved signal transduction kinase <ref type="bibr" target="#b19">20</ref> . It is one of many orphan genes found in fungi, plants insects and other organisms <ref type="bibr" target="#b20">21</ref> for which no MSA is available.</p><p>We have previously described an end-to-end differentiable, ML-based recurrent geometric network (hereafter RGN1) <ref type="bibr" target="#b21">22</ref> that predicts protein structure from position-specific scoring matrices (PSSMs) derived from MSAs; related end-to-end approaches have since been reported <ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref> . RGN1 did not rely on the co-evolutionary information present in MSAs but a requirement for PSSMs necessitates that multiple homologous sequences be available. Here, we describe a new end-to-end differentiable system, RGN2 (Figure <ref type="figure" target="#fig_0">1</ref>), that predicts protein structure from single protein sequences by using a protein language model (AminoBERT). Such models were first developed for natural language processing (NLP) as a means to extract semantic information from a sequence of words <ref type="bibr" target="#b25">26</ref> . In the context of proteins, AminoBERT aims to capture the latent information in a string of amino acids that implicitly specifies protein structure. RGN2 also makes use of a natural way of describing polypeptide geometry that is inherently rotationally-and translationally-invariant at the level of the polypeptide as a whole. This involves using the Frenet-Serret formulas to embed a reference frame at each CÎ± carbon; the backbone is then easily constructed by a series of transformations. In this paper we describe the implementation and training of AminoBERT, the use of Frenet-Serret formulae in RGN2, and a performance assessment for natural and designed proteins with no significant sequence homologs. We find that RGN2 consistently outperforms AlphaFold2 (AF2) <ref type="bibr" target="#b7">8</ref> and RoseTTAFold (RF) <ref type="bibr" target="#b26">27</ref> on naturally occurring orphan proteins without known homologs, and is competitive on de novo designed proteins (RGN2 outperforms trRosetta in both cases). While RGN2 is not as performant as MSA-based methods for proteins that permit MSAs, it is up to six orders of magnitude faster, enabling efficient exploration of sequence and structure landscapes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RGN2 and AminoBERT models</head><p>RGN1 <ref type="bibr" target="#b21">22</ref> processed a protein PSSM through a recurrent neural network that implicitly learned PSSMstructure relationships. These relationships were parameterized as torsion angles between adjacent residues making it possible to sequentially position the protein backbone in 3D space (the backbone is described by ð, ð¶ ! , and ð¶ ! atoms). All RGN1 components were differentiable and the system could therefore be optimized from end to end to minimize prediction error (as measured by distance-based root mean squared deviation; dRMSD). The RGN2 model described here involves two primary innovations.</p><p>First, it uses amino acid sequence itself as the primary input as opposed to a PSSM, making it possible to predict structure from a single sequence. In the absence of a PSSM or MSA, latent information on the relationship between protein sequence (as a whole) and 3D structure is captured using a protein language model we term AminoBERT. Second, rather than describe the geometry of protein backbones as a sequence of torsion angles, RGN2 uses a simpler and more powerful approach based on the Frenet-Serret formulas; these formulas describe motion along a curve using the reference frame of the curve itself. This approach to protein geometry is inherently translationally-and rotationally-invariant. We optionally refine predicted structures using a Rosetta-based protocol <ref type="bibr" target="#b27">28</ref> that imputes the backbone and side-chain atoms.</p><p>Refinement is first performed in torsion space to optimize side-chain conformations and eliminate clashes and then in Cartesian space using quasi-Newton-based energy minimization. This protocol is nondifferentiable but improves the quality of predicted structures.</p><p>Language models were originally developed for natural language processing and operate on a simple but powerful principle: they acquire linguistic understanding by learning to fill in missing words in a sentence, akin to a sentence completion task in standardized tests. By performing this task across large text corpora, language models develop powerful reasoning capabilities. The Bidirectional Encoder Representations from Transformers (BERT) model 29 instantiated this principle using Transformers, a class of neural networks in which attention is the primary component of the learning system <ref type="bibr" target="#b29">30</ref> . In a Transformer, each token in the input sentence can "attend" to all other tokens through the exchange of activation patterns corresponding to the intermediate outputs of neurons in the neural network. In AminoBERT we utilize the same approach, substituting protein sequences for sentences and using amino acid residues as tokens. We train a 12-layer Transformer using ~250 million natural protein sequences obtained from the UniParc sequence database 31 . To enhance the capture of information in full protein sequences we introduce two training objectives not part of BERT or previously reported protein language models <ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref> . First, 2-8 contiguous residues are masked simultaneously in each sequence making the reconstruction task harder and emphasizing learning from global rather than local context. Second, chunk permutation is used to swap contiguous protein segments; chunk permutations preserve local sequence information but disrupt global coherence. Training AminoBERT to identify these permutations is another way of encouraging the Transformer to discover information from the protein sequence as whole. The AminoBERT module of RGN2 is trained independently of the geometry module in a self-supervised manner without fine-tuning (see Methods for details).</p><p>In RGN2 we parameterize backbone geometry using the discrete version of the Frenet-Serret formulas for one-dimensional curves <ref type="bibr" target="#b36">37</ref> . In this parameterization, each residue is represented by its ð¶ ! atom and an oriented reference frame centered on that atom. Local residue geometry is described by a single rotation matrix relating the preceding frame to the current one, which is the geometrical object that RGN2 predicts at each residue position. This rotationally-and translationally-invariant parameterization has two advantages over our use of torsion angles in RGN1. First, it ensures that specifying a single biophysical parameter, namely the sequential ð¶ ! â ð¶ ! distance of ~3.8Ã (which corresponds to a trans conformation) results in only physically-realizable local geometries. This overcomes a limitation of RGN1 which yielded chemically unrealistic values for some torsion angles. Second, it reduces by ~10-fold the computational cost of chain extension calculations, which often dominate RGN training and inference times (see Methods).</p><p>RGN2 training was performed using both the ProteinNet12 dataset <ref type="bibr" target="#b37">38</ref> and a smaller dataset comprised solely of single protein domains derived from the ASTRAL SCOPe dataset (v1.75) <ref type="bibr" target="#b38">39</ref> . Since we observed no detectable difference between the two, all results in this paper derive from the smaller dataset as it required less computing time to train.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Predicting structures of proteins with no homologs</head><p>To assess how well RGN2 predicts the structures of orphan proteins having no known sequence homologs, we compared it to AlphaFold2 (AF2) <ref type="bibr" target="#b7">8</ref> , RoseTTAFold (RF) <ref type="bibr" target="#b26">27</ref> , and trRosetta 7 , currently the best publicly available methods. As a test set, we used 222 orphan proteins that constitute a complete cluster in the Uniclust30 dataset <ref type="bibr" target="#b39">40</ref> (i.e., they have no homologs). Of these, 196 have structures available in the Protein Databank (PDB) <ref type="bibr" target="#b40">41</ref> but are not part of the training sets used for RGN2 or trRosetta (Methods). However, more than 95% of these sequences are included in the training sets for AF2 and RF, which may result in an overestimate of their accuracy. We predicted the structures of these orphan proteins using all methods and assessed accuracy with respect to experimentally-determined structures (Figure <ref type="figure" target="#fig_1">2A</ref>) using dRMSD and GDT_TS (the global distance test, which roughly captures the fraction of the structure that is correctly predicted). We found that RGN2 outperformed AF2, RF, and trRosetta on both metrics in 51%, 56%, and 63% of cases, respectively (these correspond to the top-left quadrant in Figure <ref type="figure" target="#fig_1">2B</ref> and Supplementary Figure <ref type="figure" target="#fig_1">2</ref>). In 10%, 21%, and 4% of cases, AF2, RF, and trRosetta outperformed RGN2 on both metrics, respectively; split results were obtained in the remaining cases. When we computed differences in error metrics among methods, we found that RGN2 outperformed AF2 (and RF; trRosetta) by an average ÎdRMSD of 1.85Ã (3.26Ã; 4.36Ã) and ÎGDT_TS of 3.51 (6.87; 10.21).</p><p>To investigate the basis for these differences in performance we determined the fraction of each secondary structure element (alpha-helix, beta-strand, hydrogen-bonded turn, 3/10-helix, bend, betabridge, and 5-helix) by applying the DSSP algorithm <ref type="bibr" target="#b41">42</ref> to the PDB structures in the orphan protein test set (Figure <ref type="figure" target="#fig_2">3A</ref> and Supplementary Figures <ref type="figure" target="#fig_3">2, 3, and 4</ref>). We found that RGN2 outperformed all other methods on proteins rich in bends and on hydrogen-bonded turns interspersing helices, while other methods-AF2 in particular-better predicted targets with high fractions of beta-strand and beta-bridges (such as hairpins). Performance on the remaining ~30% targets was split between RGN2 and competing methods (Figure <ref type="figure" target="#fig_1">2B</ref>). We also examined performance as a function of protein length and observed that RGN2 generally outperformed AF2 on longer proteins. One possible reason for these findings is that alpha helix formation is strongly driven by local sequence preferences, making helices more readily detectable from single sequences by RGN2.   <ref type="figure">(B-E</ref>) Alpha helical targets with bends or turns between helical domains tend to be better predicted by RGN2. AF2, as shown in 2KMP and 2AXZ, often predicts spurious beta strands for orphan proteins.</p><p>In Figure <ref type="figure" target="#fig_2">3B-E</ref> we show examples of structures for which RGN2 outperformed AF2. PDB structure 2MN9 (76% alpha helical) has two short alpha helices (Figure <ref type="figure" target="#fig_2">3B</ref>) connected by a hydrogenbonded turn and a polypeptide bend. RGN2 correctly predicts the challenging, less-structured bends and turns in this protein, yielding a 37-point gain in GDT_TS (ÎdRMSD &gt; 6Ã) over AF2. Similar trends were observed for structures 2KMP, 2KJF, and 2AXZ (Figure <ref type="figure" target="#fig_2">3C-E</ref>). 2AXZ is another protein with a single stranded helical bundle connected by bends, albeit one that is longer than 2KJF (305 residues as opposed to 60 residues). AF2 predicted the majority of helical domains in 2AXZ accurately, but a 24 amino acid fragment from Y17 through R40 was incorrectly predicted to be a beta-beta hairpin; in contrast, RGN2 correctly predicted this polypeptide sequence to comprise two helices connected by a bend. This contributed to a ~17-point gain in GDT_TS and 5.3Ã increase in dRMSD by RGN2 relative to AF2. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Predicting the structures of de novo (designed) proteins</head><p>To determine RGN2 accuracy on designed proteins we used a set of 35 proteins generated de novo using the Rosetta energy function; such proteins are expected to be well-suited to prediction by RF and trRosetta.</p><p>The designed proteins primarily have therapeutic applications ranging from anti-microbial activity (1D9J and 6CL3) to blocking of human potassium channels (1WQD). As before, we assessed prediction accuracy using dRMSD and GDT_TS. We found that RGN2 outperformed AF2, RF, and trRosetta on both metrics in 17%, 26%, and 45% of cases, respectively (Supplementary Figure <ref type="figure" target="#fig_4">5</ref>) but underperformed on both metrics in 49%, 54%, 7% of cases (Figure <ref type="figure" target="#fig_3">4</ref>). On average, AF2 exhibited the best overall performance; however, relative to RF, RGN2 exhibited a 3.5Ã better average dRMSD and marginally worse (lower) average GDT_TS of 3.2 (Figure <ref type="figure" target="#fig_3">4B</ref>). When we grouped proteins by secondary structure content (Figure <ref type="figure" target="#fig_4">5</ref>) we found that AF2 outperformed RGN2 for proteins such as 6D0T, 87% of which is a beta sheet arranged in a radially symmetric manner. In contrast, RGN2 outperformed AF2 (and RF) on structures such as 6TJ1 that comprise alpha helices held in place by hydrogen-bonded turns. We conclude that RGN2 can learn sequence-structure relationships for de novo regions of protein space, but that beta sheet prediction from single sequences remains a challenge. The superior performance of RGN2 relative to AF2 on a non-trivial fraction of de novo proteins also suggests that the two methods may be complementary and a hybrid model may outperform either one alone. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RGN2 prediction speed</head><p>Rapid prediction of protein structure is essential for tasks such as protein design and analysis of allelic variation or disease mutations. By virtue of being end-to-end differentiable, RGN2 predicts unrefined structures using fast neural network operations and does not require physics-based conformational sampling to assemble a folded chain. Moreover, by virtue of operating directly on single sequences, RGN2 avoids expensive MSA calculations. To quantify these benefits, we compared the speed of RGN2 and other methods on orphan and de novo proteins datasets of varying lengths (breaking down computation time by prediction stage; Table <ref type="table" target="#tab_0">1</ref>). In MSA-based methods, MSA generation scaled linearly with MSA depth (i.e., the number of homologous sequences used) whereas distogram prediction (by trRosetta) scaled quadratically with protein length. AF2 predictions scale cubically with protein length. In contrast, RGN2 scales linearly with protein length and both template-free and MSA-free implementations of AF2 and RF were &gt;10 5 fold slower than RGN2. In the absence of post-prediction refinement, RGN2 is up to 10 6 folds faster, even for relatively short proteins Moreover, predictions for 40 orphan targets (e.g., 2MN9 and 2MOA) failed to converge using MSA or template-free versions of RF, despite being less than 65 residues long on average. Adding physics-based refinement increases compute cost for all methods, but even so RGN2 remains the fastest available method. Of interest, even when MSA generation is discounted, neural network-based inference for AF2 and RF remains much slower than RGN2, inclusive of post-prediction refinement. This gap will only widen for design tasks involving longer proteins, as is increasingly becoming possible, and we interpret it to be a measure of the value of using a protein language model such as AminoBERT in a prediction algorithm. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DISCUSSION</head><p>RGN2 represents one of the first attempts to use ML to predict protein structure from a single sequence. This is computationally efficient and has many advantages in the case of orphan and designed proteins for which generation of multiple sequence alignment is not possible. RGN2 accomplishes this by fusing a protein language model (AminoBERT) with a simple and intuitive approach to describing CÎ± backbone geometry based on the Frenet-Serret formulation. Whereas most recent advances in ML-based structure prediction have relied on MSAs 5 , AminoBERT learns information from proteins as a whole without alignment and was trained to capture global structural properties by using sequences with masked residues and block permutations. We speculate that the latent space of the language model also captures recurrent evolutionary relationships <ref type="bibr" target="#b42">43</ref> . The use of Frenet-Serret formulas in RGN2 addresses the requirement that proteins exhibit translational and rotational invariance. From a practical standpoint, the speed and accuracy of RGN2 shows that language models are nearly as effective as MSAs at learning structural information from primary sequence while being able to extrapolate beyond known proteins, allowing for effective prediction of orphan and designed proteins.</p><p>Transformers and their embodiment of local and distant attention is a key feature of language models such as AminoBERT. Very large Transformer-based models trained on hundreds of millions and potentially billions of protein sequences are increasingly available <ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36</ref> and the scaling previously observed in natural language applications <ref type="bibr" target="#b43">44</ref> makes it likely that the performance of RGN2 and similar methods will continue to improve. AlphaFold2 also exploits attention mechanisms based on Transformers to capture the latent information in MSAs. Similarly, the self-supervised MSA Transformer 45 uses a related attention strategy that attends to both positions and sequences in an MSA, and achieves state-ofthe-art contact prediction accuracy. It is possible that language models will replace MSAs in general use but more likely are new architectures that merge the two. Data augmentation by learning from highconfidence structures in the newly reported AlphaFold Database 8 is expected to further improve performance. Finally, training on experimental data is likely to be invaluable in selected applications such as predicting structural variation within kinases or G protein-coupled receptors.</p><p>We consider RGN2 to be a first step in learning a direct sequence-to-structure map without a requirement for explicit evolutionary information. One limitation of RGN2 as currently implemented is that the immediate output of the recurrent geometric network only constrains local dependencies between CÎ± atoms (curvature and torsion angles) resulting in sequential reconstruction of backbone geometry.</p><p>Allowing the network to reason directly on arbitrary pairwise dependencies throughout the structure, and using a better inductive prior than learning immediate contacts may further improve the quality of model predictions. Furthermore, as currently implemented, refinement in RGN2 is not yet part of an end-to-end implementation; refinement via a 3D rotationally-and translationally-equivariant neural network would be more efficient and likely yield better quality structures.</p><p>It has been known since Anfinsen's refolding experiments that single polypeptide chains contain the information needed to specify fold <ref type="bibr" target="#b45">46</ref> . The demonstration that a language model can learn information on structure directly from protein sequences and then guide accurate prediction of an unaligned protein suggests that RGN2 behaves in a manner that is more similar to the actual folding process than MSAbased methods. Language models learned by deep neural networks are readily formulated in a maximum entropy framework <ref type="bibr" target="#b46">47</ref> and the physical process of protein folding is also entropically driven, potentially suggesting a means to compare the two. A fusion of biophysical and learning-based perspectives may ultimately prove the key to direct sequence-to-structure prediction from single polypeptides at the accuracy of experimental methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AminoBERT summary</head><p>AminoBERT is a 12-layer Transformer where each layer is composed of 12 attention heads. It is trained to distill protein sequence semantics from ~260 million natural protein sequences obtained from the UniParc sequence database 31 (downloaded May 19, 2019).</p><p>During training each sequence is fed to AminoBERT according to the following algorithm:</p><p>1. With probability 0.3 select sequence for chunk permutation, and with probability 0.7 select sequence for masked language modeling. 2. If sequence was selected for chunk permutation, then:</p><p>With probability 0.35 chunk permute, else (with probability 0.65) leave the sequence unmodified. 3. Else if the sequence was selected for masked language modeling, then: With probability 0.3 introduce 0.15 Ã sequence_length masks into the sequence with clumping, else (with probability 0.7) introduce the same number of masks into the sequence randomly across the length of the sequence (standard masked language modeling).</p><p>The loss for an individual sequence (seq) is given by:</p><formula xml:id="formula_0">ð¿ðð ð (ð ðð) = ð¼[ð ðð ðð  ðâð¢ðð ððððð¢ð¡ðð] Ã ðâð¢ðð_ððððð¢ð¡ðð¡ððð_ððð ð (ð ðð) + (1 â ð¼[ð ðð ðð  ðððððððð¦ ðððð¡ð¢ðððð] Ã ððð ððð_ðð_ððð ð (ð ðð)</formula><p>where I[x] is the indicator of the event x, and returns 1 if x is true, and 0 if x is false. Chunk_permutation_loss(seq) is a standard cross entropy loss reflecting the classification accuracy of predicting whether seq has been chunk permuted. Finally, masked_lm_loss(seq) is the standard masked language modeling loss as previously described in Devlin et al. <ref type="bibr" target="#b28">29</ref> . Note, that mask clumping does not affect how the loss is calculated.</p><p>Chunk permutation is performed by first sampling an integer x uniformly between 2 and 10, inclusively. The sequence is then randomly split into x equal-sized fragments, which are subsequently shuffled and rejoined.</p><p>Mask clumping is performed as follows:</p><p>1. Sample an integer clump_size ~ Poisson (2.5) + 1 2. Let n_mask = 0.15 Ã sequence_length. Randomly select n_mask/clump_size positions in the sequence around which to introduce a set of clump_size contiguous masks</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AminoBERT architecture</head><p>Each multi-headed attention layer in AminoBERT contains 12 attention heads, each with hidden size 768. The output dimension of the feed-forward unit at the end of each attention layer is 3072. As done in BERT <ref type="bibr" target="#b28">29</ref> , we prepend a [CLS] token at the beginning of each sequence, for which an encoding is maintained through all layers of the AminoBERT Transformer. Each sequence was padded or otherwise clipped to length 1024 (including the [CLS] token).</p><p>For chunk permutation classification, the final hidden vector of the [CLS] token is fed through another feed forward layer of output dimension 768, followed by a final feed forward layer of output dimension 2, which are the logits corresponding to whether the sequence is chunk permuted or not. Masked language modeling loss calculations are set up as described in Devlin et al. <ref type="bibr" target="#b28">29</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AminoBERT training procedure</head><p>AminoBERT was trained with batch size 3072 for 1,100,000 steps, which is approximately 13 epochs over the 260 million sequence corpus. For our optimizer we used Adam with a learning rate of 1e-4, Î²1 = 0.9, Î²2 = 0.999, epsilon=1e-6, L2 weight decay of 0.01, learning rate warmup over the first 20,000 steps, and linear decay of the learning rate. We used a dropout probability of 0.1 on all layers, and used GELU activations as done for BERT. Training was performed on a 512 core TPU pod for approximately one week.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Geometry module</head><p>The geometry of the protein backbone as summarized by the ð¶ ! trace can be thought of as a onedimensional discrete open curve, characterized by a bond and torsion angle at each residue. Following Niemi et al. <ref type="bibr" target="#b36">37</ref> , the starting point for describing such discrete curves is to assign a frame, a triplet of orthonormal vectors, to each ð¶ ! atom. If we denote by ð " the vector characterizing the position of a ð¶ ! atom at the ð-th vertex, we could then define a unit tangent vector along an edge connecting two consecutives ð¶ ! atoms</p><formula xml:id="formula_1">ð¡ " = ð "#$ â ð " |ð "#$ â ð " |</formula><p>For assigning frames to each ð-th ð¶ ! atom, we need two extra vectors, the binormal and normal vectors defined as follows:</p><formula xml:id="formula_2">ð " = ð¡ "%$ Ã ð¡ " |ð¡ "%$ Ã ð¡ " | ð " = ð " Ã ð¡ "</formula><p>While for a protein (in a given orientation) the tangent vector is uniquely defined, the normal and binormal vectors are arbitrary. Indeed, when assigning frames to each residue we could take any arbitrary orthogonal basis on the normal plane to the tangent vector. Such arbitrariness does not affect our strategy of predicting 3D structures starting from bond and torsion angles.</p><p>To derive the equivalent of the Frenet-Serret formulas-which describe the geometry of continuous and differentiable one-dimensional curves-for the discrete case, we need to relate two consecutive frames along the protein backbone in terms of rotation matrices</p><formula xml:id="formula_3">E ð "#$ ð "#$ ð¡ "#$ F= â "#$," E ð " ð " ð¡ "</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F</head><p>In three-dimensions, rotation matrices are in general parametrized in term of three Euler angles. However, in our case the rotation matrices relating two consecutive frames are fully characterized by only two angles, a bond angle ð and a torsion angle ð, as the third Euler angle vanishes, reflecting the following condition ð "#$ . ð¡ " = 0. We can now write the equivalent of the Frenet-Serret formulas for the discrete case </p><formula xml:id="formula_4">E ð "#$ ð "#$ ð¡ "#$ F=E cos ð</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F</head><p>The bond and torsion angles are defined by the following relations cos ð "#$," = ð¡ "#$ . ð¡ " cos ð "#$," = ð "#$ . ð "</p><p>We now turn to backbone reconstruction starting from bond and torsion angles. First, using tangent vectors along the backbone edges, we can reconstruct all ð¶ ! atom positions, and thus the full protein backbone in the ð¶ ! trace, by using the following relation:</p><formula xml:id="formula_5">ð ( = Q|ð "#$ â ð " | . ð¡ " (%$ ")*</formula><p>where |ð "#$ â ð " | is the length of the virtual bonds connecting two consecutive ð¶ ! atoms. In most cases, the average virtual bond length is â¼ 3.8 Ã, which corresponds to trans conformations. In terms of the familiar torsion angles ð, ð, and ð, those conformations are achieved for ð â¼ ð. For cis conformations, mainly involving proline residues, the virtual bond length is â¼ 3.0 Ã (and it corresponds to ð â¼ 0). In RGN2, for backbone reconstruction, we impose the condition that the virtual bond length is strictly equal to 3.8 Ã, and for reconstructing the backbone we use the following relation:</p><formula xml:id="formula_6">ð ( = Q 3.8 Ã ð¡ " (%$ ")*</formula><p>The intuition behind the previous equation is the idea of a moving observer along the protein backbone. We could think of the tangent vector ð¡ " as the velocity of the observer along a given edge, and the constant virtual bond length as the effective time spent for travelling along the edge. The only freedom allowed for such observer is to abruptly change the direction of the velocity vector at each vertex.</p><p>The model outputs bond and torsion angles. By centering the first ð¶ ! atom of the protein backbone at the origin of our coordinate system, we sequentially reconstruct all the ð¶ ! atom coordinates using the following relation:</p><formula xml:id="formula_7">Z ð "#$ ð "#$ ð¡ "#$ ð "#$ [=Z â "#$," 0 0 0 0 0 3.8 1 [ Z ð " ð " ð¡ " ð " [</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data preparation for comparison with trRosetta</head><p>Performance of RGN2 was compared against trRosetta across two sets of non-homologous proteins: (i) 196 orphans from the Uniclust30 database <ref type="bibr" target="#b39">40</ref> , and (ii) 35 de novo proteins by Xu et al. <ref type="bibr" target="#b47">48</ref> . Both sets were filtered to ensure no overlap with the training sets of RGN2 and trRosetta. While RGN2 is trained on the ASTRAL SCOPe (v1.75) dataset <ref type="bibr" target="#b38">39</ref> , trRosetta was trained on a set of 15,051 single chain proteins (released before May 1, 2018).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Structure prediction with trRosetta, AF2, and RF</head><p>Conventional trRosetta-based structure prediction involves first feeding the input sequence through a deep MSA generation step. For orphans and de novo proteins without any sequence homologs, the MSA only includes the original query sequence. Next, the MSA is used by the trRosetta neural network to predict a distogram (and orientogram) that captures inter-residue (Cð¼-Cð¼ and CÎ²-CÎ²) distances and orientations. This information is subsequently utilized by a final Rosetta-based refinement module. This module first threads a naÃ¯ve sequence of polyalanines of length equaling the target protein that maximally obeys the distance and orientation constraints. After side-chain imputation that reflects the original sequence, multiple steps including clash elimination, rotamer repacking, and energy minimization are performed to identify the lowest energy structure.</p><p>AF2 and RF predictions did not require MSAs since our target proteins don't have homologs and so we made our predictions using their respective official Google Colab notebooks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Structure refinement in RGN2</head><p>Raw predictions from RGN2 contain a single Cð¼ trace of the target protein. After performing a local internal coordinate building step to generate the backbone and side-chain atoms corresponding to the target sequence, we use Rosetta-based refinement to finetune the structure. This refinement comprises hybrid optimization of side chains using five invocations of energy minimization in torsional space followed by a single step of quasi-Newton all-atom minimization in Cartesian space (using the FastRelax protocol of RosettaScripts <ref type="bibr" target="#b48">49</ref> ). An optional CartesianSampler 49 mover step can be added to further correct local strain density in the predicted model. The six-step FastRelax protocol is repeated for 300 cycles for each target. Finally, 100 cycles of coarse-grained, fast minimization using MinMover 49 is applied to obtain the predicted structure.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Organization and application of RGN2. RGN2 combines a Transformer-based protein language model (AminoBERT) with a recurrent geometric network that utilizes Frenet-Serret frames to generate the backbone structure of a protein. Placement of side chain atoms and refinement of hydrogenbonded networks are subsequently performed using the Rosetta energy function.</figDesc><graphic url="image-1.png" coords="4,54.00,157.49,504.00,284.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. (A) Absolute performance metrics for RGN2 (purple), AF2 (green), and RF (pink) across 196 orphan proteins lacking known homologs. Outliers in RGN2 plot correspond to short targets where a few poorly predicted residues substantially lower GDT_TS. (B) Differences in prediction accuracy between RGN2 and AF2 / RF are shown for the 196 orphan proteins, using dRMSD and GDT_TS as metrics. RF failed to converge for 40 targets, yielding no predictions. Points in top-left quadrant correspond to targets with negative ÎdRMSD and positive ÎGDT_TS, i.e., where RGN2 outperforms the competing method on both metrics, and vice-versa for the bottom-right quadrant. The other two quadrant (white) indicate targets where there is no clear winner as the two metrics disagree.</figDesc><graphic url="image-2.png" coords="8,95.00,54.00,421.70,515.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. (A) Stacked bar chart shows the relative fractions of secondary structure elements in orphan proteins where RGN2 outperforms AF2 (the 18 proteins on which AF2 outperforms RGN2 are shown in the gray inset). Bar height indicates protein length.(B-E) Alpha helical targets with bends or turns between helical domains tend to be better predicted by RGN2. AF2, as shown in 2KMP and 2AXZ, often predicts spurious beta strands for orphan proteins.</figDesc><graphic url="image-3.png" coords="9,54.00,54.00,504.00,390.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. (A) Absolute performance metrics for RGN2 (purple), AF2 (green), and RF (pink) across 35 de novo designed proteins with no known homologs. (B) Differences in prediction accuracy between RGN2 and AF2/RF are shown for these 35 proteins, using dRMSD and GDT_TS as metrics. Points in top-left quadrant correspond to targets with negative ÎdRMSD and positive ÎGDT_TS, i.e., where RGN2 outperforms the competing method on both metrics, and vice-versa for the bottom-right quadrant. The other two quadrant (white) indicate targets where there is no clear winner as the two metrics disagree.</figDesc><graphic url="image-4.png" coords="10,113.00,95.40,386.00,471.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. (A) Stacked bar chart shows the relative fractions of secondary structure elements in 35 de novo proteins. Bar height indicates protein length. AF2 outperforms RGN2 on proteins with high beta-sheet content. (B) Alpha helical target 6TJ1 does not have a bend, but AF2 predicts a spurious bend while the RGN2 prediction is closer to the experimental structure. (C) 6D0T, a combination of an hour-glass shaped beta barrel connected by disordered loops, is better predicted by AF2.</figDesc><graphic url="image-5.png" coords="12,54.00,54.00,504.00,507.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison of prediction times between RGN2 and AF2, RF, and trRosetta across 231 targets spanning our orphan and de novo protein datasets. RGN2 predictions were performed in batches with maximum permissible batch size set to 128 targets. The trRosetta MSA generation step was not used since none of the targets had known homologous proteins.</figDesc><table><row><cell>Protein length (L) bins (# residues)</cell><cell>Total targets</cell><cell>Mean protein length (# residues)</cell><cell cols="2">Mean trRosetta prediction time per structure (s)</cell><cell>Mean AF2 prediction time (s)</cell><cell>Mean RF prediction time (s)</cell><cell>Mean RGN2 prediction time (ms)</cell><cell>Mean RGN2 prediction + refinement time (s)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Distogram</cell><cell>3D-Structure</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0 &lt; ð¿ â¤ 100</cell><cell>124</cell><cell>33.35</cell><cell>1868</cell><cell>1021</cell><cell>227.8</cell><cell>346.2</cell><cell>2.6</cell><cell>140.36</cell></row><row><cell>100 &lt; ð¿ â¤ 200</cell><cell>69</cell><cell>132.68</cell><cell>2762</cell><cell>1845</cell><cell>252.2</cell><cell>376.3</cell><cell>2.5</cell><cell>169.87</cell></row><row><cell>200 &lt; ð¿ â¤ 300</cell><cell>18</cell><cell>240.27</cell><cell>2940</cell><cell>1698</cell><cell>221.6</cell><cell>389.4</cell><cell>3.6</cell><cell>178.41</cell></row><row><cell>300 &lt; ð¿ â¤ 400</cell><cell>10</cell><cell>328.8</cell><cell>3511</cell><cell>2135</cell><cell>226.9</cell><cell>388.6</cell><cell>5.1</cell><cell>202.31</cell></row><row><cell>400 &gt; ð¿</cell><cell>10</cell><cell>457.3</cell><cell>4003</cell><cell>2977</cell><cell>235.1</cell><cell>386.8</cell><cell>5.3</cell><cell>226.23</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements:</head><p>We gratefully acknowledge the support of the NVIDIA Corporation for the donation of GPUs used for this research. This work is supported by the DARPA PANACEA program grant HR0011-19-2-0022 and NCI grant U54-CA225088 to PKS.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Competing interests: M.A. is a member of the SAB of FL2021-002, a Foresite Labs company, and consults for Interline Therapeutics. P.K.S. is a member of the SAB or Board of Directors of Glencoe Software, Applied Biomath, RareCyte and NanoString and has equity in several of these companies. A full list of G.M.C.'s tech transfer, advisory roles, 559 and funding sources can be found on the lab's website: http://arep.med.harvard.edu/gmc/tech.html. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">New development for protein structure and function predictions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I-Tasser</forename><surname>Server</surname></persName>
		</author>
		<idno type="DOI">10.1093/nar/gkv342</idno>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Automatic atom type and bond type perception in molecular mechanical calculations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Kollman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Case</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mol. Graph. Model</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="247" to="260" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Algorithms for highly efficient, load-balanced, and scalable molecular simulation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kutzner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Van Der Spoel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lindahl</surname></persName>
		</author>
		<author>
			<persName><surname>Grgmacs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Chem. Theory Comput</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="435" to="447" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The Rosetta All-Atom Energy Function for Macromolecular Modeling and Design</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Alford</surname></persName>
		</author>
		<idno type="DOI">10.1021/acs.jctc.7b00125</idno>
	</analytic>
	<monogr>
		<title level="j">J. Chem. Theory Comput</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Machine learning in protein structure prediction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Alquraishi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>Elsevier Enhanced Reader</publisher>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Improved protein structure prediction using potentials from deep learning. | Nat</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1923">1923</date>
			<biblScope unit="volume">577</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Improved protein structure prediction using predicted interresidue orientations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Natl. Acad. Sci. U. S. A</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="page" from="1496" to="1503" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Highly accurate protein structure prediction with AlphaFold</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jumper</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41586-021-03819-2</idno>
	</analytic>
	<monogr>
		<title level="j">Nat</title>
		<imprint>
			<date type="published" when="2021">2021 1-11 (2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An introduction to sequence similarity (&apos;homology&apos;) searching</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Pearson</surname></persName>
		</author>
		<idno type="DOI">10.1002/0471250953.bi0301s42</idno>
	</analytic>
	<monogr>
		<title level="j">Curr. Protoc. Bioinforma</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unexpected features of the dark proteome</title>
		<author>
			<persName><forename type="first">N</forename><surname>PerdigÃ£o</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.1508380112</idno>
	</analytic>
	<monogr>
		<title level="j">Proc. Natl. Acad. Sci. U. S. A</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A wellness study of 108 individuals using personal, dense, dynamic data clouds</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Price</surname></persName>
		</author>
		<idno type="DOI">10.1038/nbt.3870</idno>
	</analytic>
	<monogr>
		<title level="j">Nat. Biotechnol</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Genomic architecture of inflammatory bowel disease in five families with multiple affected individuals</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Stittrich</surname></persName>
		</author>
		<idno type="DOI">10.1038/hgv.2015.60</idno>
	</analytic>
	<monogr>
		<title level="j">Hum. Genome Var</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">EvoEF2: accurate and fast energy function for computational protein design</title>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pearce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/btz740</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">De novo computational design of retro-aldol enzymes</title>
		<author>
			<persName><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.1152692</idno>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="page">80</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Expanding the enzyme universe: Accessing non-natural reactions by mechanism-guided directed evolution</title>
		<author>
			<persName><forename type="first">H</forename><surname>Renata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">H</forename><surname>Arnold</surname></persName>
		</author>
		<idno type="DOI">10.1002/anie.201409470</idno>
	</analytic>
	<monogr>
		<title level="j">Angewandte Chemie -International Edition</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">De novo enzyme design using Rosetta3</title>
		<author>
			<persName><forename type="first">F</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Leaver-Fay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Khare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bjelic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Baker</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0019230</idno>
	</analytic>
	<monogr>
		<title level="j">PLoS One</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recent advances in rational approaches for enzyme engineering</title>
		<author>
			<persName><forename type="first">K</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schwab</surname></persName>
		</author>
		<idno type="DOI">10.5936/csbj.201209010</idno>
	</analytic>
	<monogr>
		<title level="j">Comput. Struct. Biotechnol. J</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improving the pH-stability of versatile peroxidase by comparative structural analysis with a naturally-stable manganese peroxidase</title>
		<author>
			<persName><forename type="first">V</forename><surname>SÃ¡ez-JimÃ©nez</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0140984</idno>
	</analytic>
	<monogr>
		<title level="j">PLoS One</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Prediction of the solvent affecting site and the computational design of stable Candida antarctica lipase B in a hydrophilic organic solvent</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">J</forename><surname>Yoo</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jbiotec.2012.11.006</idno>
	</analytic>
	<monogr>
		<title level="j">J. Biotechnol</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An orphan protein of Fusarium graminearum modulates host immunity by mediating proteasomal degradation of TaSnRK1Î±</title>
		<author>
			<persName><forename type="first">C</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Commun</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The evolutionary origin of orphan genes</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tautz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Domazet-LoÅ¡o</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Genetics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="692" to="702" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">End-to-End Differentiable Learning of Protein Structure</title>
		<author>
			<persName><forename type="first">M</forename><surname>Alquraishi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell Syst</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">e3</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning protein structure with a differentiable simulator</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ingraham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Riesselman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Marks</surname></persName>
		</author>
		<idno>ICLR 2019</idno>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Universal Transforming Geometric Network</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Ultrafast end-to-end protein structure prediction enables high-throughput exploration of uncharacterised proteins</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Kandathil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Greener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">T</forename><surname>Jones</surname></persName>
		</author>
		<idno type="DOI">10.1101/2020.11.27.401232</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rives</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Natl. Acad. Sci. U. S. A</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Accurate prediction of protein structures and interactions using a three-track neural network</title>
		<author>
			<persName><forename type="first">M</forename><surname>Baek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">8754</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Relaxation of backbone bond geometry improves protein energy landscape modeling</title>
		<author>
			<persName><forename type="first">P</forename><surname>Conway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Tyka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Dimaio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Konerding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Baker</surname></persName>
		</author>
		<idno type="DOI">10.1002/pro.2389</idno>
	</analytic>
	<monogr>
		<title level="j">Protein Sci</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><surname>Bert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL HLT 2019 -2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies -Proceedings of the Conference</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Attention Is All You Need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">UniProt archive</title>
		<author>
			<persName><forename type="first">R</forename><surname>Leinonen</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/bth191</idno>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Language models enable zero-shot prediction of the effects of mutations on protein function</title>
		<author>
			<persName><forename type="first">J</forename><surname>Meier</surname></persName>
		</author>
		<idno type="DOI">10.1101/2021.07.09.450648</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">CodeTrans: Towards Cracking the Language of Silicone&apos;s Code Through Self-Supervised Deep Learning and High Performance Computing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Elnaggar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unified rational protein engineering with sequence-only deep representation learning</title>
		<author>
			<persName><forename type="first">E</forename><surname>Alley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Khimulya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alquraishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Church</surname></persName>
		</author>
		<idno type="DOI">10.1101/589333</idno>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<biblScope unit="volume">589333</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Modeling the language of life -Deep learning protein sequences</title>
		<author>
			<persName><forename type="first">M</forename><surname>Heinzinger</surname></persName>
		</author>
		<idno type="DOI">10.1101/614313</idno>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">ProGen: Language modeling for protein generation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Madani</surname></persName>
		</author>
		<idno type="DOI">10.1101/2020.03.07.982272</idno>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Discrete Frenet frame, inflection point solitons, and curve visualization with applications to folded proteins</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lundgren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Niemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. E -Stat. Nonlinear, Soft Matter Phys</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">ProteinNet: A standardized data set for machine learning of protein structure</title>
		<author>
			<persName><forename type="first">M</forename><surname>Alquraishi</surname></persName>
		</author>
		<idno type="DOI">10.1186/s12859-019-2932-0</idno>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">SCOPe: Structural Classification of Proteins -Extended, integrating SCOP and ASTRAL data and classification of new structures</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Brenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Chandonia</surname></persName>
		</author>
		<idno type="DOI">10.1093/nar/gkt1240</idno>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Uniclust databases of clustered and deeply annotated protein sequences and alignments</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mirdita</surname></persName>
		</author>
		<idno type="DOI">10.1093/nar/gkw1081</idno>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">RCSB Protein Data Bank: Powerful new tools for exploring 3D structures of biological macromolecules for basic and applied research and education in fundamental biology, biomedicine, biotechnology, bioengineering and energy sciences</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Burley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="D437" to="D451" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A series of PDB-related databanks for everyday needs</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">G</forename><surname>Touw</surname></persName>
		</author>
		<idno type="DOI">10.1093/nar/gku1028</idno>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Transformer protein language models are unsupervised structure learners</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ovchinnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rives</surname></persName>
		</author>
		<idno type="DOI">10.1101/2020.12.15.422761</idno>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Scaling laws for neural language models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<idno>arXiv</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">R</forename><surname>Rao</surname></persName>
		</author>
		<idno type="DOI">10.1101/2021.02.12.430858</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">The kinetics of formation of native ribonuclease during oxidation of the reduced polypeptide chain</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Anfinsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">H</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Natl. Acad. Sci. U. S. A</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="1309" to="1314" />
			<date type="published" when="1961">1961</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Strategies for Training Large Scale Neural Network Language Models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Improved protein structure prediction by deep learning irrespective of co-evolution information</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mcpartlon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1101/2020.10.12.336859</idno>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Rosettascripts: A scripting language interface to the Rosetta Macromolecular modeling suite</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Fleishman</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0020161</idno>
	</analytic>
	<monogr>
		<title level="j">PLoS One</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
