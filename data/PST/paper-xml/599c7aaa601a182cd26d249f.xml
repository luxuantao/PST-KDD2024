<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Detecting Anomalous Events in Videos by Learning Deep Representations of Appearance and Motion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2016-10-26">October 26, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Dan</forename><surname>Xu</surname></persName>
							<email>dan.xu@unitn.it</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Trento</orgName>
								<address>
									<settlement>Trento</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yan</forename><surname>Yan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Trento</orgName>
								<address>
									<settlement>Trento</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Fondazione Bruno Kessler (FBK)</orgName>
								<address>
									<settlement>Trento</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Trento</orgName>
								<address>
									<settlement>Trento</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Computer Vision and Image Understanding</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Detecting Anomalous Events in Videos by Learning Deep Representations of Appearance and Motion</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2016-10-26">October 26, 2016</date>
						</imprint>
					</monogr>
					<idno type="MD5">E1A4E5E400F1BC792098D16DE64528CD</idno>
					<idno type="DOI">10.1016/j.cviu.2016.10.010</idno>
					<note type="submission">Received date: 15 December 2015 Revised date: 28 September 2016 Accepted date: 19 October 2016</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Video surveillance</term>
					<term>abnormal event detection</term>
					<term>unsupervised learning</term>
					<term>stacked denoising auto-encoders</term>
					<term>feature fusion</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Anomalous event detection is of utmost importance in intelligent video surveillance. Currently, most approaches for the automatic analysis of complex video scenes typically rely on hand-crafted appearance and motion features. However, adopting user defined representations is clearly suboptimal, as it is desirable to learn descriptors specific to the scene of interest. To cope with this need, in this paper we propose Appearance and Motion DeepNet (AMDN), a novel approach based on deep neural networks to automatically learn feature representations. To exploit the complementary information of both appearance and motion patterns, we introduce a novel double fusion framework, combining the benefits of traditional early fusion and late fusion strategies. Specifically, stacked denoising autoencoders are proposed to separately learn both appearance and motion features as well as a joint representation (early fusion). Then, based on the learned features, multiple one-class SVM models are used to predict the anomaly scores of each input. Finally, a novel late fusion strategy is proposed to combine the computed scores and detect abnormal events. The proposed ADMN is extensively evaluated on publicly available video surveillance datasets, showing competitive performance with respect to state of the art approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Highlights</head><p>• To the best of our knowledge, this paper represents the first attempt to address the anomalous event detection task using deep learning architectures. In this way, discriminative feature representations are automatically learned for the scene of interest, showing significant advantages over previous methods based on hand-crafted features.</p><p>• The proposed approach for learning feature representations combines appearance and motion information. Deep learning methods for fusing multiple modalities have been investigated in previous works. However, none of these works consider the problem of anomaly detection in intelligent video surveillance.</p><p>• A novel double fusion scheme is proposed to integrate appearance and motion deep repre-sentations for detecting unusual activities in video surveillance streams.</p><p>• We carried out an extensive evaluation of the proposed approach on three publicly available datasets, namely UCSD (Ped1 and Ped2), Subway and Train, and our approach yields very competitive performance with respect to state of the art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In the last few years the massive deployment of distributed camera systems in public spaces has increased the need for advanced tools performing the automatic analysis of video surveillance streams. A fundamental challenge in intelligent video surveillance is to automatically detect anomalous events in complex and crowded scenes. This problem has attracted considerable attention in the computer vision research community <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>.</p><p>Early works in the literature are based on the analysis of individual moving objects in the scene <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>. First, visual tracking is performed to compute the trajectories of the targets and a model is learned describing typical activities. Then, anomalous events are identified by looking at patterns which distinctly diverge from the model. However, these methods are not suitable for analyzing complex scenes, as the accuracy of visual tracking algorithms significantly degrades in case of several occluded targets. Therefore, more recently, unsupervised non-object centric approaches have gained popularity <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>. These methods address the anomaly detection task by analyzing the cooccurence of atomic spatio/temporal patterns and are based on hand-crafted features extracted from low-level appearance and motion cues. Commonly used low-level features include histogram of oriented gradients (HOG), 3D spatio-temporal gradient, histogram of optical flow (HOF). However, adopting generic user defined features is a clear limitation of these approaches and improved performance can be obtained by learning scene specific descriptors.</p><p>Recently, deep learning approaches have been successfully used to tackle various computer vision tasks, such as object classification <ref type="bibr" target="#b13">[14]</ref>, object detection <ref type="bibr" target="#b14">[15]</ref> and activity recognition <ref type="bibr" target="#b15">[16]</ref>. While these works mostly focus on supervised learning tasks and Convolutional Neural Networks, unsupervised approaches have also gained popularity. In particular, autoencoder networks <ref type="bibr" target="#b16">[17]</ref> have been investigated to address fundamental tasks such as object tracking <ref type="bibr" target="#b17">[18]</ref> and face align- ment <ref type="bibr" target="#b18">[19]</ref>. In both scenarios, improved performance over traditional methods can be achieved, since using deep architectures rich and discriminative features can be learned via multi-layer nonlinear transformations.</p><p>Following this intuition, in this paper we propose a novel approach for detecting anomalous activities in complex video surveillance scenes. Opposite to previous works <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b12">13]</ref> which rely on hand-crafted features to model spatio/temporal activity patterns, we propose to learn discriminative feature representations in a fully unsupervised manner adopting stacked denoising autoencoders (SDAE) <ref type="bibr" target="#b19">[20]</ref>. Figure <ref type="figure" target="#fig_0">1</ref> shows an overview of the proposed method, named Appearance and Motion DeepNet (AMDN). Our AMDN is based on a novel double fusion scheme (integrating both traditional early fusion and late fusion strategies) for combining low-level features of appearance and motion. Specifically, in the first phase, still image patches and optical flow fields are provided as input to two separate SDAE networks, to learn appearance and motion features, respectively. A third SDAE is used to learn a joint representation of appearance and motion from the concatenation of image pixels and the corresponding optical flow (early fusion). In the second phase, multiple one-class SVM models, corresponding to the learned feature representations, are used to compute a set of anomaly scores. Then, a novel late fusion scheme is proposed to combine the computed scores for abnormal event prediction. The proposed AMDN is evaluated on three challenging video surveillance datasets and compared with several state of the art methods. Our experiments clearly demonstrate the effectiveness of the proposed double fusion framework as well as the importance of learning features with SDAEs.</p><p>To summarize, the main contributions of this work are threefold:</p><p>• To the best of our knowledge, this paper represents the first attempt to address the anomalous event detection task using deep learning architectures. In this way, discriminative feature representations can be automatically learned for the scene of interest, showing significant advantages over previous methods based on hand-crafted features.</p><p>• Our AMDN learns appearance and motion features as well as their correlations. Deep learning methods for combining multiple modalities have been investigated in previous works <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>. However, none of these works consider the anomaly detection task.</p><p>• A double fusion scheme is proposed to combine appearance and motion features. The advantages of combining early and late fusion approaches have been demonstrated in previous works <ref type="bibr" target="#b22">[23]</ref>. However in <ref type="bibr" target="#b22">[23]</ref> the authors did not consider a deep learning framework, neither the problem of discovering unusual activities in video surveillance streams.</p><p>This paper extends our previous work in <ref type="bibr" target="#b23">[24]</ref>. Specifically, with respect to <ref type="bibr" target="#b23">[24]</ref> in this paper we added a section discussing related work on abnormal video event detection and on learning deep representations in unsupervised settings (Section 2). Moreover, in Section 3 we provide further insights on the proposed AMDN framework, enriching the descriptions of the main components of our systems (SDAEs -Section 3.2.2, one-class</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>SVM -Section 3.3.1) and introducing a novel late fusion strategy based on p -norm (Section 3.3.2). Finally, we significantly expanded the experimental evaluation, adding results on a third publicly available dataset and performing a detailed analysis of the different components of our method (Section 4).</p><p>In the rest of this paper, we first review the related work in Section 2. Then, we introduce the proposed AMDN framework in Section 3, describing the AMDN structure in detail and the approach we used for training our SDAEs. Finally, the proposed method is evaluated extensively and the experimental results are presented in Section 4. Conclusions are drawn in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section we review previous works considering: (i) the addressed task, i.e. abnormal video event detection and (ii) deep learning approaches in unsupervised settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Abnormal Event Detection in Videos</head><p>Existing techniques tackling the abnormal video event detection are extensively reviewed in <ref type="bibr" target="#b24">[25]</ref>. These methods can be mostly partitioned in two categories depending on the types of event representations adopted, namely trajectory-based methods and non-object centric methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1.">Trajectory-based Methods</head><p>Trajectories are widely used features for abnormal video event detection <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b5">6]</ref>, due to their ability of describing synthetically the dynamic information of foreground objects. Trajectory-based methods usually rely on two phases. First, visual tracking algorithms are used to estimate the motion of the objects and the people in the scene. Then, features representing the trajectories of the targets are employed to construct statistical models describing typical activities. In the second phase, the activities corresponding to trajectories which deviate significantly from the learned model are identified as anomalous <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>.</p><p>A pioneering work on this research line is <ref type="bibr" target="#b29">[29]</ref>, where object trajectories are modeled using probability density functions. In <ref type="bibr" target="#b30">[30]</ref> Hu et al. developed a multiple objects tracking algorithm to collect trajectories, which which are then used to learn statistical distributions. Both spatial and temporal information are considered for anomaly detection. Markris and Ellis <ref type="bibr" target="#b31">[31]</ref> proposed a Bayesian approach for detecting abnormal trajectories based on annotated scene semantics. Jiang et al. <ref type="bibr" target="#b32">[32]</ref> introduced a dynamic hierarchical clustering framework for trajectory grouping using Hidden Markov Models (HMMs) to represent each group of trajectories.</p><p>In general, trajectory-based methods guarantee satisfactory performance when foreground objects are easy to detect and track, e.g. in indoor environments or when there are few targets in the scene. On the contrary, performance significantly degrades in unconstrained scenarios (e.g. in case of dense crowds), when the several occlusions make traditional tracking and detection approaches not reliable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2.">Non-object Centric Methods</head><p>This category of approaches address the anomaly detection task by learning representative activity patterns from behavior-related attributes of objects and people within spatial/temporal contexts. Commonly considered behavioral attributes include size, gradient, direction and speed of the targets in the scene, which are described with low-level representations such as HOG <ref type="bibr" target="#b33">[33]</ref>, 3D spatio-temporal gradient <ref type="bibr" target="#b34">[34]</ref>, HOF <ref type="bibr" target="#b35">[35]</ref> and dense spatial-temporal interest points (Dense STIPs) <ref type="bibr" target="#b36">[36]</ref>.</p><p>Cong et al. <ref type="bibr" target="#b3">[4]</ref> employed multi-scale histograms of optical flow and a sparse coding model and used the reconstruction error as a metric for outlier detection. Mehran et al. <ref type="bibr" target="#b8">[9]</ref> proposed a "social force" model based on optical flow features to represent crowd activity patterns and identify anomalous activities. In <ref type="bibr" target="#b9">[10]</ref> co-occurrence statistics of spatio-temporal events are adopted in combination with Markov Random Fields (MRFs) to discover unusual activities. Spatio-temporal MRFs are also employed in <ref type="bibr" target="#b37">[37]</ref>. Multiple spatiotemporal filters at different scales and local feature descriptors are considered in <ref type="bibr" target="#b1">[2]</ref>. Kratz et al. <ref type="bibr" target="#b38">[38]</ref> introduced an HMMs-based approach for detecting abnormal events through analyzing the motion variation of local space-time volumes. Ricci et al. <ref type="bibr" target="#b12">[13]</ref> proposed a convex hierarchical clustering approach to detect abnormal events in time and space at different scale.</p><p>The advantage of these methods over trajectory-based ones is that, working at pixel level or, more generally, on 2D cells/3D cubes, they are more robust in case of complex scenes. However, all these approaches rely on hand-crafted features which are difficult to define a priori due to the huge variations of anomalous behaviors. Our AMDN represents one of the first attempts in the computer vision community to overcome these issues by learning deep feature representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Deep Learning Models in Unsupervised Setting</head><p>Deep learning techniques have recently achieved remarkable success in the computer vision field, beating the state-of-the-art in various challenging tasks <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18]</ref>. Unsupervised deep learning approaches have also received increasing popularity, as a large amount of annotated training data is usually relatively difficult to obtain in a variety of real-world applications. Commonly used unsupervised deep models include deep belief networks <ref type="bibr" target="#b39">[39]</ref> and stacked autoencoders <ref type="bibr" target="#b40">[40]</ref>, which can be efficiently trained with layer-wise pretraining and finetuning <ref type="bibr" target="#b41">[41]</ref>. These models have shown much more representative power than their shallow counterparts, resulting in improved performance on several tasks. For instance, Kan et al. <ref type="bibr" target="#b42">[42]</ref> addressed the cross-pose face recognition problem using stacked autoencoders. In this way pose-invariant features are learned from faces with different poses, obtaining superior performance than previous methods. In <ref type="bibr" target="#b17">[18]</ref>, the authors proposed a deep denoising autoencoder network for robust visual tracking and learning effective target representations. To our knowledge, no existing works in the literature considered an unsupervised deep learning framework for learning multiple features to tackle the abnormal event detection problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">AMDN for Abnormal Event Detection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>As discussed in Section 1, the proposed AMDN framework for detecting anomalous activities is based on two main steps (Fig. <ref type="figure" target="#fig_0">1</ref>). In the first phase, SDAEs are used to learn appearance and motion representations of visual data, as well as a joint representation capturing the correlation between appearance and motion features (Sec. 3.2). In the second phase (Sec. 3.3), three separate one-class SVMs <ref type="bibr" target="#b43">[43]</ref> are learned based on the different types of feature representations. Once the oneclass SVM models are trained, given a test sample corresponding to an image patch, three anomaly scores are computed and combined. The combination of the oneclass SVM scores is obtained with a novel late fusion scheme. In the following we describe the proposed approach in details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">SDAEs in AMDN</head><p>In this subsection we first review some basic concepts about denoising autoencoders (DAEs) and then describe the details of the proposed approach for learning deep representations of appearance and motion. </p><formula xml:id="formula_0">(c) ! ! ! ! ! ! ! ! ! (a) ! ! ! ! ! ! ! ! ! (b) ! Bottleneck hidden layer W ′A l+1 , b ′A l+1 W ′A 2l , b ′A 2l W A 1 , b A 1 W A l , b A l Decoder Encoder W ′J 2l , b ′J 2l W ′J l+1 , b ′J l+1 W J l , b J l W J 1 , b J 1 xA i (x M i ) x A i (x M i ) xJ i x M i x A i</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Background on Denoising Autoencoders</head><p>A denoising autoencoder <ref type="bibr" target="#b16">[17]</ref> is a one-hidden-layer neural network which is trained to reconstruct a sample </p><formula xml:id="formula_1">x i -xi 2 2 + λ( W 2 F + W 2 F ) (1)</formula><p>where • F denotes the Frobenius norm. The first term represents the average reconstruction error, while the weight penalty term is introduced for regularization. The parameter λ balances the importance of the two terms. Typically, sparsity constraints are imposed on the output of the hidden units to discover meaningful representations from the data <ref type="bibr" target="#b17">[18]</ref>. If we let µ j be</p><formula xml:id="formula_2">A C C E P T E D M A N U S C R I P T</formula><p>the target sparsity level and μ j = 1 N N i=1 h j i be the average activation values all over all training samples for the j-th unit, an extra penalty term based on cross-entropy, ϕ(µ|| μ) = -H j=1 [µ j log( μ j )+(1-µ j ) log(1-μ j )], can be added to (2) to learn a sparse representation. Here, H is the number of hidden units. The optimization problem (2) has a non-convex objective function and gradient descent can be used to compute a local optima.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">SDAEs Structure and Training</head><p>Structure. The proposed approach for detecting anomalous event rely on three SDAE networks (Fig. <ref type="figure" target="#fig_0">1</ref>) associated to different types of low-level inputs. These SDAE are used to learn appearance and motion features as well as a joint representation of them. The basic structures of the proposed SDAE networks is illustrated in Fig. <ref type="figure" target="#fig_1">2 (a</ref>) and (b). For the encoder part, we use an over-complete set of filters in the first layer to capture a representative information from the data. Then, the number of neurons is reduced by half in the next layer until reaching the "bottleneck" hidden layer. The decoder part has a symmetric structure with respect to the encoder part.</p><p>Specifically, the first SDAE learns mid-level appearance representations from the original image pixels. To capture rich appearance attributes, a multi-scale slidingwindow approach with a stride d is used to extract dense image patches, which are then warped into equal size w a × h a × c a , where w a , h a are the width and height of each patch and c a is the number of the channels (c a = 1 for gray images). The warped patches x A i ∈ I R w a ×h a ×c a are used for training. All the patches are linearly normalized into a range [0, 1]. We stack 4 encoding layers with ν a × w a × h a × c a neurons in the first layer, where ν a &gt; 1 is an amplification factor for constructing an over-complete set of filters. The use of over-complete representations in combination with sparsity terms have been shown to be effective in learning meaningful compressed representations in previous works <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b44">44]</ref>.</p><p>The second SDAE is used to learn the motion features. We compute dense optical flow and we use a sliding window approach with windows of fixed size w m × h m × c m (c m = 2 for optical flow magnitude along x and y axes) for motion representation learning. Similar to the appearance feature pipeline, the patches x M i ∈ I R w m ×h m ×c m are normalized into [0,1] within each channel and 4 encoding layers are used. The number of neurons of the first layer is set to</p><formula xml:id="formula_3">ν m × w m × h m × c m , ν m &gt; 1.</formula><p>While the first two SDAEs learn appearance and motion features separately, to take into account the correlations between motion and appearance we propose to couple these two pipelines to learn a joint representation (Fig. <ref type="figure" target="#fig_1">2 (b)</ref>). The network training data x J i ∈ I R w j ×h j ×(c a +c m ) are obtained through a pixel-level early fusion of the gray image patches and the corresponding optical flow patches.</p><p>Training. The proposed SDAEs are trained separately. We rely on the typical learning scheme based on two steps: pretraining and fine-tuning. The network parameters are initialized through pretraining all layers, and then fine-tuning is used to adjust parameters over the whole network. Given a training set</p><formula xml:id="formula_4">T k = {x k i } N k i=1 , k ∈ {A, M</formula><p>, J} corresponding to appearance, motion and joint representation, the layer-wise pretraining learns the parameters of each SDAE minimizing the reconstruction loss regularized by a sparsity-inducing term, i.e.:</p><formula xml:id="formula_5">J = N k i=1 x i -xi 2 2 + λ( W 2 F + W 2 F ) + γϕ(µ|| μ). (<label>2</label></formula><formula xml:id="formula_6">)</formula><p>In each layer, the input is corrupted to learn the mapping function, which is then used to produce the representation for the next layer with uncorrupted inputs. Finetuning consider all the layers of each SDAE as a single model. The backpropagation algorithm can be used to fine-tune the network.</p><p>The following objective function is used for finetuning:</p><formula xml:id="formula_7">J = N k i=1 x k i -xk i 2 2 + λ F L j=1 ( W k j 2 F + W k j 2 F ) (3)</formula><p>where λ F is a user defined parameter and 2L + 1 is the number of layers in the SDAEs. Similar to previous works <ref type="bibr" target="#b41">[41]</ref>, here we remove the sparsity regularization because the pre-trained weights will serve as regularization to the network. To speed up the convergence during training, stochastic gradient descent is employed and the training set is divided into mini-batches.</p><p>Once training is complete, the appearance, motion and joint feature representations can be computed to perform video anomaly detection.</p><p>In this work, we choose the output of the "bottleneck" hidden layer to obtain a more compact representation. Let x k i be the i-th input data sample, and σ k l (W k l , b k l ) be the mapping function of the l-th hidden layer of the kth SDAE pipeline. The learned features, s k i , can be extracted through a forward pass computing, i.e.</p><formula xml:id="formula_8">s k i = σ L (σ L-1 (• • • σ 1 (W k 1 x k i + b k 1 ))),<label>(4)</label></formula><p>where the L-th hidden layer is the "bottleneck" hidden layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Abnormal Event Detection with Deep Representations</head><p>In this work the video anomaly detection problem is formulated as a patch-based binary categorization problem, i.e. given a test frame we adopt a sliding window approach and classify each patch as corresponding to a normal or an abnormal event. Specifically, given the t-th test patch, we compute the associated deep features representations s k t , k ∈ {A, M, J}. Then, we rely on three one-class SVM models to calculate a set of anomaly scores A(s k t ) (Subsection 3.3.1). Finally, the scores are linearly combined to obtain the global anomaly score</p><formula xml:id="formula_9">A(s k t ) = k∈{A,M,J} α k A(s k t ) (Subsection 3.3.2).</formula><p>In the following we describe these phases in details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1.">One-class SVM Modeling</head><p>One-class SVM is a widely used algorithm for outlier detection, where the main idea is to learn a hypersphere in the feature space and map most of the training data into it. The outliers of the data distribution correspond to point lying outside the hypersphere. While other approaches can be considered to compute anomaly scores, we consider one-class SVMs as it has been shown to be effective in previous works on abnormal event detection from surveillance videos using hand-crafted features <ref type="bibr" target="#b45">[45]</ref>. Formally, given a set of training samples S = {s k i } N k i=1 , the underlying problem of one-class SVM can be formulated as the following quadratic program:</p><formula xml:id="formula_10">min θ,ρ 1 2 θ 2 + 1 µN k N k i=1 ξ i -ρ s.t. θ T φ(s k i ) ≥ ρ -ξ i , ξ i ≥ 0. (<label>5</label></formula><formula xml:id="formula_11">)</formula><p>where θ is the learned weight vector, ρ is the offset, φ(•) is a feature projection function which maps the feature vector s k i into a higher dimensional feature space. The user defined parameter µ ∈ (0, 1] regulates the expected fraction of outliers distributed outside the hypersphere. Introducing a nonlinear mapping, the projection function φ(•) can be defined implicitly by introducing an associated kernel function k(s k i , s k j ) = φ(s k i ) T φ(s k j ) and ( <ref type="formula" target="#formula_10">5</ref>) can be solved in the corresponding dual form <ref type="bibr" target="#b43">[43]</ref>. In our experiments we consider a radial basis function ker-</p><formula xml:id="formula_12">nel, k(s k i , s k j ) = e -s k i -s k j 2 2σ 2 .</formula><p>Given the optimal θ and ρ obtained by solving <ref type="bibr" target="#b4">(5)</ref>, an outlier score for a test sample s k t of the k-th SDAE pipeline can be estimated by computing:</p><formula xml:id="formula_13">A(s k t ) = ρ -θ T φ(s k t )<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2.">Flexible p -norm Late Fusion for Anomaly Detection</head><p>In this subsection, we propose a flexible unsupervised late fusion scheme to automatically learn the weights α = [α A , α M , α J ]. These parameters are used to compute the anomaly score A(s k t ) = k∈{A,M,J} α k A(s k t ). At test time for each patch t an abnormal activity is identified by computing A(s k t ) and comparing it with a threshold η, i.e. A(s k t ) &gt; η denotes an anomalous event. The weights α k are meant to reflect the importance of different feature representations, corresponding to different one-class SVM models. While many choices are possible to learn α k , in this paper we propose to solve the following optimization problem: min</p><formula xml:id="formula_14">P k ∈P,α k ≥0 - k α k tr P k S k P k S k T + λ s α p p (<label>7</label></formula><formula xml:id="formula_15">)</formula><p>where λ s is a regularization parameter and P = {P : PP T = I}. Similarly to Principal Component Analysis, the matrix P k ∈ I R m×M , m &lt;&lt; M, maps the samples s k i ∈ I R M associated to the k-th modality into a new subspace in order to maximize the variance of the first m-components, subject to orthogonality constraints. The matrix P k S k P k S k T represents the covariance of k-th feature type in the new subspace and measures the spread of the projected samples for each modality. Setting the weights α k by solving the optimization problem <ref type="bibr" target="#b6">(7)</ref> we favor feature types associated with data sets with smaller variance: our intuition is that scattered data sets correspond to noisy features which must be deemphasized.</p><p>In the proposed optimization problem <ref type="bibr" target="#b6">(7)</ref> we also introduce a p -norm term, which, compared with traditional 2 -norm and 1 -norm terms, guarantees an enhanced flexibility, by allowing to tune for p <ref type="bibr" target="#b46">[46,</ref><ref type="bibr" target="#b47">47]</ref>. Intuitively, 1 -norm imposes sparsity on the learned weights, while 2 norms produces an "averaging" effect. Setting a priori one of the two may be suboptimal in term of performance. Moreover, the complexity of solving the problem <ref type="bibr" target="#b6">(7)</ref> with p -norm is the same as for 2 -norm <ref type="bibr" target="#b48">[48]</ref>. Therefore, in our experiments, we tune the parameter p with the interval of 0.1 from [1.1, 1.2, ... , 2.5]. We also set the parameter m = 100, as it empirically provides the best performance. The projection matrices P k are introduced for learning the feature weights as explained above and they are not used in the test phase. The same value m = 100 is chosen for the three feature types. The proposed optimization problem ( <ref type="formula" target="#formula_14">7</ref>) is a convex problem and if p &gt; 1 an alternating minimization algorithm can be used to solve it with respect to P k and α respectively <ref type="bibr" target="#b48">[48]</ref>. Finally,  it is worth noting that, while the proposed late fusion scheme is applied to AMDN considering three underlying SDAEs, our strategy is general and can be used also in case of a different number of models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section we evaluate the performance of the proposed AMDN framework for abnormal event detection on three challenging video surveillance datasets. Specifically, we consider the UCSD pedestrian anomaly dataset (Ped1 and Ped2) <ref type="bibr" target="#b2">[3]</ref>, the Subway dataset <ref type="bibr" target="#b49">[49]</ref> and the Train dataset <ref type="bibr" target="#b34">[34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>The UCSD pedestrian dataset <ref type="bibr" target="#b2">[3]</ref> includes two subsets: Ped1 and Ped2 The Subway dataset <ref type="bibr" target="#b49">[49]</ref> is collected using CCTV cameras and consists of two video streams corresponding to two different subway station scenarios (an entrance and an exit gate). The length of the videos is 1 http://www.svcl.ucsd.edu/projects/anomaly/ dataset.html 96 min and 43 min, respectively. In the entrance subset, there are 66 abnormal events including people moving in a wrong direction, unusual gesture interactions between people and sudden stopping or running. In the exit subset 19 abnormal events are included, such as people moving in a wrong direction and loitering near the exit gate. The image resolution is 512 × 384 pixels.</p><p>The Train dataset <ref type="bibr" target="#b34">[34]</ref> depicts moving people inside a train<ref type="foot" target="#foot_0">2</ref> . The dataset consists of 19218 frames, and the anomalous events are mainly due to unusual movements of people on the train. This is a challenging abnormal event detection dataset due to dynamic illumination changes and camera shake problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation details</head><p>The proposed method is mainly implemented in Matlab and C++ based on the Caffe framework <ref type="bibr" target="#b50">[50]</ref>. The code for optical flow calculation is written in C++ and wrapped with Matlab mex for computational efficiency <ref type="bibr" target="#b51">[51]</ref>. For one-class SVMs, we use the LIBSVM library (version 3.2) <ref type="bibr" target="#b52">[52]</ref>. The experiments are carried out on a PC with a middle-level graphics card (NVIDIA Quadro K4000) and a multi-core 2.1 GHz CPU with 32 GB memory.</p><p>To improve the computational speed of our framework in the test phase, in this paper we introduce a foreground detection approach based on background subtraction. This is motivated by the fact that abnormal events are typically found in correspondence of moving pixels. An illustration of the proposed foreground detection scheme is provided in Fig. <ref type="figure" target="#fig_4">3</ref>. For an input test image, the probability map of the foreground pixels is estimated with a background subtraction algorithm and binarized. The foreground regions are detected by identifying the patches which contains more than a certain number of foreground pixels (10% of the patch size in our test). We use the ViBe <ref type="bibr" target="#b53">[53]</ref> method to perform background subtraction. ViBe has a low computational complexity and can obtain near real-time performance (almost 16 frames/second with a resolution of 360 × 240 in our Matlab environment).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation on the UCSD pedestrian dataset</head><p>In the first series of experiments we evaluate the performance of the proposed method on the UCSD dataset. For learning appearance features, patches are extracted using a sliding window approach at three different scales, i.e. 15 × 15, 18 × 18 and 20 × 20 pixels. MDT <ref type="bibr" target="#b15">[16]</ref> MPPCA <ref type="bibr" target="#b9">[10]</ref> Social Force + MPPCA <ref type="bibr" target="#b15">[16]</ref> Social Force <ref type="bibr" target="#b16">[17]</ref> Sparse reconstruction <ref type="bibr" target="#b3">[4]</ref> Local statistical <ref type="bibr" target="#b21">[22]</ref> Detection at 150FPS <ref type="bibr">[</ref> MDT <ref type="bibr" target="#b15">[16]</ref> MPPCA <ref type="bibr" target="#b9">[10]</ref> Social Force + MPPCA <ref type="bibr" target="#b15">[16]</ref> Social Force <ref type="bibr" target="#b16">[17]</ref> Sparse Reconstruction <ref type="bibr" target="#b3">[4]</ref> Detection at 150FPS <ref type="bibr" target="#b14">[15]</ref> AMDN (b) Pixel-level ROC curve  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Ped1(frame) Ped1(pixel) Ped2 MPPCA <ref type="bibr" target="#b37">[37]</ref> 59.0% 20.5% 69.3% Social force <ref type="bibr" target="#b8">[9]</ref> 67.5% 19.7% 55.6% Social force+MPPCA <ref type="bibr" target="#b2">[3]</ref> 66.8% 21.3% 61.3% Sparse reconstruction <ref type="bibr" target="#b3">[4]</ref> -45.3% -Mixture dynamic texture <ref type="bibr" target="#b2">[3]</ref> 81.8% 44.1% 82.9% Local Statistical Aggregates <ref type="bibr" target="#b1">[2]</ref> 92.7% --Detection at 150 FPS <ref type="bibr" target="#b0">[1]</ref> 91.8% For the pretraining of the SDAEs, the corrupted inputs are produced by adding a Gaussian noise with variance 0.0003. The network fine-tuning is based on stochastic gradient descent with the momentum parameter set to 0.9. We set the parameters λ = 0.01, λ F = 0.0001 and the mini-batch size N b = 256. For one-class SVMs, the parameter µ is tuned with cross validation.</p><p>To perform a quantitative evaluation, we use both a frame-level ground truth and a pixel-level ground truth. The frame-level ground truth indicates whether one or more anomalies occur in a test frame. The pixel-level ground truth is used to assess the anomaly localization performance. If the detected anomaly region overlaps more than 40% with the annotated region, it is considered a true detection. We carry out a frame-level evaluation on both Ped1 and Ped2. Ped1 also provides 10 test image sequences with pixel-level ground truth. The pixel-level evaluation is performed on these sequences.</p><p>The proposed approach is compared with several state of the art methods. Specifically, we consider the Mixture of Probabilistic Principal Component Analyz-  ers (MPPCA) approach in <ref type="bibr" target="#b37">[37]</ref>, the social force model in <ref type="bibr" target="#b8">[9]</ref> and its extension in <ref type="bibr" target="#b2">[3]</ref>, the sparse reconstruction method in <ref type="bibr" target="#b3">[4]</ref>, mixture of dynamic texture (MDT) <ref type="bibr" target="#b2">[3]</ref>, Local Statistical Aggregates <ref type="bibr" target="#b1">[2]</ref> and detection at 150 FPS <ref type="bibr" target="#b0">[1]</ref>. Table <ref type="table" target="#tab_3">1</ref> and Fig. <ref type="figure">6</ref> show a quantitative comparison of different methods respectively in terms of Area Under Curve (AUC) and Equal Error Rate (EER). Figure <ref type="figure" target="#fig_7">5</ref> and Fig. <ref type="figure" target="#fig_6">4</ref> (a) and (b) report the associated ROC curves. The ROC curves are produced by varying the threshold parameter η. The performance of the baseline methods are taken from the original papers (when available). From the frame-level evaluation, it is evident that our method outperforms most previous approaches and that its performance are competitive with the best two baselines <ref type="bibr" target="#b1">[2]</ref> and <ref type="bibr" target="#b0">[1]</ref>. Moreover, considering pixel-level evaluation, i.e. accuracy in anomaly localization, our method outperforms all the competing approaches.</p><formula xml:id="formula_16">A C C E P T E D M A N U S C</formula><p>Table <ref type="table" target="#tab_2">2</ref> demonstrates the advantages of the proposed double fusion strategy, comparing our AMDN with early fusion and late fusion approaches. Specifically, for early fusion we only consider the learned joint appearance/motion representation and a single one-class SVM. For late fusion we use the two separate appearance and motion pipelines and the proposed fusion scheme but we discard the joint representation pipeline. We observe that the late fusion strategy outperforms the early fusion and that the combination of the two schemes lead to a clear advantage. Finally, we also report some examples of anomalous events detected with our method on the UCSD dataset in Fig. <ref type="figure" target="#fig_0">10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Evaluation on the Subway dataset</head><p>To train the AMDN network, we follow previous works <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b0">1]</ref>   <ref type="bibr" target="#b3">[4]</ref> and the detection at 150 fps <ref type="bibr" target="#b0">[1]</ref> performance on Ped2 dataset are not available.  <ref type="bibr" target="#b33">[33]</ref>, MPPCA <ref type="bibr" target="#b37">[37]</ref>, Spatio-Temporal Oriented Energy (STOE) <ref type="bibr" target="#b34">[34]</ref>, Dynamic Sparse Coding (DSC) <ref type="bibr" target="#b54">[54]</ref>, Sparse Reconstruction <ref type="bibr" target="#b3">[4]</ref> and Local Optical Flow <ref type="bibr" target="#b49">[49]</ref>. the best detection performance from both the perspectives of abnormal events detection (61/66) and false alarm <ref type="bibr" target="#b0">(1)</ref>. The effectiveness of the proposed double fusion scheme is also verified in Table <ref type="table" target="#tab_7">3</ref>. Our AMDN outperforms both early fusion and late fusion techniques.</p><p>Figure <ref type="figure" target="#fig_0">11</ref> shows some examples of the detected abnormal events, such as people entering through the exit gate, people entering without payment and people exiting from the entrance gate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Evaluation on the Train dataset</head><p>In the third series of experiments we consider the Train dataset. The frames of the dataset are resized to 280 × 380 pixels for computational efficiency. For the AMDN parameters, we use the same experimental setting of the UCSD dataset experiments, except from the parameters λ F and N b which are set to 0.00001 and 100, respectively.</p><p>We compare the proposed approach with several methods in the literature considering the same datasets, including Spatio-Temporal Composition <ref type="bibr" target="#b33">[33]</ref>, Spatio-Temporal Oriented Energies <ref type="bibr" target="#b34">[34]</ref>, Local Optical Flow <ref type="bibr" target="#b49">[49]</ref>, Behavior Templates <ref type="bibr" target="#b55">[55]</ref> and Mixture of Gaussian. From the precision/recall curve shown in Fig. <ref type="figure" target="#fig_11">12 (c)</ref>, it is clear that our method outperforms all the baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Analysis of AMDN</head><p>In this section, we further analyze the proposed method to underline the importance of its main components. Fist of all, in order to demonstrate the effectiveness of the adopted SDAEs, we replace the stacked  denoising autoencoder structure with a regular stacked autoencoder (SAE) into AMDN. Figure <ref type="figure">7</ref> shows the results of our comparison. Using SDAE we obtain a slight improvement in terms of AUC and EER on three datasets. We believe that the corrupted training data used with SDAE helps to learn more effective feature representations, as the corruption increases the variability of training data. This in line with what observed in previous works on deep architectures.</p><p>To further demonstrate the validity of the learned deep representations, we evaluate the performance of AMDN by using different settings, including (i) AMDN-RawPatches: removing SDAEs from our framework and directly input patches to one-class SVMs; (ii) AMDN-RawPatches-PCA256: the same setting as (i) but using PCA for reducing the dimension of the patches to 256; and (iii) AMDN-RawPatches-PCA128: the same setting as (i) but using PCA for reducing the dimension of the patches to 128. Figure <ref type="figure" target="#fig_8">8</ref> compares the performance of AMDN with the different settings on the UCSD ped1 dataset. It is clear that the performance of AMDN-RawPatches is the worst among all the baselines. By using PCA on raw patches the performance is slightly improved, while AMDN is significantly better than all the baseline methods, thus demonstrating the effectiveness of the learned deep representations. We also report the TPR and FPR computed considering as anomaly score the reconstruction error of the SDAE model which uses the joint motion and appearance representation (denoted as AMDM (Joint) RecError). As shown in the figure, the proposed approach outperforms this model. We believe that it is probably because the simple reconstruction error is sensitive for identifying the anomaly regions. These results thus confirm the benefit of adopting one-class SVMs for anomaly detection in combination with a late fusion  scheme. Table <ref type="table" target="#tab_11">5</ref> compares AMDN with another method based on deep representations <ref type="bibr" target="#b56">[56]</ref>. As shown in the table, the proposed framework outperforms <ref type="bibr" target="#b56">[56]</ref> in the Ped1, Ped2 and Subway exit datasets. The approach in <ref type="bibr" target="#b56">[56]</ref> is a frame-based deep learning method, which uses a convolution-deconvolution autoencoder network to learn a representation of the whole frame. We believe that our approach is more successful for the anomaly detection task, as it operates on a patch-level basis.</p><formula xml:id="formula_17">A C C E P T E D M A N U S C R I P T</formula><p>Finally, to evaluate the influence of the p -norm in the flexible late fusion scheme, we perform a sensitivity analysis of the parameter p. Figure <ref type="figure" target="#fig_9">9</ref> shows the EER at varying p on UCSD Ped1 and Ped2 datasets. It is immediate to observe that in this case when p is set to 2, AMDN yields the best performance. Moreover, changing the parameter p around 2 only have a small effect on the final performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Computational Cost Analysis</head><p>To evaluate the performance of our approach in terms of computational cost, we conduct an empirical analysis on the considered datasets. As discussed above, different datasets have different image resolutions which affect the time required to process each frame. Specifically, the resolutions are 238×158, 360×240, 320×240 and 280 × 380 pixels, for the UCSD Ped1, UCSD Ped2, Subway and Train datasets, respectively.  During the training phase, the learning of the proposed AMDN takes about 9, 4, 2.5 and 3.5 hours on the UCSD Ped1, UCSD Ped2, Subway and Train datasets, respectively. Table <ref type="table" target="#tab_10">4</ref> shows the average running time of each frame during the test phase. Processing one frame to detect anomalous events takes about 9 -14 seconds when the cell size is 15 × 15 pixels and no foreground detection is applied. Adopting the proposed foreground detection scheme (Subsection 4.2), a significant improvement in terms of computational speed is achieved (average improvement ∼ 44.3%). For sake of completeness, we also analyze the impact of the foreground detection approach on the performance in terms of EER and AUC. As shown in Table <ref type="table" target="#tab_12">6</ref>, no significant variations are observed. This is probably due to the fact that most false positive detections correspond to the foreground region.</p><p>Finally, in Table <ref type="table" target="#tab_10">4</ref> we also provide a comparison with some previous methods in terms of computational cost during test time. Since the original implementations of baseline approaches are not publicly available, we report the running times taken from <ref type="bibr" target="#b0">[1]</ref> specifying the working environment. The methods in <ref type="bibr" target="#b0">[1]</ref> and <ref type="bibr" target="#b2">[3]</ref> are advantageous with respect to our approach in terms of detection speed. This is somehow expected as, similarly to other applications, the gain in terms of accuracy obtained with deep architectures comes at a price of an increased computational cost. Future works will be devoted to address this issue.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>This paper introduced a novel unsupervised learning approach for video anomaly detection based on deep learning architectures. The proposed AMDN method is based on multiple SDAEs for learning both appearance and motion representations of activities in a video scene. A double fusion scheme is designed to combine the learned feature representations. We carried out an extensive experimental evaluation, considering three challenging publicly available video anomaly detection datasets (UCSD, Subway and Train), and we demonstrated the effectiveness and robustness of the proposed approach, showing competitive performance with respect to existing methods. The fundamental advantages of our approach are that it does not rely on any prior knowledge for designing features (the input of our framework are raw pixels) and does not require any object-level analysis (e.g. object detection or tracking). From the experimental results, it is obvious that our learned deep features are more powerful than traditional hand-crafted descriptors for representing the dynamic of the video scenes.</p><p>Currently, the computational overhead of AMDN in the test phase is too high for real-time processing. Strategies to reduce the computational cost will be studied in future works. Further research directions will include investigating other deep network architectures as well as alternative approaches for fusing data of multiple modalities in the context of SDAEs. Moreoever, an interesting follow-up of this work will be to extend ADMN in order to include contextual information. In fact, while at the present anomalous behaviours are detected by considering patches in isolation, it will be beneficial to look at co-occurrence of multiple patterns to spot additional unusual events.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of the proposed AMDN approach for abnormal video event detection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: (a) The structure of the (a) appearance and motion and (b) joint SDAEs for learning feature representations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>x i from its (partially) corrupted version xi . Typical corrupted inputs are obtained by drawing samples from a conditional distribution p(x|x) (e.g. common choices for corrupting samples are additive Gaussian white noise or salt-pepper noise). A DAE can be divided into two parts, i.e. encoder and decoder, connected by a single hidden layer. The two parts are used to learn two mapping functions, f e (W, b) and f d (W , b ), where W, b denote the weights and the bias term of the encoder part, while W , b refer to the corresponding parameters of the decoder. For a corrupted input xi , a compressed hidden layer representation h i can be obtained through h i = f e (x i | W, b) = σ(Wx i + b). Then, the decoder tries to recover the original input x i from h i computing xi = f d (h i | W , b ) = s(W h i + b ). The function σ(•) and s(•) are activation functions, which are typically nonlinear transformations such as the sigmoid. Using this encoder/decoder structure, the network can learn a more stable and robust feature representations of the input. At training time, given a training set T = {x i } N i=1 , a DAE learns its parameters (W, W , b, b ) by solving the following optimization problem: min W,W ,b,b N i=1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: Illustration of the proposed foreground detection scheme using background subtraction for improving computational efficiency in the test phase. Green patches are selected foreground regions, red ones correspond to anomalies.</figDesc><graphic coords="9,171.12,183.55,97.61,64.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: UCSD dataset (Ped1 sequence): comparison of different methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: UCSD dataset (Ped2 sequence): comparison of frame-level performance (ROC curve) with different methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Comparison of different feature representations in AMDN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Performance at varying p.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 :Figure 11 :</head><label>1011</label><figDesc>Figure 10: Examples of anomaly detection results on Ped1 (top) and Ped2 (bottom) sequences.</figDesc><graphic coords="14,66.34,311.08,107.72,80.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Results of anomaly detection on the Train dataset: (left) a frame depicting typical normal activities; (center) examples of detected anomalies; (right) precision/recall curve.</figDesc><graphic coords="14,55.10,550.06,460.89,125.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>UCSD dataset: comparison of different feature fusion schemes in terms of EER and AUC.</figDesc><table><row><cell>Method</cell><cell>Ped1(frame) EER AUC</cell><cell>Ped1(pixel) EER AUC</cell><cell>EER</cell><cell cols="2">Ped2</cell><cell>AUC</cell></row><row><cell>Joint representation (early fusion)</cell><cell cols="2">22.0% 84.9% 47.1% 57.8%</cell><cell cols="4">24.0% 81.5%</cell></row><row><cell cols="3">Fusion of appearance and motion pipelines (late fusion) 18.0% 89.1% 43.6% 62.1%</cell><cell cols="4">19.0% 87.3%</cell></row><row><cell>AMDN (double fusion)</cell><cell cols="2">16.0% 92.1% 40.1% 67.2%</cell><cell cols="2">17.0%</cell><cell cols="2">90.8%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>UCSD dataset: comparison (AUC) with the state of the art methods.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>This generates more than 50 million image patches, 10 million of which are randomly sampled and warped into the same size (w a × h a = 15 × 15 pixels) for training. For learning the motion representation, the patch size is fixed to w m × h m = 15 × 15 pixels, and 6 million training patches are randomly sampled. In the test phase, we use a sliding widow approach with stride d = 15 and consider patches with size 15 × 15. The number of neurons of the first layer of the appearance and motion network is both set to 1024, while for the joint pipeline is 2048. Then, for the appearance and motion SDAE the structure of the encoder part can be simply defined as 1024 → 512 → 256 → 128. The decoder part has a symmetric structure. Similarly, for the joint SDAE the structure of the encoder part is 2048 → 1024 → 512 → 256.</figDesc><table><row><cell></cell><cell></cell><cell>63.8%</cell><cell>-</cell></row><row><cell>AMDN</cell><cell>92.1%</cell><cell>67.2%</cell><cell>90.8%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>R I P T</figDesc><table><row><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>True Positive Rate (TPR)</cell><cell>0.3 0.4 0.5 0.6 0.7 0.8 0.2</cell><cell></cell><cell></cell><cell></cell><cell cols="2">MDT MPPCA Social Force + MPPCA Social Force</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">JointRepresentation(early fusion)</cell></row><row><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell cols="2">AMDN(double fusion)</cell></row><row><cell></cell><cell>0</cell><cell>0</cell><cell>0.2</cell><cell>0.4</cell><cell>0.6</cell><cell>0.8</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">False Positive Rate (FPR)</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>and use the first 15 minutes of the video</figDesc><table><row><cell></cell><cell>70</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>MPPCA</cell><cell></cell><cell></cell></row><row><cell></cell><cell>60</cell><cell>Social Force Social Force + MPPCA</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>MDT</cell><cell></cell><cell></cell></row><row><cell>Equal Error Rate (%)</cell><cell>20 30 50 40</cell><cell>31% 32% 25% 19% 15% 16% Sparse reconstruction 30% Detection at 150 FPS 40% AMDN</cell><cell>25% 36% 42%</cell><cell>15%</cell></row><row><cell></cell><cell>10</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>Ped1</cell><cell>Ped2</cell><cell>----</cell></row><row><cell></cell><cell></cell><cell>Dataset</cell><cell></cell><cell></cell></row><row><cell cols="5">Figure 6: UCSD dataset: comparison of frame-level performance</cell></row><row><cell cols="5">(Equal Error Rate) with different methods. Note that for the sparse re-</cell></row><row><cell cols="3">construction method</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Comparison of different methods on the Subway dataset. In the third column, the first number denotes the detected anomalous events, while the second is the actual number of anomalous events.</figDesc><table><row><cell>Method</cell><cell cols="3">Dataset Abnormal events False alarm</cell></row><row><cell>STC [33]</cell><cell>Entrance Exit</cell><cell>60/66 19/19</cell><cell>4 2</cell></row><row><cell>MPPCA [37]</cell><cell>Entrance Exit</cell><cell>57/66 19/19</cell><cell>6 3</cell></row><row><cell>DSC [54]</cell><cell>Entrance Exit</cell><cell>60/66 -</cell><cell>5 -</cell></row><row><cell>Sparse</cell><cell>Entrance</cell><cell>27/31</cell><cell>4</cell></row><row><cell>reconstruction [4]</cell><cell>Exit</cell><cell>19/19</cell><cell>3</cell></row><row><cell>Local</cell><cell>Entrance</cell><cell>27/31</cell><cell>4</cell></row><row><cell>optical flow [49]</cell><cell>Exit</cell><cell>19/19</cell><cell>3</cell></row><row><cell>Joint representation</cell><cell>Entrance</cell><cell>56/66</cell><cell>8</cell></row><row><cell>(early fusion)</cell><cell>Exit</cell><cell>15/19</cell><cell>4</cell></row><row><cell>Fusion of appearance</cell><cell>Entrance</cell><cell>58/66</cell><cell>6</cell></row><row><cell>&amp; motion (late fusion)</cell><cell>Exit</cell><cell>17/19</cell><cell>2</cell></row><row><cell>AMDN (double fusion)</cell><cell>Entrance Exit</cell><cell>61/66 19/19</cell><cell>4 1</cell></row><row><cell cols="4">sequences. The rest of the videos is used for test-</cell></row><row><cell cols="4">ing. The frames are resized to a pixel resolution of</cell></row><row><cell cols="4">320 × 240 for computational efficiency. In these ex-periments the patch size for learning both appearance</cell></row><row><cell cols="4">and motion features is 15 × 15 pixels. For SDAEs we use the same network configuration and training param-</cell></row><row><cell cols="4">eters of the experiments on the UCSD dataset. As base-</cell></row><row><cell cols="4">line methods we consider recent approaches adopting</cell></row><row><cell cols="4">the same dataset, including Spatio-Temporal Composi-</cell></row><row><cell>tion (STC)</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>provides the results of the comparison of</cell></row><row><cell>AMDN with other baseline methods. AMDN obtains</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 4 :</head><label>4</label><figDesc>Comparison of different methods in terms of computational time during test (seconds per frame).</figDesc><table><row><cell cols="3">Method</cell><cell></cell><cell>Platform</cell><cell>CPU</cell><cell>GPU</cell><cell>Memory</cell><cell cols="2">Running time UCSD-Ped1 UCSD-Ped2 Subway (exit) Train</cell></row><row><cell cols="4">Mixture dynamic texture [3]</cell><cell>-</cell><cell>3.0 GHz</cell><cell>-</cell><cell>2.0 GB</cell><cell>25</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">Sparse reconstruction [4]</cell><cell cols="3">MATLAB 2.6 GHz</cell><cell>-</cell><cell>2.0 GB</cell><cell>3.8</cell><cell>-</cell><cell>4.6</cell><cell>-</cell></row><row><cell cols="3">Detection at 150 FPS [1]</cell><cell cols="3">MATLAB 3.4 GHz</cell><cell>-</cell><cell>8.0 GB</cell><cell>0.00697</cell><cell>-</cell><cell>0.00641</cell><cell>-</cell></row><row><cell cols="7">AMDN without foreground detection MATLAB 2.1 GHz Nvidia Quadro K4000 AMDN with foreground detection</cell><cell>32 GB</cell><cell>9.4 5.2</cell><cell>13.5 7.5</cell><cell>12.8 6.3</cell><cell>14.2 8.8</cell></row><row><cell></cell><cell>17.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>17</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>16.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>EER(%)</cell><cell>16</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>15.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>15</cell><cell></cell><cell></cell><cell></cell><cell>UCSD-Ped2 UCSD-Ped1</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>14.5</cell><cell>1</cell><cell>1.5</cell><cell>2</cell><cell>2.5</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>p</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 5 :</head><label>5</label><figDesc>Performance comparison of different deep learning based methods in terms of EER and AUC.</figDesc><table><row><cell>Method</cell><cell>EER</cell><cell cols="2">Ped1</cell><cell>AUC</cell><cell>EER</cell><cell cols="2">Ped2</cell><cell>AUC</cell><cell>Subway (exit) EER AUC</cell></row><row><cell cols="3">Conv-AE [56] 27.9%</cell><cell cols="2">81.0%</cell><cell cols="2">21.7%</cell><cell cols="2">90.0%</cell><cell>9.9% 80.7%</cell></row><row><cell>AMDN</cell><cell cols="4">16.0% 92.1%</cell><cell cols="4">17.0% 90.8%</cell><cell>6.8% 87.9%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 6 :</head><label>6</label><figDesc>Impact of foreground detection (FD) on performance.</figDesc><table><row><cell>Method</cell><cell>EER</cell><cell>Ped1</cell><cell>AUC</cell><cell>EER</cell><cell>Ped2</cell><cell>AUC</cell><cell>Subway (exit) EER AUC</cell></row><row><cell>AMDN without FD</cell><cell cols="6">16.5% 91.4% 16.7% 89.6%</cell><cell>6.6% 88.2%</cell></row><row><cell>AMDN with FD</cell><cell cols="6">16.0% 92.1% 17.0% 90.8%</cell><cell>6.8% 87.9%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>http://vision.eecs.yorku.ca/research/ anomalous-behaviour-data/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was partially supported by the MIUR Cluster project Active Aging at Home, the EC H2020 project ACANTO. The authors also would like to thank NVIDIA for GPU donation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Abnormal event detection at 150 fps in matlab</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Video anomaly detection based on local statistical aggregates</title>
		<author>
			<persName><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Anomaly detection in crowded scenes</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bhalodia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Sparse reconstruction cost for abnormal event detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Similarity based vehicle trajectory clustering and anomaly detection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Trajectory-based anomalous event detection</title>
		<author>
			<persName><forename type="first">C</forename><surname>Piciarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Micheloni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Foresti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1544" to="1554" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning patterns of activity using realtime tracking</title>
		<author>
			<persName><forename type="first">C</forename><surname>Stauffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Grimson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="747" to="757" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Learning semantic scene models by trajectory analysis</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grimson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Abnormal crowd behavior detection using social force model</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mehran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Abnormal events detection based on spatio-temporal co-occurences</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Benezeth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jodoin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rosenberger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Video behaviour mining using a dynamic topic model</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="303" to="323" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Improved anomaly detection in crowded scenes via cell-based analysis of foreground speed, size and texture</title>
		<author>
			<persName><forename type="first">V</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lovell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A prototype learning framework using emd: Application to complex scenes analysis</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Messelodi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="513" to="526" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Manzagol</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning a deep compact image representation for visual tracking</title>
		<author>
			<persName><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Coarse-to-fine autoencoder networks (cfan) for real-time face alignment</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="3371" to="3408" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Multimodal deep learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Multimodal learning with deep boltzmann machines</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Double fusion for multimedia event detection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Multimedia Modeling</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning deep representations of appearance and motion for anomalous event detection</title>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Video-based abnormal human behavior recognitiona review</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">P</forename><surname>Popoola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics, Part C: Applications and Reviews</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="865" to="878" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Event detection and analysis from video streams</title>
		<author>
			<persName><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Brémond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hongeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="873" to="889" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning, modeling, and classification of vehicle track patterns from live video</title>
		<author>
			<persName><forename type="first">B</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="425" to="437" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">On-line trajectory clustering for anomalous events detection</title>
		<author>
			<persName><forename type="first">C</forename><surname>Piciarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Foresti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName><surname>A C C E P T E D M A N U S C R I P T</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1835" to="1842" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning the distribution of object trajectories for event recognition</title>
		<author>
			<persName><forename type="first">N</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hogg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IVC</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="609" to="615" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A system for learning statistical motion patterns</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maybank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on PAMI</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1450" to="1464" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning semantic scene models from observing activity in visual surveillance</title>
		<author>
			<persName><forename type="first">D</forename><surname>Makris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ellis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="397" to="408" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A dynamic hierarchical clustering method for trajectory-based unusual video event detection</title>
		<author>
			<persName><forename type="first">F</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="907" to="913" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Online dominant and anomalous behavior detection in videos</title>
		<author>
			<persName><forename type="first">M</forename><surname>Roshtkhari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Levine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Anomalous behaviour detection using spatiotemporal oriented energies, subset inclusion histogram comparison and event-driven processing</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zaharescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wildes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Histograms of optical flow orientation for visual abnormal events detection</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Snoussi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>AVSS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Video anomaly detection and localization using hierarchical feature representation and gaussian process regression</title>
		<author>
			<persName><forename type="first">K.-W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-H</forename><surname>Fang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Observe locally, infer globally: a spacetime mrf for detecting abnormal activities with incremental updates</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Anomaly detection in extremely crowded scenes using spatio-temporal motion pattern models</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kratz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nishino</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Greedy layer-wise training of deep networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Why does unsupervised pre-training help deep learning?</title>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="625" to="660" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Stacked progressive autoencoders (spae) for face recognition across poses</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Estimating the support of a high-dimensional distribution</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1443" to="1471" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Efficient learning of sparse representations with an energy-based model</title>
		<author>
			<persName><forename type="first">C</forename><surname>Poultney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">L</forename><surname>Cun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Anomaly detection in surveillance video using motion direction statistics</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>ICIP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Multi kernel learning with onlinebatch optimization</title>
		<author>
			<persName><forename type="first">F</forename><surname>Orabona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="227" to="253" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">On p-norm path following in multiple kernel learning for non-linear feature selection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Jawanpuria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nath</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Joint schatten p-norm and p-norm robust matrix completion for missing value recovery</title>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Knowledge Information System</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Robust real-time unusual event detection using multiple fixed-location monitors</title>
		<author>
			<persName><forename type="first">A</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Shimshoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Reinitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="555" to="560" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>ACM MM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Beyond pixels: exploring new representations and applications for motion analysis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Libsvm: a library for support vector machines</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Vibe: A universal background subtraction algorithm for video sequences</title>
		<author>
			<persName><forename type="first">O</forename><surname>Barnich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Droogenbroeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1709" to="1724" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Online detection of unusual events in videos via dynamic sparse coding</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Modeling background activity for behavior subtraction</title>
		<author>
			<persName><forename type="first">P</forename><surname>Jodoin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Konrad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>ICDSC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Learning temporal regularity in video sequences</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
