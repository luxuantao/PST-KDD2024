<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RETRIEVE: Coreset Selection for Efficient and Robust Semi-Supervised Learning</title>
				<funder ref="#_Wv6pHNy #_762BvQk #_U5kDj4m #_YJku4mG">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder>
					<orgName type="full">UT Dallas</orgName>
				</funder>
				<funder>
					<orgName type="full">Google and Adobe</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Krishnateja</forename><surname>Killamsetty</surname></persName>
							<email>krishnateja.killamsetty@utdallas.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of Texas at Dallas Richardson</orgName>
								<address>
									<settlement>Texas</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xujiang</forename><surname>Zhao</surname></persName>
							<email>xujiang.zhao@utdallas.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of Texas at Dallas Richardson</orgName>
								<address>
									<settlement>Texas</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Feng</forename><surname>Chen</surname></persName>
							<email>feng.chen@utdallas.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of Texas at Dallas Richardson</orgName>
								<address>
									<settlement>Texas</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rishabh</forename><surname>Iyer</surname></persName>
							<email>rishabh.iyer@utdallas.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of Texas at Dallas Richardson</orgName>
								<address>
									<settlement>Texas</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">RETRIEVE: Coreset Selection for Efficient and Robust Semi-Supervised Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semi-supervised learning (SSL) algorithms have had great success in recent years in limited labeled data regimes. However, the current state-of-the-art SSL algorithms are computationally expensive and entail significant compute time and energy requirements. This can prove to be a huge limitation for many smaller companies and academic groups. Our main insight is that training on a subset of unlabeled data instead of entire unlabeled data enables the current SSL algorithms to converge faster, significantly reducing computational costs. In this work, we propose RETRIEVE 1 , a coreset selection framework for efficient and robust semi-supervised learning. RETRIEVE selects the coreset by solving a mixed discrete-continuous bi-level optimization problem such that the selected coreset minimizes the labeled set loss. We use a one-step gradient approximation and show that the discrete optimization problem is approximately submodular, enabling simple greedy algorithms to obtain the coreset. We empirically demonstrate on several real-world datasets that existing SSL algorithms like VAT, Mean-Teacher, FixMatch, when used with RETRIEVE, achieve a) faster training times, b) better performance when unlabeled data consists of Out-of-Distribution (OOD) data and imbalance. More specifically, we show that with minimal accuracy degradation, RETRIEVE achieves a speedup of around 3? in the traditional SSL setting and achieves a speedup of 5? compared to state-of-the-art (SOTA) robust SSL algorithms in the case of imbalance and OOD data. RETRIEVE is available as a part of the CORDS toolkit: https://github.com/decile-team/cords.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep learning algorithms have had great success over the past few years, often achieving human or superhuman performance in various tasks like computer vision <ref type="bibr" target="#b9">[10]</ref>, speech recognition <ref type="bibr" target="#b17">[18]</ref>, natural language processing <ref type="bibr" target="#b4">[5]</ref>, and video games <ref type="bibr" target="#b44">[45]</ref>. One of the significant factors attributing to the recent success of deep learning is the availability of large amounts of labeled data <ref type="bibr" target="#b54">[55]</ref>. However, creating large labeled datasets is often time-consuming and expensive in terms of costs. Moreover, some domains like medical imaging require a domain expert for labeling, making it nearly impossible to create a large labeled set. In order to reduce the dependency on the availability of labeled data, semi-supervised learning (SSL) algorithms <ref type="bibr" target="#b6">[7]</ref> were proposed to train models using large amounts of unlabeled data along with the available labeled data. Recent works <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b52">53]</ref> show that semi-supervised learning algorithms can achieve similar performance to standard supervised learning using significantly fewer labeled data instances. However, the current SOTA SSL algorithms are compute-intensive with large training times. For example, from our personal experience, training a WideResNet model <ref type="bibr" target="#b59">[60]</ref> on a CIFAR10 <ref type="bibr" target="#b26">[27]</ref> dataset with 4000 labels using the SOTA FixMatch algorithm <ref type="bibr" target="#b52">[53]</ref> for 500000 iterations takes around four days on a single RTX2080Ti GPU. This also implies increased energy consumption and an associated carbon footprint <ref type="bibr" target="#b53">[54]</ref>. Furthermore, it is common to tune these SSL algorithms over a large set of hyper-parameters, which means that the training needs to be done hundreds and sometimes thousands of times. For example, <ref type="bibr" target="#b43">[44]</ref> performed hyperparameter tuning by running 1000 trails of Gaussian Process-based Blackbox optimization <ref type="bibr" target="#b15">[16]</ref> for each SSL algorithm (which runs for 500000 iterations). This process implies significantly higher experimental turnaround times, energy consumption, and CO2 emissions. Furthermore, this is not something that can be done at most universities and smaller companies. The first problem we try to address in this work is: Can we efficiently train a semi-supervised learning model on coresets of unlabeled data to achieve faster convergence and reduction in training time?</p><p>Despite demonstrating encouraging results on standard and clean datasets, current SSL algorithms perform poorly when OOD data or class imbalance is present in the unlabeled set <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b7">8]</ref>. This performance degradation can be attributed to the fact that the current SSL algorithms assume that both the labeled set and unlabeled set are sampled from the same distribution. A visualization of OOD data and class imbalance in the unlabeled set is shown in Figure <ref type="figure" target="#fig_0">1</ref>. Several recent works <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b16">17]</ref> were proposed to mitigate the effect of OOD in unlabeled data, in turn improving the performance of SSL algorithms. However, the current SOTA robust SSL method <ref type="bibr" target="#b16">[17]</ref> is 3X slower than the standard SSL algorithms, further increasing the training times, energy costs, and CO2 emissions. The second problem we try to address in this work is: In the case where OOD data or class imbalance exists in the unlabeled set, can we robustly train an SSL model on coresets of unlabeled data to achieve similar performance to existing robust SSL methods while being significantly faster?  accuracy degradation with speedup compared to the base SSL or robust SSL (DS3L) approach. We observe speedups of 3? in standard SSL case with 0.7% accuracy drop and 2? speedup with no accuracy drop. In the robust SSL case, we observe 5? speedup compared to DS3L <ref type="bibr" target="#b16">[17]</ref> while outperforming it in terms of accuracy.</p><p>To this end, we propose RETRIEVE, a coreset selection framework that enables faster convergence and robust training of SSL algorithms. RETRIEVE selects coreset of the unlabeled data resulting in minimum labeled set loss when trained upon in a semi-supervised manner. Intuitively, RETRIEVE tries to achieve faster convergence by selecting data instances from the unlabeled set whose gradients are aligned with the labeled set gradients. Furthermore, RETRIEVE also achieves distribution matching by selecting a coreset from the unlabeled set with similar gradients to the labeled set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Our Contributions</head><p>The contributions are our work can be summarized as follows:</p><p>? RETRIEVE Framework: We propose a coreset selection algorithm RETRIEVE for efficient and robust semi-supervised learning. RETRIEVE poses the coreset selection as a discrete-continuous bi-level optimization problem and solves it efficiently using an online approximation of single-step gradient updates. Essentially, RETRIEVE selects a coreset of the unlabeled set, which, when trained using the combination of the labeled set and the specific unlabeled data coreset, minimizes the model loss on the labeled dataset. We also discuss several implementation tricks to speed up the coreset selection step significantly (c.f., Section 3.3, Section 3.4)</p><p>? RETRIEVE in Traditional SSL: We empirically demonstrate the effectiveness of RETRIEVE in conjunction with several SOTA SSL algorithms like VAT, Mean-Teacher, and FixMatch. The speedups obtained by RETRIEVE are shown in Figure <ref type="figure" target="#fig_1">2a</ref>. Specifically, we see that RETRIEVE consistently achieves close to 3? speedup with accuracy degradation of around 0.7%. RETRIEVE also achieves more than 4.2? speedup with a slightly higher accuracy degradation. Furthermore, when RETRIEVE is trained for more iterations, RETRIEVE can match the performance of VAT while having a 2? speedup (see VAT Extended bar plot in Figure <ref type="figure" target="#fig_1">2a</ref>). RETRIEVE also consistently outperforms simple baselines like early stopping and random sampling.</p><p>? RETRIEVE in Robust SSL: We further demonstrate the utility of RETRIEVE for robust SSL in the presence of OOD data and imbalance in the unlabeled set. We observe that with the VAT SSL algorithm, RETRIEVE outperforms SOTA robust SSL method DS3L <ref type="bibr" target="#b16">[17]</ref> (with VAT) while being around 5? faster. RETRIEVE also significantly outperforms just VAT and random sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Related Work</head><p>Semi-supervised learning: Several papers have been proposed for semi-supervised learning over the past few years. Due to space constraints, we do not talk about generative <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b2">3]</ref> and graph-based <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b32">33]</ref> methods for SSL in this work. We instead focus on the main components of the existing SOTA SSL algorithms, viz., a) consistency regularization and b) entropy minimization.</p><p>The consistency regularization component forces the model to have consistent prediction given an unlabeled data point and its perturbed (or augmented) version. The Entropy-minimization component forces the model instances to have low-entropy predictions on unlabeled data instances to ensure that the classes are well separated. One can achieve entropy minimization by directly adding the entropy loss component on the unlabeled class prediction or using methods like Pseudo-Labeling to enforce it implicitly. Mean-Teacher <ref type="bibr" target="#b55">[56]</ref> approach uses a consistency regularization component that forces the predictions of the exponential moving average of the model to be the same as the model prediction of the augmented unlabeled images. VAT <ref type="bibr" target="#b41">[42]</ref> instead computes the perturbation of the unlabeled data point that changes the prediction distribution the most and enforces the model to have the same prediction on both unlabeled data instance and unlabeled data instance with computed perturbation as a form of consistency regularization. MixMatch <ref type="bibr" target="#b3">[4]</ref> uses K standard image augmentations for consistency regularization and enforces entropy minimization by using a sharpening function on the average predicted distribution of K augmentations of unlabeled data instances. FixMatch <ref type="bibr" target="#b52">[53]</ref> induces consistency regularization by forcing the model to have the same prediction on a weakly augmented and strongly augmented image instance. Furthermore, FixMatch <ref type="bibr" target="#b52">[53]</ref> also employs confidence thresholding to mask unlabeled data instances on which the model's prediction confidence is below a threshold from being used in consistency loss.</p><p>Robust Semi-supervised learning: Several methods have been proposed to make the existing semi-supervised learning algorithms robust to label noise in labeled data and robust to OOD data in the unlabeled set. A popular approach <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b51">52]</ref> to deal with label noises and class imbalance in a supervised learning setting is by reweighing each data instance and jointly learning these weights along with the model parameters. Safe-SSL (DS3L) <ref type="bibr" target="#b16">[17]</ref> is a recently proposed SOTA method for robust SSL learning. DS3L is similar to the reweighting in the supervised case and adopts a reweighting approach to deal with OOD data in the unlabeled set. Safe-SSL uses a neural network to predict the weight parameters of unlabeled instances that result in maximum labeled set performance, making it a bi-level optimization problem. In this regard, both RETRIEVE and Safe-SSL approach solves a bi-level optimization problem, except that RETRIEVE solves a discrete optimization problem at the outer level, thereby enabling significant speedup compared to SSL algorithms and an even more considerable  speedup compared to safe-SSL (which itself is 3? slower than SSL algorithms). In contrast to safe-SSL and other robust SSL approaches, RETRIEVE achieves both efficiency and robustness. Other approaches for robust SSL include UASD <ref type="bibr" target="#b8">[9]</ref> uses an Uncertainty aware self-distillation with OOD filtering to achieve robust performance and a distributionally robust model to deal with OOD <ref type="bibr" target="#b7">[8]</ref>.</p><p>Coreset and subset selection methods: Coresets <ref type="bibr" target="#b12">[13]</ref> are small and informative weighted data subsets that approximate original data. Several works <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b22">23]</ref> have studied coresets for efficient training of deep learning models in the supervised learning scenarios. CRAIG <ref type="bibr" target="#b38">[39]</ref> selects representative coresets of the training data that closely estimates the full training gradient. Another approach, GLISTER <ref type="bibr" target="#b23">[24]</ref> posed the coreset selection as optimizing the validation set loss for efficient learning focused on generalization. Another approach, GRAD-MATCH <ref type="bibr" target="#b22">[23]</ref> select subsets that approximately match the full training loss or validation loss gradient using orthogonal matching pursuit. Similarly, coreset selection methods <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b23">24]</ref> were also used for active learning scenario, where a subset of data instances from the unlabeled set is selected to be labeled. Finally, several recent works have used submodular functions for finding diverse and representative subsets for data subset selection <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b57">58]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>Notation: Denote D = {x i , y i } n i=1 to be the labeled set with n labeled data points, and U = {x j } m j=1 to be the unlabeled set with m data points. Let ? be the classifier model parameters, l s be the labeled set loss function (such as cross-entropy loss) and l u be the unlabeled set loss, e.g. consistencyregularization loss, entropy loss, etc.. Denote L S (D, ?)</p><formula xml:id="formula_0">= i?D l s (?, x i , y i ) and L U (U, ?, m) = j?U m i l u (x j , ?)</formula><p>where m ? {0, 1} m is the binary mask vector for unlabeled set. For notational convenience, we denote l si (?) = l s (x i , y i , ?) and denote l uj (?) = l u (x j , ?).</p><p>Semi-supervised loss: Following the above notations, the loss function for many existing SSL algorithms can be written as L S (D, ?) + ?L U (U, ?, m), where ? is the regularization coefficient for the unlabeled set loss. For Mean Teacher <ref type="bibr" target="#b55">[56]</ref>, VAT <ref type="bibr" target="#b41">[42]</ref>, MixMatch <ref type="bibr" target="#b3">[4]</ref>, the mask vector m is made up entirely of ones, whereas for FixMatch <ref type="bibr" target="#b52">[53]</ref>, m is confidence-thresholded binary vector, indicating whether to include an unlabeled data instance or not. Usually, L S is a cross-entropy loss for classification experiments and squared loss for regression experiments. A detailed formulation of the loss function L U used in different SSL algorithms is given in Appendix C</p><p>Robust Semi-supervised loss: However, for robust semi-supervised loss, the mask vector m is replaced with a weight vector w ? R m denoting the contribution of data instances in the unlabeled set. The weight vector w is unknown and needs to be learned. The weighted SSL loss is:</p><formula xml:id="formula_1">L S (D, ?) + ?L U (U, ?, w)</formula><p>, where ? is the regularization coefficient for the unlabeled set loss.</p><p>The state-of-the-art robust SSL method, Safe-SSL <ref type="bibr" target="#b16">[17]</ref> poses the learning problem as:</p><formula xml:id="formula_2">outer-level w * = argmin w L S (D, argmin ? (L S (D, ?) + ?L U (U, ?, w)) inner-level )<label>(1)</label></formula><p>In order to solve the problem at the inner level efficiently, Safe-SSL <ref type="bibr" target="#b16">[17]</ref> method uses a single-gradient step approximation to estimate the inner problem solution. The weight vector learning problem after the one-step approximation is:</p><formula xml:id="formula_3">w * = argmin w L S (D, ? -?? ? L S (D, ?) -??? ? L U (U, ?, w)</formula><p>Safe-SSL also uses a single-step gradient approximation to solve the outer level problem as well.</p><p>As discussed before, the optimization problem of the Safe-SSL <ref type="bibr" target="#b16">[17]</ref> algorithm involves continuous optimization at both inner and outer levels, whereas for RETRIEVE, the outer level involves a discrete optimization problem which makes it significantly faster than Safe-SSL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RETRIEVE framework</head><p>In RETRIEVE, the coreset selection and classifier model learning on the selected coreset is performed in conjunction. As shown in the Figure <ref type="figure" target="#fig_2">3</ref>, RETRIEVE trains the classifier model on the previously selected coreset for R epochs in a semi-supervised manner, and every R th epoch, a new coreset is selected, and the process is repeated until the classifier model reaches convergence, or the required number of epochs is reached. The vital feature of RETRIEVE is that the coresets selected are adapted with the training. Let ? t be the classifier model parameters and the S t be the coreset at time step t.</p><p>Since coreset selection is done every R epochs, we have S t = S t/R , or in other words, the subsets change only after R epochs. The SSL loss function on the selected coreset at iteration t is as follows:</p><formula xml:id="formula_4">L S (D, ? t ) + ? t j?St m jt l u (x j , ? t )<label>(2)</label></formula><p>where m jt is the mask binary value associated with the j th point based on model parameters ? t and ? t is the unlabeled loss coefficient at iteration t. Note that objective function given in Equation ( <ref type="formula" target="#formula_4">2</ref>) is dependent on the SSL algorithm used in RETRIEVE framework. If gradient descent is used for learning, the parameter update step from time step t to t + 1 is as follows:</p><formula xml:id="formula_5">? t+1 = ? t -? t ? ? L S (D, ? t ) -? t ? t j?St m jt ? ? l u (x j , ? t )<label>(3)</label></formula><p>where ? t is the learning rate at iteration t. The update step for mini-batch SGD is similar, just that it does the above on minibatches of the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation</head><p>The coreset selection problem of RETRIEVE at timestep t is as follows:</p><p>outer-level</p><formula xml:id="formula_6">S t = argmin S?U :|S|?k L S D, argmin ? L S (D, ? t ) + ? t j?S m jt l u (x j , ? t ) inner-level<label>(4)</label></formula><p>where k is the size of the coreset and m jt is the binary value associated with the j th instance based on model parameters ? t . k is a fraction of the entire dataset (e.g. 20% or 30%), and the goal is to select the best subset of the unlabeled set, which maximizes the labeled loss based. The outer level of the above optimization problem is a discrete subset selection problem. However, solving the inner-optimization problem naively is computationally intractable, and, we need to make some approximations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">One-Step Gradient Approximation</head><p>To solve the inner optimization problem efficiently, RETRIEVE adopts a one-step gradient approximation based optimization method similar to <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b48">49]</ref>. More specifically, RETRIEVE approximates the solution to the inner level problem by taking a single gradient step towards the descent direction of the loss function. The idea here is to jointly optimize the model parameters and the subset as the learning proceeds. After this approximation, the coreset selection optimization problem becomes:</p><formula xml:id="formula_7">S t = argmin S?U :|S|?k L S (D, ? t -? t ? ? L S (D, ? t ) -? t ? t j?S m jt ? ? l u (x j , ? t ))<label>(5)</label></formula><p>However, even after this approximation, the above optimization problem (Equation ( <ref type="formula" target="#formula_7">5</ref>)) is NP-hard.</p><p>Theorem 1 Optimization problem (Equation ( <ref type="formula" target="#formula_7">5</ref>)) is NP hard, even if l s is a convex loss function. If the labeled set loss function l s is cross-entropy loss, then the optimization problem give in the Equation ( <ref type="formula" target="#formula_7">5</ref>) can be converted into an instance of cardinality constrained weakly submodular maximization.</p><p>The proof is given in Appendix B. The given Theorem 1 holds as long as l s is a cross-entropy loss irrespective of the form of l u . Further, Theorem 1 implies that the optimization problem given in Equation ( <ref type="formula" target="#formula_7">5</ref>) can be solved efficiently using greedy algorithms <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38]</ref> with approximation guarantees. RETRIEVE uses stochastic-greedy algorithm <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b37">38]</ref> to solve the optimization problem Equation ( <ref type="formula" target="#formula_7">5</ref>) with an approximation guarantee of 1-1/e ?in O(m log(1/ )) iterations where m is the unlabeled set size and ? is the weak submodularity coefficient (see Appendix B). And the set function used in stochastic greedy algorithm is as follows:</p><formula xml:id="formula_8">f (? t , S) = -L S (D, ? t -? t ? ? L S (D, ? t ) -? t ? t j?S m jt ? ? l u (x j , ? t ))<label>(6)</label></formula><p>Notice that during each greedy iteration, we need to compute the set function value f (? t , S ? e) to find the maximal gain element e that can be added to the set S. This implies that the loss over the entire labeled set needs to be computed multiple times for each greedy iteration, making the entire greedy selection algorithm computationally expensive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">RETRIEVE Algorithm</head><p>To make the greedy selection algorithm efficient, we approximate the set function value f (? t , S ? e) with the first two terms of it's Taylor-series expansion Let,</p><formula xml:id="formula_9">? S = ? t -? t ? ? L S (D, ? t ) - ? t ? t j?S m jt ? ? l u (x j , ? t ).</formula><p>The modified set function value with Taylor-series approximation is as follows:</p><formula xml:id="formula_10">f (? t , S ? e) = -L S (D, ? S ) + ? t ? t ? ? L S (D, ? S ) T m et ? ? l u (x e , ? t )<label>(7)</label></formula><p>where m et is the binary mask value associated with element e. Note that the term m et ? ? l u (x e , ? t ) can be precomputed at the start of the greedy selection algorithm, and the term ? ? L S (D, ? S ) needs to be computed only once every greedy iteration, thereby reducing the computational complexity of the greedy algorithm. Set ?t+1 = ? tB t = t + 1 until t ? T return ? T , S T A detailed pseudo-code of the RETRIEVE algorithm is given in Algorithm 1. RETRIEVE uses a greedy selection algorithm for coreset selection, and the detailed pseudo-code of the greedy algorithm is given in Algorithm 2. RETRIEVE can be easily implemented with popular deep learning frameworks <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b0">1]</ref> that provide auto differentiation functionalities. In all our experiments, we set R = 20, i.e., we update the coreset every 20 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Additional Implementation Details:</head><p>In this subsection, we discuss additional implementational and practical tricks to make RE-TRIEVE scalable and efficient. Last-layer gradients. Computing the gradients over deep models is time-consuming due to an enormous number of parameters in the model. To address this issue, we adopt a last-layer gradient approximation similar to <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b22">23]</ref> by only considering the last classification layer gradients of the classifier model in RETRIEVE. By simply using the last-layer gradients, we achieve significant speedups in RETRIEVE. Warm-starting data selection: We warm start the classifier model by training it on the entire unlabeled dataset for a few epochs similar to <ref type="bibr" target="#b22">[23]</ref>. Warm starting allows the classifier model to have a  good starting point to provide informative loss gradients used for coreset selection. More specifically, we train the classifier model on the entire unlabeled set for T w = ?T k m epochs where k is coreset size, T is the total number of epochs, ? is the fraction of warm start, and m is the size of the unlabeled set. To be fair, we consider all baselines in the standard SSL setting with the same warm start. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Epochs vs. Iterations Algorithm 2: GreedySelection</head><formula xml:id="formula_11">for i = 1 to k do Compute ? S = ?t -?t? ? L S (D, ?t) -?t?t j?S mjt? ? lu(xj , ?t) Compute ? ? L S (D, ? S ) V ? Sample mlog(1/ ) instances from U best -gain = -? for e ? V do Compute gain ?(e) = ?t?t? ? L S (D, ? S ) T me? ? lu(xe, ?t) if ?(e) &gt; best -gain then Set s = e Set best -gain = ?(e) St = St ? s U = U \ s return St</formula><p>Most of the existing SSL algorithms are trained using a fixed number of iterations instead of epochs. However, for easier comprehension of the RETRIEVE algorithm, we use the epoch notation in our work. A single epoch here meant a pass over random mini-batches of data points, such that the total number of data points encountered is equal to the size of the coreset of the unlabeled data. For example, if the unlabeled set size is 50000 and the unlabeled batch size is 50, then a single epoch over 100%, 50%, and 30% subsets are equivalent to 1000, 500, and 300 iterations, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Our experimental section aims to verify the efficiency and effectiveness of RETRIEVE by evaluating RETRIEVE through three semi-supervised learning scenarios a) traditional SSL scenario with clean data, b) robust SSL with OOD, and c) robust SSL with class imbalance, to demonstrate the efficiency and the robustness of RETRIEVE. Furthermore, our work's experimental scenarios are very relevant in terms of research and real-world applications. We have implemented the RETRIEVE algorithmic framework using PyTorch <ref type="bibr" target="#b45">[46]</ref>. We repeat the same experiment for three runs with different initialization and report the mean test accuracies in our plots. A detailed table with both mean test accuracy and the standard deviations was given in Appendix (G, H). For a fair comparison, we use the same random seed in each trial for all methods. We explain implementation details, datasets, and baselines used in each scenario in the following subsections.</p><p>Baselines in each setting. In this section, we discuss baselines that are used in all the scenarios considered. We begin with the traditional SSL scenario. In this setting, we run RETRIEVE (and all baselines) with warm-start. We incorporate RETRIEVE with three representative SSL methods, including Mean Teacher (MT) <ref type="bibr" target="#b55">[56]</ref>, Virtual Adversarial Training (VAT) <ref type="bibr" target="#b41">[42]</ref> and FixMatch <ref type="bibr" target="#b52">[53]</ref>. The baselines considered are RANDOM (where we just randomly select a subset of unlabeled data points of the same size as RETRIEVE), CRAIG <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b22">23]</ref> and FULL-EARLYSTOP. CRAIG <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b22">23]</ref> was actually proposed in the supervised learning scenario. We adapt it to SSL by choosing a representative subset of unlabeled points such that the gradients are similar to the unlabeled loss gradients. We run the per-batch variant of CRAIG proposed in <ref type="bibr" target="#b22">[23]</ref>, where we select a subset of mini-batches instead of data instances for efficiency and scalability. Similarly, we use the per-batch version of GRADMATCH proposed in <ref type="bibr" target="#b22">[23]</ref> adapted to SSL setting as another baseline. For more information on the formulation of CRAIG and GRADMATCH in the SSL case, see Appendix D, E. Again, we emphasize that RANDOM, CRAIG, and GRADMATCH are run with early stopping for the same duration as RETRIEVE. In FULL-EARLYSTOP baseline, we train the model on the entire unlabeled set for the time taken by RETRIEVE and report the test accuracy. In the traditional SSL scenario, we use warm variants of RETRIEVE, RANDOM, CRAIG for SSL training because warm variants are better in accuracy and efficiency compared to not performing warm start -see Appendix G for a careful comparison of both. Robust SSL with OOD and Imbalance: In the robust learning scenario for both OOD and imbalance, we analyze the performance of RETRIEVE with the VAT <ref type="bibr" target="#b41">[42]</ref> algorithm. Note that for the Robust SSL scenario, we do not warm start the model by training for a few iterations on the full unlabeled set because training on an entire unlabeled set(containing OOD or class imbalance) creates a biased model due to a distribution mismatch between labeled set and unlabeled set. We empirically compare not warm starting the model with warm starting in Appendix H. In the robust SSL case, we compare RETRIEVE with two robust SSL algorithms DS3L <ref type="bibr" target="#b16">[17]</ref> and L2RW <ref type="bibr" target="#b48">[49]</ref>. DS3L (also called Safe-SSL) is a robust learning approach using a meta-weight network proposed specifically for robust SSL. We adapt L2RW(Learning to Reweight), originally proposed for robust supervised learning, to the SSL case and use it as a baseline. Similarly, we adapt the robust coreset selection method CRUST <ref type="bibr" target="#b39">[40]</ref> originally proposed to tackle noisy labels scenario in supervised learning to SSL setting and use it as a baseline in Robust SSL scenario. We begin by providing details common across the three scenarios. We perform experiments on the following image classification datasets: CIFAR-10 <ref type="bibr" target="#b27">[28]</ref> (60000 instances), SVHN <ref type="bibr" target="#b42">[43]</ref> (99289 instances) and the following sentiment analysis datasets: IMDB <ref type="bibr" target="#b35">[36]</ref> <ref type="foot" target="#foot_2">2</ref> (10000 instances), and ELEC <ref type="bibr" target="#b19">[20]</ref> <ref type="foot" target="#foot_3">3</ref> (246714 instances) datasets. We use a modified version of the ELEC dataset where the duplicate sentences are removed. For CIFAR-10, we use a labeled set of 4000 instances with 400 instances from each class, an unlabeled set of 50000 instances, a test set of 10000 instances. For SVHN, we used a labeled set of 1000 instances with 100 instances from each class, an unlabeled set of 73257 instances, a test set of 26032 instances. For IMDB and ELEC datasets, we use the labeled and unlabeled splits following the work <ref type="bibr" target="#b40">[41]</ref>. For CIFAR10 and SVHN datasets, we use the Wide-ResNet-28-2 <ref type="bibr" target="#b59">[60]</ref> model that is commonly used in SSL <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b52">53]</ref>. For MNIST, we use a variant of LeNet <ref type="bibr" target="#b30">[31]</ref> (see Appendix F for details). For IMDB and ELEC datasets, we use a model comprising of Word Embedding layer, LSTM model, and two-layer MLP model following the architecture given in the work <ref type="bibr" target="#b40">[41]</ref>. Similar to the work <ref type="bibr" target="#b40">[41]</ref>, we initialize the embedding matrix and LSTM model weights using a pretrained recurrent language model using both the labeled and unlabeled data. For Image datasets, with RE-TRIEVE(and baselines like RANDOM, GRADMATCH and CRAIG), we use the Nesterov's accelerated SGD optimizer with a learning rate of 0.03, weight decay of 5e-4, the momentum of 0.9, and a cosine annealing <ref type="bibr" target="#b34">[35]</ref> learning rate scheduler for all the experiments. For the FULL and FULLEARLYSTOP, we use the Adam optimizer <ref type="bibr" target="#b24">[25]</ref> and follow the experimental setting from the SSL papers <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b55">56]</ref>.</p><p>For Text datasets, with RETRIEVE(and baselines like RANDOM, GRADMATCH and CRAIG), we set the hyperparameter values following the work <ref type="bibr" target="#b40">[41]</ref>. Similarly, for the robust SSL baselines, we use the settings from the corresponding papers <ref type="bibr" target="#b16">[17]</ref>. For all our experiments using image datasets, we use a batch size of 50 for labeled and unlabeled sets. Next, we discuss the specific settings for the traditional SSL scenario. For CIFAR10, we train the model for 500 epochs, and for SVHN, we train the model for 340 epochs on an unlabeled set. Note that we mention the epochs here because the number of iterations depends on the size of the unlabeled sets since that would determine the number of mini-batches. For a fair comparison, we train all algorithms for a fixed number of epochs. Next, we look at robust SSL for OOD. In this scenario, we consider the presence of OOD in the unlabeled set. We introduce OOD into CIFAR-10 following <ref type="bibr" target="#b43">[44]</ref>, by adapting it to a 6-class dataset, with 400 labels per class (from the 6 animal classes) as ID and rest of the classes as OOD (ID classes are: "bird", "cat", "deer", "dog", "frog", "horse", and OOD data are from classes: "airline", "automobile", "ship", "truck"). Similarly, we adapt MNIST <ref type="bibr" target="#b31">[32]</ref> to a 6-class dataset, with classes 1-6 as ID and classes 7-10 as OOD. We denote the OOD ratio=U ood /(U ood + U in ) where U in is ID unlabeled set, U ood is OOD unlabeled set. For CIFAR10, we use a labeled set of 2400 instances and an unlabeled set of 20000 instances, and for MNIST, we use a labeled set of 60 instances and an unlabeled set of 30000 instances. Finally, for robust SSL for class imbalance, we consider imbalance both in the labeled set and unlabeled set. We introduce imbalance into the CIFAR-10 dataset by considering classes 1-5 as imbalanced classes and a class imbalance ratio. The class imbalance ratio is defined as the ratio of instances from classes 1-5 and the number of instances from classes 6-10. For CIFAR-10, we use a labeled set of 2400 and an unlabeled set of 20000 instances. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Traditional SSL Results:</head><p>The results comparing the accuracyefficiency tradeoff between the different subset selection approaches are shown in Figure <ref type="figure">4</ref>. We compare the performance for different subset sizes of the unlabeled data: 10%, 20%, and 30% and three representative SSL algorithms VAT, Mean-Teacher, and FixMatch. For warm-start, we set kappa value to ? = 0.5 (i.e., training for 50% epochs on the entire unlabeled set and 50% using coresets). Our experiments use a R value of 20 (i.e., coreset selection every 20 epochs). Sub-figures(4a, 4b, 4d, 4e, 4c) shows the plots of relative error vs speedup, both w.r.t full training (i.e., original SSL algorithm). Sub-figure <ref type="figure">4f</ref> shows the plot of relative error vs CO2 emissions efficiency, both w.r.t full training. CO2 emissions were estimated based on the total compute time using the Machine Learning Impact calculator presented in <ref type="bibr" target="#b28">[29]</ref>. From the results, it is evident that RETRIEVE achieved the best speedup vs. accuracy tradeoff and is environmentally friendly based on CO2 emissions compared to other baselines (including CRAIG and FULLEARLYSTOP). In particular, RETRIEVE achieves speedup gains of 2.7x and 4.4x with a performance loss of 0.7% and 0.3% using VAT on CIFAR10 and SVHN datasets. Further, RETRIEVE achieves speedup gains of 2.9x, 3.2x with a performance loss of 0.02% and 0.5% using Mean-Teacher on CIFAR10 and SVHN datasets. Additionally, RETRIEVE achieves a speedup of 3.8x with a performance loss of 0.7% using FixMatch on the CIFAR10 dataset. Sub-figures(6a, 6b) shows the plots of relative error vs speedup both w.r.t full training (i.e., original SSL algorithm) on IMDB and ELEC datasets for 30% subset size. In particular, RETRIEVE achieves speedup gains of 2.68x and 2.5x with a performance loss of 0.5% and 0.6% for 30% subset of IMDB and ELEC datasets. Figure <ref type="figure" target="#fig_6">5</ref> shows the results comparing the BAYESIANCORESET method <ref type="bibr" target="#b5">[6]</ref>, adapted to the SSL setting with RETRIEVE using the VAT algorithm for 20% and 30% CIFAR10 subsets. The results show that RETRIEVE achieves better performance than the SSL extension of the BAYESIANCORESET selection method in terms of model performance and speedup. One possible explanation for it is that the BAYESIANCORESET approach was not developed for efficient learning but instead was developed to capture coresets that try to represent the log-likelihood of the entire dataset that MCMC methods can further use. We would also like to point out that we used the original code implementation of BAYESIANCORESET that is not meant for GPU usage in our  experiments. Hence the speedups of the BAYESIANCORESET approach can be further improved with efficient code implementation. Subfigure 4h shows that RETRIEVE achieves faster convergence compared to all other methods on CIFAR10 for 30% subset with Mean-Teacher. Subfigure 4g shows the extended convergence of RETRIEVE on CIFAR10 for 20% and 30% subsets using VAT, where the RETRIEVE is allowed to train for larger epochs to achieve comparable accuracy with Full training at the cost of losing some efficiency. Note that the points marked by * in subfigure 4g denote the actual training endpoint, i.e. the usual number of epochs/iterations used to obtain points in subfigures (4a, 4b, 4d, 4e, 4c). We observe that RETRIEVE matches the performance of VAT while being close to 2? faster in running times (and correspondingly energy efficiency). We repeat this experiment with MT in the Appendix G. Also, more detailed results with additional convergence plots and tradeoff curves are in Appendix G.</p><p>Robust SSL Results: We test the performance of RETRIEVE on CIFAR10 and MNIST datasets with OOD in the unlabeled set and CIFAR10 dataset with the class imbalance in both labeled and unlabeled sets. sub-figures 7a, 7b shows the accuracy plots of RETRIEVE for different OOD ratios of 25%, 50% and 75%. The results show that RETRIEVE with VAT outperforms all other baselines, including DS3L <ref type="bibr" target="#b16">[17]</ref>, a state-of-the-art robust SSL baseline in the OOD scenario. Next, sub-figure <ref type="figure" target="#fig_8">7c</ref> shows the accuracy plots of RETRIEVE for different class imbalance ratios of 10%, 30% and 50% on CIFAR-10 dataset. The results show that RETRIEVE with VAT outperforms all other baselines, including DS3L <ref type="bibr" target="#b16">[17]</ref> (also run with VAT) in the class imbalance scenario as well. In particular, RETRIEVE outperforms other baselines by at least 1.5% on the CIFAR-10 with imbalance. Sub-figure <ref type="figure" target="#fig_8">7d</ref> shows the time taken by different algorithms on the CIFAR10 dataset with a 50% class imbalance ratio. The results show that CRUST did not perform well in terms of accuracy and speedups achieved compared to RETRIEVE. Except for MixUP, CRUST is similar to CRAIG, which did not perform well compared to RETRIEVE in a traditional SSL setting. Furthermore, the performance gain due to MixUP for coreset selection in the SSL setting is minimal. The minimal gain can be attributed to the fact that the hypothesized labels used for MixUP in the earlier stages of training are noisy. Furthermore, as stated earlier, CRUST was developed to tackle noisy labels in a supervised learning setting and is not developed to deal with OOD or Class Imbalance in general. The results show that RETRIEVE is more efficient compared to the other baselines. In particular, RETRIEVE is 5x times faster compared to DS3L method. Other detailed results (tradeoff curves and convergence curves) are in Appendix H.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Broader Impacts</head><p>We introduce RETRIEVE, a discrete-continuous bi-level optimization based coreset selection method for efficient and robust semi-supervised learning. We show connections with weak-submodularity, which enables the coreset selection in RETRIEVE to be solved using a scalable stochastic greedy algorithm. Empirically, we show that RETRIEVE is very effective for SSL. In particular, it achieves 3? speedup on a range of SSL approaches like VAT, MT, and FixMatch with around 0.7% accuracy loss and a 2? speedup with no accuracy loss. In the case of robust SSL with imbalance and OOD data, RETRIEVE outperforms existing SOTA methods while being 5? faster. We believe RETRIEVE has a significant positive societal impact by making SSL algorithms (specifically robust SSL) significantly faster and more energy-efficient, thereby reducing the CO2 emissions incurred during training.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: (a )Unlabeled set with the same distribution as the labeled set, (b) Unlabeled set containing OOD instances, (c) Unlabeled set where the class distribution is imbalanced</figDesc><graphic url="image-1.png" coords="2,276.30,75.69,217.81,79.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Comparison of RETRIEVE with VAT, FixMatch, and MT on CIFAR-10 and SVHN: We contrast the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Flowchart of RETRIEVE framework, where coreset selection is performed every R epochs and the model is trained on the selected coreset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 1 : 1 t=0, 1 t=0Input:</head><label>111</label><figDesc>RETRIEVE AlgorithmInput: Labeled Set: D, Unlabeled Set: U , Reg. Coefficients:{?t} t=T -Learning rates: {?t} t=T -Total no of epochs: T , Epoch interval for subset selection: R, Size of the coreset: k Set t = 0; Randomly initialize model parameters ?0 and coresetS0 ? U : |S0| = k; repeat if (t%R == 0) ? (t &gt; 0) then St = GreedySelection(D, U , ?t, ?t, ?t, k) else St = St-1 Compute batches D b = ((x b , y b ); b ? (1 ? ? ? B)) from D Compute batches S tb = ((x b ); b ? (1 ? ? ? B)) from S *** Mini-batch SGD *** Set ?t0 = ?t for b = 1 to B do Compute mask mt on S tb from current model parameters ? t(b-1) ? tb = ? t(b-1) -?t? ? L S (D b , ?t)?t?t j?S tb mjt? ? lu(xj , ?t(b -1))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Input:</head><label></label><figDesc>Labeled Set: D, Unlabeled Set: U , Model Parameters: ?t, Learning rate: ?t Input: Regularization Coefficient: ?t, Budget: k Initialize St = ? Set m = |U | Compute mask values m based on current model parameters ?t for e ? U do Compute ?e = me? ? lu(xe, ?t)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Performance comparison of RETRIEVE vs Bayesian Coreset using VAT for 20% and 30% CIFAR10 subsets Datasets, Model architecture and Experimental Setup:We begin by providing details common across the three scenarios. We perform experiments on the following image classification datasets: CIFAR-10<ref type="bibr" target="#b27">[28]</ref> (60000 instances), SVHN<ref type="bibr" target="#b42">[43]</ref> (99289 instances) and the following sentiment analysis datasets: IMDB<ref type="bibr" target="#b35">[36]</ref> 2 (10000 instances), and ELEC<ref type="bibr" target="#b19">[20]</ref> 3 (246714 instances) datasets. We use a modified version of the ELEC dataset where the duplicate sentences are removed. For CIFAR-10, we use a labeled set of 4000 instances with 400 instances from each class, an unlabeled set of 50000 instances, a test set of 10000 instances. For SVHN, we used a labeled set of 1000 instances with 100 instances from each class, an unlabeled set of 73257 instances, a test set of 26032 instances. For IMDB and ELEC datasets, we use the labeled and unlabeled splits following the work<ref type="bibr" target="#b40">[41]</ref>. For CIFAR10 and SVHN datasets, we use the Wide-ResNet-28-2<ref type="bibr" target="#b59">[60]</ref> model that is commonly used in SSL<ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b52">53]</ref>. For MNIST, we use a variant of LeNet<ref type="bibr" target="#b30">[31]</ref> (see Appendix F for details). For IMDB and ELEC datasets, we use a model comprising of Word Embedding layer, LSTM model, and two-layer MLP model following the architecture given in the work<ref type="bibr" target="#b40">[41]</ref>. Similar to the work<ref type="bibr" target="#b40">[41]</ref>, we initialize the embedding matrix and LSTM model weights using a pretrained recurrent language model using both the labeled and unlabeled data. For Image datasets, with RE-TRIEVE(and baselines like RANDOM, GRADMATCH and CRAIG), we use the Nesterov's accelerated SGD optimizer with a learning rate of 0.03, weight decay of 5e-4, the momentum of 0.9, and a cosine annealing<ref type="bibr" target="#b34">[35]</ref> learning rate scheduler for all the experiments. For the FULL and FULLEARLYSTOP, we use the Adam optimizer<ref type="bibr" target="#b24">[25]</ref> and follow the experimental setting from the SSL papers<ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b55">56]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: A comparison of RETRIEVE with baselines (RANDOM, CRAIG, GRADMATCH, FULL, and FULLEARLYSTOP in the traditional SSL setting for text datasets. SpeedUp vs Accuracy Degradation, both compared to SSL for (a) VAT on 30% IMDB, (b) VAT on 30% ELEC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Subfigures (a) to (c) compare the performance of RETRIEVE with baselines (including DS3L and L2RW) for different OOD ratios (a,b) and imbalance ratios (c). We see that RETRIEVE outperforms both baselines in most of the cases. Furthermore, RETRIEVE achieves this while being close to 2? faster than VAT (the SSL algorithm) and 5? faster than the robust SSL algorithms (DS3L and L2RW).</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>coResets for EfficienT and Robust semI-supErVised lEarning 35th Conference on Neural Information Processing Systems (NeurIPS</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>2021).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>https://ai.stanford.edu/ amaas/data/sentiment/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3"><p>http://riejohnson.com/cnn_data.html</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="6">Acknowledgments and Disclosure of Funding</head><p>We would like to thank NeurIPS area chairs and anonymous reviewers for their efforts in reviewing this paper and their constructive comments! RI and KK were funded by the <rs type="funder">National Science Foundation(NSF)</rs> under Grant Number <rs type="grantNumber">2106937</rs>, a startup grant from <rs type="funder">UT Dallas</rs>, and a <rs type="funder">Google and Adobe</rs> research award. FC and XZ were funded by <rs type="funder">National Science Foundation(NSF)</rs> under Grant Numbers <rs type="grantNumber">1815696</rs>, <rs type="grantNumber">1750911</rs>, and <rs type="grantNumber">2107449</rs>. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the <rs type="funder">National Science Foundation</rs>, Google or Adobe.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_Wv6pHNy">
					<idno type="grant-number">2106937</idno>
				</org>
				<org type="funding" xml:id="_762BvQk">
					<idno type="grant-number">1815696</idno>
				</org>
				<org type="funding" xml:id="_U5kDj4m">
					<idno type="grant-number">1750911</idno>
				</org>
				<org type="funding" xml:id="_YJku4mG">
					<idno type="grant-number">2107449</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for largescale machine learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th USENIX Symposium on Operating Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep batch active learning by diverse, uncertain gradient lower bounds</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Ash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps and spectral techniques for embedding and clustering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Conference on Neural Information Processing Systems: Natural and Synthetic, NIPS&apos;01</title>
		<meeting>the 14th International Conference on Neural Information Processing Systems: Natural and Synthetic, NIPS&apos;01<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="585" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Mixmatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<author>
			<persName><forename type="first">O</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fishel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Huck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jimeno-Yepes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>N?v?ol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Neves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Turchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Verspoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Conference on Machine Translation</title>
		<meeting>the Fourth Conference on Machine Translation<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Research Papers. Association for Computational Linguistics</publisher>
			<date type="published" when="2019-08-01">2019. August 1-2, 2019. 2019</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bayesian coreset construction via greedy iterative geodesic ascent</title>
		<author>
			<persName><forename type="first">T</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Broderick</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Dy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</editor>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018-07">Jul 2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="10" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Semi-Supervised Learning</title>
		<author>
			<persName><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schlkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zien</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
	<note>1st edition</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Distributionally robust semisupervised learning for people-centric sensing</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3321" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Semi-supervised learning under class distribution mismatch</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020-04">Apr. 2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="3569" to="3576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Multi-column deep neural networks for image classification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cire?an</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The importance of encoding versus training with sparse coding and vector quantization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on International Conference on Machine Learning, ICML&apos;11</title>
		<meeting>the 28th International Conference on International Conference on Machine Learning, ICML&apos;11<address><addrLine>Madison, WI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Omnipress</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="921" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Submodular meets spectral: Greedy algorithms for subset selection, sparse approximation and dictionary selection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kempe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on International Conference on Machine Learning, ICML&apos;11</title>
		<meeting>the 28th International Conference on International Conference on Machine Learning, ICML&apos;11<address><addrLine>Madison, WI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Omnipress</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1057" to="1064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Core-sets: Updated survey</title>
		<author>
			<persName><forename type="first">D</forename><surname>Feldman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sampling Techniques for Supervised or Unsupervised Tasks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="23" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">On the network visibility problem</title>
		<author>
			<persName><forename type="first">K</forename><surname>Gatmiry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gomez-Rodriguez</surname></persName>
		</author>
		<idno>CoRR, abs/1811.07863</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Golovin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Solnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kochanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Karro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sculley</surname></persName>
		</author>
		<title level="m">Google Vizier: A Service for Black-Box Optimization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Safe deep semi-supervised learning for unseen-class unlabeled data</title>
		<author>
			<persName><forename type="first">L.-Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">H</forename><forename type="middle">D</forename><surname>Iii</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</editor>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020-07">Jul 2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="13" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Super-human multi-talker speech recognition: A graphical modeling approach</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Olsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Kristjansson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Speech Lang</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="45" to="66" />
			<date type="published" when="2010-01">Jan. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Using deep belief nets to learn covariance kernels for gaussian processes</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Platt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Roweis</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semi-supervised convolutional neural networks for text categorization via region embedding</title>
		<author>
			<persName><forename type="first">R</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="919" to="927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning from less data: A unified data subset selection and active learning framework for computer vision</title>
		<author>
			<persName><forename type="first">V</forename><surname>Kaushal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kothawade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mahadev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Doctor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ramakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1289" to="1299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Scalable greedy feature selection via weak submodularity</title>
		<author>
			<persName><forename type="first">R</forename><surname>Khanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Elenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dimakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Negahban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ghosh</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1560" to="1568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Grad-match: A gradient matching based data subset selection for efficient learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Killamsetty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sivasubramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mirzasoleiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Iyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Glister: Generalization based data subset selection for efficient and robust learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Killamsetty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sivasubramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Iyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with deep generative models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Lawrence</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Quantifying the carbon emissions of machine learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Luccioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dandres</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.09700</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Simple and scalable predictive uncertainty estimation using deep ensembles</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<title level="m">MNIST handwritten digit database</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Deep metric transfer for label propagation with limited annotated data</title>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">SVitchboard II and FiSVer I: High-quality limitedcomplexity corpora of conversational English speech</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kirchhoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixteenth Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">SGDR: stochastic gradient descent with restarts</title>
		<author>
			<persName><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno>CoRR, abs/1608.03983</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011-06">June 2011</date>
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Accelerated greedy algorithms for maximizing submodular set functions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Minoux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Optimization techniques</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1978">1978</date>
			<biblScope unit="page" from="234" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Lazier than lazy greedy</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mirzasoleiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Badanidiyuru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karbasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vondr?k</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, AAAI&apos;15</title>
		<meeting>the Twenty-Ninth AAAI Conference on Artificial Intelligence, AAAI&apos;15</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1812" to="1818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Coresets for data-efficient training of machine learning models</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mirzasoleiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bilmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Coresets for robust training of deep neural networks against noisy labels</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mirzasoleiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="11465" to="11477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Adversarial training methods for semi-supervised text classification</title>
		<author>
			<persName><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<title level="s">Conference Track Proceedings. OpenReview.net</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24">2017. April 24-26, 2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Virtual adversarial training: A regularization method for supervised and semi-supervised learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ishii</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Realistic evaluation of deep semi-supervised learning algorithms</title>
		<author>
			<persName><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">:</forename><surname>Openai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>D?biak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dennison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Farhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hashme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>J?zefowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pachocki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P D O</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Raiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schlatter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sidor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dota</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>with large scale deep reinforcement learning</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Alch?-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Variational autoencoder for deep learning of images, labels and captions</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><surname>Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning to reweight examples for robust deep learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4334" to="4343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><surname>Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Active learning for convolutional neural networks: A core-set approach</title>
		<author>
			<persName><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Meta-weight-net: Learning an explicit mapping for sample weighting</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Fixmatch: Simplifying semi-supervised learning with consistency and confidence</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Energy and policy considerations for deep learning in nlp</title>
		<author>
			<persName><forename type="first">E</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Revisiting unreasonable effectiveness of data in deep learning era</title>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Submodularity in data subset selection and active learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1954" to="1963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Submodular subset selection for large-scale speech training data</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kirchhoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bartels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3311" to="3315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Robust semi-supervised learning through label aggregation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirtieth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<title level="m">Wide residual networks</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Semi-supervised learning using gaussian fields and harmonic functions</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twentieth International Conference on International Conference on Machine Learning, ICML&apos;03</title>
		<meeting>the Twentieth International Conference on International Conference on Machine Learning, ICML&apos;03</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="912" to="919" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
