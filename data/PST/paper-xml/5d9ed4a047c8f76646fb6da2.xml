<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Neural Multi-digraph Model for Chinese NER with Gazetteers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ruixue</forename><surname>Ding</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Pengjun</forename><surname>Xie</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xiaoyan</forename><surname>Zhang</surname></persName>
							<email>xiaoyan.loic@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Beihang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Lu</surname></persName>
							<email>luwei@sutd.edu.sg</email>
							<affiliation key="aff1">
								<orgName type="institution">Singapore University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Linlin</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Luo</forename><surname>Si</surname></persName>
							<email>luo.si@alibaba-inc.com</email>
						</author>
						<author>
							<persName><forename type="first">Alibaba</forename><surname>Group</surname></persName>
						</author>
						<title level="a" type="main">A Neural Multi-digraph Model for Chinese NER with Gazetteers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Gazetteers were shown to be useful resources for named entity recognition (NER) (Ratinov  and Roth, 2009). Many existing approaches to incorporating gazetteers into machine learning based NER systems rely on manually defined selection strategies or handcrafted templates, which may not always lead to optimal effectiveness, especially when multiple gazetteers are involved. This is especially the case for the task of Chinese NER, where the words are not naturally tokenized, leading to additional ambiguities. To automatically learn how to incorporate multiple gazetteers into an NER system, we propose a novel approach based on graph neural networks with a multidigraph structure that captures the information that the gazetteers offer. Experiments on various datasets show that our model is effective in incorporating rich gazetteer information while resolving ambiguities, outperforming previous approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Previous work <ref type="bibr">(Ratinov and Roth, 2009)</ref> shows that NER is a knowledge intensive task. Background knowledge is often incorporated into an NER system in the form of named entity (NE) gazetteers <ref type="bibr" target="#b14">(Seyler et al., 2018)</ref>. Each gazetteer is typically a list containing NEs of the same type. Many earlier research efforts show that an NER model can benefit from the use of gazetteers <ref type="bibr" target="#b9">(Li et al., 2005)</ref>. On the one hand, the use of NE gazetteers alleviates the need of manually labeling the data and can handle rare and unseen cases <ref type="bibr" target="#b16">(Wang et al., 2018)</ref>. On the other hand, resources of gazetteers are abundant. Many gazetteers have been manually created by previous studies <ref type="bibr" target="#b19">(Zamin and Oxley, 2011)</ref>. Besides, gazetteers can also be easily constructed from knowledge bases (e.g., Freebase <ref type="bibr" target="#b0">(Bollacker et al., 2008)</ref>) or com- While such background knowledge can be helpful, in practice the gazetteers may also contain irrelevant and even erroneous information which harms the system's performance <ref type="bibr" target="#b3">(Chiu and Nichols, 2016)</ref>. This is especially the case for Chinese NER, where enormous errors can be introduced due to wrongly matched entities. Chinese language is inherently ambiguous since the granularity of words is less well defined than other languages (such as English). Thus massive wrongly matched entities can be generated with the use of gazetteers. As we can see from the example shown in Figure <ref type="figure" target="#fig_0">1</ref>, matching a simple 9-character sentence with 4 gazetteers may result in 6 matched entities, among which 2 are incorrect.</p><p>To effectively eliminate the errors, we need a way to resolve the conflicting matches. Existing methods often rely on hand-crafted templates or predefined selection strategies. For example, <ref type="bibr" target="#b12">Qi et al. (2019)</ref> defined several n-gram templates to construct features for each character based on dictionaries and contexts. These templates are taskspecific and the lengths of the matched entities are constrained by templates. Several selection strategies are proposed, such as maximizing the total number of matched tokens in a sentence <ref type="bibr" target="#b15">(Shang et al., 2018)</ref>, or maximum matching with rules <ref type="bibr" target="#b13">(Sassano, 2014)</ref>. Though general, these strategies are unable to effectively utilize the contextual information. For example, as shown in Figure <ref type="figure" target="#fig_0">1</ref>, maximizing the total number of matched tokens in a sentence results in wrongly matched entity 张 三在 (Zhang Sanzai) instead of 张三 (Zhang San).</p><p>While such solutions either rely on manual efforts for rules, templates or heuristics, we believe it is possible to take a data-driven approach here to learn how to combine gazetteer knowledge. To this end, we propose a novel multi-digraph structure which can explicitly model the interaction of the characters and the gazetteers. Combined with an adapted Gated Graph Sequence Neural Networks (GGNN) <ref type="bibr" target="#b10">(Li et al., 2016)</ref> and a standard bidirectional LSTM-CRF <ref type="bibr" target="#b7">(Lample et al., 2016)</ref> (BiLSTM-CRF), our model learns a weighted combination of the information from different gazetteers and resolves matching conflicts based on contextual information.</p><p>We summarize our contributions as follows: 1) we propose a novel multi-digraph model to learn how to combine the gazetteer information and to resolve conflicting matches in learning with contexts. To the best of our knowledge, we are the first neural approach to NER that models the gazetteer information with a graph structure; 2) experimental results show that our model significantly outperforms previous methods of using gazetteers and the state-of-the-art Chinese NER models; 3) we release a new dataset in the e-commerce domain. Our code and data are publicly available 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model Architecture</head><p>The overall architecture of our model is shown in Figure <ref type="figure">2</ref>. Specifically, our model is comprised of a multi-digraph, an adapted GGNN embedding layer and a BiLSTM-CRF layer. The multidigraph explicitly models the text together with the NE gazetteer information. The information in such a graph representation is then transformed to a feature representation space using an improved GGNN structure. The encoded feature representation is then fed to a standard BiLSTM-CRF to predict the final structured output.</p><p>Text Graph. As shown in Figure <ref type="figure">2</ref>, given the input sentence 张 三 在 北 京 人 民 公 园 (Zhang San is at the Beijing People's Park) consisting of 9 Chinese characters and 4 gazetteers PER1, PER2, LOC1, LOC2 (PER1 and PER2 are gazetteers of the same type PER -"person", but are from different sources; similarly for LOC1  <ref type="table" target="#tab_2">and LOC2</ref>). We construct nodes as follows. We first use 9 nodes to represent the complete sentence, where each Chinese character corresponds to one node. We also use another 4 pairs of nodes (8 in total) for capturing the information from the 4 gazetteers, where each pair corresponds to the start and end of every entity matched by a specific gazetteer. Next we add directed edges between the nodes. First, for each pair of adjacent Chinese characters, we add one directed edge between them -from the left character to the right one.</p><p>Next, for each matched entity from a gazetteer, edges are added from the entity start node, connecting through the character nodes composing the entity and ending with the entity end node for the corresponding gazetteer. For instance, as we have illustrated in Figure <ref type="figure">2</ref>, with c 1 c 2 , or 张 三 (Zhang San) matched by the gazetteer PER2, the following edges are constructed:</p><formula xml:id="formula_0">(v PER2 s , v c 1 ), (v c 1 , v c 2 ) and (v c 2 , v PER2 e</formula><p>), where v PER2 s and v PER2 e are the start and end nodes for the gazetteer PER2, and each edge is associated with a label indicating its type information (PER in this case). When edges of the same label overlap, they are merged into a single edge. Such a simple process leads to a multi-digraph (or "directed multigraph") representation encoding the character ordering information, the knowledge from multiple NE gazetteers, as well as their interactions.</p><p>Formally, a multi-digraph is defined as G := (V, E, L), where V is the set of nodes, E is the set of edges, and L is the set of labels. With n Chinese characters in the input sentence and m gazetteers used in the model, the node set</p><formula xml:id="formula_1">V = V c ∪ V s ∪ V e .</formula><p>Here, V c is the set of nodes representing characters. Given a gazetteer g, we introduce two special nodes v g s and v g e to the graph which we use to denote the start and end of an entity matched with g. V s (V e ) is a set that contains the special nodes such as v g s (v g e ). Each edge in E is assigned with a label to indicate the type of the connection between nodes. We have the label set</p><formula xml:id="formula_2">L = { c } ∪ { g i } m</formula><p>i=1 . The label c is assigned to edges connecting adjacent characters, which are used to model the natural ordering of characters in the text. The label g i is assigned to all edges that are used to indicate the presence of an text span that matches with an entity listed in the gazetteer g i .</p><p>Adapted GGNN. Given a graph structure, the idea of GGNN is to produce meaningful outputs or to learn node representations through neural networks with gated recurrent units (GRU) <ref type="bibr" target="#b4">(Cho et al., 2014)</ref>. While other neural architectures for graphs exist, we believe that GGNN is more suitable for the Chinese NER task for its better capability of capturing the local textual information compared to other GNNs such as GCN <ref type="bibr" target="#b6">(Kipf and Welling, 2017)</ref>.</p><p>However, the traditional GGNN <ref type="bibr" target="#b10">(Li et al., 2016</ref>) is unable to distinguish edges with different labels. We adapt GGNN so as to learn a weighted combination of the gazetteer information suitable for our task. To cope with our multi-digraph structure, we first extend the adjacency matrix A to include edges of different labels. Next, we define a set of trainable contribution coefficients α c , α g 1 , . . . , α gm for each type of edges. These coefficients are used to define the amount of contribution from each type of structural information (the gazetteers and the character sequence) for our task.</p><p>In our model, an adapted GGNN architecture is utilized to learn the node representations. The initial state h (0) v of a node v is defined as follows:</p><formula xml:id="formula_3">h (0) v = W g (v) v ∈ V s ∪ V e [W c (v) , W bi (v) ] v ∈ V c</formula><p>(1) where W c and W g are lookup tables for the character or the gazetteer the node represents. In the case of character nodes, a bigram embedding table W bi is used since it has been shown to be useful for the NER task <ref type="bibr" target="#b2">(Chen et al., 2015)</ref>.</p><p>The structural information of the graph is stored in the adjacency matrix A which serves to retrieve the states of neighboring nodes at each step. To adapt to the multi-digraph structure, A is extended to include edges of different labels, A = [A 1 , ..., A |L| ]. The contribution coefficients are transformed into weights of edges in A:</p><formula xml:id="formula_4">[w c , w g 1 , . . . , w gm ] = σ([α c , α g 1 , . . . , α gm ]) (2)</formula><p>Edges of the same label share the same weight. Next, the hidden states are updated by GRU. The basic recurrence for this propagation network is:</p><formula xml:id="formula_5">H = [h (t−1) 1 , . . . , h (t−1) |V | ]<label>(3)</label></formula><formula xml:id="formula_6">a (t) v = [(HW 1 ) , . . . , (HW |L| ) ]A v + b (4) z (t) v = σ(W z a (t) v + U z h (t−1) v ) (5) r (t) v = σ(W r a (t) v + U r h (t−1) v ) (6) ĥ(t) v = tanh(W a (t) v + U (r (t) v h (t−1)<label>v</label></formula><p>)) ( <ref type="formula">7</ref>)</p><formula xml:id="formula_7">h (t) v = (1 − z (t) v ) h (t−1) v + z (t) v ĥ(t) v (<label>8</label></formula><formula xml:id="formula_8">)</formula><p>where</p><formula xml:id="formula_9">h (t)</formula><p>v is the hidden state for node v at time step t, and A v is the row vector corresponding to node v in the adjacency matrix A. W and U are parameters to be learned. Equation 3 creates the state matrix H at time step (t − 1). Equation <ref type="formula">4</ref>shows the information to be propagated through adjacent nodes. Equations 5, 6, 7, and 8 combine the information from adjacent nodes and the current hidden state of the nodes to compute the new hidden state at time step t. After T steps, we have our final state h | v ∈ V c } are then fed to a standard BiLSTM-CRF following the character order in the original sentence, to produce the output sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Setup</head><p>Dataset. The three public datasets used in our experiments are OntoNotes 4.0 <ref type="bibr" target="#b17">(Weischedel et al., 2010)</ref>, MSRA <ref type="bibr" target="#b8">(Levow, 2006)</ref>, and Weibo-NER <ref type="bibr" target="#b11">(Peng and Dredze, 2016)</ref>. OntoNotes and MSRA are two datasets consisting of newswire text. Weibo-NER is in the domain of social media. We use the same split as <ref type="bibr" target="#b1">Che et al. (2013)</ref> and <ref type="bibr" target="#b11">Peng and Dredze (2016)</ref> on OntoNotes and on Weibo-NER. To demonstrate the effectiveness of our model in the e-commerce domain, we further constructed a new dataset by crawling and manually annotating the NEs of two types, namely PROD ("products") and BRAN ("brands"). We name our dataset as "E-commerce-NER". The NER task in the e-commerce domain is more challenging. The NEs of interest are usually the names of products or brands. In practice, the number of unique entities that can appear in such a domain can be easily tens of millions. The training data is typically far from being enough to cover even a small portion of all such NEs. Thus, the effectiveness of an NER system in the e-commerce domain relies heavily on domain-specific gazetteers. Gazetteers. For the three public datasets, we collect gazetteers of 4 categories (PER, GPE, ORG, LOC). Each category has 3 gazetteers with different sizes, selected from multiple sources including "Sougou"<ref type="foot" target="#foot_0">2</ref> , "HanLP"<ref type="foot" target="#foot_1">3</ref> and "Hankcs"<ref type="foot" target="#foot_2">4</ref> . We add an extra indomain gazetteer of type PER for Weibo-NER dataset since the online community has a rich set of nicknames and aliases. For our dataset in the e-commerce domain, we collect 3 product name gazetteers and 4 brand name gazetteers crawled from product catalogues from the e-commerce site Taobao<ref type="foot" target="#foot_3">5</ref> . To better demonstrate the problem of conflicting matches with gazetteers added as knowledge source, the entity conflict rate of each dataset with respect to the gazetteers it references is analyzed. The entity conflict rate (ECR) is defined as the ratio of non-identical overlapping entity matches to all unique entities matched with all gazetteers. The ECR of OntoNotes, MSRA, Weibo-NER and E-commerce-NER are respec-tively 39.70%, 44.75%, 36.10% and 46.05%.</p><p>Models for Comparison. We use BiLSTM-CRF <ref type="bibr" target="#b7">(Lample et al., 2016)</ref> with character+bigram embedding without using any gazetteer as the comparison baseline<ref type="foot" target="#foot_4">6</ref> . We explore the three different methods of adding gazetteer features that we compare against: N -gram features, Position-Independent Entity Type (PIET) features and Position-Dependent Entity Type (PDET) features. These feature construction processes follow the work of <ref type="bibr" target="#b16">Wang et al. (2018)</ref>. We refer the readers to their paper for further details.</p><p>To show the effect of adding gazetteer information, a trivial version of our model without using any gazetteer information is also implemented as one of our baselines (our model w/o gazetteers).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>From Table <ref type="table" target="#tab_1">1</ref>, it can be seen that our model with 12 general gazetteers of 4 entity types has an overall highest performance in the news domain. By adding domain specific gazetteers, our model is capable of improving the NER quality in both the social media and the e-commerce domains, as shown in Table <ref type="table" target="#tab_2">2</ref>. Previous methods of using gazetteers do improve the performance of the BiLSTM-CRF model, but the performance gains are not significant. We can observe the performance on both OntoNotes and Weibo-NER drop, when the N -gram and the PIET features were used on top of the BiLSTM-CRF model. We believe this is due to the erroneous information the model captured, especially when multiple conflicting gazetteers were used together. Compared to these methods, our model achieves a remarkably higher performance. Our model is not only able to improve recall by using the gazetteer knowledge, but is also able to offer an improved precision.</p><p>To understand the effect of using gazetteers by different methods, we conducted some detailed experiments on OntoNotes. We first split all the sentences in the test set into 3 groups, based on if the entities also appear in the training data or not: "All" contains those sentences in which all entities can be found in the training set, "Some" contains sentences which contain some of the entities from the training set but not all, "None" contains sentences where none of the entities appear in the training set. For the last set of sentences, we con- ducted additional experiments by further splitting them into three sub-groups, based on whether their entities appear in the gazetteers.</p><p>We compare three models under each setting: 1) PDET, 2) our model and 3) our model with all gazetteer nodes removed. We note that the last model can be regarded as a trivial version of both PDET and our model. As shown in Table <ref type="table" target="#tab_3">3</ref>, when none of the entities in a test sentence has been seen during training, with increasing gazetteer coverage our model has a more significant improvement compared to PDET. When none or some of the test entities appear in the training data, both PDET and our model perform better than the trivial model. This shows the benefit of utilizing gazetteer knowledge. Furthermore, in this case, our model still yields a relatively better F1 score, due to its better way of representing gazetteer information using multi-digraph. In the case where all the entities appear during training, both PDET and our model yield lower performance than the trivial model. We believe this is due to errors introduced by the gazetteers. Nonetheless, our model is more robust than PDET in this case.</p><p>Ablation Study. We also conducted an ablation study to explore the contributions brought by the weighted combination of gazetteers, so as to understand how our model can effectively use the gazetteer information.</p><p>As shown in Table <ref type="table" target="#tab_4">4</ref>, by fixing the gazetteer contribution coefficients to 1, the model's performance drops by 1.8 points in terms of F1 score. The precision is even lower than that of our model without gazetteers. This experiment shows that, without a good combination of the gazetteer information, the model fails to resolve conflicting matches. In that case, errors are introduced with the use of gazetteers. These errors harm the model's performance and have a negative effect on the precision.</p><p>We use the following ablation test to understand whether the gazetteer information can be fully utilized by our model. There are three types of infor- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion and Future Work</head><p>We present a novel neural multi-digraph model for performing Chinese named entity recognition with gazetteers. Based on the proposed multi-digraph structure, we show that our model is better at resolving entity-matching conflicts. Through extensive experiments, we have demonstrated that our approach outperforms the state-of-the-art models and previous methods for incorporating gazetteers into a Chinese NER system. The ablation study confirms that a suitable combination of gazetteers is essential and our model is able to make good use of the gazetteer information. Although we specifically investigated the NER task for Chinese in this work, we believe the proposed model can be extended and applied to other languages, for which we leave as future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example of Entity Matching mercial data sources (e.g., product catalogues of e-commence websites).While such background knowledge can be helpful, in practice the gazetteers may also contain irrelevant and even erroneous information which harms the system's performance<ref type="bibr" target="#b3">(Chiu and Nichols, 2016)</ref>. This is especially the case for Chinese NER, where enormous errors can be introduced due to wrongly matched entities. Chinese language is inherently ambiguous since the granularity of words is less well defined than other languages (such as English). Thus massive wrongly matched entities can be generated with the use of gazetteers. As we can see from the example shown in Figure1, matching a simple 9-character sentence with 4 gazetteers may result in 6 matched entities, among which 2 are incorrect.To effectively eliminate the errors, we need a way to resolve the conflicting matches. Existing methods often rely on hand-crafted templates or predefined selection strategies. For example, Qi et al. (2019) defined several n-gram templates to construct features for each character based on dictionaries and contexts. These templates are taskspecific and the lengths of the matched entities are constrained by templates. Several selection strategies are proposed, such as maximizing the total number of matched tokens in a sentence<ref type="bibr" target="#b15">(Shang et al., 2018)</ref>, or maximum matching with rules<ref type="bibr" target="#b13">(Sassano, 2014)</ref>. Though general, these strategies are unable to effectively utilize the contextual information. For example, as shown in Figure1,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>v</head><label></label><figDesc>for the node v. BiLSTM-CRF. The learned feature representations of characters {h (T ) v</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>𝒗𝒄 𝟏 ) GNN(𝒗𝒄 𝟐 ) GNN(𝒗𝒄 𝟑 ) GNN(𝒗𝒄 𝟒 ) GNN(𝒗𝒄 𝟓 )</head><label></label><figDesc>𝑐 @ 𝑐 A 𝑐 B 𝑐 C 𝑐 D 𝑐 E 𝑐 F 𝑐 G 𝑐 H</figDesc><table><row><cell>𝒗𝒆 𝑷𝑬𝑹𝟐 𝒗 𝒔 𝑷𝑬𝑹𝟐 𝒗 𝒔 𝑳𝑶𝑪𝟐</cell><cell cols="2">𝑣 ,-𝑣 ,/ 𝑣 ,. 𝑣 ,0 𝑣 ,1 𝒗𝒔 𝑷𝑬𝑹𝟏 𝒗𝒆 𝑷𝑬𝑹𝟏 GNN(𝑣 ,7 GNN(𝒗𝒄 𝟔 ) 𝒗𝒔 𝑳𝑶𝑪𝟏</cell><cell>L S T M</cell><cell>C R F</cell></row><row><cell>𝒗 𝒆 𝑳𝑶𝑪𝟐</cell><cell>𝑣 ,8 𝑣 ,9</cell><cell>GNN(𝒗𝒄 𝟕 ) GNN(𝒗𝒄 𝟖 ) 𝒗𝒆 𝑳𝑶𝑪𝟏</cell><cell></cell></row><row><cell></cell><cell>𝑣 ,:</cell><cell>GNN(𝒗𝒄 𝟗 )</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Embedding</cell><cell></cell></row><row><cell>Raw space</cell><cell></cell><cell>space</cell><cell></cell></row><row><cell></cell><cell cols="2">Figure 2: System architecture</cell><cell></cell></row><row><cell>1 https://github.com/PhantomGrapes/</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MultiDigraphNER</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Results on the newswire data</figDesc><table><row><cell>Models</cell><cell></cell><cell cols="2">OntoNotes</cell><cell></cell><cell>MSRA</cell><cell></cell></row><row><cell></cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>P</cell><cell>R</cell><cell>F</cell></row><row><cell>BiLSTM-CRF</cell><cell cols="6">72.0 75.1 73.5 92.3 92.4 92.4</cell></row><row><cell>(+ N -gram)</cell><cell cols="6">71.1 75.5 73.3 92.7 92.7 92.7</cell></row><row><cell>(+ PIET)</cell><cell cols="6">71.6 74.6 73.1 92.9 93.4 93.1</cell></row><row><cell>(+ PDET)</cell><cell cols="6">73.8 73.8 73.8 93.1 93.1 93.1</cell></row><row><cell cols="7">Our model (w/o gazetteers) 74.8 73.0 73.9 93.2 92.7 92.9</cell></row><row><cell>Our model</cell><cell cols="6">75.4 76.6 76.0 94.6 94.2 94.4</cell></row><row><cell>Zhang and Yang (2018)</cell><cell cols="6">76.4 71.6 73.9 93.6 92.8 93.2</cell></row><row><cell>Dong et al. (2016)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">91.3 90.6 91.0</cell></row><row><cell>Zhang et al. (2006)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">90.2 90.2 91.2</cell></row><row><cell>Models</cell><cell></cell><cell cols="2">Weibo-NER</cell><cell cols="3">E-commerce-NER</cell></row><row><cell></cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>P</cell><cell>R</cell><cell>F</cell></row><row><cell>BiLSTM-CRF</cell><cell cols="6">60.8 52.9 56.6 71.1 76.1 73.6</cell></row><row><cell>(+ N -gram)</cell><cell cols="6">57.8 53.6 55.6 71.2 75.9 73.5</cell></row><row><cell>(+ PIET)</cell><cell cols="6">57.7 54.4 56.0 71.7 75.8 73.7</cell></row><row><cell>(+ PDET)</cell><cell cols="6">59.2 54.4 56.7 72.6 75.1 73.8</cell></row><row><cell cols="7">Our model (w/o gazetteers) 62.1 52.7 57.0 70.7 74.6 72.6</cell></row><row><cell>Our model</cell><cell cols="6">63.1 56.3 59.5 74.3 76.2 75.2</cell></row><row><cell>Zhang and Yang (2018)</cell><cell>-</cell><cell>-</cell><cell>58.8</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Peng and Dredze (2016)</cell><cell>-</cell><cell>-</cell><cell>59.0</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results on social media/e-commerce domains</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Detailed results on OntoNotes (Train: Training data, Gaze: Gazetteers).</figDesc><table><row><cell cols="2">Entities Appear In</cell><cell></cell><cell>PDET</cell><cell></cell><cell></cell><cell>Our model</cell><cell></cell><cell cols="3">Our model (w/o gazetteers)</cell></row><row><cell>Train</cell><cell>Gaze</cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>P</cell><cell>R</cell><cell>F</cell></row><row><cell>All</cell><cell>-</cell><cell cols="9">84.6 85.3 85.0 85.3 88.8 87.0 87.4 88.1 87.7</cell></row><row><cell>Some</cell><cell>-</cell><cell cols="9">78.2 73.2 75.7 79.5 76.0 77.7 78.0 72.0 74.9</cell></row><row><cell>None</cell><cell>-</cell><cell cols="9">66.7 62.9 64.7 68.5 65.0 66.7 66.5 59.2 62.6</cell></row><row><cell>None</cell><cell>All</cell><cell cols="9">69.8 64.8 67.2 74.2 67.0 72.0 71.4 59.9 65.1</cell></row><row><cell cols="11">None Some 66.7 61.0 63.7 66.1 61.8 63.9 64.0 56.7 60.1</cell></row><row><cell cols="11">None None 63.6 62.7 63.1 64.8 62.9 63.8 64.2 60.9 62.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Ablation study on OntoNotes mation provided by gazetteers: boundary information, entity-type information, and source information. The All in One Gazetteer (AI1G) experiment shows what role the boundary information plays in our model by merging all 12 gazetteers into one lexicon where entity type information is discarded. It outperforms the model without gazetteers by 1.1 points in terms of F1 score. The One Type One Gazetteer (1T1G) model adds the entity type information on top of the AI1G model by adding only the entity type labels (i.e., there is one gazetteer for one type, by merging all gazetteers of the same type into one). Doing so leads to a 0.8 points improvement over the AI1G model. From the experiments we can see that the entities' source information is also helpful. For example, an entity that appears in multiple PER gazetteers is more likely to be an entity of type PER than an entity appearing only in one gazetteer. Our model can effectively capture such source information and has an improvement of 0.2 points in terms of F1 compared to the 1T1G model.</figDesc><table><row><cell>Models</cell><cell>P</cell><cell>R</cell><cell>F</cell></row><row><cell>Our model</cell><cell cols="3">75.4 76.6 76.0</cell></row><row><cell cols="4">(fixed coefficients) 73.8 74.5 74.2</cell></row><row><cell>(AI1G)</cell><cell cols="3">73.4 76.7 75.0</cell></row><row><cell>(1T1G)</cell><cell cols="3">78.9 73.0 75.8</cell></row><row><cell>(w/o gazetteers)</cell><cell cols="3">74.8 73.0 73.9</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">A crowdsourced gazetteer used by the Chinese IME Sougou: https://pinyin.sogou.com/dict/.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">A gazetteer from a widely used open-source Chinese NLP toolkit: https://github.com/hankcs/HanLP.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2">A gazetteer which consists of over ten million entries: http://www.hankcs.com/nlp/corpus.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3">http://www.taobao.com</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4">We implemented the baseline models using the NCRFPP toolkit<ref type="bibr" target="#b18">(Yang and Zhang, 2018)</ref>.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank the anonymous reviewers for their thoughtful comments. Wei Lu is supported by SUTD project PIE-SGP-AI-2018-01.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGMOD</title>
				<meeting>of SIGMOD</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Named entity recognition with bilingual constraints</title>
		<author>
			<persName><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengqiu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL-HLT</title>
				<meeting>of NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Long short-term memory neural networks for chinese word segmentation</title>
		<author>
			<persName><forename type="first">Xinchi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenxi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
				<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Named entity recognition with bidirectional lstm-cnns</title>
		<author>
			<persName><forename type="first">Jason P C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Nichols</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of TACL</title>
				<meeting>of TACL</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
				<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Characterbased lstm-crf with radical-level features for chinese named entity recognition</title>
		<author>
			<persName><forename type="first">Chuanhai</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masanori</forename><surname>Hattori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Di</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-50496-4_20</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICCPOL</title>
				<meeting>of ICCPOL</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semisupervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
				<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Neural Architectures for Named Entity Recognition</title>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL-HLT</title>
				<meeting>of NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The third international chinese language processing bakeoff: Word segmentation and named entity recognition</title>
		<author>
			<persName><forename type="first">Gina-Anne</forename><surname>Levow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Fifth SIGHAN Workshop on Chinese Language Processing</title>
				<meeting>of the Fifth SIGHAN Workshop on Chinese Language essing</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Svm based learning system for information extraction. Deterministic and statistical methods in machine learning</title>
		<author>
			<persName><forename type="first">Yaoyong</forename><surname>Li</surname></persName>
			<affiliation>
				<orgName type="collaboration">Hamish Cunningham</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Kalina</forename><surname>Bontcheva</surname></persName>
			<affiliation>
				<orgName type="collaboration">Hamish Cunningham</orgName>
			</affiliation>
		</author>
		<idno type="DOI">10.1007/11559887_19</idno>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
				<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improving named entity recognition for chinese social media with word segmentation representation learning</title>
		<author>
			<persName><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
				<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neural Networks Incorporating Dictionaries for Chinese Word Segmentation</title>
		<author>
			<persName><forename type="first">Zhang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liu</forename><surname>Xiaoyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fu</forename><surname>Jinlan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI. Lev Ratinov and</title>
				<meeting>of AAAI. Lev Ratinov and<address><addrLine>Dan Roth</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2019. 2009</date>
		</imprint>
	</monogr>
	<note>Proc. of CoNLL</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deterministic Word Segmentation Using Maximum Matching with Fully Lexicalized Rules</title>
		<author>
			<persName><forename type="first">Manabu</forename><surname>Sassano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EACL</title>
				<meeting>of EACL</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A study of the importance of external knowledge in the named entity recognition task</title>
		<author>
			<persName><forename type="first">Dominic</forename><surname>Seyler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatiana</forename><surname>Dembelova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luciano</forename><surname>Del Corro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Hoffart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
				<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning named entity tagger using domain-specific dictionary</title>
		<author>
			<persName><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaotao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
				<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Incorporating dictionaries into deep neural networks for the chinese clinical named entity recognition</title>
		<author>
			<persName><forename type="first">Qi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhang</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Ruan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Daqi Gao, and Ping He</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename></persName>
		</author>
		<title level="m">Ontonotes release 4.0. LDC2011T03</title>
				<meeting><address><addrLine>Philadelphia, Penn</addrLine></address></meeting>
		<imprint>
			<publisher>Linguistic Data Consortium</publisher>
			<date type="published" when="2010">Marcus. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ncrf++: An opensource neural sequence labeling toolkit</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
				<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>System Demonstrations</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Building a corpus-derived gazetteer for named entity recognition</title>
		<author>
			<persName><forename type="first">Norshuhani</forename><surname>Zamin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Oxley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Software Engineering and Computer Systems</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Word Segmentation and Named Entity Recognition for SIGHAN Bakeoff3</title>
		<author>
			<persName><forename type="first">Suxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojie</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Fifth SIGHAN Workshop on Chinese Language Processing</title>
				<meeting>of the Fifth SIGHAN Workshop on Chinese Language essing</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Chinese NER Using Lattice LSTM</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
				<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
