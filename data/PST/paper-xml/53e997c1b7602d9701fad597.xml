<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Swarm Testing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Alex</forename><surname>Groce</surname></persName>
							<email>agroce@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">Oregon State University</orgName>
								<address>
									<settlement>Corvallis</settlement>
									<region>OR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chaoqiang</forename><surname>Zhang</surname></persName>
							<email>zhangch@onid.orst.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">Oregon State University</orgName>
								<address>
									<settlement>Corvallis</settlement>
									<region>OR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Eric</forename><surname>Eide</surname></persName>
							<email>eeide@cs.utah.edu</email>
							<affiliation key="aff1">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">University of Utah</orgName>
								<address>
									<settlement>Salt Lake City</settlement>
									<country>UT USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yang</forename><surname>Chen</surname></persName>
							<email>chenyang@cs.utah.edu</email>
							<affiliation key="aff1">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">University of Utah</orgName>
								<address>
									<settlement>Salt Lake City</settlement>
									<country>UT USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">John</forename><surname>Regehr</surname></persName>
							<email>regehr@cs.utah.edu</email>
							<affiliation key="aff1">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">University of Utah</orgName>
								<address>
									<settlement>Salt Lake City</settlement>
									<country>UT USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Swarm Testing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DE4C32E43F08EDBFA2F721363B554BD8</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>D.2.5 [Software Engineering]: Testing and Debugging-testing tools; D.3.4 [Programming Languages]: Processors-compilers General Terms Algorithms</term>
					<term>Experimentation</term>
					<term>Languages</term>
					<term>Reliability Random testing</term>
					<term>configuration diversity</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Swarm testing is a novel and inexpensive way to improve the diversity of test cases generated during random testing. Increased diversity leads to improved coverage and fault detection. In swarm testing, the usual practice of potentially including all features in every test case is abandoned. Rather, a large "swarm" of randomly generated configurations, each of which omits some features, is used, with configurations receiving equal resources. We have identified two mechanisms by which feature omission leads to better exploration of a system's state space. First, some features actively prevent the system from executing interesting behaviors; e.g., "pop" calls may prevent a stack data structure from executing a bug in its overflow detection logic. Second, even when there is no active suppression of behaviors, test features compete for space in each test, limiting the depth to which logic driven by features can be explored. Experimental results show that swarm testing increases coverage and can improve fault detection dramatically; for example, in a week of testing it found 42% more distinct ways to crash a collection of C compilers than did the heavily hand-tuned default configuration of a random tester.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>This paper focuses on answering a single question: In random testing, can a diverse set of testing configurations perform better than a single, possibly "optimal" configuration? An example of a test configuration would be, for example, a list of API calls that can be included in test cases. Conventional wisdom in random testing <ref type="bibr" target="#b19">[19]</ref> has assumed a policy of finding a "good" configuration and running as many tests as possible with that configuration. Considerable research effort has been devoted to the question of how to tune a "good configuration," e.g., how to use genetic algorithms to optimize the frequency of various method calls <ref type="bibr" target="#b6">[6]</ref>, or how to choose a length for tests <ref type="bibr" target="#b5">[5]</ref>. As a rule, the notion that some test configurations are "good" and that finding a good (if not truly optimal, given the size of the search space) configuration is important has not been challenged. Furthermore, in the interests of maximizing coverage and fault detection, it has been assumed that a good random test configuration includes as many API calls or other input domain features as possible, and this has been the guiding principle in largescale efforts to test C compilers <ref type="bibr" target="#b36">[36]</ref>, file systems <ref type="bibr" target="#b17">[17]</ref>, and utility libraries <ref type="bibr" target="#b29">[29]</ref>. The rare exceptions to this rule have been cases where a feature makes tests too difficult to evaluate or slow to execute, or when static analysis or hand inspection can demonstrate that an API call is unrelated to state <ref type="bibr" target="#b17">[17]</ref>. For example, including pointer assertions may make compiling random C programs too slow with some compilers.</p><p>In general, if a call or feature is omitted from some tests, it is usually omitted from all tests. This approach seems to make intuitive sense: omitting features, unless it is necessary, means giving up on detecting some faults. However, this objection to feature omission only holds so long as testing is performed using a single test configuration. Swarm testing, in contrast, uses a diverse "swarm" of test configurations, each of which deliberately omits certain API calls or input features. As a result, given a fixed testing budget, swarm testing tends to test a more diverse set of inputs than would be tested under a so-called "optimal" configuration (perhaps better referred to as a default configuration) in which every feature is available for use by every test.</p><p>One can visualize the impact of swarm testing by imagining a "test space" defined by the contents of tests. As a simple example, consider testing an implementation of a stack ADT that provides two operations, push and pop. One can visualize the test space for the stack ADT using these features as axes: each test is characterized by the number of times it invokes each operation. Any method for randomly generating test cases results in a probability distribution over the test space, with the value at each point (x, y) giving the probability that a given test will contain exactly x pushes and y pops (in any order). To make this example more interesting, imagine the stack implementation has a capacity bug, and will crash whenever the stack is required to hold more than 32 items.</p><p>Figure <ref type="figure" target="#fig_1">1</ref>(a) illustrates the situation for testing the stack with a test generator that chooses pushes and pops with equal probability. The generator randomly chooses an input length and then decides if each operation is a push or a pop. The graph shows the distribution of tests produced by this generator over the test space. The graph also shows contour lines for significant regions of the test space. Where P f ail = 1, a test chosen randomly from that region is certain to trigger the stack's capacity bug; where P f ail = 0, no test can trigger the bug. As Figure <ref type="figure" target="#fig_1">1</ref>(a) shows, this generator only rarely  produces test cases that can trigger the bug. Now consider a test generator based on swarm testing. This generator first chooses a non-empty subset of the stack API and then generates a test case using that subset. Thus, one-third of the test cases contain both pushes and pops, one-third just pushes, and one-third just pops. Figure <ref type="figure" target="#fig_1">1(b)</ref> shows the distribution of test cases output by this generator. As is evident from the graph, this generator often produces test cases that trigger the capacity bug. Although simple, this example illustrates the dynamics that make swarm testing work. The low dimensionality of the stack example is contrived, of course, and we certainly believe that programmers should make explicit efforts to test boundary conditions. As evidenced by the results presented in this paper, however, swarm testing generalizes to real situations in which there may be dozens of features that can be independently turned on or off. It also generalizes to testing real software in which faults are very well hidden.</p><p>Every test generated by any swarm configuration can, in principle, be generated by a test configuration with all features enabled. However-as the stack example illustrates-the probability of covering parts of the state space and detecting certain faults can be demonstrably higher when a diverse set of configurations is tested.</p><p>Swarm testing has several important advantages. First, it is low cost: in our experience, existing random test case generators already support or can be easily adapted to support feature omission. Second, swarm testing reduces the amount of human effort that must be devoted to tuning the random tester. In our experience, tuning is a significant ongoing burden. Finally-and most importantly-swarm testing makes significantly better use of a fixed CPU time budget than does random testing using a single test configuration, in terms of both coverage and fault detection. For example, we performed an experiment where two machines, differing only in that one used swarm testing and one did not, used Csmith <ref type="bibr" target="#b36">[36]</ref> to generate tests for a collection of production-quality C compiler versions for x86-64. During one week of testing, the swarm machine found 104 distinct ways to crash compilers in the test suite whereas the other machinerunning the default Csmith test configuration, which enables all features-found only 73. An improvement of more than 40% in terms of number of bugs found, using a random tester that has been intensively tuned for several years, is surprising and significant.</p><p>Even more surprising were some of the details. We found, for example, a compiler bug that could only be triggered by programs containing pointers, but which was almost never triggered by inputs that contained arrays. This is odd because pointer dereferences and array accesses are very nearly the same thing in C. <ref type="foot" target="#foot_0">1</ref> Moreover, we found another bug in the same compiler that was only triggered by programs containing arrays, but which was almost never triggered by inputs containing pointers. Fundamentally, it appears that omitting features while generating random test cases can lead to improved test effectiveness.</p><p>Our contributions are as follows. First, we characterize swarm testing, a pragmatic variant of random testing that increases the diversity of generated test cases with little implementation effort. The swarm approach to diversity differs from previous methods in that it focuses solely on feature omission diversity: variance in which possible input features are not present in test cases. Second, we show that-in three case studies-swarm testing offers improved coverage and bug-finding power. Third, we offer some explanations as to why swarm testing works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">SWARM TESTING</head><p>Swarm testing uses test configurations that correspond to sets of features of test cases. A feature is an attribute of generated test inputs that the generator can directly control, in a computationally efficient way. For example, an API-based test generator might define features corresponding to inclusion of API functions (e.g., push and pop); a C program generator might define features corresponding to the use of language constructs (e.g., arrays and pointers); and a media-player tester might define features over the properties of media files, e.g., whether or not the tester will generate files with corrupt headers. In our work, a feature determines a configuration of test generation, not the System Under Test (SUT)-in this work we use the same build of the SUT for all testing. In particular, we are configuring which aspects of the SUT will be tested (and not tested) only by controlling the test cases output. Features can be thought of simply as constraints on test cases, in particular those the test case generator lets us control.</p><p>Assume that a test configuration C is a set of features, f 1 . . . f n . C is used as the input to a random testing function <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b19">19]</ref> gen(C, s), which given configuration C and seed s generates a test case for the SUT containing only features in C. We may ignore the details of how the exact test case is built. The values f 1 . . . f n determine which features are allowed to appear in the test case. For example, if we are testing a simple file system, the set of all features might be: read , write , open , close , unlink , sync , mkdir , rmdir , unmount , mount . A typical default C would then be read , write , open , close , unlink , sync , mkdir , rmdir , which omits mount and unmount in order to avoid wasting test time on operations while the file system is unmounted.</p><p>Assume that a test engineer has two CPUs available and 24 hours to test a file system. The conventional strategy would be to choose a "good" test case length, divide the set of random-number-generator seeds into two sets, and simply generate, execute, and evaluate as many tests as possible on each CPU, with a single C.</p><p>In contrast, a swarm approach to testing the same system, under the same assumptions, would use a "swarm"-a set {C 1 ,C 2 , . . .C n }. A fixed set could be chosen in advance, or a fresh C i could be generated for each test. In most of our experimental results, we use large but fixed-size sets, generated randomly. That is, we "toss a fair coin" to determine feature presence or absence in C. In Section 3.1.3 we discuss other methods for generating each C. With a fixed swarm set, we divide the total time budget on each CPU such that each C i receives equal testing time, likely generating multiple tests for the each C i . For testing without a fixed swarm set, we would simply keep generating a C i and running a test until time is up. For the file system example, where there are nine features and thus 2 9 (512) possible configurations, a fixed set might consist of 64 C, each of which would receive a test budget of 45 minutes (48 CPU-hours divided by 64)-a large number of tests would be generated for each C i . The default approach is equivalent to swarm testing if we use a singleton set {C D }, where C D includes all features we are interested in testing. Some C i (those omitting features that slow down the SUT) may generate tests that are quicker to execute than C D ; others may produce tests that execute slower due to a high concentration of expensive features. On average, the total number of tests executed will be similar to the standard approach, though perhaps with a greater variance. The distribution of calls made, over all tests, will also be similar to that found using just C D : each call will be absent in roughly half of configurations, but will be called more frequently in other configurations. Why, then, might results from swarms differ from those with the default approach?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Advantages of Configuration Diversity</head><p>The key insight motivating this paper is that the possibility of a test being produced is not the same as the probability that it will be produced. In particular, consider a fault that relies on making 64 calls to open, without any calls to close, at which point the file descriptor table overflows and the file system crashes. If the test length in both the default and swarm settings is fixed at 512 operations, we know that testing with C D is highly unlikely to expose the fault: the rate is much less than 1 in 100,000 tests. With swarm, on the other hand, many C i (16 on average) will produce tests containing calls to open but no calls to close. Furthermore, some of these C i will also disable other calls, increasing the proportion of open calls made. For C i such that open ∈ C i but without close and at least one other feature, the probability of detecting the fault improves from close to 0% to over 80%. If 48 hours of testing produces approximately 100,000 tests, it is almost certain that using C D will fail to detect the fault, and at the same time almost certain that any swarm set of size 64 will detect it. The same argument holds even if we improve the chances of C D by assuming that close calls do not decrement the file descriptor count: swarm is still much more likely to produce any failure that requires many open calls.</p><p>While such resource-exhaustion faults may seem to be a rare special case, the concept can be generalized. Obviously, many data structures, as in the stack example, may have overflow or underflow problems where one or more API calls moves the system away from exhibiting failure. In the file system setting, it seems likely that many faults related to buffering will be masked by calls to sync. In a compiler, many potentially faulty optimizations will never be applied to code that contains pointer accesses, because of failed safety checks based on aliasing. In other words, including a feature in a test does not always improve the ability of the test to cover behavior and expose faults: some features can actively suppress the exhibition of some behaviors. Formally, we say that a feature suppresses a behavior in a given tester if, over the set of all test cases the tester in question can produce, test cases containing the suppressing feature are less likely to display the behavior than those without the suppressing feature.</p><p>Furthermore, if we assume that some aspects of system state are affected more by some features than others, and assume that test cases are limited in size, then by shifting the distribution of calls within each test case (though not over all test cases), swarm testing results in a much higher probability of exploring "deep" values of state variables. Consider adding a top call that simply returns the top value of the stack to the ADT above. For every call to top in a finite test case, the number of push calls possible is reduced by one. Only if all features equally affect all state variables are swarm and using just C D equally likely to explore "deep" states. Given that real systems exhibit a strong degree of modularity and that API calls and input features are typically designed to have predictable, localized effects on system state or behavior, this seems extremely unlikely. Many fault-detection or property-proof techniques, from abstraction to compositional verification to k-wise combinatorial testing, take this modularity of interaction for granted. We therefore hypothesize that many features "passively" suppress some behaviors by "crowding out" relevant features in finite test cases.</p><p>Active and passive suppression mean that we may need tests that exhibit a high degree of feature omission diversity, since we do not know which features will suppress which behaviors, and features are almost certain both to suppress some behaviors and be required or at least helpful for producing others!</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Disadvantages of Configuration Diversity</head><p>An immediate objection to swarm testing is that it may significantly reduce the probability of detecting certain faults. Consider a file system bug that can only be exposed by a combination of calls to read, write, open, mkdir, rmdir, unlink, and sync. Because there is only a 1/128 chance that a given C i will enable all of these, it is likely that a swarm set of size 64 cannot find this fault. At first examination, it seems that the swarm approach to testing will expose fewer faults and result in worse coverage than using a single inclusive C. Furthermore, recalling that any test possible with any C i in a swarm set is also possible under C D , but that some tests produced by the C D may be impossible to produce for almost all C i , it may be hard to imagine how swarm can compensate.</p><p>This apparent disadvantage of swarm-that feature subsetting will necessarily miss some bugs-in fact has rather limited impact. First, when features appear together only infrequently over C i , this may lower the probability of finding the "right" test for a particular bug, but does not preclude it. Second, since other features will almost certainly be omitted from the few C i that do contain the right combination, the features may interact more than in C D -thus increasing the likelihood of finding the bug (Section 2.1) in each test.</p><p>For bugs that can be discovered only when many features are enabled, the relevant question is this: how likely is it that swarm testing will not include any C i with all the needed features? Using the simplest form of coin-toss generation for swarm sets, the chance of a given set of k features never appearing together in any of C 1 . . .C n is (1 -0.5 k ) n . Even a very small swarm set of 100 configurations is 95% likely to contain at least one C i for any given choice of five features. If a tester uses a swarm set size of 1,000 (as we do in Section 3.2) there is a 95% chance of covering any given set of eight features, and a 60% chance with ten. If a tester believes that even these probabilities are unsatisfying, a simple mitigation strategy is to include C D in every swarm set. We chose not to do this in our experiments in order to heighten the value of our comparison with the default, all-inclusive configuration strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">An Empirical Question</head><p>In general, all that can be said is that what we call C D may be optimal for some hypothetical set of coverage targets and faults, while a swarm set {C 1 . . .C n } will contain some C i that are optimal for different coverage targets and faults, but will perform less testing under each C i than the conventional approach will perform under C D . Our hypothesis is that for many real-world programs, the conventional "C D approach" to testing will expose the same faults and cover the same behaviors many times, while swarm testing may expose more faults and cover more targets, but might well produce fewer failing tests for each fault and execute fewer tests that cover each branch/statement/path/etc. Given that the precise distribution of faults and coverage targets in the state space of any realistic system is complex and not amenable to a priori analysis, only experimental investigation of how the two random testing approaches compare on real systems can give us practical insight into what strategies might be best for large-scale, random testing of critical systems. The remainder of this paper shows how conventional, single-C testing and swarm testing compare for coverage and fault detection on the software in three case studies: (1) a widely used open-source flash file system, (2) seventeen versions of five widely used C compilers, and (3) a container in the widely used Sglib library.</p><p>The thesis of this paper is that turning features off during test case generation can lead to more effective random testing. Thus, one of our evaluation criteria is that swarm should find more defects and lead to improved code coverage, when compared to the default configuration of a random tester, with other factors being equal.</p><p>In the context of any individual bug, it is possible to evaluate swarm in a more detailed fashion by analyzing the features found and not found in test cases that trigger the bug. We say that a test case feature is significant with respect to some bug if its presence or absence affects the likelihood that the bug will be found. For example, push operations are obviously a significant feature with respect to the example in Section 1 because a call to push causes the bug to manifest. But this is banal: it has long been known that effective random testing requires "featureful" test inputs. The power of swarm is illustrated when the absence of one or more features is statistically significant in triggering a bug. Section 3 shows that such (suppressing) features for bugs are commonplace, providing strong support for our claim that swarm testing is beneficial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">CASE STUDIES</head><p>We evaluated swarm testing using three case studies in which we tested software systems of varying size and complexity. The first study was based on YAFFS2, a flash file system; the second (and largest) used seventeen versions of five production-quality C compilers; and the third, a "mini-study," focused on a red-black tree implementation. The file system and red-black tree were small enough (15 KLOC and 476 LOC, respectively) to be subjected to mutation testing. The compilers, on the other hand, were not conveniently sized for mutation testing, but provided something better: a large set of real faults that caused crashes or other abnormal exits.</p><p>In all case studies, we used relatively small (n ≤ 1, 000) swarm sets, to show that swarm testing improves over using C D even with relatively small sets, which may be necessary if there are complex or expensive-to-check constraints on valid C i . In practice, all of our case studies would support a much simpler approach of simply using any random C for each test, and we believe this may be the best practice when it is possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Case Study: YAFFS Flash File System</head><p>YAFFS2 <ref type="bibr" target="#b35">[35]</ref> is a popular open-source NAND flash file system for embedded use; it is the default image format for the Android operating system. Our test generator for YAFFS2 produces random tests of any desired length and executes the tests using the RAM flash emulation mode. By default, tests can include or not include any of 23 core API calls, as specified by a command line argument to the test generator: these command line arguments are the C i , and calls are features. Our tester generates a test case by randomly choosing an API from the feature set, and calling the API with random parameters (not influenced by our test configuration). This is repeated n times to produce a length n test case, consisting of the API calls and parameter choices. Feedback <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b29">29]</ref> is used in the YAFFS2 tester to ensure that calls such as close and readdir occur only in states where valid inputs are available. We ran one experiment with 100 test configurations and another with 500 configurations, all including API calls with 50% probability. Both sets were large enough compared to the number of tests run to make unusual effectiveness or poor performance due to a small set of especially good or bad C i highly unlikely. Both experiments compared 72 hours of swarm testing to 72 hours of testing with C D only, evaluating test effectiveness based on block, branch, du-path, prime path, path, and mutation coverage <ref type="bibr" target="#b3">[3]</ref>. For prime and du-paths, we limited lengths to a maximum of ten. Path coverage was measured at the function level (that is, a path is the path taken from entry to exit of a single function).</p><p>Both experiments used 532 mutants, randomly sampled from the space of all 12,424 valid YAFFS2 mutants, using the C program mutation approach (and software) shown to provide a good proxy for fault detection by Andrews et al. <ref type="bibr" target="#b4">[4]</ref>. Unfortunately, evaluation on all possible mutants would require prohibitive computational resources: evaluation on 532 mutants required over 11 days of compute time. Random sampling of mutants has been shown to provide useful results in cases where full evaluation is not feasible <ref type="bibr" target="#b38">[38]</ref>. Our sampled mutants were not guaranteed to be killable by the API calls and emulation mode tested. We expect that considerably more than half of these mutants lie in code that cannot execute due to our using YAFFS2's RAM emulation in place of actual flash hardware, or due to the set of YAFFS2 API calls that we test. Some unknown portion of the remainder are semantically equivalent mutants. Unfortunately, excluding mutants for these three cases statically is very difficult, and we do not want to prune out all hard-to-kill mutants-those are precisely the mutants we want to keep! The only difference between experiments, other than the number of configurations, was that in the first experiment mutation coverage was computed online, and was included in the test budget for each approach. The second experiment did not count mutation coverage computations as part of the test budgets, but executed all mutation tests offline, in order to show how results changed with increased number of tests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Results-Coverage and Mutants Killed</head><p>Table <ref type="table" target="#tab_0">1</ref> shows how swarm configuration affects testing of YAFFS2. For each experiment, the first column of results shows how C D performed, the second column shows the coverage for swarm testing, and the last column shows the coverage for combining the two test suites. Each individual test case contained 200 file system operations: the swarm test suite in the second experiment executed a total of 1.17 million operations, and testing all mutants required an additional 626 million YAFFS2 calls.</p><p>The first experiment (columns 2-4 of Table <ref type="table" target="#tab_0">1</ref>) shows that, despite In the second experiment (columns 5-7 of Table <ref type="table" target="#tab_0">1</ref>), test throughput was about 3× greater due to offline computation of mutation coverage. Here C D covered slightly more blocks and branches than the swarm tests. However, of the six blocks and nine branches covered only by C D , all but four (two of each) were low-probability behaviors of the rename operation. In file system development and testing at NASA's Jet Propulsion Laboratory, rename was by far the most complex and faulty operation, and we expect that this is true for many file systems <ref type="bibr" target="#b17">[17]</ref>. Discussion with the primary author of YAFFS has confirmed that he also believes rename to be the most complex function we tested. This result suggests a vulnerability (or intelligent use requirement) for swarm: if a single feature is expected to account for a large portion of the behavioral complexity and potential faults in a system, it may well be best to set the probability of that feature to more than 50%. Nonetheless, swarm managed to produce better coverage for all other metrics, executing almost 30,000 more paths than testing under C D only, and still killed all of the mutants killed by C D , plus two additional mutants. The swarm advantage in nontrivial mutant kills was reduced to 13% (15 vs. 17 kills). In fact, the additional 4,295 tests (with a different, larger, set of C i ) did not add mutants to the set killed by swarm, suggesting good fault detection ability for swarm on YAFFS2, even with a small test budget. Using C D once more tended towards "overkill," with an average of 3,756 killing tests per mutant. Swarm only killed each mutant an average of 2,669 times. fd0 = yaffs_open("/ram2k/umtpaybhue",O_APPEND|O_EXCL |O_TRUNC|O_RDWR|O_CREAT,S_IREAD); yaffs_write(fd0, rwbuf, 9243); fd2 = yaffs_open("/ram2k/iri",O_WRONLY|O_RDONLY|O_CREAT, S_IWRITE); fd3 = yaffs_open("/ram2k/iri",O_WRONLY|O_TRUNC|O_RDONLY |O_APPEND,S_IWRITE); yaffs_write(fd3, rwbuf, 5884); yaffs_write(fd3, rwbuf, 903); fd6 = yaffs_open("/ram2k/wz",O_WRONLY|O_CREAT,S_IWRITE); yaffs_write(fd2, rwbuf, 3437); yaffs_write(fd6, rwbuf, 8957); yaffs_write(fd3, rwbuf, 2883); yaffs_write(fd3, rwbuf, 4181); yaffs_read(fd2, rwbuf, 8405); fd12 = yaffs_open("/ram2k/gddlktnkd", O_TRUNC|O_RDWR|O_WRONLY|O_APPEND|O_CREAT, S_IREAD); yaffs_write(fd0, rwbuf, 3387); yaffs_write(fd12, rwbuf, 2901); yaffs_write(fd12, rwbuf, 9831); yaffs_freespace("/ram2k/wz");  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Results-Significant Features</head><p>Figure <ref type="figure" target="#fig_2">2</ref> shows a delta-debugged <ref type="bibr" target="#b37">[37]</ref> version of one of the swarm that killed mutant #62. Using C D never killed this mutant. The original line of code at line 2 of yaffs_UseChunkCache is:</p><formula xml:id="formula_0">if (dev-&gt;srLastUse &lt; 0 || dev-&gt;srLastUse &gt; 100000000)</formula><p>The mutant changes &lt; 0 to &gt; 0. The minimized test requires no operations other than yaffs_open, yaffs_write, yaffs_read and yaffs_freespace. The five tests killing this mutant in the first experiment were all produced by two C i , both of which disabled close, lseek, symlink, link, readdir, and truncate. The universal omission of close, in particular, probably indicates that this API call actively interferes with triggering this fault: it is difficult to perform the necessary write operations to expose the bug if files can be closed at any point in the test. The other missing features may all indicate passive interference: without omitting a large number of features it is difficult to explore the space of combinations of open and write, and observe the incorrect free space, in the 200 operations performed in each test.</p><p>Figure <ref type="figure" target="#fig_3">3</ref> shows which YAFFS2 test features were significant in killing mutant #62. The 95% confidence intervals (computed using the Wilson score for a binomial proportion) are somewhat wide because this mutant was killed only 10 times in the second experiment. Calls to freespace, open, and write are clearly "triggers" (and almost certainly necessary) for this bug, while close is, as  expected, an active suppressor. Calls to mkdir and rmdir are probably passive suppressors (we have not discovered any mechanism for active suppression, at least); we know that in model checking <ref type="bibr" target="#b16">[16]</ref>, omitting directory operations can give much better coverage of operations such as write and read.</p><p>Figure <ref type="figure" target="#fig_5">4</ref> shows 95% confidence intervals for each feature in the 137 test cases (from the second experiment, again) killing another mutant that was only killed by swarm testing. This mutant negates a condition in the code for marking chunks dirty: bi-&gt;pagesInUse == 0 becomes bi-&gt;pagesInUse != 0. Here, freespace is again required, but only chmod acts as a major trigger. This mutant affects a deeper level of block management, so either a file or directory can expose the fault: thus mkdir is a moderate trigger and open is possibly a marginal trigger but neither is required, as either call will set up conditions for exposure. A typical delta-debugged test case killing this mutant includes 15 calls to chmod, but chmod is not required to expose the fault-it is simply a very effective way to repeatedly force any directory entry to be rewritten to flash. Moreover, rmdir completely suppresses the fault-presumably by deleting directories before the chmod-induced problems can exhibit. It is possible to imagine hand-tuning of the YAFFS2 call mix helping to detect a fault like mutant #62. It is very hard to imagine a file system expert, one not already aware of the exact fault, tuning a tester to expose a bug like mutant #400.</p><p>As a side note, we suspect that these kinds of confidence interval graphs, which appear as a natural byproduct of swarm testing, may be quite helpful as aids to fault localization, debugging, and program understanding. In principle such graphs may also be produced from C D , but with random testing and realistic test case sizes, the chance of a feature not appearing in a test case is close to zero; even if measuring feature frequency provides useful information, which seems unlikely, this complicates producing useful graphs.</p><p>Table <ref type="table" target="#tab_1">2</ref> shows which YAFFS2 features contributed to mutant killing and which features suppressed mutant killing. Percentages represent the fraction of killed mutants for which the feature's presence or absence was statistically significant. While mutants are not necessarily good representatives of real faults (we show how swarm performs for real faults below), the particular features that trigger and suppress the most bugs are quite surprising. For example, it is at first puzzling why fchmod is such a helpful feature. We believe this to be a result of the power of fchmod and chmod to "burn" through flash pages quickly by forcing rewrites of directory entries for either a file or a directory. The most likely explanation for fchmod's value over chmod lies in feedback's ability to always select a valid file descriptor, giving a rewrite of an open file; we know that open files are more likely to be involved in file system faults. Table <ref type="table" target="#tab_1">2</ref>  shows that picking any single C is likely to weaken testing. The three features that suppress the most faults are, respectively, also triggers for 11%, 16%, and 26% of mutants killed. The obvious active suppressors close and closedir are triggers for 57% and 35% of mutants killed, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Other Configuration Possibilities</head><p>Random generation with 50% probability per feature of omission ("coin tossing") is not the only way to build a set {C 1 . . .C n }. Although we did not explore this space in depth, we did perform some limited comparisons of the coin-toss approach with using 5-way covering arrays from NIST <ref type="bibr" target="#b27">[27]</ref> to produce C i . A 5-way covering array guarantees that all 32 possible combinations for each set of 5 features are covered in the set; we used 5-way coverage because we speculate that very few YAFFS2 bugs require more than 5 features to expose. For the 23 YAFFS2 test features, a 5-way covering requires n = 167. Each C i≤n has some features specified as "on," some as "off," and others as "don't care." The "don't care" features can be included or excluded as desired.</p><p>We compared coverage results for covering-array-based swarm sets with "don't care" set two ways-first to inclusion, then to exclusion-with a swarm set using our coin-toss approach and with the {C D } only. Both non-random 5-way covering sets and random swarm performed much better than C D for path coverage. The nonrandom 5-way covering sets slightly improved on coin tossing if "don't care" features were included, but gave lower path coverage when they were omitted. Coin-toss swarm always performed best (by at least six blocks and seven branches) for block and branch coverage-both non-random 5-way covering sets performed slightly worse than C D for block and branch coverage.</p><p>These results at minimum suggest that random generation of C i is not obviously worse than some combinatorics-based approaches. As we would expect, comparison of coin-toss swarm with 5-way covering sets with all "don't care" values picked via coin toss showed very little difference in performance, with pure coin-toss slightly better by some measures and the covering-array sets slightly better by other measures. Any set of unbiased random C i of size 120 or greater is 99% likely to be 3-way covering, and 750 C i are 99% likely to be 5-way covering <ref type="bibr" target="#b25">[25]</ref>. Sets the size of those used in our primary YAFFS2 experiments are very likely 3-way covering, and quite possibly approximately 5-way covering. Finally, we investigated the simplest approach to swarm: using a new random C i for each test. Table <ref type="table" target="#tab_2">3</ref> compares 5,000 tests with C D and 5,000 tests with random C i . For these results, we were able to use a new, much faster, version of our YAFFS2 tester, supporting 14 more features (calls) and computing a version of Ball's predicatecomplete test (PCT) coverage (pr) <ref type="bibr" target="#b9">[9]</ref>. Since this result was based on equal tests, not equal time, we also show total test runtime in seconds (rt), not counting mutant analysis or coverage computations, which required an additional 21-27 hours. If C i generation is inexpensive, choosing a random C i for each test simplifies swarm and produces very good results, at least for YAFFS2: a set of tests based on full random swarm takes less time to run than a C D based suite and produces universally better coverage, including an additional 10 mutant kills.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Case Study: C Compilers</head><p>Csmith <ref type="bibr" target="#b36">[36]</ref> is a random C program generator; its use has resulted in more than 400 bugs being reported to commercial and opensource compiler developers. Most of these reports have led to compiler defects being fixed. Csmith generates test cases in the form of random C programs. A Csmith test configuration is a set C language features that can be included in these generated random C programs. In most cases the feature is essentially a production rule in the grammar for C programs. By default, Csmith errs on the side of expressiveness: C D emits test cases containing all supported parts of the C language. Command line arguments can prohibit Csmith from including any of a large variety of features in generated programs, however. To support swarm testing, we did not have to modify Csmith in any way: we simply called the Csmith tool with arguments for our test configuration, and compiled the resulting random C program with each compiler to be tested. Feature control had previously been added by the Csmith developers (including ourselves) to support testing compilers for embedded platforms that only compile subsets of the C language, and for testing and debugging Csmith itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Methodology</head><p>We used Csmith to generate random C programs and fed these programs to 17 compiler versions targeting the x86-64 architecture; these compilers are listed in Table <ref type="table" target="#tab_3">4</ref>. While these 17 versions arise from only 5 different base compilers, in our experience major releases of the GCC and LLVM compilers are quite different, in terms of code base as well as, most critically, new bugs introduced and old bugs fixed. All of these tools are (or have been) in general use to compile production code. All compilers were run under Linux. When possible, we used the pre-compiled binaries distributed by the compilers' vendors.</p><p>Our testing focused on distinct compiler crash errors. This metric are two distinct ways that GCC 4.6.0 can crash. We believe that this metric represents a conservative estimate of the number of true compiler bugs. Our experience-based on hundreds of bug reports to real compiler teams-is that it is almost always the case that distinct error messages correspond to distinct bugs. The converse is not true: many different bugs may hide behind a generic error message such as Segmentation fault. Our method for counting crash errors may over-count in the case where we are studying multiple versions of the same compiler, and several of these versions contain the same (unfixed) bug. However, because the symptom of this kind of bug typically changes across versions (e.g., the line number of an assertion failure changes due to surrounding code being modified), it is difficult to reliably avoid double-counting. We did not attempt to do so. However, as noted below, our results retain their significance if we simply consider the single buggiest member of each compiler family.</p><p>We tested each compiler using vanilla optimization options ranging from "no optimization" to "maximize speed" and "minimize size." For example, GCC and LLVM/Clang were tested using -O0, -O1, -O2, -Os, and -O3. We did not use any of the architecture or feature-specific options (e.g., GCC's -m3dnow or Intel's -openmp) options that typically make compilers extremely easy to crash.</p><p>We generated 1,000 unique C i , each of which included some of the following (with 50% probability for each feature):</p><p>• declaration of main() with argc and argv • the comma operator, as in x = (y, 1);</p><p>• compound assignment operators, e.g. x += y;</p><p>• embedded assignments, as in x = (y = 1);</p><p>• the auto-increment and auto-decrement operators ++ and --• goto </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Results-Distinct Bugs Found</head><p>The top-level result from this case study is that with all other factors being equal, a week of swarm testing on a single, reasonably fast machine found 104 distinct ways to crash our collection of compilers, whereas using C D (the way we have always run Csmith in the past) found only 73-an improvement of 42%. Table <ref type="table" target="#tab_3">4</ref> breaks these results down by compiler. A total of 47,477 random programs were tested under C D , of which 22,691 crashed at least one compiler. <ref type="foot" target="#foot_1">2</ref> A total of 66,699 random programs were tested by swarm, of which 15,851 crashed at least one compiler. Thus, swarm found 42% more distinct ways to crash a compiler while finding 30% fewer actual instances of crashes. Test throughput increased for the swarm case because simpler test cases (i.e., those lacking some features) are faster to generate and compile.</p><p>To test the statistical significance of our results, we split each of the two one-week tests into seven independent 24-hour periods and used the t-test to check if the samples were from different populations. The resulting p-value for the data is 0.00087, indicating significance at the 99.9% level. We also normalized for number of test cases, giving C D a 40% advantage in CPU time. Swarm remained superior in a statistically significant sense.</p><p>Even if we attribute some of this success to over-counting of bugs across LLVM or GCC versions, we observe that taking only the most buggy version of each compiler (thus eliminating all double counting), swarm revealed 56 distinct faults compared to only 37 for C D , which is actually a larger improvement (51%) than in the full case study. Using C D detected more faults than swarm in only one compiler version, Intel CC 12.0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Results-Code Coverage</head><p>Table <ref type="table" target="#tab_4">5</ref> shows the effect that swarm testing has on coverage of two compilers: LLVM/Clang 2.9 and GCC 4.6.0. To compute confidence intervals for the increase in coverage, we ran seven 24-hour tests for each compiler and for each of swarm and C D . The absolute values of these results should be taken with a grain of salt: LLVM/Clang and GCC are both large (2.6 MLOC and 2.3 MLOC, respectively) and contain much code that is impossible to cover during our tests. We believe that these incremental coverage values-for example, around 2,000 additional branches in GCC were covered-support our claim that swarm testing provides a useful amount of additional test coverage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Results-Significant Features</head><p>During the week-long swarm test run described in Section 3.2.2, swarm testing found 104 distinct ways to crash a compiler in our test set. Of these 104 crash symptoms, 54 occurred enough times for us to analyze crashing test cases for significant features. (Four occurrences are required for a feature present in all four test cases to become recognizable as not including the baseline 50% occurrence rate for the feature in its 95% confidence interval.) 52 of these 54 crashes had at least one feature whose presence was significant and 42 had at least one feature whose absence was significant. Table <ref type="table" target="#tab_5">6</ref> shows which of the C program features that we used in this swarm test run were statistically likely to trigger or suppress crash bugs. Some of these results, such as the frequency with which pointers, arrays, and structs trigger compiler bugs, are unsurprising. On the other hand, we did not expect pointers, embedded assignments, jumps, arrays, or the auto increment/decrement operators to figure so highly in the list of bug suppressors.</p><p>We take two lessons from the data in Table <ref type="table" target="#tab_5">6</ref>. First, some features (most notably, pointers) strongly trigger some bugs while strongly suppressing others. This observation directly motivates swarm testing. Second, our intuitions (built up over the course of reporting 400 compiler bugs) did not serve us well in predicting which features would most often trigger and suppress bugs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.5">An Example Bug</head><p>A bug we found in Clang 2.6 causes it to crash when compiling-at any optimization level-the code in Figure <ref type="figure">5</ref>, with this message:  This crash happened 395 times during our test run. Figure <ref type="figure" target="#fig_6">6</ref> shows that packed structs (and therefore, of course, also structs) are found in 100% of test cases triggering this bug. This is not surprising since the error is in Clang's logic for dealing with packed structs. The only other feature that has a significant effect on the incidence of this bug is bitfields, which suppresses it. We looked at the relevant Clang source code and found that the assertion violation happens in a function that helps build up a struct by adding its next field. It turns out that this (incorrect) function is not called when a struct containing bitfield members is being built. This explains why the presence of bitfields suppress this bug.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Miniature Case Study: Sglib RBTree</head><p>Our primary target for swarm testing is complex systems software with many features. However, swarm testing can also be applied to simpler SUTs. To evaluate how swarm performs on smaller programs, we applied it to the red-black tree implementation in the widely used Sglib library. The implementation is 476 lines of code and has seven API calls (the features). A test case, as with YAFFS2, is a sequence of API calls with parameters. Like the YAFFS2-37 API results, these results include PCT coverage. Test-case execution time varied only trivially with C, so we were able to simply perform 20,000 tests for each experiment.  <ref type="table" target="#tab_7">7</ref> compares coin-toss swarm sets of two sizes (one set of ten C i and another of twenty), 2-way and 3-way covering-array swarm sets, a complete swarm set (all 127 feature combinations), and the default strategy, all for test cases of length ten. The benefits of swarm here are limited: with length-10 test cases and only seven features, each feature already has a 20% chance of being omitted from any given test. The best value for each coverage type is shown in bold, and the worst in italics. The swarm sets in this experiment outperformed C D by a respectable margin for every kind of coverage. The results for the size-20 coin toss, however, show that where the benefits of swarm over the default in terms of diversity are marginal, a bad set can make testing less effective. It is also interesting to note that even when it is easy to do, complete coverage of all combinations does not do best in all coverage metrics, and increased k for covering is also sometimes harmful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Threats to Validity</head><p>The statistical analysis strongly supports the claim that the swarm treatment did indeed have positive effects for Csmith, including increased code coverage for GCC and LLVM. While there is a threat from multiple counting of bugs across GCC and LLVM versions, the overall fault-detection advantage of swarm increased if we considered only the version of each compiler with the most faults.</p><p>The YAFFS2 coverage results are more anecdotal and varied. For the mutation results, the 95% confidence intervals on features showing that some features were included in no killing tests support the claim that detection of these particular mutants is a result of swarm's configuration diversity, as C D is extremely unlikely to produce tests of the needed form (e.g., without any close/mkdir/rmdir calls). Sampling more mutants would increase our confidence in these results, but we believe it is safe to say we found at least 5 mutants that statistically, C D , will not kill with reasonable-sized test suites; we found no such mutants swarm is unlikely to kill.</p><p>The primary threats come from external validity: use of limited systems/feature definitions limits generalization. However, file systems and compilers are good representatives of programs for which people will devote the effort to build strong random testers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">RELATED WORK</head><p>Swarm testing is a low-cost and effective approach to increasing the diversity of (randomly generated) test cases. It is inspired by swarm verification <ref type="bibr" target="#b21">[21]</ref>, which runs a model checker in multiple search configurations to improve coverage of large state spaces. The core idea of swarm verification is that given a fixed time/memory budget, a "swarm" of diverse searches can explore more of the state space than a single search. Swarm verification is successful in part because a single "best" search cannot easily exploit parallelism: the communication overhead for checking whether states have already been visited by another worker gives a set of independent searches an advantage. This advantage disappears in random testing: runs are always completely independent. The benefits of our swarm thus do not depend on any loss of parallelism inherent in the default test configuration or on the failure of depth-first search to "escape" a subtree when exploring a very large state space <ref type="bibr" target="#b14">[14]</ref>. Our results reflect the value of (feature omission) diversity in test configurations. Swarm verification and swarm testing are orthogonal approaches: swarm verification could be applied in combination with feature omission to produce further state-space exploration diversity.</p><p>Another related area is configuration testing, which diversifies the SUT itself (e.g., for programs with many possible builds) <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b32">32]</ref>. As noted above, we vary the "configuration" of the test generation system to produce a variety of tests, rather than varying the SUT. Configuration testing is thus also orthogonal to our work. In practice, our test configurations often do not require new builds of even the test generation system, but only require use of different command-line arguments to constrain tests. Another approach that may be conceptually related is the idea of testability transformation proposed by Korel et al. <ref type="bibr" target="#b23">[23]</ref>. While considerably more heavyweight than swarm, and aimed at improving source-based test data generation rather than random testing, the idea of "slicing away" some parts of the program under test is in some ways like configuration testing, but is directed by picking a class of test cases on which the slice is based (those hitting a given target).</p><p>In partition testing <ref type="bibr" target="#b18">[18]</ref> an input space is divided into disjoint partitions, and the goal of testing is to sample (cover) at least one input from each partition. Two underlying assumptions are usually that (1) the partition forces diversity and (2) inputs from a single partition are largely interchangeable. Without a sound basis for the partitioning scheme (which can be hard to establish), partition testing can perform worse than pure random testing <ref type="bibr" target="#b18">[18]</ref>. Category-partition testing <ref type="bibr" target="#b28">[28]</ref> may improve partition testing, through programmer identification of functional units and parameters and external conditions that affect each unit. The kind of thinking used in category-partition testing could potentially be used in determining features in a random tester. Swarm testing differs from partition testing and category partition testing partly in that it has no partitions which must be disjoint and cover the space, only a set of features which somehow constrain generated tests.</p><p>Combinatorial testing <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b26">26]</ref> seeks to test input parameters in combinations, with the goal of either complete test coverage for interactions of parameters, as in k-wise testing, or reducing total testing time while maximizing diversity (in a linear arithmetical sense) when k-way coverage of combinations is too expensive, as in orthogonal array testing <ref type="bibr" target="#b30">[30]</ref>. Combinatorial techniques can be used to generate swarm sets (the input parameters are the features). Combinatorial testing has been shown to be quite effective for testing appropriate systems, almost as effective as exhaustive testing.</p><p>Swarm testing differs from partition testing and combinatorial testing approaches primarily in the number of tests associated with a "configuration." In partition approaches, each partition typically includes a large number of test cases, but coverage is usually based on picking one test from each partition. Swarm does not aim at exhaustively covering a set of partitions (such as the cross-product of all feature values), but may generate many tests for the same test configuration. Traditional combinatorial testing is based on actual input parameter values: each combination in the covering array will result in one test case. In swarm testing, a combination of features defines only a constraint on test cases, and thus a very large or even infinite set of tests. Many tests may be generated from the set defined by a test configuration. The use of combinatorial techniques in generating test configurations, rather than actual tests, merits further study.</p><p>Adaptive random testing (ART) <ref type="bibr" target="#b11">[11]</ref> modifies random testing by sampling the space of tests and only executing those most "distant," as determined by a distance metric over inputs, from all previously executed tests. Many variations on this approach have been proposed. Unlike ART, swarm testing does not require the workhuman effort or computational-of using a distance metric. The kind of "feature breakdown" that swarm requires is commonly provided by test generators; in our experience developing over a dozen test harnesses, we implemented the generator-configuration options long before we considered utilizing them as part any methodology other than simply finding a good default configuration; implementing "features" where these are API call choices or grammar productions is usually almost trivial. Swarm testing has been applied to real-world systems with encouraging results; ART has not always been shown to be effective for complex real-world programs <ref type="bibr">[7]</ref>, and has mostly been applied to numeric input programs.</p><p>More generally, structural testing <ref type="bibr" target="#b33">[33]</ref>, statistical testing <ref type="bibr" target="#b31">[31]</ref>, many meta-heuristic testing approaches <ref type="bibr" target="#b1">[1]</ref>, and even concolic testing <ref type="bibr" target="#b15">[15]</ref> can be viewed as aiming at a set of test cases exhibiting diversity in the targets they cover-e.g., statements, branches, or paths <ref type="bibr" target="#b10">[10]</ref>. Other approaches make diversity explicit-e.g., in looking for operational abstractions <ref type="bibr" target="#b20">[20]</ref> or contract violations <ref type="bibr" target="#b34">[34]</ref>, or in feedback <ref type="bibr" target="#b29">[29]</ref>. Some of these techniques are, given sufficient compute time and appropriate SUT, highly effective. Swarm is a more lightweight technique than most of these approaches, which often require symbolic execution, considerable instrumentation, or machine learning. The most scalable but effective techniques often focus on a certain kind of application and type of fault. Some approaches refine their notion of diversity in such a way that future exploration relies on past results, making them nontrivial to parallelize. Swarm testing is inherently massively parallel. Finally, swarm testing is in principle applicable to any software-testing (or model-checking) approach that can use a test configuration, including many of those discussed above, whereas, e.g., ART is tied to random testing. We have performed preliminary experiments in using swarm to improve the scalability of bounded model checking of C programs <ref type="bibr" target="#b12">[12]</ref>, with some success <ref type="bibr" target="#b2">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSIONS AND FUTURE WORK</head><p>Swarm testing relies on the following claim: for realistic systems, randomly excluding some features from some tests can improve coverage and fault detection, compared to a test suite that potentially uses every feature in every test. The benefit of using of a single, inclusive, default configuration-that every test can potentially expose any fault and cover any behavior, heretofore usually taken for granted in random testing-does not, in practice, make up for the fact that some features can, statistically, suppress behaviors. Effective testing therefore may require feature omission diversity. We show that this not only holds for simple containerclass examples (e.g., pop operations suppress stack overflow) but for a widely used flash file system and 14 out of 17 versions of five production-quality C compilers. For these real-world systems, if we compare testing with a single inclusive configuration to testing with a set of 100-1,000 unique configurations, each omitting features with 50% probability per feature, we have observed (1) significantly better fault detection, (2) significantly better branch and statement coverage, and (3) strictly superior mutant detection. Test configuration diversity does indeed produce better testing in many realistic situations.</p><p>Swarm testing was inspired by swarm verification, and we hope that its ideas can be ported back to model checking. We also plan to investigate swarm in the context of bounded exhaustive testing and learning-based testing methods. Finally, we believe there is room to better understand why swarm provides its benefits, particularly in the context of large, idiosyncratic SUTs such as compilers, virtual machines, and OS kernels. More case studies will be needed to generate data to support this work. We also plan to investigate how swarm testing's increased diversity of code coverage in test cases can benefit fault localization and program understanding algorithms relying on test cases <ref type="bibr" target="#b22">[22]</ref>; traditional random tests are far more homogeneous than swarm tests.</p><p>We have made Python scripts supporting swarm testing available at http://beaversource.oregonstate.edu/projects/ cswarm/browser/release.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Swarm testing changes the distribution of test cases for a stack. If push and pop operations are selected with equal probability, about 1 in 370,000 test cases will trigger a bug in a 32-element stack's overflow-detection logic. Swarm testing (note the test cases concentrated near the x-axis and y-axis of Figure1(b)) triggers this bug in about 1 of every 16 cases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :e s p a c e f s t a t l i n k l s e e k l s t a t m k d i r o p e n o p e n d i r r e a d r e a d d i r r e a d l i n</head><label>2</label><figDesc>Figure 2: Operations in a minimized test case for killing YAFFS2 mutant #62. The mutant returns an incorrect amount of remaining free space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: 95% confidence intervals for the percentage of test cases killing YAFFS2 mutant #62 containing each call</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: 95% confidence intervals for the percentage of test cases killing mutant #400 containing each YAFFS2 API call</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: 95% confidence intervals for the percentage of test cases triggering the Clang bug triggered by the code in Figure 5 containing each Csmith program feature</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>YAFFS2 coverage results tests, swarm testing improved on the default configuration in all coverage measures-the difference is particularly remarkable for path coverage, where swarm testing explored over 10,000 more paths. The combined test suite results show that default and swarm testing overlapped in most forms of coverage, but that C D did explore some blocks, branches, and paths that swarm did not. For pure path coverage, the two types of testing produced much more disjoint coverage. Surprisingly, swarm was strictly superior in terms of mutation kills: swarm killed three mutants that C D did not, and killed every mutant killed by C</figDesc><table><row><cell cols="5"># = number of tests; coverage: bl = blocks; br = branches; du = du-paths; pr = prime paths; pa = paths; mu = mutants killed</cell></row><row><cell></cell><cell cols="2">swarm 100, mutants online C D Swarm Both</cell><cell cols="2">swarm 500, offline C D Swarm Both</cell></row><row><cell># bl br du pr pa mu</cell><cell>1,747 1,161 1,247 2,487 2,834 14,153 94</cell><cell>1,593 1,168 1,253 2,507 2,872 25,484 35,478 3,340 1,173 1,261 2,525 2,964 97 97</cell><cell>5,665 1,173 1,261 2,525 2,907 35,432 95</cell><cell>5,888 11,553 1,172 1,178 1,259 1,268 2,538 2,552 2,967 3,018 64,845 91,280 97 97</cell></row><row><cell cols="3">executing 154 fewer</cell><cell></cell><cell></cell></row></table><note><p>D . On average, C D killed each mutant 1,173 times. Swarm only killed each mutant (counting only those killed by both test suites, to avoid any effect from swarm also killing particularly hard-to-kill mutants) an average of 725 times. An improvement of three mutants out of 94 may seem small, but a better measure of fault detection capabilities may be kill rates for nontrivially detectable mutants. It seems reasonable to consider any mutant killed by more than 10% of random tests (each with only 200 operations) to be uninteresting. Even quite desultory random testing will catch such faults. Of the 97 mutants killed by C D , only 14 were killed by &lt; 10% of tests, making swarm's 17 nontrivial kills an improvement of over 20%.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>also Top trigger and suppressor features for YAFFS2</figDesc><table><row><cell>Triggers fchmod 66% lseek 65% read 62% write 61% open 58% close 57% fstat 56% symlink 41% closedir 35% opendir 35% truncate 34% rmdir 32% chmod 32% readdir 31% mkdir 30% freespace 28% link 26% stat 25% readlink 18% unlink 16% lstat 14% rename 11% rewinddir 10%</cell><cell>Suppressors rename 29% unlink 26% link 24% rewinddir 22% closedir 12% lstat 12% opendir 12% stat 11% mkdir 11% readdir 9% close 9% symlink 8% truncate 8% rmdir 5% chmod 5% fstat 4% lseek 4% readlink 4% open 4% freespace 4% read 3% write 3% fchmod 3%</cell></row></table><note><p>Values in the table show the percentage of YAFFS2 mutants that were statistically likely to be triggered (killed) or suppressed by test cases containing the listed API calls.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>YAFFS2 37 API results    </figDesc><table><row><cell>Method</cell><cell>bl</cell><cell>br</cell><cell>pa</cell><cell>pr mu</cell><cell>rt</cell></row><row><cell>Swarm C D</cell><cell cols="5">1,459 1,641 112,944 61,864 123 136 1,446 1,626 70,587 61,380 113 158</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Distinct crash bugs found during one week of testing</figDesc><table><row><cell>Compiler</cell><cell cols="3">C D Swarm Both</cell></row><row><cell cols="2">LLVM/Clang 2.6 10</cell><cell>12</cell><cell>14</cell></row><row><cell>LLVM/Clang 2.7</cell><cell>5</cell><cell>6</cell><cell>7</cell></row><row><cell>LLVM/Clang 2.8</cell><cell>1</cell><cell>1</cell><cell>1</cell></row><row><cell>LLVM/Clang 2.9</cell><cell>0</cell><cell>1</cell><cell>1</cell></row><row><cell>GCC 3.2.0</cell><cell>5</cell><cell>10</cell><cell>11</cell></row><row><cell>GCC 3.3.0</cell><cell>3</cell><cell>4</cell><cell>5</cell></row><row><cell>GCC 3.4.0</cell><cell>1</cell><cell>2</cell><cell>2</cell></row><row><cell>GCC 4.0.0</cell><cell>8</cell><cell>8</cell><cell>10</cell></row><row><cell>GCC 4.1.0</cell><cell>7</cell><cell>8</cell><cell>10</cell></row><row><cell>GCC 4.2.0</cell><cell>2</cell><cell>5</cell><cell>5</cell></row><row><cell>GCC 4.3.0</cell><cell>7</cell><cell>8</cell><cell>9</cell></row><row><cell>GCC 4.4.0</cell><cell>2</cell><cell>3</cell><cell>4</cell></row><row><cell>GCC 4.5.0</cell><cell>0</cell><cell>1</cell><cell>1</cell></row><row><cell>GCC 4.6.0</cell><cell>0</cell><cell>1</cell><cell>1</cell></row><row><cell>Open64 4.2.4</cell><cell>13</cell><cell>18</cell><cell>20</cell></row><row><cell>Sun CC 5.11</cell><cell>5</cell><cell>14</cell><cell>14</cell></row><row><cell>Intel CC 12.0.5</cell><cell>4</cell><cell>2</cell><cell>5</cell></row><row><cell>Total</cell><cell>73</cell><cell>104</cell><cell>120</cell></row><row><cell cols="4">considers two crashes of the same compiler to be distinct if and only if the compiler tells us that it crashed in two different ways. For example</cell></row></table><note><p>internal compiler error: in vect_enhance_data_refs_alignment, at tree-vect-data-refs.c:1550 and internal compiler error: in vect_create_epilog_for_reduction, at tree-vect-loop.c:3725</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Compiler code coverage</figDesc><table><row><cell cols="2">Compiler Metric</cell><cell cols="2">C D Swarm</cell><cell>Change (95% conf.)</cell></row><row><cell>Clang</cell><cell>line branch function</cell><cell>95,021 63,285 43,098</cell><cell>95,695 64,052 43,213</cell><cell>446-903 619-915 37-193</cell></row><row><cell>GCC</cell><cell>line branch function</cell><cell cols="3">142,422 144,347 1,547-2,303 114,709 116,664 1,631-2,377 9,177 9,263 61-112</cell></row><row><cell cols="3">• integer division • integer multiplication • long long integers • 64-bit math operations • structs • bitfields • packed structs • unions • arrays • pointers • const-qualified objects • volatile-qualified objects • volatile-qualified pointers</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Top trigger and suppressor features for C compilersValues in the table show the percentage of compiler crash bugs that were statistically likely to be triggered or suppressed by test cases containing the listed C program features.</figDesc><table><row><cell>Triggers Embedded assignments 15% Pointers 33% Arrays 31% Structs 29% Volatiles 21% Bitfields 15% Consts 13% Comma operator 11% Jumps 11% Unions 11% Packed structs 10% Long long ints 10% 64-bit math 10% Integer division 8% Compound assignments 8% Integer multiplication 6%</cell><cell>Suppressors Embedded assignments 24% Pointers 41% Jumps 21% Arrays 17% ++ and -16% Volatiles 15% Unions 13% Comma operator 11% Long long ints 11% Compound assignments 11% Bitfields 10% Consts 10% Volatile pointers 10% 64-bit math 8% Structs 7% Packed structs 7%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Sglib red-black tree coverage results</figDesc><table><row><cell>Method</cell><cell>bl</cell><cell>br</cell><cell>pa</cell><cell>pr mu</cell></row><row><cell cols="5">Coin-Toss 10 181 206 169 2,839 190 Coin-Toss 20 157 182 165 2,469 175 2-Way Cover 182 209 219 2,823 188 3-Way Cover 169 209 167 2,518 187 Complete 176 203 192 2,688 190 C D 170 192 149 2,504 187</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>In C/C++, a[i] is syntactic sugar for *(a+i).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>We realize that it may be hard to believe that nearly half of random test cases would crash some compiler. Nevertheless, this is the case. The bulk of the "easy" crashes come from Open64 and Sun CC, which have apparently not been the target of much random testing. Clang, GCC, and Intel CC are substantially more robust, particularly in recent versions.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">ACKNOWLEDGMENTS</head><p>The authors thank the anonymous reviewers for their comments, Jamie Andrews for the use of mutation generation code for C, and Amin Alipour, Mike Ernst, Patrice Godefroid, Gerard Holzmann, Rajeev Joshi, Alastair Reid, and Shalini Shamasunder for helpful comments on the swarm testing concept. A portion of this research was funded by NSF grant CCF-1054786.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A systematic review of the application and empirical investigation of search-based test case generation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Briand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hemmati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Panesar-Walawege</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Software Eng</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="742" to="762" />
			<date type="published" when="2010-12">Nov./Dec. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bounded model checking and feature omission diversity</title>
		<author>
			<persName><forename type="first">A</forename><surname>Alipour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Groce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CFV</title>
		<meeting>CFV</meeting>
		<imprint>
			<date type="published" when="2011-11">Nov. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Ammann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Offutt</surname></persName>
		</author>
		<title level="m">Introduction to Software Testing</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Is mutation an appropriate tool for testing experiments?</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Briand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Labiche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICSE</title>
		<meeting>ICSE</meeting>
		<imprint>
			<date type="published" when="2005-05">May 2005</date>
			<biblScope unit="page" from="402" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Random test run length and effectiveness</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Groce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R.-G</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASE</title>
		<meeting>ASE</meeting>
		<imprint>
			<date type="published" when="2008-09">Sept. 2008</date>
			<biblScope unit="page" from="19" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Nighthawk: A two-level genetic-random unit test data generator</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">C H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Menzies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASE</title>
		<meeting>ASE</meeting>
		<imprint>
			<date type="published" when="2007-11">Nov. 2007</date>
			<biblScope unit="page" from="144" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adaptive random testing: An illusion of effectiveness?</title>
		<author>
			<persName><forename type="first">A</forename><surname>Arcuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Briand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ISSTA</title>
		<meeting>ISSTA</meeting>
		<imprint>
			<date type="published" when="2011-07">July 2011</date>
			<biblScope unit="page" from="265" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Formal analysis of the effectiveness and predictability of random testing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Arcuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Z</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Briand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ISSTA</title>
		<meeting>ISSTA</meeting>
		<imprint>
			<date type="published" when="2010-07">July 2010</date>
			<biblScope unit="page" from="219" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A theory of predicate-complete test coverage and generation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ball</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. FMCO</title>
		<meeting>FMCO</meeting>
		<imprint>
			<date type="published" when="2004-11">Nov. 2004</date>
			<biblScope unit="page" from="1" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fundamentals of test case selection: Diversity, diversity, diversity</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SEDM</title>
		<meeting>SEDM</meeting>
		<imprint>
			<date type="published" when="2010-06">June 2010</date>
			<biblScope unit="page" from="723" to="724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adaptive random testing</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">K</forename><surname>Mak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Computer Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">3321</biblScope>
			<biblScope unit="page" from="320" to="329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A tool for checking ANSI-C programs</title>
		<author>
			<persName><forename type="first">E</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K F</forename><surname>Lerda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. TACAS, volume 2988 of LNCS</title>
		<meeting>TACAS, volume 2988 of LNCS</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="168" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Configuration fuzzing for software vulnerability detection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kaiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ARES</title>
		<meeting>ARES</meeting>
		<imprint>
			<date type="published" when="2010-02">Feb. 2010</date>
			<biblScope unit="page" from="525" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Parallel randomized state-space search</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Dwyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Elbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Person</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Purandare</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICSE</title>
		<meeting>ICSE</meeting>
		<imprint>
			<date type="published" when="2007-05">May 2007</date>
			<biblScope unit="page" from="3" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">DART: Directed automated random testing</title>
		<author>
			<persName><forename type="first">P</forename><surname>Godefroid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Klarlund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. PLDI</title>
		<meeting>PLDI</meeting>
		<imprint>
			<date type="published" when="2005-06">June 2005</date>
			<biblScope unit="page" from="213" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">(Quickly) testing the tester via path coverage</title>
		<author>
			<persName><forename type="first">A</forename><surname>Groce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WODA</title>
		<meeting>WODA</meeting>
		<imprint>
			<date type="published" when="2009-07">July 2009</date>
			<biblScope unit="page" from="22" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Randomized differential testing as a prelude to formal verification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Groce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Holzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Joshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICSE</title>
		<meeting>ICSE</meeting>
		<imprint>
			<date type="published" when="2007-05">May 2007</date>
			<biblScope unit="page" from="621" to="631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Partition testing does not inspire confidence</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hamlet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Software Eng</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1402" to="1411" />
			<date type="published" when="1990-12">Dec. 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Random testing</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hamlet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Encyclopedia of Software Engineering</title>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="970" to="978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improving test suites via operational abstraction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Harder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mellen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Ernst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICSE</title>
		<meeting>ICSE</meeting>
		<imprint>
			<date type="published" when="2003-05">May 2003</date>
			<biblScope unit="page" from="60" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Swarm verification techniques</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Holzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Groce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Software Eng</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="845" to="857" />
			<date type="published" when="2011-12">Nov./Dec. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Empirical evaluation of the Tarantula automatic fault-localization technique</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Harrold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASE</title>
		<meeting>ASE</meeting>
		<imprint>
			<date type="published" when="2005-11">Nov. 2005</date>
			<biblScope unit="page" from="273" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Data dependence based testability transformation in automated test generation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Korel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Harman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Apirukvorapinit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ISSRE</title>
		<meeting>ISSRE</meeting>
		<imprint>
			<date type="published" when="2005-11">Nov. 2005</date>
			<biblScope unit="page" from="245" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Software fault interactions and implications for software testing</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Gallo</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Software Eng</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="418" to="421" />
			<date type="published" when="2004-06">June 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A survey of binary covering arrays</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">N</forename><surname>Kacker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Forbes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electron. J. Comb</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A survey of combinatorial testing</title>
		<author>
			<persName><forename type="first">C</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Leung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2011-01">Jan. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><surname>Nist</surname></persName>
		</author>
		<ptr target="http://math.nist.gov/coveringarrays/ipof/tables/table.5.2.html" />
		<title level="m">NIST covering array tables</title>
		<imprint>
			<date type="published" when="2008-02">Feb. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The category-partition method for specifying and generating functional tests</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Ostrand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Balcer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CACM</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="676" to="686" />
			<date type="published" when="1988-06">June 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Feedback-directed random test generation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Pacheco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Lahiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ball</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICSE</title>
		<meeting>ICSE</meeting>
		<imprint>
			<date type="published" when="2007-05">May 2007</date>
			<biblScope unit="page" from="75" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Planning efficient software tests</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Phadke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CrossTalk</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="11" to="15" />
			<date type="published" when="1997-10">Oct. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Efficient software verification: Statistical testing using automated search</title>
		<author>
			<persName><forename type="first">S</forename><surname>Poulding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Software Eng</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="763" to="777" />
			<date type="published" when="2010-12">Nov./Dec. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Configuration-aware regression testing: An empirical study of sampling and prioritization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rothermel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ISSTA</title>
		<meeting>ISSTA</meeting>
		<imprint>
			<date type="published" when="2008-07">July 2008</date>
			<biblScope unit="page" from="75" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">An investigation of statistical software testing</title>
		<author>
			<persName><forename type="first">P</forename><surname>Thévenod-Fosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Waeselynck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Software Testing, Verification and Reliability</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="5" to="25" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Furia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Horton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Steindorfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nordio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Meyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Stateful testing: Finding more errors in code and contracts</title>
		<imprint>
			<date type="published" when="2011-08">Aug. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<ptr target="http://www.yaffs.net/" />
		<title level="m">YAFFS: A flash file system for embedded use</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Finding and understanding bugs in C compilers</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Eide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Regehr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. PLDI</title>
		<meeting>PLDI</meeting>
		<imprint>
			<date type="published" when="2011-06">June 2011</date>
			<biblScope unit="page" from="283" to="294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Simplifying and isolating failure-inducing input</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zeller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hildebrandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Software Eng</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="183" to="200" />
			<date type="published" when="2002-02">Feb. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Is operator-based mutant selection superior to random mutant selection?</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-S</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICSE</title>
		<meeting>ICSE</meeting>
		<imprint>
			<date type="published" when="2010-05">May 2010</date>
			<biblScope unit="page" from="435" to="444" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
