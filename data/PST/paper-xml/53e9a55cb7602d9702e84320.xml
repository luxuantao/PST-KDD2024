<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Understanding Random SAT: Beyond the Clauses-to-Variables Ratio</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Eugene</forename><surname>Nudelman</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Alex</forename><surname>Devkar</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yoav</forename><surname>Shoham</surname></persName>
							<email>shoham¤@cs.stanford.edu</email>
						</author>
						<author>
							<persName><forename type="first">Kevin</forename><surname>Leyton-Brown</surname></persName>
							<email>kevinlb@cs.ubc.ca</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<settlement>Stanford</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">University of British Columbia</orgName>
								<address>
									<settlement>Vancouver</settlement>
									<region>BC</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Understanding Random SAT: Beyond the Clauses-to-Variables Ratio</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">42FCE8FF639A99BA941AB6E15C549425</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>It is well known that the ratio of the number of clauses to the number of variables in a random ¦ -SAT instance is highly correlated with the instance's empirical hardness. We consider the problem of identifying such features of random SAT instances automatically with machine learning. We describe and analyze models for three SAT solvers-kcnfs, oksolver and satz-and for two different distributions of instances: uniform random 3-SAT with varying ratio of clauses-to-variables, and uniform random 3-SAT with fixed ratio of clauses-tovariables. We show that surprisingly accurate models can be built in all cases. Furthermore, we analyze these models to determine which features are most useful in predicting whether an instance will be hard to solve. Finally we discuss other applications of our models including SATzilla, a portfolio of existing SAT solvers, which competed in the 2003 and 2004 SAT competitions. 3  </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>SAT is among the most studied problems in computer science, representing a generic constraint satisfaction problem with binary variables and arbitrary constraints. It is also the prototypical § ©¨-Hard problem, and so its worst-case complexity has received much attention. Accordingly, it is not surprising that SAT has become a primary platform for the investigation of average-case and empirical complexity. Particular interest has been shown for randomly-generated SAT instances:this testbed offers a range of very easy to very hard instances for any given input size, yet the simplicity of the algorithm used to generate such instances makes them easier to understand analytically. Moreover, working on this testbed offers the opportunity to make connections to a wealth of existing work.</p><p>A seminal paper by Selman, Mitchell and Levesque <ref type="bibr" target="#b12">[13]</ref> considered the empirical performance of DPLL-type solvers running on uniform-random -SAT instances. <ref type="foot" target="#foot_1">4</ref> It found a strong correlation between the instance's hardness and the ratio of the number of clauses to the number of variables in the instance. Furthermore, it demonstrated that the hardest region (e.g., for random 3-SAT, a clauses-to-variables ratio of roughly 4.26) corresponds exactly to a phase transition in a non-algorithm-specific theoretical property of the instance: the probability that a randomly-generated formula having a given ratio will be satisfiable. This well-publicized finding led to increased enthusiasm for the idea of studying algorithm performance experimentally, using the same tools as are used to study natural phenomena. Over the past decade, this approach has complemented more traditional theoretical worst-case analysis of algorithms, with interesting findings on (e.g.) islands of tractability <ref type="bibr" target="#b6">[7]</ref>, search space topologies for stochastic local search algorithms <ref type="bibr" target="#b5">[6]</ref>, backbones <ref type="bibr" target="#b11">[12]</ref>, backdoors <ref type="bibr" target="#b15">[16]</ref> and random restarts <ref type="bibr" target="#b4">[5]</ref> that have improved our understanding of algorithms' empirical behavior.</p><p>Inspired by the success of this work in SAT and related problems, in 2001 we proposed a new methodology for using machine learning to study empirical hardness <ref type="bibr" target="#b9">[10]</ref>. We applied this methodology to the Combinatorial Auction Winner Determination Problem (WDP)-an § ©¨-hard combinatorial optimization problem equivalent to weighted set packing. In later work <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b7">8]</ref> we extended our methodology, demonstrating techniques for improving empirical algorithm performance through the construction of algorithm portfolios, and for automatically inducing hard benchmark distributions. In this paper we come full circle and apply our methodology to SAT-the original inspiration for its development.</p><p>This work has three goals. Most directly, it aims to show that inexpensively-computable features can be used to make accurate predictions about the empirical hardness of random SAT instances, and to analyze these models in order to identify important features. We consider three different SAT algorithms (kcnfs, oksolver and satz, each of which performed well in the Random category in one or more past SAT competitions) and two different instance distributions. The first instance distribution, random 3-SAT instances where the ratio of clauses-to-variables is drawn uniformly from ! " # $ &amp;% , allows us to find out whether our techniques would be able to automatically discover the importance of the clauses-to-variables ratio in a setting where it is known to be important, and also to investigate the importance of other features in this setting. Our second distribution is uniform-random 3-SAT with the ratio of clauses-to-variables held constant at the phase transition point of 4.26. This distribution has received much attention in the past, and poses an interesting puzzle: orders-of-magnitude runtime variation persists in this so-called "hard region."</p><p>Second, we show that empirical hardness models have other useful applications for SAT. Most importantly, we describe a SAT solver, SATzilla, which uses hardness models to choose among existing SAT solvers on a per-instance basis. We explain some details of its construction and summarize its performance in the 2003 SAT competition.</p><p>Our final goal is to offer a concrete example in support of our abstract claim that empirical hardness models are a useful tool for gaining understanding about the behavior of algorithms for solving § ©¨-hard problems. Thus, while we believe that our SAT results are interesting in their own right, it is important to emphasize that very few of our techniques are particular to SAT, and indeed that we have achieved equally strong results when applying our methodologies to other, qualitatively different problems. <ref type="foot" target="#foot_2">5</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>Although the work surveyed above has led to great advances in understanding the empirical hardness of SAT problems, most of these approaches scale poorly to more complicated domains. In particular, most of these methods involve exhaustive exploration of the search and/or distribution parameter spaces, and require considerable human intervention and decision-making. As the space of relevant features grows and instance distributions become more complex, it is increasingly difficult either to characterize the problem theoretically or to explore its degrees of freedom exhaustively. Moreover, most current work focuses on understanding algorithms' performance profiles, rather than trying to characterize the hardness of individual problem instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Empirical Hardness Models</head><p>In <ref type="bibr" target="#b9">[10]</ref> we proposed a novel experimental approach for predicting the runtime of a given algorithm on individual problem instances:</p><p>1. Select a problem instance distribution.</p><p>Observe that since we are interested in the investigation of empirical hardness, the choice of distribution is fundamental-different distributions can induce very different algorithm behavior. It is convenient (though not necessary) for the distribution to come as a parameterized generator; in this case, a distribution must be established over the generator's parameters.</p><p>2. Select one or more algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Select a set of inexpensive, distribution-independent features.</head><p>It is important to remember that individual features need not be perfectly predictive of hardness; ultimately, our goal will be to combine features together. The process of identifying features relies on domain knowledge; however, it is possible to take an inclusive approach, adding all features that seem reasonable and then removing those that turned out to be unhelpful (see <ref type="bibr">step 5)</ref>. It should be noted, furthermore, that many features that proved to be useful for one constraint problem can carry over into another.</p><p>4. Sample the instance distribution to generate a set of instances. For each instance, determine the running time of the selected algorithms and compute the features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Eliminate redundant or uninformative features.</head><p>As a practical matter, much better models tend to be learned when all features are informative. A variety of statistical techniques are available for eliminating or deemphasizing the effect of such features. The simplest one is to manually examine pairwise correlations, eliminating features that are highly correlated with what to terminate the algorithm the moment a solution is found, as in SAT. While algorithms for WDP usually find the optimal solution quickly, they spend most of their time proving optimality, a process analogous to proving unsatisfiability. We also have unpublished initial results showing promising hardness model performance for TSP and computation of Nash equilibria.</p><p>remains. Shrinkage techniques (such as lasso <ref type="bibr" target="#b13">[14]</ref> or ridge regression) are another alternative.</p><p>6. Use machine learning to select a function of the features that predicts each algorithm's running time.</p><p>Since running time is a continuous variable, regression is the natural machinelearning approach to use for building runtime models. (For more detail about why we prefer regression to other approaches such as classification, see <ref type="bibr" target="#b9">[10]</ref>.) We describe the model-construction process in more detail in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Building Models</head><p>There are a wide variety of different regression techniques; the most appropriate for our purposes perform supervised learning. <ref type="foot" target="#foot_3">6</ref> Such techniques choose a function from a given hypothesis space (i.e., a space of candidate mappings from the given features to the running time) in order to minimize a given error metric (a function that scores the quality of a given mapping, based on the difference between predicted and actual running times on training data, and possibly also based on other properties of the mapping). Our task in applying regression to the construction of hardness models thus reduces to choosing a hypothesis space that is able to express the relationship between our features and our response variable (running time), and choosing an error metric that both leads us to choose good mappings from this hypothesis space and can be tractably minimized.</p><p>The simplest regression technique is linear regression, which learns functions of the form ' )( 0 ( 21 &amp;( , where 1 &amp;( is the 3 th feature and the 0 's are free variables, and has as its error metric root mean squared error. Linear regression is a computationally appealing procedure because it reduces to the (roughly) cubic-time problem of matrix inversion. In comparison, most other regression techniques depend on more difficult optimization problems such as quadratic programming.</p><p>Choosing an Error Metric Linear regression uses a squared-error metric, which corresponds to the 4 ¡ distance between a point and the learned hyperplane. Because this measure penalizes outlying points superlinearly, it can be inappropriate in cases where data contains many outliers. Some regression techniques use 4 error (which penalizes outliers linearly); however, optimizing these error metrics often requires solution of a quadratic programming problem. Some error metrics express an additional preference for models with small (or even zero) coefficients to models with large coefficients. This can lead to much more reliable models on test data, particularly in cases where features are correlated. Some examples of such "shrinkage" techniques are ridge, lasso and stepwise regression. Shrinkage techniques generally have a parameter that expresses the desired tradeoff between training error and shrinkage; this parameter is generally tuned using either cross-validation or a validation set.</p><p>Choosing a Hypothesis Space Although linear regression seems quite limited, it can actually be used to perform regression in a wide range of hypothesis spaces. There are two key tricks. The first is to introduce new features which are functions of the original features. For example, in order to learn a model which is a quadratic rather than a linear function of the features, the feature set can be augmented to include all pairwise products of features. A hyperplane in the resulting much-higher-dimensional space corresponds to a quadratic manifold in the original feature space. The key problem with this approach is that the set of features grows quadratically, which may cause the regression problem to become intractable (e.g., because the feature matrix cannot fit into memory) and can also lead to overfitting (when the hypothesis space becomes expressive enough to fit noise in the training data). In this case, it can make sense to add only a subset of the pairwise products of features; e.g., one heuristic is to add only pairwise products of the most important features in the linear regression model. Of course, we can use the same idea to reduce many other nonlinear hypothesis spaces to linear regression: all hypothesis spaces which can be expressed by ' ( &amp;0 ( 65 ( 87 f9 , where 5 @( is an arbitrary function and f is the set of all features.</p><p>Sometimes we want to consider hypothesis spaces of the form A 7 ' B( &amp;0 ( 5 ( 7 f9 C9 . For example, we may want to fit a sigmoid or an exponential curve. When A is a one-to-one function, we can transform this problem to a linear regression problem by replacing our response variable D in our training data by A FE 7 D 9 , where A GE is the inverse of A , and then training a model of the form ' B( &amp;0 ( 5 ( 7 f9 . On test data, we must evaluate the model A 7 ' B( &amp;0 ( H5 ( C7 f9 C9 . One caveat about this trick is that it distorts the error metric: the errorminimizing model in the transformed space will not generally be the error-minimizing model in the true space. In many cases this distortion is acceptable, however, making this trick a tractable way of performing many different varieties of nonlinear regression.</p><p>Two examples that we will discuss later in the paper are exponential models (A 7 6I 9 QP R TS U ; A E 7 6I 9 )P WV YX ` ba 7 6I 9 ) and logistic models (A 7 HI 9 cP R &amp;d 7 R fe hg E U 9 ; A E 7 6I 9 iP V qp 7 HI 9 rV Yp 7 R ts I 9 with values of I first mapped on to the interval 7 S R 9 ). Because they evaluate to values on a finite interval, we have found logistic models to be particularly useful in cases where runs were capped.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Evaluating the Importance of Variables in a Hardness Model</head><p>If we are able to construct an accurate empirical hardness model, it is natural to try to explain why it works. A key question is which features were most important to the success of the model. It is tempting to interpret a linear regression model by comparing the coefficients assigned to the different features, on the principle that larger coefficients indicate greater importance. This can be misleading for two reasons. First, features may have different ranges, a problem that can be addressed by normalization. A more fundamental problem arises in the presence of correlated features. For example, if two features are unimportant but perfectly correlated, they could appear in the model with arbitrarily large coefficients but opposite signs. A better approach is to force models to contain fewer variables, on the principle that the best low-dimensional model will involve only relatively uncorrelated features. Once such a model has been obtained, we can evaluate the importance of each feature to that model by looking at each feature's cost of omission. That is, we can train a model without the given feature and report the resulting increase in (cross-validated) prediction error. To make them easier to compare, we scale the cost of omission of the most important feature to 100 and scale the other costs of omission in proportion.</p><p>There are many different "subset selection" techniques for finding good, small models. Ideally, exhaustive enumeration would be used to find the best subset of features of desired size. Unfortunately, this process requires consideration of a binomial number of subsets, making it infeasible unless both the desired subset size and the number of base features are very small. When exhaustive search is impossible, heuristic search can still find good subsets. The most well-known ones heuristics are forward selection, backward elimination and sequential replacements. Forward selection starts with an empty set, and greedily adds the feature that, combined with the current model, makes the largest reduction to cross-validated error. Backward elimination starts with a full model and greedily removes the features that yields the smallest increase in cross-validated error. Sequential replacement is like forward selection, but also has the option to replace a feature in the current model with an unused feature. Finally, the recently introduced LAR <ref type="bibr" target="#b1">[2]</ref> algorithm is a shrinkage technique for linear regression that can set the coefficients of sufficiently unimportant variables to zero as well as simply reducing them; thus, it can be also be used for subset selection. Since none of these four techniques is guaranteed to find the optimal subset, we combine them together by running all four and keeping the model with the smallest cross-validated (or validation-set) error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Hardness Models for SAT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Features</head><p>Fig. <ref type="figure" target="#fig_0">1</ref> summarizes the 91 features used by our SAT models. However, these features are not all useful for every distribution: as we described above, we eliminate uninformative or highly correlated features after fixing the distribution. For example, while ratio of clauses-to-variables was important for SATzilla, it is not at all useful for the dataset that studies solvers performance at a fixed ratio at a phase transition point. In order to keep values to sensible ranges, whenever it makes sense we normalize features by either the number of clauses or the number of variables in the formula.</p><p>The features can be roughly categorized into 9 groups. The first group captures problem size, measured by the number of clauses, variables, and the ratio of the two. Because we expect this ratio to be an important feature, we include squares and cubes of both the ratio and its reciprocal. Also, because we know that features are more powerful in simple regression models when they are directly correlated with the response variable, we include a "linearized" version of the ratio which is defined as the absolute value of the difference between the ratio and the phase transition point, 4.26. The next three groups correspond to three different graph representations of a SAT instance. Variable-Clause Graph (VCG) is a bipartite graph with a node for each variable, a node for each clause, and an edge between them whenever a variable occurs in a clause. Variable Graph (VG) has a node for each variable and an edge between variables that occur together in at least one clause. Clause Graph (CG) has nodes representing clauses and an edge between two clauses whenever they share a negated literal. All of these graphs correspond to constraint graphs for the associated CSP; thus, they encode the problem's u wv yx ! C C b b v b ed f g Number of Clauses: h i 2j 2k l qi ml n bo p q r !g Number of Variables: h Ci bj 2k bl Yi l 6n bo p s t 2u v g Ratio: q bw 8s yx {z Yq bw 8s | 6} {x {z ~q bw s | H 2u q !g Ratio Reciprocal: z qs w !q 2| 2x yz qs w !q 2| 6} x Tz qs w !q 2| 6 bu ~f f g Linearized Ratio: { ¡ 8 E q bw 8s &amp; x w { ¡ 8 E q bw 8s &amp; } x { ¡ 8 E q bw 8s &amp; $ bv y 2 q $ C m v y The eighth group involves running DPLL "probes." First, we run a DPLL procedure (without backtracking) to an exponentially-increasing sequence of depths, measuring the number of unit propagations done at each depths. We also run depth-first random probes by repeatedly instantiating random variables and performing unit propagation until a contradiction is found. The average depth at which a contradiction occurs is an unbiased estimate of the log size of the search space <ref type="bibr" target="#b10">[11]</ref>. Our final group of features probes the search space with two stochastic local search algorithms, GSAT and SAPS. We run both algorithms many times, each time continuing the search trajectory until a plateau cannot be escaped within a given number of steps. We then average various statistics collected during each run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experimental Setup</head><p>Our first dataset contained 20000 uniformly-random 3-SAT instances with 400 variables each. To determine the number of clauses in each instance, we determined the clausesto-variables ratio by drawing a uniform sample from # $ 8 " # $ $% (i.e., the number of clauses varied between 1304 and 2104). Our second dataset also contained 20000 fixed size 3-SAT instances. In this case each instance was generated uniformly at random with a fixed clauses-to-variables ratio of 4.26. We again generated 400-variable formulas; thus each formula had 1704 clauses. On each dataset we ran three solvers-kcnfs, oksolver and satz-which performed well on random instances in previous years' SAT competitions. Our experiments were executed on 2.4 GHz Xeon processors, under Linux 2.4.20. Our fixed-size experiments took about four CPU-months to complete. In contrast, our variable-size dataset took only about one CPU-month, since many instances were generated in the easy region away from the phase transition point. Every solver was allowed to run to completion on every instance.</p><p>Each dataset was split into 3 parts-training, test and validation sets-in the ratio Ø S Ó R Ó R . All parameter tuning was performed with the validation set; the test set was used only to generate the graphs shown in this paper. We used the R and Matlab software packages to perform all machine learning and statistical analysis tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Variable-Size Random Data</head><p>Our first set of experiments considered a set of uniform random 3-SAT instances where the clauses-to-variables ratio was drawn from the interval ! " # $ &amp;% . We had three goals with this distribution. First, we wanted to show that our empirical hardness model training and analysis techniques would be able to automatically "discover" that the clauses-to-variables ratio was important to the empirical hardness of instances from this distribution. Second, having included nine features derived from this ratio among our 91 features-the clauses-to-variables ratio itself, the square of the ratio, the cube of the ratio, its reciprocal (i.e., the variables-to-clauses ratio), the square and cube of this reciprocal, the absolute value minus 4.26, and the square and cube of this absolute value-we wanted to find out what particular function of these features would be most predictive of hardness. Third, we wanted to find out what other features, if any, would be important to a model in this setting.</p><p>It is worthwhile to start by examining the clauses-to-variables ratio in more detail. To build models, we first considered linear, logistic and exponential models in our 91 features, evaluating the models on our validation set. Of these, linear were the worst and logistic and exponential were similar, with logistic being slightly better. Next, we wanted to consider quadratic models under these same three transformations. However, a full quadratic model would have involved 4277 features, and given that our training data involved 14000 different problem instances, training the model would have entailed inverting a matrix of nearly sixty million values. In order to concentrate on the most important quadratic features, we first used our variable importance techniques to identify the best 30-feature subset of our 91 features. We computed the full quadratic expansion of these features, then performed forward selection-the only subset selection technique that worked with such a huge number of features-to keep only the most useful features. We ended up with 368 features, some of which were members of our original set of 91 features and the rest of which were products of these original features. Again, we evaluated linear, logistic and exponential models; all three model types were better with the expanded features, and again logistic models were best.</p><p>Figs <ref type="figure">4</ref> and<ref type="figure">5</ref> show our logistic models in this quadratic case for kcnfs and satz (both evaluated for the first time on our test set). First, note that these are incredibly accurate models: perfect predictions would lie exactly on the line D ÚP I , and in these scatter- plots the vast majority of points like on or very close to this line, with no significant    bias in the residuals. <ref type="foot" target="#foot_4">7</ref> The RMSE for the kcnfs, satz and oksolver models are 16.02, 18.64 and 18.96 seconds respectively. Second, the scatterplots look very similar (as does the plot for oksolver, not shown here.)</p><formula xml:id="formula_0">Û ÝÜ ßÞ Û à ¯Û á ãâ Û â Ü â Þ à à à á ä Û ä Ü ä Þ Üà ÃÜ á ¹å Û å Ü S u</formula><p>The next natural question is whether this similar model performance occurs because runtimes of the two algorithms are strongly correlated. Fig. <ref type="figure">3</ref> shows kcnfs runtime vs. satz runtime on all instances. Observe that there appear to be two qualitatively different patterns in this scatterplot. We plotted satisfiable and unsatisfiable instances separately in Figs. <ref type="figure">6</ref> and<ref type="figure">7</ref>, and indeed the different categories exhibit entirely different behavior: runtimes of unsatisfiable instances were almost perfectly correlated, while runtimes of satisfiable instances were almost entirely uncorrelated. We conjecture that this is because proving unsatisfiability of an instance requires exploring the whole search tree, which does not differ substantially between the algorithms, while finding a satisfiable assignment depends on each algorithm's different heuristics. We can conclude that the similarly accurate model performance between the algorithms is due jointly to the correlation between their runtimes on UNSAT instances and to the ability of our features to express both the runtimes of these UNSAT instances and the runtimes of each algorithm's runtime profile on SAT instances.</p><p>We now turn to the question of what variables were most important to our models. Because of space constraints, for the remainder of this paper we focus only on our</p><formula xml:id="formula_1">Cbp 6é Yê &amp;ë C• ì &amp; ¶ í î {• ï y ¶ ï T• » rð ñ u ¢ g r ò é ò f £ C£ î {• rï y ¶ ê &amp;ë C• $ì $ ¶ í ¶ ï T• » rð ñ ê &amp;ì ó í Ãé ò ¢ C ¶ 8• Ţ ¶ ó yi Hp Yl Y ¶ l Yi 2 ê Tk 8i 6ô î $ b é b¢ Cò Fõ ¶ • r{ ¶ ó yi Hp Yl ê {k 8i 6ô {î &amp; b ö i e Cj «é b£ Cò ¢ 8f º ¶ 8• » ó wi Hp Yl Y ¶ l qi b ö i e Cj é C ò Fõ º ¶ 8• » • $ ² 8ð b k b 2i H» Tk 8ó yi Hp Yl ö i Cj Fé b¢ Cò t Table 1.</formula><p>Variable Importance in Variable Size Models models for kcnfs.<ref type="foot" target="#foot_5">8</ref> Fig. <ref type="figure">8</ref> shows the validation set RMSE of our best subset of each size. Note that our best four-variable model achieves a root-mean-squared error of 19 seconds, while our full 368-feature model had an error of about 15.5 seconds. Table <ref type="table">4</ref> lists the four variables in this model along with their normalized costs of omission.</p><p>Note that our most important feature (by far) is the linearized version of Ù d Ê , and our second most important feature is 7 Ê d Ù T9 ÷ . This represents the satisfaction of our first and second objectives for this dataset: our techniques correctly identified the importance of the clauses-to-variables ratio and also informed us of which of our nine variants of this feature were most useful to our models. The third and fourth variables in this model satisfy our third objective: we see that Ù d Ê variants are not the only useful features in this model. Interestingly, both of these re- maining variables are constructed from local search probing features. It may be initially surprising that local search probes can convey meaningful information about the runtime behavior of DPLL searches. However, observe that the two approaches' search spaces and search strategies are closely related. Consider the local-search objective function "number of satisfied clauses." Non-backtrack steps in DPLL can be seen as monotonic improvements to this objective function in the space of partial truth assignments, with backtracking occurring only when no such improvement is possible. Furthermore, a partial truth assignment corresponds to a set of local search states, where each variable assignment halves the cardinality of this set and every backtrack doubles it. Since both search strategies alternate between monotonic improvements to the same objective functions and jumps to other (often nearby) parts of the search space, it is not surprising that large-scale topological features of the local search space are correlated with the runtimes of DPLL solvers. Since GSAT and SAPS explore the local search space very differently, their features give different but complementary views of the same search topology. Broadly speaking, GSAT goes downhill whenever it can, while SAPS uses its previous search experience to construct clause weights, which can sometimes influence it to move uphill even when a downhill option exists.</p><p>In passing, we point out an interesting puzzle that we encountered in analyzing our variable-size models. We discovered that the clause graph clustering coefficient is almost perfectly correlated with Ê d Ù , as illustrated in Fig. <ref type="figure" target="#fig_3">9</ref>. This is particularly interesting as the clustering coefficient of the constraint graph has been shown to be an important statistic in a wide range of combinatorial problems. We haven't been able to to find any reference in the SAT literature relating CGCC to Ê d Ù . However, our intuition leads us to believe that this nearly-linear relationship should hold with high probability and that it will provide new insights into why Ê d Ù is correlated with hardness. Conventional wisdom has it that uniform-random 3-SAT is easy when far from the phase-transition point, and hard when close to it. In fact, while the first part of this statement is generally true, the second part is not. Fig. <ref type="figure" target="#fig_0">10</ref> shows histograms of our three algorithms on our second dataset, fixed-size instances generated with Ù d Ê ÝP ùø m . We can see that there is substantial runtime variation in this "hard" region: each algorithm is well represented in at least three of the order-of-magnitude bins. This distribution is an interesting setting for the construction of empirical hardness models-our most important features from the variable size distribution, variants of Ù d Ê , are constant in this case. We are thus forced to concentrate on new sources of hardness. This distribution is also interesting because, since the identification of the Ù d Ê phase transition, it has become perhaps the most widely used SAT benchmark. We built models in the same way as described in Section 4, except that we omitted all variants of Ù d Ê because they were constant. Again, we achieved the best (validation set) results with logistic models on a (partial) quadratic expansion of the features. Fig. <ref type="figure" target="#fig_0">12</ref> shows the performance of our logistic model for kcnfs on test data. For the sake of comparison, Fig. <ref type="figure" target="#fig_0">13</ref> shows the performance of our linear model for kcnfs (again with the quadratic expansion of the features) on the same test data. Although both models are surprisingly good-especially given the difficulty of the dataset-the linear model shows considerably more bias in the residuals.  Fig. <ref type="figure" target="#fig_0">11</ref> shows the validation set RMSE of the best model we found at each subset size. In this case we chose to study the 8-variable model. The variables in the model, along with their costs of omission, are given in Table <ref type="table" target="#tab_2">2</ref>. This time local search probing features are nearly dominant; this is particularly interesting since the same features appear for both local search algorithms, and since most of the available features never appear. The most important local search concepts appear to be the objective function value at the deepest plateau reached on a trajectory (BestSolution), and the number of steps required to reach this deepest plateau (BestStep).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">SATzilla and Other Applications of Hardness Models</head><p>While the bulk of this paper has aimed to show that accurate empirical hardness models are useful because of the insight they give into problem structure, these models also have other applications <ref type="bibr" target="#b7">[8]</ref>. For example, it is very easy to combine accurate hardness models with an existing instance generator to create a new generator that makes harder instances, through the use of rejection sampling techniques. Within the next few months, we intend to make available a new generator of harder random 3-SAT formulas. This generator will work by generating an instance from the phase transition region and then rejecting it in inverse proportion to the log time of the minimum of our three algorithms' predicted runtimes.</p><p>A second application of hardness models is the construction of algorithm portfolios. It is well known that for SAT (as for many other hard problems) different algorithms often perform very differently on the same instances. Indeed, this is very clear in Fig. <ref type="figure">6</ref>, which shows that kcnfs and satz are almost entirely uncorrelated in their runtimes on satisfiable random 3-SAT instances. On distributions for which this sort of uncorrelation holds, selecting an algorithm to run on a per-instance basis offers the potential for substantial improvements over per-distribution algorithm selection. Empirical hardness models allow us to do just this, build algorithm portfolios that select an algorithm to run based on predicted runtimes.</p><p>We can offer concrete evidence for the utility of our this second application of hardness models: SATzilla, an algorithm portfolio that we built for the 2003 SAT competition <ref type="bibr" target="#b2">[3]</ref>. This portfolio consisted of 2clseq, eqSatz, HeerHugo, JeruSat, Limmat, oksolver , Relsat, Sato, Satz-rand and zChaff. At the time of writing, a second version of SATzilla is participating in the 2004 SAT competition. This version drops HeerHugo, but adds Satzoo, kcnfs , and BerkMin, new solvers that appeared in 2003 and performed well in the 2003 competition.</p><p>To construct SATzilla we began by assembling a library of about 5000 SAT instances, which we gathered from various public websites, for which we computed runtimes and the features described in Section 3.1. We built models using ridge regression. To yield better models, we dropped from our dataset all instances that were solved by all algorithms, by no algorithms, or as a side-effect of feature computation.</p><p>Upon execution, SATzilla begins by running a UBCSAT <ref type="bibr" target="#b14">[15]</ref> implementation of WalkSat for 30 seconds to filter out easy satisfiable instances. Next, SATzilla runs the Hypre <ref type="bibr" target="#b0">[1]</ref> preprocessor to clean up instances, allowing the subsequent analysis of their structure to better reflect the problem's combinatorial "core." <ref type="foot" target="#foot_6">9</ref> Third, SATzilla computes its features, terminating if any feature (e.g., probing; LP relaxation) solves the problem. Some features can also take an inordinate amount of time, particularly with very large inputs. To prevent feature computation from consuming all of our allotted time, certain features run only until a timeout is reached, at which point SATzilla gives up on computing the given feature. Fourth, SATzilla evaluates a hardness model for each algorithm. If some of the features have timed out, SATzilla uses a different model which does not involve the missing feature and which was trained only on instances where the same feature timed out. Finally, SATzilla executes the algorithm with the best predicted runtime. This algorithm continues to run until the instance is solved or until the allotted time is used up.</p><p>As described in the official report written by the 2003 SAT competition organizers <ref type="bibr" target="#b2">[3]</ref>, SATzilla's performance in this competition demonstrated the viability of our portfolio approach. SATzilla qualified to enter the final round in two out of three benchmark categories -Random and Handmade. Unfortunately, a bug caused SATzilla to crash often on Industrial instances (due to their extremely large size) and so SATzilla did not qualify for the final round in this category. During the competition, instances were partitioned into different series based on their similarity. Solvers were then ranked by the number of series in which they managed to solve at least one benchmark. SATzilla placed second in the Random category (the first solver was kcnfs, which wasn't in the portfolio as it hadn't yet been publicly released). In the Handmade instaces category SATzilla was third ( È û on satisfiable instances), again losing only to new solvers.</p><p>Figures <ref type="figure" target="#fig_0">14</ref> and<ref type="figure" target="#fig_6">15</ref> show the raw number of instances solved by the top four finalists in each of the Random and Handmade categories, in both cases also including the topfour solvers from the other category which qualified. In general the solvers that did well in one category did very poorly (or didn't qualify for the final) in the other. SATzilla is the only solver which achieved strong performance in both categories.</p><p>During the 2003 competition, we were allowed to enter a slightly improved version of SATzilla that was run as an hors concours solver, and thus was not run in the finals. According to the competition report, this improved version was first in the Random instances category both in the number of actual instances solved, and in the total runtime used (though still not in the number of series solved). As a final note, we should point out that the total development time for SATzilla was under a monthconsiderably less than most world-class solvers, though of course SATzilla relies on the existence of base solvers. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. SAT Instance Featurescombinatorial structure. For all graphs we compute various node degree statistics. For CG we also compute statistics of clustering coefficients, which measure the extent to which each node belongs to a clique. For each node the clustering coefficient is the number of edges between its neighbors divided by 7 s )R 9 d , where is the number of neighbors. The fifth group measures the balance of a formula in several different senses: we compute the number of unary, binary, and ternary clauses; statistics of the number of positive vs. negative occurrences of variables within clauses and per variable. The sixth group measures the proximity of the instance to a Horn formula, motivated by the fact that such formulas are an important SAT subclass. The seventh group of features is obtained by solving a linear programming relaxation of an integer program representing the current SAT instance. (In fact, on occasion this relaxation is able to solve the SAT instance!) Denote the formula AE µÇ T { Ç AE ÃÈ and let I É denote both boolean and LP variables. Define Ê 7 6I É 9 P I É and Ê 7 Ë «I "É 9 ÅP R ¹s I É . Then the pro- gram is maximize È ' ( qÌ ' Í Î Ï Ð Ê 7 eÑ 9 subject to Ò GAE ( Ó ' Í Î Ï Ð Ê 7 eÑ 9 ÕÔ R , Ò I É Ó S cÖ I "É Ö ×R . The objective function prevents the trivial solution where all variables are set to S # .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 showsFig. 2 .Fig. 3 .Fig. 4 . 5 .</head><label>22345</label><figDesc>Fig.2shows kcnfs runtime (log scale) vs. Ù d Ê . First observe that, unsurprisingly, there is a clear relationship between runtime and Ù d Ê . At the same time, Ù d Ê is not a</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 6 .Fig. 7 .</head><label>67</label><figDesc>Fig. 6. kcnfs vs. satz, SAT instances</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. CG Clustering Coefficient vs. ae ç {è</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 11 .Fig. 12 . 13 .</head><label>111213</label><figDesc>Fig. 10. FS Gross Hardness</figDesc><graphic coords="12,141.16,203.54,169.82,127.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>¶ 8• Ţ ¶ ó yi Hp Yl Y ¶ 8k ® ¬ 2l Yo k 8j ö i Cj Fé ò Gõ º ¶ 8• » ó yi Hp Yl ~ ¶ k 8® ¬ 2l Yo k 8j ö i Cj é b£ Cò f £ £ ¶ 8• Ţ ¶ ó yi Hp Yl Y ¶ l Yi 2 ö i e Cj é r ò õ º ¶ • {» ó yi Hp Yl Y ¶ 8k ® ¬ 2l Yo k 8j ö i e Cj é b£ Cò ¶ 8• Ţ ¶ ó yi Hp Yl Y ¶ 8k ® ¬ 2l Yo k 8j ö i Cj Fé ò õ ö i Cj ú "wë !ë ú @i b l n «é ò ¢ C ¶ 8• Ţ ¶ ó yi Hp Yl Y ¶ 8k ® ¬ 2lYo k 8j ê Tk 8i 6ô î $ b é ò Fõ ¶ 8• Ţ ¶ ó yi ep ql ~ ¶ l qi 2 ö i Cj é r Cò r r ¶ 8• Ţ ¶ ó yi Hp Yl Y ¶ 8k ® ¬ 2l Yo k 8j ê Tk 8i 6ô î $ b é ò Fõ ¶ 8• Ţ ¶ ó yi ep ql ê Tk i Hô î $ 2 ö i e Cj «é b£ Cò f 2 ¶ 8• Ţ ¶ ó yi Hp Yl Y ¶ 8k ® ¬ 2l Yo k 8j ö i Cj Fé ò Gõ ¶ • r{ ¶ ó yi Hp Yl ~ ¶ Cl Yi 2 ê Tk 8i 6ô î $ b @é b¢ ò f 2t ¶ 8• Ţ ¶ ó yi Hp Yl Y ¶ l Yi 2 ê Tk 8i 6ô î $ b @é b¢ Cò Gõ ¶ • r{ ¶ ó yi ep ql ê Tk i Hô î $ 2 ö i e Cj é b£ Cò ê º ú @i H² ! i Hi ö i e Cj é r ò Fõ ¶ 8• Ţ ¶ ó yi Hp Yl Y ¶ 8k ® ¬ 2l Yo k 8j ê Tk 8i 6ô î $ b é ò f</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 15 .</head><label>15</label><figDesc>Fig. 14. SAT-2003 Random Category</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Variable Importance in Fixed Size Models</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>We'd like to acknowledge very helpful assistance from Holger Hoos and Nando De Freitas, and our indebtedness to the authors of the algorithms in the SATzilla portfolio.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>Similar, contemporaneous work on phase transition phenomena in other hard problems was performed by Cheeseman<ref type="bibr" target="#b3">[4]</ref>, among others.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2"><p>WDP is a very different problem from SAT: feasible solutions for WDP can be identified in constant time, and the goal is to find an optimal feasible solution. There is thus no opportunity</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3"><p>Because of our interests in being able to analyze our models and in keeping model sizes small (e.g., so that models can be distributed as part of an algorithm portfolio), we avoid model-free approaches such as nearest neighbor.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_4"><p>The banding on very small runtimes in this and other scatterplots is a discretization effect due to the low resolution of the operating system's process timer.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_5"><p>We choose to focus on this algorithm because it is currently the state-of-the-art random solver; our results with the other two algorithms are comparable.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_6"><p>Despite the fact that this step led to more accurate models, we did not perform it in our investigation of uniform-random 3-SAT because it implicitly changes the instance distribution. Thus, while our models would have been more accurate, they would also have been less informative.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Effective preprocessing with hyper-resolution and equality reduction</title>
		<author>
			<persName><forename type="first">Fahiem</forename><surname>Bacchus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Winter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SAT</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Regression shrinkage and selection via the lasso</title>
		<author>
			<persName><forename type="first">B</forename><surname>Efron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Johnstone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The essentials of the SAT 2003 competition</title>
		<author>
			<persName><forename type="first">D</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Berre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Simon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SAT</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Where the Really Hard Problems Are</title>
		<author>
			<persName><forename type="first">P</forename><surname>Cheeseman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kanefsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI-91</title>
		<imprint>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Heavy-tailed phenomena in satisfiability and constraint satisfaction problems</title>
		<author>
			<persName><forename type="first">C</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Selman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Crato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. of Automated Reasoning</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Towards a characterisation of the behaviour of stochastic local search algorithms for SAT</title>
		<author>
			<persName><forename type="first">H</forename><surname>Holger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Hoos</surname></persName>
		</author>
		<author>
			<persName><surname>Stutzle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="213" to="232" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Constraint satisfaction, databases and logic</title>
		<author>
			<persName><forename type="first">Phokion</forename><surname>Kolaitis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Boosting as a metaphor for algorithm design</title>
		<author>
			<persName><forename type="first">K</forename><surname>Leyton-Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Nudelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mcfadden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shoham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Constraint Programming</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A portfolio approach to algorithm selection</title>
		<author>
			<persName><forename type="first">K</forename><surname>Leyton-Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Nudelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mcfadden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shoham</surname></persName>
		</author>
		<idno>IJCAI-03</idno>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning the empirical hardness of optimization problems: The case of combinatorial auctions</title>
		<author>
			<persName><forename type="first">K</forename><surname>Leyton-Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Nudelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shoham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CP</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Branch and bound algorithm selection by performance prediction</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lobjois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lemaître</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Determining computational complexity for characteristic &apos;phase transitions&apos;</title>
		<author>
			<persName><forename type="first">R</forename><surname>Monasson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zecchina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Selman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Troyansky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">400</biblScope>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generating hard satisfiability problems</title>
		<author>
			<persName><forename type="first">B</forename><surname>Selman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Levesque</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="17" to="29" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Regression shrinkage and selection via the lasso</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">UBCSAT: An implementation and experimentation environment for SLS algorithms for SAT and MAX-SAT</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tompkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hoos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SAT</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Backdoors to typical case complexity</title>
		<author>
			<persName><forename type="first">R</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Selman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
