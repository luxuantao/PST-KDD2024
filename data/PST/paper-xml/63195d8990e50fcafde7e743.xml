<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AudioLM: a Language Modeling Approach to Audio Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-09-07">7 Sep 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zal?n</forename><surname>Borsos</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Rapha?l</forename><surname>Marinier</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Damien</forename><surname>Vincent</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Eugene</forename><surname>Kharitonov</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Olivier</forename><surname>Pietquin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Matt</forename><surname>Sharifi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Olivier</forename><surname>Teboul</surname></persName>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Marco</forename><surname>Tagliasacchi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Neil</forename><surname>Zeghidour</surname></persName>
						</author>
						<title level="a" type="main">AudioLM: a Language Modeling Approach to Audio Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-09-07">7 Sep 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2209.03143v1[cs.SD]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce AudioLM, a framework for highquality audio generation with long-term consistency. AudioLM maps the input audio to a sequence of discrete tokens and casts audio generation as a language modeling task in this representation space. We show how existing audio tokenizers provide different trade-offs between reconstruction quality and long-term structure, and we propose a hybrid tokenization scheme to achieve both objectives. Namely, we leverage the discretized activations of a masked language model pre-trained on audio to capture long-term structure and the discrete codes produced by a neural audio codec to achieve high-quality synthesis. By training on large corpora of raw audio waveforms, AudioLM learns to generate natural and coherent continuations given short prompts. When trained on speech, and without any transcript or annotation, AudioLM generates syntactically and semantically plausible speech continuations while also maintaining speaker identity and prosody for unseen speakers. Furthermore, we demonstrate how our approach extends beyond speech by generating coherent piano music continuations, despite being trained without any symbolic representation of music.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>A UDIO signals, be they speech, music or environmental sounds, involve multiple scales of abstractions. For instance, speech can be analyzed at a very local acoustic or phonetic level but also in terms of prosody, syntax, grammar, or semantics. Music also follows a long-term structure, while being composed of highly non-stationary acoustic signals.</p><p>When it comes to audio synthesis, these multiple scales interact in such a way that achieving high audio quality while displaying high-level consistency remains a challenge, in particular in the absence of strong supervision.</p><p>Recent audio synthesis models have achieved nearly veridical signal quality by leveraging methods such as autoregressive waveform modeling <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, adversarial training <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref> or diffusion <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>. Yet, when not provided with strong conditioning (e.g., linguistic features, a MIDI sequence), even powerful models like WaveNet <ref type="bibr" target="#b0">[1]</ref> generate unstructured audio, such as babbling speech. Language models, on the other hand, have demonstrated their ability to model high-level, longterm structure for different content types, and the consequent advances in text <ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref> and image <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> generation have paved the way towards synthesis of natural audio that remains intelligible and consistent over time. An important step in that direction, coined as "textless NLP", has been recently achieved for unconditioned speech generation <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>. In ? Work done while at Google particular, <ref type="bibr" target="#b13">[14]</ref> shows that a Transformer <ref type="bibr" target="#b14">[15]</ref> trained on discretized speech units can generate coherent speech without relying on textual annotations. Yet, the acoustic diversity and the quality remain limited: the model is trained on clean speech only and synthesis is restricted to a single speaker.</p><p>In this work, we introduce AudioLM, a framework that enables high-quality audio generation with long-term coherent structure, as demonstrated by our experiments on both speech and piano music continuation. We achieve this objective by combining recent advances in adversarial neural audio compression <ref type="bibr" target="#b15">[16]</ref>, self-supervised representation learning <ref type="bibr" target="#b16">[17]</ref> and language modeling <ref type="bibr" target="#b17">[18]</ref>. Specifically, starting from raw audio waveforms, we first construct coarse semantic tokens from a model pre-trained with a self-supervised masked language modeling objective <ref type="bibr" target="#b18">[19]</ref>. Autoregressive modeling of these tokens captures both local dependencies (e.g., phonetics in speech, local melody in piano music) and global long-term structure (e.g., language syntax and semantic content in speech; harmony and rhythm in piano music). However, these tokens lead to poor reconstruction. To overcome this limitation, in addition to semantic tokens, we rely on fine-level acoustic tokens produced by a SoundStream neural codec <ref type="bibr" target="#b15">[16]</ref>, which capture the details of the audio waveform and allow for high-quality synthesis. Training a language model to generate both semantic and acoustic tokens leads simultaneously to high audio quality and long-term consistency. In summary, we make the following contributions:</p><p>? We propose AudioLM, a framework for audio generation that combines semantic and acoustic tokens in a hierarchical fashion to achieve long-term consistency and high quality.</p><p>? We compare the semantic tokens extracted from a pre-trained w2v-BERT <ref type="bibr" target="#b16">[17]</ref> and the acoustic tokens from SoundStream <ref type="bibr" target="#b15">[16]</ref> on a speech dataset, and we show that they complement each other in terms of phonetic discriminability and reconstruction quality. ? We demonstrate the ability of AudioLM to generate coherent speech in terms of phonetics, syntax and semantics, without relying on textual annotations. Moreover, when conditioned on a prefix (or prompt) of only 3 seconds of speech from a speaker not seen during training, AudioLM produces consistent continuations while maintaining the original speaker voice, prosody and recording conditions (e.g., level of reverberation, background noise). ? We show that AudioLM is also suited for music generation. When training on piano recordings, it generates convincing continuations that are coherent with the prompt in terms of melody, harmony, tone and rhythm. ? We acknowledge the potential risks associated with the use of generative models that enable speech continuation, and we mitigate these risks by training a classifier that can detect synthetic speech generated by AudioLM with very high accuracy. We encourage the reader to listen to the samples produced by AudioLM in the accompanying material. 1   II. RELATED WORK High-fidelity neural audio synthesis. Recent years have seen tremendous progress in the quality of audio generated by neural networks, largely attributed to the introduction of objective functions that improve over simple waveform regression. In particular, WaveNet <ref type="bibr" target="#b0">[1]</ref> introduced an autoregressive classification approach to speech synthesis, with quality that significantly outperformed traditional concatenative and parametric approaches at the cost of slow inference. While WaveNet inspired more computationally efficient alternatives such as WaveRNN <ref type="bibr" target="#b1">[2]</ref> or parallel WaveNet <ref type="bibr" target="#b19">[20]</ref>, a significant paradigm shift occurred with the introduction of adversarial audio generation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b20">21]</ref>, which enables high fidelity generation without any autoregressive component. Moreover, combining such high-quality synthesis systems with differentiable quantization <ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref>, allows training end-to-end neural codecs <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref> by compressing activations in a bottleneck layer. AudioLM leverages the tokens produced by a SoundStream neural codec <ref type="bibr" target="#b15">[16]</ref>, not as intermediate representations for lossy reconstruction, but rather as targets for a sequence modeling task operating at a lower sampling rate, which can be decoded back to audio at the original sampling rate. Self-supervised learning of audio representations. While neural audio synthesis typically focuses on modeling fine details of the signal, most self-supervised learning approaches rather aim at discovering high-level representations that correlate with coarse, symbolic features (e.g., phonemes, musical notes, class labels). This is typically achieved by proposing proxy objectives that do not rely on any transcript or label, but rather exploit regularities in the structure of the audio signals. Among these approaches, contrastive training learns representations for which pairs of positive examples are closer to each other than negative pairs. Positive pairs can be, for example, two segments that are close temporally <ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref> or two augmented views of the same sequence <ref type="bibr" target="#b30">[31]</ref>.</p><p>Another line of work, inspired by NLP systems pretraining <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>, has explored the discretization of audio signals into a finite vocabulary of tokens to serve as targets for masked language modeling pre-training <ref type="bibr" target="#b18">[19]</ref>, i.e. predicting long contiguous spans of masked tokens from a wide context. The discretization strategy is critical to the downstream performance of such models. Popular quantization strategies include quantizing representations optimized for future time step prediction <ref type="bibr" target="#b31">[32]</ref>, starting from quantizing lowlevel audio features followed by iterations of quantization target refinement <ref type="bibr" target="#b32">[33]</ref>, and jointly learning the quantization 1 https://google-research.github.io/seanet/audiolm/examples along with the masked language model <ref type="bibr" target="#b16">[17]</ref>. The discriminative nature of these contrastive and predictive objectives, as well as the fact that they require exploiting long-term dependencies, allow learning representations that encode coarse, high-level information about the signal (e.g., phonemes and word identity when trained on speech <ref type="bibr" target="#b33">[34]</ref>). These representations are thus particularly useful for discriminative downstream tasks such as speech recognition <ref type="bibr" target="#b32">[33]</ref> or audio classification <ref type="bibr" target="#b29">[30]</ref>. However, as they are not optimized to encode fine details of original audio signals, they are poorly invertible and thus not directly usable for synthesis. AudioLM avoids this limitation by leveraging these high-level representations as a conditioning signal that carries semantic information and guides the prediction of highquality acoustic tokens.</p><p>Generating natural signals with language models. Neural language models have demonstrated remarkable abilities for tasks as diverse as open-ended dialog modeling <ref type="bibr" target="#b34">[35]</ref>, code completion <ref type="bibr" target="#b35">[36]</ref> or even solving integrals and differential equations <ref type="bibr" target="#b36">[37]</ref>. The key underlying mechanism of the best of these models is self-attention <ref type="bibr" target="#b14">[15]</ref>, which is suitable for modeling rich and complex long-range dependencies but, in the standard form, has a computational cost that grows quadratically with the length of the input sequences. This cost is acceptable for sequences of up to 10 3 tokens <ref type="bibr" target="#b37">[38]</ref>, however, it prevents modeling natural signals in their raw form (for example, modeling a 512 ? 512 image at the pixel level). While several works have explored efficient alternatives to self-attention <ref type="bibr" target="#b38">[39]</ref><ref type="bibr" target="#b39">[40]</ref><ref type="bibr" target="#b40">[41]</ref>, another solution to this scaling problem is to work with mappings of the natural signals to a compact, discrete representation space. A common approach is to model the representations in this space with an autoregressive Transformer, whose predictions are then mapped back to the original signal space. This approach has been used to generate high-resolution images <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b41">42]</ref> and long videos <ref type="bibr" target="#b42">[43]</ref>.</p><p>For audio, Jukebox <ref type="bibr" target="#b43">[44]</ref> adopts a hierarchical approach to generate tokens at various temporal resolutions which are then combined to reconstruct music. Another notable line of work is "textless NLP" <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46]</ref>, which models language directly in the speech domain, without any transcription, by training autoregressive generative models of low-bitrate audio tokens <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b32">33]</ref>. While Jukebox and GSLM <ref type="bibr" target="#b13">[14]</ref> show high temporal coherence (e.g., spoken language generated by GSLM is meaningful), their audio quality remains limited: the music generated by Jukebox displays significant artifacts, while the speech sampled from GSLM is limited to a single speaker in a clean setting. This is unlike Perceiver AR <ref type="bibr" target="#b40">[41]</ref>, which trains an autoregressive model on the discrete codes of a highbitrate SoundStream <ref type="bibr" target="#b15">[16]</ref> codec. The model can then generate piano music of high signal-level quality; however the temporal structure of the generated sequences can be further improved. AudioLM tackles both challenges of long-term coherence and high-quality by combining semantic and acoustic tokens in a generative framework. This leads to improvements over GSLM by generating speech continuations that preserve the original speaker's identity and intonation, as well as extending audio continuation beyond speech by generating piano sequences with high-level coherence.  <ref type="bibr" target="#b15">[16]</ref> and enable high-quality audio synthesis. The semantic tokens are derived from representations produced by an intermediate layer of w2v-BERT <ref type="bibr" target="#b16">[17]</ref> and enable long-term structural coherence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. MODEL</head><p>In this section, we first describe the components of our framework, together with the representation and modeling challenges in audio generation. We address these challenges by proposing a hybrid tokenization scheme together with a multi-stage Transformer-based language model operating on the proposed tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Components</head><p>We consider a single channel audio sequence x ? R T , which is processed by the following three components of the AudioLM framework:</p><p>? A tokenizer model, which maps x into a sequence y = enc(x), y = (y 1 , . . . , y T ) of discrete tokens from a finite vocabulary, with T T . ? A decoder-only Transformer language model that operates on the discrete tokens y, trained to maximize the likelihood T t=1 p(y t |y &lt;t ). At inference time, the model predicts the token sequence ? autoregressively.</p><p>? A detokenizer model, which maps the sequence of predicted tokens back to audio, producing the waveform x = dec(?). It is important to emphasize the following aspects: i) the number of tokens T is typically 2-3 orders of magnitude smaller than T . This is critical to significantly increase the temporal context size of the language model, since the computational complexity of standard self-attention grows quadratically with respect to the sequence length; ii) the tokenizer and detokenizer are pre-trained and frozen ahead of training the language model, which decouples the tokenizers and the language model and simplifies the training setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Trade-offs of discrete audio representations</head><p>The tokenizer and detokenizer models allow us to operate on discrete audio representations. On the one hand, we want to be able to reconstruct audio waveforms at high quality, which introduces a lower bound on the bitrate and hence on the length of the token sequence. On the other hand, we aim at obtaining a compact representation that captures long-term dependencies. To reconcile these conflicting requirements, we rely on a combination of acoustic and semantic tokens, which are illustrated in Figure <ref type="figure" target="#fig_0">1</ref>. In this tokenization scheme, the semantic tokens enable long-term structural coherence, while modeling the acoustic tokens conditioned on the semantic tokens enables high-quality audio synthesis.</p><p>We compute acoustic tokens using SoundStream <ref type="bibr" target="#b15">[16]</ref>, a state-of-the-art neural audio codec, which significantly outperforms non-neural codecs like Opus and EVS at low bitrates. SoundStream adopts a convolutional encoder to map the input waveform to a sequence of embeddings, whose sampling rate is significantly lower than the sampling rate of the original audio. We configure SoundStream to produce embeddings at 50 Hz (one every 20 ms) for input waveforms at 16 kHz. This is a 16000 / 50 = 320-fold reduction in the sampling rate. Each embedding is discretized using a residual vector quantizer (RVQ), which consists of a hierarchy of Q vector quantizers, each using a vocabulary of N symbols. For example, using N = 1024, Q = 4 results in a bitrate of 2000 bps. Hence, the input audio samples x are represented by a matrix Y ? {1, . . . , N } T A ?Q of codebook symbols, with T A = T /320. Then, the convolutional decoder of SoundStream maps this discrete representation to real-valued embeddings and then reconstructs the waveform. The codec achieves high quality by being trained end-to-end with a combination of reconstruction and adversarial losses.</p><p>We compute semantic tokens using w2v-BERT <ref type="bibr" target="#b16">[17]</ref>, a recently proposed model for learning self-supervised audio representations. When trained on large speech corpora, w2v-BERT learns to map the input audio waveform to a rich set of linguistic features. This is achieved by training a 0.6Bparameter Conformer-based model <ref type="bibr" target="#b46">[47]</ref> using a combination of two self-supervised objectives: a masked language modeling (MLM) loss and a contrastive loss. While this model can be fine-tuned for discriminative tasks such as speech recognition or speech-to-text translation <ref type="bibr" target="#b47">[48]</ref>, AudioLM rather leverages the representations of the pre-trained w2v-BERT to model longterm temporal structure in a generative framework. To this end, we select an intermediate layer of the MLM module of w2v-BERT and compute embeddings at this level. We train a k-means with K clusters on these embeddings and use the centroid indices as semantic tokens. We found that normalizing w2v-BERT embeddings such that each dimension has zero mean and unit variance before clustering significantly improves their phonetic discriminability. w2v-BERT performs downsampling along the temporal dimension, so that realvalued 1024-dimensional feature vectors are computed at a sampling rate of 25 Hz (one every 40 ms). Hence, the input audio samples x are transformed into a sequence of semantic tokens z = (z 1 , . . . , z T S ) ? {1, . . . , K} T S with T S = T /640. For example, when T = 16000, K = 1024, this results in a bitrate equal to 250 bps. We note that our proposal for the extraction of semantic tokens from w2v-BERT resembles the token extraction from HuBERT in prior works <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b44">45]</ref>.</p><p>To motivate our hybrid tokenization scheme, we contrast the different properties of the acoustic tokens obtained from SoundStream, and the semantic tokens obtained from w2v-BERT, by comparing them in terms of audio quality reconstruction and phonetic discriminability. We evaluate the reconstruction quality by training a SoundStream decoder to reconstruct audio from tokens. We then compute the ViSQOL score <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b49">50]</ref>, a computational proxy for perceived similarity between a reference audio and its reconstruction. In particular, we use the "speech" mode, which operates on 16 kHz signals.</p><p>We measure phonetic discriminability in terms of ABX error rate <ref type="bibr" target="#b50">[51]</ref>. It is a distance-based metric that considers a set of phoneme trigrams which only differ in the central phoneme (e.g., "bit" vs. "bet"). ABX error rate measures how often a random instance X of a trigram ("bit") is closer to an instance B of another trigram ("bet") rather than to a different instance A of the same trigram ("bit"). We consider cases where all three sounds A, B, and X are uttered by the same speaker (withinspeaker) and where X is coming from a different speaker (across-speaker) <ref type="bibr" target="#b51">[52]</ref>. To allow uniform comparison across the two representations, we represent speech using residual vector-quantized embeddings, where each frame is represented by its corresponding centroid for w2v-BERT or by the output of a SoundStream quantizer. We calculate ABX using scripts published with the Libri-Light dataset <ref type="bibr" target="#b52">[53]</ref> with the default settings and report scores obtained on LibriSpeech dev-clean <ref type="bibr" target="#b53">[54]</ref>. Table <ref type="table" target="#tab_0">I</ref> shows that acoustic tokens provide a good reconstruction quality (ViSQOL of 3.3 for 2000 bps, 3.9 for 6000 bps), but poor phonetic discriminability. Conversely, semantic tokens extracted from the 7th layer from the MLM module of w2v-BERT significantly improve phonetic discriminability, but they do not attain high reconstruction quality, even when matching the bitrate of the acoustic tokens.</p><p>Consequently, achieving both high quality and long-term consistency with only one of the tokenizers is challenging. To illustrate this point further, we can model the sequences of one of the token types and inspect the properties of the resulting model. We perform this on the acoustic tokens, since the semantic tokens only allow for poor audio synthesis. We train a decoder-only Transformer on the sequence of acoustic tokens, by flattening Y in a row-major order to a sequence of tokens y+o of length</p><formula xml:id="formula_0">T A ?Q, where y = (y 1 1 , y 2 1 , . . . , y Q 1 , y 1 2 , . . . , y Q T A ), y q</formula><p>t is the token produced by the q-th quantizer for the t-th time step, and o = (o 1 , . . . , o T A ?Q ) is the vector of offsets for creating unique token indices for the Q layers of the residual vector quantizer, with o i = (i -1 mod Q) ? N . In the following, we omit the offsets from the notation and assume proper offsetting implicitly. Using the model trained only on the acoustic tokens, we sample speech continuations from a prompt of 4 seconds. While both the recording conditions and the speaker identity from the prompt are preserved, the linguistic content is inconsistent, and often akin to babbling (see "Generation without semantic tokens" in the accompanying material 1 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Hierarchical modeling of semantic and acoustic tokens</head><p>The observations in the previous section suggest that, by modeling both semantic and acoustic tokens within the same framework, the semantic tokens would ensure longterm consistency (by capturing linguistic content for speech, melody and rhythm for music), while the acoustic tokens would ensure high-quality audio synthesis (by capturing the acoustic details). We build the AudioLM framework on this hypothesis. Concretely, we adopt a hierarchical approach, by first modeling the semantic tokens for the entire sequence, and then use these as conditioning to predict the acoustic tokens. This approach has two main advantages: i) the hierarchical modeling reflects the conditional independence assumption that semantic tokens are expected to be conditionally independent from past acoustic tokens given past semantic tokens, that is, p(z t |z &lt;t , y &lt;t ) ? p(z t |z &lt;t ); ii) the token sequence per stage is reduced compared to alternatives such as modeling the interleaved sequence of semantic and acoustic tokens, allowing for computationally more efficient training and inference.</p><p>AudioLM performs three subsequent stages, as illustrated in Figure <ref type="figure">2</ref>. In all stages, we use a separate decoder-only Transformer trained for predicting next tokens given all previous ground-truth tokens in the corresponding stage. Semantic modeling. The first stage models p(z t |z &lt;t ), the autoregressive prediction of semantic tokens to capture longterm temporal structure. Coarse acoustic modeling. The second stage proceeds analogously on the acoustic tokens, but it only predicts the acoustic tokens from the coarse Q SoundStream quantizers, conditioned on the semantic tokens. Due to residual quantization in SoundStream, the acoustic tokens have a hierarchical structure: tokens from the coarse quantizers recover acoustic properties like speaker identity and recording conditions, while leaving only the fine acoustic details to the fine quantizer tokens, which are modeled by the next stage. We rely on the simple approach of flattening the acoustic tokens in a row-major order to handle their hierarchical structure. Consequently, the second stage models p(y q t |z, y ?Q &lt;t , y &lt;q t ), for q ? Q , where the corresponding token sequence is (z 1 , . . . , z T S , y 1  1 , y 2 1 , . . . , y Q 1 , y 1 2 , . . . , y Q T A ), with y 1 1 being the first token predicted during training. Fine acoustic modeling. The third stage operates on acoustic tokens corresponding to the fine quantizers, using the Q coarse tokens as conditioning and modeling the conditional probability distribution p(y q t |y ?Q , y &gt;Q &lt;t , y &lt;q t ) for q &gt; Q . That is, y q t is predicted based on all tokens corresponding to the coarse Q quantizers, followed by the fine Q -Q quantizers at previous time steps, together with the already decoded tokens at the Fig. <ref type="figure">2</ref>. The three stages of the hierarchical modeling of semantic and acoustic tokens in AudioLM: i) semantic modeling for long-term structural coherence, ii) coarse acoustic modeling conditioned on the semantic tokens and iii) fine acoustic modeling. With the default configuration, for every semantic token there are 2Q acoustic tokens in the second stage and 2(Q -Q ) tokens in the third stage. The factor of 2 comes from the fact that the sampling rate of SoundStream embeddings is twice as that of the w2v-BERT embeddings. current time step corresponding to the coarser quantizers. In this stage, we further improve audio quality, removing the lossy compression artifacts that remain after the second stage.</p><p>Although the second and third stage could be merged into a single stage, we adopt the solution with two separate stages to limit the sequence length that the model has to process at once. First, considering that fine acoustic tokens are conditionally independent from semantic tokens when conditioned on coarse acoustic tokens, the third stage can ignore the semantic tokens, which reduces the total sequence length. Moreover, under the assumption that the fine acoustic details are determined locally by the coarse acoustic tokens, we perform the third stage on batches of non-overlapping audio chunks of 3 seconds, allowing us to scale this stage independently of the target audio sequence length as well as to use more residual quantization layers Q to achieve higher quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Inference</head><p>After training, we can generate audio with AudioLM as detailed below. Depending on the conditioning signal used, we obtain different forms of generation. Unconditional generation. In this setting, we sample unconditionally all semantic tokens ?, which we then use as conditioning for acoustic modeling. The samples in the accompanying material 1 show that the model generates diverse, syntactically and semantically consistent linguistic content, with varying speaker identity, prosody, acoustic conditions. Section IV-E furthermore validates quantitatively the lexical and syntactic knowledge of the model. Acoustic generation. In this setting, we use the ground-truth semantic tokens z extracted from a test sequence x as conditioning to generate the acoustic tokens. Sections IV-C and IV-D show that, in this case, the generated audio sequences still vary in speaker identity but the content of the spoken sentence remains the same, matching the ground-truth transcript of x. This shows that the semantic tokens capture the semantic content. Generating continuations. Our main application of interest is generating continuations from a short prompt x. To do so, we first map the prompt to the corresponding semantic tokens z ?ts and to the coarse acoustic tokens y ?Q ?ta . The first stage generates ?&gt;ts , the continuation of semantic tokens autoregressively based on the conditioning z ?ts . In the second stage, we concatenate the entire semantic token sequence (z ?ts , ?&gt;ts ) along with the coarse acoustic tokens of the prompt y ?Q ?ta and feed it as conditioning to the coarse acoustic model, which then samples the continuations of the corresponding acoustic tokens. In the third stage, we process the coarse acoustic tokens with the fine acoustic model. Finally, we feed both the prompt and the sampled acoustic tokens to the SoundStream decoder to reconstruct a waveform x. Section IV-F shows that, when prompted with only 3 seconds of speech from an unseen speaker, AudioLM generates continuations that are hardly distinguishable from the original voice. Moreover, Section IV-I demonstrates the performance of AudioLM beyond speech, by continuing piano performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In order to showcase the general applicability of the AudioLM framework, we consider two tasks from different audio domains:</p><p>? Speech continuation, where the model is expected to keep the speaker identity, prosody and recording conditions of the prompt and produce new content, which is syntactically correct and semantically consistent. ? Piano continuation, where the model is expected to generate piano music, which is coherent with the prompt in terms of melody, harmony and rhythm. As the speech and piano prompts we use for evaluation are respectively from unseen speakers and unseen performances, generating consistent continuations requires AudioLM to generalize beyond training data. We furthermore conduct experiments that provide empirical support for our hierarchical approach and shed light on the properties of the semantic and acoustic tokens. In order to mitigate the potential misuse of our framework, we provide an effective method for detecting speech generated by AudioLM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>For speech, all components of AudioLM (SoundStream, w2v-BERT, the k-means quantizer for w2v-BERT embeddings, the decoder-only Transformers) are trained on the unlab-60k train split of Libri-Light <ref type="bibr" target="#b52">[53]</ref>, consisting of 60k hours of English speech. While previous works <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b44">45]</ref> use the 6k-hour clean subset <ref type="bibr" target="#b54">[55]</ref> of Libri-Light for training the language model, AudioLM shows strong performance when trained on the more diverse and noisy unlab-60k subset. The increased robustness to the quality of the training data reduces the data preparation effort needed to apply our framework. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Model selection, training and inference</head><p>Semantic tokens. We use w2v-BERT XL <ref type="bibr" target="#b16">[17]</ref> (0.6B parameters) and adopt a set of heuristics for choosing the intermediate layer to quantize and the number of k-means clusters K. Namely, we inspect ABX, sWUGGY and sBLIMP scores (see Sections III-B and IV-E) computed for different layers of the MLM module of w2v-BERT (on LibriSpeech dev-clean, scaled embeddings), as illustrated in Figure <ref type="figure" target="#fig_2">3</ref>. In addition, we performed a small subjective evaluation test by listening to a few continuations produced by the different choices. We identified the 7th layer in the MLM module of w2v-BERT XL and K = 1024 clusters as our best candidate.</p><p>Acoustic tokens. We train a SoundStream codec with 12 residual vector quantizer layers with a codebook size of 1024 per layer and 4 convolutional blocks having strides <ref type="bibr" target="#b1">(2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b7">8)</ref>. This results in embeddings sampled at 50 Hz for 16 kHz inputs and a bitrate of 6000 bps (with 600 tokens per second). As shown in Table <ref type="table" target="#tab_0">I</ref>, this model achieves very good reconstruction quality as measured by ViSQOL given the low bitrate. To split the 12 levels of quantization between coarse and fine, we set Q = 4 such that we predict the flattened tokens corresponding to the coarse 4 layers in the second stage, whereas the third stage models the fine 8 layers. Hence, the third stage increases the audio bitrate from 2000 bps to 6000 bps, which, as shown in Table <ref type="table" target="#tab_0">I</ref>, improves the audio quality significantly.</p><p>Model. We use identical decoder-only Transformers in all stages, with 12 layers, 16 attention heads, embedding dimension of 1024, feed-forward layer dimension of 4096 and dropout of 0.1, together with T5-style relative positional embeddings <ref type="bibr" target="#b37">[38]</ref>, resulting in a model parameter size of 0.3B per stage. During training, we use random cropping to equivalent input lengths of 30, 10 and 3 seconds for the three stages. Furthermore, in the first two stages, we follow the previously proposed practice of removing consecutive repetitions of the semantic tokens <ref type="bibr" target="#b13">[14]</ref>.</p><p>Inference. We use temperature sampling in all stages, with temperatures of 0.6, 0.8 and 0.6 for the three stages, respectively. We found that these temperature values provide a good tradeoff between diversity and semantic consistency of the generated speech. For speech continuation, we use prompts of 3 seconds. For generating the prompts, we truncate samples to the desired prompt length, extract the corresponding w2v-BERT and SoundStream tokens and use them as conditioning as described in Section III-D. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Information represented by the semantic tokens</head><p>We investigate the information represented by the different types of tokens to motivate the proposed hierarchical approach based on the separation of semantic and acoustic tokens. In particular, we design experiments for testing the hypothesis that, when modeling speech, the linguistic content is mostly captured by the semantic tokens, while speaker identity and recording conditions are captured by the acoustic tokens.</p><p>In the first experiment, we sample the acoustic tokens based on ground-truth semantic tokens extracted from the original speech samples, which corresponds to the "Acoustic generation" setup described in Section III-D. This is done by running the second and third stages, then comparing the linguistic content of the sampled speech to the original speech. Under the hypothesis that the linguistic content is mostly captured by the semantic tokens, the lexical semantics of the sampled and the original speech should coincide. To test this, we perform ASR using a Conformer Transducer-L <ref type="bibr" target="#b55">[56]</ref> on the generated audio, and calculate the word error rate (WER) and the character error rate (CER) with respect to the original transcripts provided with the data as reference. We use samples from LibriSpeech test-clean with length between 4 and 10 seconds (thus retaining 2.2 hours out of 5.4) and repeat the acoustic generation three times for each sample. For comparison, we also evaluate WER and CER on the same set of samples using the unit-to-speech synthesis module of GSLM <ref type="bibr" target="#b13">[14]</ref> as provided by textless-lib <ref type="bibr" target="#b45">[46]</ref>. For GSLM resynthesis, we use a vocabulary of 200 tokens derived from HuBERT representations <ref type="bibr" target="#b56">[57]</ref> which was found to provide the lowest resynthesis error <ref type="bibr" target="#b13">[14]</ref>.</p><p>Table <ref type="table" target="#tab_1">II</ref> shows the results, where the low WER and CER achieved by AudioLM provide two important insights. First, we can conclude that the semantic content is fully captured by the semantic tokens, as the transcripts obtained from the output of acoustic generation closely follow the original transcripts. Second, the acoustic generation based on sampling SoundStream tokens and decoding them to audio samples preserves good transcription quality. Inspecting the generated samples, we observe that the primary source of errors is the synthesis of proper nouns. A secondary source of errors is the end-of-sentence tokens not being generated at the proper position. Furthermore, since the acoustic generation can synthesize different recording environments, the resulting samples might contain background noise, which also degrades the performance of ASR. Table II also shows that AudioLM performs similarly to GSLM. However, GSLM is trained to synthesize only a single voice in a clean recording environment. Finally, we observe that the error rates of the SoundStream reconstruction are comparable to those of the original audio, suggesting that most of the errors are coming from the mapping of semantic to acoustic tokens. Samples of the acoustic generation are available in the accompanying material. 1   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Information represented by the acoustic tokens</head><p>In the second experiment, we verify the hypothesis that speaker identity and recording conditions are captured by the acoustic tokens. Qualitatively, one can listen to the samples produced by the previous experiment and observe that repeating the sampling of acoustic tokens conditioned on the same semantic tokens results in a wide variety of speakers and recording conditions.</p><p>In order to perform a quantitative assessment of this observation, we design the following experiment. We train a convolutional network for speaker classification inspired by <ref type="bibr" target="#b57">[58]</ref>, which operates on the log-mel spectrogram of the inputs (25 ms window length, 10 ms hop length, 64 mel bins), cropped to 1 second. The network is composed of six convolution blocks, using convolutions along the time and the frequency axes with 3?1 and 1?3 kernels, followed by ReLU and batch normalization. The number of channels used by each block increases with depth and is equal to <ref type="bibr" target="#b63">[64,</ref><ref type="bibr">128,</ref><ref type="bibr">256,</ref><ref type="bibr">256,</ref><ref type="bibr">512,</ref><ref type="bibr">512]</ref>. Whenever the number of channels is increased, max pooling with a stride of 2 is also applied along both time and frequency axes. To perform speaker classification on a sequence longer than 1 second, at inference time, we run the classifier on overlapping windows of 1 second with 250 ms hop length and aggregate the predictions. We train this model on the union of LibriSpeech train-clean-100 and test-clean using the original uncompressed samples, resulting in 291 speakers in total. Then, we randomly split the dataset, with 90% used for training and 10% for evaluation. The classifier achieves almost perfect accuracy on the evaluation split of the dataset, and Table <ref type="table" target="#tab_1">III</ref> shows that it is also robust to lossy compression introduced by SoundStream.</p><p>To verify that acoustic generation synthesizes different speakers given the same semantic tokens, we run the speaker classifier on the samples generated by AudioLM in that setting. Table <ref type="table" target="#tab_1">III</ref> shows that, while higher than chance (3.2% compared to 100 / 291 = 0.3%), the speaker classification accuracy remains low in this case. We can conclude that the semantic tokens carry little information about the speaker identity, which is instead mostly determined by the acoustic tokens. Furthermore, based on a subjective assessment done by comparing the synthesized samples generated from the same semantic tokens, we observe that rhythm and intonation have only slight variations across different samples, suggesting that prosodic features are captured mostly by the semantic tokens, with some contribution from the acoustic tokens. In addition, we notice a large diversity in the sampled recording conditions, an indication that this characteristic is mainly represented by the acoustic tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Probing the linguistic knowledge of AudioLM</head><p>The previous section shows the linguistic content is captured mostly by modeling of the semantic tokens in the first stage. We now conduct a series of probing experiments to assess the degree of lexical and syntactic knowledge acquired by language modeling of the semantic tokens. We use two zero-shot metrics, sWUGGY and sBLIMP, introduced in the ZeroResource Challenge 2021 <ref type="bibr" target="#b12">[13]</ref>. The sWUGGY metric measures whether in a pair of a similar-sounding word and a non-word (e.g., "brick" and "blick"), the model gives a higher probability to the word. In turn, sBLIMP measures how often, according to the model, a grammatically correct sentence has a higher probability than a similar incorrect one (e.g., "the dogs sleep" vs. "the dog sleep").</p><p>We use the development datasets provided by the organizers of the ZeroResource Challenge 2021, containing 10,000 and 6,300 pairs for the sWUGGY and sBLIMP metrics, respectively, each synthesized using four voices. Following the leaderboard of the challenge, we separately consider the case where the sWUGGY pairs are pre-filtered to contain words that occur in the LibriSpeech data (referred to as the "in-vocab" subset). For both metrics, we identify the positive sample in a pair as the sequence with the higher log-likelihood according to the model. However, positive examples in the sBLIMP data are on average shorter than their negative counterparts, which can implicitly bias scores towards higher success rates. Thus, we normalize the log-likelihood returned by the model by the sequence length in all experiments.</p><p>We compare the performance of AudioLM to the results reported in the ZeroResource Challenge 2021 leaderboard <ref type="foot" target="#foot_0">2</ref> and in the literature. First, we include two text-based toplines which correspond to a BERT model trained on ground-truth phonetic transcriptions, with and without forced alignment, along with a baseline BERT model trained on CPC-derived tokens <ref type="bibr" target="#b27">[28]</ref>. Next, we include the leaderboard entry "Tu Anh et al." that corresponds to HuBERT-only model, without an additional language modeling component, which attained the highest sWUGGY and sBLIMP scores in the leaderboard (reported in <ref type="bibr" target="#b58">[59]</ref>). Apart from the baseline, the second-best entry in terms of sWUGGY and sBLIMP is that of Harwath et al. <ref type="bibr" target="#b59">[60]</ref> which is based on a RoBERTA <ref type="bibr" target="#b60">[61]</ref> model trained on top of speech representations obtained with visual grounding. We also add an improved CPC-BERT model by Nguyen et al. <ref type="bibr" target="#b58">[59]</ref>.</p><p>Unlike AudioLM, the aforementioned models are not causal, so they are not well suited for speech generation. Hence, we also consider causal baselines. Firstly, we include a variant of GSLM that achieves the best sWUGGY and sBLIMP scores reported in <ref type="bibr" target="#b13">[14]</ref>. This model is a decoder-only Transformer language model trained using quantized HuBERT representations <ref type="bibr" target="#b32">[33]</ref>. Next, we include the entry of van Niekerk et al. <ref type="bibr" target="#b61">[62]</ref>, obtaining the highest scores in the challenge among causal models with an LSTM model trained on CPC-based speech tokens. We report the results in Table <ref type="table" target="#tab_2">IV</ref>. Compared to other systems without text supervision, AudioLM achieves the highest sWUGGY scores across both splits. Similarly, it also attains the highest score in the sBLIMP metric, improving by 8% relative over the previous state-of-the-art (CPC-BERT <ref type="bibr" target="#b58">[59]</ref>). AudioLM even outperforms a supervised topline using forced aligned phonetic transcriptions. <ref type="foot" target="#foot_1">3</ref> Overall, our method demonstrates a high ability to model linguistic content without any textual supervision. In particular, it significantly improves over previous work in terms of lexical and syntactic judgement quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Generating coherent continuations</head><p>The experiments in Section IV-E show the capacity of AudioLM to model semantically and syntactically correct linguistic content. For generating convincing continuations, however, we also need coherent acoustic generation. One hallmark of our framework is its ability to continue short prompts of only 3 seconds coherently.</p><p>To validate the acoustic consistency at the level of speaker identity, we reuse our speaker classifier from Section IV-D and check whether the same speaker is detected in the prompt as in the generated continuation. Concretely, we generate three continuations of 7 seconds for each 3-second prompt, where the prompts are obtained by cropping samples from Librispeech test-clean, whose length is between 4 and 10 seconds. Then, we run the speaker classifier on the sampled continuations (excluding the prompts). The last column of Table <ref type="table" target="#tab_1">III</ref> shows that the speaker classification accuracy is higher than 92%, demonstrating that AudioLM generates continuations that strongly preserve the speaker identity. Thus, while semantic tokens carry very little speaker information, prompting AudioLM with both semantic and acoustic tokens allows preserving the speaker identity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Subjective evaluation</head><p>We further validate the result from the previous section by means of a subjective evaluation based on the following task. Raters are asked to listen to a sample of exactly 10 seconds and decide whether it is an original recording of human speech or a synthetic continuation generated by our framework. We use 100 samples in total selected from LibriSpeech test-clean, chosen at random from those with a length of at least 10 seconds, so that we can truncate the length to be exactly 10 seconds without introducing any padding. Half of these samples are compressed with SoundStream; from the remaining half, we extract prompts of 3 seconds from the beginning of the samples and generate the corresponding continuations of exactly 7 seconds (resulting in samples of 10 seconds after concatenating with the prompts). We rely on 10 raters screened for proficiency in English and instruct them that the first 3 seconds in each sample is original human speech, and thus their decision should be based on the segment following the first 3 seconds. This subjective evaluation task tests at the same time multiple desirable properties: i) the semantic and syntactic correctness of the generated linguistic content; ii) the acoustic coherence of the continuation in context of the prompt (speaker identity, prosody, recording conditions) and iii) the absence of generation artifacts. Based on the 1000 ratings collected, we find that the rate of success for assigning the correct label (original vs. synthesized) is 51.2%, which, according to a binomial test, is not statistically significantly different (p = 0.23) from assigning labels uniformly at random (50% success rate). Since human raters struggle to differentiate short speech samples synthesized by AudioLM from real speech samples in an unpaired setup, the responsible model development practices call for addressing this aspect systematically, which we pursue in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Detecting synthesized speech</head><p>We acknowledge that speech generation capabilities of AudioLM carry potential risks, which we further elaborate in Section VI. As a way of mitigating such risks, we accompany our framework with a method for detecting whether a speech sequence was synthesized by AudioLM. To this end, we train a convolutional network with the same architecture as the one described in Section IV-D, but for the binary classification task of differentiating between original samples and continuations generated by AudioLM (excluding the prompt). More precisely, we compare continuations to original samples compressed through SoundStream rather than uncompressed audio, since otherwise i) the task is trivial (the model quickly converges to 100% accuracy) and ii) eventual compression artifacts would become a confounding factor that would prevent evaluating the generative abilities of AudioLM. For training, we extract the original samples and prompts from LibriSpeech train-clean-100. We train on crops of 1 seconds, and compute predictions on longer sequences with the same approach as described in Section IV-D. On a balanced evaluation set, this model achieves 98.6% accuracy. This shows that despite being (almost) indistinguishable to human ears as shown by the subjective evaluation presented in Section IV-G, continuations generated by AudioLM are very easy to detect with a simple audio classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Piano continuation</head><p>We demonstrate how our approach extends beyond speech by generating coherent piano music continuations. For this, we retrain all components of AudioLM on an internal dataset of 40k hours of piano music that includes players from beginner to expert level, and exhibits a wide range of different acoustic conditions, with content ranging from piano scale exercises to famous pieces. The model hyperparameters are identical to the speech continuation setup, except for the acoustic generation stage: we found that a codec with 3 layers of quantization and a larger codebook size of 2 14 per layer already provides high reconstruction quality, so the experiments on piano continuation ignore the third stage and directly predict the 3 levels of acoustic tokens in the second stage. At inference, we extract a 4-second prompt from the Maestro dataset <ref type="bibr" target="#b62">[63]</ref>. The accompanying material 1 shows side-by-side comparisons of generations based on acoustic tokens only, or using the full AudioLM framework. While both are of equally high audio quality, analogously to the speech continuation experiments, only the latter display consistent melody and temporal structure. To substantiate this observation, we conduct a subjective evaluation test with 10 raters, in which the raters are asked to express their preference between 15 pairs of continuations (20 seconds each) generated using a model trained on only acoustic tokens and AudioLM, using the same prompt for each pair. The raters preferred the samples produced by AudioLM in 83.3% of the pairs. This shows that the hierarchical modeling of AudioLM, from semantic to acoustic tokens, not only benefits speech generation by separating linguistic content from speaker identity, but more generally improves audio generation by explicitly disentangling the long-term structure and local acoustic details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>We introduce AudioLM, a framework for audio generation that provides both long-term coherence and high audio quality. Relying on a hybrid tokenization scheme of semantic and acoustic tokens, AudioLM performs autoregressive prediction by cascading three stages of language modeling that hierarchically generate audio from the coarsest semantic level up to the finest acoustic details. Experiments on speech generation show that not only AudioLM can generate syntactically and semantically coherent speech without any text, but also that continuations produced by our model are almost indistinguishable from real speech by humans. We alleviate the risks associated to these realistic continuations by training a classifier which recognizes speech generated by our method with very high accuracy. Furthermore, we show that AudioLM can generate high-quality piano continuations, demonstrating the benefits of our framework for audio generation beyond speech. This encourages the future extensions to other types of audio (e.g., multilingual speech, polyphonic music, and audio events) as well as integrating AudioLM into an encoder-decoder framework for conditioned tasks such as text-to-speech or speech-to-speech translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. BROADER IMPACT</head><p>The ability of AudioLM to synthesize high-quality audio with long-term coherent structure unlocks use-cases ranging from helping people with speech impediments to assisting in composing music. However, there are several risks associated with our model. When modeling speech, AudioLM inherits all concerns about language models for text, such as reflecting the societal biases in the underlying data -we refer to <ref type="bibr" target="#b9">[10]</ref> for a detailed discussion on the ethical considerations for text-based language models. Furthermore, the generated speech continuations might not be consistent with the prompt in terms of accent and dialect for underrepresented groups in the training data. The ability to continue short speech segments while maintaining speaker identity and prosody can potentially lead to malicious use-cases such as spoofing biometric identification <ref type="bibr" target="#b63">[64]</ref> or impersonating a specific speaker <ref type="bibr" target="#b64">[65]</ref>. Therefore, following the responsible AI practices, it is of paramount importance to design mechanisms that safeguard against the misuse of AudioLM. As an important step towards this direction, in Section IV-H we provide a model for accurately detecting audio synthesized by AudioLM.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. Overview of the tokenizers used in AudioLM. The acoustic tokens are produced by SoundStream<ref type="bibr" target="#b15">[16]</ref> and enable high-quality audio synthesis. The semantic tokens are derived from representations produced by an intermediate layer of w2v-BERT<ref type="bibr" target="#b16">[17]</ref> and enable long-term structural coherence.</figDesc><graphic url="image-1.png" coords="3,51.12,68.03,95.29,86.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(from layers 1:Q' of the RVQ) Coarse acoustic tokens (from layers 1:Q' of the RVQ) Fine acoustic tokens (from layers Q'+1:Q of the RVQ)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Left: ABX (?) scores achieved by the (unquantized) embeddings extracted from different layers of the MLM module of w2v-BERT. Right: Scores on the development sets of sWUGGY (?) and sBLIMP (?) obtained with different numbers of k-means cluster centers for layer 7.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I COMPARISON</head><label>I</label><figDesc>OF TOKEN TYPES IN TERMS OF PHONETIC DISCRIMINABILITY WITHIN AND ACROSS SPEAKERS (LOWER IS BETTER) AND RECONSTRUCTION QUALITY (HIGHER IS BETTER). PHONETIC DISCRIMINABILITY IS MEASURED BY ABX, WHILE RECONSTRUCTION QUALITY IS REPORTED IN VISQOL UNITS.</figDesc><table><row><cell>Tokenization</cell><cell>Bitrate</cell><cell cols="2">Phonetic discriminability within/across (?) Reconstruction quality (?)</cell></row><row><cell>Semantic (w2v-BERT)</cell><cell>250 bps 6000 bps</cell><cell>6.7 / 7.6 5.6 / 6.2</cell><cell>1.1 1.4</cell></row><row><cell>Acoustic (SoundStream)</cell><cell>2000 bps 6000 bps</cell><cell>22.4 / 28.7 17.8 / 26.6</cell><cell>3.3 3.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II CHARACTER</head><label>II</label><figDesc>(CER) AND WORD (WER) ERROR RATES OF THE ASR SYSTEM ON AUDIO GENERATED BY AUDIOLM FROM GROUND-TRUTH SEMANTIC TOKENS, OR BY GSLM<ref type="bibr" target="#b13">[14]</ref> ON GROUND-TRUTH DISCRETE UNITS. WE ALSO REPORT THE ERROR RATES ON THE GROUND-TRUTH AUDIO AND ITS SOUNDSTREAM RECONSTRUCTION FOR REFERENCE.</figDesc><table><row><cell></cell><cell>Original</cell><cell cols="2">Reconstruction with SoundStream</cell><cell cols="2">AudioLM</cell><cell>GSLM [14] unit-to-speech</cell></row><row><cell>CER</cell><cell>0.8</cell><cell></cell><cell>0.9</cell><cell>3.4</cell><cell>2.9</cell></row><row><cell>WER</cell><cell>2.5</cell><cell></cell><cell>2.6</cell><cell>6.0</cell><cell>6.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell>TABLE III</cell><cell></cell></row><row><cell cols="6">SPEAKER CLASSIFICATION ACCURACY (%) ON AUDIO GENERATED FROM</cell></row><row><cell cols="6">GROUND-TRUTH SEMANTIC TOKENS ("ACOUSTIC GENERATION WITH</cell></row><row><cell cols="6">AUDIOLM") AND ON CONTINUATIONS OF A PROMPT ("CONTINUATION</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">WITH AUDIOLM").</cell></row><row><cell></cell><cell>Reconstruction</cell><cell></cell><cell cols="2">Acoustic generation</cell><cell>Continuation</cell></row><row><cell cols="3">with SoundStream</cell><cell cols="2">with AudioLM</cell><cell>with AudioLM</cell></row><row><cell></cell><cell>100.0</cell><cell></cell><cell>3.2</cell><cell></cell><cell>92.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE IV SUCCESS</head><label>IV</label><figDesc>RATE (%) ON THE DEVELOPMENT SETS OF SWUGGY AND SBLIMP. IN BOLD ARE BEST SCORES AMONG MODELS THAT DO NOT HAVE</figDesc><table><row><cell cols="3">TEXT SUPERVISION.</cell><cell></cell></row><row><cell>Model</cell><cell cols="2">sWUGGY (?)</cell><cell>sBLIMP (?)</cell></row><row><cell></cell><cell>all</cell><cell>in-vocab</cell><cell></cell></row><row><cell cols="3">Text-based toplines</cell><cell></cell></row><row><cell>Forced alignment topline [13]</cell><cell>92.2</cell><cell>-</cell><cell>63.7</cell></row><row><cell>Phone topline [13]</cell><cell>97.9</cell><cell>-</cell><cell>66.8</cell></row><row><cell cols="2">Non-causal</cell><cell></cell><cell></cell></row><row><cell>BERT baseline [13]</cell><cell>67.7</cell><cell>75.6</cell><cell>56.1</cell></row><row><cell>HuBERT-only [59]</cell><cell>70.9</cell><cell>79.8</cell><cell>59.5</cell></row><row><cell>Harwath et al. [60]</cell><cell>67.6</cell><cell>75.4</cell><cell>56.7</cell></row><row><cell>CPC-BERT [59]</cell><cell>-</cell><cell>80.0</cell><cell>59.9</cell></row><row><cell cols="2">Causal</cell><cell></cell><cell></cell></row><row><cell>van Niekerk et al. [62]</cell><cell>64.3</cell><cell>72.3</cell><cell>54.0</cell></row><row><cell>GSLM [14]</cell><cell>-</cell><cell>68.7</cell><cell>57.1</cell></row><row><cell>AudioLM</cell><cell>71.5</cell><cell>83.7</cell><cell>64.7</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>https://zerospeech.com/2021/results.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>Without the log-likelihood normalization discussed above, AudioLM achieves a sBLIMP score of 67.5, outperforming the phone topline.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>VII. ACKNOWLEDGEMENTS</head><p>The authors thank <rs type="person">John Hershey</rs> and <rs type="person">Johnny Soraker</rs> for their feedback on this work.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Wavenet: A generative model for raw audio</title>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03499</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Efficient neural audio synthesis</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Noury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Casagrande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lockhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Stimberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning ICML</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="2415" to="2424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">MelGAN: Generative adversarial networks for conditional waveform synthesis</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>De Boissiere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gestin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Z</forename><surname>Teoh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sotelo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>De Brebisson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bae</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">SEANet: A multimodal speech enhancement network</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Misiunas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roblek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Diffwave: A versatile diffusion model for audio synthesis</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Wavegrad: Estimating gradients for waveform generation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Aslanides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Young</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.11446</idno>
		<title level="m">Scaling language models: Methods, analysis &amp; insights from training gopher</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Schuh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tsvyashchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Maynez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Prabhakaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Reif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pope</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gur-Ari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Duke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Michalewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Garc?a</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ippolito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Spiridonov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sepassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Omernick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pellat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lewkowycz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">O</forename><surname>Moreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Polozov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Saeta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>D?az</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Meier-Hellstern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Eck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Fiedel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.02311</idno>
		<title level="m">Palm: Scaling language modeling with pathways</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Vector-quantized image modeling with improved VQGAN</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Baldridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.04627</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Scaling autoregressive models for content-rich text-to-image generation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Baid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K</forename><surname>Ayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Baldridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.10789</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The zero resource speech challenge 2021: Spoken language modelling</title>
		<author>
			<persName><forename type="first">E</forename><surname>Dunbar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hamilakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Seyssel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Roz?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rivi?re</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kharitonov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dupoux</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="1574" to="1578" />
		</imprint>
	</monogr>
	<note>Interspeech. ISCA, 2021</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On generative spoken language modeling from raw audio</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lakhotia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kharitonov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-N</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Adi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bolte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Copet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1336" to="1354" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Soundstream: An end-to-end neural audio codec</title>
		<author>
			<persName><forename type="first">N</forename><surname>Zeghidour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Luebs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Skoglund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tagliasacchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE ACM Trans. Audio Speech Lang. Process</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="495" to="507" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">w2vbert: Combining contrastive learning and masked language modeling for self-supervised speech pre-training</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Automatic Speech Recognition and Understanding Workshop</title>
		<imprint>
			<publisher>ASRU. IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="244" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Scaling up models and data with t5x and seqio</title>
		<author>
			<persName><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Andor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gaffney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hawthorne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lewkowycz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Salcianu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Zee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">B</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tsvyashchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bastings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bulian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kenealy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Garrette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee-Thorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Maitin-Shepard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Fiedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Omernick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Saeta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sepassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Spiridonov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Newlan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gesmundo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.17189</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT)</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Parallel wavenet: Fast high-fidelity speech synthesis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lockhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Cobo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Stimberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Casagrande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grewe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Noury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Walters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Belov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<title level="s">ser. Proceedings of Machine Learning Research</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="3915" to="3923" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">High fidelity speech synthesis with adversarial networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Binkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Casagrande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Cobo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Neural discrete representation learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6306" to="6315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Generating diverse highfidelity images with VQ-VAE-2</title>
		<author>
			<persName><forename type="first">A</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">847</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Soft-to-hard vector quantization for end-to-end learned compression of images and neural networks</title>
		<author>
			<persName><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mentzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cavigelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Benini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00648</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">End-to-end optimized speech coding with deep neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kankanahalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2521" to="2525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Cascaded crossmodule residual learning towards lightweight end-to-end speech coding</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Beack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISCA</title>
		<imprint>
			<biblScope unit="page" from="3396" to="3400" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Harp-net: Hyper-autoencoded reconstruction propagation for scalable neural audio coding</title>
		<author>
			<persName><forename type="first">D</forename><surname>Petermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Beack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.10843</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">wav2vec 2.0: A framework for self-supervised learning of speech representations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.11477</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Contrastive learning of generalpurpose audio representations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Saeed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zeghidour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3875" to="3879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Data augmenting contrastive learning of speech representations in the time domain</title>
		<author>
			<persName><forename type="first">E</forename><surname>Kharitonov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rivi?re</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mazar?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dupoux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Spoken Language Technology Workshop, SLT 2021</title>
		<meeting><address><addrLine>Shenzhen, China</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">January 19-22, 2021. 2021</date>
			<biblScope unit="page" from="215" to="222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">vq-wav2vec: Self-supervised learning of discrete speech representations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Hubert: Self-supervised speech representation learning by masked prediction of hidden units</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bolte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lakhotia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.07447</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Layer-wise analysis of a selfsupervised speech representation model</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pasad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-C</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="914" to="921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Thoppilan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kulshreshtha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghafouri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Menegali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Krivokon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Rusch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pickett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Meier-Hellstern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Doshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Duke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Soraker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zevenbergen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Prabhakaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Olson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Molina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hoffman-John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Aroyo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rajakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Butryna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lamm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kuzmina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fenton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kurzweil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Aguera-Arcas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Croak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.08239</idno>
		<title level="m">Lamda: Language models for dialog applications</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>De Oliveira Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Khlaaf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Power</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bavarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tillet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">P</forename><surname>Such</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cummings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chantzis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Guss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Paino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tezak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Morikawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brundage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Murati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mcgrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.03374</idno>
		<title level="m">Evaluating large language models trained on code</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep learning for symbolic mathematics</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Charton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">67</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Efficient contentbased sparse attention with routing transformers</title>
		<author>
			<persName><forename type="first">A</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saffar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Assoc. Comput. Linguistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="53" to="68" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Rethinking attention with performers</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sarl?s</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Q</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Colwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Weller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">General-purpose, long-context autoregressive modeling with perceiver AR</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hawthorne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jaegle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cangea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Nash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sheahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zeghidour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Engel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML), ser. Proceedings of Machine Learning</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Research</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Jegelka</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Song</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Szepesv?ri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Niu</surname></persName>
		</editor>
		<editor>
			<persName><surname>Sabato</surname></persName>
		</editor>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">162</biblScope>
			<biblScope unit="page" from="8535" to="8558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Taming transformers for highresolution image synthesis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">883</biblScope>
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Long video generation with time-agnostic VQGAN and time-sensitive transformer</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.03638</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Payne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00341</idno>
		<title level="m">Jukebox: A generative model for music</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Text-free prosody-aware generative spoken language modeling</title>
		<author>
			<persName><forename type="first">E</forename><surname>Kharitonov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Adi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Copet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lakhotia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rivi?re</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dupoux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="8666" to="8681" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Kharitonov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Copet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lakhotia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tomasello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Elkahky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dupoux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Adi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.07359</idno>
		<title level="m">textless-lib: a library for textless spoken language processing</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Conformer: Convolution-augmented transformer for speech recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="5036" to="5040" />
		</imprint>
	</monogr>
	<note>Interspeech. ISCA, 2020</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lozhkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rivera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Van Esch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Axelrod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khanuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Riesa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.10752</idno>
		<title level="m">XTREME-S: evaluating cross-lingual speech representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">ViSQOL: an objective speech quality model</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Skoglund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Kokaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Harte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Audio, Speech, and Music Processing</title>
		<imprint>
			<biblScope unit="volume">2015</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">ViSQOL v3: an open source production ready objective speech and audio metric</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">S C</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Skoglund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gureev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>O'gorman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hines</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twelfth International Conference on Quality of Multimedia Experience (QoMEX)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Evaluating speech features with the minimal-pair ABX task: analysis of the classical MFC/PLP pipeline</title>
		<author>
			<persName><forename type="first">T</forename><surname>Schatz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hermansky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dupoux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech. ISCA</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1781" to="1785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Abx-discriminability measures and applications. (mesures de discriminabilit? ABX et applications)</title>
		<author>
			<persName><forename type="first">T</forename><surname>Schatz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<pubPlace>Paris, France</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Pierre and Marie Curie University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Libri-light: A benchmark for ASR with limited or no supervision</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rivi?re</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kharitonov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mazar?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Karadayi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Liptchinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fuegen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Likhomanenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dupoux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7669" to="7673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Librispeech: An ASR corpus based on public domain audio books</title>
		<author>
			<persName><forename type="first">V</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Towards unsupervised learning of speech features in the wild</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rivi?re</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dupoux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Spoken Language Technology Workshop (SLT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="156" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Conformer: Convolution-augmented transformer for speech recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="5036" to="5040" />
		</imprint>
	</monogr>
	<note>Interspeech. ISCA, 2020</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Hubert: How much can a bad teacher benefit ASR pre-training?</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bolte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6533" to="6537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Self-supervised audio representation learning for mobile devices</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gfeller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>De Chaumont Quitry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roblek</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11796</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Are discrete units necessary for spoken language modeling</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sagot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dupoux</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.05936</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Self-supervised representation learning for speech using visual grounding and masked language modeling</title>
		<author>
			<persName><forename type="first">P</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Harwath</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.03543</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized BERT pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Analyzing speaker information in self-supervised models to improve zero-resource speech processing</title>
		<author>
			<persName><forename type="first">B</forename><surname>Van Niekerk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nortje</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Baas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kamper</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="1554" to="1558" />
		</imprint>
	</monogr>
	<note>in Interspeech. ISCA, 2021</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Enabling factorized piano music modeling and generation with the MAESTRO dataset</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hawthorne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stasyuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Eck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Asvspoof 2021: Automatic speaker verification spoofing and countermeasures challenge evaluation plan</title>
		<author>
			<persName><forename type="first">H</forename><surname>Delgado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">W D</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kinnunen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nautsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Patino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sahidullah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Todisco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.00535</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Yourtts: Towards zero-shot multi-speaker TTS and zero-shot voice conversion for everyone</title>
		<author>
			<persName><forename type="first">E</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Shulby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>J?nior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>G?lge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Ponti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML), ser. Proceedings of Machine Learning Research</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title/>
		<author>
			<persName><surname>Pmlr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2709" to="2720" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
