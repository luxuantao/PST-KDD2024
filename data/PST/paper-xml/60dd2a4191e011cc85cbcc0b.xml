<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cascaded Diffusion Models for High Fidelity Image Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-05-30">30 May 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
							<email>jonathanho@google.com</email>
							<affiliation key="aff3">
								<orgName type="department">Equal contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chitwan</forename><surname>Saharia</surname></persName>
							<email>sahariac@google.com</email>
							<affiliation key="aff3">
								<orgName type="department">Equal contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">William</forename><surname>Chan</surname></persName>
							<email>williamchan@google.com</email>
						</author>
						<author>
							<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
							<email>davidfleet@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
							<email>mnorouzi@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
							<email>salimans@google.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<address>
									<addrLine>Model 1 Model 2 Model 3</addrLine>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Cascaded Diffusion Models for High Fidelity Image Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-05-30">30 May 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2106.15282v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We show that cascaded diffusion models are capable of generating high fidelity images on the class-conditional ImageNet generation challenge, without any assistance from auxiliary image classifiers to boost sample quality. A cascaded diffusion model comprises a pipeline of multiple diffusion models that generate images of increasing resolution, beginning with a standard diffusion model at the lowest resolution, followed by one or more super-resolution diffusion models that successively upsample the image and add higher resolution details. We find that the sample quality of a cascading pipeline relies crucially on conditioning augmentation, our proposed method of data augmentation of the lower resolution conditioning inputs to the super-resolution models. Our experiments show that conditioning augmentation prevents compounding error during sampling in a cascaded model, helping us to train cascading pipelines achieving FID scores of 1.48 at 64×64, 3.52 at 128×128 and 4.88 at 256×256 resolutions, outperforming BigGAN-deep. 32×32 64×64 256×256 Class ID = 213 "Irish Setter"</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Diffusion models <ref type="bibr" target="#b23">[24]</ref> have recently been shown to synthesize high quality images and audio <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b26">27]</ref>, which have long been led by other classes of generative models such as autoregressive models, GANs, VAEs, and flows <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b31">32]</ref>. Most previous work on diffusion models demonstrating high quality samples has focused on datasets of modest size, or data with strong conditioning signals. Our goal is to improve the sample quality of diffusion models on difficult high-entropy data. To showcase the capabilities of the original diffusion formalism, we focus on simple, straightforward techniques to improve the sample quality of diffusion models; for example, we avoid using extra image classifiers to boost sample quality metrics <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b17">18]</ref>.</p><p>Our key contribution concerns the use of cascades to improve the class-conditional ImageNet sample quality of diffusion models. Here, cascading refers to a simple technique to model high resolution data by learning a pipeline of separately trained models at multiple resolutions; a base model generates low resolution samples, followed by super-resolution models that upsample low resolution samples into high resolution samples. Sampling from a cascading pipeline occurs sequentially, first sampling from the low resolution base model, followed by sampling from super-resolution models in order of increasing resolution. While any type of generative model could be used in a cascading pipeline [e.g., <ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b17">18]</ref>, here we restrict ourselves to diffusion models <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b23">24]</ref>.</p><p>The simplest and most effective technique we found to improve cascading diffusion pipelines is to apply strong data augmentation to the conditioning input of each super-resolution model. We refer to this technique as conditioning augmentation. In our experiments, conditioning augmentation is crucial for our cascading pipelines to generate high quality samples at the highest resolution. With this approach we attain FID scores on class-conditional ImageNet generation that are better than BigGAN-Deep <ref type="bibr" target="#b1">[2]</ref> at any truncation value. We empirically find that conditioning augmentation is effective because it alleviates compounding error in cascading pipelines due to train-test mismatch, sometimes referred to as exposure bias in the sequence modeling literature <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b16">17]</ref>.</p><p>The key contributions of this paper are as follows:</p><p>• We show that our Cascaded Diffusion Models (CDM) yield high fidelity samples superior to BigGAN. We achieve this with pure generative models that are not combined with any classifier. • We introduce conditioning augmentation for our super-resolution models, and find it critical towards achieving high sample fidelity. We perform an in-depth exploration of augmentation policies, and find Gaussian augmentation to be a key ingredient for low resolution upsampling, and Gaussian blurring for high resolution upsampling.</p><p>Section 2 briefly reviews recent work on diffusion models. Section 3 describes the most effective types of conditioning augmentation that we found for class-conditional ImageNet generation. Section 4 contains our sample quality results and ablations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Diffusion models A diffusion model <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b9">10]</ref> is defined by a forward process that gradually destroys data x 0 ∼ q(x 0 ) over the course of T timesteps:</p><formula xml:id="formula_0">q(x 1:T |x 0 ) = T t=1 q(x t |x t−1 ), q(x t |x t−1 ) = N (x t ; 1 − β t x t−1 , β t I)<label>(1)</label></formula><p>and a parameterized reverse process p θ (x 0 ) = p θ (x 0:T ) dx 1:T , where</p><formula xml:id="formula_1">p θ (x 0:T ) = p(x T ) T t=1 p θ (x t−1 |x t ), p θ (x t−1 |x t ) = N (x t−1 ; µ θ (x t , t), Σ θ (x t , t))<label>(2)</label></formula><p>The forward process hyperparameters β t are set so that x T is approximately standard normal, so p(x T ) is set to a standard normal prior as well. The reverse process is trained to match the joint distribution of the forward process by optimizing the evidence lower bound (ELBO) −L θ (x 0 ) ≤ log p θ (x 0 ):</p><formula xml:id="formula_2">L θ (x 0 ) = E q L T (x 0 ) + t&gt;1 D KL (q(x t−1 |x t , x 0 ) p θ (x t−1 |x t )) − log p θ (x 0 |x 1 )<label>(3)</label></formula><p>where L T (x 0 ) = D KL (q(x T |x 0 ) p(x T )). The forward process posteriors q(x t−1 |x t , x 0 ) and marginals q(x t |x 0 ) are Gaussian, and the KL divergences in the ELBO can be calculated in closed form. Thus it is possible to train the diffusion model by taking stochastic gradient steps on random terms of Eq. ( <ref type="formula" target="#formula_2">3</ref>). As previously suggested <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b15">16]</ref>, we use the reverse process parameterizations:</p><formula xml:id="formula_3">µ θ (x t , t) = 1 √ α t x t − β t √ 1 − ᾱt θ (x t , t)<label>(4)</label></formula><formula xml:id="formula_4">Σ ii θ (x t , t) = exp(log βt + (log β t − log βt )v i θ (x t , t))<label>(5)</label></formula><p>where α t = 1 − β t , ᾱt = t s=1 α s , and βt = 1− ᾱt−1 1− ᾱt β t . Sample quality can be improved, at the cost of log likelihood, by optimizing modified losses instead of the ELBO. We use the simplified loss:</p><formula xml:id="formula_5">L simple (θ) = E x0, ∼N (0,I),t∼U ({1,...,T }) θ ( √ ᾱt x 0 + √ 1 − ᾱt , t) − 2<label>(6)</label></formula><p>which is suitable for the case of non-learned Σ θ . It is a weighted form of the ELBO that resembles denoising score matching over multiple noise scales <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b25">26]</ref>. For the case of learned Σ θ , we employ a hybrid loss <ref type="bibr" target="#b15">[16]</ref> that simultaneously learns µ θ using L simple , and learns Σ θ using the ELBO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conditional diffusion models</head><p>In the conditional generation setting, the data x 0 has an associated conditioning signal c, for example a label in the case of class-conditional generation, or a low resolution image in the case of super-resolution <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b15">16]</ref>. The only modification that needs to be made to the diffusion model is to include c as input to the reverse process:</p><formula xml:id="formula_6">p θ (x 0:T |c) = p(x T ) T t=1 p θ (x t−1 |x t , c), p θ (x t−1 |x t , c) = N (x t−1 ; µ θ (x t , t, c), Σ θ (x t , t, c)) L θ (x 0 |c) = E q L T (x 0 ) + t&gt;1 D KL (q(x t−1 |x t , x 0 ) p θ (x t−1 |x t , c)) − log p θ (x 0 |x 1 , c)<label>(7)</label></formula><p>The data and conditioning signal (x 0 , c) are sampled jointly from the data distribution, now called q(x 0 , c), and the forward process q(x 1:T |x 0 ) remains unchanged.</p><p>Architectures The current best architectures for image diffusion models are U-Nets <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b22">23]</ref>, which are a natural choice to map corrupted data x t to reverse process parameters (µ θ , Σ θ ) that have the same spatial dimensions as x t . Scalar conditioning, such as a class label or a diffusion timestep t, is provided by adding embeddings into intermediate layers of the network <ref type="bibr" target="#b9">[10]</ref>. Lower resolution image conditioning is provided by channelwise concatenation of the low resolution image, processed by bilinear or bicubic upsampling to the desired resolution, with the reverse process input x t <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b15">16]</ref>.</p><p>Cascading pipelines Suppose x 0 is high resolution data and z 0 is its low resolution counterpart. We use the term cascading pipeline to refer to a sequence of generative models. At the low resolution we have a diffusion model p θ (z 0 ), and at the high resolution, a super-resolution diffusion model p θ (x 0 |z 0 ). The cascading pipeline forms a latent variable model for high resolution data; i.e., p θ (x 0 ) = p θ (x 0 |z 0 )p θ (z 0 ) dz 0 . It is straightforward to extend this to more than two resolutions, and it is also straightforward to condition an entire cascading pipeline on class information.</p><p>Cascading pipelines have been shown to be useful with other generative model families <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b17">18]</ref>. A major benefit to training a cascading pipeline over training a standard model at the highest resolution is that most of the modeling capacity can be dedicated to low resolutions, which empirically are the most important for sample quality, and training and sampling at low resolutions tends to be the most computationally efficient. In addition, cascading allows the individual models to be trained independently, and architecture choices can be tuned at each specific resolution for the best performance of the entire pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Conditioning augmentation in cascaded diffusion models</head><p>The most effective technique we found to improve the sample quality of cascading pipelines is to train each super-resolution model using data augmentation on its low resolution input. We refer to this general technique as conditioning augmentation. At a high level, for some super-resolution model p θ (x 0 |z) from a low resolution image z to a high resolution image x 0 , conditioning augmentation refers to applying some form of data augmentation to z. This augmentation can take any form, but what we found most effective at low resolutions is adding Gaussian noise (forward process noise), and for high resolutions, randomly applying Gaussian blur to z.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Blurring augmentation</head><p>One simple instantiation of conditioning augmentation is augmentation of z by blurring. We found this to be most effective for upsampling to images with resolution 128×128 and 256×256. More specifically, we apply a Gaussian filter of size k and sigma σ to obtain z b . We use a filter size of k = (3, 3) and randomly sample σ from a fixed range during training. We perform hyper-parameter search to find the range for σ. During training, we apply this blurring augmentation to 50% of the examples. During inference, no augmentation is applied to low resolution inputs. We explored applying different amounts of blurring augmentations during inference, but did not find it helpful in initial experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Truncated conditioning augmentation</head><p>Here we describe what we call truncated conditioning augmentation, a form of conditioning augmentation that requires a simple modification to the training and architecture of the super-resolution models, but no change to the low resolution model at the initial stage of the cascade. We found this method to be most useful at resolutions smaller than 128×128. Normally, generating a high resolution sample x 0 involves first generating z 0 from the low resolution model p θ (z 0 ), then feeding that result into the super-resolution model p θ (x 0 |z 0 ). In other words, generating a high resolution sample is performed using ancestral sampling from the latent variable model:</p><formula xml:id="formula_7">p θ (x 0 ) = p θ (x 0 |z 0 )p θ (z 0 ) dz 0 = p θ (x 0 |z 0 )p θ (z 0:T ) dz 0:T .<label>(8)</label></formula><p>(For simplicity, we have assumed that the low resolution and super-resolution models both use the same number of timesteps T .) Truncated conditioning augmentation refers to truncating the low resolution reverse process to stop at timestep s &gt; 0, instead of 0; i.e.,</p><formula xml:id="formula_8">p s θ (x 0 ) = p θ (x 0 |z s )p θ (z s ) dz s = p θ (x 0 |z s )p θ (z s:T ) dz s:T .<label>(9)</label></formula><p>The base model is now p θ (z s ) = p θ (z s:T )dz s+1:T , and the super-resolution model is now</p><formula xml:id="formula_9">p θ (x 0 |z s ) = p(x T ) T t=1 p θ (x t−1 |x t , z s )dx 1:T , where p θ (x t−1 |x t , z s ) = N (x t−1 ; µ θ (x t , t, z s , s), Σ θ (x t , t, z s , s))<label>(10)</label></formula><p>The reason truncating the low resolution reverse process is a form of data augmentation is that the training procedure for p θ (x 0 |z s ) involves conditioning on noisy z s ∼ q(z s |z 0 ), which, up to scaling, is z 0 augmented with Gaussian noise. See Appendix A for details.</p><p>We would like to search over multiple values of s to select for the best sample quality. To make this search practical, we avoid retraining models by amortizing a single super-resolution model over uniform random s at training time. Because each possible truncation time corresponds to a distinct super-resolution task, the super-resolution model for µ θ and Σ θ must takes z s as input along with s, and this can be accomplished using a single network with an extra time embedding input for s. We leave the low resolution model training unchanged, because the standard diffusion training procedure already trains with random s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Non-truncated conditioning augmentation</head><p>Another form of conditioning augmentation, which we call non-truncated conditioning augmentation, uses the same model modifications and training procedure as truncated conditioning augmentation (Section 3.2). The only difference is at sampling time. Instead of truncating the low resolution reverse process, in non-truncated conditioning augmentation we always sample z 0 using the full, non-truncated low resolution reverse process; then we corrupt z 0 using the forward process into z s ∼ q(z s |z 0 ) and feed the corrupted z s into the super-resolution model.</p><p>The main advantage of non-truncated conditioning augmentation over truncated conditioning augmentation is a practical one during the search phase over s. In the case of truncated augmentation, if we want to run the super-resolution model over all s in parallel, we must store all low resolution samples z s for all values of s considered. In the case of non-truncated augmentation, we need to store the low resolution samples just once, since sampling z s ∼ q(z s |z 0 ) is computationally inexpensive.</p><p>Truncating low resolution sampling reflects the training procedure more accurately than not truncating, because the super-resolution model is trained on corrupted z 0 sampled from the dataset. Nonetheless, z s and z s should have similar marginal distributions if the low resolution model is trained well enough. Indeed, in Section 4.3, we empirically find that sample quality metrics are similar for both truncated and non-truncated conditioning augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We designed experiments to improve the sample quality metrics of cascaded diffusion models on class-conditional ImageNet generation. Our final results are described in Section 4.1. We begin with improvements on a baseline non-cascaded model at the 64×64 resolution (Section 4.2), then we show that cascading up to 64×64 improves upon our best non-cascaded 64×64 model, but only in conjunction with conditioning augmentation. We also show that truncated and non-truncated conditioning augmentation perform equally well (Section 4.3). Finally, we study random Gaussian blur augmentation to train super-resolution models to resolutions of 128×128 and 256×256 (Section 4.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Main cascading pipeline results</head><p>Table <ref type="table" target="#tab_0">1</ref> reports the main results on the cascaded diffusion model (CDM), for the 64×64, 128×128, and 256×256 ImageNet dataset resolutions, along with baselines. CDM outperforms BigGANdeep in terms of FID score on the image resolutions considered, but GANs perform better in terms of Inception score when their truncation parameter is optimized for Inception score <ref type="bibr" target="#b1">[2]</ref>. We also outperform concurrently released diffusion models that do not use classifier guidance to boost sample quality scores <ref type="bibr" target="#b3">[4]</ref>. See Fig. <ref type="figure" target="#fig_3">4</ref> for a qualitative assessment of sample quality and diversity compared to VQ-VAE-2 <ref type="bibr" target="#b17">[18]</ref> and BigGAN-deep <ref type="bibr" target="#b1">[2]</ref>.</p><p>Our cascading pipelines are structured as a 32×32 base model, a 32×32→64×64 super-resolution model, followed by 64×64→128×128 or 64×64→256×256 super-resolution models. Models at 32×32 and 64×64 resolutions use 4000 diffusion timesteps and architectures similar to DDPM <ref type="bibr" target="#b9">[10]</ref> and Improved DDPM <ref type="bibr" target="#b15">[16]</ref>. Models at 128×128 and 256×256 resolutions have fewer timesteps, determined by post-training hyperparameter search (Section 4.4), and they use architectures similar to SR3 <ref type="bibr" target="#b20">[21]</ref>. All base resolution and super-resolution models are conditioned on class labels. See Appendix B for details.</p><p>Generally, throughout our experiments, we selected models and performed early stopping based on FID score calculated over 10k samples, but all reported FID scores are calculated over 50k samples for comparison with other work <ref type="bibr" target="#b7">[8]</ref>. We additionally report FID scores calculated against validation set statistics, rather than only training set statistics, to give some measure of overfitting. We report Inception scores using the standard practice of generating 50k samples and calculating the mean and standard deviation over 10 splits <ref type="bibr" target="#b21">[22]</ref>. We cropped and resized the ImageNet dataset <ref type="bibr" target="#b19">[20]</ref> in the same manner as BigGAN <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baseline model improvements</head><p>To set a strong baseline for class-conditional ImageNet generation at the 64×64 resolution, we reproduced and improved upon a 4000 timestep non-cascaded 64×64 class-conditional diffusion model from Improved DDPM <ref type="bibr" target="#b15">[16]</ref>. Our reimplementation used dropout and was trained longer than reported in <ref type="bibr" target="#b15">[16]</ref>; we found that adding dropout generally slowed down convergence of FID and Inception scores, but improved their best values over the course of a longer training period. We further improved the training set FID score and Inception score by adding noise to the trained model's samples using the forward process to the 2000 timestep point, then restarting the reverse process from that point. See Table <ref type="table" target="#tab_2">2a</ref> for the resulting sample quality metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Conditioning augmentation experiments up to the 64×64 resolution</head><p>Building on our reimplementation in Section 4.2, we verify in a small scale experiment that cascading improves sample quality at the 64×64 resolution. We train a two-stage cascading pipeline that comprises a 16×16 base model and a 16×16→64×64 super-resolution model. The super-resolution model architecture is identical to the best 64×64 non-cascaded baseline model in Section 4.2, except for the trivial modification of adding in the low resolution image conditioning information by channelwise concatenation at the input (see Section 2).</p><p>See Table <ref type="table" target="#tab_2">2b</ref> for the results of this 16×16→64×64 cascading pipeline. Interestingly, we find that without conditioning augmentation, the cascading pipeline attains lower sample quality than the non-cascaded baseline 64×64 model; the FID score, for example, degrades from 2.35 to 6.02. With sufficient conditioning augmentation, however, the sample quality of the cascading pipeline becomes better than the non-cascaded baseline. We train two super-resolution models with non-truncated conditioning augmentation, one at truncation time s = 101 and another at s = 1001 (we could have amortized both into a single model, but we chose not to do so in this particular experiment to prevent potential model capacity issues from confounding the results). The first model achieves better sample quality than the non-augmented model but is still worse than the non-cascaded baseline. The second model achieves a FID score of 2.13, outperforming the non-cascaded baseline. Conditioning augmentation is therefore crucial to improve sample quality in this particular cascading pipeline.</p><p>To further improve sample quality at the 64×64 resolution, we found it helpful to increase model sizes and to switch to a cascading pipeline starting with a 32×32 base resolution model.  s as an extra time embedding input to the network (Section 2), allowing us to perform a more fine grained search over s without retraining the model.</p><p>Table <ref type="table" target="#tab_3">3a</ref> displays the resulting sample quality scores for both truncated and non-truncated augmentation. The sample quality metrics improve and then degrade non-monotonically as the truncation time is increased. This indicates that moderate amounts of conditioning augmentation are beneficial to sample quality of the cascading pipeline, but too much conditioning augmentation causes the super-resolution model to behave as a non-conditioned model unable to benefit from cascading. For comparison, Table <ref type="table" target="#tab_3">3b</ref> shows sample quality when the super-resolution model is conditioned on ground truth data instead of generated data. Here, sample quality monotonically degrades as truncation time is increased. Conditioning augmentation is therefore useful precisely when conditioning on generated samples, so as a technique it is uniquely suited to cascading pipelines.</p><p>Based on these findings on non-monotonicity of sample quality with respect to truncation time, we conclude that conditioning augmentation works because it alleviates compounding error from a traintest mismatch for the super-resolution model. This occurs when low-resolution model samples are out of distribution compared to the ground truth data on which the super-resolution model is trained.</p><p>A sufficient amount of Gaussian conditioning augmentation prevents the super-resolution model from attempting to upsample erroneous, out-of-distribution details in the low resolution generated samples. In contrast, sample quality degrades monotonically with respect to truncation time when conditioning the super-resolution model on ground truth data, because there is no such train-test mismatch.</p><p>Table <ref type="table" target="#tab_3">3a</ref> additionally shows that truncated and non-truncated conditioning augmentation are approximately equally effective at improving sample quality of the cascading pipeline, albeit at different values of the truncation time parameter. Thus we generally recommend non-truncated augmentation due to its practical benefits described in Section 3.3. While we found Gaussian noise augmentation to be a key ingredient to boost the performance of our cascaded models at low resolutions, our initial experiments with similar augmentations for 128×128 and 256×256 upsampling yielded negative results. Hence, we explore Gaussian blurring augmentation for these resolutions. As mentioned in Section 3.1, we apply the blurring augmentation 50% of the time during training, and use no blurring during inference. We explored other settings (e.g. applying blurring to all training examples, and using varying amounts of blurring during inference), but found this to be most effective in our initial experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Experiments at the 128×128 and 256×256 resolutions</head><p>Table <ref type="table" target="#tab_4">4a</ref> shows the results of applying Gaussian blur augmentation to the 64×64 → 256×256 super-resolution model. While any amount of blurring helps improve the scores of the 256×256 samples over the baseline model with no blur, we found that sampling σ ∼ U(0.4, 0.6) gives the best results. Table <ref type="table" target="#tab_4">4b</ref> shows further improvements. While we find class conditioning helpful for upsampling at low resolution settings, it is interesting that it still gives a huge boost to the upsampling performance at high resolutions even when the low resolution inputs at 64×64 can be sufficiently informative. We also found increasing the training batch size from 256 to 1024 further improved performance by a significant margin. We also obtain marginal improvements by training the super-resolution model on randomly flipped data.</p><p>Since the sampling cost increases quadratically with the target image resolution, we attempt to minimize the number of denoising iterations for our 64×64 → 256×256 and 64×64 → 128×128 super-resolution models. To this end, we train these super-resolution models with continuous noise conditioning, like <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b2">3]</ref>, and tune the noise schedule for a given number of steps during inference. This tuning is relatively inexpensive as we do not need to retrain the models. We report all results using 100 inference steps for these models. Figure <ref type="figure" target="#fig_2">3</ref> shows FID vs number of inference steps for our 64×64 → 256×256 model. The FID score deteriorates marginally even when using just 4 inference steps. Interestingly, we do not observe any concrete improvement in FID by increasing the number of inference steps from 100 to 1000. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related work</head><p>Recent interest in diffusion models <ref type="bibr" target="#b23">[24]</ref> started with work connecting diffusion models to denoising score matching over multiple noise scales <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b25">26]</ref>. There have been a number of improvements and alternatives proposed to the diffusion framework, for example generalization to continuous time <ref type="bibr" target="#b27">[28]</ref>, deterministic sampling <ref type="bibr" target="#b24">[25]</ref>, adversarial training <ref type="bibr" target="#b10">[11]</ref>, and others <ref type="bibr" target="#b5">[6]</ref>. For simplicity, we base our models on DDPM <ref type="bibr" target="#b9">[10]</ref> with modifications from Improved DDPM <ref type="bibr" target="#b15">[16]</ref> to stay close to the original diffusion framework. Cascading pipelines have been investigated for diffusion models <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b3">4]</ref> and other types of generative models <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b14">15]</ref>; our work here focuses on improving cascaded diffusion models for ImageNet generation. Our conditioning augmentation work also mimics scheduled sampling in autoregressive sequence generation <ref type="bibr" target="#b0">[1]</ref>, where noise is used to alleviate the mismatch between train and inference conditions.</p><formula xml:id="formula_10">CDM (ours) VQ-VAE-2 BigGAN-deep</formula><p>Concurrent work <ref type="bibr" target="#b3">[4]</ref> showed that diffusion models are capable of generating high quality ImageNet samples using an improved architecture, named ADM, and a classifier guidance technique in which a class-conditional diffusion model sampler is modified to simultaneously take gradient steps on the logits of an extra trained image classifier. By contrast, our work focuses solely on improving sample quality by cascading, so we avoid introducing extra elements such as the image classifier. This comes at the expense of using thousands of diffusion timesteps in our low resolution models, where ADM uses hundreds. Our models outperform ADM without classifier guidance as reported in <ref type="bibr" target="#b3">[4]</ref>; nonetheless, classifier guidance and cascading were noted to complement each other as techniques to improve sample quality, and we expect classifier guidance would improve our results too.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have shown that cascaded diffusion models are capable of outperforming GANs on the ImageNet class-conditional generation benchmark when paired with conditioning augmentation, our technique of introducing data augmentation into the conditioning information of super-resolution models. We found that conditioning augmentation helps sample quality because it combats compounding errors in cascading pipelines due to train-test mismatch in super-resolution models.</p><p>Although there could be negative impact of our work in the form of malicious uses of image generation, our work has the potential to improve beneficial downstream applications such as data compression while advancing the state of knowledge in fundamental machine learning problems. We see our results as a conceptual study of the image synthesis capabilities of diffusion models in their original form with minimal extra techniques, and we hope our work serves as inspiration for future advances in the capabilities of diffusion models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Details on truncated conditioning augmentation</head><p>To be more precise about training a cascading pipeline with truncated conditioning augmentation (Section 3.2), let us examine the ELBO for p s θ (x 0 ) in Eq. ( <ref type="formula" target="#formula_8">9</ref>). We can treat p s θ (x 0 ) as a VAE with a diffusion model prior, a diffusion model decoder, and the approximate posterior q(x 1:T , z 1:T |x 0 , z 0 ) = T t=1 q(x t |x t−1 )q(z t |z t−1 ) , <ref type="bibr" target="#b10">(11)</ref> which runs forward processes independently on a low and high resolution pair. The ELBO is − log p s θ (x 0 ) ≤ E q L T (z 0 ) + t&gt;s D KL (q(z t−1 |z t , z 0 ) p θ (z t−1 |z t )) − log p θ (x 0 |z s ) , <ref type="bibr" target="#b11">(12)</ref> where L T (z 0 ) = D KL (q(z T |z 0 ) p(z T )). Note that the sum over t is truncated at s, and the decoder p θ (x 0 |z s ) is the super-resolution model conditioned on z s . The decoder itself has an ELBO − log p θ (x 0 |z s ) ≤ L θ (x 0 |z s ), where L θ (x 0 |z s ) = E q L T (x 0 ) + t&gt;1 D KL (q(x t−1 |x t , x 0 ) p θ (x t−1 |x t , z s )) − log p θ (x 0 |x 1 , z s ) (13)</p><p>Thus we have an ELBO for the combined model:</p><formula xml:id="formula_11">− log p s θ (x 0 ) ≤ E q L T (z 0 ) + t&gt;s D KL (q(z t−1 |z t , z 0 ) p θ (z t−1 |z t )) + L θ (x 0 |z s ) .<label>(14)</label></formula><p>It is apparent that optimizing Eq. ( <ref type="formula" target="#formula_11">14</ref>) trains the low and high resolution models separately. For a fixed value of s, the low resolution process is trained up to the truncation timestep s, and the super-resolution model is trained on a conditioning signal corrupted using the low resolution forward process stopped at timestep s.</p><p>In practice, since we pursue sample quality as our main objective, we do not use these ELBO expressions directly when training models with learnable reverse process variances. Rather, we train on their hybrid loss counterparts; see Section 2 and <ref type="bibr" target="#b3">[4]</ref> for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Hyperparameters</head><p>Here we give the hyperparameters of the models in our ImageNet cascading pipelines. Each model in the pipeline is described by its diffusion process, its neural network architecture, and its training hyperparameters. Architecture hyperparameters, such as the base channel count and the list of channel multipliers per resolution, refer to hyperparameters of the U-Net in DDPM and related models <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23]</ref>. The cosine noise schedule and the hybrid loss method of learning reverse process variances are from Improved DDPM <ref type="bibr" target="#b15">[16]</ref>.</p><p>32×32 base model </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A cascaded diffusion model comprising a base model and two super-resolution models.</figDesc><graphic url="image-3.png" coords="1,399.67,473.55,99.00,99.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Selected synthetic 256×256 ImageNet samples.</figDesc><graphic url="image-17.png" coords="2,175.33,202.67,65.34,65.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: FID on 256×256 images vs inference steps used in 64×64 → 256×256 super-resolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Comparing the quality and diversity of model samples in selected 256×256 ImageNet classes {Tench(0), Goldfish(1) and Ostrich(9)}. VQVAE-2 and BigGAN samples are taken from [18].</figDesc><graphic url="image-28.png" coords="9,110.47,208.10,128.70,128.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Main class-conditional ImageNet results on classifier guidance-free methods.</figDesc><table><row><cell>Model</cell><cell>FID vs train</cell><cell>FID vs validation</cell><cell>IS</cell></row><row><cell>32×32 resolution</cell><cell></cell><cell></cell><cell></cell></row><row><cell>CDM (ours)</cell><cell>1.11</cell><cell>1.99</cell><cell>26.01 ± 0.59</cell></row><row><cell>64×64 resolution</cell><cell></cell><cell></cell><cell></cell></row><row><cell>BigGAN-deep, by [4]</cell><cell>4.06</cell><cell></cell><cell></cell></row><row><cell>Improved DDPM [16]</cell><cell>2.92</cell><cell></cell><cell></cell></row><row><cell>ADM [4]</cell><cell>2.07</cell><cell></cell><cell></cell></row><row><cell>CDM (ours)</cell><cell>1.48</cell><cell>2.48</cell><cell>67.95 ± 1.97</cell></row><row><cell>128×128 resolution</cell><cell></cell><cell></cell><cell></cell></row><row><cell>BigGAN-deep [2]</cell><cell>5.7</cell><cell></cell><cell>124.5</cell></row><row><cell>BigGAN-deep, max IS [2]</cell><cell>25</cell><cell></cell><cell>253</cell></row><row><cell>LOGAN [33]</cell><cell>3.36</cell><cell></cell><cell>148.2</cell></row><row><cell>ADM [4]</cell><cell>5.91</cell><cell></cell><cell></cell></row><row><cell>CDM (ours)</cell><cell>3.52</cell><cell>3.76</cell><cell>128.80 ± 2.51</cell></row><row><cell>256×256 resolution</cell><cell></cell><cell></cell><cell></cell></row><row><cell>BigGAN-deep [2]</cell><cell>6.9</cell><cell></cell><cell>171.4</cell></row><row><cell>BigGAN-deep, max IS [2]</cell><cell>27</cell><cell></cell><cell>317</cell></row><row><cell>VQ-VAE-2 [18]</cell><cell>31.11</cell><cell></cell><cell></cell></row><row><cell>Improved DDPM [16]</cell><cell>12.26</cell><cell></cell><cell></cell></row><row><cell>SR3 [21]</cell><cell>11.30</cell><cell></cell><cell></cell></row><row><cell>ADM [4]</cell><cell>10.94</cell><cell></cell><cell>100.98</cell></row><row><cell>ADM+upsampling [4]</cell><cell>7.49</cell><cell></cell><cell>127.49</cell></row><row><cell>CDM (ours)</cell><cell>4.88</cell><cell>4.63</cell><cell>158.71 ± 2.26</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>64×64 ImageNet sample quality: ablations</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Conditioning</cell><cell>FID vs train</cell><cell cols="2">FID vs validation</cell><cell>IS</cell></row><row><cell>Model</cell><cell>FID vs train</cell><cell>FID vs validation</cell><cell>IS</cell><cell cols="3">No cascading 2.35 16×16→64×64 cascading</cell><cell>2.91</cell><cell>52.72 ± 1.15</cell></row><row><cell>Improved DDPM [16]</cell><cell>2.92</cell><cell></cell><cell></cell><cell>s = 0</cell><cell>6.02</cell><cell></cell><cell>5.84</cell><cell>35.59 ± 1.19</cell></row><row><cell>Our reimplementation</cell><cell>2.44</cell><cell>2.91</cell><cell>49.81 ± 0.65</cell><cell>s = 101</cell><cell>3.41</cell><cell></cell><cell>3.67</cell><cell>44.72 ± 1.12</cell></row><row><cell cols="2">+ more sampling steps 2.35</cell><cell>2.91</cell><cell>52.72 ± 1.15</cell><cell>s = 1001</cell><cell>2.13</cell><cell></cell><cell>2.79</cell><cell>54.47 ± 1.05</cell></row><row><cell cols="4">(a) Improvements to a non-cascaded baseline</cell><cell cols="5">(b) Small-scale ablation comparing no cascading to</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">16×16→64×64 cascading, using non-amortized trunca-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">tion time conditioning</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>64×64 ImageNet sample quality: large scale experiment comparing truncated and non-truncated conditioning augmentation for 32×32→64×64 cascading, using amortized truncation time conditioning</figDesc><table><row><cell>Conditioning</cell><cell>FID vs train</cell><cell>FID vs validation</cell><cell>IS</cell><cell>Conditioning</cell><cell>FID vs train</cell><cell>FID vs validation</cell><cell>IS</cell></row><row><cell cols="4">No conditioning augmentation (baseline)</cell><cell cols="3">Ground truth training data</cell><cell></cell></row><row><cell>s = 0</cell><cell>1.71</cell><cell>2.46</cell><cell>61.34 ± 1.58</cell><cell>s = 0</cell><cell>0.76</cell><cell>1.76</cell><cell>74.84 ± 1.43</cell></row><row><cell cols="3">Truncated conditioning augmentation</cell><cell></cell><cell>s = 251</cell><cell>0.87</cell><cell>1.85</cell><cell>71.79 ± 0.89</cell></row><row><cell>s = 251 s = 501 s = 751 s = 1001 s = 1251</cell><cell>1.50 1.48 1.48 1.49 1.51</cell><cell>2.44 2.48 2.51 2.51 2.54</cell><cell>66.76 ± 1.76 67.95 ± 1.97 68.48 ± 1.77 67.95 ± 1.51 67.20 ± 1.94</cell><cell>s = 501 s = 751 s = 1001 s = 1251 s = 1501</cell><cell>0.92 0.95 0.98 1.03 1.11</cell><cell>1.91 1.94 1.97 1.99 2.04</cell><cell>70.68 ± 1.26 69.93 ± 1.40 69.03 ± 1.26 67.92 ± 1.65 66.7 ± 1.21</cell></row><row><cell>s = 1501</cell><cell>1.54</cell><cell>2.56</cell><cell>67.09 ± 1.67</cell><cell cols="3">Ground truth validation data</cell><cell></cell></row><row><cell cols="4">Non-truncated conditioning augmentation</cell><cell>s = 0</cell><cell>1.20</cell><cell>0.59</cell><cell>64.33 ± 1.24</cell></row><row><cell>s = 251 s = 501 s = 751 s = 1001 s = 1251 s = 1501</cell><cell>1.58 1.53 1.48 1.49 1.48 1.50</cell><cell>2.50 2.51 2.47 2.48 2.46 2.47</cell><cell>66.21 ± 1.51 67.59 ± 1.85 67.48 ± 1.31 66.51 ± 1.59 66.28 ± 1.49 65.59 ± 0.86</cell><cell>s = 251 s = 501 s = 751 s = 1001 s = 1251 s = 1501</cell><cell>1.27 1.32 1.38 1.42 1.47 1.53</cell><cell>0.96 1.17 1.32 1.44 1.54 1.64</cell><cell>63.17 ± 1.19 62.65 ± 0.76 62.21 ± 0.94 61.53 ± 1.39 60.58 ± 0.93 60.02 ± 0.84</cell></row><row><cell cols="4">(a) Base model samples for low resolution conditioning</cell><cell cols="4">(b) Ground truth for low resolution conditioning</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>256×256 ImageNet sample quality: experiments on 64×64 → 256×256 super-resolution</figDesc><table><row><cell>Blur σ</cell><cell>FID vs train</cell><cell>FID vs validation</cell><cell>IS</cell><cell>Model</cell><cell>FID vs train</cell><cell>FID vs validation</cell><cell>IS</cell></row><row><cell>σ = 0 (no blur)</cell><cell>7.26</cell><cell>6.42</cell><cell>134.53 ± 2.97</cell><cell>Baseline</cell><cell>6.18</cell><cell>5.57</cell><cell>142.71 ± 2.83</cell></row><row><cell cols="2">σ ∼ U (0.4, 0.6) 6.18</cell><cell>5.57</cell><cell>142.71 ± 2.83</cell><cell>+ Class Conditioning</cell><cell>5.75</cell><cell>5.27</cell><cell>152.17 ± 2.29</cell></row><row><cell cols="2">σ ∼ U (0.4, 0.8) 6.90</cell><cell>6.31</cell><cell>136.57 ± 4.34</cell><cell>+ Large Batch Training</cell><cell>5.00</cell><cell>4.71</cell><cell>157.84 ± 2.60</cell></row><row><cell cols="2">σ ∼ U (0.4, 1.0) 6.35</cell><cell>5.76</cell><cell>141.40 ± 4.34</cell><cell cols="2">+ Flip LR Augmentation 4.88</cell><cell>4.63</cell><cell>158.71 ± 2.26</cell></row><row><cell cols="4">(a) Gaussian blur noise in conditioning</cell><cell cols="4">(b) Further improvements on super-resolution</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head><p>We thank Jascha Sohl-Dickstein for feedback on this work.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Scheduled sampling for sequence prediction with recurrent neural networks</title>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.03099</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Large scale GAN training for high fidelity natural image synthesis</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">WaveGrad: Estimating gradients for waveform generation</title>
		<author>
			<persName><forename type="first">Nanxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.00713</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Diffusion models beat GANs on image synthesis</title>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.05233</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.08803</idno>
		<title level="m">Density estimation using Real NVP</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Learning energy-based models by diffusion recovery likelihood</title>
		<author>
			<persName><forename type="first">Ruiqi</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><forename type="middle">Nian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.08125</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">GANs trained by a two time-scale update rule converge to a local Nash equilibrium</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6626" to="6637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Flow++: Improving flow-based generative models with variational dequantization and architecture design</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6840" to="6851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adversarial score matching and improved sampling for image generation</title>
		<author>
			<persName><forename type="first">Alexia</forename><surname>Jolicoeur-Martineau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Piché-Taillefer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.05475</idno>
	</analytic>
	<monogr>
		<title level="m">Rémi Tachet des Combes, and Ioannis Mitliagkas</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Glow: Generative flow with invertible 1x1 convolutions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Dhariwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="10215" to="10224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">Auto-encoding variational Bayes. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Diffwave: A versatile diffusion model for audio synthesis</title>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaji</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kexin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.09761</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generating high fidelity images with subscale pixel networks and multidimensional upscaling</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Improved denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.09672</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Sequence level training with recurrent neural networks</title>
		<author>
			<persName><forename type="first">Aurelio</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><surname>Zaremba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Generating diverse high-fidelity images with VQ-VAE-2</title>
		<author>
			<persName><forename type="first">Ali</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="14837" to="14847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">ImageNet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Image super-resolution via iterative refinement</title>
		<author>
			<persName><forename type="first">Chitwan</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.07636</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">PixelCNN++: Improving the PixelCNN with discretized logistic mixture likelihood and other modifications</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep unsupervised learning using nonequilibrium thermodynamics</title>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niru</forename><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2256" to="2265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.02502</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">Denoising diffusion implicit models. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Generative modeling by estimating gradients of the data distribution</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="11895" to="11907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Improved techniques for training score-based generative models</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09011</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Score-based generative modeling through stochastic differential equations</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Diederik P Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.13456</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03499</idno>
		<title level="m">WaveNet: A generative model for raw audio</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pixel recurrent neural networks</title>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Conditional image generation with PixelCNN decoders</title>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4790" to="4798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00937</idno>
		<title level="m">Neural discrete representation learning</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Logan: Latent optimisation for generative adversarial networks</title>
		<author>
			<persName><forename type="first">Yan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.00953</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
