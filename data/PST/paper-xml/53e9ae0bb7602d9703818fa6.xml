<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Question Answering from the Web Using Knowledge Annotation and Knowledge Mining Techniques</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
							<email>jimmylin@ai.mit.edu</email>
							<affiliation key="aff0">
								<orgName type="department">MIT Computer Science and Artificial Intelligence Laboratory 200 Technology Square Cambridge</orgName>
								<address>
									<region>Massachusetts</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Boris</forename><surname>Katz</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT Computer Science and Artificial Intelligence Laboratory 200 Technology Square Cambridge</orgName>
								<address>
									<region>Massachusetts</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Question Answering from the Web Using Knowledge Annotation and Knowledge Mining Techniques</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">586A4731C40BA83F494B24BB2403E0B2</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3.4 [Information Systems]: Information Storage and Retrieval</term>
					<term>Systems and Software Design</term>
					<term>Performance semistructured data</term>
					<term>data-redundancy</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a strategy for answering fact-based natural language questions that is guided by a characterization of realworld user queries. Our approach, implemented in a system called Aranea, extracts answers from the Web using two different techniques: knowledge annotation and knowledge mining. Knowledge annotation is an approach to answering large classes of frequently occurring questions by utilizing semistructured and structured Web sources. Knowledge mining is a statistical approach that leverages massive amounts of Web data to overcome many natural language processing challenges. We have integrated these two different paradigms into a question answering system capable of providing users with concise answers that directly address their information needs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The vast amounts of information available on the World Wide Web makes it an attractive resource for answering a variety of questions that users may have. However, the effectiveness of such a "knowledge repository" is limited by practical means of information access. The sheer size of the Web threatens to overwhelm both causal users and information professionals alike; Web search engines frequently return hundreds of thousands of documents in response to a query. To satisfy an information need, users are often forced to engage in the labor-intensive task of manually perusing "potentially relevant" documents returned by keywordbased search engines.</p><p>Question answering has recently emerged as a technology that promises to provide more intuitive methods of information access. In contrast to the traditional information retrieval model of formulating queries and browsing results, a question answering system simply accepts user information requests phrased in everyday language and responds with a concise answer. Currently, question answering research has focused on fact-based questions like "When did Montana become a state?" or "How far is it from the pitcher's mound to home plate?" Most of these "factoid" questions can be answered with simple entities such as dates, locations, people, organizations, measures, etc. This paper describes Aranea, an open-domain question answering system that takes advantage of Web data to answer factoid questions. Our system embraces two different views of the World Wide Web: as a heterogeneous collection of unorganized documents and as a source of carefully crafted and organized knowledge about specific topics. To take advantage of these different facets of the Web, our system integrates two different paradigms of question answering: knowledge annotation using semistructured database techniques and knowledge mining using redundancy-based statistical techniques.</p><p>Aranea's approach to question answering is primarily motivated by an observation about the empirical distribution of user queries, which turns out to quantitatively obey Zipf's Law-a small fraction of question types accounts for a significant portion of all question instances. Many questions ask for the same type of information, differing only in the specific object questioned, e.g., "What is the population of the United States?", "What is the population of Mexico?", "What is the population of Canada?", etc. Not only do such questions appear frequently, but they can also be naturally grouped together into a single type or class, i.e., "What is the population of x?" where x is a variable that can stand in for any country. Figure <ref type="figure" target="#fig_0">1</ref> presents an analysis of testsets from the TREC-9 and TREC-10 question answering evaluations, where unique question types are plotted against cumulative distribution of all questions. We can see, for example, that twenty question types account for over twenty percent of questions from the TREC-9 and forty percent of the questions from the TREC-10 testsets. Analysis of logs from the Start question answering system <ref type="bibr" target="#b15">[15]</ref>, which has answered millions of questions over the last decade, and commercial search engine logs <ref type="bibr" target="#b25">[25]</ref> leads to the same conclusions. Large classes of commonly-occurring questions translate naturally into database queries and are handled by Aranea using a technique we call knowledge annotation, which allows our system to access semistructured and heterogeneous data as if it were a uniform database.</p><p>As with all Zipf curves, there is a broad tail where individual instances are either unique or account for an insignificant fraction of total questions. In addition to asking large classes of commonly-occurring questions, users also pose a significant number of unique questions that cannot be easily classified into common categories or grouped by simple patterns.</p><p>To answer these questions, Aranea employs what we call redundancy-based knowledge mining techniques. Knowledge mining leverages the massive amounts of information available on the Web to overcome many thorny problems associated with natural language processing. This paper describes the workings of our Aranea question answering system and presents the effectiveness of our approach, as evaluated at the 2002 Text Retrieval Conference (TREC-2002) <ref type="bibr" target="#b28">[28]</ref>. The question answering track at TREC, which began in 1999, is an annual event designed to evaluate question answering systems against a common testset using a shared methodology, with the goal of encouraging discourse within the research community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">OVERALL FRAMEWORK</head><p>The overall architecture of Aranea is shown in Figure <ref type="figure" target="#fig_1">2</ref>. User questions are sent to two separate components, one that employs knowledge annotation (described in Section 3) and one that utilizes knowledge mining (described in Section 4). Both components consult the World Wide Web to generate candidate answers, which are then piped through a knowledge boosting module (Section 5) that checks the candidate answers against a number of heuristics to ensure their validity. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">KNOWLEDGE ANNOTATION</head><p>Although the Web consists largely of unorganized pages, pockets of structured and semistructured knowledge exist as valuable resources for question answering. For example, the CIA World Factbook provides political, geographic, and economic information about every country in the world; 50states.com contains numerous properties related to US states from state bird to land area; Biography.com has collected profiles of over twenty-five thousand famous people; the Internet Movie Database stores entries for hundreds of thousands of movies, including information about their cast, production staff, and dozens of other properties.</p><p>To effectively use these resources, a system must integrate them under a common interface. Drawing from database concepts, we have developed a schema-based technique called knowledge annotation that connects natural language queries with semistructured knowledge sources. This technique is a simplified implementation of the technologies pioneered by Start <ref type="bibr" target="#b15">[15]</ref> and Omnibase <ref type="bibr" target="#b16">[16]</ref>, two components of the world's first Web-based question answering system.</p><p>Since it came on-line in December, 1993, Start<ref type="foot" target="#foot_0">1</ref> has engaged in exchanges with hundreds of thousands of users all over the world, supplying them with useful knowledge. However, because the system provides users with paragraphsized answers that often contain multimedia fragments such as pictures and audio clips, it is not suitable for a TRECstyle evaluation. In general, we believe that paragraph-sized chunks form the most suitable unit of response to a user question, because complementing the short answer with additional contextual information may help with interpretation and analysis <ref type="bibr" target="#b24">[24]</ref>. However, because the TREC QA track accepts only exact answers, we found it inappropriate to directly evaluate Start and Omnibase. Instead, we have channeled our experiences into Aranea, a broader coverage, but less linguistically-sophisticated question answering system designed specifically to return TREC-style answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Database Access Schemata</head><p>The heart of Aranea's knowledge annotation component is a collection of database access schemata. Each schema is composed of two connected parts: the question signature and the database query. A question signature is a collection of regular expressions that match a specific class of user questions, e.g., requests for birth dates of people. <ref type="foot" target="#foot_1">2</ref> These annotations are paired with unfilled database queries that are dynamically instantiated with bindings extracted from the question signature. Consider a typical schema:</p><p>When was x born? What is the birth date of x? . . . → (biography.com x birthdate)</p><p>In this example, questions that ask for birth dates are translated into an object-property-value database query <ref type="bibr" target="#b16">[16]</ref>. These queries specify the data source where the answer could be found (biography.com), the object in question (x), and the property sought after (birthdate). The value of the object's property typically answers the user's question. In practice, we have discovered that this data model is expressive enough to capture a significant fraction of semistructured Web resources as well as user questions.</p><p>The knowledge annotation component of Aranea operates by matching user questions against stored schemata and executing database queries generated as a result. The execution of these queries varies with the data source: Some sources are stored locally and translate into a simple file lookup. Other sources are stored on remote Web sites behind CGI scripts; executing queries on these sources requires dynamically reconstructing an HTTP request and properly parsing the resulting HTML document. Because each resource is structured differently, wrappers must be manually crafted for each individual data source.</p><p>The Aranea system deployed for the 2002 TREC evaluation included twenty-eight schemata that access seven different data sources. Here are two examples:</p><p>• Biography.com provides information about the birth dates, death dates, etc., of various well-known people.</p><p>Answering questions about such properties involves dynamically retrieving pages from biography.com (via CGI) and performing simple pattern matching on the HTML to extract exact dates.</p><p>• CIA World Factbook provides various useful facts about countries, e.g., population, area, capital, etc. This information was downloaded and structured into a locally-stored tab-delimited file. Questions about various properties of world countries are translated into simple file lookups.</p><p>Despite the manual labor involved in "wrapping" data sources, knowledge annotation nevertheless remains an effective question answering strategy. Because users often ask similar questions, a few well-chosen knowledge sources suffice to answer a significant fraction of questions. For example, we have verified that ten Web sources can provide answers to 27% of TREC-9 and 47% of TREC-2001 QA track questions <ref type="bibr" target="#b23">[23]</ref>. In addition, the importance of specific knowledge sources has been noticed by other researchers as well <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b8">8]</ref>. By letting the distribution of user questions guide our wrapper development, we can achieve good performance with modest amounts of knowledge engineering. are parsed into and stored as ternary expressions. Because matching occurs at the level of the parsed structures, powerful linguistic machinery can be employed to handle different linguistic phenomena, e.g., synonymy, hyper/hyponymy, alternations, etc. In Aranea, we have attempted to approximate natural language processing techniques with regular expressions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Related Work</head><p>The knowledge annotation strategy for question answering was first introduced and implemented in Start <ref type="bibr" target="#b15">[15]</ref> and Omnibase <ref type="bibr" target="#b16">[16]</ref>. With the adaptation of our techniques in the Aranea system, we have had, for the first time, an opportunity to evaluate the technology in a formal setting at the TREC 2002 question answering competition.</p><p>The idea of applying database techniques to the World Wide Web is not new (cf. <ref type="bibr" target="#b10">[10]</ref>). Many existing systems, e.g., Araneus <ref type="bibr" target="#b1">[1]</ref>, Ariadne <ref type="bibr" target="#b20">[20]</ref>, Tsimmis <ref type="bibr" target="#b11">[11]</ref>, just to name a few, have attempted to integrate heterogeneous Web sources under a common interface. Unfortunately, queries to such systems must be formulated in SQL, Datalog, or some similar formal language, which render them inaccessible to the average user. The unique contribution of our approach is the integration of database and natural language techniques.</p><p>The viability of our annotation-based question answering technique has also been demonstrated commercially. For example, Ask Jeeves, currently the Web's second most popular search engine, licenses certain technologies <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b19">19]</ref> pioneered by the Start system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">KNOWLEDGE MINING</head><p>The knowledge mining approach to question answering is a data-driven strategy that capitalizes on the enormous amounts of text freely available on the World Wide Web.</p><p>One of the biggest challenges in question answering is relating the formulation of a question to different formulations of its answers. Consider a question like "When did Alaska become a state?": if a document plainly stated that "Alaska became a state on January 3, 1959", the answer would be relatively easy to extract. Unfortunately, the expressiveness of natural language allows the same meaning to be expressed in a variety of different ways; more likely, the answer to the above question would be stated as "Alaska was admitted to the Union on January 3, 1959." Since this answer shares few keywords in common with the question, a system would require sophisticated reasoning, e.g., the ability to recognize paraphrases, in order to relate the answer to the question. The knowledge mining approach to question answering attempts to overcome this difficulty by leveraging the massive size of the Web.</p><p>The most important implication of the Web's size is data redundancy-each item of information has potentially been stated in many ways, in many different documents. A question answering system can capitalize on this redundancy in two ways: as a surrogate for sophisticated natural language techniques and as a method for overcoming poor document quality. Consider the question "When did Wilt Chamberlain score 100 points?" Here are two possible answers:</p><p>(1) Wilt Chamberlain scored 100 points on March 2, 1962 against the New Yorks Knicks.</p><p>( Obviously, the answer could be more easily extracted from sentence (1) than from passage <ref type="bibr" target="#b2">(2)</ref>. In general, the task of answering a question is not very difficult if the document collection contains the answer stated as a simple reformulation of the question. In these cases, simple keyword-based techniques coupled with named-entity detection technology suffice to identify the answer. However, without the luxury of massive amounts of data, a question answering system may be forced to extract answers from passages in which they are not obviously stated, e.g., passage <ref type="bibr" target="#b2">(2)</ref>. In these cases, sophisticated natural language processing may be required, e.g., recognizing syntactic alternations, resolving anaphora, making commonsense inferences, performing relative date calculations, etc.</p><p>The Web is so big that simple pattern matching techniques can often obviate the need to understand both the structure and meaning of language. With enough data, there is a good chance that an answer will appear as a simple reformulation of the question. In such cases, the answer could be extracted by searching directly for an anticipated answer form, e.g., in the above example, by searching for the string "Wilt Chamberlain scored 100 points on" and extracting words occurring to the right. Naturally, this simple technique depends crucially on the corpus having an answer formulated in a specific way-the larger the text collection is, the greater the probability that simple pattern matching techniques will yield the correct answer. Data redundancy enables a simple trick to overcome many troublesome issues in natural language processing.</p><p>The effect of data redundancy has been quantified by other researchers as well. Breck et al. <ref type="bibr" target="#b3">[3]</ref> noticed a correlation between the number of times an answer appeared in the TREC corpus and the average performance of TREC systems on that particular question. Similarly, Clarke et al. <ref type="bibr" target="#b7">[7]</ref> noticed an upward trend in performance as a question answering system was given a larger corpus from which to extract answers. These results verify our intuition: the more times an answer appears (in different formulations), the easier it is to find it.</p><p>In a Web environment, data redundancy also serves as a guard against erroneous information. Because the overall quality of documents on the Web is lower than typical closed corpora (e.g., collections of newspaper articles), any single instance of an answer is inherently untrustworthy. However, because a fact is usually stated multiple times in multiple documents, a question answering system could utilize the distribution of answers across multiple sources to gauge its reliability.</p><p>The tremendous amounts of information on the World Wide Web would be useless without an effective method of data access. Providing the basic infrastructure for indexing and retrieving text at such scales is a tremendous engineering task. Fortunately, such services already exist, in the form of search engines. Using existing search engines as information retrieval backends, we can focus our efforts on answer extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Knowledge Mining Modules</head><p>The data flow in the knowledge mining component of Aranea is shown in Figure <ref type="figure" target="#fig_2">3</ref>. In the following sections, we describe each module in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Formulate Requests</head><p>The first step in answering factoid questions is to translate them into queries, or requests. These requests specify  the context in which answers are likely to be found, and are analogous to queries posed to traditional information retrieval systems. However, because Aranea relies on Web search engines to fulfill these requests, fine-grained control over the query and result set is difficult; Aranea instead relies on quantity to make up for lack of quality.</p><p>Two types of queries are generated by this module: exact queries and inexact queries. Queries of both types are concurrently generated, but usually given different scores. An inexact query indicates that the answer is likely to be found within the vicinity of a set of keywords. They are composed by treating the natural language question as a bag of words. In contrast, an exact query details the specific location of a potential answer, e.g., the answer to "When did the Mesozoic period end?" is likely to appear within ten words and fifty bytes to the right of the exact phrase "the Mesozoic period ended". Exact queries in Aranea are generated by approximately a dozen pattern matching rules based on query terms and their part-of-speech tags; morpho-lexical pattern matches trigger the creation of reformulated exact queries. As an example, the previous query was generated by the rule "wh-word did . . . verb → . . . verb+ed". An internal lexicon ensures that the generated verb remains properly conjugated.</p><p>As a complete example, the requests generated in response to the question "When did the Mesozoic period end?" are shown in Figure <ref type="figure" target="#fig_3">4</ref>. Aranea generates two inexact and one exact request; each query is also assigned a basic score, which helps establish the relative importance of the queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Execute Requests</head><p>The request execution module is responsible for retrieving textual snippets that honor the constraints set forth by each request. Currently, Google is used to mine text from the Web. In the case of inexact requests, the entire summary  provided by Google is extracted for further processing. For exact queries, the request execution module performs additional pattern matching to ensure that the correct positional constraints are satisfied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Generate N-Grams</head><p>This module exhaustively generates all possible unigrams, bigrams, trigrams, and tetragrams from the text fragments generated by the request execution module. These n-grams, which are given initial scores equal to the weight of the request from which they derive, serve as candidate answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Vote</head><p>The voting module collates the n-grams generated by the previous module. The new score of each answer candidate is equal to the sum of the scores of all occurrences of that particular n-gram. This module has the effect of promoting text fragments that occur frequently (in the context of query terms), and are hence more likely to answer the user question.</p><p>This process of voting is meant to counteract the low average quality of individual documents. Many Web documents are poorly written, barely edited, or simply contain incorrect information. Although text extracted from a single document cannot be trusted as the correct answer, multiple occurrences of the same answer in different documents lends credibility to the proposed answer. 3   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.5">Filter Candidates</head><p>In this stage of processing, a coarse-grained filter is applied to the answer candidates:</p><p>• Candidates that begin or end with stopwords are discarded.</p><p>• Candidates that contain words found in the original user question are discarded. The only exception to this rule is question focus words, e.g., a question beginning with "How many meters. . . " can be answered by an expression containing the word "meters".</p><p>This stage also encodes a few heuristics that can potentially decrease the number of answer candidates. For example, the answer to "how far", "how fast", "how tall", etc., 3 Unfortunately, this technique equates the most popular answer with the correct answer, which occasionally results in very comical responses.</p><p>questions must invariably contain a numeric component (either numeric digits or numerals); thus, we can safely discard all answer candidates that do not fit this criteria. The heuristics employed by this module tend to filter with high confidence, erring on the side of being too lenient. False positive results can always be sorted out by later modules, but the system will not be able to recover from false negatives.</p><p>In addition, a set of fixed-list filters is applied to questions whose answer types are closed-class items. For example, a question like "What language do most people speak in Brazil?" must be answered with a language; thus, we can safely throw out any answer candidate that isn't a language. For a variety of question types, e.g., "What sport. . . ", "What nationality. . . ", it is relatively straightforward to enumerate all acceptable answer candidates. In these cases, fixed lists can be used as high-precision filters to throw out irrelevant candidates. We have implemented roughly a dozen of such filters in Aranea.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.6">Combine Candidates</head><p>In this module, shorter answers are used as evidence to boost the score of longer answers. If a portion of a candidate answer appears itself as a candidate answer, then the score of the shorter answer is added to the score of the longer answer. For example, if "de Soto" appears on the list of candidate answers along with "Hernando de Soto", the score of the shorter candidate would be added to the score of the longer one. This module counteracts the tendency of the n-gram generation and voting modules to favor shorter answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.7">Score Candidates</head><p>The score of each answer candidate is multiplied by the following factor:</p><formula xml:id="formula_0">1 |A| w∈A log( N wc )</formula><p>A is the set of keywords in the candidate answer; N is the total number of words in our corpus; wc is the number of occurrences of word w in the corpus. This scoring balances the effect of individual keywords having different (unconditioned) priors. Since the exact distribution of unigrams on the Web can not be easily obtained in a reliable manner, Aranea uses statistics from our corpus as a surrogate. We used the official corpus of the TREC question answering track-the AQUAINT corpus-which is comprised of roughly one million articles from the New York Times, the Associated Press, and the Xinhua News Service.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.8">Get Support</head><p>This module performs a final sanity check on the candidate answers. It verifies that final candidate answers actually appear in the original text snippets mined from the Web. Occasionally, the various modules within the knowledge mining component of the system will assemble a nonsensical answer; this module discards such answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Related Work</head><p>Many other works <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b4">4]</ref> are similar in spirit to the knowledge mining paradigm employed by Aranea. In general, we view the knowledge mining component of Aranea as the next generation of redundancy-based techniques for the World Wide Web. Mulder <ref type="bibr" target="#b22">[22]</ref>, one of the earliest question answering systems to take advantage of commercial search engines, attempted to perform sophisticated linguistic analysis on both questions and potential answer candidates. As a result, it did not take advantage of data redundancy. Furthermore, the system was not formally evaluated on standardized testsets, and therefore its performance cannot be compared against other systems in a meaningful way. Shapqa <ref type="bibr" target="#b6">[6]</ref>, another system that attempted to apply linguistically-sophisticated techniques to answer extraction, performed worse than the average system at TREC-2001. In contrast, the AskMSR [5], 4 one of the top performers at TREC-2001, embraced data-redundancy and applied extremely simple word-counting techniques on Web data. However, it was an ad-hoc agglomeration of two separate systems whose output were then stitched together by yet another "combiner" system. As such, the performance contribution of various components was difficult to determine. In comparison, Aranea boasts a modular architecture that also serves as a testbed for a variety of knowledge mining techniques. MultiText <ref type="bibr" target="#b7">[7]</ref>, another question answering system that takes advantage of data redundancy, employs a different approach: instead of using the Web directly to answer questions, it treated the Web as an auxiliary corpus to validate candidate answers extracted from a primary, more authoritative, corpus.</p><p>With Aranea, we have taken advantage of previous experiences to refine the knowledge mining paradigm within a better engineered framework. Our system supports a modular architecture that allows specific functionality to be encoded into manageable components. This not only allows for faster development cycles, but facilitates glass-box testing to properly determine the effectiveness of various techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">ANSWER BOOSTING</head><p>Results from both the knowledge annotation and knowledge mining components of Aranea are subjected to a series of heuristic checks.</p><p>Within the answer boosting module of Aranea, a set of procedures is specifically dedicated to detecting and verifying geographic locations. We have gathered large lists of known geographic entities, e.g., countries, cities, U.S. states, Canadian provinces, etc. Using these lists, we were able to construct accurate recognizers for locative expressions. In response to location questions such as "Where is the Isle of Man?" or "Where is Toronto?", Aranea can identify answer candidates of the correct type and "boost" their scores. Unlike filtering (with fixed-lists), this process is heuristicbased, and can recognize a greater range of expressions, e.g., "in the Irish Sea" or "on the north shore of Lake Ontario". Using heuristics to "boost" potentially relevant answers is preferable to filtering out irrelevant answers in cases where enumeration of all acceptable responses is impractical.</p><p>Questions requiring dates as answers similarly receive special treatment. Since dates and temporal expressions have relatively fixed form, it is straightforward to detect such entities and promote them as potentially relevant answers. Knowledge of dates also helps Aranea extract exact answers. For example, a candidate answer to a "What year. . . " question often contains extra information such as the month and day; Aranea removes this extraneous information. 4 One of the authors of this paper was on the team that designed the AskMSR system.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">RESULTS</head><p>The Aranea system participated in the question answering track at TREC-2002 <ref type="bibr" target="#b28">[28]</ref>. The annual track not only conducts formal evaluations of question answering systems, but also serves as a focal point for question answering research to facilitate the dissemination of results; see <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b28">28]</ref> for overviews of results from recent years. Notable changes in the 2002 evaluation is the exact answer requirement (explained below) and single-response requirement (i.e., each system is only allowed to return one answer for each question). A third new aspect of the evaluation, confidenceweighted scoring, is a relatively experimental procedure and not discussed here.</p><p>Results of the evaluation are shown in Table <ref type="table" target="#tab_3">1</ref>. In the TREC QA track, an answer was judged correct only if the system produced a document from the AQUAINT corpus that supported the answer. If the answer string was correct, but the supporting document did not confirm the answer, it would be judged as unsupported. Since Aranea extracts answers from the Web, the system subsequently needed to "project" the Web answers back onto the AQUAINT corpus to find a supporting document. We believe that this answer projection process is an artifact of the TREC evaluations, and argue that the process of answer projection is not directly part of the question answering task. Therefore, the results presented here are evaluations of the answer only; we have manually rescored unsupported judgments into either exact or inexact, disregarding the supporting document.</p><p>As an example of the difference between exact and inexact judgments, consider the question "What province in Canada is Niagara Falls located in?": "southern Ontario" would be judged as inexact whereas "Ontario" would be judged as exact. Aranea's results show a relatively large number of inexact answers, which could be rectified by some superficial linguistic processing. For example, inexact answers often contain additional leading adjectives or common nouns, e.g., "western Montana" or "Baptist leader Roger Williams". These extraneous words could be easily stripped off with the help of a part-of-speech tagger and fixed lists of occupations, cardinal directions, etc.</p><p>Individual analysis of the knowledge annotation and knowledge mining components of Aranea is shown in Table <ref type="table" target="#tab_4">2</ref>. In general, the knowledge annotation component achieves much higher accuracy than the knowledge mining component. However, knowledge mining achieve broader coverage than knowledge annotation.</p><p>Here are some typical answers given by Aranea:</p><p>When is Gerald Ford's birthday? <ref type="bibr">July 14, 1913</ref> (extracted using knowledge annotation techniques from biography.com)</p><p>Who founded Taoism? Lao Tzu</p><p>(extracted using knowledge mining techniques)</p><p>What was the name of the first child of English parents to be born in America? Virginia Dare</p><p>(extracted using knowledge mining techniques)</p><p>Approximately 16% (30/183) of answers judged as exact were retrieved using the knowledge annotation paradigm. We believe that such a performance is remarkable, considering that our system contained only twenty-eight database access schemata with access to seven knowledge sources. In total, the knowledge annotation component represented no more than a few person-days worth of manual labor.</p><p>These results also verify that analysis of typical user question distribution can help guide the knowledge engineering effort. Our database access schemata were geared towards answering the most frequently occurring questions from the previous TREC evaluations; many of the same question types also appeared in the 2002 testset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">LIMITATIONS</head><p>Despite our efforts in streamlining the knowledge engineering process for structuring Web resources, the need for manual labor remains the biggest limitation of the knowledge annotation approach to question answering. In our approach, database queries must be mapped to procedures capable of extracting only the relevant fragments of Web pages (using wrappers), and natural language queries must be mapped to specific database queries (using annotations). Because each site contains inconsistencies and idiosyncrasies, human effort is required to achieve high precision. To address the wrapper generation problem, we believe that machine learning techniques for automatically or semi-automatically inducing wrappers <ref type="bibr" target="#b21">[21,</ref><ref type="bibr">9,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b26">26]</ref> are promising. For crafting the mappings from natural language to database queries, we believe that developments in the automatic recog-nition of paraphrases <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b14">14]</ref> will help systems equate questions that are asking for the same information.</p><p>Although our knowledge mining paradigm can achieve remarkable performance using only simple techniques, there remain difficult problems that only a deeper understanding of language can solve. One such problem is associated with questions whose answers are temporally dependent, e.g., "Who was the prime minister of Britain in 1979?" or "Who was the first governor of Missouri?" Since Aranea does not recognize temporal expressions, it usually answers with the current prime minister and current governor. Another set of problems center around the semantic importance of modifiers, e.g., "Who was the second man to set foot on the moon?" or "What is the third tallest peak in the world?" In these questions, the keywords "second" and "third" are extremely important semantically; however, Aranea considers them statistically unimportant (because they appear very frequently on the Web and in corpora). As a result, our system often ignores the constraints imposed by those modifiers; i.e., Aranea might answer the above questions with "Neil Armstrong" and "Mt. Everest", respectively. We believe that integrating linguistically-sophisticated techniques into Aranea is the solution to these problems (cf. <ref type="bibr" target="#b17">[17]</ref>). However, the effective integration of precise, but brittle natural language processing technology with robust, but less precise statistical techniques remains a research challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">CONTRIBUTIONS</head><p>The Aranea system implements and integrates two different paradigms for answering open-domain factoid questions. Answering questions can be viewed as executing database queries using our knowledge annotation technique. Alternatively, answers to questions could be "mined" from the Web using knowledge mining techniques. A primary contribution of Aranea is the smooth integration of these two approaches into a uniform framework. Because the distribution of factoid questions roughly follows Zipf's Law, we can employ knowledge annotation techniques to handle the "head" of the curve and utilize knowledge mining techniques to handle its "tail."</p><p>Another salient aspect of the Aranea approach is a taskdriven strategy. Since the ultimate goal of a question answering system is to answer user questions, it is only natural to analyze what users actually ask, in the form of logs and various testsets. By doing exactly this, we formed a characterization of real-world user queries, which enabled the smooth integration of two very different approaches to question answering.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Cumulative distribution of typical user questions: number of question types (or classes) plotted against the cumulative percentage of the testset that those question types account for.</figDesc><graphic coords="2,65.76,53.91,214.97,180.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overall architecture of Aranea.</figDesc><graphic coords="2,322.80,53.82,227.50,117.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Data flow in the knowledge mining component of Aranea.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Typical requests generated by Aranea.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>2) On December 8, 1961, Wilt Chamberlain scored 78 points in a triple overtime game. It was a new NBA record, but Warriors coach Frank McGuire didn't expect it to last long, saying, "He'll get 100 points someday." McGuire's prediction came true just a few months later in a game against the New York Knicks on March 2.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 : Results from TREC-2002 question answer- ing track.</head><label>1</label><figDesc></figDesc><table><row><cell># of q. %</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 : Performance of individual components.</head><label>2</label><figDesc></figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>http://www.ai.mit.edu/projects/infolab/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>These question signatures are a simplified version of the natural language annotations used by Start<ref type="bibr" target="#b15">[15]</ref>, which</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semistructured and structured data in the Web: Going back and forth</title>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Atzeni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giansalvatore</forename><surname>Mecca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Merialdo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Management of Semistructured Data at PODS/SIGMOD&apos;97</title>
		<meeting>the Workshop on Management of Semistructured Data at PODS/SIGMOD&apos;97</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Extracting paraphrases from a parallel corpus</title>
		<author>
			<persName><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics (ACL-2001)</title>
		<meeting>the 39th Annual Meeting of the Association for Computational Linguistics (ACL-2001)</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Looking under the hood: Tools for diagnosing your question answering engine</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Breck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Light</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gideon</forename><forename type="middle">S</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellen</forename><surname>Riloff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brianne</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mats</forename><surname>Rooth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Thelen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics (ACL-2001) Workshop on Open-Domain Question Answering</title>
		<meeting>the 39th Annual Meeting of the Association for Computational Linguistics (ACL-2001) Workshop on Open-Domain Question Answering</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An analysis of the AskMSR question-answering system</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Brill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susan</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Banko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2002 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Data-intensive question answering</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Brill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Banko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susan</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Text REtrieval Conference</title>
		<meeting>the Tenth Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Using grammatical relations, answer frequencies and the World Wide Web for question answering</title>
		<author>
			<persName><forename type="first">Sabine</forename><surname>Buchholz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Text REtrieval Conference</title>
		<meeting>the Tenth Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Exploiting redundancy in question answering</title>
		<author>
			<persName><forename type="first">Charles</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gordon</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Lynam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note>SIGIR-2001</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning to understand the Web</title>
		<author>
			<persName><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dallan</forename><surname>Quass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bulletin of the IEEE Computer Society Technical Committee on Data Engineering</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="17" to="24" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to extract symbolic knowledge from the World Wide Web</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Craven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Dipasquo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dayne</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamal</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Slattery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth National Conference on Artificial Intelligence (AAAI-1998)</title>
		<meeting>the Fifteenth National Conference on Artificial Intelligence (AAAI-1998)</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Database techniques for the World-Wide Web: A survey</title>
		<author>
			<persName><forename type="first">Daniela</forename><surname>Florescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alon</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Mendelzon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGMOD Record</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="59" to="74" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Extracting semistructured information from the Web</title>
		<author>
			<persName><forename type="first">Joachim</forename><surname>Hammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hector</forename><surname>Garcia-Molina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junghoo</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Aranha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arturo</forename><surname>Crespo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Management of Semistructured Data at PODS/SIGMOD&apos;97</title>
		<meeting>the Workshop on Management of Semistructured Data at PODS/SIGMOD&apos;97</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Using knowledge to facilitate factoid answer pinpointing</title>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ulf</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Ravichandran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Computational Linguistics (COLING-2002)</title>
		<meeting>the 19th International Conference on Computational Linguistics (COLING-2002)</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Finite-state transducers for semi-structured text mining</title>
		<author>
			<persName><forename type="first">Chun-Nan</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chien-Chi</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IJCAI-99 Workshop on Text Mining: Foundations, Techniques, and Applications</title>
		<meeting>the IJCAI-99 Workshop on Text Mining: Foundations, Techniques, and Applications</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Extracting structural paraphrases from aligned monolingual corpora</title>
		<author>
			<persName><forename type="first">Ali</forename><surname>Ibrahim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second International Workshop on Paraphrasing (IWP-2003)</title>
		<meeting>the Second International Workshop on Paraphrasing (IWP-2003)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Annotating the World Wide Web using natural language</title>
		<author>
			<persName><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th RIAO Conference on Computer Assisted Information Searching on the Internet (RIAO &apos;97)</title>
		<meeting>the 5th RIAO Conference on Computer Assisted Information Searching on the Internet (RIAO &apos;97)</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Omnibase: Uniform access to heterogeneous data for question answering</title>
		<author>
			<persName><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sue</forename><surname>Felshin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deniz</forename><surname>Yuret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Ibrahim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Marton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alton</forename><forename type="middle">Jerome</forename><surname>Mcfarland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baris</forename><surname>Temelkuran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Workshop on Applications of Natural Language to Information Systems</title>
		<meeting>the 7th International Workshop on Applications of Natural Language to Information Systems</meeting>
		<imprint>
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Selectively using relations to improve precision in question answering</title>
		<author>
			<persName><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EACL-2003 Workshop on Natural Language Processing for Question Answering</title>
		<meeting>the EACL-2003 Workshop on Natural Language Processing for Question Answering</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Method and apparatus for generating and utilizing annotations to facilitate computer text retrieval</title>
		<author>
			<persName><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Winston</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>United States Patent No</publisher>
			<biblScope unit="volume">309</biblScope>
			<biblScope unit="page">359</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Method and apparatus for utilizing annotations to facilitate computer retrieval of database material</title>
		<author>
			<persName><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Winston</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>United States Patent No</publisher>
			<biblScope unit="volume">404</biblScope>
			<biblScope unit="page">295</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The Ariadne approach to Web-based information integration</title>
		<author>
			<persName><forename type="first">Craig</forename><surname>Knoblock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Minton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><surname>Luis Ambite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naveen</forename><surname>Ashish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Muslea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Philpot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheila</forename><surname>Tejada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Cooperative Information Systems (IJCIS) Special Issue on Intelligent Information Agents: Theory and Applications</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1/2</biblScope>
			<biblScope unit="page" from="145" to="169" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Wrapper induction for information extraction</title>
		<author>
			<persName><forename type="first">Nickolas</forename><surname>Kushmerick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Doorenbos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth International Joint Conference on Artificial Intelligence (IJCAI-97)</title>
		<meeting>the Fifteenth International Joint Conference on Artificial Intelligence (IJCAI-97)</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Scaling question answering to the Web</title>
		<author>
			<persName><forename type="first">Cody</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International World Wide Web Conference</title>
		<meeting>the Tenth International World Wide Web Conference</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The Web as a resource for question answering: Perspectives and challenges</title>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Conference on Language Resources and Evaluation (LREC-2002)</title>
		<meeting>the Third International Conference on Language Resources and Evaluation (LREC-2002)</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The role of context in question answering systems</title>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dennis</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vineet</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karun</forename><surname>Bakshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">R</forename><surname>Karger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 SIGCHI Conference on Human Factors in Computing Systems (CHI 2003)</title>
		<meeting>the 2003 SIGCHI Conference on Human Factors in Computing Systems (CHI 2003)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">What&apos;s in store for question answering?</title>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">B</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP/VLC-2000)</title>
		<meeting>the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP/VLC-2000)</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
	<note>invited talk</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A hierarchical approach to wrapper induction</title>
		<author>
			<persName><forename type="first">Ion</forename><surname>Muslea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Minton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Craig</forename><surname>Knoblock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Autonomous Agents</title>
		<meeting>the 3rd International Conference on Autonomous Agents</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Overview of the TREC 2001 question answering track</title>
		<author>
			<persName><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Text REtrieval Conference</title>
		<meeting>the Tenth Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Overview of the TREC 2002 question answering track</title>
		<author>
			<persName><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh Text REtrieval Conference</title>
		<meeting>the Eleventh Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Overview of the TREC-9 question answering track</title>
		<author>
			<persName><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><forename type="middle">M</forename><surname>Tice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Text REtrieval Conference (TREC-9)</title>
		<meeting>the Ninth Text REtrieval Conference (TREC-9)</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
