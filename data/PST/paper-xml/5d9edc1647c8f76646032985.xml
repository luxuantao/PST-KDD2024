<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Named Entity Recognition with Bidirectional LSTM-CNNs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jason</forename><forename type="middle">P C</forename><surname>Chiu</surname></persName>
							<email>jsonchiu@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of British Columbia</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Eric</forename><surname>Nichols</surname></persName>
							<email>e.nichols@jp.honda-ri.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Honda Research Institute Japan Co.,Ltd</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Named Entity Recognition with Bidirectional LSTM-CNNs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Named entity recognition is a challenging task that has traditionally required large amounts of knowledge in the form of feature engineering and lexicons to achieve high performance.</p><p>In this paper, we present a novel neural network architecture that automatically detects word-and character-level features using a hybrid bidirectional LSTM and CNN architecture, eliminating the need for most feature engineering. We also propose a novel method of encoding partial lexicon matches in neural networks and compare it to existing approaches. Extensive evaluation shows that, given only tokenized text and publicly available word embeddings, our system is competitive on the CoNLL-2003 dataset and surpasses the previously reported state of the art performance on the OntoNotes 5.0 dataset by 2.13 F1 points. By using two lexicons constructed from publicly-available sources, we establish new state of the art performance with an F1 score of 91.62 on CoNLL-2003 and 86.28 on OntoNotes, surpassing systems that employ heavy feature engineering, proprietary lexicons, and rich entity linking information.</p><p>We saw paintings of Picasso Word Embedding</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Named entity recognition is an important task in NLP. High performance approaches have been dominated by applying CRF, SVM, or perceptron models to hand-crafted features <ref type="bibr" target="#b30">(Ratinov and Roth, 2009;</ref><ref type="bibr" target="#b25">Passos et al., 2014;</ref><ref type="bibr" target="#b21">Luo et al., 2015)</ref>. However, <ref type="bibr" target="#b8">Collobert et al. (2011b)</ref> proposed an effective neural network model that requires little feature engineering and instead learns important features from word embeddings trained on large quantities of unlabelled text -an approach made possible by recent advancements in unsupervised learning of word embeddings on massive amounts of data <ref type="bibr" target="#b6">(Collobert and Weston, 2008;</ref><ref type="bibr" target="#b23">Mikolov et al., 2013)</ref> and neural network training algorithms permitting deep architectures <ref type="bibr" target="#b31">(Rumelhart et al., 1986)</ref>.</p><p>Unfortunately there are many limitations to the model proposed by <ref type="bibr" target="#b8">Collobert et al. (2011b)</ref>. First, it uses a simple feed-forward neural network, which restricts the use of context to a fixed sized window around each word -an approach that discards useful long-distance relations between words. Second, by depending solely on word embeddings, it is unable to exploit explicit character level features such as prefix and suffix, which could be useful especially with rare words where word embeddings are poorly trained. We seek to address these issues by proposing a more powerful neural network model.</p><p>A well-studied solution for a neural network to process variable length input and have long term memory is the recurrent neural network (RNN) <ref type="bibr" target="#b12">(Goller and Kuchler, 1996)</ref>. Recently, RNNs have shown great success in diverse NLP tasks such as speech recognition <ref type="bibr" target="#b13">(Graves et al., 2013)</ref>, machine translation <ref type="bibr" target="#b3">(Cho et al., 2014)</ref>, and language modeling <ref type="bibr" target="#b22">(Mikolov et al., 2011)</ref>. The long-short term memory (LSTM) unit with the forget gate allows highly non-trivial long-distance dependencies to be easily learned <ref type="bibr" target="#b11">(Gers et al., 2000)</ref>. For sequential labelling tasks such as NER and speech recognition, a bi-directional LSTM model can take into account an effectively infinite amount of context on both sides of a word and eliminates the problem of limited context that applies to any feed-forward model <ref type="bibr" target="#b13">(Graves et al., 2013)</ref>. While LSTMs have been studied in the past for the NER task by <ref type="bibr" target="#b14">Hammerton (2003)</ref>, the lack of computational power (which led to the use  The CNN (Figure <ref type="figure" target="#fig_1">2</ref>) extracts a fixed length feature vector from character-level features. For each word, these vectors are concatenated and fed to the BLSTM network and then to the output layers (Figure <ref type="figure" target="#fig_2">3</ref>).</p><p>of very small models) and quality word embeddings limited their effectiveness. Convolutional neural networks (CNN) have also been investigated for modeling character-level information, among other NLP tasks. <ref type="bibr" target="#b32">Santos et al. (2015)</ref> and <ref type="bibr" target="#b18">Labeau et al. (2015)</ref> successfully employed CNNs to extract character-level features for use in NER and POS-tagging respectively. <ref type="bibr" target="#b8">Collobert et al. (2011b)</ref> also applied CNNs to semantic role labeling, and variants of the architecture have been applied to parsing and other tasks requiring tree structures <ref type="bibr" target="#b2">(Blunsom et al., 2014)</ref>. However, the effectiveness of character-level CNNs has not been evaluated for English NER. While we considered using character-level bi-directional LSTMs, which was recently proposed by <ref type="bibr" target="#b20">Ling et al. (2015)</ref> for POStagging, preliminary evaluation shows that it does not perform significantly better than CNNs while being more computationally expensive to train.</p><p>Our main contribution lies in combining these neural network models for the NER task. We present a hybrid model of bi-directional LSTMs and CNNs that learns both character-and word-level features, presenting the first evaluation of such an architecture on well-established English language evaluation datasets. Furthermore, as lexicons are crucial to NER performance, we propose a new lexicon encoding scheme and matching algorithm that can make use of partial matches, and we compare it to the simpler approach of <ref type="bibr" target="#b8">Collobert et al. (2011b)</ref>. Extensive evaluation shows that our proposed method establishes a new state of the art on both the CoNLL-2003 NER shared task and the OntoNotes 5.0 datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model</head><p>Our neural network is inspired by the work of <ref type="bibr" target="#b8">Collobert et al. (2011b)</ref>, where lookup tables transform discrete features such as words and characters into continuous vector representations, which are then concatenated and fed into a neural network. Instead of a feed-forward network, we use the bi-directional long-short term memory (BLSTM) network. To induce character-level features, we use a convolutional neural network, which has been successfully applied to Spanish and Portuguese NER <ref type="bibr" target="#b32">(Santos et al., 2015)</ref> and German POS-tagging <ref type="bibr" target="#b18">(Labeau et al., 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Sequence-labelling with BLSTM</head><p>Following the speech-recognition framework outlined by <ref type="bibr" target="#b13">Graves et al. (2013)</ref>, we employed a stacked 1 bi-directional recurrent neural network with long short-term memory units to transform word features into named entity tag scores. Figures <ref type="figure" target="#fig_1">1, 2</ref>, and 3 illustrate the network in detail.</p><p>The extracted features of each word are fed into a forward LSTM network and a backward LSTM network. The output of each network at each time step is decoded by a linear layer and a log-softmax layer into log-probabilities for each tag category. These two vectors are then simply added together to produce the final output.</p><p>We tried minor variants of output layer architecture and selected the one that performed the best in preliminary experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Extracting Character Features Using a Convolutional Neural Network</head><p>For each word we employ a convolution and a max layer to extract a new feature vector from the percharacter feature vectors such as character embeddings (Section 2.3.2) and (optionally) character type (Section 2.5). Words are padded with a number of special PADDING characters on both sides depending on the window size of the CNN.</p><p>The hyper-parameters of the CNN are the window size and the output vector size.</p><p>1 For each direction (forward and backward), the input is fed into multiple layers of LSTM units connected in sequence (i.e. LSTM units in the second layer take in the output of the first layer, and so on); the number of layers is a tuned hyperparameter. Figure <ref type="figure" target="#fig_0">1</ref> shows only one unit for simplicity. We also experimented with two other sets of published embeddings, namely Stanford's GloVe embeddings<ref type="foot" target="#foot_1">3</ref> trained on 6 billion words from Wikipedia and Web text <ref type="bibr" target="#b26">(Pennington et al., 2014</ref>) and Google's word2vec embeddings<ref type="foot" target="#foot_2">4</ref> trained on 100 billion words from Google News <ref type="bibr" target="#b23">(Mikolov et al., 2013)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Category</head><p>In addition, as we hypothesized that word embeddings trained on in-domain text may perform better, we also used the publicly available GloVe <ref type="bibr" target="#b26">(Pennington et al., 2014)</ref> program and an in-house re-implementation<ref type="foot" target="#foot_3">5</ref> of the word2vec <ref type="bibr" target="#b23">(Mikolov et al., 2013)</ref> program to train word embeddings on Wikipedia and Reuters RCV1 datasets as well. <ref type="foot" target="#foot_4">6</ref>Following <ref type="bibr" target="#b8">Collobert et al. (2011b)</ref>, all words are lower-cased before passing through the lookup table to convert to their corresponding embeddings. The pre-trained embeddings are allowed to be modified during training.<ref type="foot" target="#foot_5">7</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Character Embeddings</head><p>We randomly initialized a lookup table with values drawn from a uniform distribution with range [−0.5, 0.5] to output a character embedding of 25 dimensions. The character set includes all unique characters in the CoNLL-2003 dataset<ref type="foot" target="#foot_6">8</ref> plus the special tokens PADDING and UNKNOWN. The PADDING token is used for the CNN, and the UNKNOWN token is used for all other characters (which appear in OntoNotes). The same set of random embeddings was used for all experiments.<ref type="foot" target="#foot_7">9</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Additional Word-level Features</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1">Capitalization Feature</head><p>As capitalization information is erased during lookup of the word embedding, we evaluate Collobert's method of using a separate lookup table to add a capitalization feature with the following options: allCaps, upperInitial, lowercase, mixedCaps, noinfo <ref type="bibr" target="#b8">(Collobert et al., 2011b)</ref>. This method is compared with the character type feature (Section 2.5) and character-level CNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2">Lexicons</head><p>Most state of the art NER systems make use of lexicons as a form of external knowledge <ref type="bibr" target="#b30">(Ratinov and Roth, 2009;</ref><ref type="bibr" target="#b25">Passos et al., 2014)</ref>.</p><p>For each of the four categories (Person, Organization, Location, Misc) defined by the CoNLL 2003 NER shared task, we compiled a list of known named entities from DBpedia <ref type="bibr" target="#b1">(Auer et al., 2007)</ref>, by extracting all descendants of DBpedia types corresponding to the CoNLL categories. <ref type="foot" target="#foot_12">14</ref> We did not construct separate lexicons for the OntoNotes tagset because correspondences between DBpedia categories and its tags could not be found in many instances. In addition, for each entry we first removed parentheses and all text contained within, then stripped trailing punctuation,<ref type="foot" target="#foot_13">15</ref> and finally tokenized it with the Penn Treebank tokenization script for the purpose of partial matching. Table <ref type="table">1</ref> shows the size of each category in our lexicon compared to Collobert's lexicon, which we extracted from their SENNA system. Figure <ref type="figure">4</ref> shows an example of how the lexicon features are applied. <ref type="foot" target="#foot_14">16</ref> For each lexicon category, we match every n-gram (up to the length of the longest lexicon entry) against entries in the lexicon. A match is successful when the n-gram matches the prefix or suffix of an entry and is at least half the length of the entry. Because of the high potential for spurious matches, for all categories except Person, we discard partial matches less than 2 tokens in length. When there are multiple overlapping matches within the same category, we prefer exact matches over partial matches, and then longer matches over shorter matches, and finally earlier matches in the sentence over later matches. All matches are case insensitive.</p><p>For each token in the match, the feature is en-  coded in BIOES annotation (Begin, Inside, Outside, End, Single), indicating the position of the token in the matched entry. In other words, B will not appear in a suffix-only partial match, and E will not appear in a prefix-only partial match.</p><p>As we will see in Section 4.5, we found that this more sophisticated method outperforms the method presented by <ref type="bibr" target="#b8">Collobert et al. (2011b)</ref>, which treats partial and exact matches equally, allows prefix but not suffix matches, allows very short partial matches, and marks tokens with YES/ NO.</p><p>In addition, since <ref type="bibr" target="#b8">Collobert et al. (2011b)</ref> released their lexicon with their SENNA system, we also applied their lexicon to our model for comparison and investigated using both lexicons simultaneously as distinct features. We found that the two lexicons complement each other and improve performance on the CoNLL-2003 dataset.</p><p>Our best model uses the SENNA lexicon with exact matching and our DBpedia lexicon with partial matching, with BIOES annotation in both cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Additional Character-level Features</head><p>A lookup table was used to output a 4-dimensional vector representing the type of the character (upper case, lower case, punctuation, other).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Training and Inference</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.1">Implementation</head><p>We implement the neural network using the torch7 library <ref type="bibr" target="#b7">(Collobert et al., 2011a)</ref>. Training and inference are done on a per-sentence level. The initial states of the LSTM are zero vectors. Except for the character and word embeddings whose initialization has been described previously, all lookup tables are randomly initialized with values drawn from the standard normal distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.2">Objective Function and Inference</head><p>We train our network to maximize the sentencelevel log-likelihood from <ref type="bibr" target="#b8">Collobert et al. (2011b)</ref>. <ref type="foot" target="#foot_15">17</ref>First, we define a tag-transition matrix A where A i,j represents the score of jumping from tag i to tag j in successive tokens, and A 0,i as the score for starting with tag i. This matrix of parameters are also learned. Define θ as the set of parameters for the neural network, and θ = θ ∪ {A i,j ∀i, j} as the set of all parameters to be trained. Given an example sentence, [x] T 1 , of length T , and define [f θ ] i,t as the score outputted by the neural network for the t th word and i th tag given parameters θ, then the score of a sequence of tags [i] T 1 is given as the sum of network and transition scores: Then, letting [y] T 1 be the true tag sequence, the sentence-level log-likelihood is obtained by normalizing the above score over all possible tag-sequences [j] T 1 using a softmax:</p><formula xml:id="formula_0">S([x] T 1 , [i] T 1 , θ ) = T t=1 A [i] t−1 ,[i]t + [f θ ] [i]t,</formula><formula xml:id="formula_1">log P ([y] T 1 | [x] T 1 , θ ) = S([x] T 1 , [y] T 1 , θ ) − log ∀[j] T 1 e S([x] T 1 ,[j] T 1 ,θ )</formula><p>This objective function and its gradients can be efficiently computed by dynamic programming <ref type="bibr" target="#b8">(Collobert et al., 2011b)</ref>.</p><p>At inference time, given neural network outputs [f θ ] i,t we use the Viterbi algorithm to find the tag sequence</p><formula xml:id="formula_2">[i] T 1 that maximizes the score S([x] T 1 , [i] T 1 , θ ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.3">Tagging Scheme</head><p>The output tags are annotated with BIOES (which stand for Begin, Inside, Outside, End, Single, indicating the position of the token in the 18 OntoNotes results taken from <ref type="bibr" target="#b9">(Durrett and Klein, 2014)</ref> 19 Evaluation on OntoNotes 5.0 done by Pradhan et al. ( <ref type="formula">2013</ref>) 20 Not directly comparable as they evaluated on an earlier version of the corpus with a different data split. 21 Numbers taken from the original paper <ref type="bibr" target="#b21">(Luo et al., 2015)</ref>. While the precision, recall, and F1 scores are clearly inconsistent, it is unclear in which way they are incorrect. entity) as this scheme has been reported to outperform others such as BIO <ref type="bibr" target="#b30">(Ratinov and Roth, 2009)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.4">Learning Algorithm</head><p>Training is done by mini-batch stochastic gradient descent (SGD) with a fixed learning rate. Each mini-batch consists of multiple sentences with the same number of tokens. We found applying dropout to the output nodes<ref type="foot" target="#foot_16">22</ref> of each LSTM layer <ref type="bibr" target="#b28">(Pham et al., 2014)</ref> was quite effective in reducing overfitting (Section 4.4). We explored other more sophisticated optimization algorithms such as momentum <ref type="bibr" target="#b24">(Nesterov, 1983)</ref>, AdaDelta (Zeiler, 2012), and RM-SProp <ref type="bibr" target="#b15">(Hinton et al., 2012)</ref>, and in preliminary experiments they did not improve upon plain SGD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation</head><p>Evaluation was performed on the well-established CoNLL-2003 NER shared task dataset (Tjong Kim Sang and De <ref type="bibr" target="#b34">Meulder, 2003)</ref> and the much larger but less-studied OntoNotes 5.0 dataset <ref type="bibr" target="#b16">(Hovy et al., 2006;</ref><ref type="bibr" target="#b29">Pradhan et al., 2013)</ref>. Table <ref type="table">2</ref> gives an overview of these two different datasets.</p><p>For each experiment, we report the average and standard deviation of 10 successful trials. Table <ref type="table">6</ref>: F1 score results of BLSTM and BLSTM-CNN models with various additional features; emb = Collobert word embeddings, char = character type feature, caps = capitalization feature, lex = lexicon features. Note that starred results are repeated for ease of comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset Preprocessing</head><p>For all datasets, we performed the following preprocessing:</p><p>• All digit sequences are replaced by a single "0".</p><p>• Before training, we group sentences by word length into mini-batches and shuffle them.</p><p>In addition, for the OntoNotes dataset, in order to handle the Date, Time, Money, Percent, Quantity, Ordinal, and Cardinal named entity tags, we split tokens before and after every digit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">CoNLL 2003 Dataset</head><p>The CoNLL-2003 dataset (Tjong Kim Sang and De <ref type="bibr" target="#b34">Meulder, 2003)</ref> consists of newswire from the Reuters RCV1 corpus tagged with four types of named entities: location, organization, person, and miscellaneous. As the dataset is small compared to OntoNotes, we trained the model on both the training and development sets after performing hyperparameter optimization on the development set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">OntoNotes 5.0 Dataset</head><p>Pradhan et al. ( <ref type="formula">2013</ref>) compiled a core portion of the OntoNotes 5.0 dataset for the CoNLL-2012 shared task and described a standard train/dev/test split, which we use for our evaluation. Following <ref type="bibr" target="#b9">Durrett and Klein (2014)</ref>, we applied our model to the portion of the dataset with gold-standard named entity annotations; the New Testaments portion was excluded for lacking gold-standard annotations. This dataset is much larger than CoNLL-2003 and consists of text from a wide variety of sources, such as broadcast conversation, broadcast news, newswire, magazine, telephone conversation, and Web text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Hyper-parameter Optimization</head><p>We performed two rounds of hyper-parameter optimization and selected the best settings based on development set performance 23 . Table <ref type="table" target="#tab_4">3</ref> shows the final hyper-parameters, and Table <ref type="table" target="#tab_5">4</ref> shows the dev set performance of the best models in each round.</p><p>In the first round, we performed random search and selected the best hyper-parameters over the development set of the CoNLL-2003 data. We evaluated around 500 hyper-parameter settings. Then, we took the same settings and tuned the learning rate and epochs on the OntoNotes development set. 24  For the second round, we performed independent hyper-parameter searches on each dataset using Optunity's implementation of particle swarm <ref type="bibr">(Claesen et al., )</ref>, as there is some evidence that it is more efficient than random search <ref type="bibr" target="#b5">(Clerc and Kennedy, 2002)</ref>. We evaluated 500 hyper-parameter settings this round as well. As we later found out that training fails occasionally (Section 3.5) as well as large variation from run to run, we ran the top 5 settings from each dataset for 10 trials each and selected the best one based on averaged dev set performance.</p><p>For CoNLL-2003, we found that particle swarm produced better hyper-parameters than random search. However, surprisingly for OntoNotes particle swarm was unable to produce better hyperparameters than those from the ad-hoc approach in round 1. We also tried tuning the CoNLL-2003 hyper-parameters from round 2 for OntoNotes and that was not any better 25 either.</p><p>We trained CoNLL-2003 models for a large num-23 Hyper-parameter optimization was done with the BLSTM-CNN + emb + lex feature set, as it had the best performance.</p><p>24 Selected based on dev set performance of a few runs. 25 The result is 84.41 (± 0.33) on the OntoNotes dev set.  <ref type="table">7</ref>: F1 scores when the Collobert word vectors are replaced. We tried 50-and 300-dimensional random vectors (Random 50d, Random 300d); GloVe's released vectors trained on 6 billion words (GloVe 6B 50d, GloVe 6B 300d); Google's released 300-dimensional vectors trained on 100 billion words from Google News (Google 100B 300d); and 50-dimensional GloVe and word2vec skip-gram vectors that we trained on Wikipedia and Reuters RCV-1 (Our GloVe 50d, Our Skip-gram 50d). ber of epochs because we observed that the models did not exhibit overtraining and instead continued to slowly improve on the development set long after reaching near 100% accuracy on the training set. In contrast, despite OntoNotes being much larger than <ref type="bibr">CoNLL-2003, training</ref> for more than about 18 epochs causes performance on the development set to decline steadily due to overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Excluding Failed Trials</head><p>On the CoNLL-2003 dataset, while BLSTM models completed training without difficulty, the BLSTM-CNN models fail to converge around 5∼10% of the time depending on feature set. Similarly, on OntoNotes, 1.5% of trials fail. We found that using a lower learning rate reduces failure rate. We also tried clipping gradients and using AdaDelta and both of them were effective at eliminating such failures by themselves. AdaDelta, however, made training more expensive with no gain in model performance.</p><p>In any case, for all experiments we excluded trials where the final F1 score on a subset of training data falls below a certain threshold, and continued to run trials until we obtained 10 successful ones.</p><p>For CoNLL-2003, we excluded trials where the final F1 score on the development set was less than 95; there was no ambiguity in selecting the threshold as every trial scored either above 98 or below 90. For OntoNotes, the threshold was a F1 score of 80 on the last 5,000 sentences of the training set; every trial scored either above 80 or below 75.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Training and Tagging Speed</head><p>On an Intel Xeon E5-2697 processor, training takes about 6 hours while tagging the test set takes about 12 seconds for CoNLL-2003. The times are 10 hours and 60 seconds respectively for OntoNotes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Discussion</head><p>Table <ref type="table" target="#tab_6">5</ref> shows the results for all datasets. To the best of our knowledge, our best models have surpassed the previous highest reported F1 scores for both CoNLL-2003 and OntoNotes. In particular, with no external knowledge other than word embeddings, our model is competitive on the CoNLL-2003 dataset and establishes a new state of the art for OntoNotes, suggesting that given enough data, the neural network automatically learns the relevant features for NER without feature engineering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Comparison with FFNNs</head><p>We re-implemented the FFNN model of <ref type="bibr" target="#b8">Collobert et al. (2011b)</ref> as a baseline for comparison. Table 5 shows that while performing reasonably well on CoNLL-2003, FFNNs are clearly inadequate for OntoNotes, which has a larger domain, showing that LSTM models are essential for NER.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Character-level CNNs vs. Character Type and Capitalization Features</head><p>The comparison of models in Table <ref type="table">6</ref> shows that on CoNLL-2003, BLSTM-CNN models significantly<ref type="foot" target="#foot_17">26</ref> outperform the BLSTM models when given the same feature set. This effect is smaller and not statistically significant on OntoNotes when capitalization features are added. Adding character type and capitalization features to the BLSTM-CNN models degrades performance for CoNLL and mostly improves performance on OntoNotes, suggesting character-level CNNs can replace hand-crafted character features in some cases, but systems with weak lexicons may benefit from character features. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Word Embeddings</head><p>Table <ref type="table" target="#tab_6">5</ref> and Table <ref type="table">7</ref> show that we obtain a large, significant 27 improvement when trained word embeddings are used, as opposed to random embeddings, regardless of the additional features used. This is consistent with <ref type="bibr" target="#b8">Collobert et. al. (2011b)</ref>'s results. Table <ref type="table">7</ref> compares the performance of different word embeddings in our best model in Table <ref type="table" target="#tab_6">5</ref> (BLSTM-CNN + emb + lex). For CoNLL-2003, the publicly available GloVe and Google embeddings are about one point behind Collobert's embeddings. For OntoNotes, GloVe embeddings perform close to Collobert embeddings while Google embeddings are again one point behind. In addition, 300 dimensional embeddings present no significant improvement over 50 dimensional embeddings -a result previously reported by <ref type="bibr" target="#b35">Turian et al. (2010)</ref>.</p><p>One possible reason that Collobert embeddings perform better than other publicly available embeddings on CoNLL-2003 is that they are trained on the Reuters RCV-1 corpus, the source of the CoNLL-2003 dataset, whereas the other embeddings are not 28 . On the other hand, we suspect that Google's embeddings perform poorly because of vocabulary mismatch -in particular, Google's embeddings were trained in a case-sensitive manner, and embeddings for many common punctuations and 27 Wilcoxon rank sum test, p &lt; 0.001 28 To make a direct comparison to <ref type="bibr" target="#b8">Collobert et al. (2011b)</ref>, we do not exclude the CoNLL-2003 NER task test data from the word vector training data. While it is possible that this difference could be responsible for the disparate performance of word vectors, the CoNLL-2003 training data comprises only 20k out of 800 million words, or 0.00002% of the total data; in an unsupervised training scheme, the effects are likely negligible. symbols were not provided. To test these hypotheses, we performed experiments with new word embeddings trained using GloVe and word2vec, with vocabulary list and corpus similar to <ref type="bibr" target="#b8">Collobert et. al. (2011b)</ref>. As shown in Table <ref type="table">7</ref>, our GloVe embeddings improved significantly<ref type="foot" target="#foot_18">29</ref> over publicly available embeddings on CoNLL-2003, and our word2vec skip-gram embeddings improved significantly<ref type="foot" target="#foot_19">30</ref> over Google's embeddings on OntoNotes.</p><p>Due to time constraints we did not perform new hyper-parameter searches with any of the word embeddings. As word embedding quality depends on hyper-parameter choice during their training <ref type="bibr" target="#b26">(Pennington et al., 2014)</ref>, and also, in our NER neural network, hyper-parameter choice is likely sensitive to the type of word embeddings used, optimizing them all will likely produce better results and provide a fairer comparison of word embedding quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Effect of Dropout</head><p>Table <ref type="table" target="#tab_9">8</ref> compares the result of various dropout values for each dataset. The models are trained using only the training set for each dataset to isolate the effect of dropout on both dev and test sets. All other hyper-parameters and features remain the same as our best model in Table <ref type="table" target="#tab_6">5</ref>. In both datasets and on both dev and test sets, dropout is essential for state of the art performance, and the improvement is statistically significant<ref type="foot" target="#foot_20">31</ref> . Dropout is optimized on the dev set, as described in Section 3.4. Hence, the chosen  value may not be the best-performing in Table <ref type="table" target="#tab_9">8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Lexicon Features</head><p>Table <ref type="table">6</ref> shows that on the CoNLL-2003 dataset, using features from both the SENNA lexicon and our proposed DBpedia lexicon provides a significant 32 improvement and allows our model to clearly surpass the previous state of the art. Unfortunately the difference is minuscule for OntoNotes, most likely because our lexicon does not match DBpedia categories well. Figure <ref type="figure" target="#fig_3">5</ref> shows that on CoNLL-2003, lexicon coverage is reasonable and matches the tags set for everything except the catchall MISC category. For example, LOC entries in lexicon match mostly LOC named entities and vice versa. However, on OntoNotes, the matches are noisy and correspondence between lexicon match and tag category is quite ambiguous. For example, all lexicon categories have spurious matches in unrelated named entities like CARDINAL, and LOC, GPE, and LANGUAGE entities all get a lot of matches from the LOC category in the lexicon. In addition, named entities in categories like NORP, ORG, LAW, PRODUCT receive little coverage. The lower coverage, noise, and ambiguity all contribute to the disappointing performance. This suggests that the DBpedia lexicon construction method needs to be improved. A reasonable place to start would be the DBpedia category to OntoNotes NE tag mappings.</p><p>In order to isolate the contribution of each lexicon and matching method, we compare different sources and matching methods on a BLSTM-CNN model with randomly initialized word embeddings and no 32 Wilcoxon rank sum test, p &lt; 0.001.</p><p>other features or sources of external knowledge. Table <ref type="table" target="#tab_11">9</ref> shows the results. In this weakened model, both lexicons contribute significant 33 improvements over the baseline.</p><p>Compared to the SENNA lexicon, our DBpedia lexicon is noisier but has broader coverage, which explains why when applying it using the same method as <ref type="bibr" target="#b8">Collobert et al. (2011b)</ref>, it performs worse on CoNLL-2003 but better on OntoNotesa dataset containing many more obscure named entities. However, we suspect that the method of Collobert et al. ( <ref type="formula">2011b</ref>) is not noise resistant and therefore unsuitable for our lexicon because it fails to distinguish exact and partial matches 34 and does not set a minimum length for partial matching. 35 Instead, when we apply our superior partial matching algorithm and BIOES encoding with our DBpedia lexicon, we gain a significant 36 improvement, allowing our lexicon to perform similarly to the SENNA lexicon. Unfortunately, as we could not reliably remove partial entries from the SENNA lexicon, we were unable to investigate whether or not our lexicon matching method would help in that lexicon.</p><p>In addition, using both lexicons together as distinct features provides a further improvement 37 on CoNLL-2003, which we suspect is because the lexi- cons are complementary; the SENNA lexicon is relatively clean and tailored to newswire, whereas the DBpedia lexicon is noisier but has high coverage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Analysis of OntoNotes Performance</head><p>Table <ref type="table">10</ref> shows the per-genre breakdown of the OntoNotes results. As expected, our model performs best on clean text like broadcast news (BN) and newswire (NW), and worst on noisy text like telephone conversation (TC) and Web text (WB).</p><p>Our model also substantially improves over previous work on all genres except TC, where the small size of the training data likely hinders learning. Finally, the performance characteristics of our model appear to be quite different than the previous CRF models <ref type="bibr" target="#b10">(Finkel and Manning, 2009;</ref><ref type="bibr" target="#b9">Durrett and Klein, 2014)</ref>, likely because we apply a completely different machine learning method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Research</head><p>Named entity recognition is a task with a long history. In this section, we summarize the works we compare with and that influenced our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Named Entity Recognition</head><p>Most recent approaches to NER have been characterized by the use of CRF, SVM, and perceptron models, where performance is heavily dependent on feature engineering. <ref type="bibr" target="#b30">Ratinov and Roth (2009)</ref> used non-local features, a gazetteer extracted from 38 We downloaded their publicly released software and model to perform the per-genre evaluation.</p><p>Wikipedia, and Brown-cluster-like word representations, and achieved an F1 score of 90.80 on <ref type="bibr">CoNLL-2003. Lin and</ref><ref type="bibr" target="#b19">Wu (2009)</ref> surpassed them without using a gazetteer by instead using phrase features obtained by performing k-means clustering over a private database of search engine query logs. <ref type="bibr" target="#b25">Passos et al. (2014)</ref>  Training an NER system together with related tasks such as entity linking has recently been shown to improve the state of the art. <ref type="bibr" target="#b9">Durrett and Klein (2014)</ref> combined coreference resolution, entity linking, and NER into a single CRF model and added cross-task interaction factors. Their system achieved state of the art results on the OntoNotes dataset, but they did not evaluate on the CoNLL-2003 dataset due to lack of coreference annotations. <ref type="bibr" target="#b21">Luo et al. (2015)</ref> achieved state of the art results on CoNLL-2003 by training a joint model over the NER and entity linking tasks, the pair of tasks whose interdependencies contributed the most to the work of <ref type="bibr" target="#b9">Durrett and Klein (2014)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">NER with Neural Networks</head><p>While many approaches involve CRF models, there has also been a long history of research involving neural networks. Early attempts were hindered by</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: The (unrolled) BLSTM for tagging named entities. Multiple tables look up word-level feature vectors. The CNN (Figure2) extracts a fixed length feature vector from character-level features. For each word, these vectors are concatenated and fed to the BLSTM network and then to the output layers (Figure3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The convolutional neural network extracts character features from each word. The character embedding and (optionally) the character type feature vector are computed through lookup tables. Then, they are concatenated and passed into the CNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: The output layers ("Out" in Figure1) decode output into a score for each tag category.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Fraction of named entities of each tag category matched completely by entries in each lexicon category of the SENNA/DBpedia combined lexicon. White = higher fraction.</figDesc><graphic url="image-1.png" coords="10,171.51,113.59,313.13,65.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>obtained nearly the same performance using only public data by training phrase vectors in their lexicon-infused skip-gram model. In order to combat the problem of sparse features, Suzuki et al. (2011) employed large-scale unlabelled data to perform feature reduction and achieved an F1 score of 91.02 on CoNLL-2003, which is the current state of the art for systems without external knowledge.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Example of how lexicon features are applied. The B, I, E, markings indicate that the token matches the Begin, Inside, and End token of an entry in the lexicon. S indicates that the token matches a single-token entry.</figDesc><table><row><cell>Text</cell><cell cols="11">Hayao Tada , commander of the Japanese North China Area Army</cell></row><row><cell>LOC</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>B</cell><cell>I</cell><cell>-</cell><cell>S</cell><cell>-</cell><cell>-</cell></row><row><cell>MISC</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>S</cell><cell>B</cell><cell>B</cell><cell>I</cell><cell>S</cell><cell>S</cell><cell>S</cell><cell>S</cell></row><row><cell>ORG</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>B</cell><cell>I</cell><cell>B</cell><cell>I</cell><cell>I</cell><cell>E</cell></row><row><cell>PERS</cell><cell>B</cell><cell>E</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>S</cell><cell>-</cell><cell>-</cell></row><row><cell>Figure 4:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Hyper-parameter search space and final values used for all experiments</figDesc><table><row><cell>Round 1 2</cell><cell>CoNLL-2003 OntoNotes 5.0 93.82 (± 0.15) 84.57 (± 0.27) 94.03 (± 0.23) 84.47 (± 0.29)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Development set F1 score performance of the best hyper-parameter settings in each optimization round.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>t Results of our models, with various feature sets, compared to other published results. The three sections are, in order, our models, published neural network models, and published non-neural network models. For the features, emb = Collobert word embeddings, caps = capitalization feature, lex = lexicon features from both SENNA and DBpedia lexicons. For F1 scores, standard deviations are in parentheses.</figDesc><table><row><cell>Model FFNN + emb + caps + lex BLSTM BLSTM-CNN BLSTM-CNN + emb BLSTM-CNN + emb + lex Collobert et al. (2011b) Collobert et al. (2011b) + lexicon Huang et al. (2015) Ratinov and Roth (2009) 18 Lin and Wu (2009) Finkel and Manning (2009) 19 Suzuki et al. (2011) Passos et al. (2014) 20 Durrett and Klein (2014) Luo et al. (2015) 21</cell><cell>CoNLL-2003 Prec. Recall 89.54 89.80 89.67 (± 0.24) 74.28 73.61 73.94 (± 0.43) OntoNotes 5.0 F1 Prec. Recall F1 80.14 72.81 76.29 (± 0.29) 79.68 75.97 77.77 (± 0.37) 83.48 83.28 83.38 (± 0.20) 82.58 82.49 82.53 (± 0.40) 90.75 91.08 90.91 (± 0.20) 85.99 86.36 86.17 (± 0.22) 91.39 91.85 91.62 (± 0.33) 86.04 86.53 86.28 (± 0.26) --88.67 -----89.59 -----90.10 ---91.20 90.50 90.80 82.00 84.95 83.45 --90.90 ------84.04 80.86 82.42 --91.02 -----90.90 --82.24 ---85.22 82.89 84.04 91.50 91.40 91.20 ---</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>F1 score results with various dropout values. Models were trained using only the training set for each dataset. All other experiments use dropout = 0.68 for CoNLL-2003 and dropout = 0.63 for OntoNotes 5.0.</figDesc><table><row><cell>Dropout -0.10 0.30 0.50 0.63 0.68 0.70 0.90</cell><cell>CoNLL-2003 Dev Test 93.72 (± 0.10) 90.76 (± 0.22) 82.02 (± 0.49) 84.06 (± 0.50) OntoNotes 5.0 Dev Test 93.85 (± 0.18) 90.87 (± 0.31) 83.01 (± 0.39) 84.94 (± 0.25) 94.08 (± 0.17) 91.09 (± 0.18) 83.61 (± 0.32) 85.44 (± 0.33) 94.19 (± 0.18) 91.14 (± 0.35) 84.35 (± 0.23) 86.36 (± 0.28) --84.47 (± 0.23) 86.29 (± 0.25) 94.31 (± 0.15) 91.23 (± 0.16) --94.31 (± 0.24) 91.17 (± 0.37) 84.56 (± 0.40) 86.17 (± 0.25) 94.17 (± 0.17) 90.67 (± 0.17) 81.38 (± 0.19) 82.16 (± 0.18)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc>Comparison of lexicon and matching/encoding methods over the BLSTM-CNN model employing random embeddings and no other features. When using both lexicons, the best combination of matching and encoding is Exact-BIOES for SENNA and Partial-BIOES for DBpedia. Note that the SENNA lexicon already contains "partial entries" so exact matching in that case is really just a more primitive form of partial matching.</figDesc><table><row><cell>Lexicon No lexicon</cell><cell cols="3">Matching Encoding CoNLL-2003 --83.38 (± 0.20) 82.53 (± 0.40) OntoNotes</cell></row><row><cell>SENNA</cell><cell>Exact Exact</cell><cell>YN BIOES</cell><cell>86.21 (± 0.39) 83.24 (± 0.33) 86.14 (± 0.48) 83.01 (± 0.52)</cell></row><row><cell>DBpedia Both</cell><cell cols="2">Exact Exact Partial Partial Collobert's method YN BIOES YN BIOES Best combination</cell><cell>84.93 (± 0.30) 83.15 (± 0.26) 85.02 (± 0.23) 83.39 (± 0.39) 85.72 (± 0.45) 83.25 (± 0.33) 86.18 (± 0.56) 83.97 (± 0.38) 85.01 (± 0.31) 83.24 (± 0.26) 87.77 (± 0.29) 83.82 (± 0.19)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">http://ml.nec-labs.com/senna/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">http://nlp.stanford.edu/projects/glove/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2">https://code.google.com/p/word2vec/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3">We used our in-house reimplementation to train word vectors because it uses distributed processing to train much quicker than the publicly-released implementation of word2vec and its performance on the word analogy task was higher than reported by<ref type="bibr" target="#b23">Mikolov et al. (2013)</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4">  6  While<ref type="bibr" target="#b8">Collobert et al. (2011b)</ref> used Wikipedia text from 2007, we used Wikipedia text from 2011.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5">Preliminary experiments showed that modifiable vectors performed better than so-called "frozen vectors."</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6">Upper and lower case letters, numbers, and punctuations</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_7">We did not experiment with other settings because the English character set is small enough that effective embeddings could be learned directly from the task data.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_8">10 By increments of</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_9">50.  11  Determined by evaluating dev set</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_10">performance. 12 Probability of discarding any</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_11">LSTM output node.13  Mini-batch size was excluded from the round 2 particle swarm hyper-parameter search space due to time constraints.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14" xml:id="foot_12">The Miscellaneous category was populated by entities of the DBpedia categories Artifact and Work.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15" xml:id="foot_13">The punctuation stripped was period, comma, semi-colon, colon, forward slash, backward slash, and question mark.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16" xml:id="foot_14">As can been seen in this example, the lexicons -in particular Miscellaneous -still contain a lot of noise.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="17" xml:id="foot_15">Much later, we discovered that training with cross entropy objective while performing Viterbi decoding to restrict output to valid tag sequences also appears to work just as well.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="22" xml:id="foot_16">Adding dropout to inputs seems to have an adverse effect.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="26" xml:id="foot_17">Wilcoxon rank sum test, p &lt; 0.05 when comparing the four BLSTM models with the corresponding BLSTM-CNN models using the same feature set. The Wilcoxon rank sum test was selected for its robustness against small sample sizes when the distribution is unknown.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="29" xml:id="foot_18">Wilcoxon rank sum test, p &lt; 0.01</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="30" xml:id="foot_19">Wilcoxon rank sum test, p &lt; 0.01</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="31" xml:id="foot_20">Wilcoxon rank sum test, no dropout vs. best setting: p &lt; 0.001 for the CoNLL-2003 test set, p &lt; 0.0001 for the OntoNotes 5.0 test set, p &lt; 0.0005 for all others.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was supported by Honda Research Institute Japan Co., Ltd. The authors would like to thank <ref type="bibr" target="#b8">Collobert et al. (2011b)</ref> for releasing SENNA with its word vectors and lexicon, the torch7 framework contributors, and Andrey Karpathy for the reference LSTM implementation.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="bibr" target="#b14">Hammerton (2003)</ref> <p>attempted NER with a singledirection LSTM network and a combination of word vectors trained using self-organizing maps and context vectors obtained using principle component analysis. However, while our method optimizes loglikelihood and uses softmax, they used a different output encoding and optimized an unspecified objective function. <ref type="bibr" target="#b14">Hammerton's (2003)</ref> reported results were only slightly above baseline models.</p><p>Much later, with the advent of neural word embeddings, <ref type="bibr" target="#b8">Collobert et al. (2011b)</ref> presented SENNA, which employs a deep FFNN and word embeddings to achieve near state of the art results on POS tagging, chunking, NER, and SRL. We build on their approach, sharing the word embeddings, feature encoding method, and objective functions.</p><p>Recently, <ref type="bibr" target="#b32">Santos et al. (2015)</ref> presented their CharWNN network, which augments the neural network of <ref type="bibr" target="#b8">Collobert et al. (2011b)</ref> with character level CNNs, and they reported improved performance on Spanish and Portuguese NER. We have successfully incorporated character-level CNNs into our model.</p><p>There have been various other similar architecture proposed for various sequential labeling NLP tasks. <ref type="bibr" target="#b17">Huang et al. (2015)</ref> used a BLSTM for the POS-tagging, chunking, and NER tasks, but they employed heavy feature engineering instead of using a CNN to automatically extract characterlevel features. <ref type="bibr" target="#b18">Labeau et al. (2015)</ref> used a BRNN with character-level CNNs to perform German POStagging; our model differs in that we use the more powerful LSTM unit, which we found to perform better than RNNs in preliminary experiments, and that we employ word embeddings, which is much more important in NER than in POS tagging. <ref type="bibr" target="#b20">Ling et al. (2015)</ref> used both word-and character-level BLSTMs to establish the current state of the art for English POS tagging. While using BLSTMs instead of CNNs allows extraction of more sophisticated character-level features, we found in preliminary experiments that for NER it did not perform significantly better than CNNs and was substantially more computationally expensive to train.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have shown that our neural network model, which incorporates a bidirectional LSTM and a character-level CNN and which benefits from robust training through dropout, achieves state-of-the-art results in named entity recognition with little feature engineering. Our model improves over previous best reported results on two major datasets for NER, suggesting that the model is capable of learning complex relationships from large amounts of data.</p><p>Preliminary evaluation of our partial matching lexicon algorithm suggests that performance could be further improved through more flexible application of existing lexicons. Evaluation of existing word embeddings suggests that the domain of training data is as important as the training algorithm.</p><p>More effective construction and application of lexicons and word embeddings are areas that require more research. In the future, we would also like to extend our model to perform similar tasks such as extended tagset NER and entity linking.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">for all others. 34 We achieve this by using BIOES encoding and prioritizing exact matches over partial matches. 35 Matching only the first word of a long entry is not very useful; this is not a problem in the SENNA lexicon because 99% of its entries contain only 3 tokens or less. 36 Wilcoxon rank sum test</title>
		<idno>&lt; 0.001. 37</idno>
	</analytic>
	<monogr>
		<title level="m">Wilcoxon rank sum test</title>
				<imprint>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
	<note>Wilcoxon rank sum test</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">DBpedia: A nucleus for a web of open data</title>
		<author>
			<persName><forename type="first">Sören</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Bizer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgi</forename><surname>Kobilarov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Cyganiak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Ives</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The semantic web</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="722" to="735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On the properties of neural machine translation: Encoder-decoder approaches</title>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation</title>
				<meeting>SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="103" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Easy hyperparameter search using Optunity</title>
		<author>
			<persName><forename type="first">Marc</forename><surname>Claesen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaak</forename><surname>Simm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dusan</forename><surname>Popovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yves</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart</forename><forename type="middle">De</forename><surname>Moor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Technical Computing for Machine Learning and Mathematical Engineering</title>
				<meeting>the International Workshop on Technical Computing for Machine Learning and Mathematical Engineering</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The particle swarm-explosion, stability, and convergence in a multidimensional complex space. Evolutionary Computation</title>
		<author>
			<persName><forename type="first">Maurice</forename><surname>Clerc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Kennedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="58" to="73" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine Learning</title>
				<meeting>the 25th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Torch7: A Matlab-like environment for machine learning</title>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clément</forename><surname>Farabet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of BigLearn, NIPS Workshop, number EPFL-CONF-192376</title>
				<meeting>BigLearn, NIPS Workshop, number EPFL-CONF-192376</meeting>
		<imprint>
			<date type="published" when="2011">2011a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011">2011b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A joint model for entity analysis: Coreference, typing, and linking</title>
		<author>
			<persName><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="477" to="490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Joint parsing and named entity recognition</title>
		<author>
			<persName><forename type="first">Jenny</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Rose</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
				<meeting>Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="326" to="334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning to forget: Continual prediction with LSTM</title>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Felix A Gers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fred</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName><surname>Cummins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2451" to="2471" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning task-dependent distributed representations by backpropagation through structure</title>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Goller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Kuchler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1996">1996. 1996</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="347" to="352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName><forename type="first">Alan</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdel-Rahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
				<meeting>the 2013 IEEE International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Named entity recognition with long short-term memory</title>
		<author>
			<persName><forename type="first">James</forename><surname>Hammerton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003</title>
				<meeting>the Seventh Conference on Natural Language Learning at HLT-NAACL 2003</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="172" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Lecture 6e: RMSProp: divide the gradient by a running average of its recent magnitude</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<ptr target="http://www.cs.toronto.edu/˜tijmen/csc321/slides/lecture_slides_lec6.pdf" />
	</analytic>
	<monogr>
		<title level="m">Neural Networks for Machine Learning</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">OntoNotes: the 90% solution</title>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lance</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers</title>
				<meeting>the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="57" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Bidirectional LSTM-CRF models for sequence tagging</title>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<idno>CoRR, abs/1508.01991</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Non-lexical neural architecture for fine-grained POS tagging</title>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Labeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Löser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Allauzen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="232" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Phrase clustering for discriminative learning</title>
		<author>
			<persName><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyun</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
				<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1030" to="1038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Finding function in form: Compositional character models for open vocabulary word representation</title>
		<author>
			<persName><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabel</forename><surname>Trancoso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramon</forename><surname>Fermandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Marujo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiago</forename><surname>Luis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1520" to="1530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Joint entity recognition and disambiguation</title>
		<author>
			<persName><forename type="first">Gang</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaiqing</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="879" to="888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">RNNLM-recurrent neural network language modeling toolkit</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Kombrink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anoop</forename><surname>Deoras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukar</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 ASRU Workshop</title>
				<meeting>the 2011 ASRU Workshop</meeting>
		<imprint>
			<date type="published" when="2011">Jan Cernocky. 2011</date>
			<biblScope unit="page" from="196" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-seventh Annual Conference on Advances in Neural Information Processing Systems</title>
				<meeting>the Twenty-seventh Annual Conference on Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A method of solving a convex programming problem with convergence rate O(1/k 2 )</title>
		<author>
			<persName><forename type="first">Yurii</forename><surname>Nesterov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Soviet Mathematics Doklady</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="372" to="376" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Lexicon infused phrase embeddings for named entity resolution</title>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vineet</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth Conference on Computational Natural Language Learning</title>
				<meeting>the Eighteenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="78" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Symbolic and neural learning for named-entity recognition</title>
		<author>
			<persName><surname>Petasis</surname></persName>
		</author>
		<author>
			<persName><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName><surname>Paliouras</surname></persName>
		</author>
		<author>
			<persName><surname>Karkaletsis</surname></persName>
		</author>
		<author>
			<persName><surname>Sj Perantonis</surname></persName>
		</author>
		<author>
			<persName><surname>Spyropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Symposium on Computational Intelligence and Learning</title>
				<meeting>the Symposium on Computational Intelligence and Learning</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="58" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dropout improves recurrent neural networks for handwriting recognition</title>
		<author>
			<persName><forename type="first">Théodore</forename><surname>Vu Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Bluche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jérôme</forename><surname>Kermorvant</surname></persName>
		</author>
		<author>
			<persName><surname>Louradour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Conference on Frontiers in Handwriting Recognition</title>
				<meeting>the 14th International Conference on Frontiers in Handwriting Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="285" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Towards robust linguistic analysis using OntoNotes</title>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Sameer Pradhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nianwen</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tou</forename><surname>Hwee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anders</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Björkelund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Uryupina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth Conference on Computational Natural Language Learning</title>
				<meeting>the Seventeenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="143" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Design challenges and misconceptions in named entity recognition</title>
		<author>
			<persName><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth Conference on Computational Natural Language Learning</title>
				<meeting>the Thirteenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="147" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName><forename type="first">David</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronald</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="page" from="323" to="533" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Boosting named entity recognition with neural character embeddings</title>
		<author>
			<persName><forename type="first">Cıcero</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Guimaraes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rio</forename><surname>Niterói</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janeiro</forename><surname>De</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth Named Entities Workshop</title>
				<meeting>the Fifth Named Entities Workshop</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="25" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning condensed feature representations from large unsupervised data sets for supervised learning</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hideki</forename><surname>Isozaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Short Papers</title>
				<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Short Papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="636" to="641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2003 shared task: Languageindependent named entity recognition</title>
		<author>
			<persName><forename type="first">Erik</forename><forename type="middle">F</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Tjong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sang</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Fien</forename><surname>De Meulder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003</title>
				<meeting>the Seventh Conference on Natural Language Learning at HLT-NAACL 2003</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Word representations: a simple and general method for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 48th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">ADADELTA: an adaptive learning rate method</title>
		<author>
			<persName><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName><surname>Zeiler</surname></persName>
		</author>
		<idno>CoRR, abs/1212.5701</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
