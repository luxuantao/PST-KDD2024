<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hardware/Software Co-Design for TinyML Voice-Recognition Application on Resource Frugal Edge Devices</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-11-22">22 November 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jisu</forename><surname>Kwon</surname></persName>
							<idno type="ORCID">0000-0002-0433-9533</idno>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic and Electrical Engineering</orgName>
								<orgName type="institution">Kyungpook National University</orgName>
								<address>
									<postCode>41566</postCode>
									<settlement>Daegu</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daejin</forename><surname>Park</surname></persName>
							<idno type="ORCID">0000-0002-5560-873X</idno>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic and Electrical Engineering</orgName>
								<orgName type="institution">Kyungpook National University</orgName>
								<address>
									<postCode>41566</postCode>
									<settlement>Daegu</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Electronics Engineering</orgName>
								<orgName type="institution">Kyungpook National University</orgName>
								<address>
									<postCode>41566</postCode>
									<settlement>Daegu</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Hardware/Software Co-Design for TinyML Voice-Recognition Application on Resource Frugal Edge Devices</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-11-22">22 November 2021</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.3390/app112211073</idno>
					<note type="submission">Received: 23 October 2021 Accepted: 19 November 2021</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>TinyML</term>
					<term>embedded system</term>
					<term>field programmable gate array (FPGA)</term>
					<term>microcontroller unit</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>On-device artificial intelligence has attracted attention globally, and attempts to combine the internet of things and TinyML (machine learning) applications are increasing. Although most edge devices have limited resources, time and energy costs are important when running TinyML applications. In this paper, we propose a structure in which the part that preprocesses externally input data in the TinyML application is distributed to the hardware. These processes are performed using software in the microcontroller unit of an edge device. Furthermore, resistor-transistor logic, which perform not only windowing using the Hann function, but also acquire audio raw data, is added to the inter-integrated circuit sound module that collects audio data in the voice-recognition application. As a result of the experiment, the windowing function was excluded from the TinyML application of the embedded board. When the length of the hardware-implemented Hann window is 80 and the quantization degree is 2 −5 , the exclusion causes a decrease in the execution time of the front-end function and energy consumption by 8.06% and 3.27%, respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Machine-learning (ML) applications based on neural networks are expanding dramatically. A neural network consists of connections between layers and a weight indicating the connectivity between nodes. Machine-learning is used in various fields, and the size and depth of networks are increasing, which is necessary for the network to cope with complex inputs. Typically, when the number of layers increases in deep learning, the number of weights approaches billions. In large-scale ML, the computation must be processed on the server. However, edge devices that make contact with humans in the internet of things (IoT) are designed on the basis of microcontroller units (MCUs). MCUs used in typical edge devices have power consumption about uW/MHz, and hundreds of kilobytes flash memory. When software (i.e., firmware) is flashed into memory, MCU repeats the same operation until reprogramming. As the name of Edge indicates, edge devices focus on the role of sensing and collecting data through interactions with objects at the end of the IoT network. To collect vast amounts of real-world data, edge devices must be able to operate in various locations and under various conditions. Moreover, the available resources, e.g., memory and power, are meager, especially for IoT edge devices, because their accessibility is low and the environment is poor. As edge device moves away from the center of network to collect data generated from close interaction between real-world, there are constraints, such as insufficient power or a small memory size. Therefore, when running artificial intelligence applications on conventional edge devices, various methods are used to transmit the collected data to a resource-rich server to perform ML operations, and then retransmit the results back to the edge device.</p><p>In this paper, we focus on the preprocessing data overhead and the process of passing it as an input to the network model when an edge device executes an ML application. In a speech-recognition application, if raw audio sample data input from the microphone are transmitted directly to the model, then the waveform will contain too much data. Thus, excessive resources will be required for calculation. In addition, if different people pronounce the same word, they will have the same classification result. However, the waveform has different shapes when compared with the raw audio data waveform. Edge devices use preprocessing techniques, such as fast Fourier transform (FFT), to reduce the amount of computation and improve the model's classification performance. In this paper, we propose a technique that aims at improving the operation speed by allocating the preprocessing operations, previously performed using software, to custom-designed hardware, as shown in Figure <ref type="figure" target="#fig_0">1</ref>. In order to prevent correct results from being obtainable because the syllables of a word are truncated by windowing in speech recognition, the accuracy increases as many iterative classifications are performed over a certain time interval. In the already existing application, the entire process of audio data collection, preprocessing, and classification by a model is performed in the MCU's software code. In this paper, an inter-integrated circuit-sound (I 2 S) hardware module was custom designed for collecting and delivering sound data to an edge device. Moreover, the I 2 S module performs part of the preprocessing process in the voice-recognition application. Preprocessing of raw audio data mainly uses digital signal processing (DSP) operations. With DSP operations, it is faster and more efficient to perform in parallel in hardware, which reduces the time taken for preprocessing audio data, thus allowing more classifications to be performed at the same time. In addition, energy consumption can be minimized if a custom I 2 S module that is optimized for voice recognition is used. This approach is a software and hardware co-design for running ML applications efficiently in embedded system edge devices.  To evaluate the proposed method, we used the edge device: an MCU board with a 32-bit ARM Cortex-M4 processor. The custom-designed DSP-embedded I 2 S module was implemented in a Xilinx field-programmable gate array (FPGA). A TinyML program was modified as bare metal to implement an ML application on the MCU evaluation board. The audio data delivered to the edge device were transferred from the external FPGA via an I 2 S communication protocol. The baseline for comparison is that the I 2 S module delivers audio data, and the ML application, including the preprocessing of raw data, is run in the software. However, if preprocessing is included in the custom I 2 S module, then the protocol used for data transfer is the same as I 2 S, except that the data to be transferred have completed preprocessing. Moreover, the computation time and energy consumed in the two cases were compared.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FPGA</head><p>The rest of the paper is organized as follows. Section 2 describes related works about the TinyML approach and audio recognition using ML. Section 3 presents the overall structure of the hardware and software co-designed audio-classification system. Section 4 describes the system's implementation and applies the proposed method to a word-classification application using an embedded board. Section 5 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Recently, an on-device ML paradigm is gaining attention in which the edge device itself processes data <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref>. This in turn reduces the potential security problems when data are transmitted from the edge device to the server as well as the energy consumption generated from data communication.</p><p>Much research has already been conducted on the addition of dedicated hardware, such as a co-processor, to the central processing unit of a device where ML applications are executed. Furthermore, recent ML studies have focused on resource-rich servers or personal computers that support graphics processing units <ref type="bibr" target="#b4">[5]</ref>. However, the conditions of the models used in a server differ from those used in an MCU. We note that, even if the model is only several megabytes in size, it will be difficult to use in MCUs <ref type="bibr" target="#b5">[6]</ref>. However, various conditions must be considered before ML operations can be performed on resourceconstrained edge devices, such as the size of the code to be written to the flash memory and the neural network layer's configuration. Therefore, the TinyML paradigm arose, and research is being conducted on performing ML applications in the MCU efficiently. TinyML is a paradigm that focuses on compressing neural network models, rather than complex inputs or performance, to enable ML applications on MCU-based edge devices <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref>. Furthermore, TinyML allows the conversion of a model trained in an ML framework and programmed in a high-level language like Python into a C/C++ program by separating it into an interpreter and weights. This process involves quantization or precision reduction to fit the edge device's code-memory size and memory usage <ref type="bibr" target="#b12">[13]</ref>.</p><p>In addition, when ML applications are used in mobile devices that have lower resources and performance than PCs or servers, research is being conducted to accelerate ML applications using FPGAs or GPUs <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>. A mobile device with hundreds of megabytes of storage is sufficient to use the TensorFlow Lite framework's network output. Research on optimizing ML applications with the help of hardware such as FPGA or GPU is used more efficiently in mobile devices application processors which includes GPU inside <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>. A research approached from a framework perspective to accelerate neural network inference in mobile on-device, added a interface layer that can be accessed from mobile applications to TensorFlow Lite, allowing the GPU embedded in mobile devices to be used in inference of ML applications <ref type="bibr" target="#b17">[18]</ref>. Furthermore, many studies have been conducted to accelerate the neural network model using FPGA, by implementing the entire neural network model on FPGA <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>. This approach resulted in reducing the time required for inference. The multiply-accumulate operation of a neural network may yield better results if it is performed in parallel in hardware than instruction by instruction executed in the processor. Other studies of TensorFlow Lite focused on performing ML applications by compressing the size of a deep learning model, which is optimized for PC resources, to use in mobile devices or minimizing amount of data communication with the cloud or server. However, neural network models for mobile devices cannot be directly applied to edge devices. Edge devices often lack the size of flash memory to store code, moreover absence of GPU. Therefore, in TensorFlow Lite for mobile, the TinyML approach for deployment to tiny edge devices should be used.</p><p>In our previous work, we proposed a prototype of a partial firmware replacement for a run-time network model <ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref>. This technique involves selecting a model corresponding to the input data domain from a memory-limited device. Another technique was described for automatically generating a suitable convolution neural network model <ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref>. This technique helps to overcome memory constraints in MCUs. Additionally, an efficient network suitable for edge devices is proposed from the same perspective, in which memory used for inference is saved using operator reordering. This approach focuses on reducing the memory used by the network model when the MCU runs ML applications. A technique for reducing the precision of parameters while minimizing the loss of network accuracy <ref type="bibr" target="#b29">[30]</ref>. This technique helps reduce the memory usage occupied by ML applications.</p><p>The methods mentioned so far involve optimizing the embedded memory to match the MCU's conditions. In this paper, we focus on reducing the computation time and energy using a model with a memory size optimized for MCU and combining hardware. Additionally, several researchers combined hardware accelerators for ML applications <ref type="bibr" target="#b30">[31]</ref>. However, only a few studies have been conducted at the level of embedded systems other than accelerators used with high-performance resources. In this paper, preprocessing hardware is used in the data-acquisition module during the data-input process, instead of accelerating ML computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Architecture</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">TinyML</head><p>TinyML can be defined as a combination of hardware and software algorithms for ML operating at the mW energy consumption range. TinyML requires an embedded system powered by a battery. Most frameworks for designing ML applications are implemented in Python, but this high-level abstraction language cannot be used in embedded systems. The independent blockchain used in embedded systems often supports only C/C++. Compiled code, called firmware, is written to the nonvolatile flash memory of the embedded system. This memory performs fixed operations until the firmware is updated. The TinyML framework is implemented in the C/C++ language for using ML applications in embedded systems.</p><p>The process of developing a TinyML application can be divided into two: generating a trained model and developing firmware to be executed in an embedded system based on the generated model. Neural network model generating and weight training are conducted at host machine, as shown in Figure <ref type="figure" target="#fig_1">2b</ref>. In this paper, The TinyML framework used is TensorFlow Lite for MCU, which was released by Google <ref type="bibr" target="#b31">[32]</ref>. When using TensorFlow in Python, typically on a PC or server, one should write code to configure and train the network architecture. Given that the output of TensorFlow is vast, it is impossible to apply that output to the limited memory of embedded systems. However, TensorFlow Lite is a framework for creating models in the range of hundreds of megabytes. These models can be used on mobile devices. Embedded systems can use small-sized models in the form of C/C++ arrays taking up several kilobytes created with the conversion functions provided within the framework. Additionally, TensorFlow Lite quantizes the weights to reduce the model's size, such as converting 32-bit floating-point weights to 8-bit integers. The model is used in the MCU-based embedded system after training in the TensorFlow Lite environment and then converted into a C/C++ array type. In the model's training process configuring the layer structure and evaluating the accuracy of the model's training is similar to the flow used in TensorFlow.</p><p>The next step is to develop the firmware that applies the model created as an array to the TinyML application. In the TinyML application, the model and interpreter are separated, and the interpreter executes the model in the form of a C/C++ array. The application's execution is described at the bottom of Figure <ref type="figure" target="#fig_1">2c</ref>. The input data to be used in ML application may be unprocessed, like an image input to the convolution layer, or may require preprocessing, like audio waveform data. First, the input data are passed to the interpreter, which configures the number and types of layers of the neural network. Then the model is combined to perform inference on the input data. Once the inference is complete, the user must interpret the model's output and respond. When classification is performed on a single image, the class from the model's output layer can be interpreted as a result. These techniques can be mapped to the IoT layer. In the physical layer, the windowing preprocessing process of raw audio data is accelerated; In the communication layer, data is received from the external using the I 2 S communication protocol; In the application layer, TinyML audio classification firmware is executed. However, in the case of an application that recognizes continuous speech, the inference is performed several times in a short time period to comprehensively interpret the output results. Finally, according to the results, the cycle is completed with the device in which the application is executed performing a predetermined response. In this paper, we focused on the aforementioned preprocess stage of the TinyML application in the edge device. When external data is input from the sensor, the software stored in the on-chip flash memory does not perform entire preprocessing, but partitions the part to be executed in hardware and the part to be executed in software. Considering the hardware characteristics useful in parallel data processing, some computations of preprocessing are conducted in hardware to reduce overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Audio Classification</head><p>In this paper, the implemented TinyML application classifies words in audio data. Unlike image classification, finding words in continuous raw audio data requires preprocessing. Audio classification is an application that requires a lot of preprocessing of raw data. A typical sampling frequency of kHz-level produces a large amount of audio raw data over time. However, the network model used for classification cannot receive an input of that size. Therefore, features are extracted by using complex preprocessing in several steps. Among various audio classifications, the case used in this paper is word classification that recognizes specific words in the speaker's speech.</p><p>The word classification application specific words in a human's voice. Each person has a different voice frequency, amplitude, and pronunciation time. Therefore, it is necessary to find the target word's unique characteristics in the voice data sequence. Figure <ref type="figure" target="#fig_2">3</ref> shows the differences between the audio waveforms and spectrograms generated when different people pronounce the same word. Figure <ref type="figure" target="#fig_2">3a</ref> shows the audio waveform when the word "yes" is pronounced, and the two-dimensional (2D) image is converted into a spectrogram. Figure <ref type="figure" target="#fig_2">3b</ref> shows the audio waveform when the word "no" is pronounced. However, it is difficult to detect a word's unique characteristics from an audio waveform, but these characteristics can be visible on the spectrogram. The features of audio waveforms may differ even for the same word, according to the starting point of pronunciation. Additionally, the waveform's amplitude may vary depending on the person's voice. In contrast, regions with high power area in the spectrogram (yellow color) shows a unique shape according to each word in a 2D image of the same size. Unique shape of spectrograms are emphasized in red dotted line in Figure <ref type="figure" target="#fig_2">3</ref>. For the word "yes", it shows a shape of bent to the right, and the word "no", it shows a dense form on the left. In learning and inferencing in ML, a spectrogram can produce a better performance because these features are less affected by the characteristics of the human voice. Moreover, a spectrogram reduces the amount of computation performed in the neural network by compressing the raw audio data. Among the neural network structures, the convolution layer specializes in processing input images, so various applications use spectrograms to visualize audio. This audio is then used as input for the model.</p><p>The Figure <ref type="figure" target="#fig_3">4</ref> shows the process before the spectrogram is generated. When performing an FFT operation on a waveform, the signal at the window boundary becomes discontinuous. The discontinuities may include harmonic components that do not exist in the waveform during the FFT operation. Therefore, Hann windowing <ref type="bibr" target="#b32">[33]</ref> is performed on the waveform, as shown in Equation <ref type="bibr" target="#b0">(1)</ref>. The maximum sample size is N.</p><formula xml:id="formula_0">w(n) = 0.5 1 − cos 2π n N , 0 ≤ n ≤ N<label>(1)</label></formula><p>Then, the mel-frequency scale, a nonlinear function, is applied to average the adjacent frequencies into the downsampled second array. This scale, as shown in Equation ( <ref type="formula">2</ref>), gives more weight to low-frequency elements due to human sound characteristics, and it merges high frequencies. At this point, one row of the spectrogram is generated. Furthermore, the number of columns generated for the spectrogram equals the number of times this operation is repeated for an audio data sequence of a certain length.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Custom I 2 S Module</head><p>The edge device was assumed to be an embedded system with an MCU. It has operating frequency of about 100 to 200 MHz. Therefore, running a TinyML word recognition application in an edge device, it is necessary to extract features from continuous speech waveforms to reduce the amount of computation required to meet up low operation clock frequency. In the conventional approach, as a peripheral of the edge device, the hardware module for collecting voice data is considered separately from the ML software application. This approach makes a simple design, which is considered an advantage because the hardware and software must be designed and tested separately. However, if all of the computations and operations, except data collection, are processed via software, then the time required for such processes may increase due to the Von Neumann architecture. This architecture requires instruction fetching. Most of the audio-preprocessing operations for voice-recognition applications are DSP operations, such as FFT. Instead of complex instructions, it is more efficient to process DSP operations, which mainly are addition and multiplication operations, in hardware.</p><p>In this paper, the implemented I 2 S module adds a windowing logic to the conventional structure using the Hann window coefficients, as shown in Figure <ref type="figure" target="#fig_4">5a</ref>, which is a communication module for sound interfaces that support stereo or mono channels, and the module may be a master or slave. Figure <ref type="figure" target="#fig_4">5b</ref> shows the synthesized Netlist schematic of the designed custom I 2 S module. Additionally, the I 2 S module is capable of both transmitting and receiving sound. This custom I 2 S module is designed to perform a part of the windowing function in hardware during the word classification preprocess. Time and energy consumption overhead can be reduced by accelerating in parallel rather than in instruction fetch-based software. The proposed approach involves receiving a voice from an external microphone and transmitting it to the embedded system in which the TinyML application is executed. The I 2 S module requires a serializer/deserializer module that deserializes the received data bit by bit and re-serializes the data for transmission. This requirement is because data transmission using the protocol consists of one line each for transmission and reception. The standard I 2 S module does not manipulate any input pulse-code modulation audio data. However, our custom-designed I 2 S module performs Hann windowing for each sample before transmitting the data received from the microphone. When the window boundary is determined, the sample matching the index of the Hann window function is multiplied by a coefficient, and the windowing is processed in resistor-transistor logic (RTL). The coefficient of the window function has a floating-point format, but in RTL, it is difficult to calculate the fraction. Hence, it is quantized as a sum of powers of 2. When data come in, they are multiplied by coefficients in order of sequence, and the results form an output sequence with a first-in, first-out policy. When running TinyML applications, if a part of the preprocessing process performed in the embedded system is distributed to hardware, then the logic area or power consumption may increase. However, if the increment is less than the resource savings in the embedded system, then it can be considered more efficient. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>The experiments with the proposed structure were conducted on an embedded board used to run the TinyML application and an FPGA, in which a custom I 2 S module was programmed. The STM32F4-Discovery embedded board was used in the experiment as a TinyML application execution system based on ARM Cortex-M4 32-bit MCU. Table <ref type="table" target="#tab_1">1</ref> shows the specifications of the board. The FPGA used an ARM Cortex-A9 processor and Xilinx 7-series combined Zynq-Z7 board <ref type="bibr" target="#b33">[34]</ref>. The neural network used in the TinyML application on the embedded board was tiny conv <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref>.</p><p>The network consists of three layers. The first layer performs a convolution operation on a 49 × 40 size spectrogram image with eight 10 × 8 filters. In the second layer, the images generated by the filter are fully connected to the output layer. The third layer is the output layer, which has two trained words and silence and unknown classes. The softmax function was used to sharpen the differences between the results. The Speech Commands dataset was used to train the above network <ref type="bibr" target="#b36">[37]</ref>. This dataset is open-source with over 100,000 WAV files of 1-s duration each. The word pair trained in this application was "yes" and "no". Afterward, FFT was applied to audio data to generate the first frequency array. The decision to recognize a word in the audio classification application used in this research was conducted at 1-s intervals. The overhead for audio preprocessing for the entire 1-s period is large. At a sampling frequency of several kHz, the size of data collected for 1-s is too large for signal processing such as FFT. Therefore, the small window is moved as much as the stride and frequency bucket. The result of Hann windowing and FFT operation, is stored as much as the operation result for 1-s.</p><p>If the period of decision is T when the window size is W and the stride size is S, then the number of frequency buckets is determined as the minimum i that satisfies Equation <ref type="bibr" target="#b2">(3)</ref>.</p><formula xml:id="formula_1">(W × i) − (W − S) × (i − 1) ≤ T<label>(3)</label></formula><p>The determined frequency bucket value affects the number of input layer nodes in the neural network.</p><p>Figure <ref type="figure" target="#fig_5">6</ref> shows the measurement of the ratio of functions corresponding to the frontend, specifically preprocessing, in the entire main loop when the TinyML application is executed on the embedded board. The front-end functions, which comprise 78.8% of the feature-generate function, include windowing, FFT, and log scaling, as described in Figure <ref type="figure" target="#fig_3">4</ref>. The feature-generate function occupies 81% of the main loop. In this experiment, the windowing function will be handled in hardware.</p><p>The I 2 S module was implemented as RTL on the FPGA board. Voice was input through a microphone connected to the I 2 S module, and the embedded system received audio data using the I 2 S protocol. Then, the audio data received by the embedded system was passed from the windowing function in the FPGA. A floating-point operation is required to calculate the coefficient values of the Hann window array. However, if a floating-point unit is added for windowing calculation, an excessive area will be required. Therefore, we quantized the coefficients as the sum of powers of negative 2. Using Equation (1), because of Hann coefficients have values from 0 to 1, the fractional part can be expressed as the sum of powers of negative 2. In this quantization technique, the error from the original coefficient varies according to the highest power of negative 2. As the number of negative power of 2 increases, the resolution of fractional parts that can be expressed increases, so the quantization error is small. The hardware usage is determined by the length of the Hann windowing function, i.e., the number of coefficients. To apply the Hann window sliding for each sample to the audio data input, coefficients need to be stored in the hardware. In the current implementation stage, the cell area after synthesis tends to be large. This increase in size is because the combinational and sequential logics are used without a separate storage device, such as RAM, for coefficients. The Xilinx Vivado uses FPGA resources, which was placed on the chip beforehand, with routing when synthesizing and implementing RTL on Zynq FPGA. Utilization and area calculations may not be accurate due to different specifications and resources for each FPGA. Therefore, in this evaluation, the cell area was measured after RTL synthesis of the custom I 2 S module using Synopsys Design Compiler.</p><p>Figure <ref type="figure" target="#fig_6">7a</ref> shows the change in the cell area of the custom I 2 S module while changing the number of coefficients, which was made according to the length of the Hann window and the maximum value of the number of the power of 2, according to the quantization precision. Figure <ref type="figure" target="#fig_6">7b</ref> is the smallest FPGA tool synthesis result when the coefficient window length is 10 and the quantization degree is −5. Figure <ref type="figure" target="#fig_6">7c</ref> is the largest case when the coefficient window length is 80 and the quantization degree is −10. Since the Hann window coefficient has a value from 0 to 1, the power of 2 is also negative. The number of coefficients was changed from 10 to 80 corresponding to the window size from 0.625 to 5 ms. Quantization degree represents the number of powers of 2 in the negative direction used to express decimal coefficients, and it was changed from −5 to −10.</p><p>As a result, the length and quantization degree of the coefficient (window) increased as the value of the synthesized cell area increased. To minimize the additional hardware usage, the coefficient length and quantization degree should be lower. However, if the window coefficient length factor is too low, the size of the window becomes smaller and the number of FFT operations performed in the MCU software, required to recognize a single word, increases. Alternatively, if the quantization degree is lowered and coarse-grained, it may affect the difference between the original Hann window function result. Considering this trade-off, it is important to determine a factor suitable for the application. In this paper, the Hann window array implemented in RTL in hardware has a length of 80, which corresponds to a window size of 5 ms at 16 kHz. This dimension matches those of the input layer of the neural network and the spectrogram-converted audio data sampling frequency. The quantization degree uses a value of −5; thus, the maximum resolution of the quantization is 2 −5 .</p><p>Table <ref type="table" target="#tab_2">2</ref> compares the changes in the execution time for the functions, the energy consumption of the embedded board, and the area after the synthesis of the RTL implemented in the FPGA board when the window process is excluded from the TinyML application using the proposed structure. In the row corresponding to the windowing function, a 0.219 ms decrease occurred, apart from the necessary time consumption for profiling. Furthermore, the execution time for other preprocessing functions also decreased slightly. Additionally, the energy consumption was reduced by 3.27%, as compared to the conventional case. Energy consumption was measured using the Microchip Power Debugger current visualizer. However, for the proposed structure, the hardware area of the I 2 S module increased by 93.39%. The size of the window function for the audio data sequence affects the I 2 S module's area increase. As the size of the window increases, the number of coefficients to be stored in the window function inevitably also increases. Therefore, it is important to select an appropriately sized window boundary according to the characteristics of the application to which the proposed structure will be applied.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions and Future Work</head><p>In this paper, we demonstrated efficient operation using both hardware and software to execute a TinyML audio classification in an embedded system using limited resources. The experimental results showed that the time and energy consumption decreased when executing the functions in the embedded board. In the audio classification application that goes through a lot of preprocessing on raw data input from external, we proved that codesigning with hardware and software reduces execution overhead rather than processing all operations with software. Previously, the I 2 S module that received external audio raw data performed only the role of data collection. This paper designed a custom I 2 S unit that added a windowing module, which is one of the audio preprocessing operations, and was able to reduce operation time and energy consumption in software. When designing the custom I 2 S module, coefficient length and quantization degree were used as parameters for the windowing module. According to the combination, it was possible to design the custom I 2 S module while minimizing the increase in the hardware area size. The proposed preprocessing technique using the hardware is suitable and can be applied in TinyML applications that require a lot of raw data preprocessing besides audio classification. In this paper, among the entire TinyML framework flow, software and hardware are partitioned based on the windowing operation. Optimized partitioning between hardware and software co-design may provide us with a new topic for future research.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Proposed architecture overview for a custom hardware preprocessor coupled with a machine learning (ML) application.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Overview of proposed TinyML application structure and design flow using machine learning (ML) framework at host. (a) hardware and Software partitioning at divided preprocessing stage (b) host machine (c) edge device.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Audio waveform and spectrogram comparison according to words. (a) "yes" word waveform and spectrogram (b) "no" word waveform and spectrogram.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Overview of the audio data preprocessing to generate a spectrogram.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Proposed architecture of the custom I 2 S module. (a) I 2 S data path (b) synthesized schematics of I 2 S implementation.</figDesc><graphic url="image-28.png" coords="9,46.29,556.13,502.75,113.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Function proportions of the TinyML application on the MCU edge device.</figDesc><graphic url="image-29.png" coords="11,235.56,234.50,200.45,88.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Synthesis result cell area comparison according to Hann window coefficient length and quantize precision degree. (a) Design Compiler synthesis result according to coefficient length and quantization degreeu (b) FPGA synthesis result with the smallest cell area (c) FPGA synthesis result with the largest cell area.</figDesc><graphic url="image-31.png" coords="12,271.46,223.65,138.83,200.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Specification of STM32F4-Discovery embedded board.</figDesc><table><row><cell>Core</cell><cell>ARM 32-bit Cortex-M4</cell></row><row><cell>MCU</cell><cell>STM32F407VGT6</cell></row><row><cell>Floating-point Unit</cell><cell>enable</cell></row><row><cell>Flash Memory</cell><cell>1 MByte</cell></row><row><cell>RAM</cell><cell>192 KBytes</cell></row><row><cell>Frequency</cell><cell>Up to 168 MHz</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparison of time, energy, and the logic area between the conventional and proposed structures.</figDesc><table><row><cell>Conventional</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Proposed ∆SystemClk Time (ms) ∆SystemClk Time (ms)</head><label></label><figDesc></figDesc><table><row><cell>Windowing</cell><cell>452,808</cell><cell>2.717</cell><cell>416,412</cell><cell>2.498 (−8.06%)</cell></row><row><cell>FFT</cell><cell>805,188</cell><cell>4.831</cell><cell>764,384</cell><cell>4.583 (−5.13%)</cell></row><row><cell>Log Scale</cell><cell>381,476</cell><cell>2.289</cell><cell>371,264</cell><cell>2.228 (−2.66%)</cell></row><row><cell>Frontend</cell><cell>3,345,885</cell><cell>20.075</cell><cell>3,251,249</cell><cell>19.507 (−2.83%)</cell></row><row><cell>Energy Consumption (mJ)</cell><cell>2.541</cell><cell></cell><cell cols="2">2.458 (−3.27%)</cell></row><row><cell>Cell Area (gate)</cell><cell>55,350</cell><cell></cell><cell></cell><cell>(93.39%)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">Appl. Sci. 2021, 11, 11073. https://doi.org/10.3390/app112211073 https://www.mdpi.com/journal/applsci</note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Funding: This study was supported by the BK21 FOUR project funded by the Ministry of Education, Korea (4199990113966, 10%), and the Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Science and ICT (NRF2019R1A2C2005099, 10%), and Ministry of Education (NRF2018R1A6A1A03025109, 10%), and Institute of Information &amp; communication Technology Planning &amp; Evaluation (IITP) grant funded by the Korea government (MSIT) (no. 2021-0-00944, Metamorphic approach of unstructured validation/verification for analyzing binary code, 70%), and the EDA tool was supported by the IC Design Education Center (IDEC), Korea.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Author Contributions: J.K. designed the entire core architecture and performed the numerical analysis; D.P. was the corresponding author. All authors have read and agreed to the published version of the manuscript.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflicts of Interest:</head><p>The authors declare no conflict of interest. The founding sponsors had no role in the design of the study; in the collection, analyses, or interpretation of data; in the writing of the manuscript, and in the decision to publish the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abbreviations</head><p>The following abbreviations are used in this manuscript: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ML</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Robust navigation with tinyML for autonomous mini-vehicles</title>
		<author>
			<persName><forename type="first">M</forename><surname>De Prado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Donze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Capotondi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rusci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Monnerat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pazos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.00302</idno>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Training on the Edge: The why and the how</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kukreja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shilova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Beaumont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huckelheim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ferrier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hovland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gorman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)</title>
				<meeting>the 2019 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)<address><addrLine>Rio de Janeiro, Brazil</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-24">20-24 May 2019</date>
			<biblScope unit="page" from="899" to="903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">The case for retraining of ML models for IoT device identification at the edge</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kolcun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Safronov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Mandalari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mortier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Haddadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.08605</idno>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Incremental on-device tiny machine learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Disabato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Roveri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd International Workshop on Challenges in Artificial Intelligence and Machine Learning for Internet of Things</title>
				<meeting>the 2nd International Workshop on Challenges in Artificial Intelligence and Machine Learning for Internet of Things</meeting>
		<imprint>
			<date type="published" when="2020-11-16">16 November 2020</date>
			<biblScope unit="page" from="7" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">DaDianNao: A Neural Network Supercomputer</title>
		<author>
			<persName><forename type="first">T</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Temam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1109/TC.2016.2574353</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="73" to="88" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.04381</idno>
		<title level="m">MobileNetV2: Inverted Residuals and Linear Bottlenecks</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">TinyML-Enabled Frugal Smart Objects: Challenges and Opportunities</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sanchez-Iborra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Skarmeta</surname></persName>
		</author>
		<idno type="DOI">10.1109/MCAS.2020.3005467</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Circuits Syst. Mag</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="4" to="18" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">TinyML with Online-Learning on Microcontrollers</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anicic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Runkler</surname></persName>
		</author>
		<author>
			<persName><surname>Tinyol</surname></persName>
		</author>
		<idno type="DOI">10.1109/IJCNN52387.2021.9533927</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 International Joint Conference on Neural Networks (IJCNN)</title>
				<meeting>the 2021 International Joint Conference on Neural Networks (IJCNN)<address><addrLine>Shenzhen, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-07-22">18-22 July 2021</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">J</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Plancher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Moroney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Banbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Banzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Brown</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.04008</idno>
	</analytic>
	<monogr>
		<title level="j">Widening Access to Applied Machine Learning with TinyML. arXiv</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Banbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Thakker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gope</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Janapa Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Whatmough</surname></persName>
		</author>
		<author>
			<persName><surname>Micronets</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11267</idno>
		<title level="m">Neural network architectures for deploying tinyml applications on commodity microcontrollers. arXiv 2021</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">TinyML Meets IoT: A Comprehensive Survey</title>
		<author>
			<persName><forename type="first">L</forename><surname>Dutta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bharali</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.iot.2021.100461</idno>
	</analytic>
	<monogr>
		<title level="j">Internet Things</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">100461</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Duke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Janapa Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jeffries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kreeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Nappier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Natraj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.08678</idno>
		<title level="m">TensorFlow Lite Micro: Embedded Machine Learning for TinyML Systems</title>
				<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Soro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.01255</idno>
		<title level="m">TinyML for Ubiquitous Edge AI. arXiv 2021</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep learning on mobile devices: A review</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mobile Multimedia/Image Processing, Security, and Applications 2019; International Society for Optics and Photonics</title>
				<meeting><address><addrLine>Bellingham, WA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">10993</biblScope>
			<biblScope unit="page">109930A</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep Learning on Mobile and Embedded Devices: State-of-the-Art, Challenges, and Future Directions</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3398209</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="1" to="37" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Using a Fine-Tuning Method for a Deep Authentication in Mobile Cloud Computing Based on Tensorflow Lite Framework</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zeroual</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Derdour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Amroune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bentahar</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICNAS.2019.8807440</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 International Conference on Networking and Advanced Systems (ICNAS)</title>
				<meeting>the 2019 International Conference on Networking and Advanced Systems (ICNAS)<address><addrLine>Annaba, Algeria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-27">26-27 June 2019</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Edge Computed NILM: A Phone-Based Implementation Using MobileNet Compressed by Tensorflow Lite</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bons</surname></persName>
		</author>
		<idno type="DOI">10.1145/3427771.3427852</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Association for Computing Machinery</publisher>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="44" to="48" />
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chirkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ignasheva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pisarchyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Riccardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sarokin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kulik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grundmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.01989</idno>
		<title level="m">On-Device Neural Net Inference with Mobile GPUs. arXiv</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Lacey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Areibi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.04283</idno>
		<title level="m">Deep Learning on FPGAs: Past, Present, and Future</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">TF2FPGA: A Framework for Projecting and Accelerating Tensorflow CNNs on FPGA Platforms</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mouselinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Leon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xydis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Soudris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Pekmestzi</surname></persName>
		</author>
		<idno type="DOI">10.1109/MOCAST.2019.8741940</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 8th International Conference on Modern Circuits and Systems Technologies (MOCAST)</title>
				<meeting>the 2019 8th International Conference on Modern Circuits and Systems Technologies (MOCAST)<address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-15">13-15 May 2019</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Metamorphic IoT Platform for On-Demand Hardware Replacement in Large-Scale IoT Applications (SCI)</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><surname>Miot</surname></persName>
		</author>
		<idno type="DOI">10.3390/s20123337</idno>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Toward Data-Adaptable TinyML using Model Partial Replacement for Resource Frugal Edge Device</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on High Performance Computing in Asia-Pacific Region</title>
				<meeting>the International Conference on High Performance Computing in Asia-Pacific Region</meeting>
		<imprint>
			<publisher>Online</publisher>
			<date type="published" when="2021-01">January 2021</date>
			<biblScope unit="page" from="133" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Low-Power Fast Partial Firmware Update Technique of On-Chip Flash Memory for Reliable Embedded IoT Microcontroller (SCI)</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Seok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
		<idno type="DOI">10.1587/transele.2020LHP0001</idno>
	</analytic>
	<monogr>
		<title level="j">IEICE Trans. Electron</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="page" from="226" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Efficient On-Demand Hardware Replacement Platform toward Metamorphic Functional Processing in Edge-Centric IoT Applications (SCI)</title>
		<author>
			<persName><forename type="first">H</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
		<idno type="DOI">10.3390/electronics10172088</idno>
		<imprint>
			<date type="published" when="2021">2021. 2088</date>
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Low-Power Metamorphic MCU using Partial Firmware Update Method for Irregular Target Systems Control (KCI)</title>
		<author>
			<persName><forename type="first">J</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Korea Inst. Inf. Commun. Eng</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="301" to="307" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Whatmough</surname></persName>
		</author>
		<author>
			<persName><surname>Sparse</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.12107</idno>
		<title level="m">Sparse Architecture Search for CNNs on Resource-Constrained Microcontrollers</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">IoTNet: An Efficient and Accurate Convolutional Neural Network for IoT Devices</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.3390/s19245541</idno>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Liberis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lane</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.05110</idno>
		<title level="m">Neural networks on microcontrollers: Saving memory at inference via operator reordering</title>
				<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><surname>Mcunet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.10319</idno>
		<title level="m">Tiny Deep Learning on IoT Devices</title>
				<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Memory-Driven Mixed Low Precision Quantization For Enabling Deep Network Inference On Microcontrollers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rusci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Capotondi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Benini</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.13082</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Energy-efficient machine learning accelerator for binary neural networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 on Great Lakes Symposium on VLSI</title>
				<meeting>the 2020 on Great Lakes Symposium on VLSI</meeting>
		<imprint>
			<date type="published" when="2020-09">September 2020</date>
			<biblScope unit="page" from="77" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Tensorflow</forename><surname>Lite</surname></persName>
		</author>
		<ptr target="https://www.tensorflow.org/lite" />
		<imprint>
			<date type="published" when="2020-06">June 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">M</forename><surname>Essenwanger</surname></persName>
		</author>
		<title level="m">Elements of Statistical Analysis; Elseview: Amsterdam</title>
				<meeting><address><addrLine>The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName><surname>Digilent</surname></persName>
		</author>
		<author>
			<persName><surname>Zybo-Z7</surname></persName>
		</author>
		<ptr target="https://reference.digilentinc.com/programmable-logic/zybo-z7/start" />
		<imprint>
			<date type="published" when="2021-04-01">1 April 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Tensorflow</forename><surname>Source</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Code</forename></persName>
		</author>
		<ptr target="https://github.com/tensorflow/tensorflow" />
		<imprint>
			<date type="published" when="2020-06">June 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Tinyml: Machine Learning with Tensorflow Lite on Arduino and Ultra-Low-Power Microcontrollers</title>
		<author>
			<persName><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Situnayake</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<pubPlace>O&apos;Reilly Media; Sevastopol, CA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03209</idno>
		<title level="m">Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
