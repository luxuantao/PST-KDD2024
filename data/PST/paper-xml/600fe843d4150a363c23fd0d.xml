<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Leveraging Pre-Trained Language Model for Summary Generation on Short Text</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shuai</forename><surname>Zhao</surname></persName>
							<idno type="ORCID">0000-0001-5174-5182</idno>
							<affiliation key="aff0">
								<orgName type="department">School of Information Engineering</orgName>
								<orgName type="institution">Beijing Institute of Graphic Communication</orgName>
								<address>
									<postCode>100000</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Fucheng</forename><surname>You</surname></persName>
							<email>youfucheng@bigc.edu.cn</email>
							<idno type="ORCID">0000-0001-7266-9406</idno>
							<affiliation key="aff0">
								<orgName type="department">School of Information Engineering</orgName>
								<orgName type="institution">Beijing Institute of Graphic Communication</orgName>
								<address>
									<postCode>100000</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">School of Information Engineering</orgName>
								<orgName type="institution">Beijing Institute of Graphic Communication</orgName>
								<address>
									<postCode>100000</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">And</forename><surname>Zeng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Engineering</orgName>
								<orgName type="institution">Beijing Institute of Graphic Communication</orgName>
								<address>
									<postCode>100000</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuan</forename><surname>Liu</surname></persName>
							<idno type="ORCID">0000-0002-4635-8492</idno>
							<affiliation key="aff0">
								<orgName type="department">School of Information Engineering</orgName>
								<orgName type="institution">Beijing Institute of Graphic Communication</orgName>
								<address>
									<postCode>100000</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Leveraging Pre-Trained Language Model for Summary Generation on Short Text</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/ACCESS.2020.3045748</idno>
					<note type="submission">Received November 20, 2020, accepted December 15, 2020, date of publication December 18, 2020, date of current version December 31, 2020.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Bidirectional Encoder Representations from Transformers represents the latest incarnation of pre-trained language models which have been obtained a satisfactory effect in text summarization tasks. However, it has not achieved good results for the generation of Chinese short text summaries. In this work, we propose a novel short text summary generation model based on keyword templates, which uses templates found in training data to extract keywords to guide summary generation. The experimental results of the LCSTS data set show that our model performs better than the baseline model. The analysis shows that the methods used in our model can generate high-quality summaries.</p><p>INDEX TERMS Summary generation, BERT, pre-trained language model, transformers.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>In deep learning research, when the target task training data is less, usually pre-training and fine-tuning methods can achieve outstanding results <ref type="bibr" target="#b0">[1]</ref>. In recent years, we have witnessed the impressive results of pre-trained language models in several sub-tasks in the field of natural language processing(NLP) <ref type="bibr" target="#b1">[2]</ref>, such as dialogue systems, machine translation, and named entity recognition. It mainly includes ELMo <ref type="bibr" target="#b2">[3]</ref>, GPT <ref type="bibr" target="#b3">[4]</ref>, BERT <ref type="bibr" target="#b4">[5]</ref>, ALBERT <ref type="bibr" target="#b5">[6]</ref> and other models. Among the pre-training methods mentioned above, BERT <ref type="bibr" target="#b4">[5]</ref> has the most outstanding performance. The model uses masked language modelling and next sentence prediction methods for pre-training on a large number of corpora. It can be fine-tuned according to the specific requirements of downstream tasks and can achieve satisfactory results.</p><p>In this article, we explore how to apply the pre-trained language model to short text summary generation better. The purpose of the summary generation is to automatically generate a coherent summary from a given document by rewriting or extracting to shorten the document or paragraph, to alleviate the reading pressure caused by excessive information on users. Text summary generation methods are usually divided into two types: extractive methods and abstractive methods <ref type="bibr" target="#b6">[7]</ref>. The extractive method directly selects significant and</p><p>The associate editor coordinating the review of this manuscript and approving it for publication was Shadi Alawneh . small redundant sentences or phrases from the text to form a summary. In contrast, the abstractive method is relatively complex, but can generate novel vocabulary and does not depend on the source document. This method is more in line with the standard for humans to write summaries. Compared with previous work, the summary generation model using the pre-trained language model has made significant progress. However, these works are based on long documents <ref type="bibr" target="#b7">[8]</ref>. Experiments show that the BERT pre-training model does not perform well to generate Chinese short text summaries. To solve this problem, we propose a short text summary generation model based on keyword templates.</p><p>Template-based summaries are an effective method in the traditional summary generation, in which domain experts manually create a number of hard templates, and then use templates to guide summary generation <ref type="bibr" target="#b8">[9]</ref>. However, it is unrealistic to create all templates manually, requiring a lot of experts and labour-intensive. Different from previous work, the templates used in our model are all from the training set without the need for experts to recreate it.</p><p>In this article, we introduce a short text summary generation model based on keyword templates, which makes the BERT model better applied to Chinese short text summarization generation tasks. Different from BERT's sentence division method, we have added a keyword-based sentence division method based on the original sentence division method. In the training phase, we extract keywords in the reference summary and divide the input text; in the testing phase, we use similarity calculations to find the most similar text in the training set and extract the keywords in the reference summary of the training text for sentence division. Experiments show that our proposed method has achieved good results in abstractive model and has generated higher-quality summaries.</p><p>Contributions made by this article:</p><p>1. We introduce a short text summary generation model based on keyword templates and improve the data preprocessing method of Chinese short text in summary generation tasks.</p><p>2. We showed how to apply the pre-trained language model to generate short text summaries efficiently, and verify it through the abstractive method.</p><p>3. Our model can be used as a stepping stone to improve the quality of the summary and make the pre-trained language model better used in the generation of short text summaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In this section, we will introduce relevant work research on pretrained language models, extractive models, and abstractive models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. PRETRAINED LANGUAGE MODELS</head><p>The research of pre-training language models is mainly aimed at language understanding tasks, which can usually be classified into feature-based models and fine-tuning-based models according to their characteristics <ref type="bibr" target="#b0">[1]</ref>. Feature-based methods mainly use pre-training models to provide language representations and features for the downstream tasks <ref type="bibr" target="#b9">[10]</ref>. EMLo <ref type="bibr" target="#b2">[3]</ref> used a bidirectional LSTM <ref type="bibr" target="#b10">[11]</ref> language model to obtain a context-sensitive pre-trained representation. In the supervised task, they are spliced into the word vector input or the top level representation of the model as features. GPT <ref type="bibr" target="#b3">[4]</ref> used Transformer <ref type="bibr" target="#b11">[12]</ref> network instead of LSTM <ref type="bibr" target="#b10">[11]</ref> as a language model to better capture long-distance language structures. When applied to downstream tasks, GPT <ref type="bibr" target="#b3">[4]</ref> does not need to rebuild a new model structure, and can effectively improve the generalization ability of supervised models and accelerate convergence. The fine-tuning methods are mainly to pre-train the model on the language modeling target, and then fine-tune the model on the downstream tasks with supervised data. The BERT model uses ''masked language modelling'' and ''next sentence prediction'' methods to train on a large-scale corpus. BERT can be widely used because it can be applied to multiple downstream tasks of natural language processing by fine-tuning and achieving outstanding results. Unlike the BERT pre-training model, ALBERT <ref type="bibr" target="#b5">[6]</ref> is faster to train and uses less memory. ALBERT used factors such as factorization and cross-layer parameter sharing to reduce model parameters and improve training speed effectively.</p><p>In past research, pre-trained language models are usually applied to natural language understanding tasks to improve their performance. Recently, many scholars have applied pre-trained language models to generation tasks.</p><p>For example, BERT can fine-tune its parameters with specific generation task parameters. In this article, we try to use the BERT model for the text summary generation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. EXTRACTIVE MODELS</head><p>The extractive summary first scores all sentences in the document according to their importance, then sorts the sentences according to the score, and finally select multiple sentences with the highest scores to form a summary. The early summary generation model mainly used human feature engineering, and its common methods include context matching <ref type="bibr" target="#b12">[13]</ref>, graph model <ref type="bibr" target="#b13">[14]</ref>, but the model effect is not very satisfactory. With the advancement of artificial intelligence, deep neural networks are widely used in summary generation tasks. Yin et al. <ref type="bibr" target="#b14">[15]</ref> applied neural networks to extractive summarization tasks, mapping sentences into vectors, and selecting vital sentences to form summaries. Yasunaga et al. <ref type="bibr" target="#b15">[16]</ref> combined a recurrent neural network with a graph convolutional network to calculate the importance of each sentence, and then select the sentence to form a summary. Shashi et al. <ref type="bibr" target="#b16">[17]</ref> performed global optimization of ROUGE <ref type="bibr" target="#b17">[18]</ref> metrics through reinforcement learning, conceptualized extractable single document summaries as sentence ordering tasks and proposed a novel training algorithm. This model improves the use of cross-entropy as the loss function in the model training process, which may lead to summary length and excessive redundant information. The SUMO <ref type="bibr" target="#b18">[19]</ref> model proposed an end-to-end extractive text summary generation method, which regards single-document extractive summaries as a tree induction problem. The model subtree is the sentence in the original document related to the summary or explains the summary, thereby improving the correlation between the model generated summary and the text. Recently, pretrained language models has been shown to be useful for improving text summarization tasks. Yang Liu <ref type="bibr" target="#b19">[20]</ref> applied BERT <ref type="bibr" target="#b4">[5]</ref> to extractive summaries for the first time, the experimental results show that the pre-training model is also suitable for text summarization generation. Hongwang et al. <ref type="bibr" target="#b20">[21]</ref> tried three different pre-training strategies, Mask, Replace and Switch, and used self-supervised methods to capture the global features of the document to more accurately grasp the main content of the article. The HIBERT <ref type="bibr" target="#b21">[22]</ref> model treated the task as a sequence labelling task and marks whether a sentence appears in summary in the original text. Zhong et al. <ref type="bibr" target="#b22">[23]</ref> proposed a completely new method to transform the extractive summary task into a semantic matching problem. The advantage is that the candidate summary (several sentences) can be directly extracted instead of sentence-level (sentence by sentence).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. ABSTRACTIVE MODELS</head><p>Compared with the extractive method, the abstractive method is closer to the standard of human summary writing. In recent years, there has been more and more research on abstractive methods. The abstractive method regards the task of summary generation as a sequence-to-sequence problem and used neural networks to solve it. Nallapati <ref type="bibr" target="#b23">[24]</ref> and Chopra et al. <ref type="bibr" target="#b24">[25]</ref> used RNN to replace traditional the encoder and decoder and achieved good results. Lin et al. <ref type="bibr" target="#b25">[26]</ref> proposed a global coding framework based on context information to improve the model's ability to control global details. The model uses a gated convolution unit to ensure that the core information is retained and filters redundant information. Chen et al. <ref type="bibr" target="#b10">[11]</ref> proposed an accurate and fast summary model, first select some crucial sentences, and then generate operations on these sentences. The model uses a novel sentence-level strategy gradient method to connect the sentence extraction network and the summary generation network. Not only the best experimental results are obtained, but also a significant speed increase in decoding and training. Zhang et al. <ref type="bibr" target="#b26">[27]</ref> applied the BERT model to abstractive summaries for the first time. The model uses BERT as the encoder to extract the input document's features and then uses the Transformer to decode to generate the initial results, masked the output, and then made predictions through another BERT. Logan Lebanoff <ref type="bibr" target="#b27">[28]</ref> proposed to map single sentences and pairs of sentences to a unified space for sorting. According to this sorting, single sentences and paired sentences that are important for the summary are selected. Finally, by compressing the single sentences, the paired sentences are merged to generate the summary.</p><p>Different from the previous work, we propose a short text summary generation model based on keyword templates. Our model uses different data processing methods, which can better apply the pre-trained language model to generate short text summaries in Chinese and generate high-quality summaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. MODEL</head><p>In this section, we will introduce the overall structure of the model from two parts: data preprocessing, model architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. DATA PREPROCESSING</head><p>Unlike the BERT data preprocessing method, we added a keyword-based sentence division form based on the original sentence division model. The experimental results show that simple sentence division does not apply BERT to the generation of short text summaries. Therefore, based on the original sentence division model, we extract the reference summary keywords in the training set and divide the text twice.</p><p>In the model training stage, we extract keywords from the reference summary and divide the input text twice. In the model testing stage, we use the similarity calculation tool (macropodus) <ref type="bibr" target="#b28">[29]</ref> to find the most similar data in the training set to the test document. The keywords in the reference summary of the data are extracted to divide the test document twice.</p><p>As shown in the left half of Figure <ref type="figure" target="#fig_0">1</ref>, we divide the text according to the sentence structure. In the right half of the figure, we first extract ''the summary generation'' as a keyword, and divide the text again based on the original sentence division.</p><p>We insert [CLS] before each sentence (or phrase) obtained after preprocessing and insert [SEP] at the end of each sentence (or phrase) to represent each sentence (or phrase). And use the inserted [CLS] symbols to collect sentence features. The document is represented as a sequence of tokens X = [w 1 , w 2 , . . . , w n ]. Each token w i consists of token embedding, segment embedding, and position embedding, as shown in Figure <ref type="figure" target="#fig_0">1</ref>. Among them, token embedding converts each word into a fixed-dimensional vector; position embedding represents the position information of each token in the text; segmentation embeddings divide the sentence into E A or E B mainly according to whether i is even or odd <ref type="bibr" target="#b19">[20]</ref>. For example, suppose the text contains a total of six sentences, we divide [sent 0 , sent 1 , sent 2 , sent 3 , sent 4 ,</p><formula xml:id="formula_0">sent 5 ] into [E A , E B , E A , E B , E A , E B ].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. MODEL ARCHITECTURE</head><p>To verify our proposed method's effectiveness in generating short text summaries, we use the traditional Encoder-Decoder structure and fine-tune BERT. The encoder uses BERT for initialization, and the decoder uses eight Transformers stacked with random initialization <ref type="bibr" target="#b7">[8]</ref>. We name our model BSA. If the model uses keyword templates to divide sentences, it named BSA*.</p><p>We represent the preprocessed data as [sent 1 , sent 2 , sent 3 ,. . . , sent n ] and input it into the BERT layer. The vector t i is the feature vector collected by the i th [CLS] symbol output by the BERT layer, which can be used to represent sent i . After the Encoder, we use the Decoder stacked by Transformers to decode the BERT output:</p><formula xml:id="formula_1">h l = LN (h l−1 + MHAtt(h l−1 )) h l = LN ( h l + FFN ( h l )) (<label>1</label></formula><formula xml:id="formula_2">)</formula><p>where LN refers to the normalization layer; MHAtt is the multi-headed attention composed of multiple self-attention mechanisms connected <ref type="bibr" target="#b19">[20]</ref>; the symbol l represents the depth of the model stack. FFN is feedforward neural network between the encoder and decoder of each layer.</p><formula xml:id="formula_3">FFN (x) = max(0, xW 1 + b 1 )W 2 + b 2 (2)</formula><p>In formula (1), h 0 is calculated by PosEmb(T ); vector T means the sentence vectors output by the encoder layer.</p><p>PosEmb refers to the function of adding positional embeddings (refers to the position information of each sentence in the text) to vector T .</p><p>PE(pos, 2i) = sin(pos/10000 2i/dmodel ) ( <ref type="formula">3</ref>)</p><formula xml:id="formula_4">PE(pos, 2i + 1) = cos(pos/10000 2i/dmodel )<label>(4)</label></formula><p>where pos indicates the position and i refers to the dimension. Using Transformers as a feature extractor can effectively extract document-level features and generate higher quality and more coherent summaries. The decoder focus on summary generation from the encoder outputs. However, the encoder uses a pre-trained model, the decoder is initialized randomly and needs to be trained from scratch, and there is a mismatch between them. To solve this problem, we will use different optimization strategies for encoder and decoder <ref type="bibr" target="#b7">[8]</ref>. Respectively, the specific formula is as follows:</p><formula xml:id="formula_5">Encoder : Adam optimizer, β 1 = 0.9 warmup = 20000 lr = lr 1 • min(step −0.5 , step • warmup −1.5 ) (5) Decoder : Adam optimizer, β 2 = 0.999 warmup = 10000 lr = lr 2 • min(step −0.5 , step • warmup −1.5 ) (6)</formula><p>where lr 1 = 2e −3 is the learning rate of the encoder, and lr 2 = 0.1 is the learning rate of the decoder. In the model's initial stage, the encoder side uses a smaller learning rate and smoothing strategy to fine-tune the pre-trained model <ref type="bibr" target="#b7">[8]</ref>. As the training continues, the decoder side tends to be stable, and better accuracy can be obtained. The BSA model architecture is shown in Figure <ref type="figure" target="#fig_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL DETAILS</head><p>In this section, we introduce the data set used in the experiment, model parameter settings, and the baseline models compared with the experiment. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. EXPERIMENTAL CORPUS</head><p>In this study, we mainly explore how to better apply the pre-trained model to Chinese short text summarization generation, so the experimental data set only uses LCSTS data set. LCSTS is a public Chinese short text summary generation data set constructed by Hu Chen in 2015 <ref type="bibr" target="#b29">[30]</ref>, which contains 2.4 million real Chinese short text data and summaries given by each text author. The data set contains a total of three parts, of which 2.4M text summary pairs are used for training, 8K text summary pairs are used for verification, and the remaining 0.7K text summary pairs are used for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. EXPERIMENT SETTINGS</head><p>We implemented our experiment in pytorch through 8 GPUs (RTX2080ti). Both source and summary were tokenized with BERT's subwords tokenizer.</p><p>In our model, we apply dropout before all linear layers, and its probability is set to 0.2; label smoothing with smoothing factor is set to 0.1. On the decoder side, we set the hidden unit size of each Transformer to 768, and the feedforward neural network of each layer to 2048. Regarding the setting of the learning rate, the lr 1 = 2e −3 for the encoder and lr 2 = 0.1 for the decoder <ref type="bibr" target="#b7">[8]</ref>. In terms of quality evaluation indicators, we learn from previous research methods and use ROUGE as the model's evaluation indicator. ROUGE is a text summary automatic evaluation method proposed by Chin-yew Lin in 2004 <ref type="bibr" target="#b17">[18]</ref>. We use the standard F1 scores of ROUGE-1, ROUGE-2 and ROUGE-L to evaluate the reference summaries and generated summaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. BASELINE MODELS</head><p>We compare the experimental results with seven baseline models. Below is the baseline model we compared on the LCSTS dataset. RNN <ref type="bibr" target="#b10">[11]</ref> and Bi-MulRnn <ref type="bibr" target="#b30">[31]</ref> are seq2seq models based on RNN, without and with attention mechanism, respectively. CopyNet <ref type="bibr" target="#b12">[13]</ref> is a seq2seq model that uses both the copy mechanism and the attention mechanism. SRB <ref type="bibr" target="#b31">[32]</ref> introduces the cosine function based on the traditional seq2seq model to calculate the similarity. TSNHG <ref type="bibr" target="#b32">[33]</ref> is attention-based seq2seq with gated recurrent unit, which improves the experimental results under different topic data by classifying the training set data according to the topic. CGU <ref type="bibr" target="#b33">[34]</ref> is a seq2seq model base on the convolutional gated unit and the attention mechanism. RTCS <ref type="bibr" target="#b34">[35]</ref> is the conventional seq2seq model with convolutional neural network.</p><p>Besides, we have proved the effectiveness of our proposed method through ablation experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENT ANALYSIS</head><p>In this section, we verify the experimental results of the BSA and BSA* models in the short text summary and conduct a comparative analysis. Besides, we also showed two summary examples to illustrate that our model can generate high-quality summaries.  Compared with the model using RNN as the feature extractor (RNN, Bi-MulRnn), our model uses Transformers as the feature extractor to capture the document's in-depth features more efficiently. Besides, our model uses a pre-trained BERT model compared with other baseline models, which can effectively improve model performance and generate higher quality summaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. RESULTS ON LCSTS DATASET</head><p>Through the comparative analysis of ablation experiments, our model results show that the use of keywords to subdivide the text can effectively improve the quality of the generated summaries and better apply the pre-trained language model to Chinese short text summaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. DISCUSSION</head><p>We show two examples of summaries generated by our model and compare them with RNN models and reference summaries. At the same time, we also show the summaries generated by our model under different data processing methods. As shown in the summary example in Table <ref type="table" target="#tab_2">2</ref>, compared with the RNN model, the summary generated by our model can highlight the main content of the text. In contrast, the BSA model can not accurately express the main idea of the text. For example, in the first example, there is no giant in Shanghai Internet, but the model's summary thinks that Shanghai is only a giants. Compared with the BSA model, the BSA* model's summary represents the text's main content more accurately. Simultaneously, the second summary example can effectively illustrate that the pre-trained model can generate a higher quality summary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>To improve the application of the pre-trained language model in Chinese short text summary generation, this article proposes a novel Chinese short text summary generation model based on the keyword templates. In our model, we divide the text twice by extracting keywords. Experimental results show that our model's document sentence division method effectively improves the generated summary quality. We offer how to efficiently apply the pre-trained language model to the generation of short text summaries.</p><p>However, this method has only been verified on the abstractive model. In future work, we will verify whether the short text summary generation model based on keyword templates can improve the extractive model summary generation's quality and verified on other pre-trained language models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FIGURE 1 .</head><label>1</label><figDesc>FIGURE 1. As shown in the right half, we first extract ''the summary generation'' as a keyword, and divide the document again based on the original sentence division.</figDesc><graphic url="image-4.png" coords="3,36.73,66.06,501.00,151.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>FIGURE 2 .</head><label>2</label><figDesc>FIGURE 2. The architecture of the BSA model. It consists of encoder(BERT) and decoder(Transformer).</figDesc><graphic url="image-6.png" coords="4,298.94,66.06,237.50,139.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>shows the experimental results of our model and baseline model on the LCSTS data set. our model achieves a better performance with 44.2 F-score of ROUGE-1, 28.9 ROUGE-2 and 39.2 ROUGE-L. Compared with the RTCS model, our model is improved by 4.3, 7.4, and 1.3 on ROUGE-1, -2, and -L. Besides, compare with the unimproved model(BSA), the BSA* model is improved by 3.8, 3.0, and 2.5 on ROUGE-1, -2, and -L.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1 .</head><label>1</label><figDesc>Results of our Model and Baseline Systems in LCSTS Dataset. Besides, the Table also Contains the Results of the Ablation Experiment</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2 .</head><label>2</label><figDesc>Two Summary Examples Include Source Text, Reference Summary, RNN Model Generation Summary, BSA Model Summary, and BSA* Model Summary</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">VOLUME 8, 2020   </note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the National Natural Science Foundation of China under Grant 61773229, and in part by the Beijing Municipal Natural Science Foundation under Grant KZ201710015010.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">MASS: Masked sequence to sequence pre-training for language generation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML Conf</title>
				<meeting>ICML Conf</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Augmenting BERT with graph embedding for text classification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">'</forename><surname>Vgcn-Bert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECIR Conf</title>
				<meeting>ECIR Conf</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="369" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<author>
			<persName><forename type="first">M</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep contextualized word representations,&apos;&apos; in Proc. Conf. North Amer</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pretraining</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<idno>abs/1704.01444</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NAACL Conf</title>
				<meeting>NAACL Conf</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">ALBERT: A lite BERT for self-supervised learning of language representations</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11942</idno>
		<ptr target="http://arxiv.org/abs/1909.11942" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Neural document summarization by jointly learning to score and select sentences</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.02305</idno>
		<ptr target="http://arxiv.org/abs/1807.02305" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Text summarization with pretrained encoders</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08345</idno>
		<ptr target="http://arxiv.org/abs/1908.08345" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">BiSET: Bi-directional selective encoding with template for abstractive summarization</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 57th Annu. Meeting Assoc. Comput. Linguistics</title>
				<meeting>57th Annu. Meeting Assoc. Comput. Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2153" to="2162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">An efficient framework for learning sentence representations</title>
		<author>
			<persName><forename type="first">L</forename><surname>Logeswaran</surname></persName>
		</author>
		<idno>abs/1803.02893</idno>
		<imprint>
			<date type="published" when="2018-03">Mar. 2018</date>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Fast abstractive summarization with reinforceselected sentence rewriting</title>
		<author>
			<persName><forename type="first">Y.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.11080</idno>
		<ptr target="http://arxiv.org/abs/1805.11080" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<ptr target="https://arxiv.org/abs/1706.03762" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Incorporating copying mechanism in Sequence-to-Sequence learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">O K</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 54th Annu. Meeting Assoc. Comput. Linguistics</title>
				<meeting>54th Annu. Meeting Assoc. Comput. Linguistics</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1631" to="1640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Get to the point: Summarization with pointer-generator networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 55th Annu. Meeting Assoc. Comput. Linguistics</title>
				<meeting>55th Annu. Meeting Assoc. Comput. Linguistics</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1073" to="1083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Optimizing sentence modeling and selection for document summarization</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 24th Int. Joint Conf</title>
				<meeting>24th Int. Joint Conf</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1383" to="1389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Graph-based neural multi-document summarization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Meelu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pareek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 21st Conf. Comput. Natural Lang. Learn</title>
				<meeting>21st Conf. Comput. Natural Lang. Learn</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="452" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ranking sentences for extractive summarization with reinforcement learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. North Amer</title>
				<meeting>Conf. North Amer</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Automatic evaluation of summaries using N-gram co-occurrence statistics</title>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. North Amer. Chapter Assoc</title>
				<meeting>Conf. North Amer. Chapter Assoc</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="71" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Single document summarization as tree induction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. North Amer</title>
				<meeting>Conf. North Amer</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1745" to="1755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Fine-tune BERT for extractive summarization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.10318</idno>
		<ptr target="http://arxiv.org/abs/1903.10318" />
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Self-supervised learning for contextualized extractive summarization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.04466</idno>
		<ptr target="http://arxiv.org/abs/1906.04466" />
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">HIBERT: Document level pretraining of hierarchical bidirectional transformers for document summarization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.06566</idno>
		<ptr target="http://arxiv.org/abs/1905.06566" />
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Extractive summarization as text matching</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proc. Assoc. Comput. Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Abstractive text summarization using Sequence-to-sequence RNNs and beyond</title>
		<author>
			<persName><forename type="first">R</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 20th SIGNLL Conf</title>
				<meeting>20th SIGNLL Conf</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="280" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Abstractive sentence summarization with attentive recurrent neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. North Amer</title>
				<meeting>Conf. North Amer</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="93" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Global encoding for abstractive summarization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.03989</idno>
		<ptr target="http://arxiv.org/abs/1805.03989" />
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">On one-round discrete Voronoi games</title>
		<author>
			<persName><forename type="first">M</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kisfaludi-Bak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mehr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09234</idno>
		<ptr target="http://arxiv.org/abs/1902.09234" />
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Scoring sentence singletons and pairs for abstractive summarization</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lebanoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Dernoncourt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.00077</idno>
		<ptr target="http://arxiv.org/abs/1906.00077" />
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Yongzhuo Mo. Macropodus</title>
		<ptr target="https://github.com/yongzhuo/Macropodus" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Scale Chinese short text summarization dataset</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Hu Large</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Empirical Methods Natural Lang. Process</title>
				<meeting>Conf. Empirical Methods Natural Lang. ess</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1967" to="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Generating news headlines with recurrent neural networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lopyrev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.01712</idno>
		<ptr target="http://arxiv.org/abs/1512.01712" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">A semantic relevance based neural network for text summarization and text simplification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.02318</idno>
		<ptr target="http://arxiv.org/abs/1710.02318" />
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Topic sensitive neural headline generation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ayana</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.05777</idno>
		<ptr target="http://arxiv.org/abs/1608.05777" />
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Global encoding for abstractive summarization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proc. Assoc. Comput. Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="163" to="169" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A reinforced topic-aware convolutional sequence-to-sequence model for abstractive text summarization</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 27th Int. Joint Conf</title>
				<meeting>27th Int. Joint Conf</meeting>
		<imprint>
			<date type="published" when="2018-07">Jul. 2018</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">He has participated in many critical projects of the Beijing Natural Science Foundation; the scientific research projects of the Beijing Municipal Education Commission; and school-level vital projects. He has published three academic papers in national and foreign publications, including three SCI / EI searches. It has been granted two national invention patents, one utility model patent, one publication, and compilation, and has won the first-class scholarship in 2019. FUCHENG YOU is a Tutor, Doctor, and Professor of Computer Science with the School of Information Engineering, Beijing Institute of Graphic Communication. He is currently the Director of the &apos;&apos;Digital Image Processing&apos;&apos; Research Office of the Beijing Key Laboratory of &apos;&apos;High-End Printing Equipment Signal and Information Processing&apos;&apos; and the Director of Research and Recruit-Office of the School of Information Engineering. The main courses include database principle, digital image processing, image processing and analysis, and so on. The main research directions are natural language processing, digital image processing, machine vision application, among others. He has presided over many key projects of the Beijing Natural Science Foundation; the scientific research projects of the Beijing Municipal Education Commission; and school-level key projects. More than 80 academic papers have been published in national and foreign publications, of which SCI / EI has searched more than 50. It has been granted 12 national invention patents, five utility model patents, two books, and won the Yachang Education Award in 2016. ZENG YUAN LIU is currently pursuing the bachelor&apos;s degree, majoring in computer science, with the School of Information Engineering, Beijing Institute of Graphic Communication, whose primary research fields are natural language processing, deep learning, among others. He has partici</title>
	</analytic>
	<monogr>
		<title level="m">whose primary research fields are natural language processing</title>
				<imprint/>
		<respStmt>
			<orgName>majoring in computer science, with the School of Information Engineering, Beijing Institute of Graphic Communication</orgName>
		</respStmt>
	</monogr>
	<note>SHUAI ZHAO is currently pursuing the bachelor&apos;s degree. pated in many critical projects of the Beijing Natural Science Foundation; the scientific research projects of the Beijing Municipal Education Commission; and school-level vital projects. He won the Second-Class Scholarship in 2019</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
