<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Deep Network Solution for Attention and Aesthetics Aware Photo Cropping</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
							<email>wenguanwang@bit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">• W. Wang and J. Shen are with Beijing Laboratory of Intelligent In-formation Technology</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
							<email>shenjianbing@bit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">• W. Wang and J. Shen are with Beijing Laboratory of Intelligent In-formation Technology</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haibin</forename><surname>Ling</surname></persName>
							<email>hbling@temple.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">• W. Wang and J. Shen are with Beijing Laboratory of Intelligent In-formation Technology</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">•</forename><forename type="middle">H</forename><surname>Ling</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer and Information Sciences</orgName>
								<orgName type="institution">Temple University</orgName>
								<address>
									<settlement>Philadelphia</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Deep Network Solution for Attention and Aesthetics Aware Photo Cropping</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">42941C46407B7C968B8AE1FE52576B4A</idno>
					<idno type="DOI">10.1109/TPAMI.2018.2840724</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2018.2840724, IEEE Transactions on Pattern Analysis and Machine Intelligence IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Photo cropping</term>
					<term>attention box prediction</term>
					<term>aesthetics assessment</term>
					<term>deep learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We study the problem of photo cropping, which aims to find a cropping window of an input image to preserve as much as possible its important parts while being aesthetically pleasant. Seeking a deep learning-based solution, we design a neural network that has two branches for attention box prediction (ABP) and aesthetics assessment (AA), respectively. Given the input image, the ABP network predicts an attention bounding box as an initial minimum cropping window, around which a set of cropping candidates are generated with little loss of important information. Then, the AA network is employed to select the final cropping window with the best aesthetic quality among the candidates. The two sub-networks are designed to share the same full-image convolutional feature map, and thus are computationally efficient. By leveraging attention prediction and aesthetics assessment, the cropping model produces high-quality cropping results, even with the limited availability of training data for photo cropping. The experimental results on benchmark datasets clearly validate the effectiveness of the proposed approach. In addition, our approach runs at 5 fps, outperforming most previous solutions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION 1.Problem Statement and Motivation</head><p>G IVEN an input photo, what is the best way to crop it?</p><p>The answer, not surprisingly, varies from person to person, and even from time to time for the same person. In this paper, we study the problem in the general setting without prior knowledge of its specific application. In such setting, it is natural to expect a good cropping window to have two properties: keeping most of the important portion and being aesthetically pleasant. The idea can be viewed from the example in Figure <ref type="figure" target="#fig_1">1</ref>.</p><p>With the above general idea, a natural strategy for photo cropping is through determining-adjusting. That is, one can first define a cropping window that covers the important region, and then adjust (iteratively) the position, size and ratio of the initial cropping 1 until achieving a satisfying result. This cropping strategy brings two advantages: (1) consideration of both image importance and aesthetics in a cascaded way; and (2) high computation efficiency since the searching space of the best cropping is limited to the neighborhood of the initial one.</p><p>Interestingly, however, most previous cropping approaches follow a different strategy. They usually generate a large number of sliding windows by varying aspect ratios and sizes over all the positions, and find the optimal cropping window by computing attention scores for all windows <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, or by analyzing their aesthetics <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. This sliding-judging strategy, as depicted in Figure <ref type="figure" target="#fig_1">1 (d)</ref>, is of high computation load, since its searching space spans all 1. In the rest of the paper, for conciseness, we call a "cropping window" as a "cropping" when there is no ambiguity caused.  0162-8828 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p><p>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2018.2840724, IEEE Transactions on Pattern Analysis and Machine Intelligence the possible sub-windows of the entire photo. By contrast, the determining-adjusting strategy is more efficient by arranging the two key components in a sequential way (hence reducing the size of the searching space).</p><p>In this paper, we design a deep learning based photo cropping algorithm, which models the cropping task as a cascade of attention bounding box regression and aesthetics classification. In particular, our model first determines an attention box that covers the most visually important area (the red rectangle in Figure <ref type="figure" target="#fig_1">1(b)</ref>), simulating a person placing an initial cropping to cover important region. Then the method generates a set of cropping candidates (the yellow rectangles in Figure <ref type="figure" target="#fig_1">1 (b)</ref>) around the attention box and selects the one with the highest aesthetics value as the final cropping (Figure <ref type="figure" target="#fig_1">1 (c)</ref>). This is similar to people iteratively adjusting an initial cropping and picking out the most beautiful one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Contribution</head><p>Compared with previous arts, we treat the photo cropping task in a more natural and efficient way, with the following major contributions:</p><p>• A deep learning framework to combine attention and aesthetics components for photo cropping. We model photo cropping with a determining-adjusting process, where attention-guided cropping candidates generation is followed by aesthetics-aware cropping window selection, as demonstrated in Figure <ref type="figure" target="#fig_1">1 (e)</ref>. Both tasks are achieved via a unified deep learning model, where attention information is exploited to avoid discarding important information, while the aesthetics assessment is employed for ensuring the high aesthetic value of the cropping result. The deep learning model is extended from the fully convolutional neural network, which naturally supports input images of arbitrary sizes, thus avoiding undesired deformation for evaluating aesthetic quality. Our method can be viewed as an early work that applies deep learning technologies for photo cropping.</p><p>• High computation efficiency. Three ingredients are introduced in our approach for enhancing computational efficiency. First, instead of exhaustively searching all sub-windows in the sliding window fashion (e.g. <ref type="bibr" target="#b5">[6]</ref>), our approach directly regresses the attention box and generates far less cropping candidates around the visually important areas. Second, the sub-networks for attention box prediction and for aesthetics assessment share several initial convolutional layers, and thus largely boost the efficiency by reusing the computation in these layers. Third, inheriting the advantage of recent object detection algorithms <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, our algorithm is trained to share convolutional features among cropping candidates. Regardless of the number of cropping candidates, these convolutional layers are calculated only once over the entire image, thus avoiding applying the network to each cropping candidate for repeatedly computing features. All these techniques help our approach to achieve a run time speed of 5 fps, significantly faster than previous solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>Learning without cropping annotation. Use of deep leaning for vision problems typically requests a large amount of training data, which, for photo cropping, means a large amount of manually annotated cropping results. Such request is however very challenging, since photo cropping is very time consuming, and more importantly, is very subjective since it is difficult to offer a clear answer to what is a "groundtruth" cropping. Thus, training a network to directly output a cropping window is difficult and practically infeasible. We bypass this issue to use rich data for human gaze prediction and photo aesthetics assessment, which are much easier to obtain. It is worth noting that, despite using no cropping results in training, our approach has shown great performance on the cropping task as shown in our thorough experiments.</p><p>These contributions together bring both effectiveness and efficiency to our proposed photo cropping algorithm. As described in Section 4, the thorough evaluations on popular benchmarks show clearly the advantage of our algorithm in comparison with state-of-the-art solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Organization</head><p>In Section 2, we first review representative works related to our algorithm. Then we present details of our algorithm in Section 3. In Section 3.1 and 3.2, we describe two subnetworks for attention box prediction and aesthetics assessment respectively, which are two key components of our deep photo cropping solution. In Section 3.3, we discuss some issues regarding the implementation of our algorithm.</p><p>In Section 4, we offer both quantitative and qualitative experimental analysis of the proposed algorithm. Specifically, in Section 4.2 and 4.3, we provide extensive studies to evaluate the performance of the key components in our algorithm in publicly available benchmarks. In Section 4.4.1 and 4.4.2, experiments on three public photo cropping benchmarks, namely IMSR-ICD <ref type="bibr" target="#b4">[5]</ref>, FLMS <ref type="bibr" target="#b5">[6]</ref>, and FCD <ref type="bibr" target="#b9">[10]</ref>, with the comparison to the state-of-the-art clearly demonstrating the robustness, effectiveness, and efficiency of our deep learning based photo cropping solution. Furthermore, in Section 4.4.3, we perform user studies for assessing the quality of cropping results from our system. To better understand the contributions of different aspects of our proposed algorithm, we implement several variants of our method to conduct detailed ablative studies (Section 4.4.4). The experimental results demonstrate that our method compares favorably with the state-of-the-art. Finally, concluding remarks can be found in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In this section, we first summarize representative works in visual attention prediction and aesthetics assessment (Section 2.1 and 2.2), respectively. Then, in Section 2.3, we give an overview of related works in photo cropping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Visual Attention Prediction</head><p>Visual attention prediction is a classic computer vision problem that aims to predict scene locations where a human 0162-8828 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2018.2840724, IEEE Transactions on Pattern Analysis and Machine Intelligence observer may fixate. This task, sometimes referred as eye fixation prediction or visual saliency detection, is for simulating the astonishing ability of humans for selectively paying attention to parts of the image instead of processing the whole scene in its entirety. A large amount of research effort has been devoted to this topic with many applications, such as object recognition <ref type="bibr" target="#b10">[11]</ref>, image segmentation <ref type="bibr" target="#b11">[12]</ref>, image cropping <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b12">[13]</ref>, etc. The output of attention prediction algorithms is usually a saliency map indicating the visual importance of each pixel.</p><p>Early visual attention models <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref> in the vision community are inspired by the studies in visual psychology and psychophysics of human attention. Those models can be further broadly classified into bottom-up approaches and top-down ones. Most of early models are based on the bottomup mechanism, which is stimulus-driven and infers the human attention based on visual stimuli themselves without the knowledge of the image semantics. Such models <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref> typically generate saliency cues based on various low-level features (e.g., color, intensity, orientation) and heuristics (e.g., center-surround contrast <ref type="bibr" target="#b18">[19]</ref>) on limited human knowledge of visual attention, and combine them at multiple scales to create the final saliency map. By contrast to the bottom-up task-independent models, some top-down task-driven approaches <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref> are proposed that explore explicitly the understanding of the scene or task context. These approaches employ high-level features, such as person or face detectors learned from specific computer vision tasks. We refer readers to two recent surveys <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref> for more details of early attention models.</p><p>Deep learning-based attention models <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref> become increasingly popular in recent years, driven by the success of deep learning in object recognition and largescale visual attention dataset (e.g., SALICON <ref type="bibr" target="#b25">[26]</ref>). Most of these models are variants of the fully convolutional network and generally give more impressive results than previously proposed non-deep learning competitors.</p><p>Traditional visual attention models concentrate on encouraging the consistency between the distribution of the predicted saliency and that of the real human fixations. Differently, in our approach, we are concerned more on predicting an attention bounding box, which covers the most informative regions of the image.</p><p>Another related topic in parallel is salient object detection, which can be dated to <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref> and has been extensively studied in computer vision in the past decade. Different from visual attention prediction, salient object detection specially focuses on detecting and uniformly highlighting one (multiple) salient object(s) in its (their) entirety. However, as stated in many literatures <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, unlike fixation datasets, most salient object detection datasets are heavily biased to few objects. Therefore, for the sake of generalization capability and applicability, we choose visual attention prediction for photo cropping and use corresponding datasets (e.g., <ref type="bibr" target="#b25">[26]</ref>), instead of the datasets of salient object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Image Aesthetics Assessment</head><p>The main goal of aesthetics assessment is to imitate human's interpretation of the beauty of natural images. Many methods have been proposed for this topic, as surveyed in <ref type="bibr" target="#b31">[32]</ref>. Traditionally, aesthetic quality analysis is viewed as a binary classification problem of predicting high-or lowquality of an image, or a regression problem of producing aesthetics scores. A common pipeline is to first extracting visual features and then employing various machine learning algorithms to predict photo aesthetic values.</p><p>Early methods are mainly concerned on manually designing good feature extractors, which requires a considerable amount of engineering skills and domain expertise. Some works <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref> use hand-crafted aesthetics features according to photographic rules or experiences, such features include lighting, contrast, global image layout (rule-of-thirds), visual balance, typical objects (human, animals, plants), etc. These rule-based approaches are intuitive in that they explicitly model the criteria used by humans in evaluating the aesthetic quality of photos. Instead of using hand-crafted features, another option <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref> for image quality assessment is to leverage more generic image descriptors, such as the Fisher vector and bag of visual words, which are previously designed for image classification but also capable of capturing aesthetic properties.</p><p>More recently, deep learning-based solutions <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref> have shown that image aesthetics representation may be better learned in a data-driven manner. This trend is more and more popular with the growth of training data, i.e., from hundreds of images to millions of images. Those deep learning methods have greatly advanced the frontier of this topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Photo Cropping</head><p>Photo cropping is an important operation for improving visual quality of digital photos. Many methods have been proposed towards automating this task, and they can be roughly categorized into attention-based or aesthetics-based.</p><p>Attention-based approaches <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b44">[45]</ref> focus on preserving the main subject or visually important area in the scene after cropping. These methods usually choose the cropping window according to certain attention scores or object-level saliency map. These methods are successful for removing unnecessary content of an image, while sometimes fail to produce visually pleasant results due to the lack of consideration in image aesthetics.</p><p>Aesthetics-based approaches, by contrast, emphasize the general attractiveness of the cropped image. Those approaches <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref> are centered on compositionrelated image properties and low-level image features. Taking various aesthetical factors into account, they attempt to find the cropping candidate with the highest quality score. These methods are in favor of preserving visually attractive solutions, while at the risk of missing important area and generally suffer from expensive computation due to evaluating a large amount of cropping candidates.</p><p>In general, conventional cropping methods search the region with the highest attention/aesthetics score from a number of cropping windows. In this paper, we consider both attention and aesthetics information, which are arranged in a natural and cascaded manner. In particular, our method treats photo cropping as a cascade of generating cropping candidates, via attention box prediction, and selecting the best cropping window, via the aesthetics criteria. Our method shares the spirit of recent object detection algorithms <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>. In fact, a branch of our network learns to predict the bounding box covers visually important area, while the other branch estimates aesthetic value.</p><p>This paper extends our recent ICCV paper <ref type="bibr" target="#b47">[48]</ref>. The improvements are multiple folds. First, we give a deeper insight into the proposed determining-adjusting based cropping protocol, with the comparison of previous slidingjudging strategy. This brings a new view into the rationale behind photo cropping. Second, we extend our attention box prediction network with supervised attention mechanism, outlining a complete model for better capturing the visual importance of input image and generating more accurate attention box prediction. It also improves the explainability of our model and leads to an implicit deep supervision. Third, we offer a more in depth discussion of the proposed algorithm, including motivations, network structures and implementation. Forth, extensive experiments and user studies are conducted for thoroughly and insightfully examination. Last but not least, based on our experiments, we draw several important conclusions, which are expected to inspire future works in this direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DEEP LEARNING-BASED PHOTO CROPPING</head><p>We model photo cropping in a determining-adjusting framework, which first creates initial cropping as a bounding box covering the most visually important area (attentionaware determining), and then selects the best cropping with the highest aesthetic quality from a few cropping candidates generated around the initial cropping (aestheticbased adjusting). The cropping algorithm can be decomposed into two cascaded stages, namely, attention-aware cropping candidates generation (Section 3.1) and aestheticsbased cropping selection (Section 3.2).</p><p>A deep learning framework is thus designed with two sub-networks: an Attention Box Prediction (ABP) network and an Aesthetics Assessment (AA) network. Specifically, the ABP network is responsible for inferring the initial cropping; and the AA network determines the final cropping. As demonstrated in Figure <ref type="figure" target="#fig_2">2</ref>, these two networks share several convolutional blocks in the bottom and are based on fully convolutional network, which will be detailed in following sections. Finally, in Section 3.3, we will give more details of our model in training and testing. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Attention-aware Cropping Candidates</head><p>In this section, we introduce our method for cropping candidates generation, which is based on an Attention Box Prediction (ABP) network. This network takes an image of any size as input and outputs a set of rectangular cropping windows, each with a score that stands for the prediction accuracy. Then the initial cropping is identified as the most accurate one, and various cropping candidates with different sizes and ratios are generated around it. After that, the final cropping is selected from those candidates according to their aesthetic quality based on an Aesthetics Assessment (AA) network (Section 3.2). The initial cropping can be viewed as a rectangle that preserves the most informative part of the image while has minimum area. Searching for an optimal solution is common for attention-based cropping methods. Let G ∈ [0, 1] w×h be an attention mask of image I of size w × h, and larger values in G indicate higher visual importance of corresponding pixels in I. Formally, we derive a set of cropping windows W considering their importance or informativeness:</p><formula xml:id="formula_0">W = W x∈W G(x) &gt; λ x∈{1..w}×{1..h} G(x) ,<label>(1)</label></formula><p>where λ ∈ [0, 1] is a threshold. Then the optimum cropping rectangle W is defined as the one with minimum area:</p><formula xml:id="formula_1">W = argmin W ∈W |W |.<label>(2)</label></formula><p>Equ. 2 can be solved via sliding window searching with O(w 2 h 2 ) computation complexity, while a recent method <ref type="bibr" target="#b2">[3]</ref> shows it can be solved with computation complexity of O(wh 2 ) (assuming h &lt; w).</p><p>Different from the above time consuming strategy, we design a neural network for predicting an optimal attention box. Given a training sample (I, G) consisting of an image I of size w × h × 3 (Figure <ref type="figure" target="#fig_3">3</ref> (a)), and a groundtruth attention map G ∈ [0, 1] w×h (Figure <ref type="figure" target="#fig_3">3 (b)</ref>), the optimum rectangle W defined in Equ. 2 is treated as the groundtruth attention prediction box. Here we apply <ref type="bibr" target="#b2">[3]</ref> for generating W over G (Figure <ref type="figure" target="#fig_3">3 (c</ref>)) for computation efficiency, and set λ = 0.9 for preserving most informative areas. Then  the task of attention box prediction can be achieved via bounding box regression similar as in object detection <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>. Note that, our ABP network is not limited to specific attention scores, and other attention models can be used for generating groundtruth bounding box as well. Figure <ref type="figure" target="#fig_5">4</ref> illustrates the architecture of the ABP network. The bottom of this network is a stack of convolutional layers, which are borrowed from the first five convolutional blocks of VGGNet <ref type="bibr" target="#b48">[49]</ref>. In our conference version <ref type="bibr" target="#b47">[48]</ref>, we build the bounding box regression layers upon the last convolutional layer with a small network of a 3×3 kernel (see Figure <ref type="figure" target="#fig_5">4</ref> </p><formula xml:id="formula_2">L att (Y, G ) = i g i log g i y i .<label>(3)</label></formula><p>The KL-Div measure, whose minimization is equivalent to cross-entropy minimization, is widely used to learn visual attention models in deep networks.</p><p>Then we slide a small network of a 3 × 3 kernel and 512 channels over the merged feature map, thus generating a 512-dimensional feature vector for each sliding location. The feature vector is further fed into two fully-connected layers: a box-regression layer for predicting attention bounding box and a box-classification layer for determining whether a box belongs to attention box. For a given location, those two fully-connected layers predict box offsets and scores over a set of default bounding boxes, which are similar to the anchor boxes used in Faster R-CNN <ref type="bibr" target="#b7">[8]</ref>.</p><p>To train the ABP network for bounding box prediction, we need to decide the positive and negative training boxes (samples) correspond to the groundthe attention box and train the network accordingly. We treat a box as a positive box if it has the Intersection-over-Union (IoU) score with the groundtruth box of larger than 0.7, or it has the largest IoU score. In such case, we give it a positive label (c = 1). By contrast, we treat a box as negative (c = 0) if it has an IoU score lower than 0.3 and drop other default boxes. The above process is illustrated in Figure <ref type="figure" target="#fig_3">3(d)</ref>. For the preserved boxes, we define pc i ∈ {1, 0} as an indicator for the label of the i-th box and vector t as a four-parameter coordinate (coordinates of center, width and height) of the groundtruth attention box. Similarly, we define p c i and t i as predicted confidence over c class and predicted attention box of the i-th default box. With the above definition, we consider the following loss function for bounding box prediction, which is derived from object detection <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b50">[51]</ref>:</p><formula xml:id="formula_3">L box (p, t) = i L cls (p i , pi ) + i p1 i L reg (t i , t).<label>(4)</label></formula><p>The classification loss L cls is the softmax loss over confidences of two classes (attention box or not). The regression loss L reg is a smooth L1 loss <ref type="bibr" target="#b49">[50]</ref> between the predicated box and the ground truth attention box, and it is only activated for positive default boxes.</p><p>With the above definition, the ABP network is trained via minimizing the following overall loss function:</p><formula xml:id="formula_4">L = L att + L box ,<label>(5)</label></formula><p>where L att (defined in Equ.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Aesthetics-based Cropping Window Selection</head><p>With the attention-aware cropping candidates by the ABP network, we next select the most aesthetically-pleasant one as the final cropping. It is important to consider aesthetics for photo cropping, since beyond preserving the important content, a nice cropping should also deliver pleasant viewing experience. For analyzing the aesthetic quality of each cropping candidates, one choice is to train an aesthetics assessment network, and iteratively applying forward-propagation for each cropping candidate over this network. This straightforward strategy is obviously very time-consuming. Inspired by the recent advantages of object detection, which shares convolutional features between regions, we propose to build a network that analyzes aesthetic values of all candidates simultaneously. We achieve this via an Aesthetics Assessment (AA) network (Figure <ref type="figure" target="#fig_8">6</ref>), which takes an entire image and a set of cropping candidates as input, and outputs the aesthetic values of the cropping candidates. The bottom of the AA network is the first four convolutional blocks of VGGNet <ref type="bibr" target="#b48">[49]</ref> excluding the pool4 layer. Here we adopt a relatively shallow network mainly due to two reasons. First, aesthetics assessment is a relatively easy problem (with only two 2. Since we resize the input image with min(w, h) = 224, we find the largest offset (40) to be sufficient. labels: high quality vs low quality) compared with image classification (with 1000 classes for ImageNet). Second, for an image of size of w × h × 3, the spatial dimensions of the final convolutional feature map of AA network is w 8 × h 8 , which preserves discriminability for the offsets defined in Section 3.1.</p><p>On the top of the last convolutional layer, we adopt a Region of Interest (RoI) pooling layer <ref type="bibr" target="#b7">[8]</ref>, which is a special case of spatial pyramid pooling (SPP) <ref type="bibr" target="#b6">[7]</ref>, to extract a fixed-length feature vector from the last convolutional feature map. The RoI pooling layer uses max-pooling to convert the features inside any cropping candidate into a small feature map with a fixed-dimensional vector, which is further fed into a sequence of fully-connected layers for aesthetic quality classification. This operation allows us to handle images with arbitrary aspect ratios, thus avoiding undesired deformation in aesthetics assessment. For a cropping candidate of size of w × h , the RoI pooling layer divides it into n × n (n=7 in our experiments) spatial bins and applies max-pooling for the features within each bins.</p><p>For training, given an image from existing aesthetics assessment datasets, it takes an aesthetic label c ∈ {1, 0}, where 1 indicates high aesthetic quality and 0 indicates low quality. We resize the image so that min(w, h) = 224, same as for the ABP net, and the whole image can be viewed as a cropping candidate for training. For the i-th training image, we define qc i ∈ {1, 0} as an indicator for its aestheticsquality label and q c i as its predicted aesthetics-quality score for class c.</p><p>Based on the above definition, the training of the AA network is done by minimizing the following softmax loss over N training samples:</p><formula xml:id="formula_5">L cls (q, q) = - 1 N i c∈{1,0} qc i log( q c i ),<label>(6)</label></formula><formula xml:id="formula_6">q c i = exp(q c i ) c ∈{1,0} exp(q c i ) . (<label>7</label></formula><formula xml:id="formula_7">)</formula><p>With the cropping candidates generated from the APB network, the AA network is capable of producing their aesthetics-quality scores ({q 1 i } i ), where the one with the highest score is selected as the final cropping (Figure <ref type="figure" target="#fig_7">5</ref> (c)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Training</head><p>Two large-scale datasets: SALICON <ref type="bibr" target="#b25">[26]</ref> and AVA <ref type="bibr" target="#b51">[52]</ref>, are used for training our model. The SALICON dataset is used for training our ABP network. It contains 20,000 natural images with eye fixation annotations. In the area of saliency prediction, the publication of SALICON dataset has enabled end-to-end training of deep architectures specifically for attention prediction. To obtain smooth saliency map, we follow <ref type="bibr" target="#b25">[26]</ref> to apply a Gaussian filter with a small kernel for filtering a binary mouse-clicking map into a grey-scale human attention map. To obtain the groundtruth attention box, we apply the algorithm in <ref type="bibr" target="#b2">[3]</ref> to the saliency map to generate attention bounding boxs according to Equ. 2 with λ = 0.9.</p><p>The AVA dataset is the largest publicly available aesthetics assessment benchmark, containing about 250,000 images in total. The aesthetics quality of each image was rated on average by roughly 200 people with the ratings ranging from one to ten, with ten indicating the highest aesthetics quality. Followed the work in <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b51">[52]</ref>, about 230,000 images are used for training our AA network. More specifically, images with mean ratings smaller than 5 are assigned as low quality and the rest as high quality. More details of the two datasets can be found in Section 4.1.</p><p>Our two sub-networks are trained simultaneously. In each training iteration, we use a min-batch of 10 images, 5 of which are from the SALICON dataset with the groundtruth attention boxes and the rest from the AVA dataset with aesthetics quality groundtruth. Before feeding the input images and ground-truth to the network, we scale the images such that the shorter side is of size 224. The whole training scheme of our model is presented in Figure <ref type="figure" target="#fig_9">7</ref>. The conv1 and conv2 blocks are shared between both the tasks of attention box prediction and esthetics assessment, and they are trained for both the tasks simultaneously using all the images in the batch. For the layers specialized for each subnetwork, they are trained using only those images in the batch with the corresponding ground-truth.</p><p>Both ABP and AA networks are initialized from the weights of VGGNet <ref type="bibr" target="#b48">[49]</ref>, which is pre-trained on a largescale image classification dataset: ImageNet <ref type="bibr" target="#b52">[53]</ref> with 1M images. Our model is implemented with Keras <ref type="bibr" target="#b53">[54]</ref> with TensorFlow <ref type="bibr" target="#b54">[55]</ref> backend and trained with the Adam optimizer <ref type="bibr" target="#b55">[56]</ref>. The learning rate is set to 0.0001. The networks were trained over 10 epochs. The entire training procedure takes about 1 day with an Nvidia TITAN X GPU. Note that, in our conference version, we adopt the popular Caffe library <ref type="bibr" target="#b56">[57]</ref> and trained with stochastic gradient descent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Testing</head><p>While our two sub-networks are trained in parallel, they work in a cascaded way (see Figure <ref type="figure" target="#fig_10">8</ref>) during testing. Given an input image (resized such that min(w, h) = 224) for cropping, we first gain a set of attention boxes generated by forward propagation on the APB network. Then the initial cropping is selected as the one with the highest accuracy of attention box prediction. After that, a set of cropping candidates are generated around the initial one. Since the two initial convolutional blocks are shared between the ABP and the AA networks, we directly feed the cropping candidates and the convolutional feature of last layer of conv2 into the AA network. The final cropping is selected as the cropping candidate with best aesthetic quality. The whole algorithm runs at about 5 fps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL RESULTS</head><p>In this section, we first detail the datasets used for training and testing in Section 4.1. Then we examine the performance of our ABP and AA networks on their specific tasks (Section 4.2 and 4.3). The goal of these experiments is to investigate the effectiveness of individual components instead of comparing them with the state-of-the-art. Then, in Section 4.4, we evaluate the performance of our whole cropping model on two widely used photo cropping datasets with other competitors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>There are totally six datasets, namely SALICON <ref type="bibr" target="#b25">[26]</ref>, PASCAL-S <ref type="bibr" target="#b29">[30]</ref>, AVA <ref type="bibr" target="#b51">[52]</ref>, Image Cropping Dataset from MSR (MSR-ICD) <ref type="bibr" target="#b4">[5]</ref>, FLMS <ref type="bibr" target="#b5">[6]</ref>, and Flickr Cropping Dataset (FCD) <ref type="bibr" target="#b9">[10]</ref>, used in our experiments. Some statistics of these datasets and experimental settings are summarized in Table <ref type="table" target="#tab_1">1</ref>. SALICON and PASCAL-S are employed, respectively, for training and testing our ABP network (Section 4.2); AVA • SALICON. This is one of the largest saliency datasets available in the public domain. It contains 20,000 natural images from the MSCOCO dataset <ref type="bibr" target="#b57">[58]</ref> with eye fixation annotations that are simulated through mouse movements of users on blurred images. These images contain diverse indoor and outdoor scenes and display a range of scene clutter. 10,000 images are marked for training, 5,000 for validation and 5,000 for testing. We use the training and validation sets (with publicly available annotations) for training our ABP network. Since the fixation data for the test set is held-out, we turn to another widely used dataset, PASCAL <ref type="bibr" target="#b29">[30]</ref>, for accessing the performance of ABP network.</p><p>• PASCAL-S. This dataset contains in total 850 natural images from the validation set of PASCAL 2010 <ref type="bibr" target="#b58">[59]</ref> segmentation challenge. There are totally eight subjects are instructed to perform the free-viewing task in the fixation experiment. Each image is presented for 2 seconds, and the eye gaze data is recorded using Eyelink 1000 eye-tracker, at 125Hz sampling rate. The smooth attention map for each image is obtained by blurring the fixation map with a fixed Gaussian kernel (σ = 0.005 of the image width).</p><p>• AVA. The Aesthetic Visual Analysis (AVA) dataset contains about 250,000 images in total. These images are obtained from DPChallenge.com and labeled for aesthetic scores. Specifically, each image receives 78∼549 votes of a score ranging from 1 to 10. For the task of binary aesthetic quality classification, images with an average score higher than 5 are treated as positive examples, and the rest image are treated as negative ones. Accordingly, a large-scale standardized partition is obtained, which has about 230,000 images for training about 20,000 images for testing.</p><p>• MSR-ICD. The MSR-ICD dataset includes 950 images which are originally from an image aesthetics assessment database <ref type="bibr" target="#b36">[37]</ref>. The photos are acquired from the professional photography websites and contributed by amateur photographers and span a variety of image categories, including animal, architecture, human, landscape, night, plant and man-made objects. Each image is carefully cropped by three expert photographers.</p><p>• FLMS. The FLMS dataset contains 500 natural images collected from Flickr. For each image, 10 expert users on Amazon Mechanical Turk who passed a strict qualification test are employed for cropping groundtruth box.</p><p>• FCD. It consists of 1,743 images collected from Flickr.</p><p>Seven workers on Amazon Mechanical Turk were recruited for annotation. The images are split into a training set (1,369) and a test set (374).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Performance of ABP Network</head><p>We first evaluate the ABP network on the PASCAL-S dataset <ref type="bibr" target="#b29">[30]</ref>, which is widely used for attention prediction. With the binary eye fixation images, we follow <ref type="bibr" target="#b29">[30]</ref> to generate grayscale attention map. Then, as described in Section 3.3, we generate a groundtruth attention box for each image.</p><p>TABLE 2 Attention box prediction with IoU for PASCAL-S dataset <ref type="bibr" target="#b29">[30]</ref>.</p><p>Method Ours <ref type="bibr" target="#b47">[48]</ref> ITTI <ref type="bibr" target="#b13">[14]</ref> AIM <ref type="bibr" target="#b14">[15]</ref> GBVS <ref type="bibr" target="#b15">[16]</ref>  We test eight state-of-the-art attention models including ITTI <ref type="bibr" target="#b13">[14]</ref>, AIM <ref type="bibr" target="#b14">[15]</ref>, GBVS <ref type="bibr" target="#b15">[16]</ref>, SUN <ref type="bibr" target="#b16">[17]</ref>, DVA <ref type="bibr" target="#b17">[18]</ref>, SIG <ref type="bibr" target="#b59">[60]</ref>, CAS <ref type="bibr" target="#b60">[61]</ref> and SalNet <ref type="bibr" target="#b26">[27]</ref>. Previous attention models are for imitating human visual attention behavior, and their output is a continuous saliency map. In contrast, our AA network that generates an important bounding box as an initial cropping. Thus PR curves or AUC curves used in visual attention prediction cannot be directly applied comparison. For the sake of a relatively fair comparison, we first extract the attention boxes of above methods via the same strategy used for generating the groundtruth bounding box. Then we apply the Intersection over Union (IoU) score for quantifying the quality of extracted attention boxes. We also report the results from our preliminary conference paper <ref type="bibr" target="#b47">[48]</ref>. The quantitative results are illustrated in Table <ref type="table">2</ref>. As seen, our attention box prediction results are more accurate than previous attention models, since our ABP network is specially designed for this task. Additionally, comparing the performance of our conference version, the improvement is significant (0.517→0.583). This is mainly due to the incorporation of visual attention supervision in our ABP network, which offers strong prior knowledge for attention box prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Performance of the AA Network</head><p>We adopt the testing set of the AVA dataset <ref type="bibr" target="#b51">[52]</ref>, as described in Section 3.3, for evaluating the performance of our AA network. The testing set of AVA dataset contains 19,930 images. The testing images with mean ratings smaller than 5 are labeled as low quality; otherwise they are labeled as high quality.</p><p>We compare our methods with the state-of-the-art methods including AVA <ref type="bibr" target="#b51">[52]</ref>, RAP <ref type="bibr" target="#b39">[40]</ref>, RAP2 <ref type="bibr" target="#b61">[62]</ref>, DMA <ref type="bibr" target="#b41">[42]</ref>, ARC <ref type="bibr" target="#b42">[43]</ref> and CPD <ref type="bibr" target="#b43">[44]</ref>, where AVA offers the state-of-theart result based on manually designed features while other methods are based on deep learning model.</p><p>We opt the overall accuracy metric, which is the most popular evaluation criterion in the research area of image aesthetics assessment, for quantitative evaluation. It can be expressed as:  where T P, T N, P and N refer to true positive, true negative, total positive, and total negative, respectively. This metric accounts for the proportion of correctly classified samples.</p><formula xml:id="formula_8">Accuracy = T P + T N P + N ,<label>(8)</label></formula><p>It is clear from Table <ref type="table" target="#tab_3">3</ref> that, our AA network achieves state-of-the-art performance even with a relatively simple network architecture. In Figure <ref type="figure" target="#fig_11">9</ref>, we present some examples with aesthetics values predicted by our AA network.</p><p>Overall, our two sub-networks generate the promising results aligned with existing top-performance approaches. This is mainly due to a relatively shallow network and simple network architecture, compared with exiting deep learning aesthetics network. Considering the shared convolutional layers in the bottom of these two networks, our model achieves a good tradeoff between performance and computation efficiency. More importantly, the robustness of these two basic components greatly contributes to the highquality of our cropping suggestions, which will be detailed in the next section.  <ref type="bibr" target="#b2">[3]</ref> 0.64 0.075 LCC <ref type="bibr" target="#b4">[5]</ref> 0.63 -MPC <ref type="bibr" target="#b62">[63]</ref> 0.41 -VBC <ref type="bibr" target="#b5">[6]</ref> 0.74 -Ours <ref type="bibr" target="#b47">[48]</ref> (conference version) 0.81 0.057 Ours 0.83 0.052 -The authors in LCC <ref type="bibr" target="#b4">[5]</ref>, MPC <ref type="bibr" target="#b62">[63]</ref> and VBC <ref type="bibr" target="#b5">[6]</ref> have not released results with the BDE measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Performance of Cropping Network</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Evaluation on MSR-ICD and FLMS Datsets</head><p>We first evaluate our full cropping model on two widelyused image cropping datasets, including the Image Cropping Dataset from MSR (MSR-ICD) <ref type="bibr" target="#b4">[5]</ref> and the FLMS <ref type="bibr" target="#b5">[6]</ref> dataset.</p><p>We adopt the same evaluation metrics as <ref type="bibr" target="#b4">[5]</ref>, i.e., the IoU score and the Boundary Displacement Error (BDE) to measure the cropping accuracy of image croppers. BDE is defined as the average displacement of four edges between the cropping box and the groundtruth rectangle:</p><formula xml:id="formula_9">BDE = i ||B g i -B c i ||/4,<label>(9)</label></formula><p>where i ∈ {lef t, right, bottom, up} and {B i } i denote the four edges of the cropped window or groundtruth cropping.</p><p>Note that BDE has to be normalized by the width or height of the image. Clearly, a good cropping solution favors a high IoU score and a low BDE. We compare our cropping method with two main categories of image cropping methods, i.e., attention-based and aesthetics-based methods.</p><p>For attention-based methods, we select the ATC algorithm <ref type="bibr" target="#b12">[13]</ref> which is a classical image thumbnail cropping method, and the AIC algorithm <ref type="bibr" target="#b2">[3]</ref>. The results of AIC algorithm are obtained via applying cropping window researching method <ref type="bibr" target="#b2">[3]</ref> with top-performing saliency detection method. Here we apply context-aware saliency <ref type="bibr" target="#b60">[61]</ref> and optimal parameters, as suggested by <ref type="bibr" target="#b2">[3]</ref>, for maximizing its performance. For aesthetics-based methods, we select LCC <ref type="bibr" target="#b4">[5]</ref>, MPC <ref type="bibr" target="#b62">[63]</ref>, and VBC <ref type="bibr" target="#b5">[6]</ref>. In addition, we consider SPC, which is an advanced version of <ref type="bibr" target="#b3">[4]</ref>, as described in <ref type="bibr" target="#b4">[5]</ref>. Additionally, we adopt a recent aesthetics ranking method <ref type="bibr" target="#b42">[43]</ref> combined with sliding window strategy as a baseline: ARC. We select the cropping as the one with the highest ranking score from sliding windows.</p><p>The quantitative comparison results on the MSR-ICD and FLMS datasets are demonstrated in Table <ref type="table" target="#tab_4">4</ref> and Table <ref type="table">5</ref>, respectively. As seen, our cropping method achieves the best performance in both datasets. The improvement over our conference version verifies the effectiveness of our improved ABP network. Qualitative results on the MSR-ICD and FLMS datasets are presented in Figure <ref type="figure" target="#fig_1">10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Evaluation on FCD Datset</head><p>We further test the proposed cropping method on the test set of the recently released FCD <ref type="bibr" target="#b9">[10]</ref> dataset. Following the settings in FCD dataset, we extend the attention-based ATC algorithm <ref type="bibr" target="#b12">[13]</ref> with two state-of-the-art attention methods, i.e., BMS <ref type="bibr" target="#b63">[64]</ref> and eDN <ref type="bibr" target="#b23">[24]</ref>, using two search strategies, i.e., MaxAvg (searching an optimal cropping window with the highest average saliency) and MaxDiff (maximizing the difference of average saliency between the crop and the outer region). For aesthetics-based methods, we consider three baselines in <ref type="bibr" target="#b9">[10]</ref>: SVM+DECAF 7 , AVA+DECAF 7 and FCD+DECAF 7 , corresponding to a combination of the SVM classifier and DECAF 7 features <ref type="bibr" target="#b64">[65]</ref>, training on the AVA and FCD datasets, respectively. As summarized in Table <ref type="table" target="#tab_5">6</ref>, the results on the FCD dataset demonstrate again that our method compares favorably with the previous state-of-theart methods over the two evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">User Study</head><p>Since photo cropping is a human-centric task, we conduct user study for assessing the quality of cropping suggestions from our system and other competitors, including ATC <ref type="bibr" target="#b12">[13]</ref> and AIC <ref type="bibr" target="#b2">[3]</ref>. A corpus of 20 participants (8 females and 12 males) with diverse backgrounds and ages were recruited to participate in our user studies. None of the participants had received any special technical instructions or had any prior knowledge about the experimental hypotheses. 200 images randomly selected from the MSR-ICD <ref type="bibr" target="#b4">[5]</ref> and FLMS <ref type="bibr" target="#b5">[6]</ref> datasets are used in this user study. The original image and its cropped versions from our method and other competitors are presented to the participants. Each participant examines all the selected images and is required to answer which cropped image they prefer. Figure <ref type="figure" target="#fig_1">11</ref> shows the distribution of votes averaged over all participants. As can be seen, our method receives the most overall votes, confirming the strong preference of the proposed method over other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.4">Ablation Study</head><p>To give a deeper insight of the proposed cropping method, we study different ingredients and variants of our method. We experiment on the FLMS dataset <ref type="bibr" target="#b5">[6]</ref> and measure the performance using the IoU metric. Four baselines derived from our method are considered:</p><p>• ABP: It directly uses the initial cropping from the ABP network as the final cropping.</p><p>• Sliding window+AA: We apply sliding windows (∼10,000 windows for an image with typical resolution of 224×224) and use the AA network to select the best aestheticspreserved one as the final cropping.</p><p>• ATC+AA: It corresponds to the results that we treat the cropping results from attention-based cropping method ATC <ref type="bibr" target="#b12">[13]</ref> as the initial cropping and further apply the AA network for determining the final cropping.</p><p>• AIC+AA: Similar to ATC+AA, we combine the AA network with the results from attention-based AIC <ref type="bibr" target="#b2">[3]</ref> for outputting the final cropping. This supports one of our motivations to combine visual importance and photo aesthetics together for correctly determining the cropping.</p><p>3) The proposed cropping solution achieves high computation efficiency. With the full implementation, the proposed algorithm achieves a high processing speed of 5 fps on a modern GPU, which is faster than other competitors.</p><p>To study the influence of the sampling strategy of our cropping candidates (Section 3.1), we further evaluate the following two baselines:</p><p>• Randomly sampling: Instead of using a set of fixed size offsets, we randomly extract 1,000 cropping candidates, all of which cover the initial cropping area.</p><p>• Larger sampling step: We enlarge the original sampling steps (=8) as 16, then apply AA network for selecting the final cropping.</p><p>From Table <ref type="table" target="#tab_5">6</ref>, we can observe performance drops of these two baselines. For random sampling, since the feature map of AA network is with ×8 downsampling, some closed candidates are repeatedly considered while some other important candidates may be missed. When we increase the sampling step, the performance becomes worse since some candidate regions are ignored. Overall, the proposed cropping algorithm that combines the ABP and AA net- We can find that the ABP network tends to select the most informative but small parts, which may discard some parts of a large object. See Section 4.5.1 for more discussion.</p><p>works achieves the best performance and gains much more computation efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Discussions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">Limitations</head><p>The proposed algorithm suffers from a few limitations. A potential drawback of utilizing visual attention is that unfaithful importance maps might negatively affect cropping results. The attention model (the ABP network) may omit parts of a salient object which occupies a large portion of the scene (see examples in Figure <ref type="figure" target="#fig_13">12</ref>). This issue can be partly alleviated by considering the aesthetics quality from the AA network. Besides, since most of the training images in the AVA dataset are manually selected and premanipulated, the discriminability of AA network may be limited with daily raw images. For training the AA network, the negative samples and the positive samples are from different scenes. However, such image-level aesthetics annotation is insufficient to provide enough supervision for the AA network for rating cropping cases from a same original image. For remedying this, more negative training examples with false cropping (e.g., splitting an important object into parts) should be mined for training a more robust AA model that handles cropped images better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Future work</head><p>The proposed method is among the earliest attempts to apply deep learning for photo cropping, and opens various research directions that are worth future exploration.</p><p>• Bottom-up attention vs top-down attention: Similar to most previous attention based cropping algorithms, our method employs bottom-up attention model to determine the image parts to preserve. The bottom-up model imitates the selective mechanisms of human visual system in general scenes without considering any high-level information. It is interesting to explore the integration of top-down taskdriven attention into the proposed cropping framework. Such attention would help reveal the rationale behind human cropping behavior, e.g., understanding the searching strategy of human, examining the correlation between purely visual importance and cropping-task-related importance.</p><p>• Classification vs ranking: In our current approach, we formulate aesthetics analysis as a binary classification problem (i.e., low-or high-aesthetics). However, the aesthetics assessment may be more a ranking problem, since individuals have different aesthetics tastes but are more consistent with the relative aesthetic ranks. This can be achieved by specially designed aesthetics rating networks and ranking loss (like <ref type="bibr" target="#b42">[43]</ref>), thus our cropping model may be more powerful and consistent with human aesthetic preference among different cropping cases.</p><p>• Incorporating high-level human knowledge: In photo aesthetics analysis, numerous efforts have been seen in designing features for encapsulating the intertwined aesthetic rules. It might be promising to incorporate humanknowledge of photographic rules (e.g., region composition and rule of thirds) into our current cropping solution, since such domain knowledge is still instructive and widely used in photographic practice and visual design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>In this work, we proposed a deep learning-based photo cropping approach. The proposed deep model is composed of two sub-networks: an Attention Box Prediction (ABP) network and an Aesthetics Assessment (AA) network, both of which share multiple initial convolution layers. Our model is designed based on the determining-adjusting philosophy.</p><p>The ABP network infers initial cropping as a bounding box covering the visually important area (attention-aware determining), and then the AA network selects the best cropping with the highest aesthetic quality from a few cropping candidates generated around the initial cropping (aestheticbased adjusting). Extensive experiments have been conducted on several publicly available benchmarks and detailed analysis are reported on issues such as the effectiveness of each key components, and the computation cost. These experiments, together with a carefully designed user study, consistently validte the effectiveness and robustness of our algorithm in comparison to the state-of-the-art.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Conventional photo cropping process (e) Our deep learning based photo cropping architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. (a) An input photo to be cropped. (b) The predicted attention box (red) and cropping candidates generated from it (yellow). (c) The final cropping with the maximum estimated aesthetic value. (d) Conventional image cropping methods with sliding-judging cropping strategy, which is time-consuming and violates natural cropping procedure. (e) Our algorithm as a cascade of attention-aware candidate generation and aesthetics-based cropping selection, which handles photo cropping more naturally via a unified neural network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Architecture of our deep cropping model. It consists of two sub-networks: Attention Box Prediction (ABP) network and Aesthetics Assessment (AA) network, which share several convolutional layers at the bottom.</figDesc><graphic coords="4,324.20,131.68,108.52,72.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. (a) Input image I. (b) Ground truth attention map G. (c) Ground truth attention box generated via [3]. (d) Positive (red) and negative (blue) defaults boxes are generated for training ABP network according to ground truth attention box.</figDesc><graphic coords="4,440.72,131.68,108.52,72.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>ABP network used in our conference paper<ref type="bibr" target="#b47">[48]</ref> (b) Improved ABP network</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Architecture of the Attention Box Prediction (ABP) network, where the blue cuboid in (b) indicates the predicted attention map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(a)). Thus the network is trained to directly produce the attention box estimation. In this paper, we further improve the ABP network with extra supervision from the visual attention map G. It is demonstrated in Figure 4 (b), showing in the last convolutional layer. Specifically, we first generate an intermediate output Y for predicting the visual attention map (the blue cuboid in Figure 4(b)) via applying a convolution layer with a 1 × 1 kernel and sigmoid activation. Then the attention map Y is concatenated with the last convolutional layer in the channel direction, and the merged feature maps are fed into the bounding box prediction layers for generating the final attention box. Such design is based on the observation that attention box is derived from the visual attention via Equ. 2. The visual attention can act as a strong prior for attention box and teach the network to infer the attention box via leveraging the strong relevance between visual attention and attention box. More specially, given the resized groundtruth attention map G ∈ [0, 1] w/16×h/16 and the corresponding prediction map Y ∈ [0, 1] w/16×h/16 , we adopt the Kullback-Leibler Divergence (KL-Div) for measuring the training loss:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. (a) Initial cropping (red rectangle) predicted by the ABP network. (b) Cropping candidates (blue rectangles) generated around the initial cropping. (c) The final cropping selected as the candidate with the highest aesthetic score by the AA network.(Figure5(a)). Next, we generate a set of cropping candidates around the initial cropping, simulating human's adjusting of the location, size and ratio of the initial cropping. A rectangle can be uniquely determined via the coordinates of its top-left and bottom-right corners. For the top-left corner of the initial cropping, we define a set of offsets {-40, -32, • • • , -8, 0} in both x-and y-axes. Similarly, a set of offsets {0, 8, ..., 32, 40} in x-and y-axes are defined for the bottom-right corner. By disturbing the top-left and bottomright corners with these offsets, 2 we generate 6 4 = 1, 296 cropping candidates in total, which is far less than the sliding windows needed for traditional exhaustive cropping methods. Each of cropping candidates is designed to cover the entire initial cropping area, since the initial cropping is a minimum importance-preserving rectangle to be maintained during the cropping process (Figure5(b)).</figDesc><graphic coords="6,48.84,44.33,249.69,145.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Architecture of the Aesthetics Assessment (AA) network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Schematic diagram of our model in training.</figDesc><graphic coords="7,108.02,209.23,53.26,51.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Schematic diagram of our model in testing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Aesthetics assessment results via our AA network. The images with the highest predicted aesthetics values and those with the lowest predicted aesthetics values are presented in (a) and (b), respectively. (c) and (d) show the images that are miscategorized.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 10 .Fig. 11 .</head><label>1011</label><figDesc>Fig.10. Qualitative results on MSR-ICD<ref type="bibr" target="#b4">[5]</ref> and FLMS<ref type="bibr" target="#b5">[6]</ref> datasets. The red rectangles indicate the initial cropping generated by the ABP network, and the yellow windows correspond to the final cropping selected by the AA network.</figDesc><graphic coords="11,49.49,44.11,509.01,342.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 12 .</head><label>12</label><figDesc>Fig.12. Two cases for illustrating the limitations of our cropping solution, where the left images in (a) (b) show the importance maps and initial cropping (red rectangles) generated by the ABP network, the yellow windows in the right images are the final cropping selected by the AA network. We can find that the ABP network tends to select the most informative but small parts, which may discard some parts of a large object. See Section 4.5.1 for more discussion.</figDesc><graphic coords="12,313.85,45.23,78.97,54.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>3) is an intermediate loss for directly feeding supervision into the hidden layers, and the learned attention acts as a strong prior to improve the final bounding box prediction. The terminal loss L box (defined in Equ. 4) is for regressing the bounding box location and predicting the attention box score.Trained on existing attention prediction datasets, the ABP network learns to generate reliable attention boxes. Then we select the one with the highest prediction score (p 1 i ) as the initial cropping. This initial cropping covers the most informative part of the image, and it simulates human's procedure to place a cropping around the desired area</figDesc><table /><note><p>0162-8828 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1</head><label>1</label><figDesc>Datasets used for training and testing our cropping model.</figDesc><table><row><cell></cell><cell>Dataset</cell><cell cols="2">Ref Year #Images</cell><cell>Purpose Train Test</cell></row><row><cell>ABP</cell><cell cols="2">SALICON [26] 2015</cell><cell>20,000</cell></row><row><cell>network</cell><cell cols="2">PASCAL-S [30] 2014</cell><cell>850</cell></row><row><cell>AA network</cell><cell>AVA</cell><cell cols="2">[52] 2012 ∼250,000</cell></row><row><cell>Deep-cropping</cell><cell>MSR-ICD FLMS FCD</cell><cell>[5] 2013 [6] 2014 [10] 2017</cell><cell>950 500 1,743</cell></row><row><cell cols="5">is used for training and testing our AA network (Section</cell></row><row><cell cols="5">4.3); MSR-ICD, FLMS and FCD are used for accessing the</cell></row><row><cell cols="5">performance of our full cropping solution (Section 4.4). Next</cell></row><row><cell cols="5">we give detailed descriptions for each of the datasets.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 3</head><label>3</label><figDesc>Aesthetics assessment accuracy on the AVA dataset<ref type="bibr" target="#b51">[52]</ref>.</figDesc><table><row><cell>Method</cell><cell>Ours</cell><cell>AVA [52]</cell><cell cols="2">RAP-DCNN [40] RAP-RDCNN [40]</cell></row><row><cell cols="2">Accuracy 0.770</cell><cell>0.667</cell><cell>0.732</cell><cell>0.745</cell></row><row><cell>Method</cell><cell>Ours</cell><cell>RAP2 [62]</cell><cell>DMA-SPP [42]</cell><cell>DMA [42]</cell></row><row><cell cols="2">Accuracy 0.770</cell><cell>0.754</cell><cell>0.728</cell><cell>0.745</cell></row><row><cell>Method</cell><cell cols="2">Ours DMA-Alex [42]</cell><cell>ARC [43]</cell><cell>CPD[44]</cell></row><row><cell cols="2">Accuracy 0.770</cell><cell>0.754</cell><cell>0.773</cell><cell>0.774</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 4</head><label>4</label><figDesc>Performance of automatic image cropping on MSR-ICD dataset<ref type="bibr" target="#b4">[5]</ref>. Higher IoU score and lower BDE indicate better cropping predictor. MSR-ICD dataset offers separate annotations from three different expert photographers.</figDesc><table><row><cell></cell><cell>Method</cell><cell cols="2">* Photographer 1 IoU ↑ BDE ↓</cell><cell cols="2">Photographer 2 IoU ↑ BDE ↓</cell><cell cols="2">Photographer 3 IoU ↑ BDE ↓</cell><cell cols="2">Average IoU ↑ BDE ↓</cell></row><row><cell></cell><cell>ATC [13]</cell><cell>0.605</cell><cell>0.108</cell><cell>0.628</cell><cell>0.100</cell><cell>0.641</cell><cell>0.095</cell><cell>0.625</cell><cell>0.101</cell></row><row><cell></cell><cell>AIC [3]</cell><cell>0.469</cell><cell>0.142</cell><cell>0.494</cell><cell>0.131</cell><cell>0.512</cell><cell>0.123</cell><cell>0.491</cell><cell>0.132</cell></row><row><cell></cell><cell>LCC [5]</cell><cell>0.748</cell><cell>0.066</cell><cell>0.728</cell><cell>0.072</cell><cell>0.732</cell><cell>0.071</cell><cell>0.736</cell><cell>0.0670</cell></row><row><cell></cell><cell>MPC [63]</cell><cell>0.603</cell><cell>0.106</cell><cell>0.582</cell><cell>0.112</cell><cell>0.608</cell><cell>0.110</cell><cell>0.598</cell><cell>0.109</cell></row><row><cell></cell><cell>SPC [4]</cell><cell>0.396</cell><cell>0.177</cell><cell>0.394</cell><cell>0.178</cell><cell>0.385</cell><cell>0.182</cell><cell>0.391</cell><cell>0.179</cell></row><row><cell></cell><cell>ARC [43]</cell><cell>0.448</cell><cell>0.163</cell><cell>0.437</cell><cell>0.168</cell><cell>0.440</cell><cell>0.165</cell><cell>0.442</cell><cell>0.165</cell></row><row><cell></cell><cell>Ours [48] (conference version)</cell><cell>0.813</cell><cell>0.030</cell><cell>0.806</cell><cell>0.032</cell><cell>0.816</cell><cell>0.032</cell><cell>0.812</cell><cell>0.031</cell></row><row><cell></cell><cell>Ours</cell><cell>0.815</cell><cell>0.031</cell><cell>0.810</cell><cell>0.030</cell><cell>0.830</cell><cell>0.029</cell><cell>0.818</cell><cell>0.029</cell></row><row><cell></cell><cell>TABLE 5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Performance of automatic image cropping on the FLMS dataset [6].</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Higher IoU score and lower BDE indicate better cropping results.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell>Method</cell><cell cols="2">Measure IoU ↑ BDE ↓</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>ATC [13]</cell><cell>0.72</cell><cell>0.063</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>AIC</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FLMS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>* </p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 6</head><label>6</label><figDesc>Performance of automatic image cropping on the test set of FCD<ref type="bibr" target="#b9">[10]</ref>. Higher IoU score and lower BDE indicate better cropping results.</figDesc><table><row><cell>Dataset</cell><cell>Method</cell><cell cols="2">Measure IoU ↑ BDE ↓</cell></row><row><cell></cell><cell>ATC [13]</cell><cell>0.58</cell><cell>0.10</cell></row><row><cell></cell><cell>AIC [3]</cell><cell>0.47</cell><cell>0.13</cell></row><row><cell></cell><cell>ATC [13] + eDN [24] (MaxAvg)</cell><cell>0.35</cell><cell>0.17</cell></row><row><cell></cell><cell>ATC [13] + eDN [24] (MaxDiff)</cell><cell>0.48</cell><cell>0.13</cell></row><row><cell></cell><cell>ATC [13] + BMS [64] (MaxAvg)</cell><cell>0.34</cell><cell>0.18</cell></row><row><cell>FCD</cell><cell>ATC [13] + BMS [64] (MaxDiff)</cell><cell>0.39</cell><cell>0.16</cell></row><row><cell></cell><cell>SVM+DeCAF 7</cell><cell>0.51</cell><cell>0.13</cell></row><row><cell></cell><cell>AVA+DeCAF 7</cell><cell>0.52</cell><cell>0.12</cell></row><row><cell></cell><cell>FCD+DeCAF 7</cell><cell>0.60</cell><cell>0.10</cell></row><row><cell></cell><cell>Ours [48] (conference version)</cell><cell>0.63</cell><cell>0.09</cell></row><row><cell></cell><cell>Ours</cell><cell>0.65</cell><cell>0.08</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 7</head><label>7</label><figDesc>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2018.2840724, IEEE Transactions on Pattern Analysis and Machine Intelligence Ablation study on FLMS dataset<ref type="bibr" target="#b5">[6]</ref>.</figDesc><table><row><cell>Aspect</cell><cell>Description</cell><cell cols="2">Measure IoU↑ Time(s)↓</cell></row><row><cell>full model</cell><cell>ABP+AA</cell><cell>0.83</cell><cell>0.23</cell></row><row><cell></cell><cell>ABP</cell><cell>0.77</cell><cell>0.12</cell></row><row><cell>variant</cell><cell>Slidingwindow+AA ATC[13]+AA</cell><cell>0.69 0.78</cell><cell>134 1.3</cell></row><row><cell></cell><cell>AIC[3]+AA</cell><cell>0.73</cell><cell>32.5</cell></row><row><cell>competitor</cell><cell>ATC[13] AIC[3]</cell><cell>0.72 0.64</cell><cell>1.1 32.3</cell></row><row><cell>cropping</cell><cell>Randomly sampling (1,000 candidates)</cell><cell>0.80</cell><cell>0.23</cell></row><row><cell>candidates</cell><cell>Larger sampling step (step=16)</cell><cell>0.81</cell><cell>0.23</cell></row><row><cell cols="4">The evaluation results and computation time are sum-</cell></row><row><cell cols="4">marized in Table 7. We can draw the following three impor-</cell></row><row><cell>tant conclusions:</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">1) Aesthetics is important for photo cropping. The</cell></row><row><cell cols="4">improvement brought from AA network (ABP+AA:</cell></row><row><cell cols="4">0.83 vs ABP: 0.77, ATC+AA: 0.78 vs ATC: 0.72,</cell></row><row><cell cols="4">AIC+AA: 0.73 vs AIC: 0.64) indicates that the crop-</cell></row><row><cell cols="4">ping performance is benefited from aesthetic assess-</cell></row><row><cell cols="4">ment. This conclusion aligns with the claims shared</cell></row><row><cell cols="4">by pervious aesthetics-based cropping methods.</cell></row><row><cell cols="4">2) Both visual importance and photo aesthetics are</cell></row><row><cell cols="4">critical. A drop of performance can be observed</cell></row><row><cell cols="4">when only considering photo aesthetics (ABP+AA:</cell></row><row><cell cols="4">0.83 vs Sliding window+AA: 0.78). This observa-</cell></row><row><cell cols="4">tion can be attributed to omitting important image</cell></row><row><cell cols="4">content when only considering photo aesthetics.</cell></row></table><note><p>0162-8828 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A framework for visual saliency detection with applications to image thumbnailing</title>
		<author>
			<persName><forename type="first">L</forename><surname>Marchesotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cifarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="2232" to="2239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Scale and object aware image thumbnailing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="135" to="153" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Automatic image cropping : A computational complexity study</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="507" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Sensation-based photo cropping</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nishiyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Okabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia</title>
		<meeting>the ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="669" to="672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning the change for automatic image cropping</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Bing</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="971" to="978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Automatic image cropping using visual composition, boundary simplicity and content preservation models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mech</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia</title>
		<meeting>the ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1105" to="1108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="346" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Quantitative analysis of automatic image cropping algorithms: A dataset and comparative study</title>
		<author>
			<persName><forename type="first">Y.-L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-C</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="226" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Discriminant saliency for visual recognition from cluttered scenes</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="481" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Active visual segmentation</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Aloimonos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Cheong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kassim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="639" to="653" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automatic thumbnail cropping and its effectiveness</title>
		<author>
			<persName><forename type="first">B</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">B</forename><surname>Bederson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual ACM Symposium on User Interface Software and Technology</title>
		<meeting>the Annual ACM Symposium on User Interface Software and Technology</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="95" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A model of saliency-based visual attention for rapid scene analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1254" to="1259" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Saliency based on information maximization</title>
		<author>
			<persName><forename type="first">N</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tsotsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="155" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Graph-based visual saliency</title>
		<author>
			<persName><forename type="first">J</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="545" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">SUN: A bayesian framework for saliency using natural statistics</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Cottrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="32" to="32" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dynamic visual attention: Searching for coding length increments</title>
		<author>
			<persName><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="681" to="688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The discriminant center-surround hypothesis for bottom-up saliency</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="497" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning to predict where humans look</title>
		<author>
			<persName><forename type="first">T</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="2106" to="2113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Boosting bottom-up and top-down visual features for saliency estimation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="438" to="445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Visual attention: The past 25 years</title>
		<author>
			<persName><forename type="first">M</forename><surname>Carrasco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Research</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="1484" to="1525" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">State-of-the-art in visual attention modeling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="185" to="207" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Large-scale optimization of hierarchical features for saliency prediction in natural images</title>
		<author>
			<persName><forename type="first">E</forename><surname>Vig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2798" to="2805" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Predicting eye fixations using convolutional neural networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="362" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">SALICON: Saliency in context</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1072" to="1080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Shallow and deep convolutional networks for saliency prediction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sayrol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Giro-I Nieto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mcguinness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">E</forename><surname>O'connor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="598" to="606" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning to detect a salient object</title>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N.-N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Frequencytuned salient region detection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hemami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1597" to="1604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The secrets of salient object segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="280" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Salient object detection: A benchmark</title>
		<author>
			<persName><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5706" to="5722" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Image aesthetic assessment: An experimental survey</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="80" to="106" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Studying aesthetics in photographic images using a computational approach</title>
		<author>
			<persName><forename type="first">R</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="288" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The design of high-level features for photo quality assessment</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="419" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">High level describable attributes for predicting aesthetics and interestingness</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1657" to="1664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Aesthetic quality classification of photographs based on color harmony</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nishiyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Okabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Content-based photo quality assessment</title>
		<author>
			<persName><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2206" to="2213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Assessing the aesthetic quality of photographs using generic image descriptors</title>
		<author>
			<persName><forename type="first">L</forename><surname>Marchesotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1784" to="1791" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Scenic photo quality assessment with bag of aestheticspreserving features</title>
		<author>
			<persName><forename type="first">H.-H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-Y</forename><surname>Chien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia</title>
		<meeting>the ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1213" to="1216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">RAPID: Rating pictorial aesthetics using deep learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia</title>
		<meeting>the ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="457" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Blind image quality assessment using semi-supervised rectifier networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2877" to="2884" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep multipatch aggregation network for image style, aesthetics, and quality estimation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mech</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="990" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Photo aesthetics ranking network with attributes and content adaptation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mech</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="662" to="679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Composition-preserving deep photo aesthetics assessment</title>
		<author>
			<persName><forename type="first">L</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="497" to="506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Stereoscopic thumbnail creation via efficient stereo saliency detection</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning to photograph</title>
		<author>
			<persName><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia</title>
		<meeting>the ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="291" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Probabilistic graphlet transfer for photo cropping</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="802" to="815" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deep cropping via attention box prediction and aesthestics assessment</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">SSD: Single shot multibox detector</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">AVA: A large-scale database for aesthetic visual analysis</title>
		<author>
			<persName><forename type="first">N</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Marchesotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2408" to="2415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Keras</title>
		<author>
			<persName><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://github.com/fchollet/keras" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Tensorflow: Large-scale machine learning on heterogeneous distributed systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">The PASCAL Visual Object Classes (VOC) challenge</title>
		<author>
			<persName><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Image signature: Highlighting sparse salient regions</title>
		<author>
			<persName><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="194" to="201" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Context-aware saliency detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Goferman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1915" to="1926" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Rating image aesthetics using deep learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2021" to="2034" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Modeling photo composition and its application to photo re-arrangement</title>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Image Processing</title>
		<meeting>the IEEE International Conference on Image Processing</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2741" to="2744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Exploiting surroundedness for saliency detection: a boolean map approach</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="889" to="902" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="647" to="655" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
