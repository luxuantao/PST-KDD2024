<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Memory Augmented Graph Neural Networks for Sequential Recommendation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-12-26">26 Dec 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chen</forename><surname>Ma</surname></persName>
							<email>chen.ma2@mail.mcgill.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">McGill University</orgName>
								<address>
									<addrLine>2 Huawei Noah&apos;s Ark Lab in Montreal</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Liheng</forename><surname>Ma</surname></persName>
							<email>liheng.ma@mail.mcgill.ca</email>
							<affiliation key="aff1">
								<address>
									<settlement>Mila</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yingxue</forename><surname>Zhang</surname></persName>
							<email>yingxue.zhang@huawei.com</email>
							<affiliation key="aff0">
								<orgName type="institution">McGill University</orgName>
								<address>
									<addrLine>2 Huawei Noah&apos;s Ark Lab in Montreal</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jianing</forename><surname>Sun</surname></persName>
							<email>jianing.sun@huawei.com</email>
						</author>
						<author>
							<persName><forename type="first">Xue</forename><surname>Liu</surname></persName>
							<email>xueliu@cs.mcgill.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">McGill University</orgName>
								<address>
									<addrLine>2 Huawei Noah&apos;s Ark Lab in Montreal</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mark</forename><surname>Coates</surname></persName>
							<email>mark.coates@mcgill.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">McGill University</orgName>
								<address>
									<addrLine>2 Huawei Noah&apos;s Ark Lab in Montreal</addrLine>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Memory Augmented Graph Neural Networks for Sequential Recommendation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-12-26">26 Dec 2019</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1912.11730v1[cs.IR]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The chronological order of user-item interactions can reveal time-evolving and sequential user behaviors in many recommender systems. The items that users will interact with may depend on the items accessed in the past. However, the substantial increase of users and items makes sequential recommender systems still face non-trivial challenges: (1) the hardness of modeling the short-term user interests; (2) the difficulty of capturing the long-term user interests; (3) the effective modeling of item co-occurrence patterns. To tackle these challenges, we propose a memory augmented graph neural network (MA-GNN) to capture both the long-and short-term user interests. Specifically, we apply a graph neural network to model the item contextual information within a short-term period and utilize a shared memory network to capture the long-range dependencies between items. In addition to the modeling of user interests, we employ a bilinear function to capture the co-occurrence patterns of related items. We extensively evaluate our model on five real-world datasets, comparing with several state-of-the-art methods and using a variety of performance metrics. The experimental results demonstrate the effectiveness of our model for the task of Top-K sequential recommendation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>With the rapid growth of Internet services and mobile devices, personalized recommender systems play an increasingly important role in modern society. They can reduce information overload and help satisfy diverse service demands. Such systems bring significant benefits to at least two parties. They can: (i) help users easily discover products from millions of candidates, and (ii) create opportunities for product providers to increase revenue.</p><p>On the Internet, users access online products or items in a chronological order. The items a user will interact with in the future may depend strongly on the items he/she has accessed in the past. This property facilitates a practical application scenario-sequential recommendation. In the sequential recommendation task, in addition to the general user interest captured by all general recommendation models, we argue that there are three extra important factors to * Work done as interns at Huawei Noahs Ark Lab in Montreal. model: user short-term interests, user long-term interests, and item co-occurrence patterns. The user short-term interest describes the user preference given several recently accessed items in a short-term period. The user long-term interest captures the long-range dependency between earlier accessed items and the items users will access in the future. The item co-occurrence pattern illustrates the joint occurrences of commonly related items, such as a mobile phone and a screen protector.</p><p>Although many existing methods have proposed effective models, we argue that they do not fully capture the aforementioned factors. First, methods like Caser (Tang and Wang 2018), <ref type="bibr">MARank (Yu et al. 2019), and</ref><ref type="bibr">Fossil (He and</ref><ref type="bibr" target="#b1">McAuley 2016a</ref>) only model the short-term user interest and ignore the long-term dependencies of items in the item sequence. The importance of capturing the long-range dependency has been confirmed by <ref type="bibr" target="#b1">(Belletti, Chen, and Chi 2019)</ref>. Second, methods like SARSRec <ref type="bibr">(Kang and McAuley 2018)</ref> do not explicitly model the user short-term interest. Neglecting the user short-term interest prevents the recommender system from understanding the time-varying user intention over a short-term period. Third, methods like GC-SAN <ref type="bibr" target="#b10">(Xu et al. 2019</ref>) and GRU4Rec+ (Hidasi and Karatzoglou 2018) do not explicitly capture the item co-occurrence patterns in the item sequences. Closely related item pairs often appear one after the other and a recommender system should take this into account.</p><p>To incorporate the factors mentioned above, we propose a memory augmented graph neural network (MA-GNN) to tackle the sequential recommendation task. This consists of a general interest module, a short-term interest module, a long-term interest module, and an item co-occurrence module. In the general interest module, we adopt a matrix factorization term to model the general user interest without considering the item sequential dynamics. In the short-term interest module, we aggregate the neighbors of items using a GNN to form the user intentions over a short period. These can capture the local contextual information and structure <ref type="bibr" target="#b0">(Battaglia et al. 2018</ref>) within this short-term period. To model the long-term interest of users, we use a shared key-value memory network to generate the interest representations based on users' long-term item sequences. By doing this, other users with similar preferences will be taken into consideration when recommending an item. To combine the short-term and long-term interest, we introduce a gating mechanism in the GNN framework, which is similar to the long short-term memory (LSTM) <ref type="bibr" target="#b2">(Hochreiter and Schmidhuber 1997)</ref>. This controls how much the long-term or the short-term interest representation can contribute to the combined representation. In the item co-occurrence module, we apply a bilinear function to capture the closely related items that appear one after the other in the item sequence. We extensively evaluate our model on five real-world datasets, comparing it with many state-of-the-art methods using a variety of performance validation metrics. The experimental results not only demonstrate the improvements of our model over other baselines but also show the effectiveness of the proposed components.</p><p>To summarize, the major contributions of this paper are: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>General Recommendation</head><p>Early recommendation studies largely focused on explicit feedback <ref type="bibr" target="#b4">(Koren 2008)</ref>. The recent research focus is shifting towards implicit data <ref type="bibr" target="#b9">(Tran et al. 2019;</ref><ref type="bibr" target="#b4">Li and She 2017)</ref>.</p><p>Collaborative filtering (CF) with implicit feedback is usually treated as a Top-K item recommendation task, where the goal is to recommend a list of items to users that users may be interested in. It is more practical and challenging <ref type="bibr" target="#b8">(Pan et al. 2008)</ref>, and accords more closely with the real-world recommendation scenario. Early works mostly rely on matrix factorization techniques <ref type="bibr" target="#b3">(Hu, Koren, and Volinsky 2008;</ref><ref type="bibr" target="#b8">Rendle et al. 2009)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sequential Recommendation</head><p>The sequential recommendation task takes as input the chronological item sequence.  <ref type="bibr" target="#b4">Huang et al. 2018</ref>) are also adopted to memorize the items that will play a role in predicting future user actions. However, our proposed model is different from previous models. We apply a graph neural network with external memories to capture the short-term item contextual information and long-term item dependencies. In addition, we also incorporate an item co-occurrence module to model the relationships between closely related items.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Problem Formulation</head><p>The recommendation task considered in this paper takes sequential implicit feedback as training data. The user preference is represented by a user-item sequence in chronological order, S u = (I 1 , I 2 , ..., I |S u | ), where I * are item indexes that user u has interacted with. Given the earlier subsequence S u 1:t (t &lt; |S u |) of M users, the problem is to recommend a list of K items from a total of N items (K &lt; N ) to each user and evaluate whether the items in S u t+1:|S u | appear in the recommended list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methodology</head><p>In this section, we introduce the proposed model, MA-GNN, which applies a memory augmented graph neural network for the sequential recommendation task. We introduce four factors that have an impact on the user preference and intention learning. Then we introduce the prediction and training procedure of the proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>General Interest Modeling</head><p>The general or static interest of a user captures the inherent preferences of the user and is assumed to be stable over time. To capture the general user interest, we employ a matrix factorization term without considering the sequential dynamics of items. This term takes the form p u • q j , where p u ∈ R d is the embedding of user u, q j ∈ R d is the output embedding of item j, and d is the dimension of the latent space. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Short-term Interest Modeling</head><p>A user's short-term interest describes the user's current preference and is based on several recently accessed items in a short-term period. The items a user will interact with in the near future are likely to be closely related to the items she just accessed, and this property of user behaviors has been confirmed in many previous works (Tang and Wang 2018; Hidasi and Karatzoglou 2018; He and McAuley 2016a). Therefore, it is very important in sequential recommendation to effectively model the user's short-term interest, as reflected by recently accessed items.</p><p>To explicitly model the user short-term interest, we conduct a sliding window strategy to split the item sequence into fine-grained sub-sequences. We can then focus on the recent sub-sequence to predict which items will appear next and ignore the irrelevant items that have less impact. For each user u, we extract every |L| successive items as input and their next |T | items as the targets to be predicted, where L u,l = (I l , I l+1 , ..., I l+|L|−1 ) is the l-th sub-sequence of user u. Then the problem can be formulated as: in the useritem interaction sequence S u , given a sequence of |L| successive items, how likely is it that the predicted items accord with the target |T | items for that user. Due to their ability to perform neighborhood information aggregation and local structure learning <ref type="bibr" target="#b0">(Battaglia et al. 2018)</ref>, graph neural networks (GNNs) are a good match for the task of aggregating the items in L u,l to learn user short-term interests.</p><p>Item Graph Construction. Since item sequences are not inherently graphs for GNN training, we need to build a graph to capture the connections between items. For each item in item sequences, we extract several subsequent items (three items in our experiments) and add edges between them. We perform this for each user and count the number of edges of extracted item pairs across all users. Then we row-normalize the adjacency matrix. As such, relevant items that appear closer to one another in the sequence can be extracted. An example of how to extract item neighbors and build the adjacency matrix is shown in Figure <ref type="figure" target="#fig_1">2</ref>. We denote the extracted adjacency matrix as A, where A i,k denotes the normalized node weight of item k regarding item i. And the neighboring items of item i is denoted as N i . Short-term Interest Aggregation. To capture the user short-term interest, we use a two-layer GNN to aggregate the neighboring items in L u,l for learning the user shortterm interest representation. Formally, for an item j in the l-th short-term window L u,l , its input embedding is represented as e j ∈ R d . The user short-term interest is then:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>……</head><formula xml:id="formula_0">h i = tanh(W (1) • [ k∈Ni e k A i,k ; e i ]) , ∀i ∈ L u,l , (1) p S u,l = tanh(W (2) • [ 1 |L| i∈L u,l h i ; p u ]) ,<label>(2)</label></formula><p>where Based on the summarized user short-term interest, the items that a user will access next can be inferred. However, directly applying the above GNN to make predictions clearly neglects the long-term user interest in the past H u,l = (I 1 , I 2 , ..., I l−1 ). There may be some items outside the short-term window L u,l that can express the user preference or indicate the user state. These items can play an important role in predicting items that will be accessed in the near future. This long-term dependency has been confirmed in many previous works <ref type="bibr" target="#b6">(Liu et al. 2018;</ref><ref type="bibr" target="#b10">Xu et al. 2019;</ref><ref type="bibr" target="#b1">Belletti, Chen, and Chi 2019)</ref>. Thus, how to model the longterm dependency and balance it with the short-term context is a crucial question in sequential recommendation.</p><formula xml:id="formula_1">[• ; •] ∈ R 2d denotes vertical concatenation, W (1) , W (2) ∈ R</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Long-term Interest Modeling</head><p>To capture the long-term user interest, we can use external memory units <ref type="bibr" target="#b8">(Sukhbaatar et al. 2015;</ref><ref type="bibr" target="#b12">Zhang et al. 2017)</ref> to store the time-evolving user interests given the user accessed items in H u,l = (I 1 , I 2 , ..., I l−1 ). However, maintaining the memory unit for each user has a huge memory overhead to store the parameters. Meanwhile, the memory unit may capture information that is very similar to that represented by the user embedding p u . Therefore, we propose to use a memory network to store the latent interest representation shared by all users, where each memory unit represents a certain type of latent user interest, such as the user interest regarding different categories of movies. Given the items accessed by a user in the past H u,l , we can learn a combination of different types of interest to reflect the user long-term interest (or state) before L u,l .</p><p>Instead of performing a summing operation to generate the as in the original memory network <ref type="bibr" target="#b8">(Sukhbaatar et al. 2015)</ref>, we apply a multi-dimensional attention model to generate the query embedding. This allows discriminating informative items that can better reflect the user preference to have a greater influence on the positioning of the corresponding external memory units. Formally, we denote the item embeddings in H u,l as H u,l ∈ R d×|H u,l | . The multidimensional attention to generate the query embedding z u,l is computed as:</p><formula xml:id="formula_2">H u,l := H u,l + PE(H u,l ) , S u,l = softmax W (3) a tanh(W (1) a H u,l + (W (2) a p u ) ⊗ 1 φ ) , Z u,l = tanh(S u,l • H u,l ) , u,l = avg(Z u,l ) ,</formula><p>(3) where PE(•) is the sinusoidal positional encoding function that maps the item positions into position embeddings, which is the same as the one used in Transformer <ref type="bibr" target="#b9">(Vaswani et al. 2017)</ref>. φ equals to |H u,l |, ⊗ denotes the outer product. W</p><p>(1)</p><formula xml:id="formula_3">a , W<label>(2)</label></formula><p>a ∈ R d×d and W</p><p>(3) a ∈ R h×d are the learnable parameters in the attention model, and h is the hyperparameter to control the number of dimensions in the attention model. S u,l ∈ R h×|H u,l | is the attention score matrix. Z u,l ∈ R h×d is the matrix representation of the query, and each of the h rows represents a different aspect of the query. Finally, z u,l ∈ R d is the combined query embedding that averages the different aspects.</p><p>Given the query embedding z u,l , we use this query to find the appropriate combination of the shared user latent interest in the memory network. Formally, the keys and values of the memory network <ref type="bibr" target="#b8">(Sukhbaatar et al. 2015;</ref><ref type="bibr" target="#b6">Miller et al. 2016</ref>) are denoted as K ∈ R d×m and V ∈ R d×m , respectively, where m is the number of memory units in the memory network. Therefore, the user long-term interest embedding can be modeled as:</p><formula xml:id="formula_4">s i = softmax(z u,l • k i ) , o u,l = i s i v i , p H u,l = z u,l + o u,l ,<label>(4)</label></formula><p>where k i , v i ∈ R d are the i-th memory unit and the superscript H denotes the representation is from the user longterm interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interest Fusion</head><p>We have obtained the user short-term interest representation and the long-term interest representation. The next aim is to combine these two kinds of hidden representations in the GNN framework to facilitate the user preference prediction on unrated items. Here, we modify Eq. 2 to bridge the user short-term interest and long-term interest. Specifically, we borrow the idea of LSTM (Hochreiter and Schmidhuber 1997) that uses learnable gates to balance the current inputs and historical hidden states. Similarly, we propose a learnable gate to control how much the recent user interest representation and the long-term user interest representation can contribute to the combined user interest for item prediction:</p><formula xml:id="formula_5">g u,l = σ   W (1) g • 1 |L| i∈L u,l h i + W (2) g • p H u,l + W (3) g • p u   , p C u,l = g u,l 1 |L| i∈L u,l h i + (1 d − g u,l ) p H u,l ,<label>(5) where W</label></formula><p>(1)</p><formula xml:id="formula_6">g , W (2) g , W<label>(3)</label></formula><p>g ∈ R d×d are the learnable parameters in the gating layer, denotes the element-wise multiplication, and g u,l ∈ R d is the learnable gate. The superscript C denotes the fusion of long-and short-term interests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Item Co-occurrence Modeling</head><p>Successful learning of pairwise item relationships is a key component of recommender systems due to its effectiveness and interpretability. This has been studied and exploited in many recommendation models (Deshpande and Karypis 2004; Ning, Desrosiers, and Karypis 2015). In the sequential recommendation problem, the closely related items may appear one after another in the item sequence. For example, after purchasing a mobile phone, the user is much more likely to buy a mobile phone case or protector. To capture the item co-occurrence patterns, we use a bilinear function to explicitly model the pairwise relations between the items in L u,l and other items. This function takes the form e i W r q j , where W r ∈ R d×d is a matrix of the learnable parameters that captures the correlations between item latent features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prediction and Training</head><p>To infer the user preference, we have a prediction layer to combine the aforementioned factors together:</p><formula xml:id="formula_7">ru,j = p u • q j + p C u,l • q j + 1 |L| i∈L u,l e i W r q j . (6)</formula><p>As the training data is derived from the user implicit feedback, we optimize the proposed model with respect to the Bayesian Personalized Ranking objective <ref type="bibr" target="#b8">(Rendle et al. 2009</ref>) via gradient descent. This involves optimizing the pairwise ranking between the positive (observed) and negative (non-observed) items:</p><formula xml:id="formula_8">arg min U,Q,E,Θ u l (L u,l ,H u,l ,j,k) − log σ(r u,j − ru,k )+ λ(||P|| 2 + ||Q|| 2 + ||E|| 2 + ||Θ|| 2 ) .<label>(7)</label></formula><p>Here j denotes the positive item in T u,l , and k denotes the randomly sampled negative item, σ is the sigmoid function, Θ denotes other learnable parameters in the model, and λ is the regularization parameter. p * , q * and e * are column vectors of P, Q and E, respectively. When minimizing the objective function, the partial derivatives w.r.t. all parameters are computed by gradient descent with back-propagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation</head><p>In this section, we first describe the experimental set-up. We then report the results of conducted experiments and demonstrate the effectiveness of the proposed modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head><p>The MovieLens-20M is a user-movie dataset collected from the MovieLens website; the dataset has 20 million user-movie interactions. The Amazon-Books and Amazon-CDs datasets are adopted from the Amazon review dataset with different categories, i.e., CDs and Books, which cover a large amount of user-item interaction data, e.g., user ratings and reviews. The Goodreads-Children and Goodreads-Comics datasets were collected in late 2017 from the goodreads website with a focus on the genres of Children and Comics. In order to be consistent with the implicit feedback setting, we keep those with ratings no less than four (out of five) as positive feedback and treat all other ratings as missing entries on all datasets. To filter noisy data, we only keep the users with at least ten ratings and the items with at least ten ratings. The data statistics after preprocessing are shown in Table <ref type="table" target="#tab_4">1</ref>.</p><p>For each user, we use the earliest 70% of the interactions in the user sequence as the training set and use the next 10% of interactions as the validation set for hyper-parameter tuning. The remaining 20% constitutes the test set for reporting model performance. Note that during the testing procedure, the input sequences include the interactions in both the training set and validation set. The learning of all the models is carried out five times to report the average results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Metrics</head><p>We evaluate all the methods in terms of Recall@K and NDCG@K. For each user, Recall@K (R@K) indicates what percentage of her rated items emerge in the top K recommended items. NDCG@K (N@K) is the normalized discounted cumulative gain at K, which takes the position of correctly recommended items into account. K is set to 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods Studied</head><p>To demonstrate the effectiveness of our model, we compare to the following recommendation methods: (1) MA-GNN, the proposed model, which applies a memory augmented GNN to combine the recent and historical user interests and adopts a bilinear function to explicitly capture the item-item relations.</p><p>Table <ref type="table">2</ref>: The performance comparison of all methods in terms of Recall@10 and NDCG@10. The best performing method is boldfaced. The underlined number is the second best performing method. * indicates the statistical significance for p &lt;= 0.01 compared to the best baseline method based on the paired t-test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CDs Books</head><p>Children Comics ML20M R@10 N@10 R@10 N@10 R@10 N@10 R@10 N@10 R@10 N@10 BPRMF 0. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment Settings</head><p>In the experiments, the latent dimension of all the models is set to 50. For the session-based methods, we treat the items in a short-term window as one session. For GRU4Rec and GRU4Rec+, we find that a learning rate of 0.001 and batch size of 50 can achieve good performance. These two methods adopt Top1 loss and BPR-max loss, respectively. For GC-SAN, we set the weight factor ω to 0.5 and the number of self-attention blocks k to 4. For Caser, we follow the settings in the author-provided code to set |L| = 5, |T | = 3, the number of horizontal filters to 16, and the number of vertical filters to 4. For SASRec, we set the number of selfattention blocks to 2, the batch size to 128, and the maximum sequence length to 50. For MARank, we follow the original paper to set the number of depending items as 6 and the number of hidden layers as 4. The network architectures of the above methods are configured to be the same as described in the original papers. The hyper-parameters are tuned on the validation set.</p><p>For MA-GNN, we follow the same setting in Caser to set |L| = 5 and |T | = 3. Hyper-parameters are tuned by grid search on the validation set. The embedding size d is also set to 50. The value of h and m are selected from {5, 10, 15, 20}. The learning rate and λ are set to 0.001 and 0.001, respectively. The batch size is set to 4096.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance Comparison</head><p>The performance comparison results are shown in Table <ref type="table">2</ref>.</p><p>Observations about our model. First, the proposed model, MA-GNN, achieves the best performance on all five datasets with all evaluation metrics, which illustrates the superiority of our model. Second, MA-GNN outperforms SASRec. Although SASRec adopts the attention model to distinguish the items users have accessed, it neglects the common item co-occurrence patterns between two closely related items, which is captured by our bilinear function. Third, MA-GNN achieves better performance than Caser, GC-SAN and MARank. One major reason is that these three methods only model the user interests in a short-term window or session, but fail to capture the long-term item dependencies. On the contrary, we have a memory network to generate the long-term user interest. Fourth, MA-GNN obtains better results than GRU4Rec and GRU4Rec+. One possible reason is that GRU4Rec and GRU4Rec+ are session-based methods that do not explicitly model the user general interests. Fifth, MA-GNN outperforms BPRMF. BPRMF only captures the user general interests, and does not incorporate the sequential patterns of user-item interactions. As such, BPRMF fails to capture the user short-term interests.</p><p>Other observations. First, all the results reported on MovieLens-20M, GoodReads-Children and GoodReads-Comics are better than the results on other datasets. The major reason is that the other datasets are sparser and data sparsity negatively impacts recommendation performance. Second, MARank, SASRec and GC-SAN outperform Caser on most of the datasets. The main reason is that these methods can adaptively measure the importance of different items in the item sequence, which may lead to more personalized user representation learning. Third, Caser achieves better performance than GRU4Rec and GRU4Rec+ in most cases. One possible reason is that Caser explicitly inputs the user embeddings into its prediction layer, which allows it to learn general user interests. Fourth, GRU4Rec+ performs better than GRU4Rec on all datasets. The reason is that GRU4Rec+ not only captures the sequential patterns in the user-item sequence but also has a superior objective function-BPR-max. Fifth, all the methods perform better than BPR. This illustrates that a technique that can only perform effective modeling of the general user interests is incapable of adequately capturing the user sequential behavior.</p><p>Table <ref type="table">3</ref>: The ablation analysis. S denotes the short-term interest module, H denotes the long-term interest module, concat denotes the concatenation operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Architecture</head><p>CDs Books R@10 N@10 R@10 N@10 (1) MF </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Analysis</head><p>To verify the effectiveness of the proposed short-term interest modeling module, long-term interest modeling module, and item co-occurrence modeling module, we conduct an ablation study in Table <ref type="table">3</ref>. This demonstrates the contribution of each module to the MA-GNN model. In (1), we utilize only the BPR matrix factorization without other components to show the performance of modeling user general interests. In (2), we incorporate the user short-term interest by the vanilla graph neural network (Eq. 1 and 2) on top of (1). In (3), we integrate the user long-term interest with the short-term interest via the proposed interest fusion module (Eq. 3, 4 and 5) on top of (2). In (4), we replace the interest fusion module in (3) with the concatenation operation to link the short-term interest and long-term interest. In (5), we replace the concatenation operation with a gated recurrent unit (Cho et al. 2014) (GRU). In (6), we present the overall MA-GNN model to show the effectiveness of the item co-occurrence modeling module.</p><p>From the results shown in Table <ref type="table">3</ref>, we make the following observations. First, comparing (1) and ( <ref type="formula" target="#formula_0">2</ref>)-( <ref type="formula">6</ref>), we can observe that although the conventional BPR matrix factorization can capture the general user interests, it cannot effectively model the short-term user interests. Second, from (1) and ( <ref type="formula" target="#formula_0">2</ref>), we observe that incorporating the short-term interest using the conventional aggregation function of the GNN slightly improves the model performance. Third, in (3), ( <ref type="formula" target="#formula_4">4</ref>) and ( <ref type="formula" target="#formula_5">5</ref>), we compare three ways to bridge the user short-term interest and long-term interest. From the results, we can observe that our proposed gating mechanism achieves considerably better performance than concatenation or the GRU, which demonstrates that our gating mechanism can adaptively combine these two kinds of hidden representations. Fourth, from (3) and ( <ref type="formula">6</ref>), we observe that by incorporating the item co-occurrence pattern, the performance further improves. The results show the effectiveness of explicitly modeling the co-occurrence patterns of the items that a user has accessed and those items that the user may interact with in the future. The item co-occurrence pattern can provide a significant amount of supplementary information to help capture the user sequential dynamics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Influence of Hyper-parameters</head><p>The dimension h of the multi-dimensional attention model and the number m of the memory units are two important hyper-parameters in the proposed model. We investigate their effects on CDs and Comics datasets in Figure <ref type="figure" target="#fig_5">3</ref>.</p><p>From the results in Figure <ref type="figure" target="#fig_5">3</ref>, we observe that both the multi-dimensional attention and the memory network contribute to capturing the long-term user interests. These two components lead to a larger improvement in performance for the CDs dataset compared to the Comics dataset, indicating that they may help to alleviate the data sparsity problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Memory Visualization</head><p>To validate whether each memory unit can represent a certain type of user interests, we conduct a case study on the MovieLens dataset to verify how each memory unit functions given different movies. We randomly select a user and    several movies she watched. For simplicity, we treat each selected item as a query to visualize the attention weight s i computed by Eq. 4. In this case, we set the number of memory units m to 10.</p><p>From Figure we observe that our memory units perform differently given different types of movies, which may illustrate that each memory unit in the memory network can represent one type of the user interest. For example, the Three trilogy has quite similar attention weights in the memory network, since these three movies are loosely based on three political ideals in the motto of the French Republic. Die Hard is an action thriller movie, which is distinct from any other movies in the case study, explaining why it has a different weight pattern.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we propose a memory augmented graph neural network (MA-GNN) for sequential recommendation. MA-GNN applies a GNN to model items' short-term contextual information, and utilize a memory network to capture the long-range item dependency. In addition to the user interest modeling, we employ a bilinear function to model the feature correlations between items. Experimental results on five real-world datasets clearly validate the performance advantages of our model over many state-of-the-art methods and demonstrate the effectiveness of the proposed modules.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The model architecture of MA-GNN. ⊕ denotes element-wise addition and denotes element-wise multiplication.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Item adjacent matrix construction example.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>proposed model is evaluated on five real-world datasets from various domains with different sparsities: MovieLens-20M (Harper and Konstan 2016), Amazon-Books and Amazon-CDs (He and McAuley 2016b), Goodreads-Children and Goodreads-Comics (Wan and McAuley 2018).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>h and m on CDs</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>h and m on Comics</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The variation of h and m.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The attention visualization of memory networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>•</head><label></label><figDesc>To model the short-term and long-term interests of users, we propose a memory augmented graph neural network to capture items' short-term contextual information and long-range dependencies.• To effectively fuse the short-term and long-term interests, we incorporate a gating mechanism within the GNN framework to adaptively combine these two kinds of hidden representations. • To explicitly model the item co-occurrence patterns, we use a bilinear function to capture the feature correlations between items.</figDesc><table /><note>• Experiments on five real-world datasets show that the proposed MA-GNN model significantly outperforms the state-of-the-art methods for sequential recommendation.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc><ref type="bibr" target="#b8">Tang and Wang 2018;</ref><ref type="bibr" target="#b11">Yu et al. 2019)</ref> indicating which items are closely relevant to the items in L u,l .</figDesc><table /><note>d×2d are the learnable parameters in the graph neural network, and the superscript S denotes that the representation is from the user short-term interest. By aggregating neighbors of items in L u,l , p S u,l represents a union-level summary (</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>The statistics of the datasets.</figDesc><table><row><cell>Dataset</cell><cell cols="3">#Users #Items #Interactions Density</cell></row><row><cell>CDs</cell><cell>17,052 35,118</cell><cell>472,265</cell><cell>0.079%</cell></row><row><cell>Books</cell><cell>52,406 41,264</cell><cell>1,856,747</cell><cell>0.086%</cell></row><row><cell cols="2">Children 48,296 32,871</cell><cell>2,784,423</cell><cell>0.175%</cell></row><row><cell>Comics</cell><cell>34,445 33,121</cell><cell>2,411,314</cell><cell>0.211%</cell></row><row><cell cols="2">ML20M 129,797 13,649</cell><cell>9,921,393</cell><cell>0.560%</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Relational inductive biases, deep learning, and graph networks</title>
		<author>
			<persName><surname>Battaglia</surname></persName>
		</author>
		<idno>CoRR abs/1806.01261</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Belletti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi</forename><forename type="middle">;</forename><surname>Belletti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The movielens datasets: History and context. TiiS 5(4):19:1-19:19. [He and McAuley</title>
				<editor>
			<persName><surname>Recsys</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2004">2019. 2019. 2018. 2018. 2013. 2013. 2014. 2014. 2004. 2016. 2016. 2016a. 2016a. 2016b. 2017. 2017. 2017. 2018. 2018</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="143" to="177" />
		</imprint>
	</monogr>
	<note>Session-based recommendations with recurrent neural networks. In ICLR (Poster</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Collaborative filtering for implicit feedback datasets</title>
		<author>
			<persName><forename type="first">Koren</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><surname>Volinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
				<imprint>
			<date type="published" when="2008">2008. 2008</date>
			<biblScope unit="page" from="263" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Factorization meets the neighborhood: a multifaceted collaborative filtering model</title>
		<author>
			<persName><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2008">2018. 2018. 2018. 2018. 2008. 2017. 2017</date>
		</imprint>
	</monogr>
	<note>CIKM</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">xdeepfm: Combining explicit and implicit feature interactions for recommender systems</title>
		<author>
			<persName><forename type="first">Lian</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Point-of-interest recommendation: Exploiting selfattentive autoencoders with neighbor-aware influence</title>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2016">2018. 2018. 2018. 2018. 2019. 2019. 2016</date>
		</imprint>
	</monogr>
	<note>CIKM</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A comprehensive survey of neighborhood-based recommendation methods</title>
		<author>
			<persName><forename type="first">Desrosiers</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Karypis ; Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Desrosiers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recommender Systems Handbook</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="37" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Personalized top-n sequential recommendation via convolutional sequence embedding</title>
		<author>
			<persName><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
				<editor>
			<persName><surname>Uai. [rendle</surname></persName>
		</editor>
		<editor>
			<persName><surname>Freudenthaler</surname></persName>
		</editor>
		<editor>
			<persName><surname>Schmidt-Thieme</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2008">2008. 2008. 2009. 2010. 2010. 2015. 2018. 2018</date>
		</imprint>
	</monogr>
	<note>WSDM</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Item recommendation on monotonic behavior chains</title>
		<author>
			<persName><forename type="first">Tran</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2017">2019. 2019. 2017. 2018. 2018</date>
		</imprint>
	</monogr>
	<note>Signed distance-based deep memory recommender. In RecSys</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Graph contextualized self-attention network for session-based recommendation</title>
		<author>
			<persName><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2017">2019. 2019. 2017</date>
		</imprint>
	</monogr>
	<note>IJCAI</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-order attentive ranking model for sequential recommendation</title>
		<author>
			<persName><forename type="first">Yu</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dynamic key-value memory networks for knowledge tracing</title>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
