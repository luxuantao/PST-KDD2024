<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards real-time unsupervised monocular depth estimation on CPU</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Matteo</forename><surname>Poggi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Filippo</forename><surname>Aleotti</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Fabio</forename><surname>Tosi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Stefano</forename><surname>Mattoccia</surname></persName>
						</author>
						<title level="a" type="main">Towards real-time unsupervised monocular depth estimation on CPU</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">CC9229FD1487A3E9FD2FB7EA90255787</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Unsupervised depth estimation from a single image is a very attractive technique with several implications in robotic, autonomous navigation, augmented reality and so on. This topic represents a very challenging task and the advent of deep learning enabled to tackle this problem with excellent results. However, these architectures are extremely deep and complex. Thus, real-time performance can be achieved only by leveraging power-hungry GPUs that do not allow to infer depth maps in application fields characterized by low-power constraints. To tackle this issue, in this paper we propose a novel architecture capable to quickly infer an accurate depth map on a CPU, even of an embedded system, using a pyramid of features extracted from a single input image. Similarly to stateof-the-art, we train our network in an unsupervised manner casting depth estimation as an image reconstruction problem. Extensive experimental results on the KITTI dataset show that compared to the top performing approach our network has similar accuracy but a much lower complexity (about 6% of parameters) enabling to infer a depth map for a KITTI image in about 1.7 s on the Raspberry Pi 3 and at more than 8 Hz on a standard CPU. Moreover, by trading accuracy for efficiency, our network allows to infer maps at about 2 Hz and 40 Hz respectively, still being more accurate than most state-of-theart slower methods. To the best of our knowledge, it is the first method enabling such performance on CPUs paving the way for effective deployment of unsupervised monocular depth estimation even on embedded systems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Several application fields such as robotic, autonomous navigation, augmented reality and many others can take advantage of accurate and real-time depth measurements. Popular active sensors such as LIDAR, Kinect, Time-of-Flight (ToF) infer depth by perturbing the sensed environment according to different technologies. Despite their effectiveness in specific circumstances (e.g., the Kinect for close range indoor deployment), passive sensors based on binocular/multi-view stereo, structure from motion and, more recent, monocular depth sensors are very attractive. In fact, they are potentially cheaper, smaller and more lightweight than active sensors. Moreover, passive depth sensors don't have moving parts like LIDAR and don't require to perturb the sensed environment thus avoiding interference with other devices.</p><p>The literature concerned with passive depth sensors is large but in recent years most methods have been outperformed by approaches leveraging on Convolutional Neural Networks (CNNs). In particular, CNNs allowed to effectively increase the accuracy of passive techniques by casting the depth data generation as a supervised learning task. CNNs Fig. <ref type="figure">1:</ref> (Top) Input image from KITTI dataset <ref type="bibr" target="#b0">[1]</ref>. Qualitative comparison between state-of-the-art unsupervised monocular depth estimation method <ref type="bibr" target="#b1">[2]</ref> (Middle) and the proposed PyD-Net architecture (Bottom). Our model runs in real-time on standard CPUs and takes, in its most accurate configuration reported in this figure, 1.7 s on the low-power ARM CPU of the Raspberry Pi 3 with an overall power consumption, including a web-camera, of about 3.5 W. also enabled depth estimation from a single input image thus avoiding acquisitions from multiple view points for this purpose. While some seminal works concerned with monocular depth estimation <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> require a large amount of training samples with depth labels, more recent works <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b1">[2]</ref> exploit unsupervised signals in form of image reconstruction losses to train CNNs on monocular sequences <ref type="bibr" target="#b5">[6]</ref> or stereo pairs <ref type="bibr" target="#b1">[2]</ref> required only for training and not for inference. With this latter strategy, difficult to source ground-truth labels are replaced with standard imagery enabling to collect training samples easily and in large amounts. Nevertheless, current architectures for monocular depth estimation are very deep and complex; for these reasons they require dedicated hardware such as high-end and power-hungry GPUs. This fact precludes to infer depth from a single image in many interesting applications fields characterized by low-power constraints (e.g. UAVs, wearable devices, ...) and thus in this paper we propose a novel architecture for accurate and unsupervised monocular depth estimation aimed at overcoming this issue. By building our deep network inspired by the success of pyramidal architectures in other fields <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref> we are able to decimate the amount of parameters w.r.t. stateof-the-art solutions thus dramatically reducing both memory footprint and runtime required to infer depth. We call our model Pyramidal Depth Network (PyD-Net) and we train it in unsupervised manner as proposed in <ref type="bibr" target="#b1">[2]</ref>, representing the top-performing method in this field. Compared to such work, our model is about 94% smaller enabling on CPUs a notable speed-up at the cost of a slightly reduced depth accuracy. Moreover, our proposal outperforms other state-of-the-art methods. Our design strategy enables the deployment of PyD-Net even on embedded devices, such as the Raspberry Pi 3, thus allowing to infer a full depth map at about 2 Hz using less than 150 MB out of 1 GB memory available in such inexpensive device. To the best of our knowledge, our proposal is the first approach enabling fast and accurate unsupervised monocular depth estimation on standard and embedded CPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Although depth estimation from images has a long history in computer vision <ref type="bibr" target="#b8">[9]</ref>, methods using a single image are much more recent and mostly based on machine learning techniques. These works and other efficient end-to-end approaches for dense prediction tasks such as optical flow are relevant to our work.</p><p>Supervised monocular depth estimation. Saxena et al. <ref type="bibr" target="#b9">[10]</ref> proposed Make3D, a patch-based model estimating 3D location and orientation of planes by means of a Markov Random Field framework. It suffers in presence of thin structures and does not process global context information because of its local nature. Liu et al. <ref type="bibr" target="#b4">[5]</ref> trained a CNN to estimate depth from single camera, while Ladicky et al. <ref type="bibr" target="#b2">[3]</ref> included semantic information into their model to obtain more accurate predictions. In <ref type="bibr" target="#b10">[11]</ref> Karsch et al. obtained more consistent predictions by casting the problem as a nearest neighbor search with respect to depth images from a training set required at testing time. Eigen et al. <ref type="bibr" target="#b3">[4]</ref> deployed a multi-scale CNN trained on a large dataset to infer depth for each pixel in a single image. Differently from <ref type="bibr" target="#b4">[5]</ref>, whose network was trained to compute more robust data terms and pairwise terms fed to a further optimization step, this approach directly infers the depth map. Several works followed <ref type="bibr" target="#b3">[4]</ref> to improve its performance by means of CRF regularization <ref type="bibr" target="#b11">[12]</ref>, casting the problem as a classification task <ref type="bibr" target="#b12">[13]</ref>, designing more robust loss functions <ref type="bibr" target="#b13">[14]</ref> or using scene priors for joint plane normals estimation <ref type="bibr" target="#b14">[15]</ref>. Ummenhofer et al. <ref type="bibr" target="#b15">[16]</ref> proposed DeMoN, a deep model to infer both depth and ego-motion from a pair of subsequent frames acquired by a single camera. Common to all these works is the supervised paradigm adopted for training, requiring a large amount of labeled data particularly crucial for successfully learn a robust depth representation from a single image.</p><p>Unsupervised monocular estimation. Other recent works exploit CNNs without using labeled data. In particular, Flynn et al. <ref type="bibr" target="#b16">[17]</ref> proposed DeepStereo, a deep architecture trained on images acquired by multiple cameras to synthesize images from new view points. In the context of binocular stereo, given an input reference image, Deep3D by Xie et al. <ref type="bibr" target="#b17">[18]</ref> generates the corresponding target view by learning a distribution over all possible disparities at each pixel on the source image. For training, an image reconstruction loss is minimized. Similarly, Garg et al. <ref type="bibr" target="#b18">[19]</ref> trained a network for monocular depth estimation using the same objective loss principle over a stereo pair, using Taylor approximation to make their loss linear and fully differentiable thus making their framework trainable in end-to-end manner but resulting in a more challenging objective function to optimize.</p><p>To overcome this issue, Godard et al. <ref type="bibr" target="#b1">[2]</ref> used a bilinear sampling <ref type="bibr" target="#b19">[20]</ref> to generate images from depth predictions. At training time, the model learns to predict depth for both images of a stereo pair by processing reference image only, enabling a left-right consistency check when computing the loss signal to minimize and a simple post-processing step to obtain a more accurate prediction. Currently, this work represents state-of-the-art for monocular depth estimation. Poggi et al. <ref type="bibr" target="#b20">[21]</ref> improved <ref type="bibr" target="#b1">[2]</ref> with an interleaved training technique, simulating a trinocular setup out of a binocular stereo dataset allowing to obtain a more accurate model. While these methods require rectified stereo pairs for training, Zhou et al. <ref type="bibr" target="#b5">[6]</ref> trained a model to infer depth from unconstrained video sequences by computing a reconstruction loss between subsequent frames and predicting, at the same time, the relative pose between them. This removes the requirement for stereo pairs, but produces a less accurate final model.</p><p>Pyramidal networks for optical flow estimation. Encoder-decoder architectures <ref type="bibr" target="#b21">[22]</ref> have been widely adopted in computer vision when dealing with dense prediction. Most of them use skip connections between encoding and decoding parts to preserve fine details as done by U-net <ref type="bibr" target="#b22">[23]</ref>. While these models count a large number of trainable parameters, pyramidal architectures recently proved to be very effective for optical flow <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b23">[24]</ref>, outperforming Unet like architectures in terms of accuracy and, at the same time, decimating network parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED METHOD</head><p>In this paper we propose a novel framework for accurate and unsupervised monocular depth estimation with very limited resource requirements enabling such task even on CPUs of low power devices. State-of-the-art architectures proposed for this purpose <ref type="bibr" target="#b1">[2]</ref> run in real time on high-end GPUs (e.g., Titan X), increasing the running time to nearly a second when running on standard CPUs and more than 10 s on embedded CPUs. Moreover, they count a huge number of parameters and thus require a large amount of memory at forward time. For these reasons, real-time performance with such models are feasible only with high-end and power hungry GPUs.</p><p>To overcome this issue, we propose a compact CNN, enabling accuracy comparable to state-of-the-art, with very limited memory footprint at test time (i.e., &lt; 150 MB) and capable to infer depth at about 2 fps on embedded devices such as the Raspberry Pi 3 and tens of fps on standard CPUs whereas other methods are far behind.</p><p>To this aim some recent works in other fields have shown how classical computer vision principles, such as image pyramid, can be effectively adopted to design more compact networks. SpyNet <ref type="bibr" target="#b7">[8]</ref> and PWC-Net <ref type="bibr" target="#b23">[24]</ref> are examples in the field of optical flow estimation with the latter representing state-of-the-art on MPI Sintel and KITTI flow benchmarks. The main difference with U-Net like networks is the presence of multiple small decoders working at different resolutions, directly on a pyramid of images <ref type="bibr" target="#b7">[8]</ref> or features <ref type="bibr" target="#b23">[24]</ref> extracted by a very simple encoder compared to popular ones such as VGG <ref type="bibr" target="#b24">[25]</ref> or ResNet <ref type="bibr" target="#b25">[26]</ref>. Results at each resolution are up-sampled to the next level to refine flow estimation. This method allows for a large reduction in the number of parameters together with a faster computation in optical flow and we follow a similar strategy for our monocular depth estimation network depicted in Figure <ref type="figure" target="#fig_0">2</ref>. To train PyD-Net we adopt the unsupervised protocol proposed by Godard et al. <ref type="bibr" target="#b1">[2]</ref> by casting depth prediction as an image reconstruction problem. For training unlabeled stereo pairs are required: for each sample, the left frame is processed through the network to obtain inverse depth maps (i.e., disparity maps) with respect to left and right images. These maps are used to warp the two input images towards each other and the reconstruction error is used as supervisory signal for backpropagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. PYD-NET ARCHITECTURE</head><p>In this section we describe the proposed PyD-Net architecture depicted in Figure <ref type="figure" target="#fig_0">2</ref>, a network enabling results comparable to state-of-the-art methods but with much less parameters, memory footprint and execution time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Pyramidal features extractor</head><p>Input features are extracted by a small encoder architecture inspired by <ref type="bibr" target="#b23">[24]</ref>, made of 12 convolutional layers. At full resolution, the first layer produces the first level of the pyramid by applying convolutions with stride 2 followed by a second convolutional layer. Adopting this scheme at each level the resolution is decimated down to the lowest resolution (highest level of the pyramid) producing a total of 6 levels, from L1 to L6, corresponding respectively to image resolution from half to 1 64 of the original input size. Each down-sampling module produces a larger number of extracted features, respectively 16, 32, 64, 96, 128, and 192, and each convolutional layers deploys 3 × 3 kernels and is followed by a leaky ReLU with α = 0.2. Despite the small receptive field, this coarse-to-fine strategy allows us to include global context at the higher levels (i.e., lower image resolution) of the pyramid as well as to refine details at the lower levels (i.e., higher image resolution) and at the same time to significantly reduce the amount of parameters and memory footprint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Depth decoders and upsampling</head><p>At the highest level of the pyramid, extracted features are processed by a depth decoder made of 4 convolutional layers, producing respectively 96, 64, 32 and 8 feature maps. The output of this decoder is used for two purposes: i) to extract a depth map at the current resolution, by means of a sigmoid operator and ii) to pass the processed features at the next level in the pyramid, by means of a 2 × 2 deconvolution with stride 2 which increases by a factor 2 the spatial resolution. The next level concatenates the features extracted from the input frame with those up-sampled and process them with a new decoder, repeating this procedure up to the highest resolution level. Each convolutional layer uses 3 × 3 kernels and is followed, as for deconvolutional layers, by leaky ReLU activations, except the last one which is followed by a Sigmoid activation to normalize the outputs. With such design, at each scale PyD-Net learns to predict depth at full resolution. We will show in the experimental results how this design strategy, up-sampling depth maps from lower resolution decoders, allows to quickly infer depth maps with accuracy comparable to state-of-the-art. Indeed, it requires only a subset of decoders at test time reducing memory requirements and runtime thus making our proposal suited for CPU deployment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Training loss</head><p>We train PyD-Net to estimate depth at each resolution deploying a multi-scale loss function as sum of different contributions computed at scales s ∈ [1..6]</p><formula xml:id="formula_0">L s = α ap (L l ap +L r ap )+α ds (L l ds +L r ds )+α lr (L l lr +L r lr )<label>(1)</label></formula><p>The loss signal computed at each level of the pyramid is a weighted sum of three contributions computed on left and right images and predictions as in <ref type="bibr" target="#b1">[2]</ref>. The first term represents the reconstruction error L ap , measuring the difference between the original image I l and the warped one Ĩl by means of SSIM <ref type="bibr" target="#b27">[28]</ref> and L1 difference.</p><formula xml:id="formula_1">L l ap = 1 N i,j α 1 -SSIM (I l i,j , Ĩl i,j ) 2 + (1 -α)||(I l i,j , Ĩl i,j )||</formula><p>(2) The disparity smoothness term, L ds , discourages depth discontinuities according to L1 penalty unless a gradient δI occurs on the image.</p><formula xml:id="formula_2">L l ds = 1 N i,j |δ x d l i,j |e -||δxI l i,j || + |δ y d i,j |e -||δyI l ij ||<label>(3)</label></formula><p>The third and last term includes the left-right consistency check, a well-known cue from traditional stereo algorithms <ref type="bibr" target="#b8">[9]</ref>, enforcing coherence between predicted left d l and right d r depth maps.</p><formula xml:id="formula_3">L l lr = 1 N i,j |d l i,j -d r i,j+d l i,j |<label>(4)</label></formula><p>The three terms are also computed for right image predictions, as shown in Equation 1. As in <ref type="bibr" target="#b1">[2]</ref>, the right input image and predicted output are used only at training time, while at testing time our framework works as a monocular depth estimator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. IMPLEMENTATION DETAILS AND TRAINING PROTOCOL</head><p>We implemented PyD-Net in TensorFlow <ref type="bibr" target="#b28">[29]</ref> and for experiments we deployed a pyramid with 6 levels (i.e., from 1 to 6) producing depth maps at a maximum resolution of half the original input size, up-sampled at full resolution by means of bilinear interpolation. We adopt this strategy since in our experiments deploying levels up to full resolution with PyD-Net does not improve the accuracy significantly and increases the complexity of the network. With this setting, PyD-Net counts 1.9 million parameters and runs in about 15ms on a Titan X Maxwell GPU while <ref type="bibr" target="#b1">[2]</ref>, with the VGG model, counts 31 million parameters and requires 35ms. More importantly, our simpler model enables its effective deployment even on low-end CPUs aimed at embedded systems or smartphones. Source code is available at https: //github.com/mattpoggi/pydnet.</p><p>We assess the effectiveness of our proposal with respect to the result reported in <ref type="bibr" target="#b1">[2]</ref>. For a fair comparison with <ref type="bibr" target="#b1">[2]</ref> we train our network with the same protocol for 50 epochs on batches of 8 images resized to 512 × 256, using 30 thousand images from KITTI raw data <ref type="bibr" target="#b0">[1]</ref>. Moreover, we also provide results training PyD-Net for 200 epochs, showing how the final accuracy increases. It is worth noting that a longer schedule does not improve the performance of <ref type="bibr" target="#b1">[2]</ref>, already reaching top performance after 50 epochs. On a Titan X GPU training takes about, respectively, 10 and 40 hours. Note that <ref type="bibr" target="#b1">[2]</ref> requires 20 hours for 50 epochs. The weights for our loss terms are always set to α ap = 1 and α lr = 1, while leftright consistency weight is set to α ds = 0.1/r, being r the down-sampling factor at each resolution layer as suggested in <ref type="bibr" target="#b1">[2]</ref>. The inferred maps are multiplied by 0.3× image width, producing an inverse depth map proportional to maximum disparity between the training pairs. We use Adam optimizer <ref type="bibr" target="#b29">[30]</ref> with β 1 = 0.9, β 2 = 0.999, and ε = 10 -8 . We used a learning rate of 10 -4 for the first 60% epochs, halved every 20% epochs until the end. We perform data augmentation by randomly flipping input images horizontally and applying the following transformations: random gamma correction in [0.8,1.2], additive brightness in [0.5,2.0], and color shifts in [0.8,1.2] for each channel separately.</p><p>In <ref type="bibr" target="#b1">[2]</ref> an additional post-processing step was proposed to filter out and replace artifacts near depth discontinuities and image borders induced by training on stereo pairs. However, it requires to forward the input image twice thus doubling the processing time and memory, for this reason we do not include it in our evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXPERIMENTAL RESULTS</head><p>We evaluate PyD-Net with respect to state-of-the-art on the KITTI dataset <ref type="bibr" target="#b0">[1]</ref>. In particular, we first test the accuracy of our model on a portion of the full KITTI dataset commonly used in this field <ref type="bibr" target="#b3">[4]</ref>, then we focus on performance analysis of PyD-Net on different hardware devices, highlighting how our model can run even on low-powered CPU at about 2 Hz still enabling satisfying results, even more accurate than most techniques known in literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Accuracy evaluation on Eigen split</head><p>We compare the performance of PyD-Net with respect to known techniques for monocular depth estimation using the same protocol of <ref type="bibr" target="#b1">[2]</ref>. To do so, we use a test split of 697 images as proposed in <ref type="bibr" target="#b3">[4]</ref>, covering a total of 29 scenes out of the 61 available from KITTI raw data. The remaining 32 scenes are used to extract 22600 frames for training as in <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b1">[2]</ref>. Velodyne 3D points are reprojected on the left input image to obtain ground-truth labels on which evaluate depth estimation. As in <ref type="bibr" target="#b1">[2]</ref>, all methods use the same crop as <ref type="bibr" target="#b3">[4]</ref> to be directly comparable. Table I reports extensive comparison with both supervised <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> and unsupervised methods <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b1">[2]</ref>. To compare the performance of the considered methods we use metrics commonly adopted in this field <ref type="bibr" target="#b3">[4]</ref> and we split our experiments into four main comparisons that we are going to discuss in detail.   <ref type="bibr" target="#b26">[27]</ref> followed by fine-tuning on KITTI as outlined in <ref type="bibr" target="#b1">[2]</ref>. On top and middle of the table evaluation of all existing methods trained on K, at the bottom evaluation of unsupervised methods trained on CS+K. We report results for PyD-Net with two training configurations.</p><p>In the first part of Table <ref type="table" target="#tab_1">I</ref>, we compare PyD-Net trained on 50 and 200 epochs to supervised works by Eigen et al. <ref type="bibr" target="#b3">[4]</ref> and Liu et al. <ref type="bibr" target="#b4">[5]</ref>, as well as with other unsupervised techniques by Zhou et al. <ref type="bibr" target="#b5">[6]</ref> and Godard et al. <ref type="bibr" target="#b1">[2]</ref>. We report for each method the amount of parameters and, for each metric, the rank with respect to all the considered models. Excluding <ref type="bibr" target="#b1">[2]</ref> we can notice how PyD-Net, with a very low number of parameters and with both training configurations, significantly outperforms all considered methods on all metrics with the exception of δ &lt; 0.125 3 on which Liu et al. <ref type="bibr" target="#b4">[5]</ref> is even better than <ref type="bibr" target="#b1">[2]</ref>. Compared to <ref type="bibr" target="#b1">[2]</ref>, our network is less accurate but training PyD-Net for 200 epochs yields almost equivalent results.</p><p>To compare with the results reported by Garg et al. <ref type="bibr" target="#b18">[19]</ref>, in the middle part of Table <ref type="table" target="#tab_1">I</ref> we evaluate predicted maps up to a maximum depth of 50 meters as in <ref type="bibr" target="#b1">[2]</ref>. Despite the smaller amount of parameters, reduced by a factor 8+, our network outperforms <ref type="bibr" target="#b18">[19]</ref> with both training configurations and has performance very close, and even better with metrics δ &lt; 0.125 2 and δ &lt; 0.125 3 training for 200 epochs, to Godard et al. <ref type="bibr" target="#b1">[2]</ref> a network counting more than 16× parameters. As for previous experiment we can notice that training PyD-Net for 200 epochs always yields better accuracy.</p><p>In the third part of Table <ref type="table" target="#tab_1">I</ref>, we compare the performance of PyD-Net with respect to Zhou et al. <ref type="bibr" target="#b5">[6]</ref> and Godard et al. <ref type="bibr" target="#b1">[2]</ref> unsupervised frameworks when trained on additional data. In particular, we first train the network for 50 epochs on CityScapes dataset and then we perform a fine-tuning on KITTI raw data according to the learning rate schedule described before. We can notice how training on additional data is beneficial for all the networks substantially confirming the previous trend. Godard et al. method outperforms all other approaches while training PyD-Net for 200 epochs yields overall best performance for this method. However, even training PyD-Net for only 50 epochs always enables to achieve a better accuracy compared to the much complex network by Zhou et al. <ref type="bibr" target="#b5">[6]</ref>.</p><p>To summarize, our lightweight PyD-Net architecture out-</p><formula xml:id="formula_4">Power 250+ [W] 91+ [W] 3.5 [W] Model</formula><p>Res. Titan X i7-6700K Raspberry Pi 3 Godard et al. <ref type="bibr">[</ref>  II: Runtime analysis. We report for PyD-Net and <ref type="bibr" target="#b1">[2]</ref> the average runtime required to process the same KITTI image with 3 heterogeneous architectures at Full, Half, Quarter and Eight resolution. The measured power consumption for the Raspberry Pi 3 concerns the whole system plus a Logitech HD C310 USB camera while for CPU and GPU it concerns only such devices.</p><p>performs more complex state-of-the-art methods <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b18">[19]</ref> and has results in most cases comparable to topperforming approach <ref type="bibr" target="#b1">[2]</ref>. Therefore, in the next section we evaluate in detail the impact of our design with respect to this latter method in terms of accuracy and execution time, on three heterogeneous hardware architectures, with different setting of the two networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Runtime analysis on different architectures</head><p>Having assessed the accuracy of PyD-Net with respect to state-of-the-art, we compare on different hardware platforms the performance of our network with the top-performing one by Godard et al. <ref type="bibr" target="#b1">[2]</ref> . The reduced amount of parameters makes our model much less memory demanding and much faster thus allowing for real-time processing even on CPUs. This fact is highly desirable since GPU acceleration is not always feasible in applications scenarios characterized by low power constraints. Moreover, the pyramidal structure depicted in Figure <ref type="figure" target="#fig_0">2</ref> infers depth at different levels, getting more accurate at the higher resolution. This also happens for other models producing multi-scale outputs <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b5">[6]</ref>. Thus, given a trained instance of any of these models, we can  process outputs up to a lower resolution (e.g., half or quarter) to reduce the amount of computations, memory requirements and runtime. Therefore, we'll also investigate the impact of such strategy in terms of accuracy and execution time for our method and <ref type="bibr" target="#b1">[2]</ref>.</p><p>Table <ref type="table" target="#tab_1">II</ref> reports running time analysis for PyD-Net and Godard et al. <ref type="bibr" target="#b1">[2]</ref> models estimating depth maps at different resolutions and with different devices. More precisely, the target systems are a Titan X Maxwell GPU, an i7-6700K CPU with 4 cores (4.2 Ghz) and a Raspberry Pi 3 board (ARM v8 processor Cortex-A53 1.2 Ghz). We report single forward time at full (F), half (H), quarter (Q) and eight (E) resolution. Full image resolution is set to 256 × 512 as in <ref type="bibr" target="#b1">[2]</ref>. For PyD-Net we report results up to half resolution for the reason previously outlined. Moreover, results do not include the post-processing step proposed in <ref type="bibr" target="#b1">[2]</ref> since it would duplicate the execution time and memory with small improvements in terms of accuracy. First of all, we can notice how the model by Godard et al. <ref type="bibr" target="#b1">[2]</ref> is very fast on the high-end Titan X GPU while its performance drops dramatically when running on the Intel i7 CPU falling below 2 Hz. Moreover, it becomes unsuited for practical deployment on embedded CPUs such as the ARM processor of the Raspberry Pi 3. In this latter case it requires more than 10 seconds to process a single depth map at full resolution. We can also notice how early stopping of the network to infer depth at reduced resolution leads to a very small decrease of running time for this method hardly bringing the framerate above 2 Hz on the Intel i7 and requiring more than 5 seconds on a Raspberry Pi 3 even stopping at 1  8 resolution. Looking at PyD-Net, even at the highest resolution H it takes 120 ms on the Intel i7 and less than 2 s on the ARM processor leading to 5× speed up with respect to <ref type="bibr" target="#b1">[2]</ref> at the same resolution. Moving to lower resolutions PyD-Net runs at 20 and 40 Hz, respectively, at Q and E resolutions yielding a speed up of 11× and 18× with respect to <ref type="bibr" target="#b1">[2]</ref>. Moreover, PyD-Net breaks the 1 Hz barrier even on the Raspberry Pi 3, with 1.2 and 2.2 Hz and a speed up w.r.t. <ref type="bibr" target="#b1">[2]</ref> of 8× and 11×, respectively, at Q and E resolutions. On the same platform, equipped with 1 GB of RAM, our model requires 200, 150 and 120 MB, respectively, at H, Q and E resolution while the Godard et al. model about 275 MB at any resolution thus leaving a significantly smaller amount of memory available for other purposes.</p><p>These experiments highlight how PyD-Net enables, at the cost of small loss in accuracy, real-time performance on a standard CPU and it is also suited for practical deployment on devices with embedded CPUs. To better assess the tradeoff between accuracy and execution time we report in Table <ref type="table" target="#tab_5">III</ref> detailed experimental results concerning PyD-Net and <ref type="bibr" target="#b1">[2]</ref> with different configurations/resolution. Results in the table were obtained from models trained on CS+K and evaluated on Eigen split <ref type="bibr" target="#b3">[4]</ref>. We can observe how at E resolution PyD-Net performs similarly to the model proposed by Godard et al. <ref type="bibr" target="#b1">[2]</ref> providing output of the same dimensions. However, the gain in terms of runtime is quite high for PyD-Net as highlighted in the previous evaluation. In particular our competitor barely breaks the 1 Hz barrier on the i7 CPU and it is far behind on the Raspberry Pi, while PyD-Net runs, respectively, at 40 fps and about 2 fps on the same platforms. As expected, from Table <ref type="table" target="#tab_5">III</ref>, stopping at lower resolution we can observe a loss in accuracy for both methods. However, it is worth to note that such reduction is more gradual for our network. Moreover, at E resolution the accuracy of Godard et al. network is substantially equivalent to PyD-Net with the advantages in terms of execution time previously discussed and reported in Table <ref type="table" target="#tab_1">II</ref>. Finally, from the table we can also notice that even at the lowest resolution E, PyD-Net outperforms all remaining methods <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b18">[19]</ref> working at full resolution reported in Table <ref type="table" target="#tab_1">I</ref>. Figure <ref type="figure" target="#fig_1">3</ref> reports a qualitative comparison between PyD-Net and Godard et al. <ref type="bibr" target="#b1">[2]</ref> outputs at different resolutions.</p><p>The detailed evaluation reported proves that the proposed method can be effectively deployed on CPUs and actually it represents, to the best of our knowledge, the first architecture suited for CPU-based embedded systems enabling, for instance, its effective deployment with a Raspberry Pi 3 and a USB camera using a standard power bank for smartphones. Moreover, despite its reduced complexity it enables unsupervised training and outperforms almost all methodologies proposed in literature for monocular depth estimation including supervised ones.  <ref type="table" target="#tab_1">II</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSIONS AND FUTURE WORK</head><p>In this paper we proposed PyD-Net, a novel and efficient architecture for unsupervised monocular depth estimation. As state-of-the-art method <ref type="bibr" target="#b1">[2]</ref>, it can be trained in unsupervised manner on rectified stereo pairs enabling comparable accuracy. However, the peculiar design of our network makes it suited for real-time applications on standard CPUs and also enables its effective deployment on embedded systems. Moreover, simplified configurations of our network allow to infer depth map at about 2 Hz on a Raspberry Pi 3 with accuracy higher than most state-of-the-art methods. Future work is aimed at mapping PyD-Net on embedded devices specifically tailored for computer vision applications, such as the Intel Movidius NCS, thus paving the way for realtime monocular depth estimation in applications with hard low-power constraints (e.g., UAVs, wearable and assistive systems, etc).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: PyD-Net architecture. A pyramid of features is extracted from the input image and at each level a shallow network infers depth at that resolution. Processed features are then up-sampled to the above level to refine estimation, up to the highest one.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Qualitative comparison on a portion of a KITTI image between PyD-net (top) and Godard et al. [2] (bottom) respectively at F, H, Q and E resolution. Detailed timing analysis at each scale is reported in TableII.</figDesc><graphic coords="7,64.20,107.53,53.86,53.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Evaluation on KITTI<ref type="bibr" target="#b0">[1]</ref> using the split of Eigen et al.<ref type="bibr" target="#b3">[4]</ref>. For training, K refers to KITTI dataset, CS+K means training on CityScapes</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE III :</head><label>III</label><figDesc>Comparison between<ref type="bibr" target="#b1">[2]</ref> and PyD-Net at different resolutions. All models were trained on CS+K datasets and results are not post-processed to achieve maximum speed. As for TableI, we report results for PyD-Net with two training configurations.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b1">2</ref> Department of Computer Science and Engineering (DISI), University of Bologna, 40136 Bologna, Italy. 1 {m.poggi, fabio.tosi5, stefano.mattoccia}@unibo.it, 2 filippo.aleotti@studio.unibo.it</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan X GPU used for this research. We also thank Andrea Guccini for Figure <ref type="figure">2</ref>.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised monocular depth estimation with left-right consistency</title>
		<author>
			<persName><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pulling things out of perspective</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="89" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2366" to="2374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images using deep convolutional neural fields</title>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2024" to="2039" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from video</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="346" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Optical flow estimation using a spatial pyramid network</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A taxonomy and evaluation of dense two-frame stereo correspondence algorithms</title>
		<author>
			<persName><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="7" to="42" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Make3d: Learning 3d scene structure from a single still image</title>
		<author>
			<persName><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="824" to="840" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Depth transfer: Depth extraction from video using non-parametric sampling</title>
		<author>
			<persName><forename type="first">K</forename><surname>Karsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Tran. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2144" to="2158" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Depth and surface normal estimation from monocular images using regression on deep features and hierarchical crfs</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1119" to="1127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Estimating depth from monocular images as classification using deep fully convolutional residual networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deeper depth prediction with fully convolutional residual networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on 3D Vision (3DV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="239" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Designing deep networks for surface normal estimation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="539" to="547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Demon: Depth and motion network for learning monocular stereo</title>
		<author>
			<persName><forename type="first">B</forename><surname>Ummenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deepstereo: Learning to predict new views from the world&apos;s imagery</title>
		<author>
			<persName><forename type="first">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Neulander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="5515" to="5524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep3d: Fully automatic 2dto-3d video conversion with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="842" to="857" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised cnn for single view depth estimation: Geometry to the rescue</title>
		<author>
			<persName><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">K</forename><surname>Bg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="740" to="756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning monocular depth estimation with unsupervised trinocular assumptions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mattoccia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1096" to="1103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.02371</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Tensorflow: Largescale machine learning on heterogeneous distributed systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kinga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
