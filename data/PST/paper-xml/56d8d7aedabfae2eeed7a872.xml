<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Graphical Models for Ordinal Data</title>
				<funder ref="#_wgKsvxT #_ySgk8pg #_BxSSSSD #_pZgfzMJ">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder ref="#_Ntu8rBh #_EBPrjpq">
					<orgName type="full">National Institutes of Health</orgName>
					<orgName type="abbreviated">NIH</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2015-05-20">20 May 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jian</forename><surname>Guo</surname></persName>
							<email>jguo@hsph.harvard.edu</email>
						</author>
						<author>
							<persName><forename type="first">Elizaveta</forename><surname>Levina</surname></persName>
							<email>elevina@umich.edu</email>
						</author>
						<author>
							<persName><forename type="first">George</forename><surname>Michailidis</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ji</forename><surname>Zhu</surname></persName>
							<email>jizhu@umich.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Biostatistics</orgName>
								<orgName type="institution">Harvard University</orgName>
								<address>
									<postCode>02138</postCode>
									<settlement>Boston</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<postCode>48109. C 2015</postCode>
									<settlement>Ann Arbor</settlement>
									<region>MI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Institute of Mathematical Statistics, and Interface Foundation of North America</orgName>
								<orgName type="institution">American Statistical Association</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Graphical Models for Ordinal Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2015-05-20">20 May 2015</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1080/10618600.2014.889023</idno>
					<note type="submission">12:33 Publisher: Taylor &amp; Francis Informa Ltd Registered in England and Wales Registered Number: 1072954 Registered office: Mortimer House, 37-41 Mortimer Street, London W1T 3JH, UK Click for updates Accepted author version posted online: 13 Mar 2014.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>for research, teaching, and private study purposes. Any substantial or systematic reproduction, redistribution, reselling, loan, sub-licensing, systematic supply, or distribution Lasso</term>
					<term>Ordinal variable</term>
					<term>Probit model</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Taylor &amp; Francis makes every effort to ensure the accuracy of all the information (the "Content") contained in the publications on our platform. However, Taylor &amp; Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor &amp; Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Graphical models have been successful in identifying directed and undirected structures from high-dimensional data. In a graphical model, the nodes of the network correspond to random variables and the edges represent their corresponding associations <ref type="bibr" target="#b13">(Lauritzen 1996)</ref>. Two canonical classes of graphical models are the Gaussian one, where the dependence structure is fully specified by the inverse covariance matrix and the Markov one, where the dependence structure is captured by the interaction effects in an exponential family model. In the latter model, each interaction effect can be interpreted as the conditional log-odds-ratio of the two associated variables given all other variables. In both models, a zero element in the inverse covariance matrix or a zero interaction effect determines a conditionally independent relationship between the corresponding nodes in the network.</p><p>Estimation of such models from high-dimensional data under a sparsity assumption has attracted a lot of interest in the statistics and machine learning literature, including regularized likelihood and regression methods, for example, see <ref type="bibr" target="#b35">Yuan and Lin (2007)</ref>; <ref type="bibr" target="#b1">Banerjee, El Ghaoui, and d'Aspremont (2008)</ref>; <ref type="bibr" target="#b6">Friedman, Hastie, and Tibshirani (2008)</ref>; <ref type="bibr" target="#b29">Rothman et al. (2008)</ref>; <ref type="bibr" target="#b5">Fan, Feng, and Wu (2009)</ref>; <ref type="bibr" target="#b21">Meinshausen and Buhlmann (2006)</ref>; <ref type="bibr" target="#b28">Rocha, Zhao, and Yu (2008)</ref>; <ref type="bibr" target="#b24">Peng et al. (2009)</ref> and references therein. For a Markov network, direct estimation of a regularized likelihood is infeasible due to the intractable partition function in the likelihood. Instead, existing methods in the literature employ variants of approximation estimation methods. Examples include the surrogate likelihood methods <ref type="bibr" target="#b1">(Banerjee, El Ghaoui, and d'Aspremont 2008;</ref><ref type="bibr" target="#b10">Kolar and Xing 2008)</ref> and the pseudo-likelihood methods <ref type="bibr" target="#b8">(H?efling and Tibshirani 2009;</ref><ref type="bibr" target="#b27">Ravikumar, Wainwright, and Lafferty 2010;</ref><ref type="bibr" target="#b7">Guo et al. 2010)</ref>.</p><p>In many applications involving categorical data, an ordering of the categories can be safely assumed. For example, in marketing studies consumers rate their preferences for a wide range of products. Similarly, computer recommender systems use customer ratings to make purchase recommendations to new customers; this constitutes a key aspect of the business model behind Netflix, Amazon, Tripadvisor, etc. <ref type="bibr" target="#b11">(Koren, Bell, and Volinsky 2009)</ref>.</p><p>Ordinal variables are also an integral part of survey data, where respondents rate items or express level of agreement/disagreement on issues/topics under consideration. Such responses correspond to Likert items, and a popular model to analyze such data is the polychotomous Rasch model (von Davier and Carstensen 2010) that obtains interval level estimates on a continuum-an idea that we explore in this work as well. Ordinal response variables in regression analysis give rise to variants of the classical linear model, including the proportional odds model <ref type="bibr" target="#b34">(Walker and Duncan 1967;</ref><ref type="bibr" target="#b19">McCullagh 1980)</ref>, the partial proportional odds model <ref type="bibr" target="#b25">(Peterson 1990</ref>), the probit model <ref type="bibr" target="#b3">(Bliss 1935;</ref><ref type="bibr" target="#b0">Albert and Chib 1993;</ref><ref type="bibr" target="#b4">Chib and Greenberg 1998)</ref>, etc. A comprehensive review of ordinal regression was given by <ref type="bibr" target="#b20">McCullagh and Nelder (1989)</ref> and <ref type="bibr" target="#b22">O'Connell (2005)</ref>.</p><p>In this article, we introduce a graphical model for ordinal variables. It is based on the assumption that the ordinal scales are generated by discretizing the marginal distributions of a latent multivariate Gaussian distribution and the dependence relationships of these ordinal variables are induced by the underlying Gaussian graphical model. In this context, an EMlike algorithm is appropriate for estimating the underlying latent network, which presents a number of technical challenges that have to be addressed for successfully pursuing this strategy.</p><p>Our work is related to <ref type="bibr" target="#b0">Albert and Chib (1993)</ref>, <ref type="bibr">Chib and</ref><ref type="bibr" target="#b4">Greenberg (1998), and</ref><ref type="bibr" target="#b31">Stern, Herbrich, and</ref><ref type="bibr" target="#b31">Graepel (2009)</ref> in the sense that they are all built on the probit model and/or the EM algorithmic framework. <ref type="bibr" target="#b0">Albert and Chib (1993)</ref> proposed an MCMC algorithm for the probit-model-based univariate ordinal regression problem, where an ordinal response is fitted on a number of covariates, while <ref type="bibr" target="#b4">Chib and Greenberg (1998)</ref> can be considered an extension to the multivariate case. <ref type="bibr" target="#b31">Stern, Herbrich, and Graepel (2009)</ref> aimed to build an online recommender system via collaborative filtering and applied the discretization/thresholding idea in the probit model to the ordinal matrix factorization problem. Our model, on the other hand, has a completely different motivation from these works. Our objective is to explore associations between a set of ordinal variables, rather than prediction and/or regression problems. Nevertheless, the EM framework employed is related to that in <ref type="bibr" target="#b4">Chib and Greenberg (1998)</ref>, but due to the different goal, the form of the likelihood function of the proposed model is different from that of the ordinal regression problem. Further, as seen in Section 2, we do not use any MCMC or Gibbs sampling scheme.</p><p>The remainder of the article is organized as follows. Section 2 presents the probit graphical model and discusses algorithmic and model selection issues. Section 3 evaluates Downloaded by [New York University] at 12:33 20 May 2015 the performance of the proposed method on several synthetic examples and Section 4 applies the model to two data examples, one on movie ratings and the other on a national educational longitudinal survey study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">METHODOLOGY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">THE PROBIT GRAPHICAL MODEL</head><p>Suppose we have p ordinal random variables X 1 , . . . , X p , where X j ? {1, 2, . . . , K j } for some integer K j , which is the number of the ordinal levels in variable j. In the proposed probit graphical model, we assume that there exist p latent random variables Z 1 , . . . , Z p from a joint Gaussian distribution with mean zero and covariance matrix = (? j,j ) p?p . Without loss of generality, we further assume that Z j 's have unit variances (? j,j = 1 for j = 1, . . . , p), that is, the Z j 's marginally follow standard Gaussian distributions. Each observed variable X j is discretized from its latent counterpart Z j . Specifically, for the jth variable (j = 1, . . . , p), we assume that (-?, +?) is split into K j disjointed intervals by a set of thresholds -? = ?</p><formula xml:id="formula_0">(j ) 0 &lt; ? (j ) 1 &lt; ? ? ? &lt; ? (j ) K j -1 &lt; ? (j ) K j = +?, such that X j = k if and only if Z j falls in the interval [? (j ) k-1 , ? (j ) k ). Thus, Pr(X j = k) = Pr ? (j ) k-1 ? Z j &lt; ? (j ) k = ? (j ) k -? (j ) k-1 ,<label>(1)</label></formula><p>where (?) denotes the cumulative density function of the standard normal distribution.</p><formula xml:id="formula_1">Let = -1 = (? j,j ) p?p , = {? (j ) k : j = 1, . . . , p; k = 1, . . . , K j }, X = (X 1 , . . . , X p ), Z = (Z 1 , . . . , Z p ). Let C(X, ) be the hypercube defined by [? (1) X 1 -1 , ? (1) X 1 ) ? ? ? ? ? [? (p) X p -1 , ? (p)</formula><p>X p ). Then we can write the joint density function of (X, Z) as f X,Z (x, z; , ) = f(z; )</p><formula xml:id="formula_2">p j =1 f (x j |z j ; ) = det ( ) (2? ) p/2 exp - 1 2 z z T I(z ? C(x, )),<label>(2)</label></formula><p>where I(?) is the indicator function. Thus, the marginal probability density function of the observed X is given by</p><formula xml:id="formula_3">f X (x; , ) = z?R p f X,Z (x, z; , )d z.</formula><p>(3)</p><p>We refer to (1)-( <ref type="formula">3</ref>) as the probit graphical model, which is motivated by the probit regression model <ref type="bibr" target="#b3">(Bliss 1935;</ref><ref type="bibr" target="#b0">Albert and Chib 1993;</ref><ref type="bibr" target="#b4">Chib and Greenberg 1998)</ref> and the polychotomous Rasch model (von Davier and Carstensen 2010).</p><p>To fit the probit graphical model, we propose maximizing an 1 -regularized loglikelihood of the observed data. Let x i,j and z i,j be the ith realizations of the observed variable X j and the latent variable Z j , respectively, with x i = (x i,1 , . . . , x i,p ) and z i = (z i,1 , . . . , z i,p ). The criterion is given by</p><formula xml:id="formula_4">n i=1 log f X (x i ; , ) -? j =j |? j,j |.</formula><p>(4) Downloaded by [New York University] at 12:33 20 May 2015</p><p>The tuning parameter ? in (4) controls the degree of sparsity in the underlying network. When ? is large enough, some ? j,j 's can be shrunken to zero, resulting in the removal of the corresponding links in the underlying network. Numerically, it is difficult to maximize criterion (4) directly, because of the integral in (3). Next, we introduce an EM-type algorithm to maximize (4) in an iterative manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">AN ALGORITHM FOR FITTING THE PROBIT GRAPHICAL MODEL</head><p>Criterion (4) depends on the parameters and and the latent variable Z. The former has a closed-form estimator. Specifically, for each j = 1, . . . , p, we set</p><formula xml:id="formula_5">? (j ) k = ? ? ? ? ? ? ? ? ? ? ? ? ? -?, if k = 0; -1 n -1 n i=1 I(x i,j &lt; k) , if k = 1, . . . , K j -1; +?, if k = K j ;</formula><p>(5)</p><p>where is the cumulative distribution function of the standard normal. One can show that consistently estimates . The estimation of , on the other hand, is nontrivial due to the multiple integrals in (3). To address this problem, we apply the EM algorithm to optimizing (4), where the latent variables z i,j 's (i = 1, . . . , n; j = 1, . . . , p) are treated as "missing data" and are imputed in the E-step, and the parameter is estimated in the M-step. E-step. Suppose is the updated estimate of from the M-step. Then the E-step computes the conditional expectation of the joint log-likelihood given the estimates and , which is usually called the Q-function in the literature:</p><formula xml:id="formula_6">Q( , ) = n i=1 E Z [log f X,Z (x i , Z; , )] = n 2 [log det( ) -trace(S ) -p log(2? )]. (<label>6</label></formula><formula xml:id="formula_7">)</formula><p>Here S is a p ? p matrix whose (j, j )th element is s j,j = n -1 n i=1 E(z i,j z i,j | x i ; , ) (1 ? j, j ? p). The distribution of z i conditional on x i is equal to that of z i conditional on z i ? C(x i , ), which follows a truncated multivariate Gaussian distribution on the hypercube C(x i , ). Therefore, E(z i,j z i,j | x i ; , ) is the second moment of a truncated multivariate Gaussian distribution and it can be directly estimated using the algorithms proposed by <ref type="bibr" target="#b32">Tallis (1961)</ref>, <ref type="bibr" target="#b14">Lee (1979)</ref>, <ref type="bibr" target="#b15">Leppard and Tallis (1989)</ref>, and <ref type="bibr" target="#b18">Manjunath and Wilhelm (2012)</ref>. Nevertheless, the computational cost of these direct estimation algorithms is extremely high and thus not suitable for even moderate size problems. An alternative approach is based on the Markov chain Monte Carlo (MCMC) method. Specifically, we randomly generate a sequence of samples from the conditional distribution f Z|X (z i | x i ; , ) using a Gibbs sampler from a multivariate truncated normal distribution <ref type="bibr" target="#b12">(Kotecha and Djuric 1999)</ref> and then E(z i,j z i,j | x i ; , ) is estimated by the empirical conditional second moment from these samples. Although the MCMC approach is faster than the direct estimation method, it is still not efficient for large-scale problems. To address this computational issue, we develop an efficient approximate estimation algorithm, discussed in Section 2. (7)</p><p>The optimization problem (7) can be solved efficiently by existing algorithms such as the graphical lasso <ref type="bibr" target="#b6">(Friedman, Hastie, and Tibshirani 2008)</ref> and SPICE <ref type="bibr" target="#b29">(Rothman et al. 2008)</ref>. However, the estimated covariance matrix, = -1 , does not necessarily have unit diagonal elements postulated by the probit graphical model. Therefore, we postprocess by scaling it to a unit-diagonal matrix and update = -1 , which will be used in the E-step of the next iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">APPROXIMATING THE CONDITIONAL EXPECTATION</head><p>Note that when j = j , the corresponding conditional expectation is the conditional second moment E(z 2 i,j | x i ; , ); when j = j , we use a mean field theory approach <ref type="bibr" target="#b26">(Peterson and Anderson 1987)</ref> </p><formula xml:id="formula_8">to approximate it as E(z i,j z i,j | x i ; , ) ? E(z i,j | x i ; , )E(z i,j | x i ; , )</formula><p>. Note that the approximation decouples the "interaction" between the two variables z i,j and z i,j . Therefore, one would expect that the approximation performs well when z j and z j are close to independence given all other random variables, which often holds when or the corresponding graph is sparse. With this approximation, it is sufficient to estimate the first moment E(z i,j | x i ; , ) and the second moment E(z 2 i,j | x i ; , ). In general, the latent variable z i,j not only depends on x i,j , but also on all other observed variables x i,-j = (x i,1 , . . . , x i,j -1 , x i,j +1 , . . . , x i,p ). We can write the first and second conditional moments as</p><formula xml:id="formula_9">E(z i,j | x i ; , ) = E[E(z i,j | z i,-j , x i,j ; , ) | x i ; , ],<label>(8)</label></formula><formula xml:id="formula_10">E z 2 i,j | x i ; , = E E z 2 i,j | z i,-j , x i,j ; , | x i ; , ,<label>(9)</label></formula><p>where z i,-j = (z i,1 , . . . , z i,j -1 , z i,j +1 , . . . , z i,p ). The inner expectations in ( <ref type="formula" target="#formula_9">8</ref>) and ( <ref type="formula" target="#formula_10">9</ref>) are relatively straightforward to compute: given the parameter estimate , z i,1 , . . . , z i,p jointly follow a multivariate Gaussian distribution with mean zero and covariance matrix = -1 . A property of the Gaussian distribution is that the conditional distribution of z i,j given z i,-j is also Gaussian, with mean ? i,j = j,-j -1 -j,-j z i,-j T and variance</p><formula xml:id="formula_11">? 2 i,j = 1 -j,-j -1</formula><p>-j,-j -j,j . Moreover, given the observed data x i,j , conditioning z i,j on z i,-j , x i,j in ( <ref type="formula" target="#formula_9">8</ref>) is equivalent to conditioning on z i,-j , ? (j )</p><formula xml:id="formula_12">x i,j -1 ? z i,j &lt; ? (j )</formula><p>x i,j , which follows a truncated Gaussian distribution on the interval [? (j )</p><formula xml:id="formula_13">x i,j -1 , ? (j )</formula><p>x i,j ). The following lemma gives the closed-form expressions for the first and second moments of the truncated Gaussian distribution.</p><p>Lemma 1. Suppose that a random variable Y follows the Gaussian distribution with mean ? 0 and variance ? 2 0 . For any constants t 1 &lt; t 2 , let ? 1 = (t 1? 0 )/? 0 and ? 2 = (t 2? 0 )/? 0 . Then the first and second moments of Y truncated to the interval (t 1 , t 2 ) are given by </p><formula xml:id="formula_14">E(Y | t 1 &lt; Y &lt; t 2 ) = ? 0 + ?(? 1 )-?(? 2 ) (? 2 )-(? 1 ) ? 0 , (<label>10</label></formula><formula xml:id="formula_15">E(Y 2 | t 1 &lt; Y &lt; t 2 ) = ? 2 0 + ? 2 0 + 2 ?(? 1 )-?(? 2 ) (? 2 )-(? 1 ) ? 0 ? 0 + ? 1 ?(? 1 )-? 2 ?(? 2 ) (? 2 )-(? 1 ) ? 2 0 , (<label>11</label></formula><formula xml:id="formula_16">)</formula><p>where ?(?) is the probability density function of the standard normal.</p><p>For more properties of the truncated Gaussian distribution, see <ref type="bibr" target="#b9">Johnson, Kotz, and Balakrishnan (1994)</ref>.</p><p>Letting</p><formula xml:id="formula_17">? i,j,k = (? (j )</formula><p>k? i,j )/ ? i,j and applying Lemma 1 to the conditional expectations in ( <ref type="formula" target="#formula_9">8</ref>) and ( <ref type="formula" target="#formula_10">9</ref>), we obtain</p><formula xml:id="formula_18">E(z i,j |z i,-j , x i,j ; , ) = ? i,j + a i,j ? i,j , (<label>12</label></formula><formula xml:id="formula_19">) E z 2 i,j |z i,-j , x i,j ; , = ? 2 i,j + ? 2 i,j + 2a i,j ? i,j ? i,j + b i,j ? 2 i,j ,<label>(13)</label></formula><p>where</p><formula xml:id="formula_20">a i,j = ?(? i,j,x i,j -1 ) -?(? i,j,x i,j ) (? i,j,x i,j ) -(? i,j,x i,j -1 ) , b i,j = ? i,j,x i,j -1 ?(? i,j,x i,j -1 ) -? i,j,x i,j ?(? i,j,x i,j ) (? i,j,x i,j ) -(? i,j,x i,j -1</formula><p>) .</p><p>Next, we plug Equations ( <ref type="formula" target="#formula_18">12</ref>) and ( <ref type="formula" target="#formula_19">13</ref>) into ( <ref type="formula" target="#formula_9">8</ref>) and ( <ref type="formula" target="#formula_10">9</ref>), respectively. Since ? i,j , a i,j , and b i,j depend on the latent variables z i,-j 's, the outer expectations in ( <ref type="formula" target="#formula_9">8</ref>) and ( <ref type="formula" target="#formula_10">9</ref></p><formula xml:id="formula_21">) depend on E( ? i,j | x i ; , ), E(a i,j | x i ; , ), E(b i,j | x i ; , )</formula><p>, and E(a i,j ? i,j | x i ; , ). Note that ? i,j is a linear function of z i,-j and ? i,j is a constant irrelevant to the latent data. For each i = 1, . . . , n and j = 1, . . . , p, the conditional expectation of ? i,j is</p><formula xml:id="formula_22">E( ? i,j | x i ; ? , ) = j,-j -1 -j,-j E z T i,-j |x i ; ? , .<label>(14)</label></formula><p>However, a i,j and b i,j are nonlinear functions of ? i,j , and thus of z i,-j . Using the first-order delta method, we approximate their conditional expectations by</p><formula xml:id="formula_23">E(a i,j | x i ; ? , ) ? ?( ? i,j,x i,j -1 ) -?( ? i,j,x i,j ) ( ? i,j,x i,j ) -( ? i,j,x i,j -1 ) , (15) E(b i,j | x i ; ? , ) ? ? i,j,x i,j -1 ?( ? i,j,x i,j -1 ) -? i,j,x i,j ?( ? i,j,x i,j ) ( ? i,j,x i,j ) -( ? i,j,x i,j -1 ) , (<label>16</label></formula><formula xml:id="formula_24">)</formula><p>where <ref type="formula" target="#formula_9">8</ref>) and ( <ref type="formula" target="#formula_10">9</ref>) can be approximated by</p><formula xml:id="formula_25">? i,j,x i,j = [? (j ) k -E( ? i,j | x i ; ? , )]/ ? i,j . Finally, we approximate E(a i,j ? i,j | x i ; , ) ? E(a i,j | x i ; , )E( ? i,j | x i ; , ). Therefore, (</formula><formula xml:id="formula_26">E(z i,j | x i ; , ) ? j,-j -1 -j,-j E z T i,-j | x i ; , + ?( ? i,j,x i,j -1 ) -?( ? i,j,x i,j ) ( ? i,j,x i,j ) -( ? i,j,x i,j -1 ) ? i,j (17) E z 2 i,j | x i ; , ? j,-j -1 -j,-j E z T i,-j z i,-j | x i ; , -1 -j,-j T j,-j + ? 2 i,j + 2 ?( ? i,j,x i,j -1 ) -?( ? i,j,x i,j ) ( ? i,j,x i,j ) -( ? i,j,x i,j -1 ) j,-j -1 -j,-j E z T i,-j | x i ; , ? i,j + ? (j ) i,j,x i,j -1 ?( ? i,j,x i,j -1 ) -? i,j,x i,j ?( ? i,j,x i,j ) ( ? i,j,x i,j ) -( ? i,j,x i,j -1 ) ? 2 i,j .</formula><p>(18) Downloaded by [New York University] at 12:33 20 May 2015 Equations ( <ref type="formula">17</ref>) and ( <ref type="formula">18</ref>) establish the recursive relationships among the elements in E(z i | x i ; , ) and E(z i T z i | x i ; , ), respectively, giving a natural iterative procedure for estimating these quantities. Algorithm 1 summarizes the main steps of the proposed combined estimation procedure outlined in Sections 2.2 and 2.3.</p><p>In Algorithm 1, Lines 1-2 initialize the conditional expectation E(z i,j | x i ) and the parameter estimate . Lines 3-16 establish the outer loop which iteratively computes the E-step and the M-step. In the E-step, Lines 5-14 consist of the inner loop which recursively estimates the first and second moments of z i,j conditional on x i . The complexity of the inner loop is O(np 2 ), which is the same as that of the graphical lasso algorithm in the M-step. Therefore, the overall complexity of Algorithm 1 is O(Mnp 2 ), where M is the number of EM steps required for convergence. In our numerical studies, we found M is often smaller than 50. For a more concrete idea about the computational cost, we note that on a linux server with four 1G Dual-Core AMD Opteron Processors and 4GB RAM, it takes about 2 min for the proposed algorithm to complete the fitting on a simulated dataset in Section 3 with n = 200 observations and p = 50 variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1</head><p>The EM Algorithm for estimating</p><formula xml:id="formula_27">1: Initialize E(z i,j | x i ; , ) ? E(z i,j | x i,j ; ), E(z 2 i,j | x i ; , ) ? E(z 2 i,j | x i,j ; ) and E(z i,j z i,j | x i ; , ) ? E(z i,j | x i,j ; )E(z i,j | x i,j ;</formula><p>) for i = 1, . . . , n and j, j = 1, . . . , p; 2: Initialize s j,j for 1 ? j, j ? p using the Line 1 above, and then estimate by maximizing criterion (7); {Start outer loop} 3: repeat Update E(z i,j | x i ; , ) using RHS of Equation ( <ref type="formula">17</ref>) for j = 1, . . . , p and then set E(z i,j z i,j</p><formula xml:id="formula_28">| x i ; , ) = E(z i,j | x i ; , )E(z i,j | x i ; , ) for 1 ? j = j ? p; 11: end if 12:</formula><p>end for 13:</p><p>Update s j,j = 1/n n i=1 E(z i,j z i,j | x i ; , ) for 1 ? j, j ? p;</p><p>14:</p><p>until The inner loop converges;</p><p>15:</p><p>M-step: update by maximizing criterion (7); 16: until The outer loop converges. Downloaded by [New York University] at 12:33 20 May 2015</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">MODEL SELECTION</head><p>In the probit graphical model, the tuning parameter ? controls the sparsity of the resulting estimator and it can be selected using cross-validation. Specifically, we randomly split the observed data X into D subsets of similar sizes and denote the index set of the observations in the dth subset by T d (d = 1, . . . , D). For any prespecified ?, we denote by <ref type="bibr">[-d</ref>] ? the maximizer of the criterion (4) estimated by Algorithm 1 using all observations except those in T d . We also denote by [-d] and S [d] = (s [d]  j,j ) p?p the analogs of and S in Section 2.2, but computed from the data in T c d and T d , respectively. In particular, an element of S [d] is defined as s [d]  j</p><formula xml:id="formula_29">,j = |T d | -1 i?T d E(z i,j z i,j | x i ; [-d] , [-d] ? ), for 1 ? j, j ? p,</formula><p>where |T d | is the cardinality of T d . Given [-d] and</p><formula xml:id="formula_30">[-d] ?</formula><p>, S [d] can be estimated by the algorithm introduced in Section 2.3, that is, the inner loop of Algorithm 1. Thus, the optimal tuning parameter can be selected by maximizing the following criterion:</p><formula xml:id="formula_31">max ? D d=1 log det [-d] ? -trace S [d] [-d] ? -p log(2? ). (<label>19</label></formula><formula xml:id="formula_32">)</formula><p>We note that we have also considered the AIC and BIC type criteria for choosing the tuning parameter ?. We found that AIC performs the worst among the three due to estimating many zero parameters as nonzeros <ref type="bibr" target="#b17">(Lian 2011</ref>); BIC and cross-validation tend to have similar performances in estimating zero parameters as zeros, but BIC also tends to estimate the nonzero parameters as zeros. Therefore, we choose to use cross-validation. Due to space limitation, the results are not included.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">NUMERICAL EXAMPLES</head><p>In this section, we use two sets of simulated experiments to illustrate the performance of the probit graphical model. The first set aims at comparing the computational cost of the three methods that estimate the Q-function in the E-step, namely the direct computation, the MCMC sampling and the approximation algorithm. The second set compares the performance of the probit graphical model using the approximation algorithm to that of the Gaussian graphical model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">COMPUTATIONAL COST AND PERFORMANCE</head><p>Note that the computational costs of the direct estimation and the MCMC sampling are extremely high when p is even of moderate size. Therefore, in this experiment, we simulate a low-dimensional dataset with p = 5 variables and n = 10 observations. Specifically, we define the underlying inverse covariance matrix as a tri-diagonal matrix with 1's on the main diagonal and 0.5 on the first subdiagonal. The corresponding covariance matrix is then scaled so that all the variances are equal to 1. Then, for i = 1, . . . , n, we generate the latent data z i = (z i,1 , . . . , z i,p ) from N (0, ) and discretize them as follows: for each Downloaded by [New York University] at 12:33 20 May 2015 </p><formula xml:id="formula_33">? (j ) k = ? ? ? ? ? ? ? -?, if k = 0; -1 (0.2) if k = 1; -1 (0.4) if k = 2; +?, if k = 3; (20) and x i,j = 2 k=0 I(z i,j ? ? (j ) k ) (i = 1, . . . , n; j = 1, . . . , p), that is, the value of x i,j is k if it locates in interval [? (j ) k-1 , ? (j ) k ).</formula><p>The probit graphical model is applied using four estimation methods in the E-step, namely the direct computation, a standard Gibbs sampling, the Gibbs sampler proposed by <ref type="bibr" target="#b23">Pakman and Paninski (2012)</ref> and the approximation algorithm proposed in this article. The procedure is repeated for 20 times, and the computational costs are shown in Table <ref type="table" target="#tab_1">1</ref>. We can see that the median CPU time of the approximation algorithm is only about 1/1000 of that of the Gibbs sampling and about 1/80,000 of that of the direct computation. To further compare the estimation accuracy of these methods, we use the Frobenius and entropy loss metrics that are defined next:</p><formula xml:id="formula_34">FL = 1?j&lt;j ?p (? j,j -? j,j ) 2 1?j&lt;j ?p ? 2 j,j , (<label>21</label></formula><formula xml:id="formula_35">) EL = trace( -1 ) -log[det( -1 )] -p, (<label>22</label></formula><formula xml:id="formula_36">)</formula><p>where denotes the estimated network. The performance of the three estimation methods is depicted in Figure <ref type="figure">1</ref>. It can be seen that the direct computation and Gibbs sampling methods are fairly similar in performance (the result using the R package "tmg" is almost identical to that of the standard Gibbs sampling and not shown); this is expected since they can all be considered "exact" approaches. In terms of the Frobenius and entropy losses, the approximation algorithm lags slightly behind its competitors when the tuning parameter ? is relatively small, whereas for larger ? it outperforms them. This is because in this simulation study, the true is very sparse and the mean field approximation also happens to implicitly enforce a conditional independence structure on the S matrix. These findings suggest that the proposed approximation algorithm achieves its orders of magnitude computational savings over the competitors with minimal degradation in performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">EXPERIMENTS WITH DIFFERENT TYPES OF GRAPHS</head><p>In this section, we evaluate the performance of the proposed method by simulation studies. These examples simulate four types of network structures: a scale-free graph, a hub graph, a nearest-neighbor graph and a block graph. Each network consists of p = 50 nodes. The details of these networks are described as follows:</p><p>Example 1: Scale-free graph. A scale-free graph has a power-law degree distribution and can be simulated by the Barabasi-Albert algorithm <ref type="bibr" target="#b2">(Barabasi and Albert 1999)</ref>. A realization of a scale-free network is depicted in Figure <ref type="figure" target="#fig_2">2(a)</ref>.</p><p>Example 2: Hub graph. A hub graph consists of a few high-degree nodes (hubs) and a large amount of low-degree nodes. In this example, we follow the simulation setting in <ref type="bibr" target="#b24">Peng et al. (2009)</ref> and generate a hub graph by inserting a few hub nodes into a very sparse graph. Specifically, the graph consists of three hubs with degrees around eight, and the other 47 nodes with degrees at most three. An example of the hub graph is shown in Figure <ref type="figure" target="#fig_2">2(b)</ref>.</p><p>Example 3: Nearest-neighbor graph. To generate nearest neighbor graphs, we slightly modify the data generating mechanism described in <ref type="bibr" target="#b16">Li and Gui (2006)</ref>. Specifically, we generate p points randomly on a unit square, calculate all p(p -1)/2 pairwise distances, and find the m nearest neighbors of each point in terms of these distances. The nearest neighbor network is obtained by linking any two points that are m-nearest neighbors of each other. The integer m controls the degree of sparsity of the network and the value m = 5 was chosen in the simulation study. Figure <ref type="figure" target="#fig_2">2</ref>(c) exhibits one realization of the nearest-neighbor network.</p><p>Example 4: Block graph. In this setting, we generate a graph using a random adjacency matrix generated from the stochastic block model. Specifically, for nodes 1-20 the probability of being linked is 0.2, for nodes 21-30 the probability of being linked is 0.5, Downloaded by [New York University] at 12:33 20 May 2015 whereas for all other pairs of nodes the probability of having a link is 0.02. Figure <ref type="figure" target="#fig_2">2(d)</ref> illustrates such a random graph.</p><p>The ordinal data are generated as follows. First, we generate the inverse covariance matrix of the latent multivariate Gaussian distribution. Specifically, each off-diagonal element ? j,j is drawn uniformly from [-1, -0.5] ? [0.5, 1] if nodes j and j are linked by an edge, otherwise ? j,j = 0. Further, the diagonal elements were all set to be 2 to ensure positive definiteness, and the corresponding covariance matrix is scaled so that all the variances are equal to 1. Second, we generate the latent data z i = (z i,1 , . . . , z i,p ) as an iid sample from N (0, ). Finally, the continuous latent data z i 's are discretized into ordinal Downloaded by [New York University] at 12:33 20 May 2015 scale with three levels by thresholding. Specifically, for each j = 1, . . . , p, we set</p><formula xml:id="formula_37">? (j ) k = ? ? ? ? ? ? ? -?, if k = 0; -1 (0.1) if k = 1; -1 (0.2) if k = 2; +?, if k = 3; (23) and set x i,j = 2 k=0 I(z i,j ? ? (j )</formula><p>k ) (i = 1, . . . , n; j = 1, . . . , p). For each example, we considered different sample sizes, with n = 50, 100, 200, and 500.</p><p>We compare the proposed probit graphical model with two other methods. One consists of direct application of the graphical lasso to the ordinal data X, ignoring their discrete nature. The second uses the graphical lasso on the latent continuous data Z. We refer to the first one as the naive method and the second one as an oracle method because it represents an ideal situation where Z is exactly recovered. Of course, the latter never occurs with real data, but serves as a benchmark for comparison purposes. The receiver operating characteristic curve (ROC) was used to evaluate the accuracy of network structure estimation. The ROC curve plots the sensitivity (the proportion of correctly detected links) against the false positive rate (the proportion of misidentified zeros) over a range of values of the tuning parameter ?. The sensitivity and the false positive rate are defined as follows:</p><formula xml:id="formula_38">Sensitivity = 1?j&lt;j ?p I(? j,j = 0, ? j,j = 0) 1?j&lt;j ?p I(? j,j = 0) , (<label>24</label></formula><formula xml:id="formula_39">)</formula><p>False positive rate = 1?j&lt;j ?p I(? j,j = 0, ? j,j = 0)</p><formula xml:id="formula_40">1?j&lt;j ?p I(? j,j = 0) , (<label>25</label></formula><formula xml:id="formula_41">)</formula><p>where I(?) is an indicator function whose value is 1 if the statement in the parenthesis is true, and is 0 if it is false. In addition, the Frobenius loss and the entropy loss defined in (21) were used to evaluate the performance of parameter estimation.</p><p>Figure <ref type="figure">3</ref> shows the ROC curves for all simulated examples. The curves are averaged over 50 replications. The oracle method provides a benchmark curve for each setting (blue dotted line in each panel). We can see that when the sample size is relatively small (n = 50, 100, or 200), the probit model (dark solid line) dominates the naive method (red dashed line). When the sample size gets larger, the two methods exhibit similar performance.</p><p>Table <ref type="table" target="#tab_3">2</ref> summarizes the parameter estimation measured by the Frobenius loss and the entropy loss. The results were again averaged over 50 repetitions and the tuning parameter ? was selected using the cross-validation introduced in Section 2.4. The oracle method evidently performs the best, as it should. Comparing the two methods based on the observed data X, we can see that the Frobenius losses from the probit model are consistently lower than those from the naive method. The advantage is more significant when the sample size is moderate (n = 100 or 200). In terms of the entropy loss, we can see that the probit model outperforms the naive method for relatively large sample sizes, such as n = 200 and 500. Downloaded by [New York University] at 12:33 20 May 2015 Figure <ref type="figure">3</ref>. The ROC curves estimated by the probit graphical model (solid dark line), the oracle method (dotted blue line), and the naive method (dashed red line). The oracle method and the naive method simply apply the graphical lasso algorithm to the latent continuous data Z and the observed discrete data X, respectively.</p><p>Downloaded by [New York University] at 12:33 20 May 2015 The oracle method and the naive method simply apply the graphical lasso algorithm to the latent continuous data Z and the observed discrete data X, respectively. The results are averaged over 50 repetitions and the corresponding standard deviations are recorded in the parentheses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">DATA EXAMPLES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">APPLICATION TO MOVIE RATING RECORDS</head><p>In this section, we apply the probit graphical model to Movielens, a dataset containing rating scores for 1682 movies by 943 users. The rating scores have five levels, where 1 corresponds to strong dissatisfaction and 5 to strong satisfaction. More than 90% of the entries are missing in the full data matrix; for this reason, we consider a subset of the data containing 193 users and 32 movies, with 15% missing values. The missing values were imputed by the median of the observed movie ratings.</p><p>The estimated network for these 32 movies is shown in Figure <ref type="figure" target="#fig_3">4</ref>. We can see that the estimated network consists of a large connected community as well as a few isolated nodes. The large community mainly consists of mass marketed commercial movies, dominated by science fiction, and action films. These movies are characterized by high production budgets, state of the art visual effects, and famous directors and actors. Examples in this data subset include the Star Wars franchise <ref type="bibr">("Star Wars" (1977)</ref>, "The Empire Strikes Back" (1980), and "Return of the Jedi" (1983), directed/produced by Lucas), the Terminator series <ref type="bibr">(1984,</ref><ref type="bibr">1991)</ref> directed by Cameron, the Indiana Jones franchise ("Raiders of Lost Ark" (1981), "The Last Crusade" (1989), directed by Spielberg), the Alien series, etc. As expected, movies within the same series are most strongly associated. Further, "Raiders of the Lost Ark" <ref type="bibr">(1981)</ref> and "Back to the Future" <ref type="bibr">(1985)</ref> form two hub nodes each having 16 connections to other movies and their common feature is that they were directed/produced by Spielberg. Downloaded by [New York University] at 12:33 20 May 2015 On the other hand, isolated nodes tend to represent "artsier" movies, such as crime films and comedies whose popularity relies more on the plot and the cast than on big budgets and special effects, many with cult status among their followers. Examples include "Pulp Fiction" (1994) (one of the most popular Tarantino movies), "Fargo" (1996) (a quintessential Coen brothers movie), "When Harry Met Sally" <ref type="bibr">(1989), and</ref><ref type="bibr">"Princess Bride" (1987)</ref>. These films have no significant connections in the network, either with each other or with the commercial movies in the large community. This is likely due to two reasons:</p><p>(1) we restricted the dataset to movies rated by a substantial fraction of the users, so while there probably are connections from "Fargo" to other Coen brothers movies, the other ones did not appear in this set; and (2) there is a greater heterogeneity of genres in this set than among the large group of science-fiction and action films. In other words, liking "When Harry Met Sally" (a romantic comedy) does not make one more likely to enjoy "Silence of the Lambs" (a thriller/horror movie), whereas liking "Terminator" suggests you are more likely to enjoy "Alien." A more complete analysis of this dataset is an interesting topic for future work and requires a more sophisticated way of dealing with missing data, which is not the focus of the current article. Downloaded by [New York University] at 12:33 20 May 2015 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">NATIONAL EDUCATION LONGITUDINAL SURVEY STUDY</head><p>The data for the second example come from the National Educational Longitudinal Study of 1988 (NELS:88), whose objective was to assess student attitudes toward a number of questions about their school, education, and activities. The data used were obtained from the study's website http://nces.ed.gov/surveys/nels88/ and correspond to a sample of 12,144 students of eighth-graders. We selected 218 questions with ordinal and/or binary responses that focused on diverse issues, including school work and home experiences, educational and occupational aspirations, access to educational resources and other support, as well as student background and school characteristics. Ordinal responses were chosen from the following options: "OFTEN", "SOMETIMES," "RARELY," and "NEVER," while binary ones corresponded to a "YES/NO" answer. Figure <ref type="figure" target="#fig_4">5</ref> depicts the histogram of the frequency of options in 218 survey questions.</p><p>The estimated network of the selected 218 survey questions is shown in Figure <ref type="figure">6</ref>. It is apparent that the estimated network exhibits a strong clustering structure. For example, the set of the following nodes "F1S33A," "F1S33B," "F1S33C," "F1S33D," and "F1S33E" forms a cluster, separated from the remaining nodes. These five questions are a part of a sequence of similar questions, focusing on vocational coursework. Specifically, the question inquires whether "In your most recent or current VOCATIONAL course, how much emphasis did/does your teacher place on the following objectives?" and the specific objectives are listed in Table <ref type="table" target="#tab_4">3</ref>. It can be seen that questions "F1S33A"-"F1S33E" reflect different aspects of knowledge and analytical ability that a student should acquire from a vocational course, and therefore it is reasonable that they form a tight cluster. Similar  </p><formula xml:id="formula_42">? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? Figure 6</formula><p>. Layout of the network estimated by the proposed probit graphical model. The nodes represent the survey questions labeled by their code. The area of a node is proportional to its degree and the width of a link is proportional to the magnitude of the corresponding partial correlations. The red lines represent positive associations, while the light green lines negative ones. clustering patterns can be observed in other parts of the graph, for example, serial "F1S7," serial "F1S8," serial "F1S12," serial "F1S25," etc.</p><p>Next, we focus on broad patterns revealed by the model, as depicted in Figure <ref type="figure">6</ref>. The upper right corner captures relationships between serial questions broadly related to coursework (F1S22-F1S25) in various disciplines (mathematics, science, English, computer education), whereas in the lower left corner there are questions related to overall attitude and study patterns regarding mathematics and science classes (F1S26-F1S32). It is interesting to observe that the model does not discover any relationships between these two question clusters. In the center of the plot we find questions related to various life aspects and being successful/accomplishing them (F1S46) which is negatively associated with a cluster of questions related to working hard in school for good grades (F1S11). In the center, we also find a cluster of serial questions related to different ways of interacting with friends (F1S44) Downloaded by [New York University] at 12:33 20 May 2015 7. Layout of the estimated network by the graphical lasso algorithm. The nodes represent the survey questions labeled by their code. The area of a node is proportional to its degree and the width of a link is proportional to the magnitude of the corresponding partial correlations. The red lines represent positive associations, while the light green lines negative ones.</p><p>which is negatively correlated to questions related to students awards (F1S8). In the upper left corner we see the serial cluster on grades performance (F1S39) which is also negatively correlated with some of the questions related to amount of coursework in various subjects (F1S22 and F1S24). Finally, in the bottom right corner we encounter questions related to school attendance and attitude toward it (F1S10, F1S12).</p><p>Overall, the model reveals interesting and informative patterns, much more so than its Gaussian counterpart shown in Figure <ref type="figure">7</ref>.</p><p>Next, we examined pairs of questions exhibiting the largest positive partial correlations (based on the theory of Gaussian graphical models, the partial correlation of variables j and j is defined as ? j,j = -? j,j / ? ? j,j ? j ,j ). The results are shown in Overall, the proposed model identifies strong clustering patterns in the questions being asked in this survey, which primarily correspond to series of related in intent and purpose questions, thus indirectly validating its usefulness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">SUMMARY AND DISCUSSION</head><p>Ordinal data occur often in practice and are usually treated as continuous for most analyses, including estimating dependencies between the variables under consideration by fitting a graphical model. Our proposed model, explicitly takes into account the ordinal nature of the data in the graphical modeling step. While direct computation for the proposed model is expensive, the approximations employed allow us to efficiently fit high-dimensional models. On those datasets that the model can be fitted directly, our numerical results show that the approximations we make result in a minimal loss of accuracy. We leave the theoretical properties of both the exact estimator and its approximate version as a topic for future work.</p><p>The method proposed in this article can also be extended to fit the multivariate ordinal regression model, where multiple ordinal responses are fitted on a number of covariates. Specifically, suppose W j 1 , . . . , W jm j are the covariates associated with the jth response. Following the notation in Section 2.1, let X j denote the jth response, which is an ordinal variable, and Z j the corresponding latent continuous variable. We may assume Z j = ? j 0 + ? j 1 W j 1 + ? ? ? + ? jm j W jm j + j , where ? j 0 is the intercept, and ? j 1 , . . . , ? jm j are regression coefficients. In addition, we assume that 1 , . . . , p jointly follow a Gaussian distribution with mean zero and covariance matrix = -1 . To estimate the regression coefficients, we may modify the M-step in Section 2.2 to estimate and ? j 's simultaneously. <ref type="bibr" target="#b30">Rothman, Levina, and Zhu (2010)</ref> discussed a similar problem as the modified M-step, and the algorithm there can be directly applied.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>3. Downloaded by [New York University] at 12:33 20 May 2015 M-step. The M-step updates by maximizing the 1 -regularized Q-function (up to a constant and a factor): ? = arg max log det ( )trace(S )? j =j |? j,j |.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>)</head><label></label><figDesc>Downloaded by [New York University] at 12:33 20 May 2015</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Illustration of the networks used in four simulated examples: Scale-free graph, hub graph, nearestneighbor graph, and block graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure4. The network estimated by the probit graphical model. The nodes represent the movies labeled by their titles. The area of a node is proportional to its degree and the width of a link is proportional to the magnitude of the corresponding partial correlations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Histogram of the number of options in 218 survey questions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>The numbers are the mean CPU times for different tuning parameter values and 20 replications, with median absolute deviations in parentheses (in sec)</figDesc><table><row><cell>Method</cell></row></table><note><p>j = 1, . . . , p, set</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Figure 1. Comparison of Frobenius loss and entropy loss over different values of the tuning parameter. The direct computation, the MCMC sampling and the approximation algorithm are, respectively, represented by blue dotted, red dashed, and black solid lines.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.54</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Frobenius loss</cell><cell>0.48 0.50 0.52</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Entropy loss</cell><cell>2.0 2.1 2.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.46</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.44</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>-7</cell><cell>-6</cell><cell>-5</cell><cell>-4</cell><cell>-3</cell><cell>-2</cell><cell>-1</cell><cell>0</cell><cell>-7</cell><cell>-6</cell><cell>-5</cell><cell>-4</cell><cell>-3</cell><cell>-2</cell><cell>-1</cell><cell>0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">log 2 (?)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">log 2 (?)</cell><cell></cell><cell></cell></row><row><cell>Downloaded by [New York University] at 12:33 20 May 2015</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>The Frobenius loss and the entropy loss estimated by the probit graphical model, the oracle method and the naive method</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Frobenius loss</cell><cell></cell><cell></cell><cell>Entropy loss</cell><cell></cell></row><row><cell>Example</cell><cell>n</cell><cell>Gaussian</cell><cell>Oracle</cell><cell>Probit</cell><cell>Gaussian</cell><cell>Oracle</cell><cell>Probit</cell></row><row><cell>Scale-free</cell><cell>50</cell><cell>2.3 (0.12)</cell><cell>0.7 (0.05)</cell><cell>2.2 (0.13)</cell><cell>12.0 (0.73)</cell><cell>3.1 (0.29)</cell><cell>23.1 (1.83)</cell></row><row><cell></cell><cell>100</cell><cell>2.2 (0.13)</cell><cell>0.4 (0.08)</cell><cell>1.7 (0.09)</cell><cell>9.4 (0.68)</cell><cell>1.9 (0.29)</cell><cell>10.1 (0.45)</cell></row><row><cell></cell><cell>200</cell><cell>1.7 (0.12)</cell><cell>0.3 (0.02)</cell><cell>1.2 (0.04)</cell><cell>6.4 (0.33)</cell><cell>1.1 (0.10)</cell><cell>5.4 (0.26)</cell></row><row><cell></cell><cell>500</cell><cell>0.9 (0.05)</cell><cell>0.1 (0.01)</cell><cell>0.7 (0.04)</cell><cell>3.3 (0.19)</cell><cell>0.5 (0.05)</cell><cell>2.7 (0.19)</cell></row><row><cell>Hub</cell><cell>50</cell><cell>1.2 (0.06)</cell><cell>0.3 (0.02)</cell><cell>1.1 (0.04)</cell><cell>21.2 (1.32)</cell><cell>5.8 (0.70)</cell><cell>29.4 (1.76)</cell></row><row><cell></cell><cell>100</cell><cell>1.1 (0.10)</cell><cell>0.1 (0.01)</cell><cell>0.8 (0.03)</cell><cell>15.9 (1.03)</cell><cell>3.2 (0.27)</cell><cell>15.1 (0.64)</cell></row><row><cell></cell><cell>200</cell><cell>0.8 (0.05)</cell><cell>0.1 (0.01)</cell><cell>0.6 (0.01)</cell><cell>11.9 (0.39)</cell><cell>1.8 (0.23)</cell><cell>10.4 (0.33)</cell></row><row><cell></cell><cell>500</cell><cell>0.6 (0.02)</cell><cell>0.0 (0.00)</cell><cell>0.5 (0.01)</cell><cell>9.1 (0.16)</cell><cell>0.7 (0.06)</cell><cell>7.5 (0.16)</cell></row><row><cell>Nearest-neighbor</cell><cell>50</cell><cell>1.4 (0.04)</cell><cell>0.6 (0.02)</cell><cell>1.3 (0.06)</cell><cell>16.5 (0.80)</cell><cell>5.6 (0.30)</cell><cell>25.6 (2.04)</cell></row><row><cell></cell><cell>100</cell><cell>1.3 (0.08)</cell><cell>0.4 (0.02)</cell><cell>1.0 (0.02)</cell><cell>12.1 (0.52)</cell><cell>3.5 (0.36)</cell><cell>12.4 (0.76)</cell></row><row><cell></cell><cell>200</cell><cell>1.0 (0.04)</cell><cell>0.2 (0.01)</cell><cell>0.7 (0.03)</cell><cell>8.6 (0.32)</cell><cell>2.0 (0.11)</cell><cell>7.5 (0.17)</cell></row><row><cell></cell><cell>500</cell><cell>0.6 (0.03)</cell><cell>0.1 (0.01)</cell><cell>0.5 (0.02)</cell><cell>5.5 (0.12)</cell><cell>0.8 (0.02)</cell><cell>4.5 (0.19)</cell></row><row><cell>Random-block</cell><cell>50</cell><cell>1.8 (0.05)</cell><cell>0.7 (0.05)</cell><cell>1.7 (0.04)</cell><cell>14.8 (1.04)</cell><cell>4.7 (0.46)</cell><cell>23.5 (1.76)</cell></row><row><cell></cell><cell>100</cell><cell>1.6 (0.16)</cell><cell>0.4 (0.02)</cell><cell>1.3 (0.03)</cell><cell>10.7 (1.10)</cell><cell>2.9 (0.27)</cell><cell>11.3 (0.46)</cell></row><row><cell></cell><cell>200</cell><cell>1.3 (0.05)</cell><cell>0.2 (0.03)</cell><cell>0.9 (0.05)</cell><cell>7.2 (0.19)</cell><cell>1.6 (0.11)</cell><cell>6.3 (0.32)</cell></row><row><cell></cell><cell>500</cell><cell>0.7 (0.03)</cell><cell>0.1 (0.01)</cell><cell>0.6 (0.03)</cell><cell>4.1 (0.15)</cell><cell>0.7 (0.06)</cell><cell>3.5 (0.13)</cell></row><row><cell>NOTE:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Objectives in survey questions "In your most recent or current vocational course, how much emphasis did/does your teacher place on the following objectives?"</figDesc><table><row><cell>Downloaded by [New York University] at 12:33 20 May 2015</cell><cell></cell></row><row><cell>F1S33A</cell><cell>Teaching you skills you can use immediately</cell></row><row><cell>F1S33B</cell><cell>Teaching you facts, rules, and steps</cell></row><row><cell>F1S33C</cell><cell>Helping you understand how scientific ideas and mathematics are used in work</cell></row><row><cell>F1S33D</cell><cell>Thinking about what a problem means and the ways it might be solved</cell></row><row><cell>F1S33E</cell><cell>Helping you to understand mathematical and scientific ideas by helping you to manipulate</cell></row><row><cell></cell><cell>physical objects (tools, machines, lab equipment)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 .</head><label>4</label><figDesc>Among the top five ones, four pairs correspond to serial questions. The only exception is pair "F1S44D-F1S43," although it inquires about extra reading, outside school. Analogously, Table5lists the pair of questions exhibiting the strongest negative partial Downloaded by [New York University] at 12:33 20 May 2015</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Downloaded by [New York University] at 12:33 20 May 2015</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p><rs type="person">E. Levina</rs>'s research is partially supported by <rs type="funder">NSF</rs> grants <rs type="grantNumber">DMS-0805798</rs>, <rs type="grantNumber">DMS-1106772</rs>, and <rs type="grantNumber">DMS-1159005</rs>. <rs type="person">G. Michailidis</rs>'s research is partially supported by <rs type="funder">NIH</rs> grant <rs type="grantNumber">1RC1CA145444-0110</rs>. <rs type="person">J. Zhu</rs>'s research is partially supported by <rs type="funder">NSF</rs> grant <rs type="grantNumber">DMS-0748389</rs> and <rs type="funder">NIH</rs> grant <rs type="grantNumber">R01GM096194</rs>.</p><p>[Received July 2012. Revised July 2013.]</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_wgKsvxT">
					<idno type="grant-number">DMS-0805798</idno>
				</org>
				<org type="funding" xml:id="_ySgk8pg">
					<idno type="grant-number">DMS-1106772</idno>
				</org>
				<org type="funding" xml:id="_BxSSSSD">
					<idno type="grant-number">DMS-1159005</idno>
				</org>
				<org type="funding" xml:id="_Ntu8rBh">
					<idno type="grant-number">1RC1CA145444-0110</idno>
				</org>
				<org type="funding" xml:id="_pZgfzMJ">
					<idno type="grant-number">DMS-0748389</idno>
				</org>
				<org type="funding" xml:id="_EBPrjpq">
					<idno type="grant-number">R01GM096194</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>correlations. Note that question pairs "F1S8F-F1S8A," "F1S15B-F1S15A," "F1S16B-F1S16D" are composed of two opposite questions. It is interesting to observe that the model identifies the pair 'F1S10B-F1S12B," which can be interpreted that although students may skip class often they do not feel good about their action. A similar negative partial correlation is present in pair "F1S10A-F1S12A" that addresses a "coming to school late" issue. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bayesian Analysis of Binary and Polychotomous Response Data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chib</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page">185</biblScope>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Model Selection Through Sparse Maximum Likelihood Estimation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>El Ghaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aspremont</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="485" to="516" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Emergence of Scaling in Random Networks</title>
		<author>
			<persName><forename type="first">A.-L</forename><surname>Barabasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Albert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">286</biblScope>
			<biblScope unit="page" from="509" to="512" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The Calculation of the Dosage-Mortality Curve</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bliss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Applied Biology</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">185</biblScope>
			<date type="published" when="1935">1935</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Analysis of Multivariate Probit Models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Greenberg</surname></persName>
		</author>
		<idno>12:33 20</idno>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page" from="347" to="361" />
			<date type="published" when="1998-05">1998. May 2015</date>
		</imprint>
	</monogr>
	<note>New York University</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Network Exploration via the Adaptive LASSO and SCAD Penalties</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Applied Statistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="521" to="541" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sparse Inverse Covariance Estimation With the Graphical Lasso</title>
		<author>
			<persName><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biostatistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="432" to="441" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>183,187</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Joint Structure Estimation for Categorical Markov Networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Levina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Michailidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<idno>No. 507</idno>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">184</biblScope>
			<pubPlace>Ann Arbor</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of Statistics, University of Michigan</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Estimation of Sparse Binary Pairwise Markov Networks Using Pseudo-Likelihoods</title>
		<author>
			<persName><forename type="first">H</forename><surname>H?efling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="883" to="906" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kotz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Balakrishnan</surname></persName>
		</author>
		<title level="m">Continuous Univariate Distributions</title>
		<meeting><address><addrLine>New Jersey</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>nd ed.</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Improved Estimation of High-Dimensional Ising Models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kolar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:0811.1239</idno>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">184</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">available at Eprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Matrix Factorization Techniques for Recommender Systems</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Gibbs Sampling Approach for Generation of Truncated Multivariate Gaussian Random Variables</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kotecha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Djuric</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Society</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1757" to="1760" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Lauritzen</surname></persName>
		</author>
		<title level="m">Graphical Models</title>
		<meeting><address><addrLine>Oxford</addrLine></address></meeting>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On the First and Second Moments of the Truncated Multi-Normal Distribution and a Simple Estimator</title>
		<author>
			<persName><forename type="first">L.-F</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Economics Letters</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="165" to="169" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Evaluation of the Mean and Covariance of the Truncated Multinormal</title>
		<author>
			<persName><forename type="first">P</forename><surname>Leppard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tallis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Statistics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="543" to="553" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Gradient Directed Regularization for Sparse Gaussian Concentration Graphs, With Applications to Inference of Genetic Networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biostatistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="302" to="317" />
			<date type="published" when="2006">2006. 192</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Shrinkage Tuning Parameter Selection in Precision Matrices Estimation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Planning and Inference</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="2839" to="2848" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Moments Calculation for the Doubly Truncated Multivariate Normal Density</title>
		<author>
			<persName><forename type="first">B</forename><surname>Manjunath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wilhelm</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>available at ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Regression Models for Ordinal Data</title>
		<author>
			<persName><forename type="first">P</forename><surname>Mccullagh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society, Series B</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="109" to="142" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Mccullagh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nelder</surname></persName>
		</author>
		<title level="m">Generalized Linear Models</title>
		<meeting><address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Chapman and Hall/CRC</publisher>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
	<note>nd ed.</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">High-Dimensional Graphs With the Lasso</title>
		<author>
			<persName><forename type="first">N</forename><surname>Meinshausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Buhlmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1436" to="1462" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Logistic Regression Models for Ordinal Response Variables</title>
		<author>
			<persName><forename type="first">A</forename><surname>O'connell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Sage Publications, Inc</publisher>
			<pubPlace>Thousand Oaks, CA</pubPlace>
		</imprint>
	</monogr>
	<note>st ed.</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Exact Hamiltonian Monte Carlo for Truncated Multivariate Gaussians</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pakman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Paninski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>available at ArXiv e-prints. [191</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Partial Correlation Estimation by Joint Sparse Regression Model</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Asscociation</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="735" to="746" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>184,192</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Partial Proportional Odds Models for Ordinal Response Variables</title>
		<author>
			<persName><forename type="first">B</forename><surname>Peterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Statistics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="205" to="217" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A Mean Field Theory Learning Algorithm for Neural Networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Complex Systems</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="995" to="1019" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">High-Dimensional Ising Model Selection Using 1 -Regularized Logistic Regression</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1287" to="1319" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">A Path Following Algorithm for Sparse Pseudo-Likelihood Inverse Covariance Estimation (SPLICE)</title>
		<author>
			<persName><forename type="first">G</forename><surname>Rocha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:0807.3734</idno>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">184</biblScope>
		</imprint>
		<respStmt>
			<orgName>Department of Statistics, University of California, Berkeley</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
	<note>New York University. at 12:33 20 May 2015 204 J. GUO ET AL</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sparse Permutation Invariant Covariance Estimation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rothman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Levina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronic Journal of Statistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="494" to="515" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>183,187</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Sparse Multivariate Regression With Covariance Estimation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rothman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Levina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational and Graphical Statistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="947" to="962" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Matchbox: Large Scale Online Bayesian Recommendations</title>
		<author>
			<persName><forename type="first">D</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Herbrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Graepel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of World Wide Web</title>
		<meeting>World Wide Web<address><addrLine>Madrid, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="111" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The Moment Generating Function of the Truncated Multinormal Distribution</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tallis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society, Series B</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="223" to="229" />
			<date type="published" when="1961">1961</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Von Davier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Carstensen</surname></persName>
		</author>
		<title level="m">Multivariate and Mixture Distribution Rasch Models: Extensions and Applications</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">184</biblScope>
			<biblScope unit="page">185</biblScope>
		</imprint>
	</monogr>
	<note>st ed.</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Estimation of the Probability of an Event as a Function of Several Independent Variables</title>
		<author>
			<persName><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Duncan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="167" to="179" />
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Model Selection and Estimation in the Gaussian Graphical Model</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="19" to="35" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
