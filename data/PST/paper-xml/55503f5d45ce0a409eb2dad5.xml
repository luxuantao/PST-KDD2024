<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The ChaLearn gesture dataset (CGD 2011)</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Isabelle</forename><surname>Guyon</surname></persName>
							<email>guyon@chalearn.org</email>
						</author>
						<author>
							<persName><forename type="first">Vassilis</forename><surname>Athitsos</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Pat</forename><surname>Jangyodsuk</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hugo</forename><forename type="middle">Jair</forename><surname>Escalante</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">ChaLearn</orgName>
								<address>
									<addrLine>955 Creston Road</addrLine>
									<postCode>94708-1501</postCode>
									<settlement>Berkeley</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Texas at Arlington</orgName>
								<address>
									<settlement>Arlington</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">INAOE</orgName>
								<address>
									<settlement>Puebla</settlement>
									<country key="MX">Mexico</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">The ChaLearn gesture dataset (CGD 2011)</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">190145356460DF2896353B766952F02D</idno>
					<idno type="DOI">10.1007/s00138-014-0596-3</idno>
					<note type="submission">Received: 31 January 2013 / Revised: 26 September 2013 / Accepted: 14 January 2014</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Computer vision</term>
					<term>Gesture recognition</term>
					<term>Sign language recognition</term>
					<term>RGBD cameras</term>
					<term>Kinect</term>
					<term>Dataset</term>
					<term>Challenge</term>
					<term>Machine learning</term>
					<term>Transfer learning</term>
					<term>One-shot-learning Mathematics Subject Classification (2000) 65D19</term>
					<term>68T10</term>
					<term>97K80</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the data used in the ChaLearn gesture challenges that took place in 2011/2012, whose results were discussed at the CVPR 2012 and ICPR 2012 conferences. The task can be described as: user-dependent, small vocabulary, fixed camera, one-shotlearning. The data include 54,000 hand and arm gestures recorded with an RGB-D Kinect TM camera. The data are organized into batches of 100 gestures pertaining to a small gesture vocabulary of 8-12 gestures, recorded by the same user. Short continuous sequences of 1-5 randomly selected gestures are recorded. We provide man-made annotations (temporal segmentation into individual gestures, alignment of RGB and depth images, and body part location) and a library of function to preprocess and automatically annotate data. We also provide a subset of batches in which the user's horizontal position is randomly shifted or scaled. We report on the results of the challenge and distribute sample code to facilitate developing new solutions. The data, data</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Motivations</head><p>Machine learning has been successfully applied to computer vision problems in the recent year, and particularly to the visual analysis of humans <ref type="bibr" target="#b34">[36]</ref>. Some of the main advances have been catalyzed by important efforts in data annotation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">15,</ref><ref type="bibr" target="#b41">43]</ref>, and the organization of challenges such as image CLEF <ref type="bibr" target="#b2">[3]</ref> and the Visual Object Classes Challenges (VOC) <ref type="bibr" target="#b3">[4]</ref>, progressively introducing more difficult tasks in image classification, 2D camera video processing and, since recently, 3D camera video processing. In 2011/2012, we organized a challenge focusing on the recognition of gestures from video data recorded with a Microsoft Kinect TM camera, which provides both an RGB (red/blue/green) image and a depth image obtained with an infrared sensor (Fig. <ref type="figure" target="#fig_0">1</ref>). Kinect TM has revolutionized computer vision in the past few months by providing one of the first affordable 3D cameras. The applications, initially driven by the game industry, are rapidly diversifying. They include: Sign language recognition Enable deaf people to communicate with machines and hearing people to get translations. This may be particularly useful for deaf children of hearing parents. Remote control At home, replace your TV remote controller by a gesture enabled controller, which you cannot lose and works fine in the dark; turn on your light switch at night; allow hospital patients to control appliances and call for help without buttons; allow surgeons and other professional users to control appliances and image displays in sterile conditions without touching. Games Replace your joy stick with your bare hands. Teach your computer the gestures you prefer to control your games. Play games of skills, reflexes and memory, which require learning new gestures and/or teaching them to your computer. Gesture communication learning Learn new gesture vocabularies from the sign language for the deaf, from professional signal vocabularies (referee signals, marshaling signals, diving signals, etc.), and from subconscious body language. Video surveillance Teach surveillance systems to watch for particular types of gestures (shop-lifting gestures, aggressions, etc.) Video retrieval Look for videos containing a gesture that you perform in front of a camera. This includes dictionary lookup in databases of videos of sign language for the deaf.</p><p>Gesture recognition provides excellent benchmarks for computer vision and machine-learning algorithms. On the computer vision side, the recognition of continuous, natural gestures is very challenging due to the multi-modal nature of the visual cues (e.g., movements of fingers and lips, facial expressions, body pose), as well as technical limita-tions such as spatial and temporal resolution and unreliable depth cues. Technical difficulties include tracking reliably hand, head and body parts, and achieving 3D invariance. On the machine-learning side, much of the recent research has sacrificed the grand goal of designing systems ever approaching human intelligence for solving tasks of practical interest with more immediate reward. Humans can recognize new gestures after seeing just one example (one-shot-learning). With computers though, recognizing even well-defined gestures, such as sign language, is much more challenging and has traditionally required thousands of training examples to teach the software.</p><p>One of our goals was to evaluate transfer learning algorithms that exploit data resources from similar but different tasks (e.g., a different vocabulary of gestures) to improve performance on a given task for which training data are scarce (e.g., only a few labeled training examples of each gesture are available). Using a one-shot-learning setting (only one labeled training example of each gesture is available) pushes the need for exploiting unlabeled data or data that are labeled, but pertain to a similar but different vocabularies of gestures. While the one-shot-learning setting has implications of academic interest, it has also practical ramifications. Every application needs a specialized gesture vocabulary. The availability of gesture recognition machines, which easily get tailored to new gesture vocabularies, would facilitate tremendously the task of application developers.</p><p>Another goal was to explore the capabilities of algorithms to perform data fusion (in this case fuse the RGB and depth modalities). Yet another goal was to force the participants to develop new methods of feature extraction from raw video data (as opposed, for instance, to work from extracted skeleton coordinates).</p><p>Beyond the goal of benchmarking algorithms of gesture recognition, our dataset offers possibilities to conduct research on gesture analysis from the semiotic, linguistic, sociological, psychological, and aesthetic points of view. We have made an effort to select a wide variety of gesture vocabularies and proposed a new classification of gesture types, which is more encompassing than previously proposed classifications <ref type="bibr" target="#b26">[28,</ref><ref type="bibr" target="#b32">34]</ref>.</p><p>Our new gesture database complements other publicly available datasets in several ways. It complements sign language recognition datasets (e.g., <ref type="bibr" target="#b14">[16,</ref><ref type="bibr" target="#b31">33]</ref>) by providing access to a wider variety of gesture types. It focuses on upper body movement of users facing a camera, unlike datasets featuring full-body movements <ref type="bibr" target="#b20">[22]</ref>, poses <ref type="bibr" target="#b40">[42]</ref> or activities <ref type="bibr" target="#b12">[14,</ref><ref type="bibr" target="#b25">27,</ref><ref type="bibr" target="#b27">29,</ref><ref type="bibr" target="#b35">37]</ref>. As such, it provides an unprecedentedly large set of signs performed with arms and hand suitable to design man-machine interfaces, complementing datasets geared towards shape and motion estimation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b20">22]</ref> or object manipulation <ref type="bibr" target="#b29">[31]</ref>. The spacial resolution of Kinect limits the use of the depth information to decipher facial expressions or even to accurately resolve finger postures. But the RGB modality provides enough resolution for such studies: Our dataset can be used both as a regular video dataset, using the RGB video track, or as a 3D camera dataset if the depth modality is used. Our choice of modalities contrasts with other datasets using, e.g., motion capture sensors <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b19">21]</ref>.</p><p>Our new dataset has been used to carve out single user gesture recognition tasks for a small vocabulary of gestures for the challenge described in more detail in the next section. We recorded short sequences of gestures randomly ordered, hence no linguistic model can be used to improve performance of recognition. The bf hand is returned to a resting position between gestures to facilitate separating individual gestures. In the challenge tasks, for each batch of data, only one labeled example of each gesture was provided for training (one-shot-learning). However, the dataset offers the possibility to define other tasks, multi-user tasks (several instances of the same gestures were recorded by multiple users) and gesture spotting tasks (recognizing gestures pertaining to a given lexicon in the presence of distracters). New defined tasks do not need to be limited to one-shot-learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Tasks of the first ChaLearn gesture challenge</head><p>To see what computer vision and machine learning are capable of, ChaLearn launched in 2012 a challenge with prizes donated by Microsoft using the dataset described in this document. We organized two rounds in conjunction with the CVPR conference (Providence, Rhode Island, USA, June 2012) and the ICPR conference (Tsukuba, Japan, November 2012). This competition consists of two main components: a development phase (December 7, 2011 to April 6, 2012) and a final evaluation phase (April 7, 2012 to April 10. 2012):</p><p>During the development phase of the competition, the participants built a learning system capable of learning from a single training example a gesture classification problem. To that end, they obtained development data consisting of several batches of gestures, each split into a training set (of one example for each gesture) and a test set of short sequences of one to five gestures. Each batch contained gestures from a different small vocabulary of 8-12 gestures, for instance diving signals, signs of American Sign Language representing small animals, Italian gestures, etc. The test data labels were provided for the development data, so the participants can self-evaluate their systems. To evaluate their progress and compare themselves with others, they could use validation data, for which one training example was provided for each gesture token in each batch, but the test data labels were withheld. Kaggle provided a website to which the prediction results could be submitted. <ref type="foot" target="#foot_0">1</ref> A real-time leaderboard showed participants their current standing based on their validation set predictions.</p><p>During the final evaluation phase of the competition, the participants performed similar tasks as those of the validation data on new final evaluation data revealed at the end of the development phase. The participants had a few days to train their systems and upload their predictions. Prior to the end of the development phase, the participants were invited to submit executable software for their best learning system to a software vault. This allowed the competition organizers to check their results and ensure the fairness of the competition.</p><p>To compare prediction labels for gesture sequences to the truth values, we used the generalized Levenshtein distances (each gesture counting as one token). The final evaluation score was computed as the sum such distances for all test sequences, divided by the total number of gestures in the test batch. This score is analogous to an error rate. However, it can exceed one. Specifically, for each video, the participants provided an ordered list of labels R corresponding to the recognized gestures. We compared this list to the corresponding list of labels T in the prescribed list of gestures that the user had to play. These are the "true" gesture labels (provided that the users did not make mistakes). We computed the generalized Levenshtein distance L(R, T), i.e., the minimum number of edit operations (substitution, insertion, or deletion) that one has to perform to go from R to T (or vice versa). The Levenhstein distance is also known as "edit distance". For example: L([</p><formula xml:id="formula_0">1 2 4], [3 2]) = 2; L([1], [2]) = 1; L([2 2 2], [2]) = 2.</formula><p>This competition pushed the state of the art in gesture recognition. The participants considerably narrowed down the gap in performance between the baseline recognition system initially provided ( 60 % error) and human performance ( 2 % error) by reaching 7 % error in the second round of the challenge. There remains still much room for improvement, particularly to recognize static postures and subtle finger positions. This paper describes the dataset of the challenge, which has been made freely available for further research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Nomenclature</head><p>In this paper, we use the following nomenclature:</p><p>-Gesture: The actual performance of a body, face, hand or arm movement as recorded, e.g., in a video. A gesture is one instance of a gesture pattern performed by a given person. -Gesture pattern: A prescribed sequence of body, face, hand or arm movements or postures. For instance, a hand wave. -Gesture token: An element of a gesture vocabulary having a designated semantic, role, or function. For instance, the music note gesture vocabulary has eight gestures: do, re, mi, fa, sol, la, ti, high do. Each gesture token is associated with a different gesture pattern, within a vocabulary. However, across vocabularies, the same gesture pattern may be used for different gesture tokens. For instance, the "V" of Victory (a common emblem) can also mean the number two (a sign for numbers). -Gesture vocabulary: A gesture vocabulary is a comprehensive set of gesture tokens complementing each other in a given application domain or context. For instance, the American sign language vocabulary. -Gesture lexicon: A subset of a gesture vocabulary, such as: all the gestures representing "small animals" in the Gestuno sign language vocabulary. For the purpose of our Occasionally, we use the term "gesture" to designate a gesture pattern or a gesture token, when the contexts disambiguates it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4">Paper organization</head><p>The rest of the paper is organized as follows: In Sect. 2, we present the gesture vocabulary and propose a new classification system for types of gestures, according to function or intention. In Sect. 3, we describe our methodology for data collection, including the protocol, formats and data collection software. In Sect. 4, we describe the data collected. In Sect. 5, we describe the manual data annotations provided. In Sect. 6, we review baseline methods and in Sect. 7 baseline results obtained in the challenge. Finally, in Sect. 8, we conclude and indicate new research directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Gesture vocabulary</head><p>Our data collection has been organized around a number of gesture vocabularies. Our aim was to illustrate a broad range of practically useful gestures to test the frontiers of gesture recognition and inspire new applications. We selected gesture lexicons (see examples in Fig. <ref type="figure">2</ref>) from nine categories The types of gesture vocabularies and the meaning of the gestures were hidden to the participants during the challenge and are now fully available to conduct further research. The rationale between this decision was the following:</p><p>1. In the validation data and the final evaluation data, each batch corresponds to a different lexicon. However, in development data, we recorded multiple batches with the same lexicon but with different users to provide enough development data. We did not want to confuse the participants into thinking that the task is a multi-user task. We gave the users who recorded the data a lot of autonomy to interpret how the gestures should be performed. Hence, two batches using the same lexicon may be very different. 2. It may be possible to exploit domain information obtained from the knowledge of the lexicon to improve the performances. However, in this challenge, we wanted the participants to strictly focus on learning from the video examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Gesture patterns</head><p>We define gestures as a sequence of body motions or postures. Gestures can originate from all parts of the body. In this challenge, however, we focus mostly on arm and hand gestures, with occasional head movements or facial expressions. The gestures may be performed with one arm or both arms. The manner in which the gestures are produced and the patterns of gestures employed is generally guided by the application domain. Gestures aimed at communicating from far, such as aviation marshalling signals, are generally performed with the arms while sign languages for communicating from a closer distance (sign language of the deaf, diving signals, surgeon signal) may use hand postures or even detailed finger postures. Dances and pantomimes often use the entire body.</p><p>Our database features these various cases, as exemplified in Figs. <ref type="figure" target="#fig_5">3</ref> and<ref type="figure" target="#fig_2">4</ref>. A gesture can be thought of as a sequence of postures. Gestures often include combinations of arm, hand, and finger postures and they may be static or dynamic. A static gesture is a gesture in which a single posture is held for a certain duration (Fig. <ref type="figure" target="#fig_5">3</ref>). A dynamic gesture (Fig. <ref type="figure" target="#fig_2">4</ref>) consists of a sequence of postures, which may be repetitive or not, and in which the posture order and the timing of the sequence may be critical.</p><p>Other features of gesture patterns may include the direction and orientation of the body parts, and their relative position to other body parts. Some gestures (actions) are performed with accessories (drinking from a mug, answering the phone). The problem is rendered more complicated by the fact that some features are critical for certain gestures and not for others and that there is some variance in the execution, including changes in posture, orientation, and timing, hesitations and spurious gestures.</p><p>All gesture vocabularies that we consider are attached to a classification problems in which gestures have a qualitative meaning. Not featured in our database are gestures having a quantitative meaning: gestures indicating a height, a distance, a rhythm or a speed; graphical gestures like handwriting or signatures; gestures used to control games, visualization software or instruments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Gesture functions</head><p>To organize the gesture vocabularies of our database, we adopted a taxonomy inspired by the "Kendon continuum" <ref type="bibr" target="#b26">[28,</ref><ref type="bibr" target="#b32">34]</ref> based on gesture function (whether or not intentional) rather than based on gesture pattern types. The original "Kendon continuum" is a unidimensional representation of gesture space considering five categories of gestures: Gesticulation → Language-like gestures → Pantomimes → Emblems → Sign Languages. The axis represents increasing This psycho-linguistic classification is not complete enough for our purpose, because it does not incorporate categories of gestures featured in our database: "body language gestures" performed subconsciously without any accompanying speech <ref type="bibr" target="#b24">[26]</ref>, patterns of "action" or "activities" sometimes performed with accessories or objects <ref type="bibr" target="#b33">[35]</ref>, and dances or other artistic performances using gestures <ref type="bibr" target="#b38">[40]</ref>. Furthermore, the originally proposed unidimensional representation confounds several factors, as noted in <ref type="bibr" target="#b32">[34]</ref>: from left to right, (1) the obligatory presence of speech declines; (2) the presence of language properties increases; (3) idiosyncratic gestures are replaced by socially regulated signs. This prompted us to create a multi-dimensional representation.</p><p>We consider three axes (Fig. <ref type="figure" target="#fig_3">5</ref>):</p><p>-Expression: From bottom to top, levels of consciousness decrease. At the bottom, the gestures (signals) are consciously performed and perceived. We qualify them of "active". In contrast, at the top, the gestures (body language) are subconsciously performed and may or may not be consciously perceived. We qualify them of "passive". In the middle of the axis are gestures performed semi-consciously to emphasize speech, called "gesticulations", including deictic gestures (pointing at people or objects) and rhythmic gestures. -Communication: From top-left to bottom-right, quantity of information communicated increases. The left most part represents purely aesthetic "artistic" purposes (dances), whereas the right most part represents purely informative "linguistic" purposes (sign languages). In the middle of that axis, we find "emblems", which are gestures not necessarily part of a language, but having a symbolic meaning. Some emblems are more artistic, and some are more communicative. They include religious or ceremonial gestures like Indian mudras, rally-ing emblems and salutes like the fist, the V of victory and military salutes, and common emblems like "fingers crossed" or"hang loose". -Action: From top-right to bottom-left, the physical impact increases. The right-most part represents "descriptive" gestures part of a narrative (so-called "illustrators") whereas the left most part represents "manipulative" gestures executing actions. By "manipulative" we mean gestures involving eventually tools of objects we manipulate, performed as part of a coordinated action or activity: combing, washing teeth, nailing. We also include activities such as walking, exercising, etc. In the middle of this axis are "pantomimes", which are imitations of manipulative gestures, performed usually without objects, with an artistic purpose to describe actions or activity without words.</p><p>We consider nine types of gestures (Table <ref type="table">1</ref>) providing a classification according to the gesture function rather than to gesture form, defined as follows:</p><p>1. Activity: Gestures performed to physically interact with the environment and attain a desired outcome. Gestures associated with activities (also called "actions") can be performed with all parts of the body. Examples: putting on glasses, answering the phone, drinking from a cup, kicking a ball. Actions can be ergodic (associated with the notion of work and the capacity of humans to manipulate the physical world, create artifacts) or epistemic (associated with learning from the environment through tactile experience or haptic exploration). Actions may be performed consciously or subconsciously. 2. Pantomime: Gestures performed to mimic an action, usually emphasizing or modifying some aspects in a codified way to make it more easily recognizable and executable in the absence of accessories. Examples: mimic climb- They may also include affect displays like tightening the fist or smiling. 6. Illustrators: Gestures generally accompanying speech and visually representing the same conveyed meaning, but which may be performed silently to substitute for speech and are generally specific of an ethnic group. Usually performed with one or both hands and sometimes the head. Example: Italian gestures. Illustrators are sometimes called quasi-linguistic gestures or propositional gestures. Illustrators include"iconic gestures" illustrating with the hand physical concrete objects or situations and "metaphoric gestures" representing abstract concepts. Illustrators are consciously used and consciously understood. 7. Emblems: Gestures having a symbolic, political or religious meaning. They are usually performed with one or both hands. Examples: The V of victory, military salutes, Indian mudras, insults and praises. Emblems are consciously used and consciously understood. 8. Sign language: Gestures usually performed with one or both hands, sometimes with concurrent facial expressions, pertaining to a sign language, used to communicate silently between two or more people, usually from a relatively small distance. Examples: deaf sign language signs, signs for music notes or for numbers. Signs are consciously used and consciously understood. 9. Signals: A gesture usually performed with one or both arms and/or hands, used to signal, often from a distance, an event, danger, or actions to be performed. Examples: Marshaling signals, man-machine communication signals. Includes deictic gestures (finger pointing). Signals are consciously used and consciously understood.</p><p>We have positioned the various types of gestures in our proposed multi-dimensional space (Fig. <ref type="figure" target="#fig_3">5</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Gesture lexicons</head><p>Each type of gestures is illustrated in our database with a few examples of gesture lexicons, which are small vocabularies of 8-12 gesture tokens (Fig. <ref type="figure">2</ref>).</p><p>Even though our nine categories of gesture types were not created according to use of body parts (arm, hand, fingers, head, entire body) or use of motion (slow or fast movement, static posture, pointing, beat, etc.), the lexicons are usually homogeneous with respect to their use of body parts and motion. This explains in part variations in recognition performance across lexicons, in the challenge. We have provided on our website a manual classification of lexicons according to body part and motion.</p><p>As indicated in Table <ref type="table">1</ref>, the various gesture types are not evenly represented. This reflects more ease of access than natural abundance of gestures since those vocabularies were to a large extent obtained from the Internet and the literature.</p><p>Accordingly, the categories "activity" and "gesticulation" may be under-represented. Activities have not yet been systematically inventoried. We used examples using familiar objects corresponding to familiar activities performed at a desk (answering the phone, taking notes, typing, etc.) or in front of a mirror (combing, brushing teeth, washing hands, etc), hence lending themselves to be performed in front of a fixed camera. Gesticulations are very culture-dependent and to a large extent personal. We drew our examples from widely acknowledged psycho-linguistic books <ref type="bibr" target="#b26">[28,</ref><ref type="bibr" target="#b32">34]</ref>. However, they probably cover only a small fraction of gesticulations.</p><p>On the contrary, lexicons derived from "sign languages" and "signals" may be over-represented. For sign language, we had available a remarkably well documented illustrated glossary of Gestuno, a universal sign language invented in the 1960s and publicly available on the Internet <ref type="bibr" target="#b37">[39]</ref>. It consists of flash card annotated with graphical signs indicating how the gesture should be dynamically performed (Fig. <ref type="figure">6</ref>). An example of Gestuno flash card is shown in Fig. <ref type="figure">7</ref>. Because these gestures are typical of sign languages for the deaf, an important domain of gesture recognition, and were well documented, we created many lexicons with subsets of the 5,000 gesture vocabularies of Gestuno. They include a wide variety of gesture pattern types: static postures and dynamic gestures, finger, hand, arm and head gestures, and gestures composed of several gesture components. They are typically more complex than other gestures, but they are designed for communication purposes, so should be easily and unambiguously recognized. We also included other examples of sign languages: baby signs, which are signs used to teach hearing infants between one and three years of age, who are capable of language, but not yet capable of pronouncing words intelligi-bly. Pediatric studies demonstrated that sign language communication with infants reduces the frustration of children (and adults) and accelerates language acquisition and mental development <ref type="bibr" target="#b5">[6]</ref>. This method has also been experimented with success with mildly mentally disabled children <ref type="bibr">[10]</ref>. We added two examples of sign vocabularies to communicate numerals (Chinese numbers and scuba diver numbers) and a vocabulary used by American music professors to teach music notes, derived from the Curwen method <ref type="bibr" target="#b9">[11]</ref>, later refined by Kodáli.</p><p>For Other types of gesture vocabularies were coarsely surveyed. Body language gestures were derived from a book on the subject <ref type="bibr" target="#b24">[26]</ref> and we created flash cards ourselves. We provided a few examples of dance lexicons (aerobics, ballet and flamenco), mostly based of well-identifiable postures that are relatively easy to perform by an athletic person who is not a professional dancer. There exist dance notation systems such as a Laban notation <ref type="bibr" target="#b43">[45]</ref>. However, hiring professional dancers was impractical as well as teaching the Laban notation to our users. For pantomimes, which are intermediate between dance and action, we created our own vocabularies. Two of them mimic the actions of our two action vocabularies (gestures performed at a desk and personal hygiene gestures), and the others correspond to action, sports, or musical instrument playing. We found examples of emblems in a book on common American gestures employed by hearing people <ref type="bibr" target="#b44">[46]</ref> and included Indian mudras, which are symbolic Indian gestures used in dances and rituals <ref type="bibr" target="#b7">[8]</ref>. As illustrators we included a set of gestures drawn from a method to teach Spanish to hearing children (mimicking musical instruments) <ref type="bibr" target="#b0">[1]</ref>, gestures borrowed from common American gestures <ref type="bibr" target="#b44">[46]</ref>, and Italian gestures <ref type="bibr" target="#b36">[38]</ref>.</p><p>For simplification, we attributed each gesture lexicon to a single category. However, in some cases, this was difficult for various reasons. First, some lexicons contain gestures pertaining to several categories. For example, Italian gestures contain both illustrators (the sign for drinking) and emblems (the OK sign). But, because they form a whole, we kept them together. Second, certain categories are not mutually exclusive. For example, sign languages for the deaf include many emblems and illustrators. Third some categories do not have a sharp decision boundary. For example, it is arguable whether Table <ref type="table">1</ref> Type of gesture lexicons: The gesture vocabularies cover the entire space of gesture types of our multi-dimensional gesture classification (Fig. <ref type="figure" target="#fig_3">5</ref>) Fig. <ref type="figure">6</ref> Gestuno graphical conventions. We show the meaning of the arrows provided with the Gestuno flash cards Fig. <ref type="figure">7</ref> Gestuno flash card. We show an example of Gestuno flash cards Indian mudras are emblems or dance postures (we classified them as emblems because of their symbolic nature), whether surgeon signals are signals (rather than signs) and Chinese numbers signs (rather than signals). It depends more on the context and the interpretation. Finally, the same gesture is often part of several distinct lexicons. For instance, the "V" of victory emblem is also the letter "V" in the American sign language, the number "2" in number signs, and the "Kartarimukha" in Indian mudra. Note in passing that many gestures carry different meanings in different cultures and should be used with caution. Classically, the American hand-wave can be misinterpreted in Greece as a sign of hostility and curse (go to hell).</p><p>The 86 gesture lexicons, together with other data annotations, have been made available on the Internet. 2  -source: Origin of the information.</p><p>-url: URL, if downloaded from the Internet.</p><p>Only the first three items are always present, the others may be omitted. There is no prescribed order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data collection setting</head><p>Every application needs a specialized gesture vocabulary. If we want gesture recognition to become part of everyday life, we need gesture recognition machines, which easily get tailored to new gesture vocabularies. This is why the focus of the challenge is on "one-shot-learning of gestures, which means learning to recognize new categories of gestures from Fig. <ref type="figure">8</ref> Data collection. We recorded a large database of hand and arm gestures using the RGB+Depth camera Kinect TM , which was made freely available. The figure shows the user interface a single video clip of each gesture. We are portraying a single user in front of a fixed camera (Fig. <ref type="figure">8</ref>), interacting with a computer by performing gestures to -play a game, -remotely control appliances or robots, or -learn to perform gestures from an educational software.</p><p>The data are organized into batches comprising 100 samples of gestures from a small vocabulary of 8-12 gesture tokens (drawn from the lexicons described in the previous section), recorded in short sequences of 1-5 gestures. We wrote data collection software to facilitate recording data. Recording a data batch of 100 gestures took less than one hour. To stimulate the data donors to be efficient, we paid them a fixed amount regardless of the time it took to record a batch. The software was made publicly available so the challenge participants can collect more data if needed. We show in Fig. <ref type="figure">8a</ref> screen shot of the interface.</p><p>We followed a specific protocol to record the data. Each user recorded without stopping a full batch of 100 gestures, following a given script, which consisted of prescribed sequences of 1-5 gestures from a given gesture vocabulary (lexicon). In the development and validation data, some batches are using the same lexicon, but none of the lexicons used for final evaluation was used to record development or validation data. Every script started with one example of each gesture of the vocabulary, performed in isolation. For example, for the musical note gesture vocabulary, you would use ten gestures: Do Re Mi Fa Sol La T i Do2. Your script may look like: </p><formula xml:id="formula_1">T i Fa La Do2 Sol Do Mi Re ⎫ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎬ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎭ T</formula><formula xml:id="formula_2">⎫ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎬ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎭ T est examples</formula><p>The recording conditions were as follow:</p><p>1. Driver: We used the Microsoft SDK drivers (beta version). 2. Sampling rate: Videos were recorded at approximately ten frames per seconds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Depth image quality:</head><p>-We limited the distance between the camera and the background to reduce the range of depth and recorded usually in front of a wall, not outside or in front of a window. The wall was no more than 12 feet (3 m) away from the Kinect. This ensured that for most batches the depth accuracy was in the range of 50-800 levels. We normalized the depth values but provided a formula to reconstruct original values. Specifically, we subtracted the closest point over one data batch and divided by the depth range over one data batch. We then mapped the depth to 8-bit integers. The normalization factors were provided to the participants and can be downloaded with the data. A subset of the normalization factors are given in Table <ref type="table">8</ref>. The original depth values can be restored (approximately) as follows: Average the R,G, and B values to get a value v, then perform v/255*(MaxDepth-MinDepth)+MinDepth. -We placed the Kinect approximately 4 feet (1m20) away from the head of the subject (to avoid saturations of the depth sensor). -We placed the subject at least 1 foot away from the wall to get some depth contrast against the background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">RGB image quality:</head><p>-We ensured a good lighting.</p><p>-We ensured a good contrast between background and subject. -We used 8-bit of resolution (256 levels of red, blue and green).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Framing and space resolution:</head><p>-We generally framed the upper body including the head, shoulders and waist. For a experimental batches, we framed the entire body or occluded the left or right slide of the body that was not performing gestures. -For hand gestures, meant to be performed from close, the subject was sitting. For ample arm gestures meant to be seen from far, the subject was standing. -We ensured that the hand(s) performing the gesture is (are) always in the frame. We did not frame only the hand performing the gesture. -The image sizes are 320 x 240 pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Video format:</head><p>The original data were saved uncompressed in Matlab format, then compressed and converted to AVI (see details in the next section).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Gesture performance:</head><p>-The gestures were played continuously, but slightly spaced in time and punctuated by returning the hands to a resting position. -For some lexicons, it was unclear how to exactly perform the gestures. The subjects were instructed to choose their own interpretation but to perform the gesture patterns as consistently as possible and to make sure that the gesture patterns of a particular lexicon are sufficiently different from one another to be unambiguously distinguished by a human viewer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Data format:</head><p>-The original data were saved using a naming convention. Each"experiment" (batch) was named:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EXPNAME = &lt;FirstName&gt;_&lt;LastName&gt;_ &lt;LexiconName&gt;_&lt;Date&gt;</head><p>with the date in the format:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>YYYY_MM_DD_hh_mm_ss</head><p>For example:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Jerome_Noriot_ChineseNumbers_ 2011_05_29_01_28</head><p>-The data for an experiment were stored in a separate directory bearing the name of the experiment. The directory of an experiment contained 47 Matlab files Gxx.mat, xx=1:47, corresponding to the 47 lines of the script. Each Matlab file contains three variables, K, M, and fps. K is the depth recording, stores as a cell array of frames, M is a Matlab RGB movie, and fps is the frame rate (in frames per seconds). We hired 22 subjects who recorded data over a period of 2 months. The subjects were asked to sign a data release form. The data collection yielded over 60,000 gestures. The data were made available on the website of the challenge. <ref type="foot" target="#foot_3">4</ref>We made available the data collection software, which can be downloaded from http://gesture.chalearn.org/data/ demo-kit. We wrote a detailed documentation <ref type="bibr" target="#b21">[23]</ref>. This document describes a Matlab library, which allows user to quickly put together gesture recognition demonstrations and to collect gesture data. The library includes two parts:</p><p>1. GROP (Gesture Recognition Object Package): An objectoriented Matlab interface to encapsulate gesture recog-nizers and allow users to build compound models using basing preprocessing, temporal segmentation, and recognition objects. 2. GDC (Gesture Data Collection): A Graphical User Interface (GUI), which interfaces regular webcams and Kinect to either record data or create gesture recognition demonstrations.</p><p>GROP includes baseline methods, which were provided for the ChaLearn gesture challenge while GCD allows users to record more data of the type used in the challenge. We also provide a simple example of game, which can serve as a basis to construct gesture recognition demonstrations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Data description</head><p>We formatted for initial release a dataset including:</p><p>-50,000 gestures recorded with the Kinect camera, including RGB and depth videos, -with image sizes 240 x 320 pixels, -at 10 frames per second, -recorded by 20 different users, -grouped in 500 batches of 100 gestures, -each batch including 47 sequences of 1-5 gestures drawn from various small gesture vocabularies of 8-12 gestures, -from over 30 different gesture vocabularies. These data included 480 "development" batches fully labeled and 20 "validation" batches for which only one label of each gesture pattern was provided (to train on the one-shotlearning task). We reserved 40 batches for the final evaluation of round 1 and round 2. For the initial release, we purposely chose data batches that were hard and span the whole rage of difficulties of the dataset. The final evaluation batches were slightly easier to avoid generating frustrations among the challenge participants. This brings the total number of gestures released to 54, 000. We have now released all the labels.</p><p>The data are available in two formats: A lossy compressed AVI format (total &lt; 5 GB) and a quasi-lossless AVI format (total &lt; 30 GB). The data in quasi-lossless format are available on multiple data mirrors. The quasi-lossless compression was performed with FFMPEG<ref type="foot" target="#foot_4">5</ref> , using: ffmpeg -i original.avi -sameq compressed.avi</p><p>The data are organized into directories: devel01 ... devel480 valid01 ... valid20 final01 ... final40</p><p>The identity of the lexicons, the users, and the date and time of recording were not provided to the challenge participants. However, some of the participants found that the header of the AVI movies included the original directory name. We released the information contained in the directory names before the end of the challenge so no unfair advantage could be derived from it. This information corresponds to the columns Lexicon, UserID, and Date in Tables <ref type="table" target="#tab_7">7</ref> and<ref type="table">8</ref>.</p><p>Each directory contains label files in CSV format, e.g., devel01_train.csv devel01_test.csv.</p><p>The first column includes the sample IDs in the format &lt;batch_name&gt;_&lt;sample_number&gt; and the second column contains a space-separated list of gesture labels that are numbers 1:N, N being the size of the lexicon. Hence the true identity of the gestures is concealed.</p><p>The training labels correspond to the first N gesture pattern examples recorded in isolation, one per class. The test labels were omitted for the valid and final batches. They have been made available separately since the challenge ended.</p><p>In the data directory, each data entry corresponding to one line in the script is associated with 2 files K _x x.avi and M_x x.avi, x x = 1 : 47, where the K files are movies made of the depth recording rendered as gray levels and the M files are RGB movies.</p><p>We reserved data batches that we did not release for eventual post-challenge verifications. Those batches were not needed in the end because we could verify the entries of the participants using submitted code without problem. We kept them for possible upcoming events and did not release them.</p><p>By design, the dataset presents a number of difficulties, which are summarized in Table <ref type="table" target="#tab_3">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Data annotations</head><p>The data were manually annotated at University of Texas at Arlington by the staff of Vassilis Athitsos. Two types of annotations were performed: The datasets devel0-20 and valid 0-20 were fully annotated with temporal segmentation: We manually determined the starting frame and the end frame of each gesture in the videos by watching them. We show in Fig. <ref type="figure">9</ref> the human segmentation overlaid with the amount of motion in the lower part of a sample video (average absolute differences of consecutive depth frames). Because the hand returns to a resting position between each gesture, there are peaks of hand motion near the segmentation points.</p><p>We also manually determined the position of head, shoulders, elbows and hands in over 3,000 movie frames selected from the devel01-20 batches.</p><p>Finally, we provided average alignment parameters for every batch devel01-20, valid01-20, final01-40. We use four parameters scale_x, scale_y, translation_x, translation_y to represent the geometrical transformation (scaling and translation) necessary to align the depth image to the RGB image. See Fig. <ref type="figure" target="#fig_0">10</ref> for an explanation of how to use these parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Baseline methods (the GROP package)</head><p>To help the participants, we provided a library of Matlab functions performing various image and video processing functions. We also provided examples of end-to-end systems to address the task of the challenge. This software has been made publicly available. <ref type="foot" target="#foot_5">6</ref> See Fig. <ref type="figure" target="#fig_0">11</ref>.</p><p>There are essentially two approaches that can be taken for data representation <ref type="bibr" target="#b34">[36]</ref>: Spatio-temporal features: Extract low level features based of local gradients, edges and temporal differences. Typical examples include histograms of oriented gradients (HOG) <ref type="bibr" target="#b10">[12]</ref> and histograms of optical flow (HOF) <ref type="bibr" target="#b11">[13]</ref>. Originally developed for RGB images and extract features from {x, y} coordinates, these methods can be easily generalized to include an additional depth coordinate z. Because we use a fixed camera and a user who moves relatively little with respect to the camera, except for executing gestures, one may consider using a vector of features extracted at fixed positions. However, it may be preferable to provide some invariance to translation and scale. This can be achieved, for instance, with a "bag-ofwords" type of representation, inspired by text processing. This approach is often taken by the researchers working on activity recognition. An example is the bag of STIP features <ref type="bibr" target="#b28">[30]</ref>. The method consists in identifying interest points in the image (usually points of high motion) and creating a feature vector that is a histogram counting the number of occurrences of a given feature anywhere in the image. We provide code to extract HOG, HOF, and STIP features. Body parts: Another approach is to identify the position of body parts (head, shoulders, elbows, hands). One popular method was introduced by Microsoft with their skeleton tracker, which is part of their SDK <ref type="bibr" target="#b39">[41]</ref>. However, at the time we recorded the data, the Microsoft skeleton tracker for partially occluded subjects (upper body tracker) was not available. Extracting body parts from pre-recorded data at a low sampling rate turned out to be very difficult. We provide code developed at ETH Zurich <ref type="bibr" target="#b15">[17]</ref>, but it does not give satisfactory results.</p><p>For temporal segmentation and sequence alignment, the canonical method is dynamic time warping (DTW) and the Viterbi algorithm <ref type="bibr" target="#b42">[44]</ref>. The method can be slightly generalized to accommodate one-shot-learning HMM/CRF to create models based on the gesture templates plus a transition state between gestures. Some approaches require separating the gesture sequences into isolated gestures first, which is relatively easy in this dataset because the users return their hands to a resting position between gestures. The first frame can be used as a template for the transition state. Once you have a vector representation of isolated gestures, to perform "one-shot-learning", the simplest method is the nearest neighbor method. But you may also look for the best match between temporal sequences directly without isolating gestures using dynamic time warping. We provided the participants with a DTW function allowing to implement transition states. Based on this function, we gave an example of method performing segmentation, using a coarse 3 x 3 grid to obtain a nine-feature motion representation derived from differences between consecutive depth frames. This simple method is not perfect, but it was used by two of the winners in round 2 (it was not available in round 1).</p><p>We provided the participants with sample code written in object-oriented Matlab . The choice of Matlab is motivated by the fact that it is the platform predominantly used by the participants of our past challenges and it is a very convenient scripting language with powerful graphics. Most universities have Matlab available for their students. The code includes a main script, which loads data, processes them with a simple recognizer, and produces prediction results in the format prescribed to submit results on the challenge platform. It also includes a data browser allowing the user to visualize data. It is easy to derive a class form the basic recognizer template and reuse the code to process data. Finally, we provided the scoring function that is used to assess performance.</p><p>To create sample submissions and generate simple features that the participants could use to get started, we distributed a very simple template of recognizer with trivial code. We encouraged people to make entries by challenging them to outperform the baseline method. The first ten participants that achieved this goal received a free Kinect TM . In round 2, we improved the baseline method <ref type="bibr" target="#b16">[18,</ref><ref type="bibr" target="#b17">19]</ref>. Consequently, the whole distribution of participant performances was shifted upwards. Many top ranking participants made use of some functions found in our sample code and indicated that this was really helpful.</p><p>In addition, we estimated human performance of recognition by having a person visualize and recognize all the videos of the validation set. There is still a large gap between the best entrant (7 % error) and human performance (2 % error).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Baseline results</head><p>The results of the challenge provide excellent baseline results for further research in gesture recognition using the ChaLearn gesture database. The first round of the challenge attracted 50 teams and the second round 35 teams. In total, 935 entries were made. This an unprecedented level of participation for a computer vision challenge requiring very specialized skills. For comparison, the popular Pascal2 VOC challenges <ref type="bibr" target="#b3">[4]</ref> attracted in 2011 between and 1 and 19 participants.</p><p>The results of the top ranking participants were checked by the organizers who reproduced their results using the code provided by the participants BEFORE they had access to the final evaluation data. All of them passed successfully the verification process. These results are shown in Tables <ref type="table" target="#tab_4">3</ref> and<ref type="table" target="#tab_5">4</ref>. A full analysis of these results can be found in <ref type="bibr" target="#b22">[24,</ref><ref type="bibr" target="#b23">25]</ref>.</p><p>To test the robustness of the participants' methods under small body horizontal translations of the users, we created a semi-artificial dataset from 20 batches, not coinciding with the sets of batches used for validation, and final testing because most of those batches included dynamic gestures covering the entire image area, therefore not leaving room for translations. We briefly describe how the dataset was created:</p><p>-We went back to the original data and selected batches including a large background area in which no gesture was taking place. This constitutes the utran data. -We visually inspected the training videos to identify a cropping area including every important gesture parts. Once selected, the cropping size was fixed for the given batch. The aspect ratio was always 4:3 (width:height), similar to the challenge data. -For every test video in the batch, using the same cropping size, a different horizontal translation was applied. This was done by visual inspection to make sure no important gesture part was occluded. No vertical translation was applied. This constitutes the tran data. -Similarly, we applied various scaling factors to generate the scaled data.</p><p>We selected 20 batches for these experiments, not coinciding with the sets of batches used for validation, and final testing because most of those batches included dynamic gestures covering the entire image area, therefore not leaving room for translations. The batches used are harder on average than those used for the challenge final evaluation, in particular, because they include more static posture recognition. We ran experiments with the un-translated batches (utran) and with the translated batches (tran) and the scaled batches (scaled).</p><p>The results summarized in Table <ref type="table">5</ref> are analyzed in reference <ref type="bibr" target="#b22">[24]</ref>. It is worth noting that the methods of the winner of both rounds (Alfnie) are robust against translation and scale while the second ranking methods (Pennect and Turtle Tamers) exhibit important performance degradation between utran and tran and scaled. This is not so surprising considering that both Pennect and Turtle Tamers use features rigidly positioned on image feature maps. Methods robust against translation include those of Jun wan <ref type="bibr" target="#b45">[47]</ref> and Immortals/Manavender (this is the same author under two different pseudonyms for round 1 and round 2) <ref type="bibr" target="#b30">[32]</ref>. Their representations are based on a bag of visual words, inspired by techniques used in action recognition <ref type="bibr" target="#b28">[30]</ref>. Such representations are inherently shift invariant. The slight performance loss in translated data may be due to partial occlusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion and further work</head><p>We made freely available a large database of gesture videos that was used in a one-shot-learning challenge. Much research progress must still be made to reduce the gap between the best entrant (7 % error) and human performance (2 % error). Our analysis of the errors made reveals that improvements should be made to recognize static postures, particularly those in which details of the finger position are important. These improvements could be made by developing better features, possibly features learned from the large development dataset, in the spirit of "transfer learning".</p><p>The dataset also lends itself to defining new tasks. The development dataset, which was recorded using 30 lexicons, includes at least 11 batches recorded with the same lexicon and different users. Thus, it is possible to define small vocabulary user-independent tasks. The same users also recorded up to 30 different lexicons. hence it is possible to define large vocabulary user-dependent tasks. Finally, some lexicons were recorded multiple times by the same user, which makes it possible to define larger training sets and move away from one-shot-learning.</p><p>We made available all the gesture labels, the identity of the gesture lexicons, manual data annotations, and sample code to facilitate further research. Future directions of research may include exploiting the domain of the lexicon.</p><p>Our effort has been followed by the collection of a new multi-media dataset recorded with Kinect, including RGB, depth, skeleton coordinates, and speech, which was used in the ICMI 2013 challenge. This new dataset offers further possibilities of user-independent medium-size vocabulary gesture recognition <ref type="bibr" target="#b18">[20]</ref>.</p><p>Acknowledgments This challenge was organized by ChaLearn http:// chalearn.org whose directors are gratefully acknowledged. The submission website was hosted by Kaggle http://kaggle.com and we thank Ben Hamner for his wonderful support. Our sponsors include Microsoft (Kinect for Xbox 360) and Texas Instrument who donated prizes. We are very grateful to Alex Kipman and Laura Massey at Microsoft and to Branislav Kisacanin at Texas Instrument who made this possible. We also thank the committee members and participants of the CVPR 2011, CVPR 2012, and ICPR 2012 gesture recognition workshop, the judges of the demonstration competitions hosted in conjunction with CVPR 2012 and ICPR 2012 and the Pascal2 reviewers who made valuable suggestions. We are particularly grateful to Richard Bowden, Philippe Dreuw, Ivan Laptev, Jitendra Malik, Greg Mori, and Christian Vogler, who provided us with useful guidance in the design of the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1">Results by challenge participant</head><p>We used the code provided by 15 top ranking participants in both challenge rounds to compute performances on the validation and final evaluation sets (Table <ref type="table">5</ref>). We also provide results on 20 other batches selected for our translation experiments. Untranslated data are referred to as "utran" and translated data as "tran". Details on the methods employed by the participants are found in reference <ref type="bibr" target="#b22">[24]</ref> and on the website of the challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2">Development data lexicons</head><p>The development data were recorded using a subset of thirty lexicons (Table <ref type="table" target="#tab_6">6</ref>). They were recorded at least 11 times each by different users. We list in Table <ref type="table" target="#tab_7">7</ref> the lexicons used for validation and final evaluation data. Note that some validation lexicons are also present in development data but that the final evaluation data include only new lexicons found in no other sets. </p><note type="other">Num ChineseNumbers 25 MusicNotes 24 TaxiSouthAfrica 24 ItalianGestures 22 DivingSignals4 21 Mudra1 21 HelicopterSignals 19 Mudra2 19 SurgeonSignals 19 GestunoDisaster 18 CraneHandSignals 17 GestunoColors 17 SwatHandSignals1 17 DivingSignals3 16 RefereeVolleyballSignals1 16 DivingSignals2 15 GestunoSmallAnimals 15 RefereeVolleyballSignals2 15 DivingSignals1 14 GangHandSignals1 14 GestunoLandscape 14 GestunoTopography 14 CanadaAviationGroundCirculation1 13 GangHandSignals2 13 MotorcycleSignals 13 RefereeWrestlingSignals1 13 TractorOperationSignals 13 CanadaAviationGroundCirculation2 12 RefereeWrestlingSignals2 11 SwatHandSignals2 11</note><p>"Num" is the number of times recorded</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.3">Results by data batch</head><p>We show in Table <ref type="table" target="#tab_7">7</ref> the performances by batch. We computed the best and average performance over 15 top ranking participants in round 1 and 2: Alfnie1, Alfnie2, Bal-azsGodeny, HITCS, Immortals, Joewan, Manavender, One-MillionMonkeys, Pennect, SkyNet, TurtleTamers, Vigilant, WayneZhang, XiaoZhuWudi, and Zonga.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.4">Depth parameters</head><p>We also provide the parameters necessary to reconstruct the original depth data from normalized values (Table <ref type="table">8</ref>). </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1</head><label>1</label><figDesc>Fig.1Videos recorded with Kinect. Users performed gestures in front of a fixed Kinect TM camera, which recorded both a regular RGB image and a depth image (rendered as gray levels here)</figDesc><graphic coords="2,203.26,56.84,340.12,216.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 Fig. 3</head><label>23</label><figDesc>Fig. 2 Gesture vocabulary. The figure shows examples of gesture lexicons: Music notes, Chinese numbers, and helicopter signals</figDesc><graphic coords="4,203.26,525.98,340.36,186.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4</head><label>4</label><figDesc>Fig. 4 Dynamic gestures. Color rendering of depth images for dynamic gestures consisting of a sequence of body postures. We show two selected frames on top of one another. From left to right: finger ges-</figDesc><graphic coords="6,87.64,56.72,419.56,159.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5</head><label>5</label><figDesc>Fig.<ref type="bibr" target="#b4">5</ref> Types of gestures. We created a classification of gesture types according to function, defined by three complementary axes: communication, expression and action. We selected 86 gesture vocabularies, including Italian gestures, Indian mudras, sign language for the deaf, diving signals, pantomimes, and body language</figDesc><graphic coords="7,203.26,56.15,340.36,268.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Each gesture 2 http://gesture.chalearn.org/data/data-annotations token is associated with an image in a standard format such as jpeg and a text file describing its attributes and source. The attributes include: -Vocabulary type: Activity, Body Language, Dance, Emblems, Gesticulations, Illustrators, Pantomine, or Sign language, Signals. -Name: Alphanumeric upper/lowercase gesture name (does not necessarily need to be related to its meaning). -Bodypart: Principal body part(s) used: one hand / two hands / head / hands and head / shoulders. -description: A string describing the performance of the gesture. -comment: Any comment, e.g., ambiguity with another gesture or existence of multiple gesture variants. -meaning: Meaning conveyed (should be in the original language of the country or origin of the gesture). -english: English translation of the meaning (if needed).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>3 -</head><label>3</label><figDesc>Each video corresponds to the recording of a sequence of 1-5 gestures as given by one line of the script. -The directory also contains four information files: EXPNAME.vocabulary: The names of the N gestures in the "vocabulary". EXPNAME.labels: Script of 47 lines, each line corresponding to the truth values of the gestures recorded in the 47 video files. Each number represents a gesture in the vocabulary. EXPNAME.hand: Dominant hand [right/left]. EXPNAME.mirror: A list of 0/1 numbers indicating whether the mirror image of a particular gesture in the lexicon was executed rather than the original.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9 Fig. 11</head><label>911</label><figDesc>Fig. 9 Data annotations. For the temporal segmentations, we represent the amount of motion in the lower part of an example of video as a function of time. We show in green the start of the gesture and in red the end of the gesture. The gesture labels are displayed in each segment. For the body parts, the colors code the head, shoulders, elbows, and hands. A cross indicates the precise position of a shoulder or elbow, a rectangle frames the head of the hands, a circle indicates the probable position of an occluded part. For the RGB/depth image alignment, we show how the body contour extracted from the depth image is overlaid with the RGB image</figDesc><graphic coords="15,203.26,56.21,340.36,212.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="11,203.26,56.63,340.36,268.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>signals, we found numerous and diverse examples of gesture vocabularies on the Internet. They include professional vocabularies of signs used to communicate from far or in presence of noise (such as aviation, helicopter, train, car, and other machinery marshalling signals and various sports referee signals) or to communicate under water (diving signals), and other signals to communicate from far or in the presence of noise (taxi South African signs to hail a cab, motorcycle and water skiing signals). They also include signs to communicate silently (surgeon signals, gang hand signals, and military and police swat signals).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc>Easy and challenging aspects of the data</figDesc><table><row><cell>Easy aspects</cell></row><row><cell>Fixed camera</cell></row><row><cell>Availability of depth data</cell></row><row><cell>Within a batch: single user, homogeneous recording conditions,</cell></row><row><cell>small vocabulary</cell></row><row><cell>Gestures separated by returning to a resting position</cell></row><row><cell>Gestures performed mostly by arms and hands</cell></row><row><cell>Camera framing upper body (some exceptions)</cell></row><row><cell>Challenging aspects</cell></row><row><cell>Within a batch: only one labeled example of each gesture</cell></row><row><cell>Skeleton tracking data not provided</cell></row><row><cell>Between batches: variations in background, clothing, skin color,</cell></row><row><cell>lighting, temperature, resolution</cell></row><row><cell>Some errors or omissions in performing gestures</cell></row><row><cell>Some users are less skilled than others</cell></row><row><cell>Some parts of the body may be occluded</cell></row><row><cell>-Temporal segmentation (beginning and end of every ges-</cell></row><row><cell>ture)</cell></row><row><cell>-Body part annotation</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc>Results of round 1</figDesc><table><row><cell></cell><cell>Team</cell><cell>Public score</cell><cell>Private score</cell><cell>For comparison score</cell></row><row><cell></cell><cell></cell><cell>on validation set</cell><cell>on final set #1</cell><cell>on final set #2</cell></row><row><cell></cell><cell>Alfnie</cell><cell>0.1426</cell><cell>0.0996</cell><cell>0.0915</cell></row><row><cell></cell><cell>Pennect</cell><cell>0.1797</cell><cell>0.1652</cell><cell>0.1231</cell></row><row><cell></cell><cell>OneMillionMonkeys</cell><cell>0.2697</cell><cell>0.1685</cell><cell>0.1819</cell></row><row><cell></cell><cell>Immortals</cell><cell>0.2543</cell><cell>0.1846</cell><cell>0.1853</cell></row><row><cell>In round 1 the baseline method was a simple template matching</cell><cell>Zonga Balazs Godeny</cell><cell>0.2714 0.2637</cell><cell>0.2303 0.2314</cell><cell>0.2190 0.2679</cell></row><row><cell>method (see text). For</cell><cell>SkyNet</cell><cell>0.2825</cell><cell>0.2330</cell><cell>0.1841</cell></row><row><cell>comparison, we show the results on the final set number 2 not available in round 1</cell><cell>XiaoZhuwWudi Baseline method 1</cell><cell>0.2930 0.5976</cell><cell>0.2564 0.6251</cell><cell>0.2607 0.5646</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc>Results of round 2</figDesc><table><row><cell></cell><cell>Team</cell><cell>Public score</cell><cell>For comparison score</cell><cell>Private score</cell></row><row><cell></cell><cell></cell><cell>on validation set</cell><cell>on final set #1</cell><cell>on final set #2</cell></row><row><cell></cell><cell>Alfnie</cell><cell>0.0951</cell><cell>0.0734</cell><cell>0.0710</cell></row><row><cell></cell><cell>Turtle Tamers</cell><cell>0.2001</cell><cell>0.1702</cell><cell>0.1098</cell></row><row><cell></cell><cell>Joewan</cell><cell>0.1669</cell><cell>0.1680</cell><cell>0.1448</cell></row><row><cell></cell><cell>Wayne Zhang</cell><cell>0.2814</cell><cell>0.2303</cell><cell>0.1846</cell></row><row><cell></cell><cell>Manavender</cell><cell>0.2310</cell><cell>0.2163</cell><cell>0.1608</cell></row><row><cell></cell><cell>HIT CS</cell><cell>0.1763</cell><cell>0.2825</cell><cell>0.2008</cell></row><row><cell>In round 2, the baseline method</cell><cell>Vigilant</cell><cell>0.3090</cell><cell>0.2809</cell><cell>0.2235</cell></row><row><cell>was the "Principal Motion" method (see text)</cell><cell>Baseline method 2</cell><cell>0.3814</cell><cell>0.2997</cell><cell>0.3172</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6</head><label>6</label><figDesc>Lexicons recorded in development dataLexicon</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7</head><label>7</label><figDesc>Results by batch. This Support Vector Machines (SVM), a machine learning technique, which has become a textbook method. She is also the primary inventor of SVM-RFE, a variable selection technique based on SVM. The SVM-RFE paper has thousands of citations and is often used as a reference method against which new feature selection methods are benchmarked. She also authored a seminal paper on feature selection that received thousands of citations. She organized many challenges in Machine Learning over the past few years supported by the EU network Pascal2, NSF, and DARPA, with prizes sponsored by Microsoft, Google, and Texas Instrument. Isabelle Guyon holds a Ph.D. degree in Physical Sciences from the University Pierre and Marie Curie, Paris, France. She is president of Chalearn, a non-profit dedicated to organizing challenges, vice-president of the Unipen foundation, adjunct professor at New-York University, action editor of the Journal of Machine Learning Research, and editor of the Challenges in Machine Learning book series of Microtome.Vassilis Athitsos is anAssociate Professor of Computer Science and Engineering in the University of Texas at Arlington. He received the BS degree in Mathematics from the University of Chicago in 1995, the MS degree in Computer Science from the University of Chicago in 1997, and the PhD degree in Computer Science from Boston University in 2006. In 2005-2006, he worked as a researcher at Siemens Corporate Research, developing methods for database-guided medical image analysis. In 2006-2007, he was a postdoctoral research associate at the Computer Science department in Boston University. Dr. Athitsos joined the faculty of the CSE department in UT Arlington in 2007.His research interests include computer vision, machine learning, and data mining, with recent work focusing primarily on gesture and sign language recognition, human motion analysis, and efficient similaritybased retrieval in image, video and multimedia databases.Pat Jangyodsuk is a Ph.D. candidate at University of Texas at Arlington, working on computer vision/machine learning problems such as gesture recognition, similarity-based retrieval in images.Hugo JairEscalante is Associate Researcher at the Department of Computational Sciences of the National Institute of Astrophysics, Optics and Electronics; previously he was assistant professor of the Graduate Program in Systems Engineering (PISIS) in Universidad Autónoma de Nuevo León. Dr. Escalante received the B.S. degree from the Universidad Autónoma de Puebla in 2004 and the Ph.D. and M.S. degrees from INAOE in 2010 and 2006, respectively. Hugo Escalante has been a researcher of the Sistema Nacional de Investigadores (SNI) for the period 2011-2013.</figDesc><table><row><cell>table lists for validation and</cell><cell>Batch</cell><cell>Lexicon</cell><cell>UserID</cell><cell>Best</cell><cell>Mean</cell><cell>Std</cell></row><row><cell>final evaluation data batches</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>their lexicon, identity of the</cell><cell>valid01</cell><cell>MotorcycleSignals</cell><cell>C</cell><cell>0.25</cell><cell>0.43</cell><cell>0.10</cell></row><row><cell>subject (user) that recorded the</cell><cell>valid02</cell><cell>HelicopterSignals</cell><cell>R</cell><cell>0.00</cell><cell>0.06</cell><cell>0.06</cell></row><row><cell>data, and performance of recognition of 15 top ranking</cell><cell>valid03</cell><cell>DivingSignals2</cell><cell>Z</cell><cell>0.12</cell><cell>0.38</cell><cell>0.13</cell></row><row><cell>participants in round 1 and 2.</cell><cell>valid04</cell><cell>SwatHandSignals2</cell><cell>B</cell><cell>0.03</cell><cell>0.14</cell><cell>0.10</cell></row><row><cell>The performance score is the</cell><cell>valid05</cell><cell>TractorOperationSignals</cell><cell>D</cell><cell>0.00</cell><cell>0.14</cell><cell>0.10</cell></row><row><cell>average generalized Levenshtein distance, which is analogous to an error rate. Best is the lowest</cell><cell>valid06 valid07</cell><cell>DanceAerobics RefereeWrestlingSignals2</cell><cell>S H</cell><cell>0.09 0.11</cell><cell>0.26 0.25</cell><cell>0.09 0.09</cell></row><row><cell>score, Mean is the average score</cell><cell>valid08</cell><cell>ActionObjects</cell><cell>F</cell><cell>0.03</cell><cell>0.12</cell><cell>0.09</cell></row><row><cell>and Std is the standard deviation</cell><cell>valid09</cell><cell>ChineseNumbers</cell><cell>A</cell><cell>0.15</cell><cell>0.50</cell><cell>0.17</cell></row><row><cell></cell><cell>valid10</cell><cell>PantomimeObjects</cell><cell>K</cell><cell>0.00</cell><cell>0.07</cell><cell>0.05</cell></row><row><cell></cell><cell>valid11</cell><cell>McNeillGesticulation2</cell><cell>A</cell><cell>0.18</cell><cell>0.24</cell><cell>0.05</cell></row><row><cell></cell><cell>valid12</cell><cell>GestunoColors</cell><cell>W</cell><cell>0.10</cell><cell>0.26</cell><cell>0.07</cell></row><row><cell></cell><cell>valid13</cell><cell>GestunoSmallAnimals</cell><cell>K</cell><cell>0.03</cell><cell>0.16</cell><cell>0.12</cell></row><row><cell></cell><cell>valid14</cell><cell>Mudra1</cell><cell>T</cell><cell>0.30</cell><cell>0.62</cell><cell>0.14</cell></row><row><cell></cell><cell>valid15</cell><cell>ItalianGestures</cell><cell>O</cell><cell>0.15</cell><cell>0.30</cell><cell>0.11</cell></row><row><cell></cell><cell>valid16</cell><cell>RefereeVolleyballSignals2</cell><cell>L</cell><cell>0.01</cell><cell>0.06</cell><cell>0.05</cell></row><row><cell></cell><cell>valid17</cell><cell>BodyLanguageDominance</cell><cell>N</cell><cell>0.00</cell><cell>0.07</cell><cell>0.06</cell></row><row><cell></cell><cell>valid18</cell><cell>MusicNotes</cell><cell>O</cell><cell>0.03</cell><cell>0.35</cell><cell>0.18</cell></row><row><cell></cell><cell>valid19</cell><cell>TaxiSouthAfrica</cell><cell>M</cell><cell>0.06</cell><cell>0.28</cell><cell>0.13</cell></row><row><cell></cell><cell>valid20</cell><cell>CanadaAviationGroundCirculation2</cell><cell>R</cell><cell>0.08</cell><cell>0.19</cell><cell>0.10</cell></row><row><cell></cell><cell>final01</cell><cell>DancePostures</cell><cell>A</cell><cell>0.01</cell><cell>0.13</cell><cell>0.07</cell></row><row><cell></cell><cell>final02</cell><cell>KendonGesticulation1</cell><cell>A</cell><cell>0.01</cell><cell>0.06</cell><cell>0.04</cell></row><row><cell></cell><cell>final03</cell><cell>GestunoWildAnimals</cell><cell>A</cell><cell>0.00</cell><cell>0.13</cell><cell>0.08</cell></row><row><cell></cell><cell>final04</cell><cell>McNeillGesticulation1</cell><cell>A</cell><cell>0.12</cell><cell>0.40</cell><cell>0.17</cell></row><row><cell></cell><cell>final05</cell><cell>CommonEmblems</cell><cell>E</cell><cell>0.10</cell><cell>0.38</cell><cell>0.14</cell></row><row><cell></cell><cell>final06</cell><cell>MilitaryInfantrySignals1</cell><cell>B</cell><cell>0.02</cell><cell>0.21</cell><cell>0.13</cell></row><row><cell></cell><cell>final07</cell><cell>CanadaHelicopterSignals</cell><cell>L</cell><cell>0.01</cell><cell>0.14</cell><cell>0.10</cell></row><row><cell></cell><cell>final08</cell><cell>WaterSkiingSignals</cell><cell>L</cell><cell>0.08</cell><cell>0.37</cell><cell>0.13</cell></row><row><cell></cell><cell>final09</cell><cell>PantomimeMusic</cell><cell>K</cell><cell>0.02</cell><cell>0.14</cell><cell>0.08</cell></row><row><cell></cell><cell>final10</cell><cell>GestunoDescriptive</cell><cell>K</cell><cell>0.00</cell><cell>0.04</cell><cell>0.04</cell></row><row><cell></cell><cell>final11</cell><cell>BabySignsAction</cell><cell>Z</cell><cell>0.15</cell><cell>0.27</cell><cell>0.10</cell></row><row><cell></cell><cell>final12</cell><cell>BodyLanguageHonesty</cell><cell>F</cell><cell>0.04</cell><cell>0.21</cell><cell>0.10</cell></row><row><cell></cell><cell>final13</cell><cell>RefereeJudoSignals</cell><cell>T</cell><cell>0.02</cell><cell>0.23</cell><cell>0.11</cell></row><row><cell></cell><cell>final14</cell><cell>GestunoPerception</cell><cell>H</cell><cell>0.06</cell><cell>0.17</cell><cell>0.14</cell></row><row><cell></cell><cell>final15</cell><cell>CommonIllustrators</cell><cell>D</cell><cell>0.02</cell><cell>0.17</cell><cell>0.06</cell></row><row><cell></cell><cell>final16</cell><cell>ScubaSignNumbers</cell><cell>D</cell><cell>0.31</cell><cell>0.51</cell><cell>0.11</cell></row><row><cell></cell><cell>final17</cell><cell>ItalianGestures3</cell><cell>D</cell><cell>0.00</cell><cell>0.11</cell><cell>0.06</cell></row><row><cell></cell><cell>final18</cell><cell>RefereeHockeySignals</cell><cell>N</cell><cell>0.00</cell><cell>0.05</cell><cell>0.03</cell></row><row><cell></cell><cell>final19</cell><cell>ForkLiftSignals1</cell><cell>N</cell><cell>0.12</cell><cell>0.21</cell><cell>0.07</cell></row><row><cell></cell><cell>final20</cell><cell>GestunoQualities</cell><cell>N</cell><cell>0.01</cell><cell>0.06</cell><cell>0.04</cell></row><row><cell></cell><cell>final21</cell><cell>GestunoGeography</cell><cell>A</cell><cell>0.10</cell><cell>0.21</cell><cell>0.07</cell></row><row><cell></cell><cell>final22</cell><cell>ICAOaircraftMarshallingSignals</cell><cell>A</cell><cell>0.00</cell><cell>0.05</cell><cell>0.06</cell></row><row><cell></cell><cell>final23</cell><cell>KendonGesticulation2</cell><cell>A</cell><cell>0.10</cell><cell>0.17</cell><cell>0.06</cell></row><row><cell></cell><cell>final24</cell><cell>BabySignsFeelings</cell><cell>D</cell><cell>0.13</cell><cell>0.32</cell><cell>0.12</cell></row><row><cell></cell><cell>final25</cell><cell>CommonOpinionGestures</cell><cell>D</cell><cell>0.01</cell><cell>0.15</cell><cell>0.08</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>For round 1: http://www.kaggle.com/c/GestureChallenge. For round</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>2: http://www.kaggle.com/c/GestureChallenge2.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>For ease of visualization, earlier experiments were recorded in a different format: depth encoded as gray levels and RGB images were concatenated vertically and stored as a single Matlab movie. However, we later realized that we were loosing depth resolution for some videos because Matlab movies used only 8 bits of resolution (256 levels) and the depth resolution of our videos attained sometimes more than 1,000. Hence, we recorded later batches using cell arrays for</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>K.<ref type="bibr" target="#b3">4</ref> http://gesture.chalearn.org/data.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>http://ffmpeg.org/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>http://gesture.chalearn.org/data/sample-code.</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This effort was initiated by the DARPA Deep Learning program and was supported by the US National Science Foundation (NSF) under grants ECCS 1128436 and ECCS 1128296, the EU Pascal2 network of excellence and the Challenges in Machine Learning foundation. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the funding agencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Guyon (B)</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="http://www.aimlanguagelearning.com/" />
		<title level="m">Accelerative Integrated Method (AIM) foreign language teaching methodology</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<ptr target="http://www.cvpapers.com/datasets.html" />
		<title level="m">Computer vision datasets on the web</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Imageclef-the clef cross language image retrieval track</title>
		<ptr target="http://www.imageclef.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<ptr target="http://pascallin.ecs.soton.ac.uk/challenges/VOC/" />
		<title level="m">The Pascal visual object classes homepage</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A unified framework for gesture recognition and spatiotemporal gesture segmentation</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><surname>Athitsos</surname></persName>
		</author>
		<author>
			<persName><surname>Vassilis</surname></persName>
		</author>
		<author>
			<persName><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stan</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Patt. Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1685" to="1699" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Teach your baby to sign: an illustrated guide to simple sign language for babies</title>
		<author>
			<persName><forename type="first">M</forename><surname>Beyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Fair Winds Press</publisher>
			<pubPlace>Minneapolis</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Collection and curation of a large reference dataset for activity recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Calatroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roggen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tröster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Systems, Man, and Cybernetics (SMC), 2011 IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="30" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Mudras of India: a comprehensive guide to the hand gestures of yoga and Indian dance</title>
		<author>
			<persName><forename type="first">C</forename><surname>Carroll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Carroll</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Jessica Kingsley Publishers</publisher>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The opportunity challenge: A benchmark database for on-body sensor-based activity recognition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Chavarriaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sagha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Calatroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Tejaswi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tröster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>José Del Millán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roggen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Patt. Recogn. Lett</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The standard course of lessons &amp; exercises in the Tonic Sol-Fa Method of teaching music: (Founded on Miss Glover&apos;s Scheme for Rendering Psalmody Congregational</title>
		<author>
			<persName><forename type="first">J</forename><surname>Curwen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>A.D. 1835.).. Nabu Press</publisher>
			<pubPlace>Charleston</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page" from="886" to="893" />
			<date type="published" when="2005">2005</date>
			<pubPlace>Providence</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Human detection using oriented histograms of flow and appearance</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th European conference on Computer Vision-Volume Part II. ECCV&apos;06</title>
		<meeting>the 9th European conference on Computer Vision-Volume Part II. ECCV&apos;06<address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="428" to="441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Guide to the carnegie mellon university multimodal activity (cmu-mmac) database</title>
		<author>
			<persName><forename type="first">F</forename><surname>De La Torre Frade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Hodgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Bargteil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xavier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Justin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Collado I Castells</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Beltran</surname></persName>
		</author>
		<idno>CMU-RI-TR-08-22</idno>
	</analytic>
	<monogr>
		<title level="j">Robotics Institute</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<pubPlace>Pittsburgh</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Ima-geNet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno>CVPR09</idno>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Benchmark databases for video-based automatic sign language recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dreuw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Neidle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Athitsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC&apos;08)</title>
		<meeting>the Sixth International Conference on Language Resources and Evaluation (LREC&apos;08)<address><addrLine>Marrakech</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resources Association (ELRA</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Vittorio: 2D articulated human pose estimation and retrieval in (almost) unconstrained still images</title>
		<author>
			<persName><forename type="first">Marcin</forename><surname>Eichner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuel</forename><surname>Marín-Jiménez</surname></persName>
		</author>
		<author>
			<persName><surname>Jesús</surname></persName>
		</author>
		<author>
			<persName><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intern. J. Comp. Vis</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="190" to="214" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Principal motion: Pca-based reconstruction of motion histograms</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Jair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<ptr target="http://www.causality.inf.ethz.ch/Gesture/principal_motion.pdf" />
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
		<respStmt>
			<orgName>ChaLearn Technical Memorandum</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Principal motion components for gesture recognition using a single-example</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Escalante</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Athitsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jangyodsuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<idno>CoRR abs/1310.4822</idno>
		<ptr target="http://arxiv.org/abs/1310.4822" />
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Multi-modal gesture recognition challenge 2013: Dataset and results</title>
		<author>
			<persName><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gonzàlez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Baró</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Reyes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Athitsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Jair</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>ChaLearn Technical Memorandum</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Choosing and modeling the hand gesture database for a natural user interface</title>
		<author>
			<persName><forename type="first">P</forename><surname>Glomb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Romaszewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Opozda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sochan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th international conference on Gesture and Sign Language in Human-Computer Interaction and Embodied Communication. GW&apos;11</title>
		<meeting>the 9th international conference on Gesture and Sign Language in Human-Computer Interaction and Embodied Communication. GW&apos;11<address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="24" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The cmu motion of body (mobo) database</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<idno>CMU-RI-TR-01-18</idno>
		<imprint>
			<date type="published" when="2001">2001</date>
			<pubPlace>Pittsburgh</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Robotics Institute, Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">ChaLearn gesture demonstration kit</title>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Athitsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jangyodsuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Jair</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
		<respStmt>
			<orgName>ChaLearn Technical Memorandum</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Results and analysis of the ChaLearn gesture challenge</title>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Athitsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jangyodsuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Jair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hamner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Depth Image Analysis and Applications</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<date type="published" when="2012">2012. 2013</date>
			<biblScope unit="volume">7854</biblScope>
			<biblScope unit="page" from="186" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Chalearn gesture challenge: design and first results</title>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Athitsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jangyodsuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hamner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Jair</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Let me see your body talk</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Hargrave</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Kendall/Hunt Pub. Co</publisher>
			<pubPlace>Dubuque</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">A full-body gesture database for automatic gesture recognition</title>
		<author>
			<persName><forename type="first">B.-W</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>IEEE Computer Society</publisher>
			<biblScope unit="page" from="243" to="248" />
		</imprint>
		<respStmt>
			<orgName>FG</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Gesture: visible action as utterance</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kendon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Cambridge University Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">HMDB: a large video database for human motion recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">On space-time interest points</title>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intern. J. Comp. Vis</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="107" to="123" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">I</forename><surname>Serrano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kragic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kyrki</surname></persName>
		</author>
		<ptr target="http://www.csc.kth.se/~danik/gesture_database/" />
		<title level="m">Cvap arm/hand activity database</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Language-motivated approaches to action recognition</title>
		<author>
			<persName><surname>Malgireddy</surname></persName>
		</author>
		<author>
			<persName><surname>Manavender</surname></persName>
		</author>
		<author>
			<persName><surname>Nwogu</surname></persName>
		</author>
		<author>
			<persName><surname>Ifeoma</surname></persName>
		</author>
		<author>
			<persName><surname>Govindaraju</surname></persName>
		</author>
		<author>
			<persName><surname>Venu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="2189" to="2212" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Purdue rvl-slll asl database for automatic recognition of american sign language</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Martnez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Wilbur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Shay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Kak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th IEEE International Conference on Multimodal Interfaces. ICMI &apos;02</title>
		<meeting>the 4th IEEE International Conference on Multimodal Interfaces. ICMI &apos;02<address><addrLine>Washington</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="167" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Hand and mind: what gestures reveal about thought. Psychology/cognitive science</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mcneill</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>University of Chicago Press</publisher>
			<pubPlace>Chicago</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Summaries of 107 computer visionbased human motion capture papers</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Moeslund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bajers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Visual analysis of humans-looking at people</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">B</forename><surname>Moeslund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><surname>Krüger</surname></persName>
		</author>
		<author>
			<persName><surname>Volker</surname></persName>
		</author>
		<editor>Sigal, L.</editor>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Documentation mocap database hdm05</title>
		<author>
			<persName><forename type="first">M</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Röder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Clausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Eberhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Krüger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Weber</surname></persName>
		</author>
		<idno>CG-2007-2</idno>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
		<respStmt>
			<orgName>Universität Bonn</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Speak Italian: the fine art of the gesture</title>
		<author>
			<persName><forename type="first">B</forename><surname>Munari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chronicle Books</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<pubPlace>San Francisco</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">World Federation of the Deaf and World Federation of the Deaf. Unification of Signs Commission. Gestuno: international sign language of the deaf. GESTUNO: International Sign Language of the Deaf</title>
	</analytic>
	<monogr>
		<title level="s">Langage Gestuel International Des Sourds</title>
		<imprint>
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
	<note>British Deaf Association [for] the World Federation of the Deaf</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Real-time classification of dance gestures from skeleton animation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Raptis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kirovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hoppes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGGRAPH/Eurographics symposium on Computer animation</title>
		<meeting>the ACM SIGGRAPH/Eurographics symposium on Computer animation</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Real-time human pose recognition in parts from single depth images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Finocchio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kipman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Humaneva: synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human motion</title>
		<author>
			<persName><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName><surname>Balan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Alexandru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comp. Vision</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="4" to="27" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">80 million tiny images: a large data set for nonparametric object and scene recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Antonio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Patt. Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Error bounds for convolutional codes and an asymptotically optimum decoding algorithm</title>
		<author>
			<persName><forename type="first">A</forename><surname>Viterbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Info. Theory IEEE Trans</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="260" to="269" />
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Laban&apos;s principles of dance and movement notation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Von Laban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lange</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Canada</title>
		<imprint>
			<date type="published" when="1975">1975</date>
			<publisher>Macdonald &amp; Evans</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Armstrong</surname></persName>
		</author>
		<title level="m">Field guide to gestures: how to identify and interpret virtually every gesture known to man. Field Guide</title>
		<meeting><address><addrLine>Philadelphia</addrLine></address></meeting>
		<imprint>
			<publisher>Quirk Books</publisher>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">One-shot learning gesture recognition from rgb-d data using bag-of-features</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
