<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unifying Molecular and Textual Representations via Multi-task Language Modelling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Dimitrios</forename><surname>Christofidellis</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research Europe</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Giorgio</forename><surname>Giannone</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research Europe</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Technical Univer-sity</orgName>
								<address>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jannis</forename><surname>Born</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research Europe</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ole</forename><surname>Winther</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Technical Univer-sity</orgName>
								<address>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Copenhagen</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Teodoro</forename><surname>Laino</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research Europe</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Matteo</forename><surname>Manica</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research Europe</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Matteo Manica</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Unifying Molecular and Textual Representations via Multi-task Language Modelling</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The recent advances in neural language models have also been successfully applied to the field of chemistry, offering generative solutions for classical problems in molecular design and synthesis planning. These new methods have the potential to optimize laboratory operations and fuel a new era of data-driven automation in scientific discovery. However, specialized models are still typically required for each task, leading to the need for problem-specific fine-tuning and neglecting task interrelations. The main obstacle in this field is the lack of a unified representation between natural language and chemical representations, complicating and limiting human-machine interaction.</p><p>Here, we propose a multi-domain, multi-task language model to solve a wide range of tasks in both the chemical and natural language domains. By leveraging multi-task learning, our model can handle chemical and natural language concurrently, without requiring expensive pre-training on single domains or task-specific models. Interestingly, sharing weights across domains remarkably improves our model when benchmarked against state-of-the-art baselines on single-domain and cross-domain tasks. In particular, sharing information across domains and tasks gives rise to large improvements in cross-domain tasks, the magnitude of which increase with scale, as measured by more than a dozen of relevant metrics. Our work suggests that such models can robustly and efficiently accelerate discovery in physical sciences by superseding problem-specific fine-tuning and enhancing human-model interactions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The transformer architecture <ref type="bibr">(Vaswani et al., 2017)</ref> has had a significant impact on several fields within computer science, such as language understanding <ref type="bibr" target="#b7">(Devlin et al., 2018</ref>), text generation <ref type="bibr" target="#b27">(Radford et al., 2019;</ref><ref type="bibr" target="#b4">Brown et al., 2020)</ref>, image understanding <ref type="bibr" target="#b8">(Dosovitskiy et al., 2020)</ref>, multi-modal generation <ref type="bibr" target="#b29">(Ramesh et al., 2022;</ref><ref type="bibr" target="#b31">Saharia et al., 2022)</ref>, among others. Scaling language models using this architecture has proven to be a powerful and general strategy for improving generalization. This has led to the emergence of multi-task <ref type="bibr" target="#b27">(Radford et al., 2019)</ref> and few-shot <ref type="bibr" target="#b4">(Brown et al., 2020;</ref><ref type="bibr" target="#b42">Winata et al., 2021)</ref> models leveraging scale and compute <ref type="bibr" target="#b32">(Sanh et al., 2021;</ref><ref type="bibr" target="#b28">Raffel et al., 2020)</ref>.  This plot compares the performance of three different models with different sizes (Text+Chem T5-base, Text+Chem T5-small, MolT5base, MolT5-small, T5-base, and T5-small) on the task of converting SMILES to captions, using six different metrics: BLUE-2, BLEU-4, Rouge-1, Rouge-2, Rouge-L, and Meteor. The models are compared by plotting their scores on the y-axis. The graph shows that our proposal, Text+Chem T5, performs the best on all metrics and improves with size, corroborating our hypothesis that joint learning on molecular and textual domains leveraging multitask learning is a powerful paradigm to bridge the gap between domains.</p><p>Recent developments in language models have fueled applications in engineering and science. One notable area of success is chemistry, where ideas from natural language have been used to make significant advancements in reaction  This plot compares the performance of three different models with different sizes (Text+Chem T5-base, Text+Chem T5-small, MolT5-base, MolT5-small, T5-base, and T5-small) on the task of converting captions to SMILES, using five different metrics: Accuracy, Morgan FTS, RDK FTS, BLEU, MACCS FTS. The models are compared by plotting their scores on the y-axis. The graph shows that our proposal, Text+Chem T5, performs the best on all metrics and improves with size, corroborating our hypothesis that joint learning on molecular and textual domains leveraging multitask learning is a powerful paradigm to bridge the gap between domains.</p><p>prediction <ref type="bibr" target="#b34">(Schwaller et al., 2019)</ref>, conditional compound generation <ref type="bibr" target="#b3">(Born et al., 2021b;</ref><ref type="bibr">a)</ref>, retrosynthesis <ref type="bibr" target="#b35">(Schwaller et al., 2020</ref>), text-conditional de novo generation <ref type="bibr" target="#b11">(Edwards et al., 2021)</ref>, molecule generation <ref type="bibr" target="#b1">(Born and Manica, 2023)</ref>, protein structure prediction <ref type="bibr" target="#b14">(Jumper et al., 2021)</ref>, among others. By interpreting chemistry as a programmable language for life sciences, transformer-based models are revolutionizing the chemical discovery pipeline, significantly speeding up laboratory and design automation <ref type="bibr" target="#b23">(O'Neill, 2021;</ref><ref type="bibr" target="#b40">Vaucher et al., 2020)</ref>, and paving the way for an age of accelerated discovery in science and engineering.</p><p>Despite these successes, language model advancements in the chemical domain are still limited. Specialized models must be built for each task of interest, which is timeconsuming and requires a significant amount of human expertise. When multiple domains are considered, e.g., generating a novel molecule from its technical description in natural language, merging information is challenging due to the domain shift between language and chemistry. Current solutions often involve pre-training the model on large, single-domain datasets and fine-tuning on each task <ref type="bibr" target="#b11">(Edwards et al., 2021)</ref>, resulting in high computational expense, sample inefficiency, and the need to repeat this process for each use-case.</p><p>In light of these limitations, it is worth considering the feasibility of a more efficient and general multi-task model that can translate between the textual and chemical domains.</p><p>This type of model would be particularly useful in cases where large amounts of data are not available and domains are unbalanced. Such models would also be critically important for tasks where information sharing is essential, like molecular captioning (given a molecule, describe it in natural language) or text-conditional de-novo generation (given a description, generate a molecule).</p><p>In this work, we propose a multi-task transformer for natural language and chemical translation, Text+Chem T5. We focus on transfer learning in the chemical domain, with a specific emphasis on cross-domain tasks, tasks that involve chemistry and natural language concurrently. Text+Chem T5 does not rely on expensive mono-domain pre-training, task-specific fine-tuning, or separate heads for each task.</p><p>Our model can be directly used on a variety of chemical and natural language-based tasks in a mono-domain and cross-domain setup.</p><p>Contribution. Our work presents the following key contributions: (i) we introduce a novel cross-domain, multi-task chemical language model (Text+Chem T5) that effectively bridges natural and chemical languages by enabling translation between the domains; (ii) we propose an efficient training strategy that leverages the strengths of both singledomain and multi-domain tasks to adapt single-domain models for cross-domain tasks. This eliminates the need for costly pre-training on large mono-domain datasets and taskspecific fine-tuning, while at the same time improving crossdomain translation by sharing information between tasks and across domains; (iii) we provide experimental validation on benchmark datasets for single and cross-domain tasks, demonstrating that our model is competitive with state-of-the-art methods specialized for single tasks. We also conduct a thorough analysis of various modeling choices, including encoder-decoder architecture, the use of frozen vs. learnable encoders, and single-domain vs. multi-domain encoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Our model is designed to handle tasks that span multiple domains (see Fig. <ref type="figure">3</ref>), specifically chemistry-based tasks (mol2mol), textual-based tasks (text2text), and crossdomain tasks (mol2text and text2mol).</p><p>In this section, we present an overview of recent advances in generative language models for transfer and multi-task learning in the natural language and chemical domains. We examine the limitations of current models, particularly in the cross-domain generation, and demonstrate the necessity for our proposed approach. Transformers, as presented in <ref type="bibr">(Vaswani et al., 2017)</ref>, are widely used in language modeling. T5 <ref type="bibr" target="#b28">(Raffel et al., 2020)</ref>, a transformer trained on a diverse set of tasks, has demonstrated impressive generalization and adaptation capabilities in multi-tasking and</p><p>The molecule is a siderophore composed from ... Given the above description generate the described molecule in SMILES.  <ref type="bibr" target="#b28">(Raffel et al., 2020)</ref>, to solve all these tasks effectively. The pre-trained models serve as a good starting point for fine-tuning the target distribution of tasks. For a variant of this pipeline with domain-specific encoders, as presented in Table <ref type="table" target="#tab_6">5</ref>, see Fig. <ref type="figure">8</ref>.</p><formula xml:id="formula_0">C(CC(=O)NCCNC(=O))CC(CC(=O)NCC(C(=O)O)N) (C(=O)O)O)C(=O)C(=O)O C(CC(=O)NCCNC(=O))CC(CC(=O)NCC(C(=O)O)N) (C(=O)O)O)C(=O)C(=O)O.</formula><p>multi-modal generation <ref type="bibr" target="#b31">(Saharia et al., 2022)</ref>. We use T5 as the backbone of our work.</p><p>Specialized models for chemistry have been developed, such as Molecular Transformers <ref type="bibr" target="#b34">(Schwaller et al., 2019)</ref> and the RXN family <ref type="bibr" target="#b33">(Schwaller et al., 2018;</ref><ref type="bibr">2020;</ref><ref type="bibr" target="#b38">Toniato et al., 2021;</ref><ref type="bibr" target="#b40">Vaucher et al., 2020)</ref>, which address tasks like forward reaction prediction and molecular retrosynthesis. However, these models require separate models for each task, leading to increased computational cost and a need for specialized expertise for each task.</p><p>T5Chem <ref type="bibr" target="#b19">(Lu and Zhang, 2022</ref>) offers a unified multi-tasking model for chemistry, using a single model for tasks like reaction prediction, regression, and classification. However, T5Chem relies on task-specific heads, is restricted to the chemical domain and has limited applicability in different sub-domains.</p><p>MolT5 <ref type="bibr" target="#b10">(Edwards et al., 2022)</ref> addresses the difficult problem of cross-domain generation by linking natural language and chemistry, tackling tasks such as text-conditional de novo molecule generation and molecule captioning. However, it relies on costly pre-training on large mono-modality datasets, as well as per-task fine-tuning for each multi-modal task, which in turn limits its ability to leverage the multitasking capabilities of T5 and the sharing of information between tasks.</p><p>Despite the progress made multi-task learning in natural language processing, transferring these advancements to the chemical domain remains a challenge. Current models focus on optimizing specific tasks <ref type="bibr" target="#b34">(Schwaller et al., 2019)</ref>, multi-tasking learning <ref type="bibr" target="#b19">(Lu and Zhang, 2022)</ref> or translation between text and chemistry <ref type="bibr" target="#b10">(Edwards et al., 2022)</ref>, but still struggle with handling multiple tasks across domains without incurring in the expense of large, sample-inefficient pre-training and fine-tuning. While multi-tasking within a single domain may be feasible, multi-tasking across multiple domains is still a challenge. Our proposed solution is a simple training strategy, which utilizes pre-trained transformers for each modality and a learnable, small output decoder to merge modalities in the later stages of the model, thus addressing these challenges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>Model. Our goal is to develop a multi-task, multi-domain model for natural and chemical language. To achieve this, we use a T5 backbone, an encoder-decoder transformer architecture specifically proposed for multi-tasking <ref type="bibr" target="#b28">(Raffel et al., 2020)</ref>. The encoder-decoder architecture is especially suited for cross-domain tasks because we can explore a family of architectural choices modifying the encoder without the need to modify the decoder (see Fig. <ref type="figure" target="#fig_4">4</ref>). Doing so we can ablate our multi-domain multi-task model with variations where we use two encoders (one for each domain), we consider a different model as chemistry encoder, and we freeze the encoders (more details in Table <ref type="table" target="#tab_6">5</ref>). We name our model Text+Chem T5.</p><p>Tasks Distribution. The Text+Chem T5 model is designed to handle tasks that span multiple domains, specifically chemistry-based tasks (mol2mol), textual-based tasks (text2text), and cross-domain tasks (mol2text and text2mol). Our objective is to train the model to learn a mapping between languages without losing proficiency in the original languages, similar to cross-domain generative tasks in the context of language translation. To achieve this, we follow the task-prompting method outlined in <ref type="bibr" target="#b28">Raffel et al. (2020)</ref>. Specifically, we focus on the following tasks for each domain:</p><p>? mol2mol is a mono-domain task that is focused on chemical reactions, it has two sub-tasks: Forward reaction. Given precursors (optionally including reagents and/or enzymes), the task is to generate the main product of the reulting chemical reaction. This sub-task is a classic example of a forward reaction prediction task, the model has to predict the outcome of a chemical reaction based on the starting chemicals.</p><p>Retrosynthesis. Given the product of a chemical reaction, the goal is to find the precursors (optionally including reagents and/or enzymes). This sub-task is an example of a retrosynthesis task, which is the inverse of a forward reaction prediction task. The model needs to predict the starting chemicals that would be required to synthesize a given compound. Note that we consider one-step retrosynthesis only.</p><p>? mol2text is a cross-domain task that is focused on generating natural text from chemical input. Molecular captioning. Given a molecule represented as SMILES (Simplified Molecular Input Line Entry System), the goal is to generate a textual description of such molecule. This task is an example of a cross-domain task as it involves both chemistry and natural language processing. The model is to generate a human-readable description of a chemical compound based on its SMILES representation.</p><p>? text2mol is a cross-domain task that is focused on generating chemical representation from text.</p><p>Text-conditional de novo generation. In this cross-domain task, a textual paragraph describing the molecule is provided and the goal is to output a SMILES representation for such molecule. This task is an example of a cross-domain task as it involves both natural language processing and chemistry. The model should generate the SMILES representation for a chemical compound based on its textual description.</p><p>? text2text is a mono-domain task that is focused on natural language processing. Paragraph to action. The task is to generate the action sequence for a certain chemical reaction described in natural language. The model is to take a natural language description of a chemical reaction and generates a step-wise execution protocol to carry out that reaction. This task is an example of text generation in natural language processing, and it's focused on understanding the chemical reactions and converting them into a list of actions.</p><p>Merging Domains. As shown in Fig. <ref type="figure" target="#fig_4">4</ref> and Table <ref type="table" target="#tab_6">5</ref>, we ablate Text+Chem T5 architecture using different encoder setups. In particular, we want to explore the cross-domain performance when using a different encoder for each domain. In this scenario, we need a mechanism to aggregate information from the natural language and chemistry encoder. Given the difference between domains, it is crucial to find an expressive way to merge information at the late stage in input to the decoder. One way to accomplish this is to simply average the encoder output embeddings. However, a more expressive approach is to use a cross-attention approach, loosely inspired by the contextual attention in <ref type="bibr">Born et al. (2021a)</ref>. Specifically, by selecting one domain as the base domain, we can translate information in a powerful way. This approach is also well suited to scale to multiple domains. We denote H t ? R (nt,ht) as the output of the base domain encoder, where n t is the sequence length (number of tokens) and h t is the hidden dimensionality. We also consider a second domain, H m ? R (nm,hm) , where n m is the number of tokens and h m is the dimensionality for the adaptation domain (e.g. chemistry information). We merge this information using cross-attention by setting the base domain as the queries Q = f t (H t ) ? R (nt,d) , and the adap- Instead, two frozen sets of weights ( ?t , ?c ) are used for the text and chemistry encoders respectively. These weights are extracted from large, pre-trained language encoders, such as T5 <ref type="bibr" target="#b28">(Raffel et al., 2020)</ref> and ChemT5 <ref type="bibr" target="#b19">(Lu and Zhang, 2022)</ref>. B: a multi-domain model is still built using two sets of weights. However, the chemistry encoder is fine-tuned (enc-training) while the text encoder remains frozen (no enc-sharing). The fine-tuning process starts from a pre-trained T5 checkpoint fine-tuned on chemistry data. C: the encoders are merged, using a joint encoder for text and chemistry (? t = ? c ) and trained jointly on the multi-domain and multi-task data (enc-training, enc-sharing). This approach allows the model to be fine-tuned on a variety of tasks and domains, which improves its generalization capabilities. A T5 decoder is used and no separate heads are used for each task or domain, instead the sharing of information between tasks and domains enriches the model generalization. V t is the vocabulary for text and V c is the one for chemistry.</p><p>tation domain as the keys ,d) . In practice, we compute ,nm) and finally H tm = W, V ? R (nt,d) . We can then apply this block to H tm in a hierarchical fashion, setting H t = H tm . Finally, the output of this attention network is fed to the T5 decoder. The use of text as the base domain means that we can feed the final H t directly to the T5 decoder, without the need for additional adaptation of the architecture. An alternative and more expressive approach is to merge the adaptation mode into the base mode to obtain H tm , and then merge the base mode into the adaptation mode to obtain H mt , and combining these intermediate results to obtain the representation input for the decoder H tm . See appendix Fig. <ref type="figure">8</ref> for additional visualizations.</p><formula xml:id="formula_1">K = f k (H m ) ? R (nm,d) and values V = f v (H m ) ? R (nm</formula><formula xml:id="formula_2">W = ?(Q, K T ) ? R (nt</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Setup. We evaluate the model's performance on five tasks: forward and backward reaction prediction in chemistry, textconditional de novo molecule generation and molecule captioning across domains, and paragraph-to-action in the language domain. The training process is carried out using the language modeling trainer based on Hugging Face transformers <ref type="bibr" target="#b43">(Wolf et al., 2020)</ref> and PyTorch Lightning <ref type="bibr">(Falcon and</ref> The PyTorch Lightning team, 2019) from the GT4SD library <ref type="bibr" target="#b20">(Manica et al., 2022)</ref>. To initialize our transformer model, we choose to use the natural language domain, as it has the most available data. For this reason, we use T5-small and T5-base as pretrained bases for our respective models. Details on the models' hyperparameters can be found in Appendix D.</p><p>Dataset. To train our model, we generated a multi-domain and a multi-task dataset by aggregating available datasets for each task of interest. Specifically, we leveraged the dataset used in A second augmented version of the training set was also been constructed by including further reactants-products pairs. This second version had 6.7M reaction pairs and in total 33.5M samples. For the second augmented dataset, we followed the same procedure to assure balance in the number of in-stances among tasks. The use of the augmented dataset in the presented results is indicated by the prefix 'augm' in the respective table rows. In the rest cases, the first version of the multi-task dataset has been used to train the respective model. In both datasets, we rely on prompts for the task definition. The prompt templates that has been used can be found in appendix (see Table <ref type="table">8</ref>).</p><p>Evaluation. Evaluating the model is challenging as it spans multiple domains. For this reason, we treat each task separately and we rely on a combination of NLP based as well as task-specific metrics.</p><p>For the molecule-to-text task (mol2text), we consider the following metrics: BLEU-2 and BLEU-4 <ref type="bibr" target="#b24">(Papineni et al., 2002)</ref> are metrics used to evaluate the quality of machine-generated text by comparing it to a reference text. BLEU-2 computes the 2-grams (word bigrams) overlap between the generated text and reference text, and BLEU-4 extends this to 4-grams. A higher score on the BLEU metric indicates a higher level of similarity between the generated text and reference text. ROUGE-1, ROUGE-2, and ROUGE-L <ref type="bibr" target="#b18">(Lin, 2004</ref>) are similar to BLEU, but compute the recall-overlap of unigrams, bigrams, and longest common subsequences between the generated and reference texts. METEOR <ref type="bibr" target="#b0">(Banerjee and Lavie, 2005</ref>) is a metric that uses a combination of unigram precision, recall, and a synonym-matching component to evaluate the generated text against the reference text. It's designed to be more sensitive to fluency, meaning, and structure than BLEU.</p><p>For the text-to-molecule task (text2mol), we consider the following metrics: BLEU scores, Accuracy, Levenshtein distance, MACCS-FTS <ref type="bibr" target="#b9">(Durant et al., 2002)</ref>, RDK-FTS (Tanimoto, 1958), Morgan-FTS <ref type="bibr" target="#b30">(Rogers and Hahn, 2010)</ref> and FCD <ref type="bibr" target="#b25">(Preuer et al., 2018)</ref>. Furthermore, we report validity as the percent of molecules which can be processed by RD-Kit as in <ref type="bibr" target="#b10">Edwards et al. (2022)</ref>. For this task, the accuracy is the number of correctly generated molecules made by the model divided by the total number of samples. Levenshtein distance is a string similarity metric counting the number of edits (insertions, deletions, or substitutions) required to change one sequence into the other <ref type="bibr" target="#b15">(Levenshtein, 1966)</ref>. It is often used in natural language processing for tasks such as spell-checking and speech recognition.</p><p>Baselines. We compare our proposed method with several baselines including a standard Transformer model, T5 finetuned on each task, the RXN family, and MolT5. For T5, MolT5, and our model, we consider a small (40M) and a base (220M) version.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Results</head><p>With these experiments, we aim to demonstrate: (i) the effectiveness of a joint, cross-domain multi-task language model in improving generalization on cross-domain tasks; (ii) that by leveraging pre-trained single-domain information, we can avoid the need for costly pretraining and task-specific fine-tuning; (iii) that by sharing information not only between tasks and domains but also between encoder weights, we can achieve the best cross-domain translation. Lastly, we will illustrate how our approach benefits from increased scale, providing a general paradigm for language models in the scientific domain.</p><p>Cross-domain Tasks. Text to Molecule. Table <ref type="table" target="#tab_5">4</ref> presents the results of different models evaluated on the Caption to SMILES (text2mol) task. The models include Transformer, T5, MolT5, and Text+Chem T5. The performance of each model is evaluated using BLEU score, Accuracy, and Levenshtein distance.</p><p>Table <ref type="table" target="#tab_3">2</ref>: Results across domains and tasks. We evaluate T5, a multi-task model for the textual domain, finetuned for each class of tasks; for the chemical domain, we consider also specialized models for forward and retrosynthesis (RXN family); and MolT5, a multi-domain model for the textual and chemical domains. Tasks evaluated include chemical-based tasks (forward and retrosynthesis), cross-domain tasks (text-conditional de novo generation and molecule captioning), and textualbased tasks (paragraph to action). Our goal is to leverage multi-task learning to improve cross-domain translation between chemistry and text. The forward and retrosynthesis RXN baseline results are re-evaluations of the original models <ref type="bibr" target="#b34">(Schwaller et al., 2019;</ref><ref type="bibr">2020)</ref> as presented in <ref type="bibr" target="#b38">Toniato et al. (2021)</ref>. For the forward prediction task the metric is accuracy; for the retrosynthesis task the metric is roundtrip accuracy <ref type="bibr" target="#b35">(Schwaller et al., 2020)</ref>; for all the other tasks the BLEU score. For more metrics see Tables <ref type="table">3</ref> and<ref type="table" target="#tab_5">4</ref>. The best results are highlighted in bold. Text+Chem T5 performed best among all the models in the task. Specifically, it achieved the highest BLEU score of 0.853, indicating that it generated SMILES strings that were highly similar to the reference SMILES. The accuracy indicates that Text+Chem T5 generates more than 32% of correct SMILES. The Levenshtein distance is a measure of the similarity between two strings. A smaller Levenshtein distance indicates that the generated SMILES is more similar to the reference SMILES. The Levenshtein distance of 16.87 for the Text+Chem T5 model indicates that this model was able to generate SMILES strings that were very similar to the reference SMILES. Overall, the table suggests that Text+Chem T5 model was the best among all the models for the Caption to SMILES task, as it performed well on all the metrics.</p><p>Architecture Ablation. Table <ref type="table" target="#tab_6">5</ref> presents the results of an ablation study on the aggregation and encoder strategy for the cross-domain tasks of text2mol and mol2text. The different models considered in the ablation study include MDe 2 -CLM, MDMTe 2 -CLM, and Text+Chem T5. The performance of each model is evaluated using the text2mol and mol2text metrics. The best results are highlighted in bold. The table compares different variants of the model (Figure <ref type="figure" target="#fig_4">4</ref>). MDe 2 -CLM denotes that the model has different encoders for each domain, whereas MDMTe 2 -CLM denotes that the model has different encoders for each domain and is trained using multiple tasks. The models are further differentiated based on the aggregation and encoder-sharing strategy. "Agg" denotes the method used for aggregating information from the different tasks, "enc-sharing" denotes whether the encoders are shared across tasks, and "enc-tuning" denotes whether the encoders are fine-tuned for each task.</p><p>Text+Chem T5 performed the best among all the models for both the text2mol and mol2text tasks. It achieved the highest score of 0.853 on the text2mol task and 0.625 on the mol2text task. The model that performed best is Text+Chem T5, it has achieved the highest scores on both cross-domain tasks, it was implemented using shared encoders and fine-tuning approach. Using a shared encoder and fine-tuning strategy for the CLM model improves performance on cross-domain tasks. Also, the ablation study has indicated that the aggregate method didn't play a big role in the performance of the model, but shared encoders and fine-tuning approach had the most significant effect on the performance of the model.</p><p>Model Size. Text+Chem T5 results improve by increasing the model size. Figure <ref type="figure" target="#fig_5">5</ref> shows the trend for different metrics (x-axis) for the SMILES to Caption (mol2text) task. We report results for T5-fine-tuned, MolT5 and Text+Chem T5 in two different model size: small (60M parameters) and base (220M parameters). These are standard sizes for T5based models. We see how the results for Text+Chem T5 not only improve increasing the model capacity, but improve much faster than baselines with similar capacity. We see similar trends in Table <ref type="table">3</ref> and<ref type="table" target="#tab_5">Table 4</ref>, corroborating the idea that joint multi-task training on textual and molecular domains is an effective mechanism to enrich representations in language models and share information among tasks and domains.</p><p>Table <ref type="table">3</ref>: Results of the SMILES to Caption (mol2text) task. The baselines include Transformer <ref type="bibr" target="#b10">(Edwards et al., 2022)</ref>, T5 (fine-tuned), and MolT5 <ref type="bibr" target="#b10">(Edwards et al., 2022)</ref>. The metrics used in the Dataset Size. The augmented version of the dataset contributes to the improved performance across the whole span of tasks. This improvement is especially high in the tasks in which the requested output modality is SMILES which is totally aligned with the new information that we have incorporated in the augmented version of the dataset. This observation underlines the need of a high volume of data in such training strategies to assist the model in better understanding the different domains or modalities of interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Transformers for Natural Language. BERT <ref type="bibr" target="#b7">(Devlin et al., 2018)</ref> is a prominent example, a bidirectional transformer trained using masked language modeling, which forces the model to reconstruct the input that has been degraded. BERT popularized the adoption of the transformer as the backbone architecture for many language tasks. In the same period, consistent, long-range text generation was achieved using autoregressive training (causal language modeling) using the GPT-family <ref type="bibr" target="#b26">(Radford et al., 2018)</ref>, transformers with a specific focus on the next token generation. Surprisingly, simple training using large architectures and vast data gave rise to generalization, multitasking <ref type="bibr" target="#b27">(Radford et al., 2019)</ref>, and few-shot capabilities <ref type="bibr" target="#b4">(Brown et al., 2020)</ref>. Training on code <ref type="bibr" target="#b5">(Chen et al., 2021)</ref>, instruction finetuning <ref type="bibr" target="#b22">(Ouyang et al., 2022;</ref><ref type="bibr" target="#b6">Chung et al., 2022)</ref> and chain-of-thought prompting <ref type="bibr" target="#b41">(Wei et al., 2022)</ref> have further improved the performance of transformer models in reasoning tasks <ref type="bibr" target="#b13">(Fu et al., 2022;</ref><ref type="bibr" target="#b16">Lewkowycz et al., 2022;</ref><ref type="bibr" target="#b17">Li?vin et al., 2022)</ref>.</p><p>Generative Transfer and Multi-task Learning. Building on the success of masked language modelling <ref type="bibr" target="#b7">(Devlin et al., 2018)</ref>, the T5 framework <ref type="bibr" target="#b28">(Raffel et al., 2020)</ref> has emerged as a leading paradigm in generative transfer and multitask learning. T5 is a multitask language model that is trained on a wide variety of tasks using masking. This model has shown impressive generalization and adaptation capabilities for multimodal generation <ref type="bibr" target="#b31">(Saharia et al., 2022)</ref>. These techniques have also greatly improved the performance of cross-domain language models, such as models that can translate between languages <ref type="bibr" target="#b32">(Sanh et al., 2021)</ref>.</p><p>Language Models in Chemistry. Recent advances in neu- ral language models have been successfully applied to the chemical domain. Several studies have utilized transformer architectures to address a variety of chemical tasks, including forward reaction prediction <ref type="bibr" target="#b34">(Schwaller et al., 2019)</ref>, multi-step retrosynthesis <ref type="bibr" target="#b35">(Schwaller et al., 2020;</ref><ref type="bibr" target="#b38">Toniato et al., 2021)</ref>, and property prediction <ref type="bibr" target="#b36">(Schwaller et al., 2021;</ref><ref type="bibr" target="#b40">Vaucher et al., 2020;</ref><ref type="bibr" target="#b1">Born and Manica, 2023)</ref>. In addition, there has been research exploring multitask generation for chemistry using a pretrained T5 model with multiple heads for different types of tasks, such as regression, classification, and generation <ref type="bibr" target="#b19">(Lu and Zhang, 2022)</ref>. However, there is still a need for models that can handle multiple tasks across domains without the need for expensive pretraining or finetuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we introduced Text+Chem T5, a multi-task, multi-domain language model for the natural and chemical domains. The model can effectively translate between natural and chemical languages, making it possible to solve a variety of tasks such as chemical reaction prediction, retrosynthesis, text-conditional de novo generation, and molecular captioning. The strength of the model lies in its ability to solve multiple tasks without the need for additional taskspecific heads or adaptation at test time. The result is a step forward in the development of a general multi-task, multi-domain language model for the life sciences. This can potentially accelerate the discovery process in this field, by providing a more efficient way to process, analyze and generate chemical and textual data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Additional Experiments</head><p>Table <ref type="table">6</ref>: Comparison on the cross-domain tasks. The models include a fine-tuned transformer, T5 models in both zero-shot and fine-tuned settings, MolT5, and Text+Chem T5. The models are compared at two different sizes, "small" and "base." We can see how T5 zero-shot (without fine-tuning) is completely unable to perform cross-domain translation, corroborating the necessity for multi-domain multitask modelling in the chemical and natural language domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Size text2mol mol2text</head><p>Transformer (fine-tuned) -0.499 0.061 T5 (zero-shot) small 0.000 0.004  This plot compares the performance of three different models with different sizes (Text+Chem T5-base, Text+Chem T5-small, MolT5-base, MolT5-small, T5-base, and T5-small) on the task of converting SMILES to captions, using six different metrics: BLUE-2, BLEU-4, Rouge-1, Rouge-2, Rouge-L, and Meteor. The models are compared by plotting their scores on the y-axis. The graph shows that our proposal, Text+Chem T5, performs the best on all metrics and improves with size, corroborating our hypothesis that joint learning on molecular and textual domains leveraging multitask learning is a powerful paradigm to bridge the gap between domains.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Molecule to Caption task. This plot compares the performance of three different models with different sizes (Text+Chem T5-base, Text+Chem T5-small, MolT5base, MolT5-small, T5-base, and T5-small) on the task of converting SMILES to captions, using six different metrics: BLUE-2, BLEU-4, Rouge-1, Rouge-2, Rouge-L, and Meteor. The models are compared by plotting their scores on the y-axis. The graph shows that our proposal, Text+Chem T5, performs the best on all metrics and improves with size, corroborating our hypothesis that joint learning on molecular and textual domains leveraging multitask learning is a powerful paradigm to bridge the gap between domains.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: Description to Molecule task. This plot compares the performance of three different models with different sizes (Text+Chem T5-base, Text+Chem T5-small, MolT5-base, MolT5-small, T5-base, and T5-small) on the task of converting captions to SMILES, using five different metrics: Accuracy, Morgan FTS, RDK FTS, BLEU, MACCS FTS. The models are compared by plotting their scores on the y-axis. The graph shows that our proposal, Text+Chem T5, performs the best on all metrics and improves with size, corroborating our hypothesis that joint learning on molecular and textual domains leveraging multitask learning is a powerful paradigm to bridge the gap between domains.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure4: The Chemical Language Model (CLM) family. The caption describes three different approaches to building a multi-domain model for text and chemistry tasks. A: a multi-domain model is built without the need to retrain the single-domain encoders (no enc-sharing, no enc-training). Instead, two frozen sets of weights ( ?t , ?c ) are used for the text and chemistry encoders respectively. These weights are extracted from large, pre-trained language encoders, such as T5<ref type="bibr" target="#b28">(Raffel et al., 2020)</ref> and ChemT5<ref type="bibr" target="#b19">(Lu and Zhang, 2022)</ref>. B: a multi-domain model is still built using two sets of weights. However, the chemistry encoder is fine-tuned (enc-training) while the text encoder remains frozen (no enc-sharing). The fine-tuning process starts from a pre-trained T5 checkpoint fine-tuned on chemistry data. C: the encoders are merged, using a joint encoder for text and chemistry (? t = ? c ) and trained jointly on the multi-domain and multi-task data (enc-training, enc-sharing). This approach allows the model to be fine-tuned on a variety of tasks and domains, which improves its generalization capabilities. A T5 decoder is used and no separate heads are used for each task or domain, instead the sharing of information between tasks and domains enriches the model generalization. V t is the vocabulary for text and V c is the one for chemistry.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure5: Molecule to Caption task. This plot compares the performance of three different models with different sizes (Text+Chem T5-base, Text+Chem T5-small, MolT5-base, MolT5-small, T5-base, and T5-small) on the task of converting SMILES to captions, using six different metrics: BLUE-2, BLEU-4, Rouge-1, Rouge-2, Rouge-L, and Meteor. The models are compared by plotting their scores on the y-axis. The graph shows that our proposal, Text+Chem T5, performs the best on all metrics and improves with size, corroborating our hypothesis that joint learning on molecular and textual domains leveraging multitask learning is a powerful paradigm to bridge the gap between domains.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Language Models for Chemistry. We compare language models by expressivity and flexibility. The column "w/o pretrain" indicates whether the model is pre-training or not on large mono-domain datasets before finetuning. The column "enc-sharing" indicates whether the model shares encoders between domains or not. The table shows that Text+Chem T5 does not use pre-training, and uses multitasking, multi-domain learning, and encoder sharing, which is more feature-rich than the other models in the literature. e 2 : the model uses domain-specific encoders. MD: multidomain. MT: multi-task.</figDesc><table><row><cell>w/o multi-multi-encoder</cell></row><row><cell>pretrain task domain sharing</cell></row><row><cell>T5 (Raffel et al., 2020)</cell></row><row><cell>MD-T5 (finetuned)</cell></row><row><cell>T5Chem (Lu and Zhang, 2022)</cell></row><row><cell>MolT5 (Edwards et al., 2022)</cell></row><row><cell>MDMTe 2 -CLM (ours)</cell></row><row><cell>Text+Chem T5 (ours)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>presents the results of dif-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>table include BLEU-2, BLEU-4, Rouge-1, Rouge-2, Rouge-L, and Meteor, all of which are common metrics used to evaluate text generation models. The table shows that our proposed model, Text+Chem T5, outperforms the other baselines in all the metrics. Overall, Text+Chem T5 is able to generate more accurate and informative captions for SMILES. Results of the Description to SMILES (text2mol) task. The performance of the models is evaluated by BLEU score, Accuracy, Levenshtein distance, and additional metrics (see Evaluation). The results show that the proposed model (Text+Chem T5) outperforms other baselines in all metrics. These results demonstrate the effectiveness of the proposed model in translating from natural language to SMILES representation of molecules.</figDesc><table><row><cell></cell><cell></cell><cell>Size</cell><cell cols="7">BLEU-2 ? BLEU-4 ? Rouge-1 ? Rouge-2 ? Rouge-L ? Meteor ?</cell></row><row><cell cols="2">Transformer (Edwards et al., 2022)</cell><cell>-</cell><cell>0.061</cell><cell>0.027</cell><cell>0.188</cell><cell>0.0597</cell><cell>0.165</cell><cell></cell><cell>0.126</cell></row><row><cell cols="3">T5 (fine-tuned) (Raffel et al., 2020) small</cell><cell>0.501</cell><cell>0.415</cell><cell>0.602</cell><cell>0.446</cell><cell>0.545</cell><cell></cell><cell>0.532</cell></row><row><cell cols="2">MolT5 (Edwards et al., 2022)</cell><cell>small</cell><cell>0.519</cell><cell>0.436</cell><cell>0.620</cell><cell>0.469</cell><cell>0.563</cell><cell></cell><cell>0.551</cell></row><row><cell>Text+Chem T5 (ours)</cell><cell></cell><cell>small</cell><cell>0.553</cell><cell>0.462</cell><cell>0.633</cell><cell>0.481</cell><cell>0.574</cell><cell></cell><cell>0.583</cell></row><row><cell cols="2">Text+Chem T5-augm (ours)</cell><cell>small</cell><cell>0.560</cell><cell>0.470</cell><cell>0.638</cell><cell>0.488</cell><cell>0.580</cell><cell></cell><cell>0.588</cell></row><row><cell cols="2">T5(fine-tuned) (Raffel et al., 2020)</cell><cell>base</cell><cell>0.511</cell><cell>0.424</cell><cell>0.607</cell><cell>0.451</cell><cell>0.550</cell><cell></cell><cell>0.539</cell></row><row><cell cols="2">MolT5 (Edwards et al., 2022)</cell><cell>base</cell><cell>0.540</cell><cell>0.457</cell><cell>0.634</cell><cell>0.485</cell><cell>0.578</cell><cell></cell><cell>0.569</cell></row><row><cell>Text+Chem T5 (ours)</cell><cell></cell><cell>base</cell><cell>0.580</cell><cell>0.490</cell><cell>0.647</cell><cell>0.498</cell><cell>0.586</cell><cell></cell><cell>0.604</cell></row><row><cell cols="2">Text+Chem T5-augm (ours)</cell><cell>base</cell><cell>0.625</cell><cell>0.542</cell><cell>0.682</cell><cell>0.543</cell><cell>0.622</cell><cell></cell><cell>0.648</cell></row><row><cell></cell><cell cols="9">Size BLEU score ? Accuracy ? Levenshtein ? MACCS FTS? RDK FTS? Morgan FTS? FCD? Validity?</cell></row><row><cell>Transformer (Edwards et al., 2022)</cell><cell>-</cell><cell>0.499</cell><cell>0</cell><cell>57.66</cell><cell>0.480</cell><cell>0.320</cell><cell>0.217</cell><cell>11.32</cell><cell>0.906</cell></row><row><cell cols="2">T5 (fine-tuned) (Raffel et al., 2020) small</cell><cell>0.741</cell><cell>0.064</cell><cell>27.7</cell><cell>0.704</cell><cell>0.578</cell><cell>0.525</cell><cell>2.89</cell><cell>0.608</cell></row><row><cell>MolT5 (Edwards et al., 2022)</cell><cell>small</cell><cell>0.755</cell><cell>0.079</cell><cell>25.99</cell><cell>0.703</cell><cell>0.568</cell><cell>0.517</cell><cell>2.49</cell><cell>0.721</cell></row><row><cell>Text+Chem T5 (ours)</cell><cell>small</cell><cell>0.739</cell><cell>0.157</cell><cell>28.54</cell><cell>0.859</cell><cell>0.736</cell><cell>0.660</cell><cell>0.066</cell><cell>0.776</cell></row><row><cell>Text+Chem T5-augm (ours)</cell><cell>small</cell><cell>0.815</cell><cell>0.191</cell><cell>21.78</cell><cell>0.864</cell><cell>0.744</cell><cell>0.672</cell><cell>0.060</cell><cell>0.951</cell></row><row><cell cols="2">T5 (fine-tuned) (Raffel et al., 2020) base</cell><cell>0.762</cell><cell>0.069</cell><cell>24.95</cell><cell>0.731</cell><cell>0.605</cell><cell>0.545</cell><cell>2.48</cell><cell>0.660</cell></row><row><cell>MolT5 (Edwards et al., 2022)</cell><cell>base</cell><cell>0.769</cell><cell>0.081</cell><cell>24.49</cell><cell>0.721</cell><cell>0.588</cell><cell>0.529</cell><cell>0.218</cell><cell>0.772</cell></row><row><cell>Text+Chem T5 (ours)</cell><cell>base</cell><cell>0.750</cell><cell>0.212</cell><cell>27.39</cell><cell>0.874</cell><cell>0.767</cell><cell>0.697</cell><cell>0.061</cell><cell>0.792</cell></row><row><cell>Text+Chem T5-augm (ours)</cell><cell>base</cell><cell>0.853</cell><cell>0.322</cell><cell>16.87</cell><cell>0.901</cell><cell>0.816</cell><cell>0.757</cell><cell>0.050</cell><cell>0.943</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Ablation study for different aggregation and encoder strategies for cross-domain tasks. The objective of this study is to understand how the different choices of aggregation and encoder strategies affect the performance of the model. The tasks considered are text-to-chemistry (text2mol) and chemistry-to-text (mol2text). The evaluation metrics used in the table are BLEU scores.</figDesc><table><row><cell>It</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Results of the Paragraph to Actions task. The performance of the models is evaluated by BLEU score and accuracy. The results show that the proposed model (Text+Chem T5) outperforms other baselines in all metrics. These results demonstrate the effectiveness of the proposed model in the text modality. RXN model is the paragraph-to-action proposed in<ref type="bibr" target="#b40">(Vaucher et al., 2020)</ref>.</figDesc><table><row><cell>T5 (fine-tuned)</cell><cell>small</cell><cell>0.762</cell><cell>0.501</cell></row><row><cell>MolT5</cell><cell>small</cell><cell>0.755</cell><cell>0.519</cell></row><row><cell>Text+Chem T5</cell><cell>small</cell><cell>0.815</cell><cell>0.560</cell></row><row><cell>T5 (zero-shot)</cell><cell>base</cell><cell>0.000</cell><cell>0.003</cell></row><row><cell>T5 (fine-tuned)</cell><cell>base</cell><cell>0.762</cell><cell>0.511</cell></row><row><cell>MolT5</cell><cell>base</cell><cell>0.769</cell><cell>0.540</cell></row><row><cell>Text+Chem T5</cell><cell>base</cell><cell>0.853</cell><cell>0.625</cell></row><row><cell></cell><cell cols="3">Size BLEU score ? Accuracy ?</cell></row><row><cell>RXN</cell><cell>-</cell><cell>0.850</cell><cell>0.608</cell></row><row><cell>T5 (fine-tuned)</cell><cell>small</cell><cell>0.953</cell><cell>0.856</cell></row><row><cell>Text+Chem T5</cell><cell>small</cell><cell>0.929</cell><cell>0.780</cell></row><row><cell cols="2">Text+Chem T5-augm small</cell><cell>0.926</cell><cell>0.780</cell></row><row><cell>Text+Chem T5</cell><cell>base</cell><cell>0.935</cell><cell>0.800</cell></row><row><cell cols="2">Text+Chem T5-augm base</cell><cell>0.943</cell><cell>0.829</cell></row><row><cell>B Additional Visualizations</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Accuracy</head><p>This plot compares the performance of three different models with different sizes (Text+Chem T5-base, Text+Chem T5-small, MolT5-base, MolT5-small, T5-base, and T5-small) on the task of converting captions to SMILES, using five different metrics: Accuracy, Morgan FTS, RDK FTS, BLEU, MACCS FTS. The models are compared by plotting their scores on the y-axis. The graph shows that our proposal, Text+Chem T5, performs the best on all metrics and improves with size, corroborating our hypothesis that joint learning on molecular and textual domains leveraging multitask learning is a powerful paradigm to bridge the gap between domains. . The performance of our multi-task model is compared to the best-known task-specific model across the five tasks investigated in this work is shown. Our multi-task model solves tasks in two modalities (text and chemistry) and can be applied to cross-modality tasks (text2mol, mol2text) as well as within modality tasks such as text2text (paragraph2actions task) or molecule2molecule (forward reaction prediction and retrosynthesis).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Forward Retrosynthesis</head><p>The molecule is a siderophore composed from ... Given the above description generate the described molecule in SMILES.</p><p>The molecule is a siderophore composed from ... Given the above description generate the described molecule in SMILES.</p><p>The reaction mixture was stirred at the same temperature .. The Text+Chem T5 pipeline is a multi-task, multidomain language model that can effectively translate between natural and chemical language. The model can solve a wide range of tasks, including language, chemical, and cross-domain tasks, without the need for task-specific fine-tuning or retraining. The chemical tasks that the model can solve are forward reaction prediction and retro-synthesis. The forward reaction task is about predicting the outcome of a chemical reaction based on the starting materials, and the retro-synthesis task is about predicting the starting materials required to synthesize a given chemical compound. The cross-domain tasks that the model can solve are text-to-molecule task (text-conditional de novo generation) and molecule-to-text task (molecular captioning). The text-to-molecule task is where the model takes a textual description of a molecule as an input and generates its SMILES representation. The molecule-to-text task is where the model takes a molecule represented as SMILES and generates its human-readable textual description. For the mono-domain, language task, we focus on paragraph-to-action, given a paragraph describing how to build a molecule, and output the actions required to obtain that result. The model leverages large, pre-trained single-domain models, such as T5 <ref type="bibr" target="#b28">(Raffel et al., 2020)</ref>, to solve all these tasks effectively. The pre-trained models serve as a good starting point for fine-tuning the target distribution of tasks.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Prompt templates</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">METEOR: An automatic metric for MT evaluation with improved correlation with human judgments</title>
		<author>
			<persName><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization</title>
		<meeting>the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization<address><addrLine>Ann Arbor, Michigan</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Regression transformer enables concurrent sequence regression and conditional generation for molecular language modeling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Born</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Manica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">a). Data-driven molecular design for discovery and synthesis of novel ligands: a case study on sars-cov-2</title>
		<author>
			<persName><forename type="first">J</forename><surname>Born</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Manica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cadow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Markert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Mill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Filipavicius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Janakarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cardinale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Laino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Mart?nez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning: Science and Technology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">25024</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Paccmannrl: De novo generation of hit-like anticancer molecules from transcriptomic data via reinforcement learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Born</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Manica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oskooei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cadow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Markert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Mart?nez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Iscience</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">102269</biblScope>
			<date type="published" when="2021">2021b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
	<note>Advances in neural information processing systems</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P D O</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Brockman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.03374</idno>
		<title level="m">Evaluating large language models trained on code</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Brahma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.11416</idno>
		<title level="m">Scaling instruction-finetuned language models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Reoptimization of mdl keys for use in drug discovery</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Durant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Leland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Nourse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and computer sciences</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1273" to="1280" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Honke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2204.11817</idno>
		<title level="m">Translation between molecules and natural language</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Text2mol: Crossmodal molecule retrieval with natural language queries</title>
		<author>
			<persName><forename type="first">C</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="595" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">W</forename><surname>Falcon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pytorch</forename><surname>The</surname></persName>
		</author>
		<author>
			<persName><surname>Lightning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Py-Torch Lightning</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Complexity-based prompting for multi-step reasoning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Khot</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.00720</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Highly accurate protein structure prediction with alphafold</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jumper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Figurnov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tunyasuvunakool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>??dek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Potapenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">596</biblScope>
			<biblScope unit="issue">7873</biblScope>
			<biblScope unit="page" from="583" to="589" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Binary codes capable of correcting deletions, insertions, and reversals</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">I</forename><surname>Levenshtein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Soviet physics doklady</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="707" to="710" />
			<date type="published" when="1966">1966</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Solving quantitative reasoning problems with language models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lewkowycz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Andreassen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Michalewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ramasesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Slone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Schlag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gutman-Solo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.14858</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><surname>Li?vin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Hother</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.08143</idno>
		<title level="m">Can large language models reason about medical questions? arXiv preprint</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text summarization branches out</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unified deep learning model for multitask reaction predictions with explanation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal of Chemical Information and Modeling</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="1376" to="1387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Manica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cadow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Christofidellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Born</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">G N</forename><surname>Teukam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Buchan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chenthamarakshan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.03928</idno>
		<title level="m">Gt4sd: Generative toolkit for scientific discovery</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Nextmove software pistachio</title>
		<author>
			<persName><surname>Nextmove</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ray</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.02155</idno>
		<title level="m">Training language models to follow instructions with human feedback</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Ai-driven robotic laboratories show promise</title>
		<author>
			<persName><forename type="first">S</forename><surname>O'neill</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">1016</biblScope>
		</imprint>
	</monogr>
	<note>Engineering, 7(1351.10</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fr?chet chemnet distance: a metric for generative models for molecules in drug discovery</title>
		<author>
			<persName><forename type="first">K</forename><surname>Preuer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Renz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Klambauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1736" to="1741" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Hierarchical text-conditional image generation with clip latents</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.06125</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Extended-connectivity fingerprints</title>
		<author>
			<persName><forename type="first">D</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="742" to="754" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Photorealistic text-toimage diffusion models with deep language understanding</title>
		<author>
			<persName><forename type="first">C</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Whang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K S</forename><surname>Ghasemipour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K</forename><surname>Ayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Mahdavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Lopes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.11487</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Multitask prompted training enables zero-shot task generalization</title>
		<author>
			<persName><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sutawika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Alyafeai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chaffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stiegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Raja</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.08207</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">found in translation&quot;: predicting outcomes of complex organic chemistry reactions using neural sequence-to-sequence models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Schwaller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gaudin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lanyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bekas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Laino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemical science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">28</biblScope>
			<biblScope unit="page" from="6091" to="6098" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Molecular transformer: a model for uncertainty-calibrated chemical reaction prediction</title>
		<author>
			<persName><forename type="first">P</forename><surname>Schwaller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Laino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gaudin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bolgar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Hunter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bekas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACS central science</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1572" to="1583" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Predicting retrosynthetic pathways using transformer-based models and a hyper-graph exploration strategy</title>
		<author>
			<persName><forename type="first">P</forename><surname>Schwaller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Petraglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Zullo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">H</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Haeuselmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pisoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bekas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Iuliano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Laino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemical science</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3316" to="3325" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Mapping the space of chemical reactions using attention-based neural networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Schwaller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Probst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Vaucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">H</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kreutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Laino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Reymond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="144" to="152" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Elementary mathematical theory of classification and prediction</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Tanimoto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1958">1958</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Unassisted noise reduction of chemical reaction datasets</title>
		<author>
			<persName><forename type="first">A</forename><surname>Toniato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Schwaller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cardinale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Geluykens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Laino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="485" to="494" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Automated extraction of chemical synthesis actions from experimental procedures</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Vaucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zipoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Geluykens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">H</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Schwaller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Laino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.11903</idno>
		<title level="m">Chain of thought prompting elicits reasoning in large language models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Language models are few-shot multilingual learners</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">I</forename><surname>Winata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.07684</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<title level="m">Transformers: State-of-the-Art Natural Language Processing</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
