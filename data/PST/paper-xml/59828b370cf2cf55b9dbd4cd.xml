<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Automatic Image-Based Plant Disease Severity Estimation Using Deep Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-07-05">5 July 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Guan</forename><surname>Wang</surname></persName>
							<email>wangjx@bjfu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Technology</orgName>
								<orgName type="institution">Beijing Forestry University</orgName>
								<address>
									<postCode>100083</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Technology</orgName>
								<orgName type="institution">Beijing Forestry University</orgName>
								<address>
									<postCode>100083</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jianxin</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Technology</orgName>
								<orgName type="institution">Beijing Forestry University</orgName>
								<address>
									<postCode>100083</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Automatic Image-Based Plant Disease Severity Estimation Using Deep Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-07-05">5 July 2017</date>
						</imprint>
					</monogr>
					<idno type="MD5">4F27BB0FBF409D35F8AE7A32F426CF13</idno>
					<idno type="DOI">10.1155/2017/2917536</idno>
					<note type="submission">Received 8 March 2017; Revised 9 May 2017; Accepted 4 June 2017;</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T10:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automatic and accurate estimation of disease severity is essential for food security, disease management, and yield loss prediction. Deep learning, the latest breakthrough in computer vision, is promising for fine-grained disease severity classification, as the method avoids the labor-intensive feature engineering and threshold-based segmentation. Using the apple black rot images in the PlantVillage dataset, which are further annotated by botanists with four severity stages as ground truth, a series of deep convolutional neural networks are trained to diagnose the severity of the disease. The performances of shallow networks trained from scratch and deep models fine-tuned by transfer learning are evaluated systemically in this paper. The best model is the deep VGG16 model trained with transfer learning, which yields an overall accuracy of 90.4% on the hold-out test set. The proposed deep learning model may have great potential in disease control for modern agriculture.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The plant diseases are a major thread to losses of modern agricultural production. Plant disease severity is an important parameter to measure disease level and thus can be used to predict yield and recommend treatment. The rapid, accurate diagnosis of disease severity will help to reduce yield losses <ref type="bibr" target="#b0">[1]</ref>. Traditionally, plant disease severity is scored with visual inspection of plant tissue by trained experts. The expensive cost and low efficiency of human disease assessment hinder the rapid development of modern agriculture <ref type="bibr" target="#b1">[2]</ref>. With the population of digital cameras and the advances in computer vision, the automated disease diagnosis models are highly demanded by precision agriculture, high-throughput plant phenotype, smart green house, and so forth.</p><p>Inspired by the deep learning breakthrough in imagebased plant disease recognition, this work proposes deep learning models for image-based automatic diagnosis of plant disease severity. We further annotate the apple healthy and black rot images in the public PlantVillage dataset <ref type="bibr" target="#b2">[3]</ref> with severity labels. To explore the best network architecture and training mechanism, we train shallow networks of different depth from scratch and fine-tune the pretrained state-ofthe-art deep networks. The models' capabilities of correctly predicting the disease severity stage are compared. The best model achieves an accuracy of 90.4% on the hold-out test set. Our results are a first step towards the automatic plant disease severity diagnosis.</p><p>An overview of the rest of the paper is as follows: Section 2 reviews the literature in this area, Section 3 presents the deep learning proposal, Section 4 describes the methodology, Section 5 presents achieved results and related discussions, and, finally, Section 6 holds our conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Various studies have found that image-based assessment approaches produce more accurate and reproducible results than those obtained by human visual assessments. <ref type="bibr">Stewart and McDonald [4]</ref> used an automated image analysis method to analyze disease symptoms of infected wheat leaves caused by Zymoseptoria tritici. This method enabled the quantification of pycnidia size and density, along with other traits and their correlation, which provided greater accuracy and precision compared with human visual estimates of virulence. <ref type="bibr">Barbedo [5]</ref> designed an image segmentation method to measure disease severity in white/black background, which eliminated the possibility of human error and reduced time taken to measure disease severity. Atoum et al. <ref type="bibr" target="#b5">[6]</ref> proposed a novel computer vision system, Cercospora Leaf Spot (CLS) Rater, to accurately rate plant images in the real field to the United States Department of Agriculture (USDA) scale. The CLS Rater achieved a much higher consistency than the rating standard deviation of human experts. Many of these image-based assessment approaches for plant diseases share the same basic procedure <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref>. Firstly, preprocessing techniques are employed to remove the background and segment the lesion tissue of infected plants. After that, discriminative features are extracted for further analysis. At last, supervised classification algorithms or unsupervised cluster algorithms are used to classify features according to the specific task. Along with advances in computer science, many interactive tools are developed. The Assess <ref type="bibr" target="#b13">[14]</ref> is the most commonly used and also the discipline-standard program to estimate disease severity. The Leaf Doctor app <ref type="bibr" target="#b14">[15]</ref>, developed as an interactive smartphone application, can be used on color images to distinguish lesion areas from healthy tissues and calculate percentage of disease severity. The application achieved even higher accuracy than the Assess.</p><p>But these aforementioned plant disease severity estimation approaches are semiautomatic because they depend heavily on series of image-processing technologies, such as the threshold-based segmentation of the lesion area and hand-engineered features extraction. There is usually great variance in color both between lesions of different diseases and between lesions from the same disease at different stages. Therefore, it is very difficult to determine the appropriate segmentation threshold for plant disease images without human assistance. What is more, the time consuming hand-crafted feature extraction should be performed again for new style images. To the best of our knowledge, completely automatic image-based plant disease severity estimation method using computer vision has not yet been reported.</p><p>The deep learning approach leads a revolution in speech recognition <ref type="bibr" target="#b15">[16]</ref>, visual object recognition <ref type="bibr" target="#b16">[17]</ref>, object detection <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>, and many other domains such as drug discovery <ref type="bibr" target="#b19">[20]</ref>, genomics <ref type="bibr" target="#b20">[21]</ref>, and building reorganization <ref type="bibr" target="#b21">[22]</ref>. Deep learning is very promising for automatically grading plant disease severity. Recently, there have been some works using deep learning method for plant species identification and plant disease identification. The recent years of the well-known annual plant species identification campaigns PlantCLEF <ref type="bibr" target="#b22">[23]</ref> were performance-wise dominated by deep learning methods. Choi <ref type="bibr" target="#b23">[24]</ref> won the PlantCLEF 2015 by using the deep learning model GoogleNet <ref type="bibr" target="#b24">[25]</ref> to classify 1000 species. Mehdipour Ghazi et al. <ref type="bibr" target="#b25">[26]</ref> combined the outputs of GoogleNet and VGGNet <ref type="bibr" target="#b26">[27]</ref> and surpassed the overall validation accuracy of <ref type="bibr" target="#b23">[24]</ref>. Hang et al. <ref type="bibr" target="#b27">[28]</ref> won the PlantCLEF 2016 by the enhanced VGGNet model. For plant disease identification, Sladojevic et al. <ref type="bibr" target="#b29">[29]</ref> created a dataset with more than 3,000 images collected from the Internet and trained a deep convolutional network to recognize 13 different types of plant diseases out of healthy leaves. Mohanty et al. <ref type="bibr" target="#b30">[30]</ref> used a public dataset PlantVillage <ref type="bibr" target="#b2">[3]</ref> consisting of 54,306 images of diseased and healthy plant leaves collected under controlled conditions and trained a deep convolutional neural network to identify 14 crop species and 26 diseases.</p><p>In comparison with classification among different diseases, the fine-grained disease severity classification is much more challenging, as there exist large intraclass similarity and small interclass variance <ref type="bibr" target="#b31">[31]</ref>. Deep learning avoids the laborintensive feature engineering and threshold-based segmentation <ref type="bibr" target="#b32">[32]</ref>, which is promising for fine-grained disease severity classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Deep Learning Proposal</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Deep Convolutional Neural Network.</head><p>To explore the best convolutional neural network architecture for the finegrained disease severity classification problem with few training data, we compare two architectures, namely, building a shallow network from scratch and transfer learning by finetuning the top layers of a pretrained deep network.</p><p>The shallow networks consist of only few convolutional layers with few filters per layer, followed by two fully connected layers, and end with a softmax normalization. We train shallow networks of 2, 4, 6, 8, and 10 convolutional layers. Each convolutional layer has 32 filters of size 3 × 3, a Rectified Linear Units (ReLU) activation, and all layers are followed by a 2 × 2 max-pooling layer, except for the last convolutional layer, which has 64 filters. The first fully connected layer has 64 units with a ReLU activation and is followed by a dropout layer with a dropout ratio of 50%. The last fully connected layer has 4 outputs, corresponding with the 4 classes, which feed into the softmax layer to calculate the probability output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Transfer Learning.</head><p>It is notable that the amount of images we can learn from is quite limited. Transfer learning is a useful approach to build powerful classification network using few data, by fine-tuning the parameters of a network pretrained on a large dataset, such as ImageNet <ref type="bibr" target="#b25">[26]</ref>. Although the disease severity classification is targeted for finer grained image category classification problem compared to the ImageNet, the lower layers only encode simple features, which can be generalized to most computer vision tasks. For example, the first layer only represents direction and color, and the visualization of activations in the first layer of VGG16 model is shown in Figure <ref type="figure" target="#fig_0">1</ref>. Though not trained on the plant disease dataset, the model can be activated against the diseased spots, the leaf, and the background.</p><p>For transfer learning, we compare the VGGNet <ref type="bibr" target="#b26">[27]</ref>, Inception-v3 <ref type="bibr" target="#b33">[33]</ref>, and ResNet50 <ref type="bibr" target="#b16">[17]</ref> architectures. VGGNet and the original Inception architecture GoogleNet yielded similar high performance in the 2014 ImageNet Large Scale Visual Recognition Challenge (ILSVRC), and ResNet won the first place of the challenge in 2016. The VGGNet involves 16 (VGG16) and 19 (VGG19) weight layers and shows a significant improvement on prior configurations by using an architecture with very small convolution filters. The original Inception architecture GoogleNet combines the network-innetwork approach and the strategy of using a series of filters of different sizes to handle multiple scales. The Inception-v3 is an improved Inception architecture which can be scaled up with high computational efficiency and low parameter count. ResNet is built up by stacking residual building blocks. Each building block is composed of several convolutional </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Material and Experiment</head><p>4.1. Data Material. The PlantVillage is an open access database of more than 50,000 images of healthy and diseased crops, which have a spread of 38 class labels. We select the images of healthy apple leaves and images of apple leaf black rot caused by the fungus Botryosphaeria obtusa. Each image is assessed into one class by botanists: healthy stage, early stage, middle stage, or end stage. The healthy-stage leaves are free of spots. The early-stage leaves have small circular spots with diameters less than 5 mm. The middle-stage leaves have more than 3 spots with at least one frog-eye spot enlarging to irregular or lobed shape. The end-stage leaves are so heavily infected that will drop from the tree. Each image is examined by agricultural experts and labeled with appropriate disease severity. 179 images which are inconsistent among experts are abandoned. Figure <ref type="figure" target="#fig_2">2</ref> shows some examples of every stage. Finally, we get 1644 images of healthy leaves, 137 early-stage, 180 middle-stage, and 125 end-stage disease images.</p><p>As healthy leaves are much more than the diseased leaves, there is much difference in the number of samples per class. The number of samples per class should be balanced to reduce the bias the network may have towards the healthy-stage class with more samples. Our strategy of balancing is as follows: for early stage, middle stage, and end stage, about 80% of the images are used as the training set and the left 20% are the hold-out test set. For healthy-stage leaves, the images are divided into 12 clusters, with 110 images in each cluster on average for training. 27 images are left for testing. The final accuracy is estimated by averaging over 12 runs on the clusters. As the PlantVillage dataset has multiple images of the same leaf taken from different orientations, all the images of the same leaf should be either in the training set or in the test set. Table <ref type="table" target="#tab_0">1</ref> shows the number of images used as training and test sets for each class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Image Preprocessing.</head><p>The samples in the PlantVillage dataset are arbitrarily sized RGB images. Thanks to the powerful end-to-end learning, deep learning models only need 4 basic image preprocessing steps. Images are processed according to the following stages: firstly, we resize all the images to 256 × 256 pixels for shallow networks, 224 × 224 for VGG16, VGG19, and ResNet50, and 299 × 299 for Inception-V3. We perform both the model optimization and prediction on these rescaled images. Secondly, all pixel values are divided by 255 to be compatible with the network's initial values. Thirdly, sample-wise normalization is performed. Normalization can significantly improve the efficiency of end-toend training. The normalization is performed as follows: for each input 𝑥, we calculate the mean value 𝑚 𝑥 and standard deviation 𝑠 𝑥 and then transform the input to 𝑥 󸀠 = (𝑥 -𝑚 𝑥 )/𝑠 𝑥 , so that the individual features more or less look like standard normally distributed data with zero mean and unit variance. Finally, several random augmentations including random rotation, shearing, zooming, and flipping are applied to the training images. The augmentation prevents overfitting and makes the model generalize better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Neural Network Training Algorithm.</head><p>The basic architecture in the convolutional neural network begins with several convolutional layers and pooling layers, followed by fully  ReLU represents the rectified linear function ReLU(𝑥) = max(0, 𝑥), which is used as the activation function in our models, as deep convolutional neural networks with ReLUs train several times faster than their equivalents with saturating nonlinearities.</p><p>A max-pooling layer computes the maximum value over nonoverlapping rectangular regions of the outputs of each convolution kernel. The pooling operation enables position invariance over larger local regions and reduces the output size.</p><p>Fully connected layers are added on top of the final convolutional layer. Each fully connected layer computes ReLU(𝑊 fc 𝑋), where 𝑋 is the input and 𝑊 fc is the weight matrix for the fully connected layer.</p><p>The loss function measures the discrepancy between the predicted result and the label of the input, which is defined as the sum of cross entropy:</p><formula xml:id="formula_0">𝐸 (𝑊) = - 1 𝑛 𝑛 ∑ 𝑥 𝑖 =1 𝐾 ∑ 𝑘=1 [𝑦 𝑖𝑘 log 𝑃 (𝑥 𝑖 = 𝑘) + (1 -𝑦 𝑖𝑘 ) log (1 -𝑃 (𝑥 𝑖 = 𝑘))] ,<label>(2)</label></formula><p>where 𝑊 indicates the weight matrixes of convolutional and fully connected layers, n indicates the number of training samples, i is the index of training samples, and 𝑘 is the index of classes. 𝑦 𝑖𝑘 = 1 if the 𝑖th sample belongs to the kth class; else 𝑦 𝑖𝑘 = 0. 𝑃(𝑥 𝑖 = 𝑘) is the probability of input 𝑥 𝑖 belonging to the kth class that the model predicts, which is a function of parameters 𝑊. So the loss function takes 𝑊 as its parameters.</p><p>Network training aims to find the value of 𝑊 that minimizes the loss function 𝐸. We use gradient descent algorithm where 𝑊 is iteratively updated as</p><formula xml:id="formula_1">𝑊 𝑘 = 𝑊 𝑘-1 -𝛼 𝜕𝐸 (𝑊) 𝜕𝑊 ,<label>(3)</label></formula><p>where 𝛼 is the learning rate, which is a very important parameter that determines the step size of the learning. The value of learning rate should be carefully evaluated. We use early stopping as the training stop strategy to stop training when the network begins to overfit the data. The performance of the network is evaluated at the end of each epoch using the test set. If the loss value of the test set stops improving, the network will stop training.</p><p>To prevent overfitting, the transfer learning is conducted as follows: fully connected layers are replaced with a new one and only fine-tune the top convolutional block for VGG16 and VGG19, the top two inception blocks for Inception-v3, and the top residual block for ResNet50, along with the new fully connected layers. To avoid triggering large gradient updates to destroy the pretrained weights, the new fully connected network should be initialized with proper values rather than with random values. So firstly we freeze all layers except the new fully connected network. The new fully connected network is trained on the output features of the final convolutional layer. The weights learned from training are initial values for fine-tuning. After that, the top convolutional block for VGG16 and VGG19, the top two inception blocks for Inception-v3, and the top residual block for ResNet50 are unfreezed and then trained along with the new fully connected network with a small learning rate.</p><p>The parameters for training shallow networks and finetuning pretrained models are presented in Table <ref type="table" target="#tab_1">2</ref>. Besides, a learning rate schedule is employed. The initial learning rate is dropped by a factor of 10 every 50 epochs for training shallow networks with less than 6 convolutional layers and finetuning deep networks. And it dropped by 10 every 100 epochs for shallow networks with 6 or more convolutional layers. Because the network goes deeper, it needs more training steps to converge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Implementation. The experiment is performed on an</head><p>Ubuntu workstation equipped with one Intel Core i5 6500 CPU (16 GB RAM), accelerated by one GeForce GTX TITAN X GPU <ref type="bibr">(12 GB memory)</ref>. The model implementation is powered by the Keras deep learning framework with the Theano backend.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Result and Discussion</head><p>Figure <ref type="figure" target="#fig_3">3</ref> shows the training and testing accuracies of shallow networks trained from scratch. Each bar represents the average result of 12 runs. Both training and test accuracies improve slightly with the depth of the model at first. The best performance, that is, a test accuracy of 79.3%, is achieved by the network with 8 convolutional layers. But the accuracies fall when the network's depth exceeds 8, as there are insufficient training data for models with too many parameters. To circumvent this problem, transfer learning is applied to the state-of-the-art deep models.</p><p>The results of fine-tuning the ImageNet pretrained models are reported in Figure <ref type="figure">4</ref>. Each bar represents the average result of 12 runs. The overall accuracy on the test set we The confusion matrix of the VGG16 model on the holdout test set is shown in Table <ref type="table" target="#tab_2">3</ref>. The fraction of accurately predicted images for each of the four stages is displayed in detail. All of the healthy-stage leaves are correctly classified.</p><p>The accuracies of early stage and end stage are 93.1% and 87.0%, respectively. Middle stage is prone to be misclassified, with an accuracy of 83.3%. However, the misclassified stages are only confused with their adjacent stages. For example, the early stage is only confused with the middle stage, and none of early-stage is classified as end stage.</p><p>From the results displayed in Figure <ref type="figure">4</ref>, it is notable that the training accuracies of deep networks are close to 100% and trigger the early stopping. Since deep learning is data-driven, training on more data will further increase the test accuracy. It is also important to note that the best performance is achieved by the VGGNet. The result is consistent with that of <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b27">28]</ref>, where the VGGNet showed better performance in the PlantCLEF plant identification task. Though ResNet achieved state-of-the-art result on the ImageNet dataset, it performs poorer than VGGNet on fine-grained classification tasks. The SGD optimizer might put the residual mapping in building blocks of ResNet to zero too early, which leads to a local optimization and results in the poor generalization in fine-grained classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>This work proposes a deep learning approach to automatically discover the discriminative features for fine-grained classification, which enables the end-to-end pipeline for diagnosing plant disease severity. Based on few training samples, we trained small convolutional neural networks of different depth from scratch and fine-tuned four stateof-the-art deep models: VGG16, VGG19, Inception-v3, and ResNet50. Comparison of these networks reveals that finetuning on pretrained deep models can significantly improve the performance on few data. The fine-tuned VGG16 model performs best, achieving an accuracy of 90.4% on the test set, demonstrating that deep learning is the new promising technology for fully automatic plant disease severity classification.</p><p>In future work, more data at different stages of different diseases will be collected with versatile sensors, like infrared camera and multispectral camera. The deep learning model can be associated with treatment recommendation, yield prediction, and so on.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Visualization of activations for an input image in the first convolutional layer of the pretrained VGG16 model: (a) original image; (b) the first convolutional layer output.</figDesc><graphic coords="3,84.96,259.16,170.77,170.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Sample leaf images of the four stages of apple black rot: (a) healthy stage, (b) early stage, (c) middle stage, and (d) end stage.</figDesc><graphic coords="4,115.14,496.58,368.53,120.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Accuracies of shallow networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The number of samples in training and test sets.</figDesc><table><row><cell>Class</cell><cell>Number of images for training</cell><cell>Number of images for testing</cell></row><row><cell>Healthy stage</cell><cell>110 × 12</cell><cell>27 × 12</cell></row><row><cell>Early stage</cell><cell>108</cell><cell>29</cell></row><row><cell>Middle stage</cell><cell>144</cell><cell>36</cell></row><row><cell>End stage</cell><cell>102</cell><cell>23</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The hyperparameters of</figDesc><table><row><cell>Parameters</cell><cell>Learning from scratch</cell><cell>Transfer learning Training fully connected layers</cell><cell>Fine-tuning</cell></row><row><cell>Training algorithm</cell><cell>SGD</cell><cell>RMSP</cell><cell>SGD</cell></row><row><cell>Learning rate</cell><cell>0.01</cell><cell>0.01</cell><cell>0.0001</cell></row><row><cell>Batch size</cell><cell></cell><cell>32</cell><cell></cell></row><row><cell>Early stopping</cell><cell></cell><cell>10 epochs</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Confusion matrix for the prediction of VGG16 model trained with transfer learning.</figDesc><table><row><cell>Predicted</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>(a) (b)</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the Fundamental Research Funds for the Central Universities: 2017JC02 and TD2014-01.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflicts of Interest</head><p>The authors declare that there are no conflicts of interest regarding the publication of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Authors' Contributions</head><p>Yu Sun and Guan Wang contributed equally to this work.</p><p>Submit your manuscripts at https://www.hindawi.com</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Computer Games Technology</head><p>International Journal of </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Plant disease severity estimated visually, by digital photography and image analysis, and by hyperspectral imaging</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Bock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Parker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gottwald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Critical Reviews in Plant Sciences</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="59" to="107" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Image-based phenotyping of plant disease symptoms</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Mutka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Bart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Plant Science</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">734</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Plant Polyphenols, Prenatal Development and Health Outcomes</title>
	</analytic>
	<monogr>
		<title level="j">Biological Systems: Open Access</title>
		<imprint>
			<biblScope unit="volume">03</biblScope>
			<biblScope unit="issue">01</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Measuring quantitative virulence in the wheat pathogen zymoseptoria tritici using high-throughput automated image analysis</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phytopathology</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="985" to="992" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An automatic method to detect and measure leaf disease symptoms using digital image processing</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G A</forename><surname>Barbedo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Plant Disease</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1709" to="1716" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On developing and enhancing plant-level disease rating systems in real fields</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Atoum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Afridi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Mcgrath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Hanson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="287" to="299" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Identification of alfalfa leaf diseases using image recognition technology</title>
		<author>
			<persName><forename type="first">F</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>Article ID e0168274</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast and Accurate Detection and Classification of Plant Diseases</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Al</forename><surname>Hiary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Reyalat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Braik</surname></persName>
		</author>
		<author>
			<persName><surname>Alrahamneh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Applications</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="38" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Potential of radial basis function-based support vector regression for apple disease detection</title>
		<author>
			<persName><forename type="first">E</forename><surname>Omrani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Khoshnevisan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shamshirband</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Saboohi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">B</forename><surname>Anuar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H N M</forename><surname>Nasir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Measurement: Journal of the International Measurement Confederation</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="512" to="519" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Integrating SOMs and a Bayesian Classifier for Segmenting Diseased Plants in Uncontrolled Environments</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Hernández-Rabadán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ramos-Quintana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Guerrero</forename><surname>Juk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific World Journal</title>
		<imprint>
			<biblScope unit="volume">2014</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>Article ID 214674</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Comparison of perceptual color spaces for natural image segmentation tasks</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">E</forename><surname>Correa-Tome</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optical Engineering</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">117203</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Probabilistic classification of disease symptoms caused by Salmonella on Arabidopsis plants, presented at the GI Jahrestagung</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schikora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schikora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Kogel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GI Jahrestagung</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>Probabilistic classification of disease symptoms caused by Salmonella on Arabidopsis plants</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A new automatic method for disease symptom segmentation in digital photographs of plant leaves</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G A</forename><surname>Barbedo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Journal of Plant Pathology</title>
		<imprint>
			<biblScope unit="volume">147</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="349" to="364" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Lamari</surname></persName>
		</author>
		<title level="m">Assess: Image Analysis software helpdesk, Version 2</title>
		<imprint>
			<publisher>APS Press</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Leaf doctor: A new portable application for quantifying plant disease severity</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Pethybridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Nelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Plant Disease</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1310" to="1316" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition: the shared views of four research groups</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR &apos;16)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR &apos;16)<address><addrLine>Las Vegas, Nev, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06">June 2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">FAST-MDL: Fast Adaptive Supervised Training of multi-layered deep learning models for consistent object tracking and classification</title>
		<author>
			<persName><forename type="first">N</forename><surname>Doulamis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Voulodimos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 IEEE International Conference on Imaging Systems and Techniques</title>
		<meeting>the 2016 IEEE International Conference on Imaging Systems and Techniques<address><addrLine>IST</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-10">2016. October 2016</date>
			<biblScope unit="page" from="318" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep Learning in Drug Discovery</title>
		<author>
			<persName><forename type="first">E</forename><surname>Gawehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Hiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Molecular Informatics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="14" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Predicting the sequence specificities of DNA-and RNA-binding proteins by deep learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Alipanahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Delong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Weirauch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Frey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Biotechnology</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="831" to="838" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Recognizing Buildings through Deep Learning: A Case Study on Halftimbered Framed Buildings in Calw City</title>
		<author>
			<persName><forename type="first">K</forename><surname>Makantasis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Doulamis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Voulodimos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Special Session on Computer Vision, Imaging and Computer Graphics for Cultural Applications</title>
		<meeting>the Special Session on Computer Vision, Imaging and Computer Graphics for Cultural Applications<address><addrLine>Porto, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="444" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">LifeCLEF plant identification task 2015</title>
		<author>
			<persName><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference and Labs of the Evaluation Forum (CLEF &apos;15)</title>
		<meeting>the Conference and Labs of the Evaluation Forum (CLEF &apos;15)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Plant identification with deep convolutional neural network: SNUMedinfo at LifeCLEF plant identification task 2015</title>
		<author>
			<persName><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference and Labs of the Evaluation Forum</title>
		<meeting>the 16th Conference and Labs of the Evaluation Forum</meeting>
		<imprint>
			<date type="published" when="2015-09">2015. September 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR &apos;15)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR &apos;15)<address><addrLine>Boston, Mass, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06">June 2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Plant identification using deep neural networks via optimization of transfer learning parameters</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Mehdipour</forename><surname>Ghazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yanikoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Aptoula</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">235</biblScope>
			<biblScope unit="page" from="228" to="235" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1409.1556" />
		<title level="m">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Open world plant image identification based on convolutional neural network</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Hang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aono</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Computational Intelligence and Neuroscience</title>
		<meeting>the Computational Intelligence and Neuroscience</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m">Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA)</title>
		<meeting><address><addrLine>Jeju, South Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12">December 2016</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep Neural Networks Based Recognition of Plant Diseases by Leaf Image Classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sladojevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Arsenovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anderla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Culibrk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stefanovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Intelligence and Neuroscience</title>
		<imprint>
			<biblScope unit="volume">2016</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>Article ID 3289801</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Using deep learning for image-based plant disease detection</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Mohanty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Salathé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Plant</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">1419</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Hyper-class augmented and regularized deep learning for fine-grained image classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015-06">2015. June 2015</date>
			<biblScope unit="page" from="2645" to="2654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 2016 IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016-07">2016. July 2016</date>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
