<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Meta-Weight-Net: Learning an Explicit Mapping For Sample Weighting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jun</forename><surname>Shu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Qi</forename><surname>Xie</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Lixuan</forename><surname>Yi</surname></persName>
							<email>yilixuan@stu.xjtu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Qian</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sanping</forename><surname>Zhou</surname></persName>
							<email>sanpingzhou@stu.xjtu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Zongben</forename><surname>Xu</surname></persName>
							<email>zbxu@mail.xjtu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Deyu</forename><surname>Meng</surname></persName>
							<email>dymeng@mail.xjtu.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Meta-Weight-Net: Learning an Explicit Mapping For Sample Weighting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">83B786C8C5FF847AB558DA18116F6631</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Current deep neural networks (DNNs) can easily overfit to biased training data with corrupted labels or class imbalance. Sample re-weighting strategy is commonly used to alleviate this issue by designing a weighting function mapping from training loss to sample weight, and then iterating between weight recalculating and classifier updating. Current approaches, however, need manually pre-specify the weighting function as well as its additional hyper-parameters. It makes them fairly hard to be generally applied in practice due to the significant variation of proper weighting schemes relying on the investigated problem and training data. To address this issue, we propose a method capable of adaptively learning an explicit weighting function directly from data. The weighting function is an MLP with one hidden layer, constituting a universal approximator to almost any continuous functions, making the method able to fit a wide range of weighting functions including those assumed in conventional research. Guided by a small amount of unbiased meta-data, the parameters of the weighting function can be finely updated simultaneously with the learning process of the classifiers. Synthetic and real experiments substantiate the capability of our method for achieving proper weighting functions in class imbalance and noisy label cases, fully complying with the common settings in traditional methods, and more complicated scenarios beyond conventional cases. This naturally leads to its better accuracy than other state-of-the-art methods. Source code is available at https://github.com/xjtushujun/meta-weight-net. * Corresponding author. 1 We call the training data biased when they are generated from a joint sample-label distribution deviating from the distribution of evaluation/test set <ref type="bibr" target="#b0">[1]</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>DNNs have recently obtained impressive good performance on various applications due to their powerful capacity for modeling complex input patterns. However, DNNs can easily overfit to biased training data 1 , like those containing corrupted labels <ref type="bibr" target="#b1">[2]</ref> or with class imbalance <ref type="bibr" target="#b2">[3]</ref>, leading to their poor performance in generalization in such cases. This robust deep learning issue has been theoretically illustrated in multiple literatures <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>.</p><p>In practice, however, such biased training data are commonly encountered. For instance, practically collected training samples always contain corrupted labels <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref>. A typical example is a dataset roughly collected from a crowdsourcing system <ref type="bibr" target="#b17">[18]</ref> or search engines <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>, which would possibly yield a large amount of noisy labels. Another popular type of biased training data is those with class imbalance. Real-world datasets are usually depicted as skewed distributions, with a long-tailed configuration. A few classes account for most of the data, while most classes are under-represented. Effective learning with these biased training data, which is regarded to be biased from evaluation/test ones, is thus an important while challenging issue in machine learning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b20">21]</ref>.</p><p>Sample reweighting approach is a commonly used strategy against this robust learning issue. The main methodology is to design a weighting function mapping from training loss to sample weight (with hyper-parameters), and then iterates between calculating weights from current training loss values and minimizing weighted training loss for classifier updating. There exist two entirely contradictive ideas for constructing such a loss-weight mapping. One makes the function monotonically increasing as depicted in Fig. <ref type="figure" target="#fig_0">1</ref>(a), i.e., enforce the learning to more emphasize samples with larger loss values since they are more like to be uncertain hard samples located on the classification boundary. Typical methods of this category include AdaBoost <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>, hard negative mining <ref type="bibr" target="#b23">[24]</ref> and focal loss <ref type="bibr" target="#b24">[25]</ref>. This sample weighting manner is known to be necessary for class imbalance problems, since it can prioritize the minority class with relatively higher training losses.</p><p>On the contrary, the other methodology sets the weighting function as monotonically decreasing, as shown in Fig. <ref type="figure" target="#fig_0">1(b)</ref>, to take samples with smaller loss values as more important ones. The rationality lies on that these samples are more likely to be high-confident ones with clean labels. Typical methods include self-paced learning(SPL) <ref type="bibr" target="#b25">[26]</ref>, iterative reweighting <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b16">17]</ref> and multiple variants <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref>. This weighting strategy has been especially used in noisy label cases, since it inclines to suppress the effects of samples with extremely large loss values, possibly with corrupted incorrect labels.</p><p>Although these sample reweighting methods help improve the robustness of a learning algorithm on biased training samples, they still have evident deficiencies in practice. On the one hand, current methods need to manually set a specific form of weighting function based on certain assumptions on training data. This, however, tends to be infeasible when we know little knowledge underlying data or the label conditions are too complicated, like the case that the training set is both imbalanced and noisy. On the other hand, even when we specify certain weighting schemes, like focal loss <ref type="bibr" target="#b24">[25]</ref> or SPL <ref type="bibr" target="#b25">[26]</ref>, they inevitably involve hyper-parameters, like focusing parameter in the former and age parameter in the latter, to be manually preset or tuned by cross-validation. This tends to further raise their application difficulty and reduce their performance stability in real problems.</p><p>To alleviate the aforementioned issue, this paper presents an adaptive sample weighting strategy to automatically learn an explicit weighting function from data. The main idea is to parameterize the weighting function as an MLP (multilayer perceptron) network with only one hidden layer (as shown in Fig. <ref type="figure" target="#fig_0">1(c</ref>)), called Meta-Weight-Net, which is theoretically a universal approximator for almost any continuous function <ref type="bibr" target="#b30">[31]</ref>, and then use a small unbiased validation set (meta-data) to guide the training of all its parameters. The explicit form of the weighting function can be finally attained specifically suitable to the learning task.</p><p>In summary, this paper makes the following three-fold contributions:</p><p>1) We propose to automatically learn an explicit loss-weight function, parameterized by an MLP from data in a meta-learning manner. Due to the universal approximation capability of this weight net, it can finely fit a wide range of weighting functions including those used in conventional research.</p><p>2) Experiments verify that the weighting functions learned by our method highly comply with manually preset weighting manners used in tradition in different training data biases, like class imbalance and noisy label cases as shown in Fig. <ref type="figure" target="#fig_0">1(d</ref>) and 1(e)), respectively. This shows that the weighting scheme learned by the proposed method inclines to help reveal deeper understanding for data bias insights, especially in complicated bias cases where the extracted weighting function is with complex tendencies (as shown in Fig. <ref type="figure" target="#fig_0">1(f)</ref>).</p><p>3) The insights of why the proposed method works can be well interpreted. Particularly, the updating equation for Meta-Weight-Net parameters can be explained by that the sample weights of those samples better complying with the meta-data knowledge will be improved, while those violating such meta-knowledge will be suppressed. This tallies with our common sense on the problem: we should reduce the influence of those highly biased ones, while emphasize those unbiased ones.</p><p>The paper is organized as follows. Section 2 presents the proposed meta-learning method as well as the detailed algorithm and analysis of its convergence property. Section 3 discusses related work. Section 4 demonstrates experimental results and the conclusion is finally made.</p><p>2 The Proposed Meta-Weight-Net Learning Method</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The Meta-learning Objective</head><p>Consider a classification problem with the training set {x i , y i } N i=1 , where x i denotes the i-th sample, y i ∈ {0, 1} c is the label vector over c classes, and N is the number of the entire training data. f (x, w) denotes the classifier, and w denotes its parameters. In current applications, f (x, w) is always set as a DNN. We thus also adopt DNN, and call it the classifier network for convenience in the following.</p><p>Generally, the optimal classifier parameter w * can be extracted by minimizing the loss 1 N N i=1 (y i , f (x i , w)) calculated on the training set. For notation convenience, we denote that L train i (w) = (y i , f (x i , w)). In the presence of biased training data, sample re-weighting methods enhance the robustness of training by imposing weight V(L train i (w); Θ) on the i-th sample loss, where V( ; Θ) denotes the weight net, and Θ represents the parameters contained in it. The optimal parameter w is calculated by minimizing the following weighted loss:</p><formula xml:id="formula_0">w * (Θ) = arg min w L train (w; Θ) 1 N N i=1 V(L train i (w); Θ)L train i (w).<label>(1)</label></formula><p>Meta-Weight-Net: Our method aims to automatically learn the hyper-parameters Θ in a metalearning manner. To this aim, we formulate V(L i (w); Θ) as a MLP network with only one hidden layer containing 100 nodes, as shown in Fig. <ref type="figure" target="#fig_0">1(c</ref>). We call this weight net as Meta-Weight-Net or MW-Net for easy reference. Each hidden node is with ReLU activation function, and the output is with the Sigmoid activation function, to guarantee the output located in the interval of [0, 1]. Albeit simple, this net is known as a universal approximator for almost any continuous function <ref type="bibr" target="#b30">[31]</ref>, and thus can fit a wide range of weighting functions including those used in conventional research.</p><p>Meta learning process. The parameters contained in MW-Net can be optimized by using the meta learning idea <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35]</ref>. Specifically, assume that we have a small amount unbiased meta-data set (i.e., with clean labels and balanced data distribution) {x</p><formula xml:id="formula_1">(meta) i , y<label>(meta) i</label></formula><p>} M i=1 , representing the meta-knowledge of ground-truth sample-label distribution, where M is the number of meta-samples and M N . The optimal parameter Θ * can be obtained by minimizing the following meta-loss:</p><p>Step 5</p><p>Step 6</p><p>Step 7</p><p>Meta-Weight-Net Classifier network ... </p><note type="other">Loss Weight</note><formula xml:id="formula_2">Θ * = arg min Θ L meta (w * (Θ)) 1 M M i=1 L meta i (w * (Θ)),<label>(2)</label></formula><p>where</p><formula xml:id="formula_3">L meta i (w) = y (meta) i , f (x (meta) i</formula><p>, w) is calculated on meta-data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The Meta-Weight-Net Learning Method</head><p>Calculating the optimal Θ * and w * require two nested loops of optimization. Here we adopt an online strategy to update Θ and w through a single optimization loop, respectively, to guarantee the efficiency of the algorithm.</p><p>Formulating learning manner of classifier network. As general network training tricks, we employ SGD to optimize the training loss (1). Specifically, in each iteration of training, a mini-batch of training samples {(x i , y i ), 1 ≤ i ≤ n} is sampled, where n is the mini-batch size. Then the updating equation of the classifier network parameter can be formulated by moving the current w (t) along the descent direction of the objective loss in Eq. ( <ref type="formula" target="#formula_0">1</ref>) on a mini-batch training data:</p><formula xml:id="formula_4">ŵ(t) (Θ) = w (t) -α 1 n × n i=1 V(L train i (w (t) ); Θ)∇ w L train i (w) w (t) ,<label>(3)</label></formula><p>where α is the step size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 The MW-Net Learning Algorithm</head><p>Input: Training data D, meta-data set D, batch size n, m, max iterations T . Output: Classifier network parameter w (T ) 1: Initialize classifier network parameter w (0) and Meta-Weight-Net parameter Θ (0) . 2:</p><formula xml:id="formula_5">for t = 0 to T -1 do 3: {x, y} ← SampleMiniBatch(D, n). 4:</formula><p>{x (meta) , y (meta) } ← SampleMiniBatch( D, m).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Formulate the classifier learning function ŵ(t) (Θ) by Eq. ( <ref type="formula" target="#formula_4">3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>Update Θ (t+1) by Eq. ( <ref type="formula">9</ref>). 7:</p><p>Update w (t+1) by Eq. ( <ref type="formula" target="#formula_7">5</ref>). 8: end for Updating parameters of Meta-Weight-Net: After receiving the feedback of the classifier network parameter updating formulation ŵ(t) (Θ)<ref type="foot" target="#foot_0">2</ref> from the Eq .( <ref type="formula" target="#formula_4">3</ref>), the parameter Θ of the Meta-Weight-Net can then be readily updated guided by Eq. ( <ref type="formula" target="#formula_2">2</ref>), i.e., moving the current parameter Θ (t) along the objective gradient of Eq. ( <ref type="formula" target="#formula_2">2</ref>) calculated on the meta-data:</p><formula xml:id="formula_6">Θ (t+1) = Θ (t) -β 1 m m i=1 ∇ Θ L meta i ( ŵ(t) (Θ)) Θ (t) ,<label>(4)</label></formula><p>where β is the step size.</p><p>Updating parameters of classifier network: Then, the updated Θ (t+1) is employed to ameliorate the parameter w of the classifier network, i.e.,</p><formula xml:id="formula_7">w (t+1) = w (t) -α 1 n × n i=1 V(L train i (w (t) ); Θ (t+1) )∇ w L train i (w) w (t) .<label>(5)</label></formula><p>The MW-Net Learning algorithm can then be summarized in Algorithm 1, and Fig. <ref type="figure" target="#fig_1">2</ref> illustrates its main implementation process (steps 5-7). All computations of gradients can be efficiently implemented by automatic differentiation techniques and generalized to any deep learning architectures of classifier network. The algorithm can be easily implemented using popular deep learning frameworks like PyTorch <ref type="bibr" target="#b35">[36]</ref>. It is easy to see that both the classifier network and the MW-Net gradually ameliorate their parameters during the learning process based on their values calculated in the last step, and the weights can thus be updated in a stable manner, as clearly shown in Fig. <ref type="figure">6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Analysis on the Weighting Scheme of Meta-Weight-Net</head><p>The computation of Eq. ( <ref type="formula">9</ref>) by backpropagation can be rewritten as <ref type="foot" target="#foot_1">3</ref> :</p><formula xml:id="formula_8">Θ (t+1) = Θ (t) + αβ n n j=1 1 m m i=1 G ij ∂V(L train j (w (t) ); Θ) ∂Θ Θ (t) ,<label>(6)</label></formula><p>where</p><formula xml:id="formula_9">G ij = ∂L meta i ( ŵ) ∂ ŵ T ŵ(t) ∂L train j (w) ∂w w (t) . Neglecting the coefficient 1 m m i=1 G ij ,</formula><p>it is easy to see that each term in the sum orients to the ascend gradient of the weight function V(L train j (w (t) ); Θ).</p><p>1 m m i=1 G ij , the coefficient imposed on the j-th gradient term, represents the similarity between the gradient of the j-th training sample computed on training loss and the average gradient of the mini-batch meta data calculated on meta loss. That means if the learning gradient of a training sample is similar to that of the meta samples, then it will be considered as beneficial for getting right results and its weight tends to be more possibly increased. Conversely, the weight of the sample inclines to be suppressed. This understanding is consistent with why well-known MAML works <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Convergence of the MW-Net Learning algorithm</head><p>Our algorithm involves optimization of two-level objectives, and therefore we show theoretically that our method converges to the critical points of both the meta and training loss function under some mild conditions in Theorem 1 and 2, respectively. The proof is listed in the supplementary material. Theorem 1. Suppose the loss function is Lipschitz smooth with constant L, and V(•) is differential with a δ-bounded gradient and twice differential with its Hessian bounded by B, and the loss function have ρ-bounded gradients with respect to training/meta data. Let the learning rate α t satisfies α t = min{1, k T }, for some k &gt; 0, such that k T &lt; 1, and β t , 1 ≤ t ≤ N is a monotone descent sequence,</p><formula xml:id="formula_10">β t = min{ 1 L , c σ √ T } for some c &gt; 0, such that σ √ T c ≥ L and ∞ t=1 β t ≤ ∞, ∞ t=1 β 2 t ≤ ∞. Then the proposed algorithm can achieve E[ ∇G(Θ (t) ) 2 2 ] ≤ in O(1/ 2 ) steps. More specifically, min 0≤t≤T E[ ∇L meta (Θ (t) ) 2 2 ] ≤ O( C √ T ),<label>(7)</label></formula><p>where C is some constant independent of the convergence process, and σ is the variance of drawing uniformly mini-batch sample at random. Theorem 2. The condions in Theorem 1 hold, then we have:</p><formula xml:id="formula_11">lim t→∞ E[ ∇L train (w (t) ; Θ (t+1) ) 2 2 ] = 0. (<label>8</label></formula><formula xml:id="formula_12">)</formula><p>3 Related Work Sample Weighting Methods. The idea of reweighting examples can be dated back to dataset resampling <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41]</ref> or instance re-weight <ref type="bibr" target="#b41">[42]</ref>, which pre-evaluates the sample weights as a preprocessing step by using certain prior knowledge on the task or data. To make the sample weights fit data more flexibly, more recent researchers focused on pre-designing a weighting function mapping from training loss to sample weight, and dynamically ameliorate weights during training process <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44]</ref>. There are mainly two manners to design the weighting function. One is to make it monotonically increasing, specifically effective in class imbalance case. Typical methods include the boosting algorithm (like AdaBoost <ref type="bibr" target="#b21">[22]</ref>) and multiple of its variations <ref type="bibr" target="#b44">[45]</ref>, hard example mining <ref type="bibr" target="#b23">[24]</ref> and focal loss <ref type="bibr" target="#b24">[25]</ref>, which impose larger weights to ones with larger loss values. On the contrary, another series of methods specify the weighting function as monotonically decreasing, especially used in noisy label cases. For example, SPL <ref type="bibr" target="#b25">[26]</ref> and its extensions <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref>, iterative reweighting <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b16">17]</ref> and other recent work <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b29">30]</ref>, pay more focus on easy samples with smaller losses. The limitation of these methods are that they all need to manually pre-specify the form of weighting function as well as their hyper-parameters, raising their difficulty to be readily used in real applications.</p><p>Meta Learning Methods. Inspired by meta-learning developments <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b49">50]</ref>, recently some methods were proposed to learn an adaptive weighting scheme from data to make the learning more automatic and reliable. Typical methods along this line include FWL <ref type="bibr" target="#b50">[51]</ref>, learning to teach <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b31">32]</ref> and MentorNet <ref type="bibr" target="#b20">[21]</ref> methods, whose weight functions are designed as a Bayesian function approximator, a DNN with attention mechanism, a bidirectional LSTM network, respectively. Instead of only taking loss values as inputs as classical methods, the weighting functions they used (i.e., the meta-learner), however, are with much more complex forms and required to input complicated information (like sample features). This makes them not only hard to succeed good properties possessed by traditional methods, but also to be easily reproduced by general users.</p><p>A closely related method, called L2RW <ref type="bibr" target="#b0">[1]</ref>, adopts a similar meta-learning mechanism compared with ours. The major difference is that the weights are implicitly learned there, without an explicit weighting function. This, however, might lead to unstable weighting behavior during training and unavailability for generalization. In contrast, with the explicit yet simple Meta-Weight-Net, our method can learn the weight in a more stable way, as shown in Fig. <ref type="figure">6</ref>, and can be easily generalized from a certain task to related other ones (see in the supplementary material).</p><p>Other Methods for Class Imbalance. Other methods for handling data imbalance include: <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b53">54]</ref> tries to transfer the knowledge learned from major classes to minor classes. The metric learning based methods have also been developed to effectively exploit the tailed data to improve the generalization ability, e.g., triple-header loss <ref type="bibr" target="#b54">[55]</ref> and range loss <ref type="bibr" target="#b55">[56]</ref>.</p><p>Other Methods for Corrupted Labels. For handling noisy label issue, multiple methods have been designed by correcting noisy labels to their true ones via a supplemental clean label inference step <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b14">15]</ref>. For example, GLC <ref type="bibr" target="#b14">[15]</ref> proposed a loss correction approach to mitigate the effects of label noise on DNN classifiers. Other methods along this line include the Reed <ref type="bibr" target="#b57">[58]</ref>, Co-training <ref type="bibr" target="#b15">[16]</ref>, D2L <ref type="bibr" target="#b58">[59]</ref> and S-Model <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><p>To evaluate the capability of the proposed algorithm, we implement experiments on data sets with class imbalance and noisy label issues, and real-world dataset with more complicated data bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Class Imbalance Experiments</head><p>We use Long-Tailed CIFAR dataset <ref type="bibr" target="#b59">[60]</ref>, that reduces the number of training samples per class according to an exponential function n = n i µ i , where i is the class index, n i is the original number of training images and µ ∈ (0, 1). The imbalance factor of a dataset is defined as the number of training samples in the largest class divided by the smallest. We trained ResNet-32 <ref type="bibr" target="#b60">[61]</ref> with softmax cross-entropy loss by SGD with a momentum 0.9, a weight decay 5×10 -4 , an initial learning rate 0.1. The learning rate of ResNet-32 is divided by 10 after 80 and 90 epoch (for a total 100 epochs), and the learning rate of WN-Net is fixed as 10 -5 . We randomly selected 10 images per class in validation set as the meta-data set. The compared methods include: 1) BaseModel, which uses a softmax cross-entropy loss to train ResNet-32 on the training set; 2) Focal loss <ref type="bibr" target="#b24">[25]</ref> and Class-Balanced <ref type="bibr" target="#b59">[60]</ref> represent the state-of-the-arts of the predefined sample reweighting techniques; 3) Fine-tuning, fine-tune the result of BaseModel on the meta-data set; 4) L2RW <ref type="bibr" target="#b0">[1]</ref>, which leverages an additional meta-dataset to adaptively assign weights on training samples.  Table <ref type="table" target="#tab_0">1</ref> shows the classification accuracy of ResNet-32 on the test set and confusion matrices are displayed in Fig. <ref type="figure">3</ref> (more details are listed in the supplementary material). It can be observed that: 1) Our algorithm evidently outperforms other competing methods on datasets with class imbalance, showing its robustness in such data bias case; 2) When imbalance factor is 1, i.e., all classes are with same numbers of samples, fine-tuning runs best, and our method still attains a comparable performance; 3) When imbalance factor is 200 on long-tailed CIFAR-100, the smallest class has only two samples. An extra fine-tuning achieves performance gain, while our method still perform well in such extreme data bias.</p><p>To understand the weighing scheme of MW-Net, we depict the tendency curve of weight with respect to loss by the learned MW-Net in Fig. <ref type="figure" target="#fig_0">1(d)</ref>, which complies with the classical optimal weighting manner to such data bias. i.e., larger weights should be imposed on samples with relatively large losses, which are more likely to be minority class sample.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Corrupted Label Experiment</head><p>We study two settings of corrupted labels on the training set: 1) Uniform noise. The label of each sample is independently changed to a random class with probability p following the same setting in <ref type="bibr" target="#b1">[2]</ref>. 2) Flip noise. The label of each sample is independently flipped to similar classes with total probability p. In our experiments, we randomly select two classes as similar classes with equal probability. Two benchmark datasets are employed: CIFAR-10 and CIFAR-100 <ref type="bibr" target="#b61">[62]</ref>. Both are popularly used for evaluation of noisy labels <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b15">16]</ref>.1000 images with clean labels in validation set are randomly selected as the meta-data set. We adopt a Wide ResNet-28-10 (WRN-28-10) <ref type="bibr" target="#b62">[63]</ref> for uniform noise and ResNet-32 <ref type="bibr" target="#b60">[61]</ref> for flip noise as our classifier network models <ref type="foot" target="#foot_2">4</ref> .</p><p>The comparison methods include: BaseModel, referring to the similar classifier network utilized in our method, while directly trained on the biased training data; the robust learning methods Reed <ref type="bibr" target="#b57">[58]</ref>, S-Model <ref type="bibr" target="#b11">[12]</ref> , SPL <ref type="bibr" target="#b25">[26]</ref>, Focal Loss <ref type="bibr" target="#b24">[25]</ref>, Co-teaching <ref type="bibr" target="#b15">[16]</ref>, D2L <ref type="bibr" target="#b58">[59]</ref>; Fine-tuning, fine-tuning the result of BaseModel on the meta-data with clean labels to further enhance its performance; typical meta-learning methods MentorNet <ref type="bibr" target="#b20">[21]</ref>, L2RW <ref type="bibr" target="#b0">[1]</ref>, GLC <ref type="bibr" target="#b14">[15]</ref>. We also trained the baseline network only on 1000 meta-images. The performance are evidently worse than the proposed method due to the neglecting of the knowledge underlying large amount of training samples. We thus have not involved its results in comparison.</p><p>All the baseline networks were trained using SGD with a momentum 0.9, a weight decay 5 × 10 -4 and an initial learning rate 0.   total of 60 epoches) in flip noise. The learning rate of WN-Net is fixed as 10 -3 . We repeated the experiments 5 times with different random seeds for network initialization and label noise generation.</p><p>We report the accuracy averaged over 5 repetitions for each series of experiments and each competing method in Tables <ref type="table" target="#tab_1">2</ref> and<ref type="table" target="#tab_3">3</ref>. It can be observed that our method gets the best performance across almost all datasets and all noise rates, except the second for 40% Flip noise. At 0% noise cases (unbiased ones), our method performs only slightly worse than the BaseModel. For other corrupted label cases, the superiority of our method is evident. Besides, it can be seen that the performance gaps between ours and all other competing methods increase as the noise rate is increased from 40% to 60% under uniform noise. Even with 60% label noise, our method can still obtain a relatively high classification accuracy, and attains more than 15% accuracy gain compared with the second best result for CIFAR100 dataset, which indicates the robustness of our methods in such cases.</p><p>Fig. <ref type="figure">4</ref> shows the performance comparison between WRN-28-10 and ResNet32 under fixed flip noise setting. We can observe that the performance gains for our method and BaseModel between two networks takes the almost same value. It implies that the performance improvement of our method is not dependent on the selection of the classifier network architectures.</p><p>As shown in Fig. <ref type="figure" target="#fig_0">1</ref>(e), the shape of the learned weight function depicts as monotonic decreasing, complying with the traditional optimal setting to this bias condition, i.e., imposing smaller weights on samples with relatively large losses to suppress the effect of corrupted labels. Furthermore, we plot the weight distribution of clean and noisy training samples in Fig. <ref type="figure">5</ref>. It can be seen that almost all large weights belongs to clean samples, and the noisy samples's weights are smaller than that of clean samples, which implies that the trained Meta-Weight-Net can distinguish clean and noisy images.</p><p>Fig. <ref type="figure">6</ref> plots the weight variation along with training epoches under 40% noise on CIFAR10 dataset of our method and L2RW. y-axis denotes the differences of weights calculated between adjacent epoches, and x-axis denotes the number of epoches. Ten noisy samples are randomly chosen to compute their mean curve, surrounded by the region illustrating the standard deviations calculated on these samples in the corresponding epoch. It is seen that the weight by our method is continuously changed, gradually stable along iterations, and finally converges. As a comparison, the weight during the learning process of L2RW fluctuates relatively more wildly. This could explain the consistently better performance of our method as compared with this competing method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experiments on Clothing1M</head><p>To verify the effectiveness of the proposed method on real-world data, we conduct experiments on the Clothing1M dataset <ref type="bibr" target="#b63">[64]</ref>, containing 1 million images of clothing obtained from online shopping  websites that are with 14 categories, e.g., T-shirt, Shirt, Knitwear. The labels are generated by using surrounding texts of the images provided by the sellers, and therefore contain many errors. We use the 7k clean data as the meta dataset. Following the previous works <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b65">66]</ref>, we used ResNet-50 pre-trained on ImageNet. For preprocessing, we resize the image to 256 × 256, crop the middle 224 × 224 as input, and perform normalization. We used SGD with a momentum 0.9, a weight decay 10 -3 , and an initial learning rate 0.01, and batch size 32. The learning rate of ResNet-50 is divided by 10 after 5 epoch (for a total 10 epoch), and the learning rate of WN-Net is fixed as 10 -3 .</p><p>The results are summarized in Table. <ref type="bibr" target="#b3">4</ref>. which shows that the proposed method achieves the best performance. Fig. <ref type="figure" target="#fig_0">1</ref>(f) plots the tendency curve of the learned MW-Net function, which reveals abundant data insights. Specifically, when the loss is with relatively small values, the weighting function inclines to increase with loss, meaning that it tends to more emphasize hard margin samples with informative knowledge for classification; while when the loss gradually changes large, the weighting function begins to monotonically decrease, implying that it tends to suppress noise labels samples with relatively large loss values. Such complicated essence cannot be finely delivered by conventional weight functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have proposed a novel meta-learning method for adaptively extracting sample weights to guarantee robust deep learning in the presence of training data bias. Compared with current reweighting methods that require to manually set the form of weight functions, the new method is able to yield a rational one directly from data. The working principle of our algorithm can be well explained and the procedure of our method can be easily reproduced ( Appendix A provide the Pytorch implement of our algorithm (less than 30 lines of codes)), and the completed training code is avriable at https://github.com/xjtushujun/meta-weight-net.). Our empirical results show that the propose method can perform superior in general data bias cases, like class imbalance, corrupted labels, and more complicated real cases. Besides, such an adaptive weight learning approach is hopeful to be employed to other weight setting problems in machine learning, like ensemble methods and multi-view learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: (a)-(b) weight functions set in focal loss and self-paced learning (SPL). (c) Meta-Weighting-Net architecture. (d)-(f) Meta-Weighting-Net functions learned in class imbalance (imbalanced factor 100), noisy label (40% uniform noise), and real dataset, respectively, by our method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Main flowchart of the proposed MW-Net Learning algorithm (steps 5-7 in Algorithm 1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: Confusion matrices for the Basemodel and ours on long-tailed CIFAR-10 with imbalance factors 200.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Figure 5: Sample weight distribution on training data under 40% uniform noise experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Test accuracy (%) of ResNet-32 on long-tailed CIFAR-10 and CIFAR-100, and the best and the second best results are highlighted in bold and italic bold, respectively.</figDesc><table><row><cell>Dataset Name</cell><cell></cell><cell></cell><cell cols="2">Long-Tailed CIFAR-10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Long-Tailed CIFAR-100</cell><cell></cell><cell></cell></row><row><cell>Imbalance</cell><cell>200</cell><cell>100</cell><cell>50</cell><cell>20</cell><cell>10</cell><cell>1</cell><cell>200</cell><cell>100</cell><cell>50</cell><cell>20</cell><cell>10</cell><cell>1</cell></row><row><cell>BaseModel</cell><cell>65.68</cell><cell>70.36</cell><cell>74.81</cell><cell>82.23</cell><cell>86.39</cell><cell>92.89</cell><cell>34.84</cell><cell>38.32</cell><cell>43.85</cell><cell>51.14</cell><cell>55.71</cell><cell>70.50</cell></row><row><cell>Focal Loss</cell><cell>65.29</cell><cell>70.38</cell><cell>76.71</cell><cell>82.76</cell><cell>86.66</cell><cell>93.03</cell><cell>35.62</cell><cell>38.41</cell><cell>44.32</cell><cell>51.95</cell><cell>55.78</cell><cell>70.52</cell></row><row><cell>Class-Balanced</cell><cell>68.89</cell><cell>74.57</cell><cell>79.27</cell><cell>84.36</cell><cell>87.49</cell><cell>92.89</cell><cell>36.23</cell><cell>39.60</cell><cell>45.32</cell><cell>52.59</cell><cell>57.99</cell><cell>70.50</cell></row><row><cell>Fine-tuning</cell><cell>66.08</cell><cell>71.33</cell><cell>77.42</cell><cell>83.37</cell><cell>86.42</cell><cell>93.23</cell><cell>38.22</cell><cell>41.83</cell><cell>46.40</cell><cell>52.11</cell><cell>57.44</cell><cell>70.72</cell></row><row><cell>L2RW</cell><cell>66.51</cell><cell>74.16</cell><cell>78.93</cell><cell>82.12</cell><cell>85.19</cell><cell>89.25</cell><cell>33.38</cell><cell>40.23</cell><cell>44.44</cell><cell>51.64</cell><cell>53.73</cell><cell>64.11</cell></row><row><cell>Ours</cell><cell>68.91</cell><cell>75.21</cell><cell>80.06</cell><cell>84.94</cell><cell>87.84</cell><cell>92.66</cell><cell>37.91</cell><cell>42.09</cell><cell>46.74</cell><cell>54.37</cell><cell>58.46</cell><cell>70.37</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Test accuracy comparison on CIFAR-10 and CIFAR-100 of WRN-28-10 with varying noise rates under uniform noise. Mean accuracy (±std) over 5 repetitions are reported ('-' means the method fails).</figDesc><table><row><cell cols="2">Datasets / Noise Rate</cell><cell>BaseModel</cell><cell>Reed-Hard</cell><cell>S-Model</cell><cell>Self-paced</cell><cell>Focal Loss</cell><cell>Co-teaching</cell><cell>D2L</cell><cell>Fine-tining</cell><cell>MentorNet</cell><cell>L2RW</cell><cell>GLC</cell><cell>Ours</cell></row><row><cell></cell><cell>0%</cell><cell>95.60±0.22</cell><cell>94.38±0.14</cell><cell>83.79±0.11</cell><cell>90.81±0.34</cell><cell>95.70±0.15</cell><cell>88.67±0.25</cell><cell>94.64±0.33</cell><cell>95.65±0.15</cell><cell>94.35±0.42</cell><cell>92.38±0.10</cell><cell>94.30±0.19</cell><cell>94.52±0.25</cell></row><row><cell>CIFAR-10</cell><cell>40%</cell><cell>68.07±1.23</cell><cell>81.26±0.51</cell><cell>79.58±0.33</cell><cell>86.41±0.29</cell><cell>75.96±1.31</cell><cell>74.81±0.34</cell><cell>85.60±0.13</cell><cell>80.47±0.25</cell><cell>87.33±0.22</cell><cell>86.92±0.19</cell><cell>88.28±0.03</cell><cell>89.27±0.28</cell></row><row><cell></cell><cell>60%</cell><cell>53.12±3.03</cell><cell>73.53±1.54</cell><cell>-</cell><cell>53.10±1.78</cell><cell>51.87±1.19</cell><cell>73.06±0.25</cell><cell>68.02±0.41</cell><cell>78.75±2.40</cell><cell>82.80±1.35</cell><cell>82.24±0.36</cell><cell>83.49±0.24</cell><cell>84.07±0.33</cell></row><row><cell></cell><cell>0%</cell><cell>79.95±1.26</cell><cell>64.45±1.02</cell><cell>52.86±0.99</cell><cell>59.79±0.46</cell><cell>81.04±0.24</cell><cell>61.80±0.25</cell><cell>66.17±1.42</cell><cell>80.88±0.21</cell><cell>73.26±1.23</cell><cell>72.99±0.58</cell><cell>73.75±0.51</cell><cell>78.76±0.24</cell></row><row><cell>CIFAR-100</cell><cell>40%</cell><cell>51.11±0.42</cell><cell>51.27±1.18</cell><cell>42.12±0.99</cell><cell>46.31±2.45</cell><cell>51.19±0.46</cell><cell>46.20±0.15</cell><cell>52.10 ±0.97</cell><cell>52.49±0.74</cell><cell>61.39±3.99</cell><cell>60.79±0.91</cell><cell>61.31±0.22</cell><cell>67.73±0.26</cell></row><row><cell></cell><cell>60%</cell><cell>30.92±0.33</cell><cell>26.95±0.98</cell><cell>-</cell><cell>19.08±0.57</cell><cell>27.70±3.77</cell><cell>35.67±1.25</cell><cell>41.11±0.30</cell><cell>38.16±0.38</cell><cell>36.87±1.47</cell><cell>48.15±0.34</cell><cell>50.81±1.00</cell><cell>58.75±0.11</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Test accuracy comparison on CIFAR-10 and CIFAR-100 of ResNet-32 with varying noise rates under flip noise.</figDesc><table><row><cell cols="2">Datasets / Noise Rate</cell><cell>BaseModel</cell><cell>Reed-Hard</cell><cell>S-Model</cell><cell>Self-paced</cell><cell>Focal Loss</cell><cell>Co-teaching</cell><cell>D2L</cell><cell>Fine-tining</cell><cell>MentorNet</cell><cell>L2RW</cell><cell>GLC</cell><cell>Ours</cell></row><row><cell></cell><cell>0%</cell><cell>92.89±0.32</cell><cell>92.31±0.25</cell><cell>83.61±0.13</cell><cell>88.52±0.21</cell><cell>93.03±0.16</cell><cell>89.87±0.10</cell><cell>92.02±0.14</cell><cell>93.23±0.23</cell><cell>92.13±0.30</cell><cell>89.25±0.37</cell><cell>91.02±0.20</cell><cell>92.04±0.15</cell></row><row><cell>CIFAR-10</cell><cell>20%</cell><cell>76.83±2.30</cell><cell>88.28±0.36</cell><cell>79.25±0.30</cell><cell>87.03±0.34</cell><cell>86.45±0.19</cell><cell>82.83±0.85</cell><cell>87.66±0.40</cell><cell>82.47±3.64</cell><cell>86.36±0.31</cell><cell>87.86±0.36</cell><cell>89.68±0.33</cell><cell>90.33±0.61</cell></row><row><cell></cell><cell>40%</cell><cell>70.77±2.31</cell><cell>81.06±0.76</cell><cell>75.73±0.32</cell><cell>81.63±0.52</cell><cell>80.45±0.97</cell><cell>75.41±0.21</cell><cell>83.89±0.46</cell><cell>74.07±1.56</cell><cell>81.76±0.28</cell><cell>85.66±0.51</cell><cell>88.92±0.24</cell><cell>87.54±0.23</cell></row><row><cell></cell><cell>0%</cell><cell>70.50±0.12</cell><cell>69.02±0.32</cell><cell>51.46±0.20</cell><cell>67.55±0.27</cell><cell>70.02±0.53</cell><cell>63.31±0.05</cell><cell>68.11±0.26</cell><cell>70.72±0.22</cell><cell>70.24±0.21</cell><cell>64.11±1.09</cell><cell>65.42±0.23</cell><cell>70.11±0.33</cell></row><row><cell>CIFAR-100</cell><cell>20%</cell><cell>50.86±0.27</cell><cell>60.27±0.76</cell><cell>45.45±0.25</cell><cell>63.63±0.30</cell><cell>61.87±0.30</cell><cell>54.13±0.55</cell><cell>63.48±0.53</cell><cell>56.98±0.50</cell><cell>61.97±0.47</cell><cell>57.47±1.16</cell><cell>63.07±0.53</cell><cell>64.22±0.28</cell></row><row><cell></cell><cell>40%</cell><cell>43.01±1.16</cell><cell>50.40±1.01</cell><cell>43.81±0.15</cell><cell>53.51±0.53</cell><cell>54.13±0.40</cell><cell>44.85±0.81</cell><cell>51.83±0.33</cell><cell>46.37±0.25</cell><cell>52.66±0.56</cell><cell>50.98±1.55</cell><cell>62.22±0.62</cell><cell>58.64±0.47</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Classification accuracy (%) of all competing methods on the Clothing1M test set.</figDesc><table><row><cell>#</cell><cell>Method</cell><cell>Accuracy</cell><cell>#</cell><cell>Method</cell><cell>Accuracy</cell></row><row><cell>1</cell><cell>Cross Entropy</cell><cell>68.94</cell><cell>5</cell><cell>Joint Optimization [66]</cell><cell>72.23</cell></row><row><cell>2</cell><cell>Bootstrapping [58]</cell><cell>69.12</cell><cell>6</cell><cell>LCCN [67]</cell><cell>73.07</cell></row><row><cell>3</cell><cell>Forward [65]</cell><cell>69.84</cell><cell>7</cell><cell>MLNT [68]</cell><cell>73.47</cell></row><row><cell>4</cell><cell>S-adaptation [12]</cell><cell>70.36</cell><cell>8</cell><cell>Ours</cell><cell>73.72</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>Notice that Θ here is a variable instead of a quantity, which makes ŵt (Θ) a function of Θ and the gradient in Eq. (9) be able to be computed.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>Derivation can be found in supplementary materials.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>We have tried different classifier network architectures as classifier networks under each noise setting to show our algorithm is suitable to different deep learning architectures. We show this effect in Fig.4, verifying the consistently good performance of our method in two classifier network settings.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was supported by the China NSFC projects under contracts 61661166011, 11690011, 61603292, 61721002,U1811461. The authors would also like to thank anonymous reviewers for their constructive suggestions on improving the paper, especially on the proofs and theoretical analysis of our paper.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to reweight examples for robust deep learning</title>
		<author>
			<persName><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyuan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning from imbalanced data</title>
		<author>
			<persName><forename type="first">Haibo</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edwardo</forename><forename type="middle">A</forename><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge &amp; Data Engineering</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Exploring generalization in deep learning</title>
		<author>
			<persName><forename type="first">Srinadh</forename><surname>Behnam Neyshabur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nati</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A closer look at memorization in deep networks</title>
		<author>
			<persName><forename type="first">Devansh</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanisław</forename><surname>Jastrzębski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maxinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tegan</forename><surname>Kanwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asja</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Kenji</forename><surname>Kawaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leslie</forename><surname>Pack Kaelbling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.05468</idno>
		<title level="m">Generalization in deep learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sensitivity and generalization in neural networks: an empirical study</title>
		<author>
			<persName><forename type="first">Roman</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasaman</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Daniel A Abolafia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><surname>Sohl-Dickstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A review on ensembles for the class imbalance problem: bagging-, boosting-, and hybrid-based approaches</title>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Galar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edurne</forename><surname>Barrenechea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Humberto</forename><surname>Bustince</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A systematic study of the class imbalance problem in convolutional neural networks</title>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Buda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atsuto</forename><surname>Maki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maciej</forename><forename type="middle">A</forename><surname>Mazurowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning from noisy labels with deep neural networks</title>
		<author>
			<persName><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR workshop</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Auxiliary image regularization for deep cnns with noisy labels</title>
		<author>
			<persName><forename type="first">Samaneh</forename><surname>Azadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Training deep neural-networks using a noise adaptation layer</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ehud</forename><surname>Ben-Reuven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning from noisy labels with distillation</title>
		<author>
			<persName><forename type="first">Yuncheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yale</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangliang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Toward robustness against label noise in training deep discriminative neural networks</title>
		<author>
			<persName><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<pubPlace>In NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Using trusted data to train deep networks on labels corrupted by severe noise</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duncan</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Co-teaching: robust training deep neural networks with extremely noisy labels</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingrui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivor</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Generalized cross entropy loss for training deep neural networks with noisy labels</title>
		<author>
			<persName><forename type="first">Zhilu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mert</surname></persName>
		</author>
		<author>
			<persName><surname>Sabuncu</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning to predict from crowdsourced data</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to detect concepts from webly-labeled video data</title>
		<author>
			<persName><forename type="first">Junwei</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Attend in groups: a weakly-supervised deep learning framework for learning from web data</title>
		<author>
			<persName><forename type="first">Bohan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels</title>
		<author>
			<persName><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A decision-theoretic generalization of on-line learning and an application to boosting</title>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computer and system sciences</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="119" to="139" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cost-sensitive boosting for classification of imbalanced data</title>
		<author>
			<persName><forename type="first">Yanmin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><forename type="middle">S</forename><surname>Kamel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew Kc</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3358" to="3378" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Ensemble of exemplar-svms for object detection and beyond</title>
		<author>
			<persName><forename type="first">Tomasz</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priyal</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Self-paced learning for latent variable models</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>M Pawan Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daphne</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName><surname>Koller</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A framework for robust subspace learning</title>
		<author>
			<persName><forename type="first">De</forename><surname>La</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torre</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Black</forename><surname>Mkchael</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="117" to="142" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Easy samples first: Self-paced reranking for zero-example multimedia search</title>
		<author>
			<persName><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teruko</forename><surname>Mitamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Self-paced learning with diversity</title>
		<author>
			<persName><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shoou-I</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Hauptmann</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Robust probabilistic modeling with bayesian data reweighting</title>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alp</forename><surname>Kucukelbir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Approximation with artificial neural networks</title>
		<author>
			<persName><forename type="first">Balázs</forename><surname>Csanád</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Csáji</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Faculty of Sciences</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page">48</biblScope>
			<date type="published" when="2001">2001</date>
			<pubPlace>Hungary</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Etvs Lornd University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Learning to teach with dynamic loss functions</title>
		<author>
			<persName><forename type="first">Lijun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lai</forename><surname>Jian-Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Learning to learn by gradient descent by gradient descent</title>
		<author>
			<persName><forename type="first">Marcin</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Misha</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergio</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">W</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Shillingford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nando</forename><forename type="middle">De</forename><surname>Freitas</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning to learn from weak supervision by full supervision</title>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sascha</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaap</forename><surname>Kamps</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS Workshop</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Bilevel programming for hyperparameter optimization and meta-learning</title>
		<author>
			<persName><forename type="first">Luca</forename><surname>Franceschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saverio</forename><surname>Salzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimilano</forename><surname>Pontil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02999</idno>
		<title level="m">Reptile: a scalable metalearning algorithm</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Gradient agreement as an optimization objective for meta-learning</title>
		<author>
			<persName><forename type="first">David</forename><surname>Amir Erfan Eshratifar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massoud</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName><surname>Pedram</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.08178</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Smote: synthetic minority over-sampling technique</title>
		<author>
			<persName><forename type="first">Kevin</forename><forename type="middle">W</forename><surname>Nitesh V Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><forename type="middle">O</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><surname>Kegelmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of artificial intelligence research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="321" to="357" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Class rectification hard mining for imbalanced deep learning</title>
		<author>
			<persName><forename type="first">Qi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning and evaluating classifiers under sample selection bias</title>
		<author>
			<persName><forename type="first">Bianca</forename><surname>Zadrozny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The foundations of cost-sensitive learning</title>
		<author>
			<persName><forename type="first">Charles</forename><surname>Elkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Cost-sensitive learning of deep feature representations from imbalanced data</title>
		<author>
			<persName><forename type="first">Munawar</forename><surname>Salman H Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ferdous</forename><forename type="middle">A</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName><surname>Togneri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Survey on deep learning with class imbalance</title>
		<author>
			<persName><forename type="first">Justin</forename><forename type="middle">M</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Johnson</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Taghi</forename><forename type="middle">M</forename><surname>Khoshgoftaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Big Data</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Active bias: Training more accurate neural networks by emphasizing high variance samples</title>
		<author>
			<persName><forename type="first">Haw-Shiuan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Human-level concept learning through probabilistic program induction</title>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Brenden M Lake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="issue">6266</biblScope>
			<biblScope unit="page" from="1332" to="1338" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Small sample learning in big data era</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongben</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.04572</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Fidelity-weighted learning</title>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arash</forename><surname>Mehrjou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaap</forename><surname>Kamps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Yang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang-Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Learning to teach. In ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning to model the tail</title>
		<author>
			<persName><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Large scale fine-grained categorization and domain-specific transfer learning</title>
		<author>
			<persName><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Learning deep representation for imbalanced classification</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yining</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Range loss for deep face recognition with long-tailed training data</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Learning from noisy large-scale datasets with minimal supervision</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gal</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Training deep neural networks on noisy labels with bootstrapping</title>
		<author>
			<persName><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR workshop</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Dimensionality-driven learning with noisy labels</title>
		<author>
			<persName><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">E</forename><surname>Houle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><forename type="middle">M</forename><surname>Erfani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu-Tao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sudanthi</forename><surname>Wijewickrema</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Class-balanced loss based on effective number of samples</title>
		<author>
			<persName><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menglin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMCV</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Learning from massive noisy labeled data for image classification</title>
		<author>
			<persName><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Making deep neural networks robust to label noise: A loss correction approach</title>
		<author>
			<persName><forename type="first">Giorgio</forename><surname>Patrini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Rozza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizhen</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Joint optimization framework for learning with noisy labels</title>
		<author>
			<persName><forename type="first">Daiki</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daiki</forename><surname>Ikami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toshihiko</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiyoharu</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Safeguarded dynamic label regression for noisy supervision</title>
		<author>
			<persName><forename type="first">Jiangchao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivor</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Learning to learn from noisy labeled data</title>
		<author>
			<persName><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongkang</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohan</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Stochastic majorization-minimization algorithms for large-scale optimization</title>
		<author>
			<persName><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
