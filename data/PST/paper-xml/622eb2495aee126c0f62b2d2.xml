<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Active Evaluation: Efficient NLG Evaluation with Few Pairwise Comparisons</title>
				<funder ref="#_V6j2gdG">
					<orgName type="full">Robert Bosch Center for Data Science and Artificial Intelligence, IIT Madras</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-03-11">11 Mar 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Akash</forename><surname>Kumar Mohankumar</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Mitesh</forename><forename type="middle">M</forename><surname>Khapra</surname></persName>
							<email>miteshk@cse.iitm.ac.in</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Bangalore</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Indian Institute of Technology Madras RBCDSAI</orgName>
								<orgName type="institution" key="instit2">IIT Madras</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Work done at Indian Institute of Technology Madras</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Active Evaluation: Efficient NLG Evaluation with Few Pairwise Comparisons</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-03-11">11 Mar 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2203.06063v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent studies have shown the advantages of evaluating NLG systems using pairwise comparisons as opposed to direct assessment. Given k systems, a naive approach for identifying the top-ranked system would be to uniformly obtain pairwise comparisons from all k 2 pairs of systems. However, this can be very expensive as the number of human annotations required would grow quadratically with k. In this work, we introduce Active Evaluation, a framework to efficiently identify the top-ranked system by actively choosing system pairs for comparison using dueling bandit algorithms. We perform extensive experiments with 13 dueling bandits algorithms on 13 NLG evaluation datasets spanning 5 tasks and show that the number of human annotations can be reduced by 80%. To further reduce the number of human annotations, we propose model-based dueling bandit algorithms which combine automatic evaluation metrics with human evaluations. Specifically, we eliminate sub-optimal systems even before the human annotation process and perform human evaluations only on test examples where the automatic metric is highly uncertain. This reduces the number of human annotations required further by 89%. In effect, we show that identifying the top-ranked system requires only a few hundred human annotations, which grow linearly with k. Lastly, we provide practical recommendations and best practices to identify the top-ranked system efficiently. Our code has been made publicly available at https://github.com/akashkm99/ duelnlg</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the last few years, the field of NLG has made rapid progress with the advent of large-scale models trained on massive amounts of data <ref type="bibr" target="#b54">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b58">Xue et al., 2020;</ref><ref type="bibr" target="#b25">Liu et al., 2020;</ref><ref type="bibr" target="#b7">Brown et al., 2020)</ref>. However, evaluation of NLG systems continues to be a challenge. On the one hand, we have automatic evaluation metrics which are easy to compute but unreliable. In particular, many studies have shown that they do not correlate well with human judgments <ref type="bibr" target="#b33">(Novikova et al., 2017;</ref><ref type="bibr" target="#b11">Elliott and Keller, 2014;</ref><ref type="bibr" target="#b40">Sai et al., 2019</ref><ref type="bibr">Sai et al., , 2020a,b),b)</ref>. On the other hand, we have human evaluations, which are relatively more reliable but tedious, expensive, and time-consuming. Further, recent studies have highlighted some limitations of human evaluations that involve direct assessment on an absolute scale, e.g., Likert scale. Specifically, human evaluations using direct assessment have been shown to suffer from annotator bias, high variance and sequence effects where the annotation of one item is influenced by preceding items <ref type="bibr" target="#b21">(Kulikov et al., 2019;</ref><ref type="bibr" target="#b52">Sudoh et al., 2021;</ref><ref type="bibr" target="#b23">Liang et al., 2020;</ref><ref type="bibr" target="#b47">See et al., 2019;</ref><ref type="bibr" target="#b27">Mathur et al., 2017)</ref>.</p><p>In this work, we focus on reducing the cost and time required for human evaluations while not compromising on reliability. We take motivation from studies which show that selecting the better of two options is much easier for human annotators than providing an absolute score, which requires annotators to maintain a consistent standard across samples <ref type="bibr" target="#b19">(Kendall, 1948;</ref><ref type="bibr" target="#b49">Simpson and Gurevych, 2018)</ref>. In particular, recent works show that ranking NLG systems using pairwise comparisons is a more reliable alternative than using direct assessment <ref type="bibr" target="#b47">(See et al., 2019;</ref><ref type="bibr" target="#b22">Li et al., 2019;</ref><ref type="bibr" target="#b46">Sedoc et al., 2019;</ref><ref type="bibr" target="#b9">Dhingra et al., 2019)</ref>. While this is promising, a naive approach for identifying the top-ranked system from a set of k systems using uniform exploration is prohibitively expensive. Specifically, uniform exploration obtains an equal number of annotations for all the k 2 system pairs; as a result, the required human annotations grows as O(k 2 ).</p><p>To reduce the number of pairwise annotations, we introduce Active Evaluation, a framework to efficiently identify the top-ranked NLG system.</p><p>Our Active Evaluation framework consists of a learner that selects a pair of systems to compare at each time step. The learner, then, receives a feedback signal indicating the (human) preference between the selected systems on one input context, randomly sampled from the test dataset. The learner's objective is to reliably compute the topranked system with as few human annotations as possible. We adopt algorithms from the stochastic dueling bandits literature <ref type="bibr" target="#b2">(Bengs et al., 2021)</ref> to decide which pair of NLG systems to compare at each time step. To check if existing dueling bandits algorithms can indeed provide reliable top-rank estimates with minimal annotations, we evaluate 13 such algorithms on 13 NLG evaluation datasets spanning five tasks viz., machine translation, summarization, data-to-text generation, paraphrase generation, and grammatical error correction. We show that the best performing dueling bandit algorithm can reduce the number of human annotations by 80% when compared to uniform exploration.</p><p>To further reduce human annotations, we leverage automatic evaluation metrics in our Active Evaluation framework. We utilize existing automatic metrics such as BLEU <ref type="bibr" target="#b35">(Papineni et al., 2002)</ref>, BertScore <ref type="bibr" target="#b61">(Zhang et al., 2020)</ref>, etc for pairwise evaluations by converting the direct evaluation scores into preference probabilities using pairwise probability models. We also develop trained pairwise metrics that directly predict the comparison outcome given pairs of generated texts and context or reference as input. To incorporate such evaluation metrics in our Active Evaluation framework, we propose three model-based dueling bandits algorithms, viz., (i) Random Mixing: human annotations and evaluation metric predictions are randomly mixed, (ii) Uncertainty-aware selection: human annotations are obtained only when the predictions from the evaluation metric is highly uncertain, (iii) UCB Elimination: poorly performing NLG systems are eliminated using an Upper Confidence Bound (UCB) on the evaluation metric scores. Through our experiments, we show that the number of human annotations can be further reduced by 89% on average (this reduction is over and above the 80% reduction that we got earlier). In effect, we show that given k systems, we can find the top-ranked NLG system efficiently with just a few hundred comparisons that vary as O(k). Lastly, we provide practical recommendations to efficiently identify the top-ranked NLG system based on our empirical study on various design choices and hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Active Evaluation Framework</head><p>We introduce the problem and our Active Evaluation setup in section 2.1. Later in section 2.2, we describe the different approaches to decide which pairs of NLG systems to compare at each time step. Finally, in section 2.3, we formalize the notion of top-ranked system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem Formulation and Setup</head><p>We consider the problem of finding the top-ranked NLG system from a given set of k systems, denoted by S = {1, 2, . . . , k}. Our Active Evaluation framework consist of a learner which at each time step t, chooses a pair of systems s</p><formula xml:id="formula_0">(t) 1 , s (t)</formula><p>2 ? S for comparison. Then, we ask human annotators to compare the outputs of the chosen systems on a randomly sampled input context and provide the comparison outcome as feedback to the learner. Specifically, we first sample an input context X (t)  from the test dataset and obtain the generated texts</p><formula xml:id="formula_1">Y (t) 1 , Y (t) 2 from the chosen systems s (t) 1 , s (t)</formula><p>2 . We then display the generated texts</p><formula xml:id="formula_2">Y (t) 1 , Y (t) 2</formula><p>along with the context X (t) to human annotators and obtain a comparison outcome w (t) = 1, 0, or 0.5 denoting whether Y (t) 1 is of better, worse, or equal (tie) quality as Y (t) 2 . Note that the feedback w (t) indicates the preference on only one input sample and not the entire test dataset. The overall framework is depicted in figure <ref type="figure" target="#fig_0">1</ref>. The learner's objective is to find the top-ranked system with as few pairwise comparisons as possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Choosing System Pairs for Comparison</head><p>The learner should decide the pair of systems (s</p><formula xml:id="formula_3">(t) 1 , s (t)</formula><p>2 ) to compare at each time step t. The naive approach is to uniformly explore all the k 2 system pairs. Specifically, the probability of selecting a pair (i, j), i = j at time t is given by</p><formula xml:id="formula_4">P unif orm ((s (t) 1 , s (t) 2 ) = (i, j)) = 1 k 2</formula><p>However, as we show in our experiments, the number of human annotations required to find the topranked system by this approach is very expensive and grows quadratically with the number of systems since we equally explore all k 2 pairs. To reduce the number of annotations, we use dueling bandit algorithms to actively choose pairs of systems to compare based on the history of previous observations. We provide an overview of 13 dueling bandits algorithms proposed in the literature in appendix B. We refer the readers to <ref type="bibr" target="#b2">(Bengs et al., 2021)</ref> for a complete survey.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Identifying the top-ranked system</head><p>We now formalize the notion of the top-ranked system. Let p ij denote the preference probability of system i over system j i.e. the probability that a generated text from system i is preferred over system j in the test dataset. We say that a system i "beats" system j if p ij &gt; 1 2 . In other words, system i beats system j if the probability of winning in a pairwise comparison is larger for i than it is for j. We define the top-ranked system i * as the one that beats all other systems, i.e. p i * j &gt; 1 2 , ?j ? S -i * .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Pairwise Probability Models</head><p>Our Active Evaluation framework, which we described in the previous section, completely relied on human annotators to compare pairs of generated texts (Y 1 , Y 2 ) to provide the preference feedback w. We can further reduce the number of required human annotations by estimating the human preference feedback using automatic evaluation metrics. However, most existing evaluation metrics are de-signed for direct assessment and not directly suitable for pairwise evaluations. In this section, we describe three pairwise probability models to convert direct evaluation scores into pairwise preference probabilities. Let f (Y ) denote the score provided by a direct assessment metric f to a generated text Y (The dependence of f on the reference/context is omitted for brevity). The pairwise preference probability p(Y 1 Y 2 ) between any two hypotheses Y 1 and Y 2 can be modeled in 3 different ways:</p><p>? Linear:</p><formula xml:id="formula_5">p(Y 1 Y 2 ) = 1 2 + (f (Y 1 ) -f (Y 2 ))</formula><p>? Bradley-Terry-Luce (BTL) <ref type="bibr" target="#b6">(Bradley and Terry, 1952;</ref><ref type="bibr" target="#b26">Luce, 1979)</ref>:</p><formula xml:id="formula_6">p(Y 1 Y 2 ) = f (Y 1 ) f (Y 1 ) + f (Y 2 )</formula><p>? BTL-logistic::</p><p>As detailed in appendix C.2, we appropriately preprocess the scores f (Y ) to ensure that preference probability lies between 0 and 1. We can now predict the comparison outcome w by thresholding the preference probability at two thresholds ? 1 and ? 2 (? ? 1 ) to incorporate ties i.e.:</p><formula xml:id="formula_7">? = ? ? ? ? ? 1, if p(Y 1 Y 2 ) &gt; ? 2 0, if p(Y 1 Y 2 ) &lt; ? 1 0.5, Otherwise</formula><p>We choose ? 1 and ? 2 using grid search on the validation set. Refer appendix C.2 for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Model-based Dueling Bandits</head><p>In the previous section, we discussed pairwise probability models to obtain the estimated preference probability p(Y 1 Y 2 ) and the comparison outcome ? using scores assigned by direct assessment metrics. We now propose three model-based dueling bandit algorithms wherein we combine such predictions from evaluation metrics with human annotations in the Active Evaluation framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Random Mixing</head><p>Here, we randomly provide either the real (human) or the evaluation metric predicted feedback to the learner. Specifically, at any time t, we use the predicted comparison outcome ?(t) as the feedback with probability p m and use human annotations w (t) as feedback with probability 1 -p m . The hyperparameter p m controls the ratio of estimated and real feedback given to the learner. As with other hyperparameters, we tune p m on the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Uncertainty-aware Selection</head><p>In this algorithm, we estimate uncertainty in the evaluation metric predictions and decide to ask for human annotations only when the evaluation metric is highly uncertain. We specifically focus on trainable neural evaluation metrics such as Bleurt <ref type="bibr" target="#b48">(Sellam et al., 2020)</ref> where we estimate the prediction uncertainty using recent advances in Bayesian deep learning. Let p(Y 1 Y 2 |?) denote the preference probability modelled by a neural evaluation metric with parameters ?. Given a training dataset D tr , Bayesian inference involves computing the posterior distribution p(?|D tr ) and marginalization over the parameters ?:</p><formula xml:id="formula_8">p(Y 1 Y 2 |D tr ) = ? p(Y 1 Y 2 |?)p(?|D tr )d?</formula><p>However, computing the true posterior and averaging over all possible parameters is intractable in practice. Hence, several approximations have been proposed in variational inference such as finding a surrogate distribution q ? (?) for the true posterior. <ref type="bibr" target="#b15">Gal and Ghahramani (2016)</ref> have shown that we can use the Dropout distribution <ref type="bibr" target="#b50">(Srivastava et al., 2014)</ref> as the approximate posterior q ? (?). Specifically, we can perform approximate Bayesian inference by applying Dropout during test time. Hence, the posterior can now be approximated with Montecarlo samples as follows:</p><formula xml:id="formula_9">p(Y 1 Y 2 |D tr ) ? 1 L L l=1 p(Y 1 Y 2 |? l )</formula><p>where {? l } L l=1 are L samples from the Dropout distribution q ? (?) (i.e. we apply Dropout L times independently during testing). We now discuss two different Bayesian uncertainty measures: BALD: The Bayesian Active Learning by Disagreement (BALD) <ref type="bibr" target="#b17">(Houlsby et al., 2011</ref>) is defined as the mutual information between the model predictions and the model posterior. Let p l = p(Y 1 Y 2 |? l ), where ? l ? q ? (?), be the evaluation metric prediction using the l th sample ? l from the Dropout distribution. Also, let p = 1 L L l=1 p l be the mean prediction. As shown in <ref type="bibr" target="#b15">(Gal et al., 2017)</ref>, we can approximate the BALD measure using samples from the Dropout distribution as:</p><formula xml:id="formula_10">? = H(p) - 1 L L l=1 H(p l )</formula><p>where H is the binary cross entropy function. The BALD uncertainty score is essentially the difference in entropy of the mean prediction p and the average entropy of the individual predictions {p l } L l=1 . Hence, the BALD uncertainty score is high when the metric's mean prediction is uncertain (high entropy) but the individual predictions are highly confident (low entropy), i.e., when the metric produces disagreeing predictions with high confidence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>STD:</head><p>We also adopt the standard deviation of the preference probability taken over the posterior distribution as a measure of uncertainty:</p><formula xml:id="formula_11">? = Var ??p(?|D tr ) (p(Y 1 Y 2 |?))</formula><p>Similar to BALD, we can approximate the above measure using the empirical standard deviation of samples drawn from the dropout distribution.</p><p>Our proposed algorithm asks for human annotations only if the uncertainty measure (BALD or STD) is above a particular threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">UCB Elimination</head><p>The key idea here is to eliminate a set of "poorly performing" NLG systems using the automatic metric and perform human evaluations with the remaining set of systems. To eliminate sub-optimal systems, we first need to quantify a performance measure for the systems. We use the Copeland score <ref type="bibr">(Zoghi et al., 2015)</ref> which is defined as the normalized total number of pairwise wins for a system:</p><formula xml:id="formula_12">C i = 1 k-1 j =i 1(p ij &gt; 1 2 ).</formula><p>Copeland score is the highest for the top-ranked system with a value of 1 and it is less than 1 for all other systems. To estimate the Copeland score, we first predict the pairwise preference probability between any two systems i and j as follows:</p><formula xml:id="formula_13">pij = 1 N Y 1 ,Y 2 ?D ij p(Y 1 Y 2 |?)</formula><p>where D ij is the test dataset consisting of generated texts from systems i and j, N is the total number of test examples, ? is the learned model parameters. We can now estimate the Copeland score ?i using the estimated preference pij and eliminate all systems with Copeland scores below a threshold. However, a major problem with this approach is that evaluation metrics are often inaccurate and we could wrongly eliminate the true top-ranked system without performing any human evaluations. For example, consider the example where i * is the top-ranked system with p i * j &gt; 0.51 , ?j ? S -i.</p><p>If several of the predicted probabilities pi * j are less than 0.5, our top-ranked system i * will receive a low estimated Copeland score and will be incorrectly eliminated. To overcome this problem, we define an Upper Confidence Bound (UCB) on the preference probability using uncertainty estimates that we described in 4.2. Specifically, the upper confidence bound ?ij is given by ?ij = pij + ?? ij where ? is a hyperparameter that controls the size of the confidence region and ?2 ij is the estimated variance given by:</p><formula xml:id="formula_14">?2 ij = 1 N 2 Y 1 ,Y 2 ?D ij Var ??q ? (?) p(Y 1 Y 2 |?)</formula><p>where q ? (?) is the Dropout distribution. Using the upper confidence estimates ?ij , we now define the optimistic Copeland score for a system i as</p><formula xml:id="formula_15">?u i = 1 K-1 j =i 1(? ij &gt; 1 2 ).</formula><p>Here, we consider a system i to beat another system j (? ij &gt; 0.5) if either the estimated preference is high (p ij is high) or if there is an high uncertainty in the estimation (? ij is high). In UCB Elimination, we eliminate a system only if the optimistic Copeland score is below a threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Setup</head><p>In this section, we describe the (i) NLG tasks and datasets used in our experiments, (ii) automatic evaluation metrics used in our model-based algorithms, and (iii) annotation complexity measure used for comparing dueling bandit algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Tasks &amp; Datasets</head><p>We use a total of 13 datasets spanning 5 tasks in our experiments which are summarized in table 1. Machine Translation (MT): We use 7 human evaluation datasets collected from the WMT news translation tasks <ref type="bibr" target="#b5">(Bojar et al., 2015</ref><ref type="bibr" target="#b4">(Bojar et al., , 2016) )</ref> viz. fin?eng, rus?eng, deu?eng language pairs in WMT 2015 and tur?eng, ron?eng, cze?eng, deu?eng language pairs in WMT 2016. Grammatical Error Correction (GEC): We utilize two human evaluation datasets collected by <ref type="bibr" target="#b29">(Napoles et al., 2019)</ref> where the source texts are  <ref type="bibr">et al., 2020)</ref>. The task here is to generate natural language utterance from dialogue acts.</p><p>Paraphrase Generation: We use human evaluations of model generated English paraphrases released with the ParaBank dataset <ref type="bibr" target="#b18">(Hu et al., 2019)</ref>. Summarization: We make use of the human evaluations <ref type="bibr" target="#b51">(Stiennon et al., 2020)</ref> of GPT3-like transformers on the TL;DR dataset <ref type="bibr" target="#b55">(V?lske et al., 2017)</ref>. We provide further details including preprocessing steps and downloadable links in appendix A.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Automatic NLG Evaluation Metrics</head><p>We can predict the comparison outcome w using two approaches. First, we can use pairwise probability models with existing direct assessment metrics as discussed in section 3. Alternatively, we can train evaluation metrics to directly predict the comparison outcome given pairs of generated texts and context/reference as input. We discuss both these approaches below: Direct Assessment Metrics: We experiment with a total of 10 direct assessment metrics viz. chrF <ref type="bibr" target="#b37">(Popovic, 2015)</ref>, BLEU-4 <ref type="bibr" target="#b35">(Papineni et al., 2002)</ref>, ROUGE-L <ref type="bibr" target="#b24">(Lin, 2004)</ref>, Embedding Average <ref type="bibr" target="#b56">(Wieting et al., 2016)</ref>, Vector Extrema <ref type="bibr" target="#b14">(Forgues et al., 2014)</ref>, Greedy Matching <ref type="bibr" target="#b39">(Rus and Lintean, 2012)</ref>, Laser <ref type="bibr" target="#b0">(Artetxe and Schwenk, 2019)</ref>, BertScore <ref type="bibr" target="#b61">(Zhang et al., 2020)</ref>, <ref type="bibr">MoverScore (Zhao et al., 2019)</ref> and Bleurt <ref type="bibr" target="#b48">(Sellam et al., 2020)</ref>. We mention the implementation details in appendix A. Table <ref type="table">2</ref>: Annotation complexity of the top 7 best performing dueling bandit algorithms along with the uniform exploration algorithm on 13 datasets spanning 5 NLG tasks pretrained Electra-base transformer model <ref type="bibr" target="#b8">(Clark et al., 2020)</ref> to directly predict the comparison outcome w. We curate task-specific human evaluation datasets consisting of tuples of the form (context/reference, hypothesis 1, hypothesis 2, label) for finetuning. Due to space constraints, we mention details on the datasets and finetuning in appendix A.3 and A.4. For the summarization task alone, we couldn't find any pairwise human judgment dataset sufficient for finetuning the Electra model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Annotation Complexity Measure</head><p>To evaluate the performance of dueling bandit algorithms, we define annotation complexity as the minimum number of human annotations needed by an algorithm to identify the top-ranked NLG system with high confidence. Let i * be the actual top-ranked system, and ? * (n) denote the estimated winner by the algorithm after n human annotations, then annotation complexity is defined as:</p><formula xml:id="formula_16">min n : ?n ? n , P ( ? * (n) = i * ) &gt; 1 -? acc</formula><p>where ? acc is the allowable failure probability i.e. the learner can make a mistake with at most ? acc probability. To compute the annotation complexity, we run each dueling bandit algorithm with 200 different random seeds and find the minimum number of human annotations after which the algorithm correctly returns the top-ranked NLG system in at least 190/200 runs (we set ? acc = 0.05).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results &amp; Discussion</head><p>We discuss the performance of dueling bandits algorithms in 6.1, automatic metrics in 6.2 and our proposed model-based algorithms in 6.3. Lastly in 6.4, we analyze the variation of annotation complexity with the number of NLG system. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Analysis of Dueling Bandit Algorithms</head><p>We report the annotation complexity of the top 7 dueling bandit algorithms along with uniform exploration on 13 datasets in table 2. We observe that the annotation complexity of uniform exploration is consistently high across all 13 datasets. In particular, the required human annotations become prohibitively expensive when the number of NLG systems is high, e.g. E2E NLG (16 systems) and ParaBank (28 systems) datasets. On the other hand, dueling bandit algorithms such as RUCB <ref type="bibr">(Zoghi et al., 2014b)</ref>, <ref type="bibr">RCS (Zoghi et al., 2014a)</ref>, RMED <ref type="bibr" target="#b20">(Komiyama et al., 2015)</ref> are able to effectively identify the top-ranked system with much fewer annotations. In particular, RMED performs the best with a reduction of 80.01% in human annotations compared to uniform exploration. We also examine an alternative approach to assess the performance of dueling bandit algorithms. Here, we fix the number of human annotations (fixed annotation budget) and compute the accuracy in predicting the top-ranked system. As we show in figure <ref type="figure">2</ref>, RMED achieves the highest top-rank prediction accuracy for any given number of human annotations. We provide the complete results in appendix F.2. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Performance of Evaluation Metrics</head><p>Before we utilize automatic evaluation metrics using our proposed model-based algorithms, we analyze the effectiveness of these metrics for pairwise NLG evaluations. In table <ref type="table" target="#tab_2">3</ref>, we report the sentencelevel accuracy in predicting the comparison outcome w using direct assessment metrics with the Linear probability model (as discussed in section 3) along with our trained Electra metric. Across the tasks, we observe that metrics that utilize contextualized word embeddings, such as BertScore, perform much better than n-gram and static word embedding-based metrics. In MT, we observe that Bleurt, specifically finetuned on WMT human judgment data, performs the best. In Data-to-Text and Paraphrase generation, our trained Electra metric finetuned on task-specific data significantly outperforms the existing metrics. Interestingly, on the summarization task, all the existing metrics perform much worse than random predictions i.e. they do not add any useful value in evaluation. Hence, we exclude the TLDR dataset from our analysis on model-based algorithms. Finally, as we show in appendix F.3, we observed that the performance is largely similar across all the three probability models: Linear, BTL, and BTL-logistic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Analysis of Model-based Algorithms</head><p>We  Our Uncertainty-aware selection algorithm with the BALD measure further reduces the annotation complexity by around 37% (compared with Random Mixing). We notice that our UCB Elimination algorithm also provides significant improvements over standard RMED. Since UCB Elimination is complementary to Uncertainty-aware selection, we apply both these algorithms together and observe the lowest annotation complexity with a reduction of 89.54% using Electra and 84.00% using Bleurt over standard RMED. Lastly, in figure <ref type="figure" target="#fig_2">3</ref>, we analyze the effect of using other evaluation metrics such as BLEU, BertScore, etc., in Random Mixing. Interestingly, we notice that using metrics such as BLEU, which have low accuracy values, results in a higher annotation complexity than standard (model-free) RMED in some datasets. That is, we may even require a greater number of human annotations to over-compensate for the inaccurate predictions from metrics like BLEU. However, with Laser, MoverScore, and BertScore, we observe significant reductions in annotation complexity. Please refer appendix F.4 for further results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Effect of Number of NLG systems</head><p>We analyze how annotation complexity varies with the number of NLG systems. Specifically, we chose a subset of k systems out of the total 28 systems in the ParaBank dataset and computed the annotation complexity among these k systems. As shown in figure <ref type="figure" target="#fig_3">4</ref>, the annotation complexity of uniform exploration grows quadratically with k as it explores  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Practical Recommendations</head><p>We summarize the key insights from this study and provide practical recommendations on efficiently identifying the top-ranked NLG system.</p><p>1. Use RMED dueling bandit algorithm to actively choose system pairs for comparison. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Related Work</head><p>Several works <ref type="bibr" target="#b3">(Bojar et al., 2014</ref><ref type="bibr" target="#b5">(Bojar et al., , 2015;;</ref><ref type="bibr" target="#b45">Sakaguchi et al., 2014</ref><ref type="bibr" target="#b44">Sakaguchi et al., , 2016) )</ref> in Machine translation and Grammatical Error Correction adopt the TrueSkill algorithm <ref type="bibr" target="#b16">(Herbrich et al., 2006)</ref>, originally used for ranking Xbox gamers, to efficiently rank NLG systems from pairwise annotations. A recent work <ref type="bibr" target="#b43">(Sakaguchi and Durme, 2018)</ref> proposes an online algorithm to rank NLG systems when we receive pairwise preference feedback in the form of a continuous scalar with bounded support. The key difference in our work is that we focus on the problem of identifying the top-rank system instead of ranking all the systems. Experimental study of dueling bandit algorithms have been limited to synthetic simulations in a few works <ref type="bibr" target="#b60">(Yue and Joachims, 2011;</ref><ref type="bibr" target="#b53">Urvoy et al., 2013)</ref>. Most others <ref type="bibr">(Zoghi et al., 2014b,a;</ref><ref type="bibr" target="#b20">Komiyama et al., 2015;</ref><ref type="bibr">Zoghi et al., 2015;</ref><ref type="bibr" target="#b57">Wu and Liu, 2016)</ref> focus on information retrieval applications that involve evaluating search retrieval algorithms <ref type="bibr" target="#b38">(Radlinski et al., 2008)</ref>. To the best of our knowledge, ours is the first work to extensively study the effectiveness of dueling bandit algorithms for NLG evaluation.</p><p>In this work, we focused on the problem of identifying the top-ranked NLG system with few pairwise annotations. We formulated this problem in an Active Evaluation framework and showed that dueling bandit algorithms can reduce the number of human annotations by 80%. We then proposed modelbased algorithms to combine automatic metrics with human evaluations and showed that human annotations can be reduced further by 89%; thereby requiring only a few hundred human annotations to identify the top-ranked system. In future work, we would like to extend our analysis to the general problem of finding the top-k ranked systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion on Ethics &amp; Broader Impact</head><p>Evaluating Natural Language Generation (NLG) models accurately and reliably with few human annotations is an important aspect of NLG research and its real-world applications. Our work shows that we can significantly reduce the number of human annotations required to find the top-ranked NLG system with high confidence. We envision that our work will benefit a wide range of applications such as translation systems, grammatical checkers, etc., where practitioners can find the best NLG model among a set of candidates more accurately and with fewer human annotations. Despite these improvements, there are still several challenges towards reliable NLG evaluation. For example, our model-based approaches, which use automatic metrics, may be subject to biases and other undesirable mistakes, depending on the metric and how they are trained in practice. Our approach may be used to evaluate models that generate fake news, toxic content, or other harmful applications, even though it is not specifically designed for such cases. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Further Details on Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Tasks &amp; Datasets</head><p>In table 5, we report the dataset statistics along with links to download the original datasets. We now discuss the preprocessing steps: Machine Translation: In WMT 2015 and 2016 tasks, human annotators were asked to rank five system outputs (translated sentences) relative to each other. As recommended by the organizers <ref type="bibr" target="#b3">(Bojar et al., 2014)</ref>, we convert each of these rankings into 5 2 pairwise comparisons of systems. Grammatical Error Correction: The Grammarly evaluation datasets follow the RankME <ref type="bibr" target="#b34">(Novikova et al., 2018)</ref> annotation style where annotators were shown 8 outputs side by side for each input and were asked to provide a numerical score to each of them. We discarded one of the outputs out of the 8, which was human crafted, and used the remaining 7 model-generated outputs. We then convert these 7 scores into 7 2 pairwise comparisons of systems. Human evaluations of the CoNLL-2014 Shared Task followed the same process as WMT 2015. Hence, we follow the same preprocessing steps as WMT. Data-to-Text Generation: The E2E NLG Challenge also follows the RankME annotation format. We follow the same preprocessing steps as the Grammarly datasets. Out of the total 21 systems, we held out 5 systems to train the Electra model and use the remaining 16 systems. Paraphrase Generation: For ParaBank, we follow the same preprocessing steps as the Grammarly datasets. Out of the total 35 systems, we held out of 7 systems and only used the remaining 28 systems. Summarization: We select 11 systems that have human annotations between each pair of them. These systems are GPT3-like models with varying model sizes (3B, 6B, 12B) and training strategies. We do not perform any additional preprocessing here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Direct Assessment Metrics: Implementation Details</head><p>We use the nlg-eval library<ref type="foot" target="#foot_0">1</ref> for the implementation of BLEU-4, ROUGE-L, Embedding Average, Vector Extrema, and Greedy Matching. For chrF, Laser and BertScore, we use the implementations from the VizSeq library<ref type="foot" target="#foot_1">2</ref> . We use the official implementation released by the original authors for Mover-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Finetuning Datasets</head><p>Here, we describe the task-specific datasets used for finetuning the Electra model (pairwise evaluation metric described in section 5.2). For MT, we used human evaluations of WMT 2013 and 2014, consisting of a total of 650k examples. For GEC, we curated a training dataset of 180k pairs of texts and human preference using data released by <ref type="bibr">(Napoles et al., 2015b)</ref> and the development set released by <ref type="bibr" target="#b29">(Napoles et al., 2019)</ref>. We utilize 11k examples from 5 held-out systems in the E2E NLG Challenge (apart from the 16 systems used for evaluations) for Data-to-Text generation. Lastly, we use a dataset of 180k examples from 7 held-out systems in the ParaBank dataset for paraphrase generation. We use 90% -10% split for splitting the dataset into train and validation sets. Note that these datasets do not have any overlap with the datasets used for evaluating dueling bandit algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Finetuning Details</head><p>We use the pretrained Electra-base model <ref type="bibr" target="#b8">(Clark et al., 2020)</ref> with 110M parameters (12 layers and 12 attention heads) as our base model. We finetune the model using ADAM optimizer with ? 1 = 0.9 and ? 2 = 0.99. We use a linear learning rate decay with a maximum learning rate of 1e-5 and warm-up for 10% of training. We use a batch size of 128 and finetune for four epochs. We finetune all the models on Google Cloud TPU v3-8. To estimate prediction, we apply Dropout to the Electra model during test time as described in 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Summary of Dueling Bandit Algorithms</head><p>We now provide an overview of various dueling bandit algorithms in the literature. We first introduce a few additional notations and terminologies in B.1. Later in B.2, we describe the various structural assumptions made by different dueling bandit algorithms. Finally, in B.3, we summarize 13 dueling bandit algorithms that we analyze in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Notations and Terminologies</head><p>Let ? ij = p ij -1 2 where p ij is the preference probability of system i over j, as defined in section 2.3. We call a system as the Copeland winner if it beats more number of systems than any other system. Mathematically, a Copeland winner i * is defined as</p><formula xml:id="formula_17">i * = arg max i k j=1 1(? ij &gt; 0). A special case</formula><p>of the Copeland winner is the Condorcet winner, which is the system that beats all other systems. In all our NLG tasks and datasets, we observed that this special case holds true i.e. there exists a system that beats all other k -1 systems, and we define it as the top-ranked system. Nevertheless, we mention these two definitions to distinguish algorithms that work for the general Copeland winner, even if the Condorcet winner does not exist.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Assumptions</head><p>All the dueling bandit algorithms that we analyze in this work assume a stochastic feedback setup in which the feedback is generated according to an underlying (unknown) stationary probabilistic process. Specifically, in our Active Evaluation framework, this is equivalent to assuming that the annotator preference is stationary over time and is given by some fixed distribution p a (w|Y</p><formula xml:id="formula_18">(t) 1 , Y (t)</formula><p>2 ). Further, many dueling bandit algorithms make various on the true pairwise preferences exploit these assumptions to derive theoretical guarantees <ref type="bibr" target="#b2">(Bengs et al., 2021)</ref>. In table 6, we describe the various commonly used assumptions by dueling bandit algorithms. For example, the stochastic triangle inequality assumption (STI), described in row 4 of table 6, assumes that the true preference probabilities between systems obey the triangle inequality. We note here that one cannot verify the validity of these assumptions apriori since we do not have access to the true preferences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Algorithms</head><p>In table 7, we describe the various dueling bandit algorithms along with the assumptions (used to provide theoretical guarantees) and the target winner. We summarize these algorithms below: Interleaved Filtering (IF) <ref type="bibr" target="#b59">(Yue et al., 2012)</ref> algorithm consists of a sequential elimination strategy where a currently selected system s i is compared against the rest of the active systems (not yet eliminated). If the system s j beats a system s i with high confidence, then s i is eliminated, and s j is compared against all other active systems. Similarly, if the system s i beats s j with high confidence, then s j is eliminated, and s i is continued to be compared against the remaining active systems. Under the assumptions of TO, SST, and STI, the authors provide theoretical guarantees for the expected regret achieved by IF.</p><p>BTM: Beat The Mean (BTM) <ref type="bibr" target="#b60">(Yue and Joachims, 2011)</ref>, similar to IF, is an elimination-based algorithm that selects the system s i with the fewest comparisons and compares it with a randomly chosen system from the set of active systems. Based on the comparison outcome, a score and confidence interval are assigned to the system s i . BTM eliminates a system as soon as there is another system with a significantly higher score.</p><p>Knockout, Seq Elim, Single Elim: Knockout <ref type="bibr">(Falahatgar et al., 2017b)</ref>, Sequential Elimination <ref type="bibr">(Falahatgar et al., 2017a)</ref>, Single Elimination <ref type="bibr" target="#b28">(Mohajer et al., 2017)</ref> are all algorithms that proceed in a knockout tournament fashion where the systems are randomly paired, and the winner in each duel will play the next round (losers are knocked out) until the overall winner is determined. During a duel, the algorithm repeatedly compares the two systems to reliably determine the winner. The key difference between the three algorithms is the assumptions they use and how they determine the number of comparisons required to identify the winning system in a duel with high probability. Plackett Luce: Plackett Luce Condorcet winner identification algorithm <ref type="bibr">(Sz?r?nyi et al., 2015)</ref> assumes that the true rank distribution follows the Placket-Luce model <ref type="bibr" target="#b36">(Plackett, 1975)</ref>. The algorithm is based on a budgeted version of QuickSort. The authors show that it achieves a worst-time annotation complexity of the order k log k under the Placket-Luce assumption. RUCB: Relative Upper Confidence Bound (RUCB) <ref type="bibr">(Zoghi et al., 2014b</ref>) is an adaptation of the well-known UCB algorithm <ref type="bibr" target="#b1">(Auer et al., 2002)</ref> to the dueling bandit setup. Similar to UCB, RUCB selects the first system s</p><p>(1) t based on "optimistic" estimates of the pairwise preference probabilities i.e. based on an upper confidence bound of preference probabilities. The second system s</p><p>(2) t is chosen to be the one that is most likely to beat s</p><p>(1) t . RCS: Relative Confidence Sampling (RCS) <ref type="bibr">(Zoghi et al., 2014a)</ref> follows a Bayesian approach by maintaining a posterior distribution over the preference probabilities. At each time step t, the algorithm samples preference probabilities from the posterior and simulates a round-robin tournament among the systems to determine the Condorcet winner. The estimated Condorcet winner is chosen as the first system s</p><p>(1) t and second system s</p><p>(2)</p><p>t is chosen such that it has the best chance of beating s</p><p>(1) t . RMED: Relative Minimum Empirical Diver-gence1 (RMED) algorithm <ref type="bibr" target="#b20">(Komiyama et al., 2015)</ref> maintains an empirical estimate of the "likelihood" that a system is the Condorcet winner. It then uses this estimate to sample the first system s ? a total order over S:</p><formula xml:id="formula_19">i j ?? ? ij &gt; 0 Strong stochastic transitivity (SST) ? ij &gt; 0, ? jk &gt; 0 =? ? ik ? max(? ij , ? jk ) Relaxed stochastic transitivity (RST) ?? ? 1: ? ij &gt; 0, ? jk &gt; 0 =? ?? ik ? max(? ij , ? jk ) Stochastic triangle inequality (STI) ? ij &gt; 0, ? jk &gt; 0 =? ? ik ? ? ij + ? jk Condorcet winner (CW) ?i * : ? i * ,j &gt; 0, ?j ? S -i *</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PL model</head><p>The underlying rank distribution follows the Plackett-Luce (PL) model <ref type="bibr" target="#b36">(Plackett, 1975;</ref><ref type="bibr" target="#b26">Luce, 1979)</ref> Table <ref type="table">6</ref>: Various assumptions made by dueling bandit algorithms in the literature Algorithm Assumptions Target IF <ref type="bibr" target="#b59">(Yue et al., 2012)</ref> TO+SST+STI Condorcet BTM <ref type="bibr" target="#b60">(Yue and Joachims, 2011)</ref> TO+RST+STI Condorcet Seq-Elim. <ref type="bibr">(Falahatgar et al., 2017a)</ref> SST Condorcet Plackett <ref type="bibr">Luce (Sz?r?nyi et al., 2015)</ref> PL model Condorcet Knockout <ref type="bibr">(Falahatgar et al., 2017b)</ref> SST+STI Condorcet Single Elim. <ref type="bibr" target="#b28">(Mohajer et al., 2017)</ref> TO Condorcet RUCB <ref type="bibr">(Zoghi et al., 2014b)</ref> CW Condorcet RCS <ref type="bibr">(Zoghi et al., 2014a)</ref> CW Condorcet RMED <ref type="bibr" target="#b20">(Komiyama et al., 2015)</ref> CW Condorcet SAVAGE <ref type="bibr" target="#b53">(Urvoy et al., 2013)</ref> -Copeland CCB <ref type="bibr">(Zoghi et al., 2015)</ref> -Copeland DTS <ref type="bibr" target="#b57">(Wu and Liu, 2016)</ref> -Copeland DTS++ <ref type="bibr" target="#b57">(Wu and Liu, 2016)</ref> -Copeland</p><p>Table <ref type="table">7</ref>: Summary of dueling bandits algorithms in the literature along with their theoretical assumptions and the target winner of the learner 2013) is a generic algorithm that can be adopted for various ranking problems such as Copeland winner identification. SAVAGE (Copeland) algorithm, at each time step, randomly samples a pair of systems from the set of active system pairs (not yet eliminated) and updates the preference estimates. A system pairs (s i , s j ) is eliminated if either (i) the result of comparison between s i and s j is already known with high probability, or (ii) there exists some system s k where the estimated Copeland score of s k is significantly higher than s i or s j .</p><p>CCB: Copeland Confidence Bound (CCB) <ref type="bibr">(Zoghi et al., 2015)</ref> is similar to the RUCB algorithm but is designed to identify the Copeland Winner (a generalization of the Condorcet winner). The CCB algorithm maintains optimistic preference estimates and uses them to choose the first system s</p><p>(1)</p><p>t and then selects the second system s</p><p>(2) t that is likely to discredit the hypothesis that s</p><p>(1) t is indeed the Copeland winner. The algorithm successively removes all other systems that are highly unlikely to be a Copeland winner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DTS, DTS++:</head><p>The Double Thompson Sampling (DTS) algorithm <ref type="bibr" target="#b57">(Wu and Liu, 2016</ref>) maintains a posterior distribution over the pairwise preference matrix, and selects the system pairs s</p><p>(1) t , s</p><p>(2) t based on two independent samples from the posterior distribution. The algorithm updates the posterior distributions based on the comparison outcome and eliminates systems that are unlikely to be the Copeland winner. DTS++ is an improvement proposed by the authors, which differs from DTS in the way the algorithm breaks ties. Both have the same theoretical guarantees, but DTS++ has been empirically shown to achieve better performance (in terms of regret minimization).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Hyperparameters Details</head><p>We discuss the details of the hyperparameters and the tuning procedure used for dueling bandit algorithm in C.1, pairwise probability models in C.2 and our model-based algorithm in C.3. In all three cases, we use the validation split of the finetuning datasets described in A.3 as our validation dataset. For example, the validation split of the finetuning datasets for MT consists of 10% of the WMT 2013 and 2014 datasets. We use this dataset to tune the hyperparameters for WMT 2015 and 2016 datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Dueling Bandit Algorithms</head><p>For all algorithms other than Knockout and Single Elimination, we use the hyperparameters recommended by the original authors for all the datasets. For example, in the RMED algorithm, described in algorithm 1 of <ref type="bibr" target="#b20">(Komiyama et al., 2015)</ref>, we use f (K) = 0.3K 1.01 as suggested by the authors. For the RCS algorithm, described in algorithm 1 of <ref type="bibr">(Zoghi et al., 2014a)</ref>, we use ? (exploratory constant) = 0.501. For RUCB (algorithm 1 of <ref type="bibr">(Zoghi et al., 2014b</ref>)), we use ? = 0.51. Similarly, for all algorithms other than Knockout and Single Elimination, we use the recommended hyperparameters mentioned in the original paper. For knockout and Single Elimination, we found that the performance was very sensitive to the hyperparameters. For these two algorithms, we manually tuned the hyperparameters on the validation set. In Knockout, algorithm 3 of <ref type="bibr">(Falahatgar et al., 2017b)</ref>, we use = 0.2, ? = 0.05, ? = 1.0 for WMT'16 ron-eng and TLDR OpenAI datasets. We use = 0.2, ? = 0.05, ? = 0.6 for ParaBank and Grammarly-Wiki datasets and = 0.2, ? = 0.09, ? = 0.6 for all other datasets. In Single Elimination, we use m (number of pairwise comparisons per duel) = for WMT'16 ron-eng, E2E NLG, Grammarly-FCE, m = 1500 for CoNLL'14 shared task and m = 500 for all other datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Pairwise Probability Models</head><p>Let f (Y ) be the unnormalized score given an automatic evaluation metric for an hypothesis Y . We preprocess the score f (Y ) to obtain f (Y ) to ensure that the pairwise probability scores is always a valid i.e. lies between 0 and 1. To preprocess the scores, we use the validation dataset consisting of tuples of the form {Y</p><formula xml:id="formula_20">(i) 1 , Y (i) 2 , w (i) } N i=1 where Y (i) 1 , Y (i) 2</formula><p>represent the ith generated texts and w (i) is the corresponding comparison outcome provided by human annotators.</p><formula xml:id="formula_21">Linear: Let ? i = | f (Y (i) 1 ) -f (Y (i)</formula><p>2 )| and ? = max i ? i . We divide the unormalized f (Y ) scores by 2? i.e.</p><formula xml:id="formula_22">f (Y ) = f (Y ) 2? . BTL: Let f m i = max{ f (Y (i) 1 ), f (Y (i)</formula><p>2 )}, f m = max i f m i . We now subtract the scores by f m to ensure that the scores are non-negative i.e.</p><formula xml:id="formula_23">f (Y ) = f (Y ) -f m</formula><p>BTL-Logistic: BTL-Logistic model always provides a score between 0 and 1. However, we found that dividing the scores by a temperature co-efficient ? can provide better results i.e.</p><formula xml:id="formula_24">f (Y ) = f (Y ) ?</formula><p>We tune ? using grid search between 0.005 and 1 on the validation set to minimize the crossentropy loss between the preference probabilities p(Y 1 Y 2 ) and the human labels w.</p><p>Thresholds: As described in section 3, we threshold the preference probabilities p(Y 1 Y 2 ) at two thresholds ? 1 and ? 2 to obtain the predicted comparison outcome ?. We perform a grid search by varying ? 1 from 0.4 to 0.5 and ? 2 from 0.5 to 0.6 with a step size of 0.001. We choose the optimal thresholds that maximize the prediction accuracy on the validation dataset. ity decreases, i.e., with a greater amount of feedback received from Bleurt, the number of required human annotations is lower. However, as shown in figure <ref type="figure">6</ref>, we observe the opposite trend when we use metrics such as BLEU, which are highly inaccurate. In these cases, we require a greater number of human annotations to compensate for the highly erroneous feedback received from the evaluation metric. Therefore, the optimal mixing probability p m in such cases is close to 0 i.e. equivalent to the model-free case. For moderately accurate metrics such as Laser, we observed the optimal p m was close to 0.4 to 0.6. The key insight from these observations is that the higher the accuracy of the metric, the higher amount of feedback can be obtained from the metric to identify the top-ranked system. In figure <ref type="figure">7</ref>, we analyze how the annotation complexity of UCB Elimination with Bleurt varies with the optimistic Copeland threshold ? cop hyperparameter. We fixed ? hyperparameter to 0.6. We observed that UCB Elimination is much more robust to ? cop and a general value of ? cop = 0.8 worked well across all datasets and metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Best Practices in Choosing Hyperparameters</head><p>The optimal approach to choose hyperparameters is usually to tune them on a validation set. But, at times, it may not be possible either because of computational reasons or because a human-annotated validation dataset may not be available. In such cases, we provide a few heuristics based on our previous analysis to choose hyperparameters in our model-based algorithms:</p><p>1. Choose the mixing probability p m in Random Mixing proportionately with the accuracy of the metric. For example, we observed that for metrics with sentence-level prediction accuracy greater than 70%, p m = 0.8 tend to work well. For accuracy between 65% to 70%, p m in the range of 0.5-0.7 worked well.</p><p>2. Once we choose a value of p m , we can find an appropriate BALD threshold ? BALD where 100?p m % of BALD scores are above ? BALD and 100?(1-p m )% of BALD score are below ? BALD . Choosing the BALD threshold this way ensures that we can directly control the desired amount of model-predicted feedback given to the learner.</p><p>3. For UCB Elimination, we recommend using the default values of ? = 0.6 and ? cop = 0.8, which we found to work well across tasks and metrics. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Robustness to Delayed Feedback</head><p>In some instances, human annotations are obtained from multiple crowdsourced annotators in parallel to reduce the time taken for annotations. In such cases, the learner is required to choose the system pairs (s</p><formula xml:id="formula_25">(t) 1 , s<label>(t)</label></formula><p>2 ) to give to some annotator i even before we obtain the result w (t-1) of the previous comparison from some other annotator j. In other words, the learner may experience a delay d &gt; 0 in feedback where at time t, the learner may only have access to the comparison history up to time t -d -1. As shown in figure <ref type="figure">8</ref>, we observe that the top-performing dueling bandit algorithms tend to be robust to delays in feedback. We notice that the variation in the annotation complexity of RMED and RCS as measured by standard deviation is only 64.49 and 62.86, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Additional Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1 Effect of number of NLG systems</head><p>In figure <ref type="figure" target="#fig_7">10</ref>, we compare the variations in annotation complexity of Random Mixing (with Electra metric) using uniform exploration and dueling bandit algorithms. Similar to the model-free case discussed in section 6.4, the annotation complexity of uniform exploration grows as O(k 2 ) but the annotation complexity only varies as O(k) for RMED, We report the annotation complexity of all 13 dueling bandit algorithms on 13 evaluation datasets in table 10. In figure <ref type="figure" target="#fig_0">11</ref>, we show the top-rank prediction accuracy as a function of the number of human annotations for various dueling bandit algorithms on all the datasets, other than WMT 16 tur-eng, which is separately depicted in figure 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.3 Performance of Evaluation Metrics</head><p>In table 11, we report the sentence-level accuracy in predicting the comparison outcome for 10 direct assessment metrics using three probability models along with the trained pairwise metric (Electra). We observe that there is little variation in performance across the three probability models. To further illustrate this, we plot the accuracy on the WMT datasets in figure <ref type="figure" target="#fig_6">9</ref> and observe that the performance is largely similar across Linear, BTL, and BTL-logistic models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.4 Model-based Algorithms</head><p>In figure <ref type="figure" target="#fig_0">12</ref>, we show the top-rank prediction accuracy as a function of the number of human annotations for various model-based algorithms using the Electra metric with RMED. We observe that Random Mixing and Uncertainty-aware Selection (BALD) algorithms have significantly higher prediction accuracy than model-free RMED for any given number of human annotations. Further, when we use UCB Elimination with Uncertainty-aware Selection, we observe the highest top-rank prediction accuracy for any given number of annotations. .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Our Active Evaluation framework consisting of a learner that chooses a pair of systems to compare at each time step. The learner receives feedback from either human annotators or the automatic metric.</figDesc><graphic url="image-1.png" coords="3,103.61,70.88,152.78,236.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 2: Top-rank prediction accuracy v/s number of human annotations used on WMT 16 tur-eng dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Annotation complexity of Random Mixing with RMED using various automatic evaluation metrics</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Annotation complexity of (model-free) uniform exploration and dueling bandit algorithms v/s the number of NLG systems on the ParaBank dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>then selects the second system s (2) t that is most likely to beat s (1) t . SAVAGE: Sensitivity Analysis of VAriables for Generic Exploration (SAVAGE) (Urvoy et al.,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Figure 6: Prediction accuracy v/s number of human annotations collected for Random Mixing with Bluert and BLEU for different mixing probability p m on the WMT 15 deu-eng dataset</figDesc><graphic url="image-2.png" coords="18,306.14,70.87,218.25,101.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>Figure 8: Annotation Complexity v/s delays in feedback on the WMT16 deu-eng dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Annotation complexity of Random Mixing using the Electra metric with uniform exploration and dueling bandit algorithms as function of number of NLG systems on the ParaBank dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Description of tasks and datasets with the number of NLG systems and pairwise human annotations</figDesc><table><row><cell>Task</cell><cell>Dataset</cell><cell># Systems</cell><cell># Human Annotations</cell></row><row><cell></cell><cell>WMT15 fin?eng</cell><cell>14</cell><cell>31577</cell></row><row><cell></cell><cell>WMT15 rus?eng</cell><cell>13</cell><cell>44539</cell></row><row><cell>Machine Translation</cell><cell>WMT15 deu?eng WMT16 tur?eng WMT16 ron?eng</cell><cell>13 9 7</cell><cell>40535 10188 15822</cell></row><row><cell></cell><cell>WMT16 cze?eng</cell><cell>12</cell><cell>125788</cell></row><row><cell></cell><cell>WMT16 deu?eng</cell><cell>10</cell><cell>20937</cell></row><row><cell>Grammatical</cell><cell>Grammarly (FCE)</cell><cell>7</cell><cell>20328</cell></row><row><cell>Error</cell><cell>Grammarly (Wiki)</cell><cell>7</cell><cell>20832</cell></row><row><cell>Correction</cell><cell>CoNLL-2014 Shared Task</cell><cell>13</cell><cell>16209</cell></row><row><cell>Data-to-Text</cell><cell>E2E NLG Challenge</cell><cell>16</cell><cell>17089</cell></row><row><cell>Paraphrase</cell><cell>ParaBank</cell><cell>28</cell><cell>151148</cell></row><row><cell cols="2">Summarization TLDR OpenAI</cell><cell>11</cell><cell>4809</cell></row></table><note><p><p><p><p><p><p>from (i) student essays (FCE), and (ii) formal articles in Wikipedia (Wiki). We also use another GEC dataset collected by</p>(Napoles et al., 2015a)  </p>from the</p>CoNLL-2014 Shared Task (Ng et al., 2014)</p>. Data-to-Text Generation: We use the human evaluation data from the E2E NLG Challenge</p>(Dusek   </p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Sentence-level accuracy of direct assessment metrics with linear probability model and our trained Electra metric in predicting the comparison outcome</figDesc><table><row><cell>Metric</cell><cell>WMT (Avg.)</cell><cell>Gramm. (Avg.)</cell><cell>CoNLL '14 Task</cell><cell>E2E NLG</cell><cell>Para-Bank</cell><cell>TL; DR</cell></row><row><cell>Chrf</cell><cell>62.6</cell><cell>75.7</cell><cell>78.4</cell><cell cols="3">47.4 66.1 34.2</cell></row><row><cell>Bleu</cell><cell>41.5</cell><cell>73.2</cell><cell>78.9</cell><cell cols="3">45.0 63.8 42.8</cell></row><row><cell>Rouge-L</cell><cell>60.7</cell><cell>73.5</cell><cell>78.0</cell><cell cols="3">44.6 64.3 43.3</cell></row><row><cell>Embed. Avg.</cell><cell>56.5</cell><cell>70.1</cell><cell>76.0</cell><cell cols="3">49.8 64.9 38.2</cell></row><row><cell>Greedy Match.</cell><cell>59.5</cell><cell>68.1</cell><cell>77.7</cell><cell cols="3">46.5 64.7 43.1</cell></row><row><cell>Vector Extr.</cell><cell>59.4</cell><cell>66.0</cell><cell>76.3</cell><cell cols="3">44.9 63.7 47.4</cell></row><row><cell>BertScore</cell><cell>65.9</cell><cell>77.4</cell><cell>82.0</cell><cell cols="3">45.9 68.1 44.5</cell></row><row><cell>Laser</cell><cell>65.3</cell><cell>75.1</cell><cell>78.0</cell><cell cols="3">47.2 67.0 35.4</cell></row><row><cell>MoverScore</cell><cell>66.1</cell><cell>74.7</cell><cell>80.6</cell><cell cols="3">50.1 68.0 40.7</cell></row><row><cell>Bleurt</cell><cell>68.2</cell><cell>77.1</cell><cell>81.5</cell><cell cols="3">48.1 67.7 42.5</cell></row><row><cell>Electra (Ours)</cell><cell>65.7</cell><cell>74.0</cell><cell>81.6</cell><cell cols="2">54.3 81.7</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>use our proposed model-based algorithms and incorporate the two best-performing evaluation metrics, viz., Bleurt and Electra with the best performing dueling bandit algorithm, viz., RMED.</figDesc><table /><note><p>We compare the annotation complexity of various model-based algorithms in table 4. We observe that the Random Mixing algorithm with Bleurt and Electra reduces annotation complexity by 70.43% and 73.15%, respectively, when compared to the</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Annotation complexity of model-based algorithms when used with RMED and Bleurt/Electra metric.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Evaluation</cell><cell></cell><cell></cell><cell cols="3">WMT 2016</cell><cell></cell><cell>WMT 2015</cell><cell>Grammarly</cell><cell>CoNLL</cell><cell>E2E</cell><cell>Para-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Metric</cell><cell cols="8">tur-eng ron-eng cze-eng deu-eng fin-eng rus-eng deu-eng FCE Wiki</cell><cell>'14 Task</cell><cell>NLG</cell><cell>Bank</cell></row><row><cell cols="5">None (Model free)</cell><cell></cell><cell cols="2">None</cell><cell cols="2">2028</cell><cell></cell><cell cols="2">5113</cell><cell cols="2">1612</cell><cell>864</cell><cell>1707</cell><cell>1929</cell><cell>4047</cell><cell>2093 5647</cell><cell>9364</cell><cell>3753 24132</cell></row><row><cell cols="5">Random Mixing</cell><cell></cell><cell cols="2">Bleurt Electra</cell><cell></cell><cell>237 728</cell><cell></cell><cell cols="2">1222 3213</cell><cell cols="2">315 385</cell><cell>161 152</cell><cell>275 236</cell><cell>304 512</cell><cell>771 650</cell><cell>406 1529 237 671</cell><cell>9584 3302</cell><cell>1151 15874 326 1044</cell></row><row><cell cols="5">Uncertainty-aware</cell><cell></cell><cell cols="2">Bleurt</cell><cell></cell><cell>103</cell><cell></cell><cell cols="2">1012</cell><cell cols="2">192</cell><cell>84</cell><cell>204</cell><cell>239</cell><cell>530</cell><cell>270</cell><cell>185</cell><cell>9356</cell><cell>1291 22876</cell></row><row><cell cols="5">Selection (STD)</cell><cell></cell><cell cols="2">Electra</cell><cell></cell><cell>978</cell><cell></cell><cell cols="2">7251</cell><cell cols="2">478</cell><cell>210</cell><cell>388</cell><cell>962</cell><cell>1259</cell><cell>477</cell><cell>234</cell><cell>4708</cell><cell>199</cell><cell>2137</cell></row><row><cell cols="5">Uncertainty-aware</cell><cell></cell><cell cols="2">Bleurt</cell><cell></cell><cell>101</cell><cell></cell><cell>653</cell><cell></cell><cell cols="2">136</cell><cell>48</cell><cell>181</cell><cell>162</cell><cell>405</cell><cell>204</cell><cell>128</cell><cell>9356</cell><cell>1167 22619</cell></row><row><cell cols="5">Selection (BALD)</cell><cell></cell><cell cols="2">Electra</cell><cell></cell><cell>737</cell><cell></cell><cell cols="2">1648</cell><cell cols="2">223</cell><cell>114</cell><cell>207</cell><cell>538</cell><cell>488</cell><cell>281</cell><cell>75</cell><cell>1557</cell><cell>67</cell><cell>858</cell></row><row><cell cols="5">UCB Eliminination</cell><cell></cell><cell cols="2">Bleurt Electra</cell><cell></cell><cell>711 264</cell><cell></cell><cell cols="2">2684 649</cell><cell cols="2">1131 1131</cell><cell>573 414</cell><cell>419 294</cell><cell>843 1126</cell><cell>3556 3556</cell><cell>967 1115 3970 1115</cell><cell>8382 2943</cell><cell>2005 14098 1112 9870</cell></row><row><cell cols="4">Uncertainty</cell><cell></cell><cell></cell><cell cols="2">Bleurt</cell><cell></cell><cell>31</cell><cell></cell><cell>415</cell><cell></cell><cell cols="2">376</cell><cell>25</cell><cell>59</cell><cell>82</cell><cell>305</cell><cell>162</cell><cell>39</cell><cell>9995</cell><cell>256</cell><cell>4570</cell></row><row><cell cols="6">(BALD) + UCB Elim.</cell><cell cols="2">Electra</cell><cell></cell><cell>721</cell><cell></cell><cell>736</cell><cell></cell><cell cols="2">144</cell><cell>51</cell><cell>76</cell><cell>288</cell><cell>280</cell><cell>312</cell><cell>45</cell><cell>782</cell><cell>40</cell><cell>2247</cell></row><row><cell>Annotation Complexity</cell><cell cols="2">0.2 0.4 0.6 0.8 1.0 ?10 6</cell><cell></cell><cell>uniform</cell><cell></cell><cell></cell><cell></cell><cell>1 2 3 4 5 6</cell><cell>?10 4</cell><cell></cell><cell>RCS RUCB RMED</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.0</cell><cell>4</cell><cell>8</cell><cell>12</cell><cell>16</cell><cell>20</cell><cell>24</cell><cell>28</cell><cell>4</cell><cell>8</cell><cell>12</cell><cell>16</cell><cell>20</cell><cell>24</cell><cell>28</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">Number of NLG systems</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Description of tasks and datasets with the number of NLG systems, number of pairwise human annotations, label distribution and the downloadable links to the datasets before preprocessing Score and Bleurt. Among these metrics, Bleurt is the only trainable metric. We use the publicly released Bleurt-base checkpoint trained on WMT direct judgments data. As described in section 4.2, we apply Dropout to the Bleurt model during test time to estimate prediction uncertainty.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>BALD for the BALD score at which we decide to ask for human annotations. For UCB elimination, we should choose a threshold ? cop for optimistic Copeland scores and the ? hyperparameter, which controls the size of the confidence region. BALD . Here, as well ? BALD implicitly controls the fraction of real and predicted feedback. In figure 5, we show the effect of p m in Random Mixing with Bleurt and ? BALD in Uncertainty-aware Selection with Bleurt. We observe that with increases in both the hyperparameters, the annotation complex-Tuned Hyperparameters of Model-based algorithms when used with the Bleurt Metric</figDesc><table><row><cell>Dataset</cell><cell>Rand. Mix.</cell><cell>Uncertainty (BALD)</cell><cell cols="2">UCB-Elim.</cell></row><row><cell></cell><cell>p m</cell><cell>? BALD</cell><cell>?</cell><cell>? cop</cell></row><row><cell>WMT (all 7 datasets)</cell><cell>0.8</cell><cell>0.025</cell><cell>0.5</cell><cell>0.8</cell></row><row><cell>Grammarly (FCE &amp; Wiki)</cell><cell>0.8</cell><cell>0.07</cell><cell>0.5</cell><cell>0.8</cell></row><row><cell>CoNLL'14</cell><cell>0.8</cell><cell>0.07</cell><cell>0.5</cell><cell>0.8</cell></row><row><cell>E2E NLG</cell><cell>0.9</cell><cell>0.035</cell><cell>0.5</cell><cell>0.8</cell></row><row><cell>ParaBank</cell><cell>0.95</cell><cell>0.15</cell><cell>0.5</cell><cell>0.8</cell></row><row><cell cols="5">Table 8: Tuned Hyperparameters of Model-based algo-</cell></row><row><cell cols="3">rithms when used with the Electra Metric</cell><cell></cell><cell></cell></row><row><cell cols="3">C.3 Model-based Algorithms</cell><cell></cell><cell></cell></row><row><cell cols="5">We manually tune the hyperparameters in our</cell></row><row><cell cols="5">model-based algorithms on the validation dataset.</cell></row><row><cell cols="5">For clarity, we first describe the hyperparameters in</cell></row><row><cell cols="5">the different model-based algorithms. In Random</cell></row><row><cell cols="5">Mixing, we need to choose the mixing probability</cell></row><row><cell cols="5">p m hyperparameter. In Uncertainty-aware Selec-</cell></row><row><cell cols="5">tion (BALD), we need to choose a threshold value</cell></row><row><cell cols="5">? In ta-</cell></row><row><cell cols="5">ble 8 and 9, we report the tuned hyperparameter</cell></row><row><cell cols="5">values when using Electra and Bleurt (with the</cell></row><row><cell cols="5">Linear probability model) as the evaluation model.</cell></row><row><cell cols="5">Another hyperparameter is the number of Monte-</cell></row><row><cell cols="5">Carlo samples L to obtain from the Dropout distri-</cell></row><row><cell cols="5">bution as discussed in section 4.2. We set L = 20,</cell></row><row><cell cols="5">i.e. we independently apply dropout 20 times for</cell></row><row><cell cols="2">each test predictions.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">D Effect of Hyperparameters in</cell><cell></cell><cell></cell></row><row><cell cols="3">Model-based Algorithms</cell><cell></cell><cell></cell></row><row><cell cols="4">D.1 Sensitivity to Hyperparameters</cell><cell></cell></row><row><cell cols="5">We study how hyperparameters in our proposed</cell></row><row><cell cols="5">model-based algorithms affect annotation complex-</cell></row><row><cell cols="5">ity. Recall that in Random Mixing, the mixing prob-</cell></row><row><cell cols="5">ability p m controls the ratio of real and model gen-</cell></row><row><cell cols="5">erated feedback given to the learner. In Uncertainty-</cell></row><row><cell cols="5">aware Selection (BALD), we obtain human anno-</cell></row><row><cell cols="5">tations when the BALD score is above a threshold</cell></row><row><cell>?</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://github.com/Maluuba/nlg-eval</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://github.com/facebookresearch/vizseq</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We thank the <rs type="institution">Department of Computer Science and Engineering, IIT Madras</rs>, and the <rs type="funder">Robert Bosch Center for Data Science and Artificial Intelligence, IIT Madras</rs> (<rs type="affiliation">RBC-DSAI</rs>), for providing us resources required to carry out this research. We also wish to thank <rs type="institution">Google</rs> for providing access to TPUs through the <rs type="programName">TFRC program</rs>. We thank the anonymous reviewers for their constructive feedback in enhancing the work.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_V6j2gdG">
					<orgName type="program" subtype="full">TFRC program</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Massively multilingual sentence embeddings for zeroshot cross-lingual transfer and beyond</title>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Assoc. Comput. Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="597" to="610" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Finite-time analysis of the multiarmed bandit problem</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicol?</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Fischer</surname></persName>
		</author>
		<idno type="DOI">10.1023/A:1013689704352</idno>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="235" to="256" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Preference-based online learning with dueling bandits: A survey</title>
		<author>
			<persName><forename type="first">Viktor</forename><surname>Bengs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R?bert</forename><surname>Busa-Fekete</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adil</forename><surname>El Mesaoudi-Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eyke</forename><surname>H?llermeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">108</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Findings of the 2014 workshop on statistical machine translation</title>
		<author>
			<persName><forename type="first">Ondrej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Buck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Leveling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Pecina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herve</forename><surname>Saint-Amand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ales</forename><surname>Tamchyna</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/w14-3302</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Workshop on Statistical Machine Translation, WMT@ACL 2014</title>
		<meeting>the Ninth Workshop on Statistical Machine Translation, WMT@ACL 2014<address><addrLine>Baltimore, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06-26">2014. June 26-27, 2014</date>
			<biblScope unit="page" from="12" to="58" />
		</imprint>
	</monogr>
	<note>The Association for Computer Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Findings of the 2016 conference on machine translation</title>
		<author>
			<persName><forename type="first">Ondrej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajen</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Huck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Jimeno-Yepes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varvara</forename><surname>Logacheva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aur?lie</forename><surname>N?v?ol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariana</forename><forename type="middle">L</forename><surname>Neves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Popel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raphael</forename><surname>Rubino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carolina</forename><surname>Scarton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Turchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karin</forename><forename type="middle">M</forename><surname>Verspoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcos</forename><surname>Zampieri</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/w16-2301</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation</title>
		<meeting>the First Conference on Machine Translation<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>The Association for Computer Linguistics</publisher>
			<date type="published" when="2016-08-11">2016. 2016. 2016. August 11-12</date>
			<biblScope unit="page" from="131" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Findings of the 2015 workshop on statistical machine translation</title>
		<author>
			<persName><forename type="first">Ondrej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajen</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Huck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hokamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varvara</forename><surname>Logacheva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carolina</forename><surname>Scarton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Turchi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/w15-3001</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Workshop on Statistical Machine Translation, WMT@EMNLP 2015</title>
		<meeting>the Tenth Workshop on Statistical Machine Translation, WMT@EMNLP 2015<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>The Association for Computer Linguistics</publisher>
			<date type="published" when="2015-09-18">2015. 18 September 2015</date>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Rank analysis of incomplete block designs: I. the method of paired comparisons</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Terry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page">324</biblScope>
			<date type="published" when="1952">1952</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">B</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Mc-Candlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><surname>Amodei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note>NeurIPS 2020, December 6-12, 2020, virtual</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">ELECTRA: pretraining text encoders as discriminators rather than generators</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations, ICLR 2020</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26">2020. April 26-30, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Handling divergent reference texts when evaluating table-to-text generation</title>
		<author>
			<persName><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ankur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipanjan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><surname>Cohen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p19-1483</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019</title>
		<meeting>the 57th Conference of the Association for Computational Linguistics, ACL 2019<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-07-28">2019. July 28-August 2, 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4884" to="4895" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Evaluating the state-of-the-art of end-to-end natural language generation: The E2E NLG challenge</title>
		<author>
			<persName><forename type="first">Ondrej</forename><surname>Dusek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jekaterina</forename><surname>Novikova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Verena</forename><surname>Rieser</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.csl.2019.06.009</idno>
	</analytic>
	<monogr>
		<title level="j">Comput. Speech Lang</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="123" to="156" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Comparing automatic evaluation measures for image description</title>
		<author>
			<persName><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Keller</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/p14-2074</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014<address><addrLine>Baltimore, MD, USA</addrLine></address></meeting>
		<imprint>
			<publisher>The Association for Computer Linguistics</publisher>
			<date type="published" when="2014-06-22">2014. June 22-27, 2014</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="452" to="457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Venkatadheeraj Pichapati, and Vaishakh Ravindrakumar. 2017a. Maxing and ranking with few assumptions</title>
		<author>
			<persName><forename type="first">Moein</forename><surname>Falahatgar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alon</forename><surname>Orlitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09">2017. December 4-9, 2017</date>
			<biblScope unit="page" from="7060" to="7070" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Maximum selection and ranking under noisy comparisons</title>
		<author>
			<persName><forename type="first">Moein</forename><surname>Falahatgar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alon</forename><surname>Orlitsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Venkatadheeraj</forename><surname>Pichapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananda</forename><surname>Theertha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suresh</forename></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning, ICML 2017</title>
		<meeting>the 34th International Conference on Machine Learning, ICML 2017<address><addrLine>Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-06-11">2017. 6-11 August 2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1088" to="1096" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bootstrapping dialog systems with word embeddings</title>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Forgues</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Marie</forename><surname>Larchev?que</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R?al</forename><surname>Tremblay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS, modern machine learning and natural language processing workshop</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dropout as a bayesian approximation: Representing model uncertainty in deep learning</title>
		<author>
			<persName><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33nd International Conference on Machine Learning</title>
		<meeting>the 33nd International Conference on Machine Learning<address><addrLine>New York City, NY, USA; Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-19">2016. 2016. June 19-24, 2016. 2017. 2017. August 2017</date>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="1183" to="1192" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Trueskill tm : A bayesian skill rating system</title>
		<author>
			<persName><forename type="first">Ralf</forename><surname>Herbrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Minka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thore</forename><surname>Graepel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 19, Proceedings of the Twentieth Annual Conference on Neural Information Processing Systems, Vancouver</title>
		<meeting><address><addrLine>British Columbia, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2006-12-04">2006. December 4-7, 2006</date>
			<biblScope unit="page" from="569" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Bayesian active learning for classification and preference learning</title>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ferenc</forename><surname>Huszar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M?t?</forename><surname>Lengyel</surname></persName>
		</author>
		<idno>CoRR, abs/1112.5745</idno>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">PARABANK: monolingual bitext generation and sentential paraphrasing via lexically-constrained neural machine translation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Edward</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Rudinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v33i01.33016521</idno>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence</title>
		<meeting><address><addrLine>Honolulu, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019-01-27">2019. 2019. January 27 -February 1, 2019</date>
			<biblScope unit="page" from="6521" to="6528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Kendall</surname></persName>
		</author>
		<title level="m">Rank correlation methods</title>
		<imprint>
			<date type="published" when="1948">1948</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Regret lower bound and optimal algorithm in dueling bandit problem</title>
		<author>
			<persName><forename type="first">Junpei</forename><surname>Komiyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junya</forename><surname>Honda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hisashi</forename><surname>Kashima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroshi</forename><surname>Nakagawa</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 28th Conference on Learning Theory, COLT 2015</title>
		<meeting>The 28th Conference on Learning Theory, COLT 2015<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07-03">2015. July 3-6, 2015</date>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1141" to="1154" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Importance of search and evaluation strategies in neural dialogue modeling</title>
		<author>
			<persName><forename type="first">Ilia</forename><surname>Kulikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">H</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-8609</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Conference on Natural Language Generation, INLG 2019</title>
		<meeting>the 12th International Conference on Natural Language Generation, INLG 2019<address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-10-29">2019. October 29 -November 1, 2019</date>
			<biblScope unit="page" from="76" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">ACUTE-EVAL: improved dialogue evaluation with optimized questions and multi-turn comparisons</title>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<idno>CoRR, abs/1909.03087</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Beyond user self-reported likert scale ratings: A comparison model for automatic dialog evaluation</title>
		<author>
			<persName><forename type="first">Weixin</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<idno>ArXiv, abs/2005.10716</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">ROUGE: A package for automatic evaluation of summaries</title>
		<author>
			<persName><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Summarization Branches Out</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multilingual denoising pre-training for neural machine translation</title>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Assoc. Comput. Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="726" to="742" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Individual choice behavior: A theoretical analysis</title>
		<author>
			<persName><forename type="first">R</forename><surname>Luce</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sequence effects in crowdsourced annotations</title>
		<author>
			<persName><forename type="first">Nitika</forename><surname>Mathur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d17-1306</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09-09">2017. 2017. September 9-11, 2017</date>
			<biblScope unit="page" from="2860" to="2865" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Active learning for top-k rank aggregation from noisy comparisons</title>
		<author>
			<persName><forename type="first">Soheil</forename><surname>Mohajer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changho</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adel</forename><forename type="middle">M</forename><surname>Elmahdy</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning, ICML 2017</title>
		<title level="s">Proceedings of Machine Learning Research</title>
		<meeting>the 34th International Conference on Machine Learning, ICML 2017<address><addrLine>Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-06-11">2017. 6-11 August 2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="2488" to="2497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Enabling robust grammatical error correction in new domains: Data sets, metrics, and analyses</title>
		<author>
			<persName><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Nadejde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tetreault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="551" to="566" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">2015a. Ground truth for grammaticality correction metrics</title>
		<author>
			<persName><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tetreault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Ground truth for grammaticality correction metrics</title>
		<author>
			<persName><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tetreault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The conll-2014 shared task on grammatical error correction</title>
		<author>
			<persName><forename type="first">Tou</forename><surname>Hwee</surname></persName>
		</author>
		<author>
			<persName><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mei</forename><surname>Siew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ted</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Briscoe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond</forename><forename type="middle">Hendy</forename><surname>Hadiwinoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Susanto</surname></persName>
		</author>
		<author>
			<persName><surname>Bryant</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/w14-1701</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task</title>
		<meeting>the Eighteenth Conference on Computational Natural Language Learning: Shared Task<address><addrLine>CoNLL; Maryland, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2014-06-26">2014. 2014. June 26-27, 2014</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Why we need new evaluation metrics for NLG</title>
		<author>
			<persName><forename type="first">Jekaterina</forename><surname>Novikova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ondrej</forename><surname>Dusek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><forename type="middle">Cercas</forename><surname>Curry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Verena</forename><surname>Rieser</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d17-1238</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-09-09">2017. 2017. September 9-11, 2017</date>
			<biblScope unit="page" from="2241" to="2252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Rankme: Reliable human ratings for natural language generation</title>
		<author>
			<persName><forename type="first">Jekaterina</forename><surname>Novikova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ondrej</forename><surname>Dusek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Verena</forename><surname>Rieser</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n18-2012</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT<address><addrLine>New Orleans, Louisiana, USA; Short Papers</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-06-01">2018. June 1-6, 2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="72" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">BLEU: a method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.3115/1073083.1073135</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The analysis of permutations</title>
		<author>
			<persName><forename type="first">R</forename><surname>Plackett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of The Royal Statistical Society Series C-applied Statistics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="193" to="202" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">chrf: character n-gram f-score for automatic MT evaluation</title>
		<author>
			<persName><forename type="first">Maja</forename><surname>Popovic</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/w15-3049</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Workshop on Statistical Machine Translation, WMT@EMNLP 2015</title>
		<meeting>the Tenth Workshop on Statistical Machine Translation, WMT@EMNLP 2015<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>The Association for Computer Linguistics</publisher>
			<date type="published" when="2015-09-18">2015. 18 September 2015</date>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="392" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">How does clickthrough data reflect retrieval quality?</title>
		<author>
			<persName><forename type="first">Filip</forename><surname>Radlinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madhu</forename><surname>Kurup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
		<idno type="DOI">10.1145/1458082.1458092</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM Conference on Information and Knowledge Management, CIKM 2008</title>
		<meeting>the 17th ACM Conference on Information and Knowledge Management, CIKM 2008<address><addrLine>Napa Valley, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008-10-26">2008. October 26-30, 2008</date>
			<biblScope unit="page" from="43" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A comparison of greedy and optimal assessment of natural language student input using word-to-word similarity metrics</title>
		<author>
			<persName><forename type="first">Vasile</forename><surname>Rus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mihai</surname></persName>
		</author>
		<author>
			<persName><surname>Lintean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Workshop on Building Educational Applications Using NLP, BEA@NAACL-HLT 2012</title>
		<meeting>the Seventh Workshop on Building Educational Applications Using NLP, BEA@NAACL-HLT 2012<address><addrLine>Montr?al, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>The Association for Computer Linguistics</publisher>
			<date type="published" when="2012-06-07">2012. June 7, 2012</date>
			<biblScope unit="page" from="157" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Re-evaluating ADEM: A deeper look at scoring dialogue responses</title>
		<author>
			<persName><forename type="first">B</forename><surname>Ananya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mithun</forename><surname>Sai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitesh</forename><forename type="middle">M</forename><surname>Das Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mukundhan</forename><surname>Khapra</surname></persName>
		</author>
		<author>
			<persName><surname>Srinivasan</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v33i01.33016220</idno>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence</title>
		<meeting><address><addrLine>Honolulu, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019-01-27">2019. 2019. January 27 -February 1, 2019</date>
			<biblScope unit="page" from="6220" to="6227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Improving dialog evaluation with a multi-reference adversarial dataset and large scale pretraining</title>
		<author>
			<persName><forename type="first">B</forename><surname>Ananya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akash</forename><surname>Sai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhartha</forename><surname>Kumar Mohankumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitesh</forename><forename type="middle">M</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><surname>Khapra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Assoc. Comput. Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="810" to="827" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">A survey of evaluation metrics used for NLG systems</title>
		<author>
			<persName><forename type="first">B</forename><surname>Ananya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akash</forename><surname>Sai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitesh</forename><forename type="middle">M</forename><surname>Kumar Mohankumar</surname></persName>
		</author>
		<author>
			<persName><surname>Khapra</surname></persName>
		</author>
		<idno>CoRR, abs/2008.12009</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Efficient online scalar annotation with bounded support</title>
		<author>
			<persName><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1020</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-07-15">2018. July 15-20, 2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="208" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Reassessing the goals of grammatical error correction: Fluency instead of grammaticality</title>
		<author>
			<persName><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><forename type="middle">R</forename><surname>Tetreault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Assoc. Comput. Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="169" to="182" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Efficient elicitation of annotations for human evaluation of machine translation</title>
		<author>
			<persName><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/w14-3301</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Workshop on Statistical Machine Translation, WMT@ACL 2014</title>
		<meeting>the Ninth Workshop on Statistical Machine Translation, WMT@ACL 2014<address><addrLine>Baltimore, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<publisher>The Association for Computer Linguistics</publisher>
			<date type="published" when="2014-06-26">2014. June 26-27, 2014</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Chateval: A tool for chatbot evaluation</title>
		<author>
			<persName><forename type="first">Jo?o</forename><surname>Sedoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daphne</forename><surname>Ippolito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Kirubarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jai</forename><surname>Thirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lyle</forename><surname>Ungar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-4011</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA; Demonstrations</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06-02">2019. June 2-2019</date>
			<biblScope unit="page" from="60" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">What makes a good conversation? how controllable attributes affect human judgments</title>
		<author>
			<persName><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-1170</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-02">2019. June 2-7, 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1702" to="1723" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">BLEURT: learning robust metrics for text generation</title>
		<author>
			<persName><forename type="first">Thibault</forename><surname>Sellam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><forename type="middle">P</forename><surname>Parikh</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.704</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07-05">2020. July 5-10, 2020</date>
			<biblScope unit="page" from="7881" to="7892" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Finding convincing arguments using scalable bayesian preference learning</title>
		<author>
			<persName><forename type="first">Edwin</forename><forename type="middle">D</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Assoc. Comput. Linguistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="357" to="371" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Learning to summarize from human feedback</title>
		<author>
			<persName><forename type="first">L</forename><surname>Nisan Stiennon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">J</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><surname>Christiano</surname></persName>
		</author>
		<idno>ArXiv, abs/2009.01325</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Is this translation error critical?: Classification-based human and automatic machine translation evaluation focusing on critical errors</title>
		<author>
			<persName><forename type="first">Katsuhito</forename><surname>Sudoh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kosuke</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satoshi</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="middle">Bal?zs</forename><surname>Humeval</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R?bert</forename><surname>Sz?r?nyi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Adil</forename><surname>Busa-Fekete</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Eyke</forename><surname>Paul</surname></persName>
		</editor>
		<editor>
			<persName><surname>H?llermeier</surname></persName>
		</editor>
		<meeting><address><addrLine>Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-07">2021. 2015. 2015. December 7-12, 2015</date>
			<biblScope unit="page" from="604" to="612" />
		</imprint>
	</monogr>
	<note>Online rank elicitation for plackett-luce: A dueling bandits approach</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Generic exploration and k-armed voting bandits</title>
		<author>
			<persName><forename type="first">F</forename><surname>Tanguy Urvoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cl?rot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sami</forename><surname>F?raud</surname></persName>
		</author>
		<author>
			<persName><surname>Naamane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09">2017. 2017. December 4-9, 2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Tl;dr: Mining reddit to learn automatic summarization</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>V?lske</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shahbaz</forename><surname>Syed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/w17-4508</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on New Frontiers in Summarization, NFiS@EMNLP 2017</title>
		<meeting>the Workshop on New Frontiers in Summarization, NFiS@EMNLP 2017<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-09-07">2017. September 7, 2017</date>
			<biblScope unit="page" from="59" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Towards universal paraphrastic sentence embeddings</title>
		<author>
			<persName><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-05-02">2016. 2016. May 2-4, 2016</date>
		</imprint>
	</monogr>
	<note>Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Double thompson sampling for dueling bandits</title>
		<author>
			<persName><forename type="first">Huasen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-05">2016. December 5-10, 2016</date>
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">A massively multilingual pre-trained text-to-text transformer</title>
		<author>
			<persName><forename type="first">Linting</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihir</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Barua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<idno>CoRR, abs/2010.11934</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">The k-armed dueling bandits problem</title>
		<author>
			<persName><forename type="first">Yisong</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josef</forename><surname>Broder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jcss.2011.12.028</idno>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Syst. Sci</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1538" to="1556" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Beat the mean bandit</title>
		<author>
			<persName><forename type="first">Yisong</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning, ICML 2011</title>
		<meeting>the 28th International Conference on Machine Learning, ICML 2011<address><addrLine>Bellevue, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Omnipress</publisher>
			<date type="published" when="2011-06-28">2011. June 28 -July 2, 2011</date>
			<biblScope unit="page" from="241" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">WMT 2016 WMT 2015 Grammarly CoNLL &apos;14 Task E2E NLG Para-Bank TL; DR tur-eng ron-eng cze-eng deu-eng fin-eng rus-eng deu-eng FCE Wiki Uniform</title>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varsha</forename><surname>Kishore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<idno>19479 24647 10262 3032 2837 12265 17795 8115 34443 61369 65739 825211 5893 IF 117762 282142 135718 75014 101380 162536 261300 226625 364304 713522 718492 605825 70071 BTM 32010 17456 &gt; 10 6 2249 2926 11108 8328 2778 &gt; 10 6 &gt; 10 6 2541 10175 2038 Seq-Elim. 10824 17514 5899 4440 16590 6881 17937 12851 48068 38554 41037 &gt; 10 6 9046 PL 7011 18513 4774 4618 7859 17049 15215 8037 13156 5682 &gt; 10 6 3871 Knockout 3415 7889 4723 3444 5104 5809 3134 3777 8055 7708 17418 4953 Sing. Elim. 4830 6000 5885 5340 6953 6465 6453 6000 9000 12940 15000 55900 9045 RUCB 3125 5697 3329 1636 1655 4536 6222 2732 5617 19024 10924 41149 1647 RCS 2442 3924 3370 1537 2662 3867 5296 1816 4606 12678 7263 34709 1903 RMED 2028 5113 1612 864 1707 1929 4047 2093 5647 9364 3753 24132 1162 SAVAGE 10289 18016 6639 2393 2675 12806 12115 5767 22959 39208 41493 255208 4733 CCB 7017 11267 5389 2884 4092 11548 10905 4386 10020 21392 16960 87138 2518 DTS 10089 9214 8618 4654 4850 13317 16473 4355 11530 18199 19940 170467 1354 DTS++ 7626 9483 5532 2729 6465 9394 14926 9284 17774 31562 15065 52606</idno>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations, ICLR 2020</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26">2020. April 26-30, 2020</date>
		</imprint>
	</monogr>
	<note>BERTScore: evaluating text generation with BERT</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
