<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Graph Neural Networks with Heterophily</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jiong</forename><surname>Zhu</surname></persName>
							<email>jiongzhu@umich.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<settlement>Ann Arbor</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ryan</forename><forename type="middle">A</forename><surname>Rossi</surname></persName>
							<email>ryrossi@adobe.com</email>
							<affiliation key="aff1">
								<orgName type="department">Adobe Research</orgName>
								<address>
									<settlement>San Jose</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Anup</forename><surname>Rao</surname></persName>
							<email>anuprao@adobe.com</email>
							<affiliation key="aff1">
								<orgName type="department">Adobe Research</orgName>
								<address>
									<settlement>San Jose</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tung</forename><surname>Mai</surname></persName>
							<email>tumai@adobe.com</email>
							<affiliation key="aff1">
								<orgName type="department">Adobe Research</orgName>
								<address>
									<settlement>San Jose</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nedim</forename><surname>Lipka</surname></persName>
							<email>lipka@adobe.com</email>
							<affiliation key="aff1">
								<orgName type="department">Adobe Research</orgName>
								<address>
									<settlement>San Jose</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nesreen</forename><forename type="middle">K</forename><surname>Ahmed</surname></persName>
							<email>nesreen.k.ahmed@intel.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Intel Labs</orgName>
								<address>
									<settlement>Santa Clara</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Danai</forename><surname>Koutra</surname></persName>
							<email>dkoutra@umich.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<settlement>Ann Arbor</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Graph Neural Networks with Heterophily</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Neural Networks (GNNs) have proven to be useful for many different practical applications. However, many existing GNN models have implicitly assumed homophily among the nodes connected in the graph, and therefore have largely overlooked the important setting of heterophily, where most connected nodes are from different classes. In this work, we propose a novel framework called CPGNN that generalizes GNNs for graphs with either homophily or heterophily. The proposed framework incorporates an interpretable compatibility matrix for modeling the heterophily or homophily level in the graph, which can be learned in an end-to-end fashion, enabling it to go beyond the assumption of strong homophily. Theoretically, we show that replacing the compatibility matrix in our framework with the identity (which represents pure homophily) reduces to GCN. Our extensive experiments demonstrate the effectiveness of our approach in more realistic and challenging experimental settings with significantly less training data compared to previous works: CPGNN variants achieve state-of-the-art results in heterophily settings with or without contextual node features, while maintaining comparable performance in homophily settings.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>As a powerful approach for learning and extracting information from relational data, Graph Neural Network (GNN) models have gained wide research interest <ref type="bibr" target="#b26">(Scarselli et al. 2008)</ref> and been adapted in applications including recommendation systems <ref type="bibr" target="#b37">(Ying et al. 2018)</ref>, bioinformatics <ref type="bibr" target="#b39">(Zitnik, Agrawal, and Leskovec 2018;</ref><ref type="bibr" target="#b34">Yan et al. 2019)</ref>, fraud detection <ref type="bibr">(Dou et al. 2020)</ref>, and more. While many different GNN models have been proposed, existing methods have largely overlooked several limitations in their formulations:</p><p>(1) implicit homophily assumptions; (2) heavy reliance on contextual node features. First, many GNN models, including the most popular GNN variant proposed by <ref type="bibr" target="#b12">Kipf and Welling (2017)</ref>, implicitly assume homophily in the graph, where most connections happen among nodes in the same class or with alike features <ref type="bibr" target="#b19">(McPherson, Smith-Lovin, and Cook 2001)</ref>. This assumption has affected the design of many GNN models, which tend to generate similar representations for nodes within close proximity, as studied in previous works <ref type="bibr" target="#b1">(Ahmed et al. 2018;</ref><ref type="bibr" target="#b23">Rossi et al. 2020;</ref><ref type="bibr" target="#b32">Wu et al. 2019)</ref>. However, there are also many instances in the real world where nodes of different classes are more likely to connect with one anotherin idiom, this phenomenon can be described as "opposites attract". As we observe empirically, many GNN models which are designed under implicit homophily assumptions suffer from poor performance in heterophily settings, which can be problematic for applications like fraudster detection <ref type="bibr" target="#b21">(Pandit et al. 2007;</ref><ref type="bibr">Dou et al. 2020)</ref> or analysis of protein structures <ref type="bibr" target="#b6">(Fout et al. 2017)</ref>, where heterophilous connections are common. Second, many existing models rely heavily on contextual node features to derive intermediate representations of each node, which is then propagated within the graph. While in a few networks like citation networks, node features are able to provide powerful node-level contextual information for downstream applications, in more common cases the contextual information are largely missing, insufficient or incomplete, which can significantly degrade the performance for some models. Moreover, complex transformation of input features usually requires the model to adopt a large number of learnable parameters, which need more data and computational resources to train and are hard to interpret.</p><p>In this work, we propose CPGNN, a novel approach that incorporates into GNNs a compatibility matrix that captures both heterophily and homophily by modeling the likelihood of connections between nodes in different classes. This novel design overcomes the drawbacks of existing GNNs mentioned above: it enables GNNs to appropriately learn from graphs with either homophily or heterophily, and is able to achieve satisfactory performance even in the cases of missing and incomplete node features. Moreover, the end-to-end learning of the class compatibility matrix effectively recovers the ground-truth underlying compatibility information, which is hard to infer from limited training data, and provides insights for understanding the connectivity patterns within the graph. Finally, the key idea proposed in this work can naturally be used to generalize many other GNN-based methods by incorporating and learning the heterophily compatibility matrix H in a similar fashion.</p><p>We summarize the main contributions as follows:</p><p>• Heterophily Generalization of GNNs. We describe a generalization of GNNs to heterophily settings by incorporating a compatibility matrix H into GNN-based methods,  which is learned in an end-to-end fashion.</p><formula xml:id="formula_0"># ' → # ' ⋅ # $ , … Neighbors # ! $ ("#$) ! " Self # ' ("#$) Self # ' (")</formula><p>• CPGNN Framework. We propose CPGNN, a novel approach that directly models and learns the class compatibility matrix H in GNN-based methods. This formulation gives rise to many advantages including better effectiveness for graphs with either homophily or heterophily, and for graphs with or without node features. We release CPGNN at https://github.com/GemsLab/CPGNN.</p><p>• Comprehensive Evaluation. We conduct extensive experiments to compare the performance of CPGNN with baseline methods under a more realistic experimental setup by using significantly fewer training data comparing to the few previous works which address heterophily <ref type="bibr" target="#b22">(Pei et al. 2020;</ref><ref type="bibr" target="#b38">Zhu et al. 2020)</ref>. These experiments demonstrate the effectiveness of incorporating the heterophily matrix H into GNN-based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Framework</head><p>In this section we introduce our CPGNN framework, after presenting the problem setup and important definitions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Preliminaries</head><p>Problem Setup. We focus on the problem of semi-supervised node classification on a simple graph G = (V, E), where V and E are the node-and edge-sets respectively, and Y is the set of possible class labels (or types) for v ∈ V. Given a training set T V ⊂ V with known class labels y v for all v ∈ T V , and (optionally) contextual feature vectors x v for v ∈ V, we aim to infer the unknown class labels y u for all u ∈ (V − T V ).</p><p>For subsequent discussions, we use A ∈ {0, 1} |V|×|V| for the adjacency matrix with self-loops removed, y ∈ Y |V| as the ground-truth class label vector for all nodes, and X ∈ R |V|×F for the node feature matrix.</p><p>Definitions. We now introduce two key concepts for modeling the homophily level in the graph with respect to the class labels: (1) homophily ratio, and (2) compatibility matrix.</p><formula xml:id="formula_1">Definition 1 (Homophily Ratio h). Let C ∈ R |Y|×|Y| where C ij = |{(u, v) : (u, v) ∈ E ∧ y u = i ∧ y v = j}|, D = diag({C ii : i = 1, . . . , |Y|})</formula><p>, and e ∈ R |V| be an allones vector. The homophily ratio is defined as h = e De e Ce . The homophily ratio h defined above is good for measuring the overall homophily level in the graph. By definition, we have h ∈ [0, 1]: graphs with h closer to 1 tend to have more edges connecting nodes within the same class, or stronger homophily; on the other hand, graphs with h closer to 0 have more edges connecting nodes in different classes, or a stronger heterophily. However, the actual homophily level is not necessarily uniform within all parts of the graph. One common case is that the homophily level varies among different pairs of classes, where it is more likely for nodes between some pair of classes to connect than some other pairs. To measure the variability of the homophily level, we define the compatibility matrix H as follows:</p><formula xml:id="formula_2">Definition 2 (Compatibility Matrix H). Let Y ∈ R |V|×|Y| where Y vj = 1 if y v = j,</formula><p>and Y vj = 0 otherwise. Then, the compatibility matrix H is defined as:</p><formula xml:id="formula_3">H = (Y AY) (Y AE) (1)</formula><p>where is Hadamard (element-wise) division and E is a |V| × |V| all-ones matrix.</p><p>In node classification settings, compatibility matrix H models the (empirical) probability of nodes belonging to each pair of classes to connect. More generally, H can be used to model any discrete attribute; in that case, H ij is the probability that a node with attribute value i connects with a node with value j. Modeling H in GNNs is beneficial for heterophily settings, but calculating the exact H would require knowledge to the class labels of all nodes in the graph, which violates the semi-supervised node classification setting. Therefore, it is not possible to incorporate exact H into graph neural networks. In the following sections, we propose CPGNN, which is capable of learning H in an end-to-end way based on a rough initial estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Framework Design</head><p>The CPGNN framework consists of two stages: (S1) prior belief estimation; and (S2) compatibility-guided propagation. We visualize the CPGNN framework in Fig. <ref type="figure" target="#fig_1">1</ref>. (S1) Prior Belief Estimation The goal for the first step is to estimate per node v ∈ V a prior belief b v ∈ R |Y| of its class label y v ∈ Y from the node features X. This separate, explicit prior belief estimation stage enables the use of any off-the-shelf neural network classifier as the estimator, and thus can accommodate different types of node features. In this work, we consider the following models as the estimators:</p><p>• MLP, a graph-agnostic multi-layer perceptron. Specifically, the k-th layer of the MLP can be formulated as following:</p><formula xml:id="formula_4">R (k) = σ(R (k−1) W (k) ),<label>(2)</label></formula><p>where W (k) are learnable parameters, and R (0) = X. We call our method with MLP-based estimator CPGNN-MLP. • GCN-Cheby <ref type="bibr" target="#b3">(Defferrard, Bresson, and Vandergheynst 2016)</ref>. We instantiate the model using a 2 nd -order Chebyshev polynomial, where the k-th layer is parameterized as:</p><formula xml:id="formula_5">R (k) = σ 2 i=0 Ti( L)R (k−1) W (k) i . (3) W (k) i</formula><p>are learnable parameters, R (0) = X, and T i ( L) is the i-th order of the Chebyshev polynomial of L = L − I defined recursively as:</p><formula xml:id="formula_6">T i ( L) = 2 LT i−1 ( L) − T i−2 ( L) with T 0 ( L) = I and T 1 ( L) = L = −D − 1 2 AD − 1 2</formula><p>. We refer to our Cheby-based method as CPGNN-Cheby. We note that the performance of CPGNN is affected by the choice of the estimator. It is important to choose an estimator that is not constrained by the homophily assumption (e.g., our above-mentioned choices), so that it does not hinder the performance in heterophilous graphs.</p><p>Denote the output of the final layer of the estimator as R (K) , then the prior belief B p of nodes can be given as</p><formula xml:id="formula_7">Bp = softmax(R (K) )<label>(4)</label></formula><p>To facilitate subsequent discussions, we denote the trainable parameters of a general prior belief estimator as Θ p , and the prior belief of node v derived by the estimator as B p (v; Θ p ).</p><p>(S2) Compatibility-guided Propagation We propagate the prior beliefs of nodes within their neighborhoods using a parameterized, end-to-end trainable compatibility matrix H.</p><p>To propagate the belief vectors through linear formulations, following <ref type="bibr" target="#b7">Gatterbauer et al. (2015)</ref>, we first center B p with</p><formula xml:id="formula_8">B(0) = Bp − 1 |Y| (5)</formula><p>We parameterize the compatibility matrix as H to replace the weight matrix W in traditional GNN models as the end-toend trainable parameter. We formulate each layer as:</p><formula xml:id="formula_9">B(k) = B(0) + A B(k−1) H (6)</formula><p>Each layer propagates and updates the current belief per node in its neighborhood. After K layers, we have the final belief B f = softmax( B(K) ).</p><p>(7) We similarly denote B f (v; H, Θ p ) as the final belief for node v, where parameters Θ p are from the prior belief estimator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Training Procedure</head><p>Pretraining of Prior Belief Estimator. We pretrain the prior belief estimator for β 1 iterations so that H can then be better initialized with informative prior beliefs. Specifically, the pretraining process aims to minimize the loss function</p><formula xml:id="formula_10">Lp(Θp) = v∈T V H (Bp(v; Θp), yv) + λp Θp 2, (8)</formula><p>where H corresponds to the cross entropy, and λ p is the L2 regularization weight for the prior belief estimator. Through an ablation study (App. §D, Fig. <ref type="figure">5</ref>), we show that pretraining prior belief estimator helps increase the final performance. Initialization and Regularization of H. We empirically found that initializing the parameters H with an estimation of the unknown compatibility matrix H can lead to better performance (cf. §4.4, Fig. <ref type="figure" target="#fig_5">4a</ref>). We derive the estimation using node labels in the training set T V , and prior belief B p estimated in Eq. ( <ref type="formula" target="#formula_7">4</ref>) after the pretraining stage. More specifically, denote the training mask matrix M as:</p><formula xml:id="formula_11">[M]i,: = 1, if i ∈ TV 0, otherwise<label>(9)</label></formula><p>and the enhanced belief matrix B, which makes use of known node labels</p><formula xml:id="formula_12">Y train = M • Y in the training set T V , as B = Ytrain + (1 − M) • Bp (<label>10</label></formula><formula xml:id="formula_13">)</formula><p>where • is the element-wise product. The estimation Ĥ of the unknown compatibility matrix H is derived as</p><formula xml:id="formula_14">Ĥ = Sinkhorn-Knopp Y train A B (11)</formula><p>where the use of the <ref type="bibr" target="#b28">Sinkhorn and Knopp (1967)</ref> algorithm is to ensure that Ĥ is doubly stochastic. We find that a doublystochastic and symmetric initial value for H boosts the training when using multiple propagation layers. Thus, we initialize the parameter H as H0 = 1 2 ( Ĥ + Ĥ ) − 1 |Y| , where Ĥ is centered around 0 (similar to B p ). To ensure the rows of H remain centered around 0 throughout the training process, we adopt the following regularization term Φ( H) for H:</p><formula xml:id="formula_15">Φ( H) = i j Hij (12)</formula><p>Loss Function for CPGNN Training. Putting everything together, we obtain the loss function for training CPGNN:</p><formula xml:id="formula_16">L f ( H, Θp) = v∈T V H B f (v; H, Θp), yv + ηLp(Θp) + Φ( H)<label>(13)</label></formula><p>The loss function consists of three parts: (1) the cross entropy loss from the CPGNN output;</p><p>(2) the co-training loss from the prior belief estimator; and (3) a regularization term that keeps H centered around 0 throughout the training process. The latter two terms are novel for the CPGNN formulation, and help increase the performance of CPGNN, as we show later through an ablation study ( §4.4). Intuitively, our separate co-training term for the prior belief estimator measures the distance of prior beliefs to the ground-truth distribution for nodes in the training set while also optimizing the final beliefs.</p><p>In other words, the second term helps keep the accuracy of the prior beliefs throughout the training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Interpretation of Parameters</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H</head><p>Unlike the hard-to-interpret weight matrix W in classic GNNs, parameter H in CPGNN can be easily understood: it captures the probability that node pairs in specific classes connect with each other. Through an inverse of the initialization process, we can obtain an estimation of the compatibility matrix Ĥ after training from learned parameter H as follows:</p><formula xml:id="formula_17">Ĥ = Sinkhorn-Knopp( 1 α H + 1 |Y| )<label>(14)</label></formula><p>where α = min{a ≥ 1 : 1 ≥ 1 a H + 1 |Y| ≥ 0} is a recalibration factor ensuring that the obtained Ĥ is a valid stochastic matrix. In §4.5, we provide an example of the estimated Ĥ after training, and show the improvements in estimation error compared to the initial estimation by Eq. ( <ref type="formula">11</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Theoretical Analysis</head><p>Theoretical Connections. Theorem 1 establishes the theoretical result that CPGNN can be reduced to a simplified version of GCN when H = I. Intuitively, replacing H with I indicates a pure homophily assumption, and thus shows exactly the reason that GCN-based methods have a strong homophily assumption built-in, and therefore perform worse for graphs without strong homophily. Theorem 1. The forward pass formulation of a 1-layer SGC <ref type="bibr" target="#b32">(Wu et al. 2019)</ref>, a simplified version of GCN without the non-linearities and adjacency matrix normalization,</p><formula xml:id="formula_18">B f = softmax ((A + I) XΘ)<label>(15)</label></formula><p>where Θ denotes the model parameter, can be treated as a special case of CPGNN with compatibility matrix H fixed as I and non-linearity removed in the prior belief estimator.</p><p>Proof The formulation of CPGNN with 1 aggregation layer can be written as follows:</p><formula xml:id="formula_19">B f = softmax( B(1) ) = softmax B(0) + A B(0) H<label>(16)</label></formula><p>Now consider a 1-layer MLP (Eq. ( <ref type="formula" target="#formula_4">2</ref>)) as the prior belief estimator. Since we assumed that the non-linearity is removed in the prior belief estimator, we can assume that B p is already centered. Therefore, we have</p><formula xml:id="formula_20">B(0) = Bp = R (K) = R (0) W (0) = XW (0)<label>(17)</label></formula><p>where W (0) is the trainable parameter for MLP. Plug in Eq. ( <ref type="formula" target="#formula_20">17</ref>) into Eq. ( <ref type="formula" target="#formula_19">16</ref>), we have</p><formula xml:id="formula_21">B f = softmax XW (0) + AXW (0) H<label>(18)</label></formula><p>Fixing compatibility matrix H fixed as I, and we have</p><formula xml:id="formula_22">B f = softmax (A + I)XW (0)<label>(19)</label></formula><p>As W (0) is a trainable parameter equivalent to Θ in Eq. ( <ref type="formula" target="#formula_18">15</ref>), the notation is interchangeable. Thus, the simplified GCN formulation as in Eq. ( <ref type="formula" target="#formula_18">15</ref>) can be reduced to a special case of CPGNN with compatibility matrix H = I. ) extra time over the selected prior belief estimator in the propagation stage (S2). Therefore, the overall complexity for CPGNN is largely determined by the time complexity of the selected prior belief estimator: when using MLP as prior belief estimator (Stage S1), the overall time complexity of CPGNN-MLP is O(|E||Y| 2 + |V||Y| + nnz(X)), while the overall time complexity of an α-order CPGNN-Cheby is O(|E||Y| 2 +|V||Y|+ nnz(X)+|E α−1 |d max +|E α |), where d max is the max degree of a node in G and nnz(X) is the number of nonzeros in X.</p><p>The overall space complexity of CPGNN is O(|E| + |V||Y| + |Y| 2 + nnz(X)), which also takes into account the space complexity for the two discussed prior belief estimators above (MLP and GCN-Cheby).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We design experiments to investigate the effectiveness of the proposed framework for node classification with and without contextual features using both synthetic and realworld graphs with heterophily and strong homophily.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Methods and Datasets</head><p>Methods. We test the two formulations discussed in §2.2: CPGNN-MLP and CPGNN-Cheby. Each formulation is tested with either 1 or 2 aggregation layers, leading to 4 variants in total. We compared our methods with the following baselines, some of which are reported to be competitive under heterophily <ref type="bibr" target="#b38">(Zhu et al. 2020)</ref>: GCN (Kipf and Welling 2017), GAT <ref type="bibr" target="#b31">(Veličković et al. 2018)</ref>, <ref type="bibr">GCN-Cheby (Defferrard, Bresson, and Vandergheynst 2016;</ref><ref type="bibr" target="#b12">Kipf and Welling 2017)</ref>, GraphSAGE <ref type="bibr" target="#b8">(Hamilton, Ying, and Leskovec 2017)</ref>, MixHop (Abu-El-Haija et al. 2019), and H 2 GCN <ref type="bibr" target="#b38">(Zhu et al. 2020)</ref>. We also consider MLP as a graph-agnostic baseline. We provide hardware and software specifications and details on hyperparameter tuning in App. B and C. Datasets. We investigate CPGNN using both synthetic and real-world graphs. For synthetic benchmarks, we generate graphs and node labels following an approach similar to <ref type="bibr" target="#b11">Karimi et al. (2017)</ref> and Abu-El-Haija et al. ( <ref type="formula">2019</ref>), which expands the Barabási-Albert model with configurable class compatibility settings. We assign to the nodes feature vectors from the recently announced Open Graph Benchmark (Hu We detail the algorithms for generating synthetic benchmarks in App. A. For real-world graph data, we consider graphs with heterophily and homophily. We use 3 heterophily graphs, namely Texas, Squirrel and Chameleon (Rozemberczki, Allen, and Sarkar 2019), and 3 widely adopted graphs with strong homophily, which are Cora, Pubmed and Citeseer <ref type="bibr" target="#b27">(Sen et al. 2008;</ref><ref type="bibr" target="#b20">Namata et al. 2012)</ref>. We use the features and class labels provided by <ref type="bibr" target="#b22">Pei et al. (2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Node Classification with Contextual Features</head><p>Experimental Setup. For synthetic experiments, we generate 3 synthetic graphs for every heterophily level h ∈ {0, 0.1, 0.2, . . . , 0.9, 1}. We then randomly select 10% of nodes in each class for training, 10% for validation, and 80% for testing, and report the average classification accuracy as performance of each model on all instances with the same level of heterophily. Using synthetic graphs for evaluation enables us to better understand how the model performance changes as a function of the level of heterophily in the graph. Hence, we vary the level of heterophily in the graph going from strong heterophily all the way to strong homophily while holding other factors constant such as degree distribution and differences in contextual features. On real-world graphs, we generate 10 random splits for training, validation and test sets; for each split we randomly select 10% of nodes in each class to form the training set, with another 10% for the validation set and the remaining as the test set. Notice that we are using a significantly smaller fraction of training samples compared to previous works that address heterophily <ref type="bibr" target="#b22">(Pei et al. 2020;</ref><ref type="bibr" target="#b38">Zhu et al. 2020)</ref>. This is a more realistic assumption in many real-world applications. Synthetic Benchmarks. We compare the performance of CPGNN to the state-of-the-art methods in Fig. <ref type="figure">2</ref>. Notably, we observe that CPGNN-Cheby-1 consistently outperforms all baseline methods across the full spectrum of low to high homophily (or high to low heterophily). Furthermore, compared to our CPGNN variants, it performs the best in all settings with h ≥ 0.3. For h &lt; 0.3, CPGNN-MLP-1 outperforms it, and in fact performs the best overall for graphs with strong heterophily. More importantly, CPGNN has a significant performance improvement over all state-of-the-art methods. In particular, by incorporating and learning the class compatibility matrix H in an end-to-end fashion, we find that CPGNN-Cheby-1 achieves a gain of up to 7% compared to GCN-Cheby in heterophily settings, while CPGNN-MLP-1 performs up to 30% better in heterophily and 50% better in homophily compared to the graph-agnostic MLP model.</p><p>Real-World Graphs with Heterophily. Results for graphs with heterophily are presented in Table <ref type="table" target="#tab_2">2</ref>. Notably, the best performing methods for each graph are always one of the CPGNN methods from the proposed framework, which demonstrates the importance of incorporating and learning the compatibility matrix H into GNNs. Overall, we observe that CPGNN-Cheby-1 performs the best overall with respect to mean accuracy across all the graphs. Notably, CPGNN-Cheby-1 significantly outperforms the other baseline methods achieving improvements between 1.68% and 10.64% in mean accuracy compared to GNN baselines. These results demonstrate the effectiveness of CPGNN in heterophily settings on real-world benchmarks. We note that our empirical analysis also confirms the small time complexity overhead of CPGNN: on the Squirrel dataset, the runtimes of CPGNN-MLP-1 and CPGNN-Cheby-1 are 39s and 697s, respectively, while the prior belief estimators, MLP and GCN-Cheby, run in 29s and 592s in our implementation.</p><p>Real-World Graphs with Homophily. For the real-world graphs with homophily, we report the results for each method in Table <ref type="table" target="#tab_3">3</ref>. Recall that our framework generalizes GNN for both homophily and heterophily. We find in Table <ref type="table" target="#tab_3">3</ref>, the methods from the proposed framework perform better or comparable to the baselines, including those which have an implicit assumption of strong homophily. Therefore, our methods are more universal while able to maintain the same level of performance as those that are optimized under a strict homophily assumption. As an aside, we observe that CPGNN-Cheby-1 is the best performing method on Pubmed. Summary. For the common settings of semi-supervised node classification with contextual features available, the above results show that CPGNN variants have the best performance in heterophily settings while maintaining comparable performance in the homophily settings. Considering both the heterophily and homophily settings, CPGNN-Cheby-1 is the best method overall, which ranked first in the heterophily settings and second in homophily settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Node Classification without Features</head><p>Most previous work on semi-supervised node classification have focused only on graphs that have contextual features on the nodes. However, the vast majority of graph data does not have such node-level features <ref type="bibr" target="#b23">(Rossi and Ahmed 2015)</ref>, which greatly limits the utility of the methods proposed in prior work that assume such features are available. Therefore, we conduct extensive experiments on semi-supervised node classification without contextual features using the same realworld graphs as before.</p><p>Experimental Setup. To investigate the performance of CPGNN and baselines when contextual feature vectors are not available for nodes in the graph, we follow the approach as <ref type="bibr" target="#b12">Kipf and Welling (2017)</ref> by replacing the node features X in each benchmark with an identity matrix I. We use the training, validation and test splits provided by <ref type="bibr" target="#b22">Pei et al. (2020)</ref>.</p><p>Heterophily. We report results on graphs with strong heterophily under the featureless settings in Table <ref type="table" target="#tab_4">4</ref>. We observe that the best performing methods for each dataset are again all CPGNN variants. From the mean accuracy perspective, all CPGNN variants outperform all baselines except H 2 GCN, which is also proposed to handle heterophily, in the overall performance; CPGNN-MLP-1 has the best overall performance, followed by CPGNN-Cheby-1. It is also worth noting that the performance of GCN-Cheby and MLP, upon which our prior belief estimator is based on, are significantly worse than other methods. This demonstrates the effectiveness of incorporating the class compatibility matrix H in GNN models and learning it in an end-to-end fashion.</p><p>Homophily. We report the results in Table <ref type="table" target="#tab_5">5</ref>. The featureless setting for graphs with strong homophily is a fundamentally easier task compared to graphs with strong heterophily, especially for methods with implicit homophily assumptions, as they tend to yield highly similar prediction within the proximity of each node. Despite this, the CPGNN variants still perform comparably to the state-of-the-art methods. Summary. Under the featureless settings, the above results show that CPGNN variants achieve state-of-the-art performance in heterophily settings, while achieving comparable performance in the homophily settings. Considering both the heterophily and homophily settings, CPGNN-Cheby-1 is again the best method overall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>To evaluate the effectiveness of our model design, we conduct an ablation study by examining variants of CPGNN-MLP-1 with one design element removed at a time. Fig. <ref type="figure" target="#fig_5">4</ref> presents  <ref type="formula">12</ref>) from the overall loss function (Eq. ( <ref type="formula" target="#formula_16">13</ref>)). In Fig. <ref type="figure" target="#fig_5">4a</ref>, we see that replacing the initializer can lead to up to 30% performance drop for the model, while removing the regularization term can cause up to 6% decrease in performance. These results support our claim that initializing H using pretrained prior beliefs and known labels in the training set and regularizing the H around 0 lead to better overall performance.</p><p>End-to-end Training of H. To demonstrate the performance gain through end-to-end training of CPGNN after the initialization of H, we compare the final performance of CPGNN-MLP-1 with the performance after H is initialized; Fig. <ref type="figure" target="#fig_5">4b</ref> shows the results. From the results, we see that the end-toend training process of CPGNN has contributed up to 21% performance gain. We believe such performance gain is due to a more accurate H learned through the training process, as demonstrated in the next subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Heterophily Matrix Estimation</head><p>As described in §2.4, we can obtain an estimation Ĥ of the class compatiblity matrix H ∈ [0, 1] |Y|×|Y| through the learned parameter H. To measure the accuracy of the estimation Ĥ, we calculate the average error of each element for the estimated Ĥ as following: δH = | Ĥ−H| |Y| 2 .    Fig. <ref type="figure" target="#fig_4">3</ref> shows an example of the obtained estimation Ĥ on the synthetic benchmark syn-products with homophily ratio h = 0 using heatmaps, along with the initial estimation derived following §2.3 which CPGNN optimizes upon, and the ground truth empirical compatibility matrix as defined in Def. 2. From the heatmap, we can visually observe the improvement of the final estimation upon the initial estimation. The curve of the estimation error with respect to the number of training epochs also shows that the estimation error decreases throughout the training process, supporting the observations through the heatmaps. These results illustrate the interpretability of parameters H, and effectiveness of our modeling of heterophily matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>SSL before GNNs. The problem of semi-supervised learning (SSL) or collective classification <ref type="bibr" target="#b27">(Sen et al. 2008;</ref><ref type="bibr" target="#b18">McDowell, Gupta, and Aha 2007;</ref><ref type="bibr" target="#b24">Rossi et al. 2012</ref>) can be solved with iterative methods (J. <ref type="bibr" target="#b10">Neville 2000;</ref><ref type="bibr" target="#b17">Lu and Getoor 2003)</ref>, graph-based regularization and probabilistic graphical models <ref type="bibr" target="#b16">(London and Getoor 2014)</ref>. Among these methods, our approach is related to belief propagation (BP) <ref type="bibr" target="#b36">(Yedidia, Freeman, and Weiss 2003;</ref><ref type="bibr" target="#b25">Rossi et al. 2018)</ref>, a message-passing approach where each node iteratively sends its neighboring nodes estimations of their beliefs based on its current belief, and updates its own belief based on the estimations received from its neighborhood. <ref type="bibr" target="#b13">Koutra et al. (2011)</ref> and <ref type="bibr" target="#b7">Gatterbauer et al. (2015)</ref> have proposed linearized versions which are faster to compute. However, these approaches require the class-compatibility matrix to be determined before the inference stage, and cannot support end-to-end training.</p><p>GNNs. In recent years, graph neural networks (GNNs) have become increasingly popular for graph-based semisupervised node classification problems thanks to their ability to learn through end-to-end training. <ref type="bibr" target="#b3">Defferrard, Bresson, and Vandergheynst (2016)</ref> proposed an early version of GNN by generalizing convolutional neural networks (CNNs) from regular grids (e.g., images) to irregular grids (e.g., graphs). <ref type="bibr" target="#b12">Kipf and Welling (2017)</ref> introduced GCN, a popular GNN model which simplifies the previous work. Other GNN models that have gained wide attention include Planetoid <ref type="bibr" target="#b35">(Yang, Cohen, and</ref><ref type="bibr" target="#b35">Salakhudinov 2016) and</ref><ref type="bibr">GraphSAGE (Hamilton, Ying, and</ref><ref type="bibr" target="#b8">Leskovec 2017)</ref>. More recent works have looked into designs which strengthen the effectiveness of GNN to capture graph information: GAT <ref type="bibr" target="#b31">(Veličković et al. 2018)</ref> and AGNN <ref type="bibr" target="#b30">(Thekumparampil et al. 2018)</ref>   <ref type="bibr" target="#b14">(Li et al. 2019</ref><ref type="bibr" target="#b15">(Li et al. , 2020;;</ref><ref type="bibr" target="#b23">Rong et al. 2020)</ref>.</p><p>Although many of these GNN methods work well when the data exhibits strong homophily, none of these methods (except Geom-GCN) was proposed to address the challenging and largely overlooked setting of heterophily, and many of them perform poorly in this setting. Recently, <ref type="bibr" target="#b38">Zhu et al. (2020)</ref> discussed effective designs which improve the representation power of GNNs under heterophily through theoretical and empirical analysis. Going beyond these designs that prior GNN works have leveraged, we propose a new GNN framework that elegantly combines the powerful notion of compatibility matrix H from belief propagation with end-to-end training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We propose CPGNN, an approach that models an interpretable class compatibility matrix into the GNN framework, and conduct extensive empirical analysis under more realistic settings with fewer training samples and a featureless setup. Through theoretical and empirical analysis, we have shown that the proposed model overcomes the limitations of existing GNN models, especially in the complex settings of heterophily graphs without contextual features.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: The general pipeline of the proposed framework (CPGNN) with k propagation layers ( §2.2). As an example, we use a graph with mixed homophily and heterophily, with node colors representing class labels: nodes in green show strong homophily, while nodes in orange and purple show strong heterophily. CPGNN framework first generates prior belief estimations using an off-the-shelf neural network classifier, which utilizes node features if available (S1). The prior beliefs are then propagated within their neighborhoods guided by the learned compatibility matrix H, and each node aggregates beliefs sent from its neighbors to update its own beliefs (S2). We describe the backward training process, including how H can be learned end-to-end in §2.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Heterophily matrices H for empirical (ground truth), initial and final estimation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Error of compatibility matrix estimation Ĥ throughout training process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Heterophily matrices H and estimation error of H for a h = 0 instance of syn-products dataset.</figDesc><graphic url="image-4.png" coords="7,68.77,52.34,79.67,64.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Ablation Study: Mean accuracy as a function of h. (a): When replacing H initialization with glorot or removing H regularization, the performance of CPGNN drops significantly; (b): The significant increase in performance shows the effectiveness of the end-to-end training in our framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>introduced an edge-level attention mechanism; MixHop (Abu-El-Haija et al. 2019) and Geom-GCN (Pei et al. 2020) designed aggregation schemes which go beyond the immediate neighborhood of each node; the jumping knowledge network (Xu et al. 2018) leverages representations from intermediate layers; GAM (Stretcu et al. 2019) and GMNN (Qu, Bengio, and Tang 2019) use a separate model to capture the agreement or joint distribution of labels in the graph. To capture more graph information, recent works trained very deep networks with 100+ layers</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Time and Space Complexity of CPGNN Let |E| and |V| denote the number of edges and nodes in G, respectively. Further, let |E i | denote the number of node pairs in G within i-hop distance (e.g., |E 1 | = |E|) and |Y| denotes the number of unique class labels. We assume the graph adjacency matrix A and node feature matrix X are stored as sparse matrices. CPGNN only introduces O(|E||Y| 2</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Statistics for our synthetic and real graphs.</figDesc><table><row><cell>Dataset</cell><cell cols="5">#Nodes #Edges #Classes #Features Homophily |V| |E| |Y| F h</cell></row><row><cell>syn-products</cell><cell>10,000</cell><cell>59,640-59,648</cell><cell>10</cell><cell>100</cell><cell>[0, 0.1, . . . , 1]</cell></row><row><cell>Texas</cell><cell>183</cell><cell>295</cell><cell>5</cell><cell>1703</cell><cell>0.11</cell></row><row><cell>Squirrel</cell><cell cols="2">5,201 198,493</cell><cell>5</cell><cell>2,089</cell><cell>0.22</cell></row><row><cell>Chameleon</cell><cell>2,277</cell><cell>31,421</cell><cell>5</cell><cell>2,325</cell><cell>0.23</cell></row><row><cell>CiteSeer</cell><cell>3,327</cell><cell>4,676</cell><cell>7</cell><cell>3,703</cell><cell>0.74</cell></row><row><cell>Pubmed</cell><cell>19,717</cell><cell>44,327</cell><cell>3</cell><cell>500</cell><cell>0.8</cell></row><row><cell>Cora</cell><cell>2,708</cell><cell>5,278</cell><cell>6</cell><cell>1,433</cell><cell>0.81</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Accuracy on heterophily graphs with features.</figDesc><table><row><cell></cell><cell>Texas</cell><cell>Squirrel</cell><cell cols="2">Chameleon Mean</cell></row><row><cell>Hom. ratio h</cell><cell>0.11</cell><cell>0.22</cell><cell>0.23</cell><cell>Acc</cell></row><row><cell>CPGNN-MLP-1</cell><cell cols="2">63.75±4.74 32.70±1.90</cell><cell>51.08±2.29</cell><cell>49.18</cell></row><row><cell>CPGNN-MLP-2</cell><cell cols="2">70.42±2.97 26.64±1.23</cell><cell>55.46±1.42</cell><cell>50.84</cell></row><row><cell cols="3">CPGNN-Cheby-1 63.13±5.72 37.03±1.23</cell><cell>53.90±2.61</cell><cell>51.35</cell></row><row><cell cols="3">CPGNN-Cheby-2 65.97±8.78 27.92±1.53</cell><cell>56.93±2.03</cell><cell>50.27</cell></row><row><cell>H2GCN</cell><cell cols="2">71.39±2.57 29.50±0.77</cell><cell>48.12±1.96</cell><cell>49.67</cell></row><row><cell>GraphSAGE</cell><cell cols="2">67.36±3.05 34.35±1.09</cell><cell>45.45±1.97</cell><cell>49.05</cell></row><row><cell>GCN-Cheby</cell><cell cols="2">58.96±3.04 26.52±0.92</cell><cell>36.66±1.84</cell><cell>40.71</cell></row><row><cell>MixHop</cell><cell cols="2">62.15±2.48 36.42±3.43</cell><cell>46.84±3.47</cell><cell>48.47</cell></row><row><cell>GCN</cell><cell cols="2">55.90±2.05 33.31±0.89</cell><cell>52.00±2.30</cell><cell>47.07</cell></row><row><cell>GAT</cell><cell cols="2">55.83±0.67 31.20±2.57</cell><cell>50.54±1.97</cell><cell>45.86</cell></row><row><cell>MLP</cell><cell cols="2">64.65±3.06 25.50±0.87</cell><cell>37.36±2.05</cell><cell>42.50</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Accuracy on homophily graphs with features.</figDesc><table><row><cell></cell><cell>Citeseer</cell><cell>Pubmed</cell><cell>Cora</cell><cell>Mean</cell></row><row><cell>Hom. ratio h</cell><cell>0.74</cell><cell>0.8</cell><cell>0.81</cell><cell>Acc</cell></row><row><cell>CPGNN-MLP-1</cell><cell cols="4">71.30±1.11 86.40±0.36 77.40±1.10 78.37</cell></row><row><cell>CPGNN-MLP-2</cell><cell cols="4">71.48±1.85 85.31±0.70 81.24±1.26 79.34</cell></row><row><cell cols="5">CPGNN-Cheby-1 72.04±0.53 86.68±0.20 83.64±1.31 80.79</cell></row><row><cell cols="5">CPGNN-Cheby-2 72.06±0.51 86.66±0.24 81.62±0.97 80.11</cell></row><row><cell>H2GCN</cell><cell cols="4">71.76±0.64 85.93±0.40 83.43±0.95 80.37</cell></row><row><cell>GraphSAGE</cell><cell cols="4">71.74±0.66 85.66±0.53 81.60±1.16 79.67</cell></row><row><cell>GCN-Cheby</cell><cell cols="4">72.04±0.58 86.43±0.31 83.29±1.20 80.58</cell></row><row><cell>MixHop</cell><cell cols="4">73.23±0.60 85.12±0.29 85.34±1.23 81.23</cell></row><row><cell>GCN</cell><cell cols="4">72.27±0.52 86.42±0.27 83.56±1.21 80.75</cell></row><row><cell>GAT</cell><cell cols="4">72.63±0.87 84.48±0.22 79.57±2.12 78.89</cell></row><row><cell>MLP</cell><cell cols="4">66.52±0.99 84.70±0.33 64.81±1.20 72.01</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Accuracy on heterophily graphs without features.</figDesc><table><row><cell></cell><cell>Texas</cell><cell>Squirrel</cell><cell cols="2">Chameleon Mean</cell></row><row><cell>Hom. ratio h</cell><cell>0.11</cell><cell>0.22</cell><cell>0.23</cell><cell>Acc</cell></row><row><cell>CPGNN-MLP-1</cell><cell cols="2">64.05±7.65 55.19±1.88</cell><cell>68.38±3.48</cell><cell>62.54</cell></row><row><cell>CPGNN-MLP-2</cell><cell cols="2">65.14±9.99 36.37±2.08</cell><cell>70.18±2.64</cell><cell>57.23</cell></row><row><cell cols="3">CPGNN-Cheby-1 63.78±7.67 54.76±2.01</cell><cell>67.19±2.18</cell><cell>61.91</cell></row><row><cell cols="3">CPGNN-Cheby-2 70.27±8.26 26.42±1.20</cell><cell>68.25±1.57</cell><cell>54.98</cell></row><row><cell>H2GCN</cell><cell cols="2">68.38±6.98 50.91±1.71</cell><cell>62.41±2.14</cell><cell>60.57</cell></row><row><cell>GraphSAGE</cell><cell cols="2">67.03±4.90 36.90±2.36</cell><cell>58.53±2.20</cell><cell>54.15</cell></row><row><cell>GCN-Cheby</cell><cell cols="2">50.00±8.08 12.62±0.73</cell><cell>14.93±1.53</cell><cell>25.85</cell></row><row><cell>MixHop</cell><cell cols="2">57.57±5.56 33.54±2.08</cell><cell>50.15±2.78</cell><cell>47.08</cell></row><row><cell>GCN</cell><cell cols="2">51.08±7.48 43.78±1.39</cell><cell>62.04±2.17</cell><cell>52.30</cell></row><row><cell>GAT</cell><cell cols="2">57.03±4.31 42.46±2.08</cell><cell>60.31±2.61</cell><cell>53.26</cell></row><row><cell>MLP</cell><cell cols="2">44.86±9.29 19.77±0.80</cell><cell>20.57±2.29</cell><cell>28.40</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Accuracy on homophily graphs without features.</figDesc><table><row><cell></cell><cell>Citeseer</cell><cell>Pubmed</cell><cell>Cora</cell><cell>Mean</cell></row><row><cell>Hom. ratio h</cell><cell>0.74</cell><cell>0.8</cell><cell>0.81</cell><cell>Acc</cell></row><row><cell>CPGNN-MLP-1</cell><cell cols="4">65.70±2.96 81.98±0.36 81.97±1.24 76.55</cell></row><row><cell>CPGNN-MLP-2</cell><cell cols="4">67.66±2.29 82.33±0.39 82.37±1.70 77.46</cell></row><row><cell cols="5">CPGNN-Cheby-1 67.93±2.86 82.44±0.58 83.76±1.81 78.04</cell></row><row><cell cols="5">CPGNN-Cheby-2 67.39±2.69 82.27±0.54 83.02±1.29 77.56</cell></row><row><cell>H2GCN</cell><cell cols="4">68.37±2.93 82.97±0.37 83.22±1.56 78.19</cell></row><row><cell>GraphSAGE</cell><cell cols="4">66.71±3.27 77.86±3.84 81.77±2.00 75.45</cell></row><row><cell>GCN-Cheby</cell><cell cols="4">67.56±3.24 79.14±0.38 83.66±1.02 76.79</cell></row><row><cell>MixHop</cell><cell cols="4">68.38±3.06 82.72±0.75 84.73±1.80 78.61</cell></row><row><cell>GCN</cell><cell cols="4">67.14±3.15 82.28±0.50 83.34±1.38 77.59</cell></row><row><cell>GAT</cell><cell cols="4">68.64±3.27 81.92±0.33 81.79±2.21 77.45</cell></row><row><cell>MLP</cell><cell cols="4">19.78±1.35 39.58±0.69 21.61±1.92 26.99</cell></row><row><cell cols="5">the results for the ablation study, with more detailed results</cell></row><row><cell cols="5">presents in Table A.2 in Appendix. We also discussed the</cell></row><row><cell cols="5">effectiveness of co-training and pretraining in Appendix  §D.</cell></row><row><cell cols="5">Initialization and Regularization of H. Here we study 2 variants of CPGNN-MLP-1: (1) No H initialization, when H is initialized using glorot initialization (similar to other</cell></row><row><cell cols="5">GNN formulations) instead of our initialization process de-scribed in  § 2.3. (2) No H regularization, where we remove the regularization term Φ( H) as defined in Eq. (</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the reviewers for their constructive feedback. This material is based upon work supported by the National Science Foundation under CAREER Grant No. IIS 1845491, Army Young Investigator Award No. W911NF1810397, an Adobe Digital Experience research faculty award, an Amazon faculty award, a Google faculty award, and AWS Cloud Credits for Research. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Quadro P6000 GPU used for this research. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation or other funding parties.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">MixHop: Higher-Order Graph Convolution Architectures via Sparsified Neighborhood Mixing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Alipourfard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">V</forename><surname>Steeg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Galstyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning Role-based Graph Embeddings</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Willke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Eldardiry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Emergence of scaling in random networks</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Barabasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Albert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">286</biblScope>
			<biblScope unit="issue">5439</biblScope>
			<biblScope unit="page" from="509" to="512" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Y</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yu</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Enhancing graph neural network-based fraud detectors against camouflaged fraudsters</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management</title>
				<meeting>the 29th ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<biblScope unit="page" from="315" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Protein interface prediction using graph convolutional networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shariat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ben-Hur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6530" to="6539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Linearized and single-pass belief propagation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Gatterbauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koutra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
				<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="581" to="592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Inductive Representation Learning on Large Graphs</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00687</idno>
		<title level="m">Open Graph Benchmark: Datasets for Machine Learning on Graphs</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Iterative classification in relational data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Neville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
				<meeting>AAAI</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="13" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Génois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Strohmaier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.00150</idno>
		<title level="m">Visibility of minorities in social networks</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unifying guilt-by-association approaches: Theorems and fast algorithms</title>
		<author>
			<persName><forename type="first">D</forename><surname>Koutra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H P</forename><surname>Chau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-K</forename><forename type="middle">K</forename><surname>Pao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="245" to="260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep-GCNs: Can GCNs Go as Deep as CNNs?</title>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Deepergcn: All you need to train deeper gcns</title>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07739</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>London</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<title level="m">Collective Classification of Network Data. Data Classification: Algorithms and Applications 399</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Link-Based Classification</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twentieth International Conference on International Conference on Machine Learning (ICML)</title>
				<meeting>the Twentieth International Conference on International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="496" to="503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cautious inference in collective classification</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Mcdowell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Aha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="596" to="601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Birds of a feather: Homophily in social networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mcpherson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Smith-Lovin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual review of sociology</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="415" to="444" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Query-driven active surveying for collective classification</title>
		<author>
			<persName><forename type="first">G</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>London</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th International Workshop on Mining and Learning with Graphs</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Netprobe: a fast and scalable system for fraud detection in online auction networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pandit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Chau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th international conference on World Wide Web</title>
				<meeting>the 16th international conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="201" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Geom-GCN: Geometric Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">.-C</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2020. 2019</date>
			<biblScope unit="page" from="5241" to="5250" />
		</imprint>
	</monogr>
	<note>International Conference on Machine Learning</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The network data repository with interactive graph analytics and visualization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Ahmed</surname></persName>
		</author>
		<ptr target="http://networkrepository.com" />
	</analytic>
	<monogr>
		<title level="m">On Proximity and Structural Role-based Embeddings in Networks: Misconceptions, Techniques, and Applications</title>
				<editor>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Rossi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Jin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Ahmed</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Koutra</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename></persName>
		</editor>
		<imprint>
			<date type="published" when="2015">2020. 2015. 2020</date>
			<biblScope unit="page">36</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence. In Transactions on Knowledge Discovery from Data (TKDD</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Transforming Graph Statistical Relational Learning</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Mcdowell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Aha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Neville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="363" to="441" />
			<date type="published" when="2012">2012</date>
			<publisher>JAIR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Relational Similarity Machines (RSM): A Similarity-based Learning Framework for Graphs</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Eldardiry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.13021</idno>
	</analytic>
	<monogr>
		<title level="m">Multi-scale attributed node embedding</title>
				<editor>
			<persName><surname>Ieee Bigdata</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Rozemberczki</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Allen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Sarkar</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2018">2018. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="93" to="93" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Concerning nonnegative matrices and doubly stochastic matrices</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sinkhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Knopp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pacific Journal of Mathematics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="343" to="348" />
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Graph Agreement Models for Semi-Supervised Learning</title>
		<author>
			<persName><forename type="first">O</forename><surname>Stretcu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Viswanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Movshovitz-Attias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Platanios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tomkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><surname>; D'alché</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8713" to="8723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Attention-based graph neural network for semi-supervised learning</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Thekumparampil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.03735</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Graph Attention Networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Simplifying Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Representation Learning on Graphs with Jumping Knowledge Networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning, ICML</title>
				<meeting>the 35th International Conference on Machine Learning, ICML</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="5449" to="5458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Groupinn: Grouping-based interpretable neural network for classification of limited, noisy brain data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Solarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sripada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koutra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="772" to="782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="40" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Understanding belief propagation and its generalizations</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Yedidia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="236" to="239" />
		</imprint>
	</monogr>
	<note>Exploring artificial intelligence in the new millennium</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Beyond Homophily in Graph Neural Networks: Current Limitations and Effective Designs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Heimann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koutra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Modeling polypharmacy side effects with graph convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="457" to="466" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
