<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient Minimum Error Bounded Particle Resampling L1 Tracker With Occlusion Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2013-05-16">May 16, 2013</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Xue</forename><surname>Mei</surname></persName>
							<email>xue.mei@tema.toyota.com</email>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Haibin</forename><surname>Ling</surname></persName>
							<email>hbling@temple.edu</email>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Yi</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Erik</forename><forename type="middle">P</forename><surname>Blasch</surname></persName>
							<email>erik.blasch@rl.af.mil</email>
						</author>
						<author>
							<persName><roleName>Prof</roleName><forename type="first">P</forename><surname>Joseph</surname></persName>
						</author>
						<author>
							<persName><forename type="first">X</forename><surname>Havlicek</surname></persName>
						</author>
						<author>
							<persName><surname>Mei</surname></persName>
						</author>
						<author>
							<persName><forename type="first">L</forename><surname>Bai</surname></persName>
							<email>lbai@temple.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Toyota Research Institute</orgName>
								<orgName type="institution" key="instit2">North America</orgName>
								<address>
									<postCode>48105</postCode>
									<settlement>Ann Arbor</settlement>
									<region>MI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Computer and Information Science</orgName>
								<orgName type="department" key="dep2">Center for Data Analytics and Biomedical Informatics</orgName>
								<orgName type="institution">Temple University</orgName>
								<address>
									<postCode>19122</postCode>
									<settlement>Philadelphia</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">School of Information and Control Engineering</orgName>
								<orgName type="institution">Nanjing University of Information Science and Technology</orgName>
								<address>
									<postCode>210044</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Department of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">University of California at Merced</orgName>
								<address>
									<postCode>95340</postCode>
									<settlement>Merced</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Information Directorate</orgName>
								<orgName type="institution" key="instit1">U.S. Air Force Research Laboratory</orgName>
								<orgName type="institution" key="instit2">AFRL/RIEA</orgName>
								<address>
									<postCode>13441</postCode>
									<region>OR, NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Temple University</orgName>
								<address>
									<postCode>19122</postCode>
									<settlement>Philadelphia</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Efficient Minimum Error Bounded Particle Resampling L1 Tracker With Occlusion Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2013-05-16">May 16, 2013</date>
						</imprint>
					</monogr>
					<idno type="MD5">22844D365A32D375B4D1A1FEC11CB46D</idno>
					<idno type="DOI">10.1109/TIP.2013.2255301</idno>
					<note type="submission">received April 9, 2012; revised January 8, 2013; accepted March 13, 2013. Date of publication March 28, 2013; date of current version</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Compressive sensing</term>
					<term>1 minimization</term>
					<term>minimum error bound</term>
					<term>occlusion detection</term>
					<term>particle filter (PF)</term>
					<term>sparse representation</term>
					<term>visual tracking</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, sparse representation has been applied to visual tracking to find the target with the minimum reconstruction error from a target template subspace. Though effective, these L1 trackers require high computational costs due to numerous calculations for 1 minimization. In addition, the inherent occlusion insensitivity of the 1 minimization has not been fully characterized. In this paper, we propose an efficient L1 tracker, named bounded particle resampling (BPR)-L1 tracker, with a minimum error bound and occlusion detection. First, the minimum error bound is calculated from a linear least squares equation and serves as a guide for particle resampling in a particle filter (PF) framework. Most of the insignificant samples are removed before solving the computationally expensive 1 minimization in a two-step testing. The first step, named τ testing, compares the sample observation likelihood to an ordered set of thresholds to remove insignificant samples without loss of resampling precision. The second step, named max testing, identifies the largest sample probability relative to the target to further remove insignificant samples without altering the tracking result of the current frame. Though sacrificing minimal precision during resampling, max testing achieves significant speed up on top of τ testing. The BPR-L1 technique can also be beneficial to other trackers that have minimum error bounds in a PF framework, especially for trackers based on sparse representations. After the error-bound calculation, BPR-L1 performs occlusion detection by investigating the trivial coefficients in the 1 minimization. These coefficients, by design, contain rich information about image corruptions, including occlusion. Detected occlusions are then used to enhance the template Manuscript</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>V ISUAL tracking is an important topic for applications such as security and surveillance, vehicle navigation, human computer interaction, and so on. The challenges in designing a robust visual tracking algorithm in a dynamic environment are caused by the presence of noise, occlusions, varying viewpoints, background clutter and illumination changes. A thorough review can be found in <ref type="bibr" target="#b40">[41]</ref>.</p><p>Recently, sparse representation <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b10">[11]</ref> has been successfully applied to visual tracking <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b24">[25]</ref>- <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b43">[44]</ref>- <ref type="bibr" target="#b45">[46]</ref>. In these methods, the tracking problem is formulated as finding a sparse representation of the target candidate using templates. The advantage of using the sparse representation lies in the robustness to a wide range of image corruptions, especially occlusion. The results show good performance, however, at a computational expense of the 1 minimization. Furthermore, the target states are estimated in a particle filter framework and the computational cost grows proportionally as the number of particle samples increases. The large computational cost prevents the tracker from being used in a real time system such as real time surveillance and security operations. Furthermore, the rich information captured in approximation coefficients has not been utilized for occlusion analysis. For example, a gradual occlusion may cause model drifting in the template set.</p><p>Inspired by the aforementioned research, we propose an efficient tracking algorithm with a minimum error bound and occlusion detection. Our first contribution is to improve the run time efficiency of the L1 tracker by using an error bound derived from the least squares computation. Specifically, we observe that the computationally expensive reconstruction error in the sparsely constrained 1 minimization is lowerbounded by the least squares reconstruction error, which can be calculated efficiently. The observation motivates us to design a bounded particle resampling (BPR) algorithm, which greatly boosts the speed of the tracking algorithm. Specifically, the probability of a tracking sample is calculated in two stages of which most samples are filtered out before solving the 1 minimization. In the first stage, the sample is reconstructed by simply projecting it onto the target template subspace. The reconstruction is solved through a linear least squares equation that runs several orders faster than a typical 1 minimization. In the second stage, only dynamically selected samples that pass the two step testings from previous stage are processed through 1 minimization. By this two stage reconstruction, the computational cost is greatly reduced and the proposed tracker runs much faster than our previous L1 tracker <ref type="bibr" target="#b29">[30]</ref>. The BPR technique can also be beneficial to other trackers that have minimum error bounds in a PF framework, especially for trackers based on sparse representations. For example, the BPR technique can be applied to <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b37">[38]</ref> to further improve the speed of their trackers.</p><p>Our second contribution is an occlusion detection method that investigates the reconstruction coefficients to improve the template update procedure. The 1 minimization based reconstruction used in our previous method <ref type="bibr" target="#b29">[30]</ref> is known to capture occlusion information, which has been previously used for face recognition <ref type="bibr" target="#b35">[36]</ref>. While this property enables tracking occluded targets, it also induces risks by introducing the occluded target information into the template set and potentially causing tracking failures. To prevent the wrong information from contaminating the template set, we introduce an occlusion detection method. The idea is to first build an occlusion map from the trivial coefficients, which indicate pixel-wise image contamination in a given candidate. The occlusion map is then used for occlusion detection and a candidate with detected occlusion will not be used to update the template set.</p><p>For evaluation, the proposed BPR-L1 tracker is tested on eleven benchmark sequences involving various challenges such as occlusion and illumination changes. The sequences come from three different application scenarios: biometrics (head movement, hand holding object, singers on stage), pedestrians (urban travel, subway monitoring), and cars in traffic (wide area motion imagery, ground-mounted perspectives). In all experiments, our method shows excellent effective (accuracy) and efficient (timeliness) performance in comparison with previously proposed trackers.</p><p>The remainder of the paper is organized as follows. The related work is explained in Section II. Section III presents the particle filter algorithm preliminaries. In Section IV-A, we briefly review the L1 tracker proposed in previous work and its limitations. Section IV-B derives the minimum error bound. The BPR-L1 technique with two step testings is described in Section IV-C. An occlusion detection method is proposed in Section V. Experimental results on both methods are reported in Section VI. Finally, we conclude the paper in Section VII.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Visual tracking is an important topic in computer vision and it has been studied for decades. The visual tracking problem can be formulated in two different categories: generative and discriminative. Generative tracking methods use an appearance model to represent the target observations. Tracking is formulated as searching the target location that has the most similar appearance to the model. One example is using subspace models. The idea is that the images of the target lie in a low dimensional manifold. The appearance of the object is represented using an eigenspace <ref type="bibr" target="#b4">[5]</ref>, affine warps of learned linear subspaces <ref type="bibr" target="#b12">[13]</ref>, an incrementally learned low-dimensional subspace <ref type="bibr" target="#b32">[33]</ref>, the novel incremental tensor subspace <ref type="bibr" target="#b14">[15]</ref>, online appearance model <ref type="bibr" target="#b7">[8]</ref>, etc. Other popular tracking methods include mean shift tracker <ref type="bibr" target="#b9">[10]</ref>, covariance tracker <ref type="bibr" target="#b31">[32]</ref>, and frag tracker <ref type="bibr" target="#b0">[1]</ref>.</p><p>Discriminative tracking methods cast the tracking as a binary classification problem. Tracking is formulated as finding the target location that can best separate the target from the background. In <ref type="bibr" target="#b1">[2]</ref>, a feature vector is constructed for every pixel in the reference image and an adaptive ensemble of classifiers is trained to separate pixels that belong to the object from pixels that belong to the background. The TLD tracker <ref type="bibr" target="#b18">[19]</ref> uses P-N learning <ref type="bibr" target="#b17">[18]</ref> to exploit the structure of the data and get feedback about the performance of the classifier. It is shown to be reliable in long sequence tracking. In <ref type="bibr" target="#b8">[9]</ref>, a confidence map is built by finding the most discriminative RGB color combination in each frame.</p><p>One intuitive way of improving discriminative and generative methods is to combine them together in a hybrid way. A hybrid approach that combines a generative model and a discriminative classifier captures appearance changes and allows object reacquisition after a total occlusion <ref type="bibr" target="#b42">[43]</ref>. A three-level hierarchy combines the discriminative and generative models for tracking under the framework of sequential Bayesian learning in <ref type="bibr" target="#b23">[24]</ref>.</p><p>Many trackers have been proposed to compensate for occlusion induced tracking failures. Online multiple instance learning is used in <ref type="bibr" target="#b2">[3]</ref> to achieve robustness to occlusions as well as other image corruptions. Global mode seeking detects the object after total occlusion and reinitializes the local tracker <ref type="bibr" target="#b41">[42]</ref>. In <ref type="bibr" target="#b21">[22]</ref>, a novel algorithm is proposed to detect occlusion for visual tracking through learning with observation likelihoods and is combined with the L1 tracker <ref type="bibr" target="#b29">[30]</ref>. Another example <ref type="bibr" target="#b5">[6]</ref> uses image fusion to determine the best appearance model for discrimination and then a generative approach for dynamic target updates.</p><p>Particle filter (PF) has been introduced for visual tracking <ref type="bibr" target="#b15">[16]</ref> and has been a popular framework due to excellent performance for nonlinear target motion and flexibility to different object representations. While the use of more particle samples can improve track robustness, the computational load required by the particle filter also increases. Some authors have proposed to speed up the particle filter framework. In <ref type="bibr" target="#b39">[40]</ref>, the observation likelihood based on multiple features is computed in a coarse-to-fine manner so that the computation can quickly focus on the more promising regions. In <ref type="bibr" target="#b19">[20]</ref>, an efficient method is introduced for using subspace representations in a particle filter by applying Rao-Blackwellization to integrate out the subspace coefficients in the state vector. Fewer samples are needed since part of the state vector posterior is analytically calculated. In <ref type="bibr" target="#b46">[47]</ref>, it adjusts the number of particle samples based on the noise variance.</p><p>Sparse representation has recently been introduced for tracking in <ref type="bibr" target="#b29">[30]</ref> and later exploited in <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b24">[25]</ref>- <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b37">[38]</ref>. In <ref type="bibr" target="#b29">[30]</ref>, a tracking candidate is sparsely represented as a linear combination of target templates and trivial templates that only have one nonzero element. The sparse representation problem is solved through an 1 minimization problem with non-negativity constraints to solve the inverse intensity pattern problem during tracking. In <ref type="bibr" target="#b25">[26]</ref> the group sparsity is integrated and very high dimensional image features are used for improving tracking robustness. In <ref type="bibr" target="#b37">[38]</ref>, a novel blurdriven tracker framework for tracking motion-blurred targets is proposed. It introduces a blur template subspace and tracks the target without performing deblurring. In <ref type="bibr" target="#b24">[25]</ref>, a real-time tracker is proposed by adopting dimensionality reduction and a customized orthogonal matching pursuit (OMP) algorithm to accelerate the tracking. In <ref type="bibr" target="#b3">[4]</ref>, a very fast numerical solver is developed to solve the resulting 1 norm related minimization problem with guaranteed quadratic convergence based on the accelerated proximal gradient approach. In <ref type="bibr" target="#b26">[27]</ref>, a tracking algorithm with a static sparse dictionary and dynamic online updated basis distribution is developed. Our work is inspired by these studies, but we use an 2 error bound to improve efficiency and introduce an occlusion map for a reliable template update. In <ref type="bibr" target="#b38">[39]</ref>, several fast 1 minimization methods have been reviewed and compared. The focus of this paper is to improve the speed by exploiting the structure of the particle filter and combining with the 2 bound. This technique is different from the efforts of finding fast 1 minimization algorithms such as <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b24">[25]</ref> and in fact the two can be combined. Therefore, we did not evaluate through all different 1 minimization algorithms.</p><p>Our work shares philosophies with works where the error bound is used to guide visual tracking. For example, in <ref type="bibr" target="#b27">[28]</ref> a boosting error bound in a co-training framework is used to guide the novel tracker construction. However, the application of using an error bound in a sparse tracker has not been well explored. Furthermore, our goal is to use the error bound for speed up without sacrificing accuracy (τ testing) or without altering the tracking result (max testing). The τ testing is first described in a preliminary conference version of this paper in <ref type="bibr" target="#b30">[31]</ref>. In comparison, this paper introduces the new max testing for further improvement and involves more thorough evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PARTICLE FILTER</head><p>The L1 tracker proposed in <ref type="bibr" target="#b29">[30]</ref> is formulated as finding a sparse representation in the template subspace. The representation is then used with the particle filter framework <ref type="bibr" target="#b15">[16]</ref> for visual tracking. Specifically, for frame at time t, we denote x t as the state variable describing the location and shape of a target. The tracking problem can be formulated as an estimation of the state probability p(x t |z 1:t ), where z 1:t = {z 1 , z 2 , . . . , z t } represents the observations from previous t frames. The tracking proceeds using a two-stage Bayesian Algorithm 1 Particle Filter for L1 Tracker sequential estimation, which recursively updates the filtering distribution as</p><formula xml:id="formula_0">p(x t |z 1:t -1 ) = p(x t |x t -1 ) p(x t -1 |z 1:t -1 ) dx t -1 (1) p(x t |z 1:t ) ∝ p(z t |x t ) p(x t |z 1:t -1 )<label>(2)</label></formula><p>where p(x t |x t -1 ) indicates the state transition probability, and p(z t |x t ) gives the observation likelihood of state x t . Direct calculation of the above distribution is practically intractable. Alternatively, the posterior p(x t |z 1:t ) is approximated by a set of N particle samples {x i t } N i=1 with importance weights π i t . The samples are updated and resampled at each frame.</p><p>In the L1 tracker, the state variable x t contains six parameters of the affine transformation. The state transition of x t are modeled independently by a Gaussian distribution around the previous state x t -1 , and N candidate samples are generated based on the state transition model p(x t |x t -1 ). The observation model p(z t |x t ) reflects the similarity between a target candidate and the target templates, which is formulated using approximation error in the sparse representation.</p><p>For tracking at time t, the candidate with the maximum observation likelihood is chosen as the tracking result. The likelihood determines sample weights and importance resampling thresholds in the particle filter. A summary of the particle filter for L1 tracker is given in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EFFICIENT L1 TRACKER WITH BOUNDED PARTICLE RESAMPLING</head><p>In this section we will first review the original L1 tracker <ref type="bibr" target="#b29">[30]</ref> that combines the sparse representation and the particle filter framework. Then we will present the least squares based minimum error bound, which can be more efficiently computed than the error calculated used in the L1 tracker through 1 minimization. After that, we propose the BPR-L1, using the BPR procedure to increase efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. L1 Tracker With Sparse Representation</head><p>To model the observation likelihood p(z t |x t ), a region corresponding to state x t is first cropped from frame z t . <ref type="foot" target="#foot_0">1</ref> The region is then normalized and reshaped to a 1D vector y, which is used as a target candidate.</p><p>The sparse representation of y is formulated as a minimum error reconstruction through a regularized 1 minimization with nonnegativity constraints</p><formula xml:id="formula_1">min c Bc -y 2 2 +λ c 1 , s.t. c 0<label>( 3 )</label></formula><p>where B = T, I, -I is an over-complete dictionary composed of the target template set T, the positive and negative trivial template sets I and -I. Each column in T is a target template generated by reshaping pixels of a candidate region into a column vector; and each column in the trivial template sets is a unit vector that has only one nonzero element. c = a , e + , e - is composed of target coefficients a and positive and negative trivial coefficients e + , e -respectively.</p><p>Finally, the observation likelihood is derived from the reconstruction error of y as</p><formula xml:id="formula_2">p(z t |x t ) = 1 exp{-α Ta -y 2 } (<label>4</label></formula><formula xml:id="formula_3">)</formula><p>where a is obtained by solving the 1 minimization (3), α is a constant controlling the shape of the Gaussian kernel, and is a normalization factor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Minimum Error Bound</head><p>In this section, we will show that the reconstruction error from the target templates with an 2 norm is bounded by a minimum error that can be calculated much faster than solving an 1 minimization.</p><p>1) Least Squares Error Bound: The observation likelihood defined in (4) builds on the reconstruction error Tay 2 measured in the 2 norm. Since a is calculated by the 1 minimization (3), there is a natural lower bound for the reconstruction error Tay 2 ≥ Tây 2  (5)</p><p>where</p><formula xml:id="formula_4">â = arg min b Tb -y 2 (6)</formula><p>is the linear least squares approximation of y in the subspace spanned by T. One can also view â as a degenerate case of a when λ = 0 in (3). Similarly, for the observation likelihood p(z t |x t ), we derive its upper bound q(z t |x t ) using the least squares approximation error</p><formula xml:id="formula_5">q(z t |x t ) = 1 exp{-α Tâ -y 2 } (7)</formula><p>where α and are the same as in (4). We immediately have</p><formula xml:id="formula_6">p(z t |x t ) ≤ q(z t |x t ). (<label>8</label></formula><formula xml:id="formula_7">)</formula><p>2) Efficiency Analysis: The linear system in (6) can be solved by Cholesky factorization or QR factorization. For dense matrices, the cost of the Cholesky factorization method is dn 2 +(1/3)n 3 , while the cost of the QR factorization method is 2dn 2 , where d is the image dimension and n is the number of templates. The QR factorization method is slower by a factor of at most two if d n, which is the case for our problem. For small and medium-size problems, the factor of two does not outweigh the difference in accuracy, and the QR factorization is the recommended method.</p><p>The original L1 tracker uses the preconditioned conjugate gradients (PCG) method <ref type="bibr" target="#b20">[21]</ref> to solve the 1 minimization. The PCG algorithm computes the search direction and the run time is determined by the product of the total number of PCG steps required over all iterations and the cost of a PCG step. The total number of PCG iterations required by the truncated Newton interior-point method depends on the value of the regularization parameter λ. In the experiments, we found that the total number of PCG steps is a few hundred. The computationally most expensive operation for a PCG step is a matrix-vector product which has O(d(2d</p><formula xml:id="formula_8">+ n)) = O(d 2 + dn) computing complexity.</formula><p>From the complexity analysis, we can see the solution to the least squares problem in ( <ref type="formula">6</ref>) is two orders faster than the 1 solution. For example, if we are using template size of 15×12, then d = 15 × 12 = 180. The number of templates is n = 10. The cost of Cholesky factorization method is <ref type="formula">32400</ref>), and there will be a few hundred such steps.</p><formula xml:id="formula_9">dn 2 +(1/3)n 3 = 180 × 100 + (1/3) × 1000 ≈ 18000. While the cost of a PCG step is O(d 2 + dn) = O(</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Bounded Particle Resampling</head><p>In the previous section, we showed that the computation is much more intensive to compute the observation probability p(z t |x t ) than to compute its upper bound q(z t |x t ). It is therefore attractive to use the upper bound for samples that are not promising enough and only conduct likelihood computations for the promising samples. Nevertheless, to search for the candidate with the maximum likelihood, we still need the observation probabilities for resampling. Fortunately, in many cases only a small portion of the samples will be preserved after resampling and an efficient algorithm can be designed to avoid computing all observation probabilities. We propose a two step testing to remove most insignificant samples from computing the 1 minimization. The first step is τ testing. The likelihood of the samples is compared with τ which is the summation of all previous calculated observation probabilities and the ones smaller than τ are removed without loss of precision during resampling. The second step is max testing. The tracking result for the current frame is defined as the sample that has the maximum observation probability p. The likelihood bound of the samples is compared with the current calculated maximum observation probability. A tracking result is found if the likelihood bound is smaller than the maximum observation probability. The samples below the maximum observation probability will not affect the current-frame tracking result, but only affect the resampling. Since the remaining samples do not affect the current frame, we approximate the remaining samples observation probability by interpolation. We divide the remaining samples into several groups. For each group, we calculate the observation probability for the first and last sample from the Although sacrificing minimal precision during resampling, we achieve significant speed up from max testing in addition to the efficiency obtained from the thresholding in the first step.</p><p>1) τ Testing: We denote X t = {x 1 t , x 2 t , . . . , x N t } as the sample set at time t, and denote p i = p(z i t |x i t ) as the observation probability and its corresponding upper likelihood bound, q i = q(z i t |x i t ), defined in previous subsection. At the end of each frame, the samples are resampled with respect to their observation probabilities. We have the following observation.</p><p>Observation 1: If the sample appears at least once in the resampled set, its observation probability must satisfy the following condition:</p><formula xml:id="formula_10">p i ≥ 1 2N N j =1 p j . (<label>9</label></formula><formula xml:id="formula_11">)</formula><p>The observation is straightforward because the number of samples remains unchanged before and after resampling. In order for the sample to appear in the resampled set at least once, the normalized probability p i / N j =1 p j has to be greater than 1/2N. The "2" in the denominator is due to rounding.</p><p>Motivated by the observation, we develop a two-stage bounded resampling algorithm to calculate the probability of the tracking candidates. The first stage is straightforward: we compute the probability bounds q i for all samples and sort them in descending order. Without loss of generality, in the following we assume the samples are already sorted, i.e.,</p><formula xml:id="formula_12">q 1 ≥ q 2 ≥ • • • ≥ q N .</formula><p>In the τ testing, our task is to calculate the observation probability p i for samples that will survive resampling. The observation probability can be done efficiently even for large number of samples by using a dynamically updated threshold τ to exclude to-be-discarded samples. In particular, τ is defined according to the following theorem.</p><p>Theorem 1: If the i th sample x i appears at least once after resampling, its likelihood bound q i is no less than a threshold τ i defined as</p><formula xml:id="formula_13">τ i = 1 2N -1 i-1 j =1 p j . (<label>10</label></formula><formula xml:id="formula_14">)</formula><p>Proof: From Observation 1, we have</p><formula xml:id="formula_15">2N p i ≥ N j =1 p j ≥ i j =1 p j . (<label>11</label></formula><formula xml:id="formula_16">)</formula><p>Subtract p i on both sides and divide by 2N -1, we have</p><formula xml:id="formula_17">p i ≥ 1 2N -1 i-1 j =1 p j = τ i . (<label>12</label></formula><formula xml:id="formula_18">)</formula><p>Using the fact that q i is an upper bound of p i , we have</p><formula xml:id="formula_19">q i ≥ p i ≥ τ i .</formula><p>From the definition, we see that the thresholds are nondecreasing, i.e., 0 = τ 1 ≤ τ 2 ≤ • • • ≤ τ N , and there is</p><formula xml:id="formula_20">τ i+1 = τ i + p i 2N -1<label>(13)</label></formula><p>which can be used for an efficient threshold update.</p><p>With the above thresholds, in the τ testing, we start with the first sample that has the largest likelihood bound q 1 , and calculate the probability p 1 according to (4) and update the corresponding threshold τ 2 according to <ref type="bibr" target="#b12">(13)</ref>. Then we continue for samples 2, 3, . . .. For the i th sample, if the likelihood bound q i ≥ τ i , we compute the observation probability p i and update threshold τ i+1 . Otherwise if q i &lt; τ i , which according to Theorem 1, implies that x i , x i+1 , . . . , x N will be discarded during resampling. Then, we directly set p i = p i+1 = • • • = p N = 0. The probabilities p 1 , p 2 , . . . , p N are then used for resampling set X .</p><p>The above τ testing procedure does not sacrifice resampling precision, which is guaranteed by Theorem 1. τ testing avoids the expensive computation on samples with low likelihoods. The amount of time saved is mainly determined by the dissimilarity between the tracking foreground and its surrounding background. Intuitively, the larger the foreground/background difference, the more speedup from the BPR procedure. Furthermore, the τ testing framework encourages using more particles with larger sampling variance in comparison with the previously proposed L1 tracker <ref type="bibr" target="#b29">[30]</ref>; this in turn helps improve tracking accuracy in addition to computational efficiency. Empirical studies. Fig. <ref type="figure" target="#fig_0">1</ref> shows the curves of p, q, and τ when 600 particles are used. The values are calculated from one testing frame. After about 100 particle samples, when q becomes smaller than τ that is defined in Theorem 1 and fails the τ testing, the probability p of the rest samples are assigned to zero without calculating the computational expensive 1 minimization. Hence log( p) drops suddenly since log( p) becomes negative infinity when p is zero. For this frame, only 20% of the samples need to solve the 1 minimization, and we achieve about five times speed up for this frame.</p><p>Fig. <ref type="figure">2</ref> shows the run time and the number of particle samples for which the 1 minimization is performed. We can see that the run time is proportional to the number of the samples and is dominated by the second stage probability calculation. For most of the frames in the sequence, only 20% of the particle samples are used in the 1 minimization update. Notice in Fig. <ref type="figure">2</ref>, from frame 190 to 230, the man comes out of the shop and the woman is partially occluded. We clearly see that the number of particles calculating for the 1 minimization increases dramatically when the target Fig. <ref type="figure">2</ref>.</p><p>Run time and number of particle samples for sequence One-LeaveShopReenter2cor using τ testing. is occluded. When the target is occluded, none of the samples can model the target appearance well enough so the probabilities are distributed between the particles. When the probabilities are evenly distributed, more particles are needed, which results in a longer run time for the frame. The more probabilities concentrate on the first few samples, the less particles are needed for the 1 minimization calculation.</p><p>2) Max Testing: If at certain point, the likelihood bound q i is smaller than the maximum of the observation probability p max , where p max = max j =1,...,i-1 { p j }, then the tracking result is found. The observation probability p j , where j = i, i + 1, . . . , n, only affects the resampling and does not impact the current-frame tracking result. Inspired by this, we propose a max testing operation to further reduce the insignificant samples without altering the tracking result. This max testing has the flexibility to be combined with the τ testing to further speed up the L1 tracker. The number of 1 minimization calculations is reduced to a fixed number depending on the number of groups that the particles are divided into. First, the rest particles from i to N are divided into k groups. Second, for each group, we calculate the observation probability of the samples whose likelihood bounds are the maximum and minimum in the group and greater than τ . For the samples whose likelihood lies in between, the observation probability is the linear interpolation of the two. The maximum number of 1 minimization is a fixed number of 2k. The proposed two stage bounded resampling is summarized in Algorithm 2.</p><p>a) Empirical studies: Fig. <ref type="figure" target="#fig_1">3</ref> shows the curves of p, q, τ , and p max calculated from one frame when 600 particles are used. After 55 particle samples, q is becoming smaller than p max , and the probability of the remaining samples is the linear interpolation between the first and last observation probability of each group. After 179 particle samples, q is becoming smaller than τ , the probability p of the rest samples are assigned to zero without calculating the computational expensive 1 minimization. For this frame, only about 10% of the samples need to solve the 1 minimization, and we achieve about 10 times speed up for this frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 Two-Stage Bounded Resampling</head><p>Fig. <ref type="figure">4</ref> shows the run time and the number of particle samples for which the 1 minimization is performed using both τ and max testing. We can see that the run time is proportional to  Run time and number of particle samples for sequence One-LeaveShopReenter2cor using both τ and max testing.</p><p>the number of the samples and is dominated by the second stage probability calculation as in Fig. <ref type="figure">2</ref>. It has similar curve as in Fig. <ref type="figure">2</ref> as well. For most of the frames in the sequence, only 7% of the particle samples calculate the 1 minimization. The average particle samples per frame for this sequence is about 30, and 300 particles are used for the experiment. We achieve about 10 times speed up for this sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. OCCLUSION DETECTION</head><p>The template set needs to be updated to capture the appearance variations of the target during tracking. In <ref type="bibr" target="#b29">[30]</ref>, the tracking result is added to the template set if none of the template is similar to the tracking result. Therefore, the tracker is vulnerable to failures when the tracking result with a large occlusion is added to the template set. To prevent an improper addition to the template set, we propose a method to detect the large occlusion in the tracking result before it is added to the template set. Fig. <ref type="figure" target="#fig_3">5</ref> illustrates the occlusion detection algorithm flow.</p><p>For occlusion detection, we investigate the responses in the trivial templates when solving the 1 minimization (3). The trivial templates are activated when the pixel intensity can not be well approximated using the target templates. Therefore, we explore the trivial template coefficients for the occlusion detection. We convert the 1D trivial coefficient vector to a 2D trivial image by reversing the way that the target template is vectorized. Each pixel in trivial image is  mapped to the pixel in the same location in the template image. We threshold the trivial image and obtain another 2D binary image that we call an occlusion map. The white pixel in the occlusion map indicates that the pixel is occluded and the black pixel indicates no occlusion. We assume that an occluder is large in size and its intensity is different enough to be separated from small random noise. Therefore, the occlusion is a large connected region in the occlusion map. The occlusion detection is then reduced to finding a white area that is large enough to be classified as an occlusion. After applying morphological operations to the occlusion map to remove the small areas and fill the small holes between regions, we count the number of pixels in the largest region. If the area is larger than a pre-defined threshold, say 30% of the area in the occlusion map, we conclude that there is an occlusion in the tracking result, and the template set should not be updated.</p><p>Normally when an occlusion is detected, it will persist for a certain period of time. For example, when the target is occluded by an object, and the object is moving away from the target, the occlusion becomes smaller before it disappears. In our occlusion detection method, we avoid updating the template set for the next five frames after an occlusion is detected. Future work would optimize the frame selection based on the scenario.</p><p>For illustration, Fig. <ref type="figure" target="#fig_4">6</ref> shows tracking results of the faceocc2 sequence with frame 710 (left column) and 742 (right column). The top row shows the tracking result without occlusion handling, while the bottom row with occlusion handling. In frame 710, the target, face, undergoes heavy occlusion by a book. Without occlusion detection, the templates are contaminated and the tracker drifts apart in frame 742. The proposed tracker, by contrast, is able to follow the target after the book is moved away from the face. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXPERIMENTS</head><p>We implemented the proposed approach in MATLAB with the SPArse Modeling (SPAM) Software package<ref type="foot" target="#foot_2">2</ref>  <ref type="bibr" target="#b28">[29]</ref> and evaluated the performance on eleven publicly available video sequences. <ref type="foot" target="#foot_3">3</ref> We use these sequences to test various challenges including background changes (illumination), target changes (pose and scale), occlusion variations (none, partial, and significant), and cluttered environmental conditions (bleaching, multiple targets, and bright spots/rays).</p><p>The α (4) controlling the shape of the Gaussian kernel and the regularization parameter λ (3) are initialized to 40 and 0.01 for all the experiments. The number of groups k in max testing is set to three. Our proposed tracker is compared with nine state-of-the-art trackers include incremental visual tracking (IVT) <ref type="bibr" target="#b32">[33]</ref>, multiple instance learning (MIL) <ref type="bibr" target="#b2">[3]</ref>, visual tracking decomposition (VTD) <ref type="bibr" target="#b22">[23]</ref>, generalized kernel tracking (GKT) <ref type="bibr" target="#b33">[34]</ref>, L1 tracker (L1) <ref type="bibr" target="#b29">[30]</ref>, covariance tensor learning (CTL) <ref type="bibr" target="#b36">[37]</ref>, online AdaBoost (OAB) <ref type="bibr" target="#b11">[12]</ref>, trackinglearning-detection (TLD) <ref type="bibr" target="#b18">[19]</ref>, and local sparse appearance model (SPT) <ref type="bibr" target="#b26">[27]</ref>. The comparative tracker results were obtained by running the source code or binaries provided by their authors using the same initial positions in the first frame.</p><p>Our method is implemented in MATLAB and can process five frames per second on an Intel Core2Duo 2.66 GHz standard PC with 8 GB memory. We expect our method to process 10 to 15 frames per second with optimized implementation. We use the same number of particles for trackers involving particle filtering for fair comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Qualitative Comparison</head><p>The first sequence, car4, shows a vehicle undergoes drastic illumination change as it passes beneath a bridge and under trees. Tracking results on several frames are presented in Fig. <ref type="figure">7</ref>. The BPR-L1 tracker, L1 tracker, IVT, TLD and CTL and SPT are a little off the target in frames 521, and 546.</p><p>In the fourth sequence, PETS01D1Human1, a person is walking from right bottom corner to the left of the image (Fig. <ref type="figure" target="#fig_0">10</ref>). The target is very small compared to the image and therefore does not have much discrimination against the background. Most trackers, including BPR-L1, track the target successfully. IVT and SPT lock to the background and are not able to recover from early failures.</p><p>The fifth sequence, OneLeaveShopReenter2cor, includes people walking in and out of a crowded hallway. In this video, the background color is similar to the color of the woman's trousers, and the man's shirt and pants have a similar color to the woman's coat. In addition, the woman undergoes partial occlusion. Some result frames are given in Fig. <ref type="figure" target="#fig_0">11</ref>. The BPR-L1, L1 and TLD trackers are able to track the target during the entire sequence. Many other trackers lock on the man when he occludes the woman after he comes out of the shop.</p><p>Results of the sixth sequence, girl, are shown in Fig. <ref type="figure" target="#fig_0">12</ref>. In this sequence, we show the robustness of our algorithm in handling occlusion and large pose changes. All the trackers track the target in the entire sequence except for the MIL, which loses the target in the frame 466.</p><p>Results on the seventh sequence, Occluded Face, are shown in Fig. <ref type="figure" target="#fig_1">13</ref>. Many trackers start drifting from the target when the man's face is almost fully occluded by the book. The BPR-L1 tracker explicitly handles occlusions, and update target templates accordingly. Therefore, it handles well appearance change in this sequence and continues tracking the target when the occlusion disappears.</p><p>Fig. <ref type="figure" target="#fig_0">14</ref> presents the most challenging scenario in that the sequence, shaking, has moving targets, variations in illumination with bright spots and rays, as well as occlusions; however retains similarity to the face tracking scenarios as above. Consistent track maintenance is achieved by the MIL and BPR-L1 trackers only.</p><p>In the ninth sequence, singer shown in Fig. <ref type="figure" target="#fig_3">15</ref>, the singer wears white clothes which are similar to the background and matches the clutter from the light. CTL loses track because it focuses on the color discrimination that is not evident in the image. The L1 tracker loses the singer target due to the frequent template update. The GKT, MIL, OAB, VTD, IVT, SPT and BPR-L1 all track the target; however, the BPR-L1 captures the target size more accurately.</p><p>Fig. <ref type="figure" target="#fig_4">16</ref> presents the tenth sequence, pktest02. The multi-car traffic scene has many objects in the image that are similar to the tracking target, thus present numerous opportunities for target switching. It confuses GKT and OAB confused throughout the sequence and does the same to MIL, TLD and IVT after a while. Another reason for failure may be attributed to the corruption of the target appearance from the occlusion and shadow of trees. The CTL, SPT, L1 and BPR-L1 successfully tracks the target through the sequence. The last experiment is on a long sequence (total 2600 frames), doll, taken by a hand-held camcorder <ref type="bibr" target="#b29">[30]</ref> and shown in the Fig. <ref type="figure" target="#fig_0">17</ref>. The doll undergoes scale, out-of-plane rotation, and occlusion. The VTD, GKT, and MIL lose the target after around 1000, 1400, and 1600 frames. The SPT does not have a consistent tracking results. The OAB gradually drift away from the target and lost the target at the end of the sequence. The IVT lose the target from frame 2400. Only CTL, L1, TLD, and BPR-L1 can track the target throughout the whole sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Quantitative Comparison and Discussion</head><p>To quantitatively compare robustness under challenging conditions, we manually labeled the ground truth of the eleven sequences. The tracking accuracy is measured by the relative position errors (in pixels) between the center of the tracking result and that of the ground truth. Ideally, the position differences should be around zero. The results are summarized in Fig. <ref type="figure" target="#fig_7">18</ref>.</p><p>Two issues are important in the quantitative results. First, when a tracker loses the target, it is difficult to reacquire the target and the error grows rapidly. Second, the proposed BPR-L1 tracker in general produces low tracking error, though there are cases where BPR-L1 does not perform the best (e.g. occluded face, frames 620-750).</p><p>In general, we attributes the excellent performance of BPR-L1 to: 1) the speed up allows more particles to be used so as to better approximate the posterior probability in the sequential Bayesian inference and 2) the occlusion detection and consequent handling reduces the chances of drifting.</p><p>One limitation of our method lies in the holistic appearance model for tracking targets. The model takes pixel position into account and may have problems when dealing with non-rigid objects in which relative pixel positions have changed. In addition, in some cases the model may suffer from misalignment, especially when images/templates of large sizes are used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>We demonstrated an efficient bounded-particle re-sampling L1 minimization (BPR-L1) tracker with minimum error bound and occlusion detection. The BPR-L1 tracker employs a twostage sample probability scheme, where most samples with small probabilities from first stage are filtered out without solving the computationally expensive 1 minimization. The occlusion detection coupled with a template update scheme effectively prevents the occluded target from contaminating the target template set. Preventing an incorrect update to the target template set reduces tracking failures. Our proposed BPR-L1 method is computationally more efficient than the previous L1 tracker, and demonstrates the effectiveness in handling a number of challenging sequences such as variations in illumination, pose, occlusions, and dense targets. We compared the BPR-L1 tracker with nine other state-of-the-art trackers including our original L1 tracker on eleven sequences to validate robustness. Overall, BPR-L1 demonstrated very promising tracking accuracy and ran significantly faster than the original L1 tracker.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. Curves of p, q, and τ . The logarithm is applied to the data for display purpose.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig.3. Curves of p, q, τ , and p max . The logarithm is applied to the data for display purpose.</figDesc><graphic coords="7,317.51,201.77,111.74,84.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Fig. 4.Run time and number of particle samples for sequence One-LeaveShopReenter2cor using both τ and max testing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Flow chart of the occlusion detection algorithm.</figDesc><graphic coords="7,317.51,117.17,111.74,83.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. (a) Example tracking results of the faceocc2 sequence without occlusion detection and (b) with occlusion detection.</figDesc><graphic coords="7,433.31,201.77,111.74,84.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .Fig. 8 .</head><label>78</label><figDesc>Fig. 7. Tracking results of the car4 sequence.</figDesc><graphic coords="8,211.31,143.93,129.38,86.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9 .Fig. 10 .Fig. 11 .Fig. 12 .Fig. 13 .Fig. 14 .Fig. 15 .Fig. 16 .Fig. 17 .</head><label>91011121314151617</label><figDesc>Fig. 9. Tracking results of the sylvester sequence. #1417 #1496 #1562</figDesc><graphic coords="9,77.99,275.33,129.74,97.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 18 .</head><label>18</label><figDesc>Fig. 18. Quantitative comparison of the trackers in terms of position errors (in pixel).</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The frame at time t is treated as the observation z t .</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>minimization, and the probability of the middle samples is approximated by interpolation. The insignificant samples are further removed without altering the tracking result.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>Available at http://www.di.ens.fr/willow/SPAMS/downloads.html.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3"><p>Sequences 1-3 were from http://www.cs.toronto.edu/ ∼ dross/ivt/. Sequences</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_4"><p>4-7 were from the PETS 2001 dataset http://www.cvg.cs.rdg. ac.uk/PETS2001/, http://groups.inf.ed.ac.uk/vision/CAVIAR/CAVIARDATA1/, http://vision.stanford.edu/∼birch/headtracker/seq/, http://vision.ucsd.edu/ ∼ bbabenko/project_miltrack.shtml, and http://www.dabi.temple.edu/ ∼ hbling/ code_data.htm.</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>. This work was supported in part by the National Science Foundation under Grant IIS-0916624 and Grant IIS-1049032. The work of Y. Wu was supported in part by the National Natural Science Foundation of China under Grant 61005027, Grant 61273259, and Grant 61272223. The associate editor coordinating the review of this manuscript and approving it for publication was</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Robust fragments-based tracking using the integral histogram</title>
		<author>
			<persName><forename type="first">A</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Shimshoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2006-06">Jun. 2006</date>
			<biblScope unit="page" from="798" to="805" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ensemble tracking</title>
		<author>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2005-06">Jun. 2005</date>
			<biblScope unit="page" from="494" to="501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Visual tracking with online multiple instance learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2009-06">Jun. 2009</date>
			<biblScope unit="page" from="983" to="990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Real time robust L1 tracker using accelerated proximal gradient approach</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2012-06">Jun. 2012</date>
			<biblScope unit="page" from="1830" to="1837" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Eigentracking: Robust matching and tracking of articulated objects using a view-based representation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Jepson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="63" to="84" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multiresolution EO/IR target tracking and identification</title>
		<author>
			<persName><forename type="first">E</forename><surname>Blasch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kahler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 8th Int. Conf. Inf. Fusion</title>
		<meeting>8th Int. Conf. Inf. Fusion</meeting>
		<imprint>
			<date type="published" when="2005-07">Jul. 2005</date>
			<biblScope unit="page" from="275" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Stable signal recovery from incomplete and inaccurate measurements</title>
		<author>
			<persName><forename type="first">E</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Romberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. Pure Appl. Math</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1207" to="1223" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Robust object tracking via online dynamic spatial bias appearance models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2157" to="2169" />
			<date type="published" when="2007-12">Dec. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On-line selection of discriminative tracking features</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis</title>
		<meeting>Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="346" to="352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Kernel-based object tracking</title>
		<author>
			<persName><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="564" to="577" />
			<date type="published" when="2003-05">May 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Compressed sensing</title>
		<author>
			<persName><forename type="first">D</forename><surname>Donoho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1289" to="1306" />
			<date type="published" when="2006-04">Apr. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Real-time tracking via online boosting</title>
		<author>
			<persName><forename type="first">H</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Brit. Mach. Vis. Conf</title>
		<meeting>Brit. Mach. Vis. Conf</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="47" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Visual tracking using learned linear subspaces</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2004-07">Jun.-Jul. 2004</date>
			<biblScope unit="page" from="782" to="789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dual-force metric learning for robust distracter resistant tracker</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="513" to="527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Incremental tensor subspace learning and its applications to foreground segmentation and tracking</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Maybank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="303" to="327" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Condensation-Conditional density propagation for visual tracking</title>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="28" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Visual tracking via adaptive structural local sparse appearance model</title>
		<author>
			<persName><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2012-06">Jun. 2012</date>
			<biblScope unit="page" from="1822" to="1829" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">P-N learning: Bootstrapping binary classifiers by structural constraints</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Kalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2010-06">Jun. 2010</date>
			<biblScope unit="page" from="49" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Tracking-learning-detection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Kalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1409" to="1422" />
			<date type="published" when="2012-07">Jul. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A Rao-Blackwellized particle filter for EigenTracking</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Balch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Dellaert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2004-07">Jun.-Jul. 2004</date>
			<biblScope unit="page" from="980" to="986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A method for large-scale 1 -regularized least squares</title>
		<author>
			<persName><forename type="first">S.-J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lustig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gorinevsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Signal Process</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="606" to="617" />
			<date type="published" when="2007-04">Apr. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning occlusion with likelihoods for visual tracking</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2011-11">Nov. 2011</date>
			<biblScope unit="page" from="1551" to="1558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Visual tracking decomposition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2010-06">Jun. 2010</date>
			<biblScope unit="page" from="1269" to="1276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Visual tracker using sequential Bayesian learning: Discriminative, generative, and hybrid</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern., B, Cybern</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1578" to="1591" />
			<date type="published" when="2008-12">Dec. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Real-time visual tracking using compressive sensing</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2011-06">Jun. 2011</date>
			<biblScope unit="page" from="1305" to="1312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Robust and fast collaborative tracking with two stage sparse optimization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kulikowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="624" to="637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Robust tracking using local sparse appearance model and K-selection</title>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kulikowsk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2011-06">Jun. 2011</date>
			<biblScope unit="page" from="1313" to="1320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A robust boosting tracker with minimum error bound in a co-training framework</title>
		<author>
			<persName><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis</title>
		<meeting>Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1459" to="1466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Online learning for matrix factorization and sparse coding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="19" to="60" />
			<date type="published" when="2010-01">Jan. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Robust visual tracking and vehicle classification via sparse representation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2259" to="2272" />
			<date type="published" when="2011-11">Nov. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Minimum error bounded efficient 1 tracker with occlusion detection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Blasch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2011-06">Jun. 2011</date>
			<biblScope unit="page" from="1257" to="1264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Covariance tracking using model update based on lie algebra</title>
		<author>
			<persName><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2006-06">Jun. 2006</date>
			<biblScope unit="page" from="728" to="735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Incremental learning for robust visual tracking</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="125" to="141" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Generalized kernel-based visual tracking</title>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="119" to="130" />
			<date type="published" when="2010-01">Jan. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Online object tracking with sparse prototypes</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="314" to="325" />
			<date type="published" when="2013-01">Jan. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Robust face recognition via sparse representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="210" to="227" />
			<date type="published" when="2009-01">Jan. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Real-time probabilistic covariance tracking with efficient model update</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Blasch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2824" to="2837" />
			<date type="published" when="2012-05">May 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Blurred target tracking by blur-driven tracker</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis</title>
		<meeting>Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1100" to="1107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Fast L1-minimization algorithms for robust face recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>to be published</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Fast multiple object tracking via a hierarchical particle filter</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Duraiswami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis</title>
		<meeting>Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="212" to="219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Object tracking: A survey</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Survey</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Object tracking and detection after occlusion via numerical hybrid local and global mode-seeking</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2008-06">Jun. 2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Online tracking and reacquistion using co-trained generative and discriminative trackers</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="678" to="691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Robust visual tracking via multi-task sparse learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2012-06">Jun. 2012</date>
			<biblScope unit="page" from="2042" to="2049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Real-time compressive tracking</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="864" to="877" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Block covariance based 1 tracker with a subtle template dictionary</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maybank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1750" to="1761" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Visual tracking and recognition using appearance-adaptive models in particle filters</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Moghaddam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1491" to="1506" />
			<date type="published" when="2004-11">Nov. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
