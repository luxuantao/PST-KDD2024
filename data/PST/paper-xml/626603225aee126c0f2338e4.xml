<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Crescent: Taming Memory Irregularities for Accelerating Deep Point Cloud Analytics</title>
				<funder ref="#_BBkMTAH #_GvUrhMt">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-04-22">22 Apr 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yu</forename><surname>Feng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Rochester Rochester</orgName>
								<address>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gunnar</forename><surname>Hammonds</surname></persName>
							<email>ghammon5@u.rochester.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Rochester Rochester</orgName>
								<address>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yiming</forename><surname>Gan</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Rochester Rochester</orgName>
								<address>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuhao</forename><surname>Zhu</surname></persName>
							<email>yzhu@rochester.edu</email>
							<affiliation key="aff3">
								<orgName type="institution">University of Rochester Rochester</orgName>
								<address>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Crescent: Taming Memory Irregularities for Accelerating Deep Point Cloud Analytics</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-04-22">22 Apr 2022</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3470496.3527395</idno>
					<idno type="arXiv">arXiv:2204.10707v1[cs.AR]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>point cloud</term>
					<term>accelerators</term>
					<term>DNN</term>
					<term>irregular memory accesses</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>3D perception in point clouds is transforming the perception ability of future intelligent machines. Point cloud algorithms, however, are plagued by irregular memory accesses, leading to massive inefficiencies in the memory sub-system, which bottlenecks the overall efficiency.</p><p>This paper proposes Crescent, an algorithm-hardware co-design system that tames the irregularities in deep point cloud analytics while achieving high accuracy. To that end, we introduce two approximation techniques, approximate neighbor search and selectively bank conflict elision, that "regularize" the DRAM and SRAM memory accesses. Doing so, however, necessarily introduces accuracy loss, which we mitigate by a new network training procedure that integrates approximation into the network training process. In essence, our training procedure trains models that are conditioned upon a specific approximate setting and, thus, retain a high accuracy. Experiments show that Crescent doubles the performance and halves the energy consumption compared to an optimized baseline accelerator with &lt; 1% accuracy loss. The code of our paper is available at: https://github.com/horizon-research/crescent.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Recent years have seen an explosive rise of intelligent machines that can perceive, process, and understand visual data. 3D visual data, a.k.a., point clouds, have become increasingly important. Prime examples include localization and mapping in autonomous vehicles <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b61">62]</ref> and robotics <ref type="bibr" target="#b51">[52]</ref>, object detection in Augmented and Virtual reality <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b71">72]</ref>, air pollutants detection <ref type="bibr" target="#b16">[17]</ref>, and geo-spatial mapping in cultural heritage preservation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b57">58]</ref>.</p><p>Despite much algorithmic development, point cloud networks are inefficient to execute on today's hardware architectures (e.g., GPUs, deep learning/stencil accelerators), most of which are designed and optimized for regular 2D perception domains such as video and image processing <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b45">46]</ref>. Point cloud algorithms, however, exhibit highly irregular computation and memory behaviors and, thus, are ill-suited for architectures built for regular kernels.</p><p>The irregularity stems from the fact that memory accesses, which dominate the overall execution efficiency, are inputdependent. As a result, point cloud algorithms exhibit excessive and random (as opposed to streaming) DRAM accesses as well as frequent SRAM bank conflicts that stall the datapath.</p><p>Many mature optimizations such as tiling, double-buffering, static data layout that are commonly applied to regular kernels such as conventional Deep Neural Networks (DNNs) are either ineffective or not applicable at all. This paper proposes Crescent, which co-designs the algorithm and hardware to tame the irregularities in point cloud algorithms. We start by understanding the sources of memory inefficiency in point cloud algorithms (Sec. 2), which points to two main sources. First, point cloud algorithms spend a significant amount of time (up to 80%) <ref type="bibr" target="#b17">[18]</ref> in explicit neighbor searches, which exhibit statically-unknown memory access patterns. Second, the irregular neighbor search necessitates that any subsequent operations must explicitly aggregate points through irregular gather operations instead of simply indexing the memory as in conventional DNNs.</p><p>Our key idea is to impose structures on memory accesses. We propose an approximate neighbor search algorithm (Sec. 3) that turns irregular DRAM accesses into streaming accesses. While there are search algorithms that preserve streaming accesses, they often do so at a cost of increasing the search work and/or redundant DRAM accesses by resorting to exhaustive search <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b65">66]</ref>. We use a different strategy: we use an irregular tree-based algorithm to reduce the search work and selectively elide on-chip bank conflicts to tame the irregularities in tree traversals (Sec. 4). This strategy reduces the search work and DRAM traffic by over 40%.</p><p>Our techniques are inexact by nature. Without care, applying them during inference leads to drastic network accuracy loss. To retain accuracy, we propose approximation-aware training (Sec. 5). Specifically, we integrate the approximation operations into training by modeling hardware behaviors (e.g., bank conflicts) at training time. We show that training with a generic hardware model is usually sufficient, which allows us to avoid tightly coupling training with a particular hardware configuration.</p><p>Our training procedure yields models that provide accuracyvs-performance trade-offs at inference time without re-training. This is achieved without increasing the network size or inference overhead. The key is to train a network by sampling not only the input distribution (as with conventional DNN training) but also the distribution of a set of approximation knobs that dictate the accuracy-vs-performance trade-off. In this way, the model's inference is conditioned upon a specific approximate setting h, naturally presenting a different accuracy-vs-performance trade-off for a given h.</p><p>We implement the Crescent hardware in a 16nm process node and evaluate it on a set of popular point cloud models. We show that the optimizations introduced in Crescent require virtually zero hardware cost and, meanwhile, provide on average 1.9 ? speedup (up to 3.1 ?) and 1.5 ? energy reduction (up to 4.2 ?) compared to an optimized baseline point cloud accelerator without our optimizations. Notably, the performance and energy gains are achieved with less than 1.0% accuracy loss.</p><p>In summary, this paper makes the following contributions.</p><p>? We introduce an approximate neighbor search algorithm and its co-designed hardware, which guarantees completely streaming DRAM accesses while reducing the DRAM traffic in point cloud DNNs. ? We introduce selectively bank conflict elision, a lightweight scheme to avoid datapath stalls from bank conflicts and reduce SRAM traffic in point cloud networks. ? We propose a network training procedure that integrates the approximate neighbor search and selective bank conflict elision into training to mitigate the accuracy loss while providing a flexible accuracy-vsperformance trade-off at inference time. ? We show that our optimizations collectively achieve 1.9 ? speedup and 1.5 ? energy reduction for a set of popular point cloud networks compared to a baseline accelerator while sacrificing less than 1% accuracy.</p><p>The rest of this paper is so organized. We first characterize the memory inefficiencies, both in DRAM and on-chip SRAM, of today's point cloud networks (Sec. 2). We then introduce two techniques to tame the memory inefficiencies: approximate neighbor search that guarantees fully streaming DRAM accesses (Sec. 3) and selectively bank conflict elision, which streamlines on-chip memory accesses (Sec. 4). We then introduce a neural network training procedure that integrates both approximate techniques into the training process to mitigate the accuracy loss (Sec. 5). After describing the experimental setup (Sec. 6), we demonstrate the efficiency of Crescent (Sec. 7). We then discuss Crescent in the broad literature (Sec. 8) before concluding the paper (Sec. 9).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">MOTIVATION</head><p>We first briefly describe the scope of deep point cloud algorithms that this paper targets, and describe the two main algorithmic stages, neighbor search and feature computation, in these algorithms (Sec. 2.1). We then quantify the memory inefficiencies in both the neighbor search stage (Sec. 2.2) and the feature computation stage (Sec. 2.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Deep Point Cloud Analytics</head><p>A point cloud is a collection of points, each of which is represented by the [x, y, z] coordinates in the 3D space. Point cloud data are becoming ever more relevant mainly because of two trends: 1) the prevalence of convenient point cloud acquisition devices, e.g., stereo cameras <ref type="bibr" target="#b38">[39]</ref> and LiDAR <ref type="bibr" target="#b54">[55]</ref>, and 2) the emergence of deep learning algorithms that can effectively extract semantics information from point clouds. Today, deep point cloud models are routinely deployed in real-world systems such as Waymo's self-driving cars <ref type="bibr" target="#b7">[8]</ref> and Google's Augmented Reality toolkit <ref type="bibr" target="#b0">[1]</ref>. We focus on algorithms that directly operate on raw points, which is by far the most common form of deep point cloud analytics. We refer interested readers to Guo et al. <ref type="bibr" target="#b25">[26]</ref> for a comprehensive survey on deep learning for point clouds.</p><p>Key Operations Generally, a point cloud DNN can be abstracted as two stages, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. Each input point undergoes a neighbor search process. The neighbor search results are stored in a matrix, where each row stores the neighbor indices of a point in the input. The feature computation stage aggregates the neighbors of a point, on which a transformation, usually a Multilayer Perceptron (MLP), is applied, to generate a new output point.</p><p>Both stages are important to optimize. A recent study on five popular point cloud networks shows that the execution time ratio of the two stages varies between 1:4 to 4:1 <ref type="bibr" target="#b17">[18]</ref>, suggesting that neither stage universally dominates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Memory Inefficiencies in Neighbor Search</head><p>Neighbor search in low-dimensional space (e.g., 3D) commonly uses the K-d tree <ref type="bibr" target="#b13">[14]</ref>, which recursively subdivides the search space into two half-spaces using axis-aligned planes. The sub-spaces are organized as a tree, and neighbor search becomes a tree traversal problem. Compared to exhaustive search, the space subdivision strategy is more efficient as it prunes the search space: if the distance of a query Q and the boundary of a subspace S is greater than the search radius, all the points in S can be skipped. While K-d tree search is inherently parallel (as different search queries are independent), tree traversals are hardware unfriendly. In particular, the memory access patterns are known only at run time, leading to massive inefficiencies in both DRAM and SRAM accesses, which we quantify below. The non-streaming nature coupled with large point cloud data size leads to redundant DRAM accesses. For instance, in the popular KITTI dataset <ref type="bibr" target="#b19">[20]</ref>, the total points and queries in a typical scene alone can be over tens of MBs (not considering the network weights, activations, etc.), larger than what a mobile SoC can accommodate. Thus, points are loaded onchip in chunks (analogous to tiling in conventional DNNs). Since not all data in each chunk will be used when they are loaded due to the non-streaming access pattern, a great amount of DRAM accesses are wasted.</p><p>Fig. <ref type="figure">3</ref> quantifies the excessive DRAM accesses and cache miss rate in neighbor search. The left ?-axis shows the ratio of the amount of DRAM requests (in bytes) to the actual data theoretically needed by the algorithm (i.e., reading each query and search point once). The data are obtained by simulating an unrealistic 10 MB fully-associated cache running a neighbor search on a typical KITTI-constructed scene with about 1.2 million points. Even with this unrealistic SRAM structure, searches in many models have about 10? more DRAM traffic than what is strictly required. Realistic mobile accelerators would allocate an even smaller buffer for neighbor search to accommodate other data structures such as DNN weights and activations. The right ?-axis quantifies the corresponding cache miss rates, which are over 85%.  SRAM The on-chip memory accesses in K-d tree search are also inefficient because of the frequent bank conflicts. In regular kernels such as stencil pipelines <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b60">61]</ref> where the memory access pattern is statically known, one could carefully interleave data in the SRAM banks to avoid bank conflicts <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b70">71]</ref>. In contrast, on-chip memory accesses in neighbor search are input-dependent and, thus, bank conflicts are inevitable.</p><p>Fig. <ref type="figure">4</ref> quantifies the bank conflicts by showing the percentage of SRAM accesses that are bank-conflicted and how the percentage varies with the number of banks. We assume an unrealistically large 10 MB buffer and 8 concurrent SRAM requests. With 4 banks the bank conflict rate is 26.9%. The bank conflict rate is reduced to 2.1% only when the number of banks quadruples the number of simultaneous requests.</p><p>Using a heavily-banked SRAM design is highly undesirable. A large number of banks requires a more costly crossbar design <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b23">24]</ref>, as the crossbar area grows quadratically with the number of banks. Using an Arm memory compiler <ref type="bibr" target="#b2">[3]</ref>, we find that the crossbar area is twice as much as the memory arrays under a 32-bank configuration. In addition, a higher bank count also reduces the memory array size, which increases the per-bank overhead (peripheral circuits, BIST, redundancy) <ref type="bibr" target="#b59">[60]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Memory Inefficiencies in Feature Computation</head><p>Unlike neighbor search, the DRAM accesses in the feature computation stage are completely streaming. The on-chip memory accesses, however, are met with frequent bank conflicts.</p><p>Feature computation is broken down into two steps: 1) aggregate the neighbors for each input point p ? using the neighbor indices generated in the neighbor search stage, and 2) compute an output point p ? from each p ? by applying a function, usually a MLP, to the neighbors of p ? . Step 2 is accelerated on today's DNN accelerators.</p><p>Step 1 is analogous to fetching data from the input feature map in a conventional DNN. However, conventional DNNs</p><formula xml:id="formula_0">Legend Q0 Q1 Q2 Q3 Q2 Q0 ? Q7 Q4 ? Q8 Q6 Q5 ? Q9 Q1 ? Q9 ?? Queries Sub-tree leaf nodes S0 S1 S2 S3</formula><p>S0 Top-tree leaf nodes; a.k.a. sub-tree root nodes ? Query queue; private to each sub-tree Top-tree nodes Fig. <ref type="figure">6</ref>: The two-level tree data structure of our neighbor search algorithm. In the first stage, queries traverse the top-tree and are assigned to a particular subtree in the end. In the second stage, queries search neighbors in their assigned sub-tree, and backtracking is limited to within the sub-tree.</p><p>access consecutive feature map elements with staticallyknown patterns. Therefore, a compiler lays out data in the SRAM such that a simple single-bank, single-port memory array (using wide words) could serve memory requests from tens or hundreds of PEs in one cycle without stalling the PEs <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b70">71]</ref>. However, point cloud networks access non-consecutive memory in this step, because the neighbors of a point can be arbitrary. Therefore, the SRAM serving points are usually banked. Worse, the access pattern is statically-unknown, as it depends on the neighbor search results, which, in turn, depend on the inputs. Therefore, bank conflicts are inevitable. Fig. <ref type="figure" target="#fig_2">5</ref> quantifies the severity of bank conflicts in point aggregation by showing the percentage of SRAM accesses that are bank-conflicted in aggregating the points. We assume a 16-bank SRAM design with a total size of 64 KB. Across the four models, the bank conflict rate is at least 38.43% and can be as high as 57.27%. Increasing the number of banks is undesirable as it requires a more costly crossbar and/or a higher per-bank overhead due to the smaller memory arrays <ref type="bibr" target="#b59">[60]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">FULLY-STREAMING NEIGHBOR SEARCH ALGORITHM</head><p>We introduce our neighbor search algorithm and explain how it fundamentally improves the DRAM access efficiency by allowing completely streaming memory accesses (Sec. 3.1).</p><p>We then describe the co-designed neighbor search hardware (Sec. 3.2). Finally, we discuss the key knob in our algorithm that dictates the accuracy-vs-performance trade-off (Sec. 3.3). Result Buffer Bypass Fig. <ref type="figure">7</ref>: Neighbor search hardware engine, which enables a fully-streaming access to DRAM. The same hardware is used for both top-tree search and for the sub-tree searches, simplifying the hardware design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Algorithm</head><p>Our algorithm splits the K-d tree into a top tree and a set of sub-trees. Each top-tree leaf node is also the root node of a sub-tree. The search is then naturally divided into two stages: a top-tree search stage and a sub-tree search stage. The two stages themselves are massively parallel but are serialized with each other. Fig. <ref type="figure">6</ref> illustrates the idea.</p><p>In the first stage, all the queries search the top-tree (a binary search tree) until they reach the leaf nodes of the top-tree, at which point the queries are assigned to the corresponding sub-trees. Conceptually, each sub-tree has a queue that stores all the incoming queries. At the end of the first stage, queries in the sub-tree queues are written back to the memory in preparation for the second stage. In actual hardware, a queue has a fixed size. Thus, the store back to the memory is phased, as we will discuss later.</p><p>Once all the queries finish the first stage, the algorithm enters the second stage, where queries in each sub-tree are searched against the corresponding tree. For each sub-tree, the search process is exactly the same as that in the top-tree with a critical difference: queries are allowed to backtrack when they reach a leaf node of the sub-tree. This is necessary for a query to find all its neighbors. However, we limit the backtracking to the sub-tree. The intuition is that nodes in other sub-trees are naturally far away from the query and thus are less likely to be neighbors. Architecturally, this ensures that each sub-tree and each query is loaded to SRAM once -at a cost of accuracy loss. We will discuss the accuracy implication of this design decision in Sec. 3.3 and how to mitigate the accuracy loss through approximation-aware network training in Sec. 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Hardware Design</head><p>The hardware designed to exploit the algorithm is shown in Fig. <ref type="figure">7</ref>. The search is carried out by a set of PEs, each of which can execute a query independently. The PEs access data from the on-chip SRAM that stores various data structures. The SRAM interfaces with the DRAM through a DMA, as all DRAM accesses are streaming.</p><p>SRAM The SRAM is split into two global buffers and two local buffers. The global tree buffer and query buffer are accessed by all the PEs. Each PE is also equipped with a local result buffer and a local stack buffer private to each query.</p><p>The global tree buffer is accessed by the PEs simultaneously. To sustain a high read bandwidth, the tree buffer is heavily banked. Unlike in regular kernels, bank conflicts here could not be avoided by optimizing the data layout in the banks, because the access pattern of the PEs is known only at run time. We will show in Sec. 4 how to mitigate the performance impact of bank conflicts.</p><p>PE Design The PE design follows the algorithm of how a query traverses the K-d tree to search for its neighbors. As shown in the left blown-up panel in Fig. <ref type="figure">7</ref>, a PE is pipelined into five stages, starting from reading the top of the traversal stack (RS) to fetch the next tree node to visit (FN), followed by calculating the distance between the query and the tree node (CD), storing results (SR) when a neighbor is found, and ended with updating the stack (US). The pipeline stalls only when the FN stage meets a bank conflict when reading the global tree buffer.</p><p>Hardware Reuse Due to the uniform traversal-based search in both top-and sub-tree searches, the hardware is reused in both phases. For instance, the PEs are designed with the generic traversal logic that is agnostic to what the search tree is and what the queries are. The US stage is skipped/bypassed in the top-tree search where no backtracking takes place (i.e., no update to the query stack).</p><p>The SRAM is also reused between the two phases. Specifically, the PE-local result buffer is re-purposed between storing the sub-tree queues in the top-tree search phase and storing the neighbor results in the sub-tree search phase. The global tree buffer is re-purposed between storing the top-tree and storing the sub-tree. During top-tree search, whenever a result buffer is full all the queries assigned to that queue (thus far) are streamed back to the DRAM.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Accuracy and Performance Trade-off</head><p>A key parameter that governs our algorithm is the top-tree height (TTH). TTH must be set to ensure both the top-tree and the sub-trees can be held in the on-chip SRAM. At the same time, TTH also dictates the performance-vs-accuracy trade-off. We explore the implication of TTH in this section. First, the top-tree height is dictated by the tree buffer size. We require that the entirety of the top-tree or a sub-tree, while is being searched, is completely stored in the tree buffer. This ensures that the PE pipeline does not stall because the required data are off-chip. Thus, the top-tree height ? ? must be within the range [H + 1 -log 2 (S + 1), log 2 (S + 1)] to satisfy the following two inequalities, where H is the total K-d tree height and S is the total tree buffer size:</p><formula xml:id="formula_1">2 ? ? -1 ? S (1) 2 H-? ? +1 -1 ? S<label>(2)</label></formula><p>Given that a TTH is within the permissible range, a shorter top-tree increases the neighbor search accuracy at a cost of more computation, and vice versa. This can be explained by a first-order analytical model, where the total number of nodes a query accesses is proportional to the sum of: (i) the number of tree nodes that are visited during the forward traversal, i.e., from the root node of the toptree to a leaf node of a sub-tree, (ii) the number of nodes that are visited during the subtree backtracking.</p><p>The cost of (i) is constant, as it depends only on the total tree height. The cost of (ii) inversely depends on the TTH: a taller top-tree translates to visiting fewer nodes in the subtree backtracking, reducing the cost of (ii) and, by extension, the total cost. Fig. <ref type="figure" target="#fig_4">8</ref> quantifies how the total number of nodes accessed per query (?-axis) varies with the TTH (?-axis) using the average statistics of PointNet++(c) on the KITTI dataset. As the TTH increases to 10, only 2% of the tree nodes are accessed by a query. Visiting fewer nodes improves the search speed but also degrades the accuracy.</p><p>An assumption we make, as with Tigris <ref type="bibr" target="#b65">[66]</ref> and QuickNN <ref type="bibr" target="#b43">[44]</ref>, is that a sub-tree can be stored completely on-chip. This is a reasonable assumption: a typical 10 MB point cloud using a 5-level top-tree would result in a sub-tree size of 640 KB, smaller than a typical on-chip buffer size found in mobile SoCs. In case of excessively large point clouds, Crescent can in theory recursively split a sub-tree; we do not observe this need in common datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Efficiency Discussion</head><p>The split-tree algorithm enables completely streaming DRAM accesses. The panel on the right of Fig. <ref type="figure">7</ref> shows how the different data structures are laid out in the DRAM and how they are accessed in a streaming fashion. Converting random DRAM accesses to streaming accesses reduces the DRAM energy <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b18">19]</ref>, and enables double-buffering, which improves performance because: 1) off-chip data accesses are overlapped with computation, and 2) data needed by the datapath are readily available on-chip without stall.</p><p>Compared to prior neighbor search algorithms that also enable streaming accesses such as Tigris <ref type="bibr" target="#b65">[66]</ref> and QuickNN <ref type="bibr" target="#b43">[44]</ref>, we reduce both the search load and DRAM traffic. We qualitatively discuss it here, and quantify the gains in Sec. 7.5.</p><p>First, Tigris and QuickNN use exhaustive search within the sub-trees, whereas we retain K-d tree search in the subtrees, thereby reducing the total search load. Retaining K-d tree search in the sub-trees is not an obvious design decision, because it introduces irregular on-chip memory accesses in the form of bank conflicts, which prior work aims to avoid at a cost of more search work.</p><p>Our strategy is different: we reduce the search work by retaining K-d tree search and mitigate the resulting irregular on-chip memory accesses through inference-training co-design. Specifically, we will show a selective bank conflict elision scheme to significantly reduce bank conflicts (Sec. 4), which, when coupled with an approximation-aware training procedure (Sec. 5), retains the application accuracy.</p><p>Second, we reduce the amount of DRAM accesses compared to Tigris and QuickNN, both of which load (and reload) a sub-tree from DRAM whenever the corresponding query buffer is full. We instead first stage all the queries to a subtree in DRAM and then process them in a batch, thus loading each sub-tree exactly once.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">SELECTIVE BANK CONFLICT ELISION</head><p>This section addresses inefficiencies pertaining to on-chip memory accesses. We first describe our main idea of selectively eliding bank conflicts (Sec. 4.1). We then discuss how point cloud algorithms proceed when bank conflicts are elided (Sec. 4.2) and the hardware support (Sec. 4.3). Finally, we identify the key knobs that dictate the accuracy-vsperformance trade-off (Sec. 4.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Main Idea</head><p>A key requirement of the SRAM design is to feed data required by the PEs without stalling them. Such a requirement is easy to meet in conventional DNNs or other regular kernels, where data access patterns are statically known and thus SRAM data layout can be statically optimized accordingly <ref type="bibr" target="#b70">[71]</ref>. The on-chip memory access patterns in point cloud algorithms, however, are only dynamically known, introducing SRAM bank conflicts that are detrimental to overall performance.</p><p>Motivated by the error-tolerance nature of neural networks, our idea is to dynamically ignore bank conflicts when appropriate. That is, when multiple memory requests fall in the same bank, instead of serializing the accesses we allow only one request to access the SRAM; other requests return immediately without stalling. While conceptually simple, actually realizing this idea requires answering three questions:</p><p>(1) What data should conflicted accesses return, and how should the algorithm proceed without the correct data? (2) How to support bank conflict elision in hardware? (3) When is it appropriate to elide bank conflicts without accuracy drop? The answers to these questions depend on where a bank conflict takes place in the algorithm, because different memory accesses request data of different significance. Both neighbor search stage and feature computation introduce bank conflicts. In neighbor search, bank conflicts are caused by accessing the tree buffer; all other accesses are regular. In feature computation, aggregating neighbors of a point as inputs to the MLP causes bank conflicts; SRAM accesses incurred during MLP are regular. We now elaborate how the three questions above are addressed in both stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">How Algorithms Proceed with Bank Conflicts Elision</head><p>Feature Computation To aggregate neighbors, SRAM accesses are made to retrieve neighbors of a point. Thus, ignoring a conflicted access essentially ignores a point's neighbor, in which case we must fill in the missing neighbors, as the subsequent MLP anticipates an input matrix of a given size (decided at the training time).</p><p>To meet the size requirement, we propose to simply reuse the point returned from the request that is allowed to access the bank. The intuition is that concurrent accesses, say ? and ?, are guaranteed to be requesting neighbors of the same point ? <ref type="bibr" target="#b17">[18]</ref>. Reusing the returned data from ? for ? is equivalent to replicating one of ?'s neighbors. This replication strategy is commonly done in point cloud network design to meet the size requirement in case a neighbor search does not return enough neighbors <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b48">49]</ref>. Our design essentially performs this replication in hardware, implicitly. Neighbor Search The situation is slightly different for neighbor search, where bank conflicts happen when the PEs access the tree nodes during tree traversal. One could use the same replication strategy used in the feature computation stage: if accesses ? 1 from PE 1 and ? 2 from PE 2 conflict on the same bank, reuse the data returned from ? 1 for ? 2 . However, this could lead to side effects such as program crash, redundant computation, and infinite loop. For instance, when the node returned from ? 1 is in the part of the tree that PE 2 has already visited, pushing ? 1 onto PE 2's stack leads to an infinite loop or, at least, redundant traversals.</p><p>Our design simply ignores the conflicted accesses. Upon a conflict, the FN stage in a PE skips the remaining pipeline stages and reads the next item on the stack. This is denoted by the "bypass" signal in the PE shown in Fig. <ref type="figure">7</ref>. Algorithmically, this is equivalent to skipping all the nodes beneath the lost node during tree traversal. This strategy omits potential neighbors but guarantees that the traversal terminates.</p><p>An optimization that we leave for future work is to check whether the node returned from ? 1 , the request allowed to access the SRAM, is beneath the node (in the tree) that would have been returned from ? 2 if the bank conflict were to be observed; if so, using ? 1 to continue the search in ? 2 is guaranteed to terminate without side effects. Doing so would skip fewer nodes and potentially increase the accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Hardware Support</head><p>Eliding bank conflicts is virtually free to implement in hardware by using many existing structures in banked SRAM design. As an example, Fig. <ref type="figure" target="#fig_6">10</ref> shows a simple banked SRAM with 2 ports and 4 banks. The key to a banked SRAM is the arbitration and crossbar logic, which detects bank conflicts and routes data from a bank to the right port (a MUX here). For simplicity, we show only the relevant hardware and assume a low-order interleaving, i.e., the two least significant bits in the address select a bank.</p><p>Assume both accesses from the two ports fall into Bank 0, and Port 0 is allowed access. In the baseline SRAM, the MUX before Port 1 would select data returned from Bank 0, but this data will be ignored because the bank conflict detection logic would raise the Conflict signal, indicating to Port 1 that a bank conflict occurs and the memory request is to be issued again. But, critically, the data returned from Bank 0 is exactly what Port 1 needs in the feature computation stage under bank conflict elision. We simply lower the Conflict signal in this case, which is accomplished by ANDing the output of bank conflict detection and the negation of the Elide signal, which indicates whether bank conflict elision is enabled.</p><p>The Mode signal operates a MUX to select between the neighbor search vs. feature computation mode. In neighbor search, the original bank conflict signal is used, except the PE will not re-issue the memory request; instead, the PE simply continues the search with the next item on the stack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">When to Elide Bank Conflicts?</head><p>Eliding bank conflicts returns incorrect data to the PEs and, thus, hurts accuracy. We find that eliding bank conflicts in feature computation leads to little to no accuracy loss whereas eliding bank conflicts during neighbor search, without care, has significant accuracy implications (Sec. 7.3). This is because in feature computation the data that would have been returned (if bank conflicts were observed) are replaced with the data returned from the conflicting access; in neighbor search, however, eliding bank conflicts directly skips all the computations associated with that node altogether. We thus focus on the neighbor search stage here.</p><p>Intuitively, the accuracy loss is smaller when ignoring a memory access made to a lower level tree node, as fewer tree nodes would be skipped later in the traversal. Fig. <ref type="figure" target="#fig_5">9</ref> shows how the percentage of skipped tree nodes (?-axis) varies with the tree level below which bank conflicts are elided (?-axis). The data are averaged across all the queries of PointNet++(c) on the ModelNet dataset, where the total tree height is 14. When bank-conflicted accesses below level 2 are ignored, almost 100% of the tree nodes are skipped, which degrades the model accuracy to almost zero (not shown). When the elision level is 12, only 10% of the tree nodes are skipped.</p><p>Skipping more nodes degrades accuracy but increases the search speed. Therefore, a natural knob that controls the trade-off of accuracy-vs-performance is the elision height ? ? , which is defined as the tree level beneath which all conflicted </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">APPROXIMATION-AWARE NETWORK TRAINING</head><p>Our neighbor search algorithm and bank conflict elision, if applied directly on a trained point cloud DNN at inferencetime, will decrease the accuracy sharply (Sec. 7.1). This is because the original network is not trained with the various approximation techniques in mind. To mitigate the accuracy drop, we propose a modified network training procedure that mitigates the accuracy loss.</p><p>The goal here is to learn a DNN that retains a high accuracy under approximation compared to the baseline network. In particular, we consider two approximation knobs: the toptree height ? ? and the elision height ? ? .A larger ? ? decreases accuracy but increases the performance; conversely, a larger ? ? increases the accuracy at a cost of a lower performance.</p><p>A straightforward idea is to integrate h =&lt; ? ? , ? ? &gt; as part of the inference such that the DNN is trained for a particular h. In essence, this is similar to fine-tuning a compressed model to regain the accuracy, where a network learns to adjust its weights given the approximation introduced by a particular compression setting.</p><p>While one could train a dedicated model for each possible h and build an ensemble, that would increase the training overhead and deployment complexity. Instead, we propose to learn one model that adapts to different h. Mathematically, we aim to learn a DNN distribution ? (?, h; ? ) ? ? such that different DNNs sampled from the distribution ? share the same model parameter ? and provide similar accuracy given an input h (along with the input point cloud).</p><p>To that end, our training procedure augments the conventional training with one simple extension: conventional  training samples input data; our training also randomly samples an h for each input. During the forward propagation, h is used to modulate the neighbor search and bank conflict elision. In this way, the model parameter ? is trained to accommodate the approximations introduced during the forward inference. The training flow is shown in Fig. <ref type="figure" target="#fig_7">11</ref>.</p><p>In order to replay the same inference-time approximation during training, we integrate a hardware simulator for modeling the bank conflict. The bank conflict model is called by both neighbor search and feature computation (the aggregation operation), as Fig. <ref type="figure" target="#fig_7">11</ref> shows. The bank conflict simulator takes in two parameters: 1) ? ? , which indicates the tree level below which bank conflicts are elided, and 2) the hardware banking configuration (e.g., number of banks, bank size). We find that training with the exact banking configuration on the inference hardware yields higher accuracy, but absent an exact hardware configuration training with a generic banking configuration provides noticeable benefits, too (Sec. 7.3).</p><p>Finally, note that neighbor search and aggregation do not participate in gradient descent; they simply construct inputs to the MLP layers. Thus, the model is end-to-end differentiable even though neighbor search and aggregation are not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTAL SETUP</head><p>Architecture Design Fig. <ref type="figure" target="#fig_8">12</ref> shows the overall point cloud accelerator, which includes three main components: a neighbor search engine as described in Sec. 3.2, a neighbor aggregation unit, which uses the design in Mesorasi <ref type="bibr" target="#b17">[18]</ref>, and a DNN accelerator for executing the MLPs. Without losing generality, we assume a systolic-array-based DNN accelerator, which is configured to have a 16 ? 16 MAC array, where each MAC unit mimics the design of that in the TPU <ref type="bibr" target="#b30">[31]</ref>.</p><p>The on-chip SRAM is partitioned to serve different purposes. The global buffer serves the weight and activations for the systolic array. It is configured to be 1.5 MB in size. Experimental Methodology We synthesize, place, and route the datapath of the neighbor search engine, the systolic array, and the aggregation unit using an EDA flow consisting of Synopsys and Cadence tools with the TSMC 16 nm Fin-FET technology. The SRAMs are generated using the Arm Artisan memory compiler. Power is estimated using Synopsys PrimeTimePX by annotating the switching activity. We then build a cycle-accurate simulator of the architecture with the latency of each component parameterized from the post-synthesis results of the RTL design.</p><p>The DRAM is modeled after Micron 16 Gb LPDDR3-1600 (4 channels) according to its datasheet <ref type="bibr" target="#b4">[5]</ref>. The DRAM energy is obtained using Micron System Power Calculators <ref type="bibr" target="#b5">[6]</ref>. On average, the energy ratio between a random DRAM access and a streaming DRAM access is about 3:1, and the energy ratio between a random DRAM access and an SRAM access is about 25:1, both matching prior work <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b66">67]</ref>.</p><p>Software Setup Tbl. 1 lists the four point cloud networks used in the evaluation, which covers common point cloud tasks including classification, segmentation, and detection. For classification, we evaluate the classic PointNet++(c) <ref type="bibr" target="#b48">[49]</ref> and DensePoint <ref type="bibr" target="#b36">[37]</ref> on the ModelNet40 dataset <ref type="bibr" target="#b64">[65]</ref>. We use the overall accuracy as accuracy metric. For segmentation, we evaluate PointNet++(s) <ref type="bibr" target="#b48">[49]</ref> on the ShapeNet dataset <ref type="bibr" target="#b14">[15]</ref>. The metric used in segmentation is the standard Intersectionover-Unit (mIoU) accuracy. For detection, we evaluate F-PointNet <ref type="bibr" target="#b46">[47]</ref> on the KITTI dataset <ref type="bibr" target="#b19">[20]</ref> and report the geometric mean of the IoU metric on the car class.</p><p>To ensure that the improvements from Crescent are not due to the inefficiencies of the network implementation, we use the versions of these models optimized by Feng et al. <ref type="bibr" target="#b17">[18]</ref>, which removes redundant MLP computations and on average achieves 1.6? speedup over the corresponding authorreleased implementations.</p><p>Baseline We compare against three baselines:</p><p>? GPU: the mobile Pascal GPU on Nvidia's Jetson TX2 development board <ref type="bibr" target="#b3">[4]</ref>. ? Tigris+GPU: this baseline executes the neighbor search on Tigris <ref type="bibr" target="#b65">[66]</ref>, a recent neighbor search accelerator that does not perform approximate eighbor search and selectively bank conflict elision, and executes the feature computation on the mobile Pascal GPU. ? Mesorasi, a prior point cloud network accelerator <ref type="bibr" target="#b17">[18]</ref> that uses Tigris <ref type="bibr" target="#b65">[66]</ref> for neighbor search and executes the feature computation on a dedicated systolic-array without selectively bank conflict elision. The exact same systolic array configuration is used in Crescent with the exception that Crescent performs selective bank conflict elision.</p><p>Area Overhead Our accelerator has a total area of 1.55 mm 2 , in which the Crescent-specific portion is almost negligible. The only hardware extension is one that selectively elides the bank conflict (Fig. <ref type="figure" target="#fig_6">10</ref>), which requires an additional MUX and an AND gate for each port of the SRAM.</p><p>Traininig Overhead Our approximation-aware training increases the training time by 38%. The main overhead is to simulate bank conflicts, which currently is a multi-threaded CPU implementation. Using a random h does not further increase the training overhead, since we still perform one search per inference. Note that the training overhead is amortized across all subsequent inferences.</p><p>Variants We evaluate two variants of Crescent to decouple the contribution of the two optimizations:</p><p>? ANS performs approximate neighbor search but does not elide bank conflicts. ? ANS+BCE performs approximate neighbor search while also eliding bank conflicts in neighbor search and aggregation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">EVALUATION</head><p>We first show that Crescent achieves similar accuracy as the baseline (Sec. 7.1) but delivers significant speedups and energy reductions (Sec. 7.2). We then provide a detailed analysis of our training procedure and understand how its effectiveness varies with respect to different algorithmic and hardware configurations (Sec. 7.3). We perform a sensitivity study to understand Crescent's performance and energy savings vary under different settings (Sec. 7.4). Finally, we provide an quantitative comparison with prior neighbor search accelerators (Sec. 7.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Accuracy</head><p>We find that directly applying Crescent optimizations without retraining significantly degrades the model accuracy.</p><p>Integrating approximation into the training process elevates the accuracy to the baseline level. Fig. <ref type="figure" target="#fig_9">13</ref>   Directly applying the two optimizations at inference time degrades the accuracy between 27.3% to 40.5%, making the models practically useless. Re-training regains the accuracy with an accuracy drop of at most 0.9% (PointNet++(c)). In PointNet++(s), re-training completely recovers the accuracy loss introduced in approximation. The fact that we can almost completely recover the accuracy loss with ANS+BCE, the most aggressive approximation setting, shows the effectiveness of our approximation-aware training. The accuracy of ANS alone is slightly higher than that of ANS+BCE, as the latter applies two approximations whereas the former applies only one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Performance and Energy</head><p>Using the re-trained ANS and ANS+BCE model shown in Fig. <ref type="figure" target="#fig_9">13</ref>, we compare Crescent's performance and energy consumption over the baseline accelerator, shown in Fig. <ref type="figure" target="#fig_11">14</ref>.</p><p>Speedup Fig. <ref type="figure" target="#fig_11">14a</ref> shows the speedup of ANS and ANS+BCE against the three baselines; all data are normalized to Mesorasi. Among the three baselines, Tigris+GPU and GPU are much slower than Mesorasi, because the latter accelerates feature computation on a systolic array.</p><p>Overall, ANS and ANS+BCE achieve a 1.7? and 1.9? speedup, respectively, over Mesorasi. Comparing the speed of ANS+BCE and ANS shows that approximation neighbor search contributes more to the speedup than bank conflict elision. The speedups on DensePoint are the highest (2.8? and 3.1?, respectively) because DensePoint's time is dominated by neighbor search (81%) whereas neighbor search takes "only" about 55% of the time in other models.</p><p>To understand the sources of speedup, Fig. <ref type="figure" target="#fig_12">15a</ref> and Fig. <ref type="figure" target="#fig_12">15b</ref> show the speedup of ANS+BCE on neighbor search and on the aggregation operation in feature computation, respectively. On average, ANS+BCE achieves a 4.9? speedup on neighbor search and a 2.1? speedup on aggregation.   Energy Savings Fig. <ref type="figure" target="#fig_11">14b</ref> shows the energy consumption of ANS and ANS+BCE normalized to Mesorasi. On average, ANS and ANS+BCE saves 33% and 36% of the total energy, respectively. The energy saving is mainly contributed by approximate neighbor search rather than bank conflict elision, because the former optimizes the DRAM traffic, which contributes more to the energy than the SRAM traffic, which the latter optimizes for. DensePoint, again, has the highest energy saving because it is dominated by neighbor search. As a comparison, Tigris+GPU and GPU consume 25? and 38? more energy, respectively, compared to Mesorasi.</p><p>Fig. <ref type="figure" target="#fig_12">15a</ref> and Fig. <ref type="figure" target="#fig_12">15b</ref> on the right ?-axes show the energy savings on neighbor search and aggregation. DensePoint's savings on these two operations in isolation are on par with other networks, confirming that its significant end-to-end savings are primarily attributed to the dominance of neighbor search in its execution time.</p><p>Tease Apart Contributions To understand the sources of energy savings, Fig. <ref type="figure" target="#fig_0">16</ref>    savings into four components: converting random DRAM accesses to streaming accesses, DRAM traffic reduction, SRAM traffic reduction in neighbor search, and SRAM traffic reduction from aggregation. The former two are due to the new neighbor search algorithm, and the latter two are due to the selective bank conflict elision optimization.</p><p>The main energy saving contributor is the SRAM traffic reduction in neighbor search, which frequently accesses the Tree Buffer. While the DRAM savings are relatively smaller, we expect the DRAM savings will become more significant in the future as the point clouds grow in size.</p><p>We quantify the impact of selective bank conflict elision (BCE) in Fig. <ref type="figure" target="#fig_0">17</ref>, where we show the reduction in bank conflicts (left ?-axis) and, as a result, the reduction in the number of tree nodes visited (right ?-axis). The results are obtained by comparing ANS+BCE with ANS. Overall, BCE avoids over 45% of bank conflicts and reduces 50% of tree node accesses in neighbor search. This result explains the 1.9? speedup over Mesorasi by ANS+BCE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Understanding the Training Procedure</head><p>We use PointNet++(c) as a representative model to drive the analyses in this section. The conclusions generally hold.  Dedicated Models We first evaluate the accuracy of models trained with dedicated approximation settings.</p><p>Fig. <ref type="figure" target="#fig_4">18</ref> shows the accuracy of PointNet++(c) trained under different top-tree heights (? ? ) and then inferenced under the same ? ? . The setting ? ? being 0 is the baseline model with exact search. As the ? ? increases, the accuracy decreases. This is because a larger ? ? reduces the search space and, thus, it is less likely to find the exact neighbors for each query. The accuracy is acceptable initially, dropping from 89.6% to 88.8% as ? ? increases from 0 to 4. Beyond 4, the accuracy drop becomes more significant. As the top-tree height reaches 12, the accuracy is only 84.4%. As we will show later, however, a higher ? ? leads to a higher speedup, providing a large trade-off space.</p><p>Fig. <ref type="figure" target="#fig_14">19</ref> performs a similar study while varying the elision height ? ? . Each marker in the figure represents a dedicated ANS+BCE model trained with different ? ? ranging from 4 to 14; ? ? in this example is fixed at 4. As ? ? increases, the accuracy increases. This is because a higher elision height skips fewer tree nodes during tree traversal, leading to a better search result. At a ? ? of 12, the accuracy loss is only 0.8%. The accuracy loss is over 5% when ? ? reduces to 4, essentially ignoring almost all nodes in the sub-tree.</p><p>Mixed Training We now evaluate how a model trained by sampling approximation settings adapts to different approximation levels at inference time. Fig. <ref type="figure" target="#fig_15">20</ref> compares three schemes: 1) a model trained with ? ? = 1, 2) a model trained with ? ? = 6, and 3) a model trained by random sampling ? ? between 1 and 6 for each input ("Mixed" in the figure). We show their accuracy under different inference-time ? ? .</p><p>When a dedicated model is trained with ? ? = 1, the accuracy significantly drops when the inference-time ? ? is greater than 1. This is not surprising: a model trained with little approximation in mind does not perform well when inference performs aggressive approximation. When a dedicated model is trained with ? ? = 6, however, it performs reasonably well across different ? ? at inference-time, even for ? ? settings that are not seen in the training time.</p><p>The mixed model consistently provides higher or similar accuracy compare the dedicated ? ? = 1 model. Compared to the dedicated ? ? = 6 model, the mixed model is significantly   BCE in Aggregation vs. Neighbor Search We perform bank conflict elision in both neighbor search and in feature aggregation. We find that the overall accuracy is insensitive to bank conflict elision in aggregation even without retraining. Across all networks, applying bank conflict elision in aggregation alone (while turning off other approximations) results in at most 0.3% accuracy loss. In contrast, accuracy typically drops by double digits if bank conflict elision is applied in neighbor search without re-training. As discussed in Sec. 4.4, this is because in the latter case eliding bank conflicts completely skips subsequent search operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Sensitivity Study</head><p>Hardware Configuration Fig. <ref type="figure" target="#fig_19">22a</ref> and Fig. <ref type="figure" target="#fig_19">22b</ref> show how Crescent's speedup and energy vary, respectively, as the numbers of PEs and the number of Tree Buffer banks vary. The energy is normalized to the corresponding baseline.</p><p>Naturally, the speedup is higher on less-capable baselines and diminishes on more capable baselines (e.g. 32 PEs and 32 banks), because performance optimizations are less important when the hardware is faster to begin with. Note,  however, that a 16-bank memory introduces a large crossbar overhead and is generally impractical for mobile-grade accelerators <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b70">71]</ref>. The significant energy saving is consistent across hardware configurations. Even with a 32 PE 32 bank configuration, Crescent still saves about 27% energy on PointNet++(c). This is because the energy is roughly proportional to the amount of work done. Changing the hardware configuration does not affect the bulk of the work needed to be done.</p><p>Approximation Degrees Fig. <ref type="figure" target="#fig_1">23a</ref> and Fig. <ref type="figure" target="#fig_1">23b</ref> show the accuracy-vs-speedup and accuracy-vs-energy trade-offs, respectively, with different ? ? and ? ? combinations, which dictate different approximation strengths. The data are reported from PointNet++(c), but the trend generally holds. Overall, varying ? ? from 0 to 12 and ? ? from 4 to 14 provide a trade-off space of about 5% accuracy range, 2.0 ? performance range, and 1.5 ? energy range.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">Comparison with Prior Neighbor Search Accelerators</head><p>QuickNN <ref type="bibr" target="#b43">[44]</ref> and Tigris <ref type="bibr" target="#b65">[66]</ref> are two recent neighbor search accelerators that both use a split-tree data structure. As discussed in Sec. 3.4, Crescent reduces both the search load and DRAM traffic. Fig. <ref type="figure" target="#fig_20">24a</ref> shows that the K-d tree-based search reduces the total number of tree nodes visited by 41% compared to exhaustive search. This explains the one order of magnitude performance improvement over the Tigris-based accelerator shown in Sec. 7.2. QuickNN, similar to Crescent, also presents streaming DRAM accesses -at the expense of redundant DRAM accesses, since each sub-tree is potentially loaded onto the accelerator multiple times. Comparing to a QuickNN implementation with the same PE configuration, Fig. <ref type="figure" target="#fig_20">24b</ref> shows that Crescent reduces the total DRAM accesses by 48%.</p><p>Finally, we target DNN-based algorithms and, thus, can mitigate the potential accuracy loss through end-to-end network training, which is not available to QuickNN and Tigris; both target a non-DNN algorithm (point cloud registration).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">RELATED WORK</head><p>Deep Learning for Point Clouds Point cloud algorithms are increasingly moving toward DNNs, which has spurred recent interests in accelerating point cloud networks <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b35">36]</ref>. Point cloud DNNs mainly come in two forms: one that operates on raw points <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b69">70]</ref>, and the other that first voxelizes points and operates on voxels, which are grid-aligned points <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b21">22]</ref>. The former requires explicitly neighbor search whereas the latter accesses neighbors through simple indexing. It is unclear whether future point cloud algorithms will definitively favor one form over the other. Crescent focuses on optimizing point-based algorithms, whose flexibility and compact data representations are shown to be critical in many application domains <ref type="bibr" target="#b25">[26]</ref>, such as object detection, localization (SLAM), segmentation, and classification.</p><p>PointAcc <ref type="bibr" target="#b35">[36]</ref>, Point-X <ref type="bibr" target="#b68">[69]</ref>, and Mesorasi <ref type="bibr" target="#b17">[18]</ref> are all recent point cloud accelerators. They are fundamentally orthogonal to our work in that they focus on accelerating the feature computation in point cloud DNNs. For instance, Point-X and Mesorasi exploit the spatial locality and computation redundancy, respectively. All three use brute-force neighbor search and, thus, can directly benefit from the optimizations (approximate neighbor search and selective bank conflict elision) proposed in this paper. We show 1.9 ? speedup and 36% energy reduction over Mesorasi in Sec. 7.2.</p><p>Neighbor Search This paper targets neighbor search in low-dimensional space (2/3D), which is a fundamental building block in many computational science and engineering fields, where physical objects naturally lie in 2/3D space, such as computational fluid dynamics <ref type="bibr" target="#b29">[30]</ref>, computer graphics <ref type="bibr" target="#b67">[68]</ref>, and vision <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b65">66]</ref>. Prior work has explored both algorithmic and hardware solutions to accelerate neighbor search <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b65">66]</ref>, many of which are approximate in their nature <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b44">45]</ref>. We provide a quantitative comparison with QuickNN <ref type="bibr" target="#b43">[44]</ref> and Tigris <ref type="bibr" target="#b65">[66]</ref>, two most relevant accelerators in Sec. 7.5.</p><p>Optimizing Irregular Memory Accesses Recent work has made significant strides in domain-agnostic prefetching for irregular applications <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b56">57]</ref>. Our split-tree structure can be seen as an application-specific prefetcher and achieves "perfect prefetching" in that 1) off-chip data accesses are overlapped with computation, 2) data needed by the accelerator are readily available on-chip without stalls, and 3) no redundant DRAM accesses are needed.</p><p>Our split-tree structure also serves as an irregular tiling strategy, akin to propagation blocking for graph algorithms <ref type="bibr" target="#b12">[13]</ref>, but the decision as to which partition (sub-tree) a point is stored is based on the geometric position of a point.</p><p>Approximation Techniques Our approximation techniques exploit the inexact nature of DNNs. Selective bank conflict elision can be seen as a form of value approximation, bearing similarity to such approximation in general-purpose processors <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b63">64]</ref>. However, different from prior systems where the accuracy control is empirical, we integrate approximation into the training process; this allows us to provide statistical accuracy guarantees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">CONCLUSION</head><p>The mismatch between 3D perception algorithms and today's hardware designed and optimized for 2D perception will only increase in the future, where 3D perception applications are expected to be much more compute-and memory-intensive while at the same time being deployed in more resourceconstrained platforms such as micro aerial vehicles.</p><p>The mismatch between 3D perception algorithms and today's hardware designed and optimized for 2D perception will only increase in the future. Crescent demonstrates an algorithm-hardware collaborative approach toward taming the irregularities in point cloud algorithms. The key idea behind Crescent is to intentionally introduce approximations at both the algorithm and the hardware level to reduce memory inefficiencies (e.g., converting random DRAM accesses to streaming accesses, selectively eliding SRAM bank conflicts), and the mitigate the accuracy loss through approximateaware network retraining.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: A typical layer in a point cloud neural network, which has two main stages: neighbor search and feature computation. Neighbor search in itself is highly irregular as it requires tree traversal and is inputdependent. The feature computation requires irregular memory accesses because the input data are from the neighbor search results.</figDesc><graphic url="image-1.png" coords="3,151.44,99.75,59.10,50.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>PFig. 2 :Fig. 3 :</head><label>23</label><figDesc>Fig. 2: Percentage of non-continuous DRAM accesses in common point cloud networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: SRAM bank conflict rate in aggregation, assuming 16 banks and 16 concurrent memory requests.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>?</head><label></label><figDesc>Streaming write in top-tree search. ? Streaming read in sub-tree search. ? Streaming read in top/sub-tree search.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 :</head><label>8</label><figDesc>Fig. 8: Number of tree nodes visited per query reduces as the top-tree height increases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 9 :</head><label>9</label><figDesc>Fig. 9: Number of tree nodes skipped per query reduces as the elision height increases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 10 :</head><label>10</label><figDesc>Fig.10: Supporting bank conflict elision is trivial in hardware, as many existing hardware structures can be reused. The shaded/colored components are the augmentation, which is required for each SRAM port. Only the relevant part of the hardware is shown for simplicity. The Mode signal selects between the neighbor search mode and the feature computation mode. The AND gate lowers the Conflict signal when bank conflict elision is enabled in the neighbor search stage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 11 :</head><label>11</label><figDesc>Fig. 11: Training a point cloud network with approxineighbor search and bank conflict elision. Note that the training is end-to-end differentiable as in conventional DNN training. The non-differentiable parts, neighbor search and aggregation, do not participate in the gradient flow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 12 :</head><label>12</label><figDesc>Fig. 12: Overall architecture of the point cloud DNN accelerator, which includes three main components: a Neighbor Search Engine, an Aggregation Unit, and a systolic array for executing the MLPs in feature computation. The Neighbor Search Buffers include all the buffers shown in Fig. 7.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 13 :</head><label>13</label><figDesc>Fig. 13: Accuracy comparison between the baseline models, ANS+BCE without re-training, ANS with retraining under ? ? = 4, and ANS+BCE with re-training under ? ? = 4 and ? ? = 12. model accuracy between four schemes: 1) the baseline models, 2) ANS+BCE without re-training, 3) ANS+BCE with retraining, and 4) ANS with re-training. In this specific case, each re-trained model is trained specifically for the approximate setting where ? ? = 4 and/or ? ? = 12.Directly applying the two optimizations at inference time degrades the accuracy between 27.3% to 40.5%, making the models practically useless. Re-training regains the accuracy with an accuracy drop of at most 0.9% (PointNet++(c)). In PointNet++(s), re-training completely recovers the accuracy loss introduced in approximation. The fact that we can almost completely recover the accuracy loss with ANS+BCE, the most aggressive approximation setting, shows the effectiveness of our approximation-aware training. The accuracy of ANS alone is slightly higher than that of ANS+BCE, as the latter applies two approximations whereas the former applies only one.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>P</head><label></label><figDesc>o in tN e t+ + (c ) P o in tN e t+ + (s ) D e n se P o in t F -P o in tN e t A Speedup. Higher is better. P o in tN e t+ + (c ) P o in tN e t+ + (s ) D e n se P o in t F -P o in tN e t A Normalized energy. Lower is better.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 14 :</head><label>14</label><figDesc>Fig. 14: End-to-end speedup and normalized energy of ANS and ANS+BCE over the baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 15 :</head><label>15</label><figDesc>Fig. 15: Speedup and energy savings of ANS+BCE on neighbor search and aggregation alone.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 17 :Fig. 18 :</head><label>1718</label><figDesc>Fig. 17: Tree node access saving and bank conflict reduction of ANS+BCE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 19 :</head><label>19</label><figDesc>Fig. 19: Accuracy of dedicated PointNet++(c) models under different elision heights (? ? ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 20 :</head><label>20</label><figDesc>Fig. 20: Accuracy comparison of different training schemes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 21 :</head><label>21</label><figDesc>Fig. 21: Sensitivity of training accuracy to bank conflict configuration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Fig. 22 :</head><label>22</label><figDesc>Fig. 22: Sensitivity of speedup and (normalized) energy to hardware configuration (PE and bank counts) on PointNet++(c).</figDesc><graphic url="image-5.png" coords="12,337.74,97.97,69.19,69.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Fig. 24 :</head><label>24</label><figDesc>Fig. 24: Comparison with prior neighbor search accelerators Tigris and QuickNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Point Cloud DNN Accelerator</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>DRAM</cell></row><row><cell>Neighbor Buffers Search</cell><cell>Neighbor Engine Search</cell><cell cols="2">Systolic MAC Unit Array</cell><cell>Input Point Cloud</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>MLP</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Kernels &amp;</cell></row><row><cell>Neighbor Index Buffer Point Buffer</cell><cell>Neighbor Aggregation Logic</cell><cell>Global Buffer (Weights/ FMaps)</cell><cell>BN/ReLU/ Maxpooling DMA</cell><cell>Activations Neighbor Index Matrix</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Evaluation models.</figDesc><table><row><cell>Application</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Algorithm</cell><cell>Dataset</cell></row><row><cell>Domains</cell><cell></cell><cell></cell></row><row><cell>Classification</cell><cell>PointNet++ (c) DensePoint</cell><cell>ModelNet40</cell></row><row><cell>Segmentation</cell><cell>PointNet++ (s)</cell><cell>ShapeNet</cell></row><row><cell>Detection</cell><cell>F-PointNet</cell><cell>KITTI</cell></row><row><cell cols="3">The Point Buffer is a 64 KB 16-banked buffer serving points</cell></row><row><cell cols="3">during aggregation. The Neighbor Index Buffer is sized at</cell></row><row><cell cols="3">12 KB with a single bank. The Tree buffer and the Query</cell></row><row><cell cols="3">buffer are sized at 6 KB and 3 KB with 4 banks and 1 bank,</cell></row><row><cell cols="3">respectively. These two buffers support selective bank elision</cell></row><row><cell cols="3">as described in Sec. 4.3. The neighbor search engine has 4</cell></row><row><cell cols="3">PEs, each with a dedicated result buffer and a stack buffer,</cell></row><row><cell cols="3">which are sized at 1.5 KB and 256 B, respectively.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>compares the</figDesc><table><row><cell></cell><cell></cell><cell>Baseline</cell><cell>ANS+BCE w/ retraining</cell></row><row><cell>Accuracy (%)</cell><cell>40 60 80 100</cell><cell>Po in tN et + + (c ) 89.7 89.2 88.8 59.2 ANS w/ retraining Po in tN et + + (s ) 84.1 84.3 84.1 54.3</cell><cell>D en se Po in t 84.7 84.4 84.1 57.4 ANS+BCE w/o retraining F-Po in tN et 81.8 81.7 81.5 43.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>decouples the memory energy</figDesc><table><row><cell>Saving Contribution (%)</cell><cell>P o in tN e t+ + (c ) P o in tN e t+ + (s ) D e n s e P o in t F -P o in tN e t 0 20 40 60 80 100</cell><cell cols="3">DRAM Traffic Reduction DRAM Streaming SRAM Neighbor Search SRAM Aggregation</cell></row><row><cell cols="5">Fig. 16: Memory energy saving contribution.</cell></row><row><cell>Bank Conflict Reduction (%)</cell><cell>P o in tN e t+ + (c ) P o in tN e t+ + (s ) D e n se P o in t 0 10 20 30 40 50 Conflict</cell><cell>F-P o in tN e t Access</cell><cell>0 20 40 60 80 100</cell><cell>Tree Node Access Reduction (%)</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="10">ACKNOWLEDGEMENTS</head><p>The work was supported, in part, by <rs type="funder">NSF</rs> under grants #<rs type="grantNumber">2044963</rs> (and its REU supplements) and #<rs type="grantNumber">2126642</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_BBkMTAH">
					<idno type="grant-number">2044963</idno>
				</org>
				<org type="funding" xml:id="_GvUrhMt">
					<idno type="grant-number">2126642</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">ARCore</title>
		<ptr target="https://developers.google.com/ar" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">ARM&apos;s First Generation ML Processor, HotChips 30</title>
		<ptr target="https://www.hotchips.org/hc30/2conf/2.07_ARM_ML_Processor_HC30_ARM_2018_08_17.pdf" />
		<imprint/>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Artisan Memory Compilers</title>
		<ptr target="https://developer.arm.com/ip-products/physical-ip/embedded-memory" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Jetson TX2 Module</title>
		<ptr target="https://developer.nvidia.com/embedded/jetson-tx2" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<ptr target="https://www.micron.com/-/media/client/global/documents/products/data-sheet/dram/mobile-dram/low-power-dram/lpddr3/178" />
		<title level="m">Single-Channel Mobile LPDDR3 SDRAM Features</title>
		<imprint>
			<biblScope unit="page" from="8" to="16" />
		</imprint>
	</monogr>
	<note>Micron 178-Ball. gb_2c0f_mobile_lpddr3.pdf</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Micron System Power Calculators</title>
		<ptr target="https://www.micron.com/support/tools-and-utilities/power-calc" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">OpenHeritage 3D dataset</title>
		<ptr target="https://www.openheritage3d.org" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Waymo Offers a Peek Into the Huge Trove of Data Collected by Its Self-Driving Cars</title>
		<ptr target="https://spectrum.ieee.org/cars-that-think/transportation/self-driving/waymo-opens-up-part-of-its-humongous-selfdriving-database" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Garnet: A detailed on-chip network model inside a full-system simulator</title>
		<author>
			<persName><forename type="first">N</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-S</forename><surname>Peh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Jha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE international symposium on performance analysis of systems and software</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="33" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An event-triggered programmable prefetcher for irregular workloads</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ainsworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Sigplan Notices</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="578" to="592" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Distributed kd-trees for retrieval from very large image collections</title>
		<author>
			<persName><forename type="first">M</forename><surname>Aly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Munich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British machine vision conference (BMVC)</title>
		<meeting>the British machine vision conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An optimal algorithm for approximate nearest neighbor searching fixed dimensions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Arya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Mount</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Netanyahu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Silverman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM (JACM)</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="891" to="923" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Reducing pagerank communication via propagation blocking</title>
		<author>
			<persName><forename type="first">S</forename><surname>Beamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Asanovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="820" to="831" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multidimensional binary search trees used for associative searching</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Bentley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="509" to="517" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012[cs.GR</idno>
		<title level="m">ShapeNet: An Information-Rich 3D Model Repository</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
		<respStmt>
			<orgName>Stanford University -Princeton University -Toyota Technological Institute at Chicago, Tech. Rep</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">4d spatio-temporal convnets: Minkowski convolutional neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3075" to="3084" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised detection of vineyards by 3d point-cloud uav photogrammetry for precision agriculture</title>
		<author>
			<persName><forename type="first">L</forename><surname>Comba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Biglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Aimonino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers and electronics in agriculture</title>
		<imprint>
			<biblScope unit="volume">155</biblScope>
			<biblScope unit="page" from="84" to="95" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Mesorasi: Architecture support for point cloud analytics via delayed-aggregation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Whatmough</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1037" to="1050" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Tetris: Scalable and efficient neural network acceleration with 3d memory</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the 22nd ACM International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 25th IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Buffer kd trees: processing massive nearest neighbor queries on gpus</title>
		<author>
			<persName><forename type="first">F</forename><surname>Gieseke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heinermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Oancea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Igel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="172" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">3d semantic segmentation with submanifold sparse convolutional networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="9224" to="9232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Approximate kd tree search for efficient icp</title>
		<author>
			<persName><forename type="first">M</forename><surname>Greenspan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yurick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth International Conference on 3-D Digital Imaging and Modeling</title>
		<imprint>
			<date type="published" when="2003">2003. 3DIM 2003. 2003</date>
			<biblScope unit="page" from="442" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Kilo-noc: a heterogeneous network-on-chip architecture for scalability and service guarantees</title>
		<author>
			<persName><forename type="first">B</forename><surname>Grot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hestness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Keckler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 38th Annual International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="401" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pcpnet learning local shape properties from raw point clouds</title>
		<author>
			<persName><forename type="first">P</forename><surname>Guerrero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kleiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ovsjanikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="75" to="85" />
			<date type="published" when="2018">2018</date>
			<publisher>Wiley Online Library</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep learning for 3d point clouds: A survey</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Darkroom: Compiling high-level image processing code into hardware pipelines</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hegarty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Brunhaver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ragan-Kelley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vasilyev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A hardware processing unit for point sets</title>
		<author>
			<persName><forename type="first">S</forename><surname>Heinzle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Guennebaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Botsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acm Siggraph/eurographics Symposium on Graphics Hardware</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Characterization and analysis of deep learning for 3d point cloud analytics</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hyun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Architecture Letters</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="106" to="109" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A parallel sph implementation on multi-core cpus</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ihmsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Akinci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Teschner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="99" to="112" />
			<date type="published" when="2011">2011</date>
			<publisher>Wiley Online Library</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bajwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Boden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Borchers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Boyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cantin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Coriell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Daley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gelb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">V</forename><surname>Ghaemmaghami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gottipati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gulland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hagmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hogberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hundt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hurt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ibarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jaffey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jaworski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Khaitan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Laudon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lundin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mackean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Maggiore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mahony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Narayanaswami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Norrie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Omernick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Penukonda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Phelps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Salek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Samadiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Severn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sizikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Snelham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Souter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Steinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Swing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Thorson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Toma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tuttle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wilcox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Yoon</surname></persName>
		</author>
		<title level="m">Datacenter Performance Analysis of a Tensor Processing Unit</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>Proc. of ISCA</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">In-datacenter performance analysis of a tensor processing unit</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bajwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Boden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Borchers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 ACM/IEEE 44th Annual International Symposium on Computer Architecture</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Programming massively parallel processors: a hands-on approach</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Kirk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Wen-Mei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Morgan kaufmann</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Kuhara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Miyajima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yoshimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Amano</surname></persName>
		</author>
		<title level="m">An FPGA Acceleration for the Kd-tree Search in Photon Mapping</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep continuous fusion for multi-sensor 3d object detection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="641" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Pointacc: Efficient point cloud accelerator</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<imprint>
			<biblScope unit="page" from="449" to="461" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>in MICRO-54: 54th</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Densepoint: Learning densely contextual representation for efficient point cloud processing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th IEEE International Conference on Computer Vision</title>
		<meeting>the 14th IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">3d sceneflownet: Self-supervised 3d scene flow estimation based on graph cnn</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Conference on Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3647" to="3651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">An iterative image registration technique with an application to stereo vision</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 7th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Low latency photon mapping using block hashing</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Mccool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGGRAPH/EUROGRAPHICS conference on Graphics hardware. Eurographics Association</title>
		<meeting>the ACM SIGGRAPH/EUROGRAPHICS conference on Graphics hardware. Eurographics Association</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="89" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Approximative fast nearest-neighbour recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Miclet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dabouz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="277" to="285" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Doppelg?nger: a cache for approximate computing</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Miguel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Albericio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moshovos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">E</forename><surname>Jerger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th International Symposium on Microarchitecture</title>
		<meeting>the 48th International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="50" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Vector runahead</title>
		<author>
			<persName><forename type="first">A</forename><surname>Naithani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ainsworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Eeckhout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="195" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Quicknn: Memory and performance optimization of kd tree based nearest neighbor search for 3d point clouds</title>
		<author>
			<persName><forename type="first">R</forename><surname>Pinkham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="180" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Photon mapping on programmable graphics hardware</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Purcell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Donner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cammarano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2005 Courses</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page">258</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Convolution engine: balancing efficiency &amp; flexibility in specialized computing</title>
		<author>
			<persName><forename type="first">W</forename><surname>Qadeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hameed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Shacham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Venkatesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Horowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="24" to="35" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Frustum pointnets for 3d object detection from rgb-d data</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 31st IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="918" to="927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Gpu-accelerated nearest neighbor search for 3d registration</title>
		<author>
			<persName><forename type="first">D</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>N?chter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Conference on Computer Vision Systems</title>
		<meeting>the 9th International Conference on Computer Vision Systems</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Exploiting staleness for approximating loads on cmps</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V</forename><surname>Rengasamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sivasubramaniam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Kandemir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 International Conference on Parallel Architecture and Compilation (PACT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="343" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Towards 3d point cloud based object maps for household environments</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">C</forename><surname>Marton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Blodow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dolha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Beetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics and Autonomous Systems</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="927" to="941" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">The bunker cache for spatio-value approximation</title>
		<author>
			<persName><forename type="first">J</forename><surname>San</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Miguel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">E</forename><surname>Albericio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jerger</surname></persName>
		</author>
		<author>
			<persName><surname>Jaleel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 49th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Load value approximation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Miguel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Badr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">E</forename><surname>Jerger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 47th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="127" to="139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Lidar: Mapping the world in 3d</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schwarz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Photonics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">429</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Visualization and labeling of point clouds in virtual reality</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Stets</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Corning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Greenwald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH Asia</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Prodigy: Improving the memory latency of data-indirect irregular workloads using hardwaresoftware co-design</title>
		<author>
			<persName><forename type="first">N</forename><surname>Talati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Behroozi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kaszyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Vasiladiotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="654" to="667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">International archives of photogrammetry remote sensing and spatial information sciences</title>
		<author>
			<persName><forename type="first">G</forename><surname>Vosselman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dijkman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="37" to="44" />
		</imprint>
	</monogr>
	<note>3d building model reconstruction from point clouds and ground plans</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">CMOS VLSI design: a circuits and systems perspective</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">H</forename><surname>Weste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Harris</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Pearson Education India</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Fixynn: Efficient hardware for mobile computer vision via transfer learning</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Whatmough</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Venkataramanaiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>-S. Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattina</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.11128</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Autonomous navigation using a real-time 3d point cloud</title>
		<author>
			<persName><forename type="first">M</forename><surname>Whitty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cossell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guivant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Katupitiya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 Australasian Conference on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Fpga-based kmeans clustering using tree-based data structures</title>
		<author>
			<persName><forename type="first">F</forename><surname>Winterstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bayliss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Constantinides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Field Programmable Logic &amp; Applications</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Approximating warps with intra-warp operand value similarity</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Annavaram</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Symposium on High Performance Computer Architecture (HPCA</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="176" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th IEEE conference on computer vision and pattern recognition</title>
		<meeting>the 28th IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Tigris: Architecture and algorithms for 3d perception in point clouds</title>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 52nd Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="629" to="642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Ganax: A unified mimd-simd acceleration for generative adversarial networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yazdanbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Samadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Esmaeilzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th ACM/IEEE Annual International Symposium on Computer Architecture</title>
		<meeting>the 45th ACM/IEEE Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Differentiable surface splatting for point-based geometry processing</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yifan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Serena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>?ztireli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Point-x: A spatial-locality-aware architecture for energy-efficient graph-based point-cloud deep learning</title>
		<author>
			<persName><forename type="first">J.-F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1078" to="1090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Linked dynamic graph cnn: Learning on point cloud via linking hierarchical features</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>De Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10014</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Characterizing and demystifying the implicit convolution algorithm on commercial matrix-multiplication accelerators</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Symposium on Workload Characterization (IISWC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="214" to="225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Target-driven visual navigation in indoor scenes using deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kolve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE international conference on robotics and automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3357" to="3364" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
