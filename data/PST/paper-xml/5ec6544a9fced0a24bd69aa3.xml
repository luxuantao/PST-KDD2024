<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TransformerCPI: improving compound-protein interaction prediction by sequence-based deep learning with self-attention mechanism and label reversal experiments</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-05-19">19 May 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lifan</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Drug Discovery and Design Center</orgName>
								<orgName type="institution" key="instit1">State Key Laboratory of Drug Research</orgName>
								<orgName type="institution" key="instit2">Shanghai Institute of Materia Medica</orgName>
								<orgName type="institution" key="instit3">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>201203</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaoqin</forename><surname>Tan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Drug Discovery and Design Center</orgName>
								<orgName type="institution" key="instit1">State Key Laboratory of Drug Research</orgName>
								<orgName type="institution" key="instit2">Shanghai Institute of Materia Medica</orgName>
								<orgName type="institution" key="instit3">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>201203</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dingyan</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Drug Discovery and Design Center</orgName>
								<orgName type="institution" key="instit1">State Key Laboratory of Drug Research</orgName>
								<orgName type="institution" key="instit2">Shanghai Institute of Materia Medica</orgName>
								<orgName type="institution" key="instit3">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>201203</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Feisheng</forename><surname>Zhong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Drug Discovery and Design Center</orgName>
								<orgName type="institution" key="instit1">State Key Laboratory of Drug Research</orgName>
								<orgName type="institution" key="instit2">Shanghai Institute of Materia Medica</orgName>
								<orgName type="institution" key="instit3">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>201203</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaohong</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Drug Discovery and Design Center</orgName>
								<orgName type="institution" key="instit1">State Key Laboratory of Drug Research</orgName>
								<orgName type="institution" key="instit2">Shanghai Institute of Materia Medica</orgName>
								<orgName type="institution" key="instit3">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>201203</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Shanghai Institute for Advanced Immunochemical Studies</orgName>
								<orgName type="department" key="dep2">School of Life Science and Technology</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<postCode>200031</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tianbiao</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Drug Discovery and Design Center</orgName>
								<orgName type="institution" key="instit1">State Key Laboratory of Drug Research</orgName>
								<orgName type="institution" key="instit2">Shanghai Institute of Materia Medica</orgName>
								<orgName type="institution" key="instit3">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>201203</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaomin</forename><surname>Luo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Drug Discovery and Design Center</orgName>
								<orgName type="institution" key="instit1">State Key Laboratory of Drug Research</orgName>
								<orgName type="institution" key="instit2">Shanghai Institute of Materia Medica</orgName>
								<orgName type="institution" key="instit3">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>201203</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kaixian</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Drug Discovery and Design Center</orgName>
								<orgName type="institution" key="instit1">State Key Laboratory of Drug Research</orgName>
								<orgName type="institution" key="instit2">Shanghai Institute of Materia Medica</orgName>
								<orgName type="institution" key="instit3">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>201203</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Shanghai Institute for Advanced Immunochemical Studies</orgName>
								<orgName type="department" key="dep2">School of Life Science and Technology</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<postCode>200031</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hualiang</forename><surname>Jiang</surname></persName>
							<email>hljiang@simm.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Drug Discovery and Design Center</orgName>
								<orgName type="institution" key="instit1">State Key Laboratory of Drug Research</orgName>
								<orgName type="institution" key="instit2">Shanghai Institute of Materia Medica</orgName>
								<orgName type="institution" key="instit3">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>201203</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Shanghai Institute for Advanced Immunochemical Studies</orgName>
								<orgName type="department" key="dep2">School of Life Science and Technology</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<postCode>200031</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mingyue</forename><surname>Zheng</surname></persName>
							<email>myzheng@simm.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Drug Discovery and Design Center</orgName>
								<orgName type="institution" key="instit1">State Key Laboratory of Drug Research</orgName>
								<orgName type="institution" key="instit2">Shanghai Institute of Materia Medica</orgName>
								<orgName type="institution" key="instit3">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>201203</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">TransformerCPI: improving compound-protein interaction prediction by sequence-based deep learning with self-attention mechanism and label reversal experiments</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-05-19">19 May 2020</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1093/bioinformatics/btaa524</idno>
					<note type="submission">Received on February 26, 2020; revised on April 13, 2020; editorial decision on May 12, 2020; accepted on May 14, 2020</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Motivation: Identifying compound-protein interaction (CPI) is a crucial task in drug discovery and chemogenomics studies, and proteins without three-dimensional structure account for a large part of potential biological targets, which requires developing methods using only protein sequence information to predict CPI. However, sequencebased CPI models may face some specific pitfalls, including using inappropriate datasets, hidden ligand bias and splitting datasets inappropriately, resulting in overestimation of their prediction performance. Results: To address these issues, we here constructed new datasets specific for CPI prediction, proposed a novel transformer neural network named TransformerCPI, and introduced a more rigorous label reversal experiment to test whether a model learns true interaction features. TransformerCPI achieved much improved performance on the new experiments, and it can be deconvolved to highlight important interacting regions of protein sequences and compound atoms, which may contribute chemical biology studies with useful guidance for further ligand structural optimization.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Identifying compound-protein interaction (CPI) plays an import role in discovering hit compounds <ref type="bibr" target="#b39">(Vamathevan et al., 2019)</ref>. Conventional methods, such as structure-based virtual screening and ligand-based virtual screening, have been studied for decades and gained great success in drug discovery. However, some cases are not suitable to apply conventional screening methods, where the protein three-dimensional (3D) structure is unknown or the amount of known ligand dataset is too small. Therefore, <ref type="bibr" target="#b2">Bredel and Jacoby (2004)</ref> introduced a novel perspective called chemogenomics to predict CPI without protein 3D structures. A variety of machine learning based algorithms have been proposed since then, which considers compound information and protein information at the same time in a unified model <ref type="bibr" target="#b1">(Bleakley and Yamanishi, 2009;</ref><ref type="bibr" target="#b5">Cheng et al., 2012;</ref><ref type="bibr" target="#b11">Gonen, 2012;</ref><ref type="bibr" target="#b14">Jacob and Vert, 2008;</ref><ref type="bibr" target="#b40">van Laarhoven et al., 2011;</ref><ref type="bibr" target="#b44">Wang et al., 2011;</ref><ref type="bibr" target="#b45">Wang and Zeng, 2013;</ref><ref type="bibr" target="#b47">Yamanishi et al., 2008)</ref>.</p><p>With the rapid development of deep learning, many types of endto-end frameworks have been utilized in CPI research. In comparison with traditional machine leaning algorithms, end-to-end learning integrates representation learning and model training in a unified architecture simultaneously and no descriptors need to be defined and calculated before modeling. Although deep neural networks have been used in several CPI models, these current methods take predefined molecular fingerprints and protein descriptors as input features, which are fixed during training process and contain less information than that of end-to-end learning <ref type="bibr" target="#b13">(Hamanaka et al., 2017;</ref><ref type="bibr" target="#b37">Tian et al., 2016;</ref><ref type="bibr" target="#b42">Wan and Zeng, 2016)</ref>. Regarding the CPI problem as binary classification task, compounds can be considered as 1D sequences or molecular graphs (i.e. traditionally called 2D structures), and protein sequences can be regarded as 1D sequences. DeepDTA <ref type="bibr" target="#b28">(Ozturk et al., 2018)</ref> used convolutional neural networks (CNNs) to extract low-dimensional real-valued features of compounds and proteins, and then concatenated two feature vectors and pass through fully connected layers to calculate the final output. WideDTA <ref type="bibr">(O ¨ztu ¨rk et al., 2019)</ref> and Conv-DTI <ref type="bibr" target="#b19">(Lee et al., 2019)</ref> followed the similar idea, and WideDTA utilized two extra features as well, ligand max common structures and protein motifs and domains, to improve model performance. From another perspective that regards compound structure as molecular graph, CPI-GNN <ref type="bibr" target="#b38">(Tsubaki et al., 2019)</ref> and GraphDTA <ref type="bibr" target="#b27">(Nguyen et al., 2019)</ref> used graph neural networks <ref type="bibr" target="#b32">(Scarselli et al., 2009)</ref> (GNNs) and graph CNNs <ref type="bibr" target="#b17">(Kipf and Welling, 2016)</ref> (GCNs) instead of CNNs to learn the representation of compounds. In addition, recurrent neural networks were used to extract feature vectors of compounds and proteins in DeepAffinity <ref type="bibr" target="#b15">(Karimi et al., 2019)</ref>, and <ref type="bibr" target="#b9">Gao et al. (2018)</ref> and <ref type="bibr" target="#b51">Zheng et al. (2020)</ref> also treated compounds and proteins as sequential information.</p><p>Due to the high relevance to chemical biology and pharmaceutical chemistry, many novel models based on deep learning or machine learning have been developed showing satisfactory performance on various datasets. However, much less efforts have been devoted to evaluate their generalization ability on external tests or practical applications. Since deep learning is a data-driven technique, it is critical to understand what the model really learns and to avoid the influences of unexpected factors. Recently, Google researchers put forward three pitfalls to avoid in machine learning <ref type="bibr" target="#b31">(Riley, 2019)</ref>, consisting of splitting data inappropriately, hidden variable and mistaking the objective. Inspired by these warnings in the AI industry, we wonder whether chemogenomics-based CPI modeling is facing similar problems, and three unique problems are summarized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Using inappropriate datasets</head><p>Data are the core foundation of deep learning models, and in a way what a model learns mainly depends on the datasets it is fed, and inappropriate datasets make the model easily deviate from the goal. In chemogenomics-based CPI modeling, the general goal of modeling is to predict interaction between different proteins and different compounds based on a form of abstract representation of protein and ligand features. Therefore, interaction information is the key ingredient that the model should learn from the datasets. Considering chemogenomics-based CPI modeling as a binary classification task, a properly designed dataset should mainly consist of such instances that a specific ligand interacts with protein A but does not interact with protein B, which forces the model to learn protein information, or preferably the interaction features, to distinguish these instances. This proper designed dataset cannot be separated by other information rather than interaction features, such as ligand patterns. Previous chemogenomics-based CPI prediction models used inappropriate datasets to build deep learning models, such as DUD-E dataset <ref type="bibr" target="#b26">(Mysinger et al., 2012)</ref> and Human dataset <ref type="bibr" target="#b20">(Liu et al., 2015;</ref><ref type="bibr" target="#b38">Tsubaki et al., 2019)</ref>, where DUD-E dataset was collected with the intention to train structure-based virtual screening. Moreover, most ligands in DUD-E, MUV, Human and BindingDB only occur in one class, and negative samples were generated by algorithms that may introduce undetectable noise <ref type="bibr" target="#b20">(Liu et al., 2015;</ref><ref type="bibr" target="#b26">Mysinger et al., 2012)</ref>. These datasets can be separated by ligand information, and cannot guarantee that models learn protein information or interaction features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Hidden ligand bias</head><p>Deep learning system is usually referred to as black-box models, thus it is difficult to interpret what exactly the model learns and based on which the model makes a prediction. Obtaining a better performance on the validation set and the test set usually means the end of the study, and fewer efforts were devoted to further investigate if the model learns in the manner as expected. The hidden ligand bias issue has been reported in DUD-E and MUV datasets <ref type="bibr" target="#b34">(Sieg et al., 2019)</ref>, raising extensive concerns in the field of drug design. Structure-based virtual screening, 3D-CNN-based models <ref type="bibr" target="#b4">(Chen et al., 2019)</ref> and other models trained on DUD-E dataset <ref type="bibr" target="#b34">(Sieg et al., 2019)</ref> have been pointed out to make predictions mainly based on ligand patterns rather than interaction features, leading to mismatch between theoretical modeling and practical application. We wondered whether chemogenomics-based CPI modeling facing similar problem, and thus revisited a previous typical model CPI-GNN trained on Human dataset as an example to study the potential effects of hidden ligand bias.</p><p>Figure <ref type="figure" target="#fig_0">1A</ref> shows the weight distribution plot of CPI-GNN model trained on Human dataset. The weights of CNN blocks used to extract protein features are significantly concentrated in zero, which indicates that little protein information has been considered when making prediction. In contrast, the weight distribution of GNN blocks utilized to extract compound features is wide and flat. We therefore argue that ligand information plays an overwhelming role as compared to protein information. Further training with ligand-only information and its comparison with the original model are elucidated in Figure <ref type="figure" target="#fig_0">1B</ref>, where the dataset was randomly split for 10 times, and two models were evaluated on 10 different trails. The P-value in a two-sample t-test for the difference of AUC distribution is greater than 0.05, suggesting that using ligand information alone may achieve competitive performance to the original CPI-GNN model using both ligand and protein information. Thus, CPI-GNN model mainly learns how to classify different ligands rather than different CPI pairs, which increases the risk that a ligand is Blue line depicts the weight distribution of CNN blocks used to extract protein features. Orange line depicts the weight distribution of GNN blocks used to extract compound features. (B) Violin plots of AUROC for two CPI-GNN models, one utilizing both protein and ligand information, and the other using ligand information only. The white dots are the average AUROCs. The upper and lower end points of the black segments are the first and the third quartile, respectively. The P-value of the t test is shown above the violin. Each model was evaluated on 10 different trials, and the P-value was 0.067 always predicted to interact or not interact with different proteins. These results highlighted the possibility that ligand patterns can mislead the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Splitting dataset inappropriately</head><p>The risk of hidden ligand bias is difficult to eliminate but can be reduced. Usually, machine learning researchers split data into training and test sets at random. However, using conventional classification measurements on a randomly split test set, we are not clear whether the model learns true interaction features or other unexpected hidden variables, which may produce precise models that answer the wrong questions <ref type="bibr" target="#b31">(Riley, 2019)</ref>. Thus, test sets should be designed according to the real goal of modeling and its application scenario.</p><p>To address these pitfalls, we proposed a novel transformer neural network named TransformerCPI, constructed new datasets specific for CPI modeling, and introduced a more rigorous label reversal experiments to evaluate whether a data-driven model falls into common pitfalls of AI. As a result, TransformerCPI achieved the best performance on three public datasets and two label reversal datasets. Moreover, we further studied the interpretability of TransformerCPI to uncover its underlying prediction mechanism by mapping attention weights back to protein sequences and compound molecules, and the results also confirmed that the self-attention mechanism of TransformerCPI is useful in capturing the desired interaction features. We hope that these findings may raise our attention to improve the generalization and interpretation capability of CPI modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Materials and methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Model architecture of TransformerCPI</head><p>The model we proposed is based on the transformer architecture <ref type="bibr" target="#b41">(Vaswani et al., 2017)</ref>, which was originally devised for neural machine translation tasks. Transformer is an autoregressive encoderdecoder model using a combination of multiheaded attention layers and positional feed forward to solve sequence-to-sequence (seq2seq) tasks. Recently, transformer architecture achieves great success in language representation learning task, and many novel and powerful pre-training models have been established, such as BERT <ref type="bibr" target="#b8">(Devlin et al., 2019)</ref>, GPT-2 , Transformer-XL <ref type="bibr" target="#b6">(Dai et al., 2019)</ref> and XLnet <ref type="bibr" target="#b49">(Yang et al., 2019)</ref>. Transformer is also applied in chemical reaction prediction <ref type="bibr" target="#b33">(Schwaller et al., 2019)</ref>, however, it is still confined in seq2seq tasks. Inspired by its great ability of capturing features between two sequences, we modified the transformer architecture to predict CPI, regarding compounds and proteins as two kinds of sequences. An overview of the proposed TransformerCPI is shown in Figure <ref type="figure" target="#fig_1">2</ref>, where we remained the decoder of the transformer and modified its encoder and final linear layers.</p><p>To convert protein sequences into sequential representation, we first split a protein sequence into an overlapping 3-gram amino acid sequence, and then translated all words to real-valued embeddings by the pretraining approach word2vec <ref type="bibr">(Mikolov et al., 2013a,b)</ref>. Word2vec is an unsupervised technique to learn high-quality distributed vector representations that describe sophisticated syntactic and semantic word relationships, comprising two pretraining technique called Skip-Gram and Continue Bag-of-Words (CBOW). Skip-Gram is used to predict a certain word from its context, while CBOW is used to predict context from a given word. Integrating Skip-Gram and CBOW, word2vec can finally map the words to lowdimensional real-valued vectors, where the words that have similar semantics map to the vectors that are close to each other. There have been some works to apply word2vec to represent protein sequences <ref type="bibr" target="#b16">(Kimothi et al., 2016;</ref><ref type="bibr" target="#b18">Kobeissy et al., 2015;</ref><ref type="bibr">Mazzaferro and Carlo, 2017;</ref><ref type="bibr" target="#b48">Yang et al., 2018)</ref>, in which the amino acid sequence of constant length k (k-mers) were split as words and the whole amino acid sequence was regarded as a document. We followed these works to preprocess protein sequence, and included all human protein sequences in UniProt as corpus to pretrain the word2vec model, and set hidden dimension to 100. After training the word2vec model for 30 epochs on the large corpus we built before, protein sequences can be inferred to real-valued 100-dimensional vectors.</p><p>Sequential feature vectors of proteins were then passed to the encoder to learn more abstract representations of proteins. Of note here we replaced the original self-attention layers in the encoder with a relatively simple structure. Considering that conventional transformer architecture usually requires a large training corpus and is easy to overfit on small or modestly sized datasets <ref type="bibr" target="#b30">(Qiu et al., 2020)</ref>, we used a gated convolutional network <ref type="bibr" target="#b7">(Dauphin et al., 2016)</ref> with Conv1D and gated linear unit instead because it showed better performance on our designed datasets. The input to gated convolutional network is a sequence of protein feature vectors. We compute the hidden layers h 0 ; . . . ; h L as Equation 1</p><formula xml:id="formula_0">h l X ð Þ ¼ XÃW 1 þ s ð Þ br XÃW 2 þ t ð Þ ; (1)</formula><p>where</p><formula xml:id="formula_1">X 2 R nÂm1 , is the input of layer h l , W 1 2 R kÂm1Âm2 , s 2 R m2 , W 2 2 R kÂm1Âm2 , t 2 R m2</formula><p>, are learned parameters, L is the number of hidden layers, n is the sequence length, m 1 , m 2 are the dimension of input and hidden features, respectively, and k is the patch size, r is the sigmoid function and is the element-wise product between matrices <ref type="bibr" target="#b7">(Dauphin et al., 2016)</ref>. The output of the gated convolutional network is the final representation of protein sequences, as shown in Figure <ref type="figure" target="#fig_1">2</ref>. In our implementation, L is 3, m 1 is 64, m 2 is 128 and k is 7. The output of the encoder is protein sequence p 1 ; p 2 ; . . . ; p b , where b is the length of protein sequence. Each of the atom features was initially represented as a vector of size 34 using RDKit python package, and the list of atom features is summarized in Table <ref type="table" target="#tab_0">1</ref>. We then used GCNs to learn the representation of each atom by integrating its neighbor atom features.</p><p>The GCN is originally devised to solve the problem of semisupervised node classification, which can be transferred to solve molecular representation problem. We here denote a graph for a compound molecule as G ¼ ðV; EÞ, where V 2 R aÂf is the set of a atoms in a molecule, each represented as a f-dimensional feature vector and E is the set of covalent bonds in a molecule represented as an adjacency matrix A 2 R aÂa . The propagation rule is shown in Equation <ref type="formula" target="#formula_3">2</ref>:</p><formula xml:id="formula_2">H lþ1 ð Þ ¼ f H l ð Þ ; A ¼ r DÀ 1 2 Ã DÀ 1 2 H l ð Þ W<label>ðlÞ</label></formula><formula xml:id="formula_3">3 ;<label>(2)</label></formula><p>where Ã ¼ A þ I; I is the identity matrix; H ðlÞ 2 R aÂf is the output of the 'th layer, W ðlÞ 3 2 R f Âf is a weight matrix for the 'th neural network layer, D 2 R aÂa is the diagonal node degree matrix of Ã 2 R aÂa ; and r Á ð Þ is a nonlinear activation function. In our implementation, we chose f to be 34, and the number of GCN layers to be 1. After processed by GCN layer, the atom sequence c 1 ; c 2 ; . . . ; c a is obtained, where a is the number of atoms.</p><p>When protein sequence representation and atom representation were obtained, we successfully converted proteins and compounds into two sequences, which fitted the transformer architecture. Interaction features are learned through the decoder of transformer, which consists of self-attention layers and feed forward layers. In our work, protein sequence is the input of encoder, while the atom sequence is the input of decoder, and the output of decoder is the interaction sequence which contains interaction features and has the same length with atom sequence. Given that the order of atom feature vectors has no effect on CPI modeling, we removed positional embeddings in TransformerCPI.</p><p>The key technique in the decoder is multiheaded self-attention layer. A multiheaded self-attention layer consists of several scaleddot attention layers to extract interaction information between the encoder and the decoder. The self-attention layer takes three inputs, the keys, K, the values, V and the queries, Q, and calculates the attention as follows:</p><formula xml:id="formula_4">attention Q; K; V ð Þ¼ softmax QK T ffiffiffiffiffi d k p ! V;<label>(3)</label></formula><p>where d k is a scaling factor depending on the layer size. This mechanism allows the decoder to focus on some crucial parts from the output of encoder dynamically, which directly captures the interaction features of the given two sequences. In addition, the original transformer was designed to solve sequence prediction tasks and utilize mask operation to cover the downstream context of each word in the decoder. Therefore, we modified the mask operation of the decoder to ensure that our model is accessible to whole sequence, which is one of the most crucial modification to transfer transformer architecture from autoregressive task to classification task (Fig. <ref type="figure" target="#fig_1">2</ref>).</p><p>After extracting interaction features in the decoder, a set of interaction sequence x 1 ; x 2 ; . . . ; x a are obtained, and the modulus of each vector is computed as follows:</p><formula xml:id="formula_5">x 0 i ¼ k x i k 2 2 ; (<label>4</label></formula><formula xml:id="formula_6">)</formula><p>where i ¼ 1; 2; 3; . . . ; a. The weight of each vector can be calculated by softmax function as follows:</p><formula xml:id="formula_7">a i ¼ e x 0 i P a i¼1 e x 0 i :<label>(5)</label></formula><p>The final interaction feature vector is calculated by weighted sum of interaction vectors with attention weights:</p><formula xml:id="formula_8">y interaction ¼ X a i¼1 a i x i :<label>(6)</label></formula><p>At last, the final interaction feature vector y interaction is fed to the following fully connected layers, and the probability ŷ that a compound interacts with a protein is returned. a conventional binary classification task, we used binary cross entropy loss to train TransformerCPI model:</p><formula xml:id="formula_9">Loss ¼ À ylogŷ þ 1 À y ð Þlog 1 À ŷ ð Þ Â Ã :<label>(7)</label></formula><p>TransformerCPI was implemented with Pytorch 1.2.0, and the word2vec model was built and trained with Gensim 3.4.0. Whereas the base transformer model had six layers with 512 hidden dimension, we decreased the number of layers from 6 to 3 and the dimension of hidden layers from 512 to 64. The dimension of protein representation, atom representation, hidden layers and y interaction is 64. We kept the original eight attention heads because this configuration achieved superior generalization ability. For the training, we used the LookAhead <ref type="bibr" target="#b50">(Zhang et al., 2019)</ref> optimizer combined with RAdam <ref type="bibr" target="#b21">(Liu et al., 2019)</ref> optimizer, which solved the most serious convergence problems caused by Adam optimizer without the learning rate warmup. The learning rate was set to 0.0001, the batch size was set to 8 and the gradients were accumulated over eight batches. All the settings and hyperparameters of TransformerCPI are summarized in Table <ref type="table" target="#tab_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Public datasets</head><p>We compared our model on three previous benchmark datasets, Human dataset, Caenorhabditis elegans dataset <ref type="bibr" target="#b38">(Tsubaki et al., 2019)</ref> and BindingDB dataset <ref type="bibr" target="#b9">(Gao et al., 2018)</ref>. Human dataset and C.elegans dataset include positive CPI pairs from DrugBank 4.1 <ref type="bibr" target="#b46">(Wishart et al., 2008)</ref> and Matador <ref type="bibr" target="#b12">(Gunther et al., 2007)</ref> and highly credible negative CPI samples obtained using a systematic screening framework <ref type="bibr" target="#b20">(Liu et al., 2015)</ref>. In detail, the human dataset contains 3369 positive interactions between 1052 unique compounds and 852 unique proteins; the C.elegans dataset contains 4000 positive interactions between 1434 unique compounds and 2504 unique proteins and the training, valid and test sets are randomly split <ref type="bibr" target="#b38">(Tsubaki et al., 2019)</ref>. BindingDB dataset contains 39 747 positive examples and 31 218 negative examples from a public database <ref type="bibr" target="#b10">(Gilson et al., 2016)</ref>. The training, valid and test sets of BindingDB are well-designed, and the test set includes CPI pairs where ligands or proteins are not observed in training set. Therefore, BindingDB dataset can assess models' generalization ability to unknown ligands and proteins.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Label reversal datasets</head><p>To construct datasets specifically for chemogenomics-based CPI modeling, we followed two rules: (i) collecting CPI data from  First, we constructed a GPCR dataset from GLASS database <ref type="bibr" target="#b3">(Chan et al., 2015)</ref>. GLASS database provides a great amount of experimentally validated GPCR-ligand associations <ref type="bibr" target="#b3">(Chan et al., 2015)</ref>, which satisfies our first rule. GLASS database used IC 50 , Ki and EC 50 as the binding affinity values, which were transformed into negative logarithm, pIC 50 , pK i and pEC 50 . Following early works <ref type="bibr" target="#b22">(Liu et al., 2007;</ref><ref type="bibr" target="#b43">Wan et al., 2019)</ref>, a threshold of 6.0 was set to divide original dataset into a positive set and a negative set. Then, we selected protein-compound pairs that follow our second rule to construct the final GPCR dataset. Our final GPCR dataset comprises 5359 ligands, 356 proteins and 15 343 CPI among them.</p><p>Second, we constructed a Kinase dataset based on KIBA dataset <ref type="bibr" target="#b36">(Tang et al., 2014)</ref>. KIBA score was developed to combine various bioactivity types, including IC 50 , K i and K d , and to remove inconsistency between different bioactivity types, which greatly reduced bias in the dataset <ref type="bibr" target="#b36">(Tang et al., 2014)</ref>. The KIBA dataset contains 467 targets and 52 498 ligands collected from ChEMBL and STITCH <ref type="bibr" target="#b35">(Szklarczyk et al., 2016)</ref>, which ensures that data in KIBA is experimentally validated. Given that the majority of ligands only occur once, we followed SimBoost <ref type="bibr" target="#b14">(He et al., 2017)</ref> to filter original KIBA dataset to comprise only compounds and proteins with at least 10 interactions, gaining a total of 229 proteins and 2111 compounds. Then, we used the suggested threshold KIBA value of 12.1 <ref type="bibr" target="#b14">(He et al., 2017;</ref><ref type="bibr" target="#b36">Tang et al., 2014)</ref> to divide dataset into a positive set and a negative set, and selected protein-compound pairs where compounds occur in both positive set and negative set, yielding a total of 1644 compounds, 229 proteins and 111 237 CPI. Table <ref type="table" target="#tab_2">3</ref> summarizes GPCR dataset and Kinase dataset we constructed.</p><p>As mentioned before, hidden ligand bias may cause a datadriven model to learn unexpected statistical clues or patterns in data other than the desired CPI information. To confirm the model actually learn the interaction features and accurately assess the impact of hidden variables, we proposed a more rigorous label reversal experiment. The schematic illustration of label reversal experiment is shown in Figure <ref type="figure" target="#fig_2">3A</ref>, where a ligand in the training set appears only in one class of samples (either positive or negative interaction CPI pairs), while the ligand appears only in the opposite class of samples in the test set. In this way, the model was forced to utilize protein information to understand interaction modes and make opposite predictions for those chosen ligands. If a model only memorizes the ligand patterns, it is unlikely to make correct predictions because the ligands it memorizes have the wrong (opposite) labels in test set. Therefore, this label reversal experiment is specifically designed to evaluate chemogenomics-based CPI models and is capable of indicating how much influence the hidden ligand bias has exerted.</p><p>For GPCR set and Kinase set, we randomly selected 500 and 300 ligands, respectively, and pooled together all the negative CPI samples involving these ligands in the test set. Also, we selected another 500 and 300 ligands, respectively, and pooled together all their associated positive samples in the test set. Under this experiment design, we finally established GPCR test set with 1537 interactions and Kinase test set with 19 685 interactions. The remaining datasets were used to determine the hyperparameters, and the best model was selected to evaluate on label reversal experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Data distribution of label reversal datasets</head><p>Before training the model, we studied the data distribution of GPCR set and Kinase set. Since each ligand may occur in multiple positive and negative classes, representing either interacting or noninteracting with different proteins, the frequency of occurrence in positive and negative samples was analyzed. On account of this issue, we calculated the log ratio of two classes for each ligand as follows to describe the data distribution:</p><formula xml:id="formula_10">log ratio ðiÞ ¼ log 10 N i ð Þ pos N i ð Þ neg 0 @ 1 A ; i ¼ 1; 2; 3; . . . ; L;</formula><p>where</p><formula xml:id="formula_11">N i ð Þ</formula><p>pos is the number of ith ligand's positive interactions while</p><formula xml:id="formula_12">N i ð Þ</formula><p>neg is the number of ith ligand's negative interactions, and L is the total number of ligands. The distributions of GPCR set and Kinase set are shown in Figure <ref type="figure" target="#fig_2">3B</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results and discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Performance on public datasets</head><p>Many machine learning methods, such as K nearest neighbors (KNN), random forest (RF), L2-logistic (L2), support vector  machines (SVM), newly reported sequence-based models CPI-GNN <ref type="bibr" target="#b38">(Tsubaki et al., 2019)</ref> and DrugVQA <ref type="bibr" target="#b51">(Zheng et al., 2020)</ref> have been evaluated on these datasets. GraphDTA <ref type="bibr" target="#b27">(Nguyen et al., 2019)</ref> was originally designed for regression task, here, we tailor its last layer to binary classification task. It should be noted that models relying on 3D structural information of protein are not compared here, due to the absence of such information for these two datasets. We followed the same training and evaluating strategies as CPI-GNN <ref type="bibr" target="#b38">(Tsubaki et al., 2019)</ref> and repeated with three different random seeds followed by DrugVQA <ref type="bibr" target="#b51">(Zheng et al., 2020)</ref> to evaluate TransformerCPI, and Area Under Receiver Operating Characteristic Curve (AUC), precision and recall of each model are shown in Tables <ref type="table" target="#tab_4">4 and 5</ref>. Since the implementation of KNN, RF, L2, SVM are not mentioned in the literature <ref type="bibr" target="#b38">(Tsubaki et al., 2019)</ref>, these models are not compared on BindingDB dataset. Area Under Precision Recall Curve (PRC) and AUC of each model are shown in Table <ref type="table">6</ref>. TransformerCPI outperformed other models on three public datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Performance on label reversal datasets</head><p>We chose CPI-GNN, GraphDTA and GCN as references and compared the performance of TransformerCPI with these models in terms of AUC and PRC. Figure <ref type="figure" target="#fig_2">3C</ref> summarizes the AUC and PRC of these models. To conduct fair comparison, each model was thoroughly fine-tuned on the same valid sets. As shown in Figure <ref type="figure" target="#fig_2">3C</ref>, all of the models achieved similar performance on both GPCR valid set and Kinase valid set, however, big performance gaps between these models were observed on test sets. Although these models have similar performance on random split valid sets, the knowledge they have learned greatly differs from each other, which is exposed by a more rigorous label reversal experiment. On GPCR set, TransformerCPI outperformed CPI-GNN, GraphDTA and GCN both in terms of AUC and PRC, showing improved power to capture interaction features between compounds and proteins. Compared with other models, CPI-GNN failed to enrich positive samples before negative samples in label reversal experiments, so we argue that ligand patterns of GPCR dataset may bring in non-negligible influence as ligand bias in CPI-GNN. On Kinase set, TransformerCPI outperforms CPI-GNN, GraphDTA and GCN in terms of AUC and PRC and AUC of reference models are all smaller than 0.5, so we argue that ligand patterns of Kinase dataset may brought in non-negligible influence in all reference models. Moreover, GraphDTA and GCN achieved good performance on GPCR dataset, which are close to TransformerCPI, but performed much worse on Kinase set. In comparison, TransformerCPI achieved the best performance on both datasets, revealing its robustness and generalization ability. Overall, these results suggested that our proposed TransformerCPI possesses capability of learning interactions between proteins and ligands, and the label reversal experiments can effectively assess the impact of hidden ligand bias on models, and, more importantly, the proposed modeling scheme is useful in reducing common risks of chemogenomics-based CPI tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">The system dependency of models</head><p>When comparing the results between GPCR set and Kinase set, it is also of note that TransformerCPI, GraphDTA and GCN perform much better on GPCR set than Kinase set. We argue that there might be two potential reasons for this performance difference. The first one is that the data distribution of GPCR set and Kinase set is different, resulting in performance gap between the two datasets. The second one is that the sequence features of GPCR are relatively easier for TransformerCPI to learn. As shown in Figure <ref type="figure" target="#fig_2">3B</ref>, the peak of log_ratio distribution of GPCR set is located at zero, which means most ligands in GPCR set have an equal quantity of interaction pairs and noninteraction pairs. In contrast, the peak of log_ratio distribution of Kinase set is significantly shifted to -1, indicating that most ligands in Kinase set possess almost ten times more noninteraction pairs than interaction pairs. Therefore, the highly unbalanced distribution of positive pairs and negative pairs may introduce severe ligand bias to the dataset, which might increase the risk that a data-driven model memorizes ligand patterns, causing inferior prediction performance on Kinase set.</p><p>Another potential reason is that the CPI-associated sequence features of GPCR are easier to learn than those of Kinase. Although GPCR family shares massive alpha-helix regions and seven transmembrane structures, the binding location and binding pockets are more diverse across the family, which is relatively easy for models to learn CPI-associated sequence features to distinguish interaction pairs and noninteraction pairs. However, compared with GPCR family, Kinase family shares a more conservative ATP binding pocket with fewer different residues. It is challenging for models to distinguish interaction and non-interaction pairs since the model has to learn to detect and understand the minor changes on protein sequences. Furthermore, the system dependency of TransformerCPI also informs us that there is still room for improvement in chemogenomics-based CPI prediction, especially the representation of protein sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Model ablation study</head><p>Previous chemogenomics-based CPI models extract ligand and protein features separately and independently, and then concatenate these two feature vectors as input features. To validate the role of the transformer encoder-decoder architecture, we next evaluated the TransformerCPI-ablation model which replaces transformer It should be noted that DrugVQA uses protein structural information as input, while its alternative version VQA-seq only using protein sequence information is listed here for a fair comparison. decoder by conventional vector concatenation on the same label reversal experiment. As shown in Figure <ref type="figure" target="#fig_2">3C</ref>, this ablation procedure significantly compromised the performance of TransformerCPI on both GPCR set and Kinase set, demonstrating that self-attention mechanism together with encoder-decoder architecture indeed plays a key role in extracting CPI features between two types of sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Model interpretation</head><p>Although deep learning is known as a black box algorithm, it is essential to understand how the model makes a prediction, and whether the model can provide suggestions or guidance for optimization. Due to the transformer architecture and self-attention mechanism, it provides easy access for understanding the mechanism behind the model through attention weights of protein sequences and compound atoms.</p><p>As illustrated in Figure <ref type="figure" target="#fig_3">4A</ref>, the attention weights were mapped to compound atoms to reveal the knowledge TransformerCPI has learned. TransformerCPI pays attention to different atoms when facing different compound protein pairs and correctly classify the compound protein pairs into two classes, interaction and noninteraction.</p><p>Here, TransformerCPI generates different ligand features corresponding to different proteins, which is consistent with the fact that binding modes of the ligand are different when interacting with different proteins. Therefore, it is difficult for TransformerCPI to memorize ligand patterns because the ligand features are changing with regard to different proteins. This result also explains why TransformerCPI shows better performance on label reversal experiments. Dynamic feature extraction of TransformerCPI based on a specific protein context helps the model extract the key information of the interaction, while also reducing the probability of hidden ligand bias. Moreover, the decoder of TransformerCPI integrates the features of protein sequences and compound atoms dynamically to form direct interaction features, which is similar to language translation task and agrees well with the binding process of ligands to proteins.</p><p>To further verify the meaning of the attention weights of atoms, we selected the compound phenothiazine to show the interpretation of TransformerCPI. Phenothiazine is a classic antipsychotic drug targeted on dopamine receptor, and its structure-activity relationship (SAR) has been thoroughly explored. As illustrated in Figure <ref type="figure" target="#fig_3">4B</ref>, the atoms of phenothiazine highlighted by attention weights agree well with the SAR of phenothiazine, which confirms that our model is capable of catching true interaction features and finding out key atoms interacting with proteins. This information of great value assists medicinal chemists to speculate the potential SAR of the target molecule and may offer useful guidance to further structural optimization.</p><p>After interpreting atom-level attention mechanism, we also studied the attention weights of the protein sequences to see which parts of the protein sequences became the focus of attention. As a result, TransformerCPI may roughly speculate whether the binding site of the ligand to the GPCR family is in the extracellular region or in the transmembrane region, and detect the ATP-binding pocket of Kinase family. We selected histamine H1 receptor, 5-HT 1B receptor and mitogen-activated protein kinase 8 (MAPK8) with their corresponding actives as examples.</p><p>As shown in Figure <ref type="figure" target="#fig_4">5</ref>, TransformerCPI successfully localized the binding site of the ligand to histamine H1 receptor in the transmembrane region, the binding site of the ligand to 5-HT 1B receptor in the extracellular region, and detected ATP-binding pocket of MAPK8, which further verifies that TransformerCPI has learned biological knowledge and gained structural insights. These results suggested that TransformerCPI can speculate whether a new compound binds to the transmembrane region or the extracellular region of a GPCR target, which is useful in drug design especially when 3D structure of the GPCR target is unknown. Meanwhile, we may also notice that the highlighted region involves more extensive regions, and does not correspond to the exact binding site residues. To address this issue, more high-quality data with precise annotation need to be incorporated, and new sequence-based deep representation learning may be also of help for better encoding and decoding the structural information. For example, a new representation scheme recently proposed by <ref type="bibr" target="#b0">Alley et al. (2019)</ref> has demonstrated efficiency improvement for studying protein sequences.</p><p>Overall, there is still a long way for chemogenomics-based CPI to go, and we hope that this work can raise our attention to problems of chemogenomics-based CPI modeling and provide useful guidance for further study. In addition, experiment design plays an import role in deep learning, and more efforts should be directed toward evaluating what a deep learning model has really learned. In this way, not only the new deep learning approaches but also new validation strategies and experiment designs should be emphasized in future development of deep learning.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this work, a transformer architecture with self-attention mechanism was modified to address sequence-based CPI classification task, resulting in a model named TransformerCPI showing high performance on three benchmark datasets. Intriguingly, we compared it with previous reported CPI models and conventional machine learning-based control models, and noticed that most of these models yielded impressive results on those benchmark tests. Given the challenging nature of CPI prediction, we argue that these models might face potential pitfalls of deep learning. To address these potential risks, we constructed new datasets specific for chemogenomics-based CPI task, and designed more rigorous label reversal experiments as new measurements for chemogenomicsbased CPI modeling. Compared with other models, TransformerCPI achieved significantly improved performance on the new experiments, suggesting it can learn desired interaction features and decrease the risk of hidden ligand bias. Finally, model interpretation capability was studied through mapping attention weights to protein sequences and compound atoms, which could help us determine whether a prediction is reliable and having physical significance. Overall, TransformerCPI provides access to model interpretation and contributes chemical biology studies with useful guidance for further ligand structural optimization.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Common pitfalls analysis. (A) Weight distribution plot of CPI-GNN model. Blue line depicts the weight distribution of CNN blocks used to extract protein features. Orange line depicts the weight distribution of GNN blocks used to extract compound features. (B) Violin plots of AUROC for two CPI-GNN models, one utilizing both protein and ligand information, and the other using ligand information only. The white dots are the average AUROCs. The upper and lower end points of the black segments are the first and the third quartile, respectively. The P-value of the t test is shown above the violin. Each model was evaluated on 10 different trials, and the P-value was 0.067</figDesc><graphic url="image-1.png" coords="2,318.10,350.53,228.44,297.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Computational graph of TransformerCPI</figDesc><graphic url="image-2.png" coords="4,64.97,433.30,230.40,288.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. (A) Schematic illustration of label reversal experiment, where a ligand in the training set appears only in one class of samples (either positive or negative interaction CPI pairs), and it appears only in the opposite class of samples in the test set. (B) Data distribution of two datasets. The red line represents where number of positive interactions equals to number of negative interactions. (C) Results of TransformerCPI, CPI-GNN, GraphDTA, GCN and TransformerCPI-ablation on GPCR valid set, Kinase valid set, GPCR test set and Kinase test set</figDesc><graphic url="image-3.png" coords="5,318.10,139.47,228.44,380.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Attention weights of atoms in different compounds. The atoms, which have high attention scores extracted from TransformerCPI, are highlighted in red. (A) A certain ligand shows different attention score distributions when interacting with Histamine H1 receptor and 5-HT 2C receptor, respectively. (B) The SAR of phenothiazine and its attention scores</figDesc><graphic url="image-4.png" coords="7,64.97,347.53,230.40,202.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Attention weights of protein sequences. The regions in proteins, which have high attention weights extracted from TransformerCPI, are highlighted in purple. (A) Attention weight of histamine H1 receptor (PDB: 3RZE). (B) Attention weight of 5-HT 1B receptor (PDB: 4IAQ). (C)Attention weight of MAPK8 (PDB: 1UKI)</figDesc><graphic url="image-5.png" coords="7,190.89,628.44,230.46,74.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>List of compound atom features</figDesc><table><row><cell>Atom type</cell><cell>C, N, O, F, P, S, Cl, Br, I, other (one hot)</cell></row><row><cell>Degree of atom</cell><cell>0, 1, 2, 3, 4, 5, 6 (one hot)</cell></row><row><cell>Formal charge</cell><cell>0 or 1</cell></row><row><cell>Number of radical electrons</cell><cell>0 or 1</cell></row><row><cell>Hybridization type</cell><cell>sp, sp2, sp3, sp3d, sp3d2, other (one hot)</cell></row><row><cell>Aromatic</cell><cell>0 or 1</cell></row><row><cell>Number of hydrogen</cell><cell>0, 1, 2, 3, 4 (one hot)</cell></row><row><cell>atoms attached</cell><cell></cell></row><row><cell>Chirality</cell><cell>0(False) or 1(True)</cell></row><row><cell>Configuration</cell><cell>R, S (one hot)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Hyperparameters of TransformerCPI</figDesc><table><row><cell>Name</cell><cell>Value</cell></row><row><cell>Number of encoder layers</cell><cell>3</cell></row><row><cell>Number of decoder layers</cell><cell>3</cell></row><row><cell>Dimension of atom representation</cell><cell>64</cell></row><row><cell>Number of attention heads</cell><cell>8</cell></row><row><cell>FFN inner hidden size</cell><cell>512</cell></row><row><cell>Hidden size</cell><cell>64</cell></row><row><cell>Patch size</cell><cell>7</cell></row><row><cell>Learning rate</cell><cell>1e-4</cell></row><row><cell>Weight decay</cell><cell>1e-4</cell></row><row><cell>Batch size</cell><cell>8</cell></row><row><cell>Dropout</cell><cell>0.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Summary of the datasets</figDesc><table><row><cell></cell><cell cols="5">Proteins Compounds Interactions Positive Negative</cell></row><row><cell>GPCR</cell><cell>356</cell><cell>5359</cell><cell>15 343</cell><cell>7989</cell><cell>7354</cell></row><row><cell>Kinase</cell><cell>229</cell><cell>1644</cell><cell>111 237</cell><cell>23 190</cell><cell>88 047</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Comparison results of the proposed model and baselines on human dataset</figDesc><table><row><cell>Method</cell><cell>AUC</cell><cell>Precision</cell><cell>Recall</cell></row><row><cell>KNN</cell><cell>0.860</cell><cell>0.927</cell><cell>0.798</cell></row><row><cell>RF</cell><cell>0.940</cell><cell>0.897</cell><cell>0.861</cell></row><row><cell>L2</cell><cell>0.911</cell><cell>0.913</cell><cell>0.867</cell></row><row><cell>SVM</cell><cell>0.910</cell><cell>0.966</cell><cell>0.969</cell></row><row><cell>GraphDTA</cell><cell cols="3">0.960 6 0.005 0.882 6 0.040 0.912 6 0.040</cell></row><row><cell>GCN</cell><cell cols="3">0.956 6 0.004 0.862 6 0.006 0.928 6 0.010</cell></row><row><cell>CPI-GNN</cell><cell>0.970</cell><cell>0.918</cell><cell>0.923</cell></row><row><cell cols="4">DrugVQA (VQA-seq) a 0.964 6 0.005 0.897 6 0.004 0.948 6 0.003</cell></row><row><cell>TransformerCPI</cell><cell cols="3">0.973 6 0.002 0.916 6 0.006 0.925 6 0.006</cell></row><row><cell>a</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Comparison results of the proposed model and baselines on C.elegans dataset</figDesc><table><row><cell>Method</cell><cell>AUC</cell><cell>Precision</cell><cell>Recall</cell><cell></cell><cell></cell><cell></cell></row><row><cell>KNN RF</cell><cell>0.858 0.902</cell><cell>0.801 0.821</cell><cell>0.827 0.844</cell><cell cols="3">Table 6. Comparison results of the proposed model and baselines on BindingDB dataset</cell></row><row><cell>L2</cell><cell>0.892</cell><cell>0.890</cell><cell>0.877</cell><cell>Method</cell><cell>AUC</cell><cell>PRC</cell></row><row><cell>SVM</cell><cell>0.894</cell><cell>0.785</cell><cell>0.818</cell><cell></cell><cell></cell><cell></cell></row><row><cell>GraphDTA</cell><cell>0.974 6 0.004</cell><cell>0.927 6 0.015</cell><cell>0.912 6 0.023</cell><cell>GraphDTA</cell><cell>0.929</cell><cell>0.917</cell></row><row><cell>GCN</cell><cell>0.975 6 0.004</cell><cell>0.921 6 0.008</cell><cell>0.927 6 0.006</cell><cell>GCN</cell><cell>0.927</cell><cell>0.913</cell></row><row><cell>CPI-GNN</cell><cell>0.978</cell><cell>0.938</cell><cell>0.929</cell><cell>CPI-GNN</cell><cell>0.603</cell><cell>0.543</cell></row><row><cell>TransformerCPI</cell><cell>0.988 6 0.002</cell><cell>0.952 6 0.006</cell><cell>0.953 6 0.005</cell><cell>TransformerCPI</cell><cell>0.951</cell><cell>0.949</cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Funding</head><p>This work was supported by the National Natural Science Foundation of China (81773634 to M.Z.), National Science &amp; Technology Major Project 'Key New Drug Creation and Manufacturing Program', China (Number: 2018ZX09711002 to H.J.) and 'Personalized Medicines-Molecular Signaturebased Drug Discovery and Development', Strategic Priority Research Program of the Chinese Academy of Sciences (XDA12050201 to M.Z.).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Conflict of Interest: none declared.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unified rational protein engineering with sequence-based deep representation learning</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Alley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1315" to="1322" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Supervised prediction of drug-target interactions using bipartite local models</title>
		<author>
			<persName><forename type="first">K</forename><surname>Bleakley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yamanishi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="2397" to="2403" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Chemogenomics: an emerging strategy for rapid target and drug discovery</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bredel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Jacoby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Rev. Genet</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="262" to="275" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">GLASS: a comprehensive database for experimentally validated GPCR-ligand associations</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">K B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="3035" to="3042" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hidden bias in the DUD-E dataset leads to misleading performance of deep learning in structure-based virtual screening</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS One</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">e0220113</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Prediction of chemical-protein interactions: multitarget-QSAR versus computational chemogenomic methods</title>
		<author>
			<persName><forename type="first">F</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mol. Biosyst</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="2373" to="2384" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2978" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Language modeling with gated convolutional networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="933" to="941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Interpretable drug target prediction using deep neural representation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence</title>
				<meeting>the Twenty-Seventh International Joint Conference on Artificial Intelligence<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3371" to="3377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">BindingDB in 2015: a public database for medicinal chemistry, computational chemistry and systems pharmacology</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Gilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="D1045" to="1053" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Predicting drug-target interactions from chemical and genomic kernels using Bayesian matrix factorization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2304" to="2310" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">SuperTarget and Matador: resources for exploring drug-target relationships</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gunther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="D919" to="D922" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note>Database issue</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">CGBVS-DNN: prediction of compound-protein interactions based on deep learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hamanaka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mol. Inform</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1" to="2" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">SimBoost: a read-across approach for predicting drug-target binding affinities using gradient boosting machines</title>
		<author>
			<persName><forename type="first">T</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Cheminform</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2149" to="2156" />
			<date type="published" when="2008">2017. 2008</date>
		</imprint>
	</monogr>
	<note>Bioinformatics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">DeepAffinity: interpretable deep learning of compound-protein affinity through unified recurrent and convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Karimi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="3329" to="3338" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Distributed Representations for Biological Sequence Analysis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kimothi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.05949</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note>In, arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note>In, arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Continuous distributed representation of biological sequences for deep proteomics and genomics</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">H</forename><surname>Kobeissy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS One</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">e0141287</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">DeepConv-DTI: prediction of drug-target interactions via deep learning with convolution on protein sequences</title>
		<author>
			<persName><forename type="first">I</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Comput. Biol</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">e1007129</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improving compound-protein interaction prediction by building up highly credible negative samples</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="221" to="229" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">On the variance of the adaptive learning rate and beyond</title>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03265</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note>In, arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">BindingDB: a web-accessible database of experimentally determined protein-ligand binding affinities</title>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="D198" to="D201" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Predicting protein binding affinity with word embeddings and recurrent neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Mazzaferro</surname></persName>
		</author>
		<idno type="DOI">10.1101/128223</idno>
		<ptr target="http://dx.doi.org/10.1101/128223" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013">2013a. 2013</date>
		</imprint>
	</monogr>
	<note>In, arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
			<date type="published" when="2013">2013b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Directory of useful decoys, enhanced (DUD-E): better ligands and decoys for better benchmarking</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Mysinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Med. Chem</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="6582" to="6594" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">GraphDTA: prediction of drug-target binding affinity using graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<idno type="DOI">10.1101/684662</idno>
		<ptr target="http://dx.doi.org/10.1101/684662" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">DeepDTA: deep drug-target binding affinity prediction</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ozturk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="821" to="829" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">WideDTA: prediction of drug-target binding affinity</title>
		<author>
			<persName><forename type="first">H</forename><surname>O ¨ztu ¨rk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.04166</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note>In, arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Pre-trained Models for Natural Language Processing: A Survey</title>
		<author>
			<persName><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.08271</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note>In, arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Three pitfalls to avoid in machine learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Riley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">572</biblScope>
			<biblScope unit="page" from="27" to="29" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Molecular Transformer: A Model for Uncertainty-Calibrated Chemical Reaction Prediction</title>
		<author>
			<persName><forename type="first">P</forename><surname>Schwaller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACS Central Science</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1572" to="1583" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">In need of bias control: evaluating chemical data for machine learning in structure-based virtual screening</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sieg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Chem. Inf. Model</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="947" to="961" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">STITCH 5: augmenting protein-chemical interaction networks with tissue and affinity data</title>
		<author>
			<persName><forename type="first">D</forename><surname>Szklarczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="D380" to="D384" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Making sense of large-scale kinase inhibitor bioactivity data sets: a comparative and integrative analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Chem. Inf. Model</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="735" to="743" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Boosting compound-protein interaction prediction by deep learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Methods</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page" from="64" to="72" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Compound-protein interaction prediction with end-to-end learning of neural networks for graphs and sequences</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tsubaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="309" to="318" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Applications of machine learning in drug discovery and development</title>
		<author>
			<persName><forename type="first">J</forename><surname>Vamathevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Rev. Drug Discov</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="463" to="477" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Gaussian interaction profile kernels for predicting drug-target interaction</title>
		<author>
			<persName><forename type="first">T</forename><surname>Van Laarhoven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3036" to="3043" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Attention Is All You Need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<meeting><address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Deep learning with feature embedding for compound-protein interaction prediction</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zeng</surname></persName>
		</author>
		<idno type="DOI">10.1101/086033</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">DeepCPI: a deep learning-based framework for large-scale in silico drug screening</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genomics Proteomics Bioinf</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="478" to="495" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Computational screening for active compounds targeting protein sequences: methodology and experimental validation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Chem. Inf. Model</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="2821" to="2828" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Predicting drug-target interactions using restricted Boltzmann machines</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="126" to="134" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">DrugBank: a knowledgebase for drugs, drug actions and drug targets</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Wishart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="D901" to="D906" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Prediction of drug-target interaction networks from the integration of chemical and genomic spaces</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yamanishi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="232" to="240" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learned protein embeddings for machine learning</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="2642" to="2648" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">XLNet: Generalized Autoregressive Pretraining for Language Understanding</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019</title>
		<title level="s">Long Papers</title>
		<meeting>the 57th Conference of the Association for Computational Linguistics, ACL 2019<address><addrLine>Florence, Italy; Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-07-28">2019. July 28-August 2, 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2978" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Lookahead optimizer: k steps forward, 1 step back</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.08610</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note>In, arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Predicting drug-protein interaction using quasi-visual question answering system</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="134" to="140" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
