<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exploring Instruction Fusion Opportunities in General Purpose Processors</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sawan</forename><surname>Singh</surname></persName>
							<email>singh.sawan@um.es</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Engineering Department</orgName>
								<orgName type="institution">University of Murcia</orgName>
								<address>
									<settlement>Murcia</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Arthur</forename><surname>Perais</surname></persName>
							<email>arthur.perais@univ-grenoble-alpes.fr</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Univ. Grenoble Alpes</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Grenoble INP</orgName>
								<orgName type="institution" key="instit4">TIMA</orgName>
								<address>
									<settlement>Grenoble</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alexandra</forename><surname>Jimborean</surname></persName>
							<email>alexandra.jimborean@um.es</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Engineering Department</orgName>
								<orgName type="institution">University of Murcia</orgName>
								<address>
									<settlement>Murcia</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alberto</forename><surname>Ros</surname></persName>
							<email>aros@ditec.um.es</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Engineering Department</orgName>
								<orgName type="institution">University of Murcia</orgName>
								<address>
									<settlement>Murcia</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Exploring Instruction Fusion Opportunities in General Purpose Processors</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>general purpose</term>
					<term>microarchitecture</term>
					<term>instruction fusion</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Complex Instruction Set Computer (CISC) paradigm has led to the introduction of instruction cracking in which an architectural instruction is divided into multiple microarchitectural instructions (µ-ops). However, the dual concept, instruction fusion is also prevalent in modern microarchitectures to maximize resource utilization. In essence, some architectural instructions are too complex to be executed as a unit, so they should be cracked, while others are too simple to waste resources on executing them as a unit, so they should be fused with others.</p><p>In this paper, we focus on instruction fusion and explore opportunities for fusing additional instructions in a highperformance general purpose pipeline. We show that enabling fusion for common RISC-V idioms improves performance by 7%. Then, we determine experimentally that enabling fusion only for memory instructions achieves 86% of the potential of fusion in this particular case. Finally, we propose the Helios microarchitecture, able to fuse non-consecutive and noncontiguous memory instructions, and discuss microarchitectural changes required to do so efficiently while preserving correctness. Helios allows to fuse an additional 5.5% of dynamic instructions, yielding a 14.2% performance uplift over no fusion (8.2% over baseline fusion).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Instruction fusion is a well-known microarchitectural technique used in many commercially available processors <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b11">[12]</ref>. Fusion is used to better exploit available hardware resources by leveraging the fact that instructions do not always require all the resources the internal instruction format allows them to claim (e.g. physical destination registers).</p><p>A first example of fusion is architectural fusion. For instance, the load pair architectural instruction of Armv8 loads a chunk of contiguous memory data and writes half of it in its first destination register and the other half in its second destination register ( <ref type="bibr" target="#b1">[2]</ref>, Section C3.2.3). Other ISAs such as x86 or RISC-V do not feature this instruction and must rely on regular load instructions (load "single"). In this case, performing the work done by a single Armv8 load pair requires two distinct architectural instructions who each occupies space in the binary and resources in the pipeline. Armv8 is therefore advantaged in this case. Generally, some ISAs already provide "fused" architectural instructions for specific operations when others rely on multiple architectural instructions to implement the operation. A typical example is loading or storing using indirect addressing, which is two instructions in RISC-V but a single one in Armv8 and x86.</p><p>Unfortunately, it is not always trivial or even desirable to add "fused" architectural instructions. Notwithstanding the fact that it introduces redundancy (e.g. two load "single" vs. one load pair achieve the same goal), it also implies that all compliant designs must support it, increasing design and validation efforts. Therefore, one could rather keep architectural instructions simple and perform any fusion necessary to optimize pipeline resource utilization at the microarchitectural level. This is the guideline followed -to the extreme-by the RISC-V ISA <ref type="bibr" target="#b6">[7]</ref>.</p><p>Moreover, microarchitectural fusion allows for more aggressive optimizations. For instance, load pair in Armv8 requires that the data be exactly contiguous in memory. However, one could imagine fusing two non-contiguous memory accesses as long as they fall within a certain memory region, e.g., a cacheline, or even fuse non-consecutive and/or asymmetric (different width) memory accesses <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b28">[29]</ref>. Consequently, this work focuses on microarchitectural fusion.</p><p>More generally, after retrieving instructions from instruction memory, many modern general purpose microarchitectures will translate architectural instructions into one or more microarchitectural operations -a.k.a. "µ-ops"-through a hardware process called instruction cracking. After cracking, all operations in flight in the pipeline are µ-ops. Cracking therefore re-arranges one complex architectural instruction into multiple µ-ops that are simple enough for the hardware to handle efficiently, while its dual, instruction fusion, rearranges multiple µ-ops into one that is just complex enough for the hardware to handle efficiently. Fusion has the potential to improve performance by decreasing latency as well as saving pipeline resources such as Reorder Buffer (ROB), Scheduler (a.k.a. Instruction Queue, or IQ) and Load/Store Queue (LQ/SQ) entries.</p><p>This work places itself in the context of the RISC-V ISA to highlight the benefits of fusion for an ISA whose primary feature is simplicity. Indeed, many RISC-V architectural instructions do not express enough work given modern hardware capabilities, therefore, fusion is a solution to achieve higher "work per µ-op" <ref type="bibr" target="#b6">[7]</ref>. However, the proposed microarchitectural techniques are by no means limited to the RISC-V ISA. Specifically, this work makes the following contributions:</p><p>• Provide an overview of the different categories of fusion and highlight their limitations (Section II). • Characterize opportunities for non consecutive and non contiguous memory fusion and highlight that focusing on memory µ-ops provides the best return on investment for the processor model we consider (Section III). • Propose the Helios microarchitecture and thoroughly discuss the challenges for providing a correct and efficient execution in the presence of non-consecutive fusion (Section IV). Our experiments with the Helios microarchitecture show that aggressive microarchitectural memory fusion improves performance by 14.2% over no fusion and 8.2% over consecutive and contiguous only microarchitectural memory fusion, on average (geomean).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Definitions and Taxonomy</head><p>To the best of our knowledge, commercially available fusion proposals and implementations focus on consecutive and contiguous fusion <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b11">[12]</ref>. ConSecutive Fusion (CSF) is the operation of fusing two (or more) µ-ops that are consecutive in the dynamic execution stream of the program. ConTiguous Fusion (CTF) is the operation of fusing two (or more) memory µ-ops that are guaranteed to access contiguous but non-overlapping memory bytes <ref type="bibr" target="#b6">[7]</ref>.</p><p>In this paper, we propose and study techniques to increase the number of fused memory instructions, notably nonconsecutive and non-contiguous fusion. Non-ConSecutive Fusion (NCSF) is the operation of fusing two (or more) µ-ops that are not consecutive in the dynamic execution stream of the program. Non-ConTiguous Fusion (NCTF) is the operation of fusing two (or more) memory µ-ops that access non-contiguous memory bytes.</p><p>In addition to the different fusion categories, this paper borrows from nuclear fusion taxonomy to differentiate between a fused µ-op and a simple µ-op that is fused with another one to create a fused µ-op. The head nucleus is the oldest µ-op (in program order) used to create a fused µ-op. The tail nucleus is the youngest µ-op used to create a fused µ-op. As we consider only 2-µop fusion in this paper, a fused µ-op is therefore always created from the head nucleus and the tail nucleus. In the context of non-consecutive fusion, we refer to the µ-ops that are "in between" (in program order) the head nucleus and the tail nucleus as the catalyst. This taxonomy is exemplified for a non-consecutive but contiguous load pair fused µ-op (ldp) in Figure <ref type="figure">1</ref>. In this case, the head and tail nucleii each access particular case, the catalyst facilitates the fusion process since there exists no register or memory dependencies between the nucleii and the catalyst. However, as we will show in Section IV-B, this is not always the case, which is likely why Kim and Lipasti <ref type="bibr" target="#b16">[17]</ref> considered fusing non consecutive memory accesses but only for contiguous memory accesses and only with specific catalysts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Baseline Microarchitecture for Fusion</head><p>Implementing µ-op fusion requires feeding the relevant decoded fields of pairs of instructions to combinatorial logic. In most cases, the opcode and architectural source and destination registers are sufficient. For instance, given the example of Figure <ref type="figure">1</ref>, fusing the two ld instructions can be determined through the following formula:</p><formula xml:id="formula_0">f use(op 0 , op 1 ) = (op 0 == ld) ^(op 1 == ld)( breg 0 == breg 1 ) ^(mem size 0 == mem size 1 )( |imm 0 imm 1 | == mem size 0 )</formula><p>The substitution of regular µ-ops by their fused equivalent, and any instruction collapsing within the decode group caused by the disappearance of one or more µ-ops has to take place before Rename. In effect, fusion is the replacement of the older µ-op (head nucleus) by a fused µ-op and the disappearance of the younger µ-op (tail nucleus) from the pipeline. This process takes place within a fusion window, which can for instance be a decode group in a superscalar processor. In this case, it is not possible for two back-to-back µ-ops that are not in the same decode group to be fused. To do so, a queue may be added between Decode and Rename.</p><p>Two of the proposed fusion idioms for RISC-V (although not limited to RISC-V) are load pair and store pair, where two consecutive loads (resp. stores) to contiguous memory are fused into a single load pair (resp. store pair) µ-op <ref type="bibr" target="#b6">[7]</ref>. Assuming the microarchitecture can handle µ-ops with two destination registers in the load execution pipeline, a load pair µ-op can then perform a single cache access to retrieve two registers worth of data, which reduces latency. Interestingly, at the microarchitectural level, we can even fuse non-contiguous memory operations if the cache circuit allows it. That is, two loads may be fused even if they access non-contiguous data as long as that data fits within a specific memory region (e.g., cacheline).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cacheline Crossers:</head><p>A key limitation of baseline fusion is that it can only inspect static information. As a result, while a load pair idiom can easily be identified, it cannot be guaranteed that both accesses fall within the same cache line. Indeed, consider ld x4, 0(x1) immediately followed by ld x5, 8(x1). These two instructions form a load pair idiom, however, there is no guarantee that a single cacheline can provide all the requested data. As a result, one has to consider that the fusion of memory µ-ops may not necessarily improve latency, as one load pair µ-op may translate to two serialized cache accesses. <ref type="foot" target="#foot_0">1</ref> Fortunately, hardware to handle this case is already present in modern pipelines since even a single memory µ-op may access two consecutive cache lines. The penalty of crossing a cache line is generally small in modern microarchitectures, e.g., a single cycle in Amd processors ([1], Section 2.6.2), suggesting two serialized accesses to the cache).</p><p>This further entails that for load pair fusion to behave optimally, the two destination registers should be provided to dependents independently.</p><p>Dependent loads: Consider instruction ld x1, 0(x1) immediately followed by ld x5, 0(x1). At first glance, the two loads appear to qualify for fusion since they use the same base register. However, the second load actually depends on the first through x1. Thus, the two loads cannot compute their effective address and access the cache concurrently and they cannot be fused.</p><p>Store-to-load forwarding (STLDF): This technique is implemented in modern microarchitectures to i) allow a load to obtain data produced by a store that is still in flight and ii) detect when a younger load was issued before an older store to the same address. STLDF already has to handle accesses that cross cachelines or even pages. This work assumes that all LQ/SQ entries contain the address of the first byte they access as well as a max access size bit-vector informing which bytes are being accessed. Given this design, finding a match requires subtracting the two base addresses, shifting one of the bit-vector by the relevant amount, and finally OR'ing and AND'ing the bit-vectors to determine overlap and full match respectively. We note that several LQ/SQ designs are possible, some of which would remove the need for shifting at the cost of larger bitvectors. However, the particular details of the LQ/SQ design are out of the scope of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. MOTIVATION</head><p>Exploring additional microarchitectural fusion opportunities is motivated by several observations. First, the rise of the RISC-V ISA which features very simple, and therefore very fuseable instructions. Second, the fact that architectural fusion (e.g. the load pair instruction in Armv8 <ref type="bibr" target="#b1">[2]</ref>) is limited in what it can fuse. Third, the fact that currently implemented microarchitectural fusion remains, to the best of our knowledge, conservative, by only considering consecutive µ-ops.</p><p>A. The Case of RISC-V RISC-V favors simple and uniform instructions that adhere strictly to the RISC "Two Sources One Destination" (2S1D) paradigm <ref type="bibr" target="#b29">[30]</ref>. This philosophy explains the absence of indirect addressing in RISC-V, even for loads. Other typical missing idioms include pre-and post-increment addressing which are natively present in Armv8 ( <ref type="bibr" target="#b1">[2]</ref>, Section C1.3.3) but require multiple instructions in RISC-V. As a result, RISC-V is an interesting vessel to revisit instruction fusion since it is bound to have noticeable impact on the performance of a RISC-V microarchitecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. A focus on Memory Pairing Fusion Idioms</head><p>The fusion idioms introduced by Celio et al. <ref type="bibr" target="#b6">[7]</ref> for RISC-V are summarized in Table <ref type="table">I</ref>. One observation we make in this work is that memory pairing idioms (in bold in Table <ref type="table">I</ref>) are more common than other idioms, and furthermore, they also provide larger performance benefits as they not only reduce IQ/ROB pressure but also LQ and SQ pressure.</p><p>This first observation is illustrated in Figure <ref type="figure">2</ref>. The Figure shows the percentage of fused pairs divided into Memory, i.e., those pairs in bold in Table <ref type="table">I</ref> and Others, i.e., the other pairs in Table <ref type="table">I</ref>, for the applications evaluated in this work (see Section V-A for details). On average 5.6% of the dynamic µ-ops belong to the Memory category while 1.1% belong to Others. Exceptions are 657.xz 2, bitcount, and susan where non memory fusion is prevalent.</p><p>Figure <ref type="figure" target="#fig_2">3</ref> reports the corresponding IPC normalized to a baseline without fusion. We can observe that differences between fusing all µ-ops and just fusing memory µ-ops are minimal (1 percentage point on average). Indeed, only susan shows a significant performance degradation (6.5 percentage points) when only memory pairs are considered compared to all idioms. Our initial findings motivate the need to explore additional fusion opportunities of memory µ-ops.     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Architectural vs. Microarchitectural Fusion of Memory Operations</head><p>Armv8 features the load pair (ldp) and store pair (stp) architectural instructions ( <ref type="bibr" target="#b1">[2]</ref>, Section C3.2.3). One could therefore argue that those instructions should be added to RISC-V (or x86) as extensions to let the compiler perform fusion and simplify the microarchitecture. However, architectural pairing is limited in that i) it requires both accesses to be exactly contiguous in memory and ii) it requires both accesses to have the same size. Hence, architectural fusion misses on accesses that have overlapping bytes, are asymmetric, or are close accesses but do not have overlapping bytes.</p><p>Conversely, at the microarchitectural level, the cache circuit access granularity may be greater than 8B and may be as high as the whole cache line (e.g., to support full width AVX512 <ref type="bibr" target="#b12">[13]</ref>). Performing wider than strictly necessary accesses is a known technique to improve cache bandwidth <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b30">[31]</ref>, but previous work has so far considered it to avoid accessing the data cache multiple times, rather than to reduce pressure on pipeline structures.</p><p>Thus, memory µ-ops may be fused in the microarchitecture as long as they access data that falls within a cache access granularity region. This requirement is satisfied by accesses with overlapping bytes, asymmetric accesses, and accesses that do not have overlapping bytes but are close enough.</p><p>Figure <ref type="figure">4</ref> reports the contribution of different consecutive memory fusion categories: contiguous, overlapping, same cacheline and two contiguous cachelines, assuming the cache access granularity is 64 bytes. Interestingly, very few pairs access overlapping bytes, hinting that architectural fusion would capture most consecutive and contiguous load pair idioms, at the cost of increasing ISA complexity. Nonetheless, architectural fusion would still leave potential on the table since 1% additional memory µ-ops could fuse with the adjacent memory µ-op if non-contiguous fusion within a 64B region were supported (SameLine + NextLine).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Limitations of Microarchitectural Fusion</head><p>Consecutivity: Fusion is usually thought of as a technique that will fuse two (or more) µ-ops that are consecutive in the dynamic instruction stream. However, we find that if this constraint were to be relaxed, a non-negligible amount of additional µ-ops could be fused. Figure <ref type="figure">5</ref> reports the additional fusion potential brought by non-consecutive fusion (NCSF) for memory µ-ops. Note that in NCSF, the number of asymetric accesses is quite high, at 12.1% of the NCSF pairs. Conversely, the vast majority of CSF pairs are symmetric accesses.</p><p>Static Information Only: Fusion hardware relies on static information to decide whether to fuse two memory µ-ops or not. In the context of RISC-V, memory µ-ops may be fused if : i) They are either all loads, or all stores and ii) They share the same physical base register and iii) They access data that resides within a cacheline size region.</p><p>However, not all fuseable µ-op pairs meeting condition iii) also validate condition ii). Indeed, we find there exist pairs of both consecutive and non-consecutive memory µ-ops that do not share a physical base register and yet access the same cacheline sized memory region, meaning that they could be fused. Yet, it is not easy or even possible to identify those pairs using only static information, as effective addresses are needed to confirm fuseability. Therefore, fusion based on static information is leaving potential on the table.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. THE HELIOS MICROARCHITECTURE</head><p>To support non-consecutive (NCS) and non-contiguous (NCT) fusion of memory µ-ops that potentially use a Different Base Register (DBR), Helios relies on multiple changes along the pipeline, which are summarized in Figure <ref type="figure">6</ref>.</p><p>At a high level, Helios relies on three key mechanisms. First, one way to perform NCSF is to have each µ-op inspect all older µ-ops in the Allocation Queue, which sits between Decode and Rename. Such an exhaustive approach to NCSF is costly. Therefore, Helios uses a predictive approach to identify not only NCS but also NCT and DBR candidates. Second, NCS fusion may suffer from the presence of dependencies between nucleii and their catalyst. Helios identifies dependencies and, when possible, addresses them so that fusion can still proceed. Third and last, Helios handles incorrectly fused instructions as well as other mispredictions (e.g., branch) within a catalyst.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. A Unified Predictor</head><p>Helios implements a unified hardware predictor for NCSF, NCTF, and DBR that, given a µ-op PC,<ref type="foot" target="#foot_2">2</ref> provides the distance, in µ-ops, to the head nucleus to fuse with.</p><p>The predictor infrastructure consist of 2 structures: the Unfused Committed History (UCH) and the Fusion Predictor (FP). UCH lives in Commit stage. It is used to find potential fusion pairs, i.e., memory operations that access the same cache line, to train FP. FP is placed in the frontend (Decode), and predicts which µ-ops should be speculatively fused.</p><p>1) Unfused Committed History (UCH): UCH keeps the cache lines accessed by the last committed memory µ-ops eligible for fusion, i.e., the non-already fused memory µ-ops. It is organized as a cache where each entry contains a valid bit, a 32-bit tag (partial cache line address), and a 7-bit commit number (CN), for a total of 5 bytes per entry. Each entry may also feature replacement information depending on the actual design (LRU is done through the CN in this work). Distinct histories are implemented for loads and for stores.</p><p>For loads, the UCH is organized as a fully-associative cache. In our experiments, we find that the head nucleus can generally be found a few loads ahead, and therefore we implement a 6-entry UCH for loads with LRU replacement. For stores, a single entry holding the last unfused committed store is kept in the UCH as in Helios, stores cannot be fused across other stores to prevent memory consistency issues. At Commit, loads search the UCH for loads (Ld-UCH) and stores search the UCH for stores (St-UCH). Overall, the UCH structure requires just 280 bits.</p><p>When a retiring µ-op matches a tag in the UCH, a potential fuseable pair has been found. The distance between the two µ-ops is computed by subtracting the CN of the committing µ-op to the matching entry's CN, and the matching entry is invalidated, as µ-ops can only fuse with a single other µ-op. The maximum distance that we allow for fusion is 64 µ-ops, so the CN field requires 7 bits (the last bit controls the CN overflow). The FP is then updated using the computed distance as explained in the next Section.</p><p>On a miss in the UCH, the µ-op is inserted into the UCH. Invalid entries resulting from a previous match are preferred victims for replacement, followed by the LRU entry. It should 2) Fusion Predictor (FP): FP contains information about potential tail nucleii. FP is organized similarly to a cache, each entry containing an 8-bit tag, a 6-bit µ-op distance to the head nucleus to fuse with, a 2-bit saturating counter, and a pseudo-LRU bit. Each entry therefore requires 17 bits.</p><p>The processor attempts to allocate an FP entry for a committing µ-op when a match is found with an older UCH entry. If a match is found, FP is searched and if the tag is already present in FP and the distance matches, the confidence counter of the entry is increased. If the distance does not match, the new distance is inserted and the confidence is set to 1. If the tag is not found, an entry is selected for eviction following a pseudo-LRU replacement policy.</p><p>In this work, we chose a tournament predictor <ref type="bibr" target="#b14">[15]</ref>, which selects from a "local" PC-based predictor and a "global" gshare-like predictor, to implement FP. It includes a 512set, 4-way structure indexed by the PC and another 512set, 4-way structure indexed by a XOR of the PC and the global branch direction history. Each structure therefore features 2048 entries, amounting to 34Kbits each. A 2048set direct-mapped and untagged selection table containing 2-bit saturating counters (4Kbits) is used to select which prediction is used. The total predictor bitcount is therefore 72Kbits (9KB). Alternatively, other predictors, such as TAGEbased <ref type="bibr" target="#b26">[27]</ref> or local history based <ref type="bibr" target="#b31">[32]</ref>, can be employed. In the context of RISC-V, which features aligned instructions, the predictor structures may be implemented as multiple single-ported banks interleaved on PC. In practice, a number of banks greater than the decode width is preferable to handle cases where µ-ops belonging to different basic blocks are at Decode. A high number of banks also permits to perform both predictions and updates in the same cycle if they go to different banks, as described by Seznec et al. <ref type="bibr" target="#b25">[26]</ref>.</p><p>Once a distance is retrieved from the FP at Decode, fusion is attempted in the Allocation Queue, and is successful only if the following conditions are met:</p><p>1) The saturating counter has the maximum value (3).</p><p>2) The two µ-ops form a valid fusion idiom, that is:</p><p>• Both µ-ops are loads or both are stores.</p><p>• The head nucleus is not already a fused µ-op.</p><p>3) The head nucleus still resides in the Allocation Queue or is in the same Decode Group as the tail nucleus. The 2-bit saturating counter is updated when the fused µ-op executes by computing the target addresses, and a misprediction is uncovered. On a correct prediction, the entry is not updated since the confidence counter has already saturated from the UCH-based training process. Updates are achieved through a dedicated structure that contains relevant prediction information (e.g., index of tables used for prediction, predicted distance, confidence) for µ-ops that flow down the pipeline, similarly to how branch or value prediction update may be handled <ref type="bibr" target="#b23">[24]</ref>. While its exact size depends on implementation details (e.g. how many entries are sufficient to prevent stalling), each entry requires 29-bit of storage given the predictor we consider (assuming selector and PC-based set indexes can be regenerated from the PC at update time). In our experiments, we consider an unlimited queue. The confidence counter is reset to 0 on a fusion misprediction.</p><p>FP can be integrated in a microarchitecture featuring a µ-op cache by having FP and the µ-uop cache searched in parallel. Further integration of FP in the µ-op cache appears wasteful because not all µ-ops are eligible for non-consecutive fusion. However, directly caching consecutively fused µ-ops in µ-op cache entries is a possibility, as long as consecutively fused µ-ops contain enough information to be unfused at the output of the cache if a branch jumps to the tail-nucleus. Caching NCSF'd µ-ops appears less synergistic because NCS fusion is inherently dynamic. For instance, depending on control flow, a load may fuse with younger load A or younger load B (e.g. if A is on the taken path and B is on the fallthrough of the same conditional branch). Statically caching one of the two possible NCSF'd µ-ops in the µ-op cache would be unable to capture this behavior. It may however be adapted to constrained NCS fusion schemes that do not allow any control-flow change within the catalyst.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Preserving ISA Semantics</head><p>Helios builds on consecutive memory fusion. Thus, µ-ops already feature three source and two destination registers, as this is required even for consecutive and contiguous fusion of memory operations <ref type="bibr" target="#b6">[7]</ref>. To support DBR, store pair µ-ops may actually need four source registers. Fortunately, we find that DBR store pair fusion represents a negligible fraction of the fused stores (0.54%), as a result, we only support SBR store pair fusion. In the remainder of this paper, the reported storage cost are compared to a baseline with consecutive and contiguous (for memory µ-ops) fusion.</p><p>To correctly fuse non-consecutive µ-ops, NCSF does not remove the tail nucleus from the Allocation Queue (AQ). At fusion time, the head nucleus is replaced by the NCSF'd µ-op, and the tail nucleus is left in the queue. This contrasts with consecutive fusion where the tail nucleus can disappear after fusion. Furthermore, we introduce the following definitions:</p><formula xml:id="formula_1">• A validated NCSF'd µ-op is known to i) Possess</formula><p>its correct source physical register identifiers, ii) Not produce a deadlock through a register dependency or a serializing instruction and iii) Not have a store in its catalyst if it is a store pair µ-op. • A pending NCSF'd µ-op is an NCSF'd µ-op that is not yet validated. 1) Allocation Queue: To track which µ-ops are head or tail nucleii, each AQ entry is augmented with the Is Head Nucleus and Is Tail Nucleus bits. Each entry is also augmented with a tag field, NCS Tag. The tag is actually a pointer to the other nucleus µ-op in the AQ, which is 140 entries in our model, yielding an 8-bit tag. The tags are managed implicitly as µ-ops enter and leave the AQ. head nucleii carry their own AQ entry number until they are dispatched to the ROB/IQ/LQ/SQ, while tail nucleii leaving the AQ carry their respective NCS Tag until they are dispatched. Those changes to the AQ are depicted in 1 of Figure <ref type="figure">7</ref> and amount to 1.37Kbits of storage in the AQ.</p><p>2) Register Renaming &amp; NCSF: For generality, we describe a scheme that can handle nested<ref type="foot" target="#foot_3">3</ref> NCSF. We first augment the Rename stage with two counters: Max Active NCS and Active NCS. The first tracks the number of head nucleii of the current NCSF nest that have entered Rename so far. The second is used to determine when Rename finishes processing the NCSF nest. Both counters are incremented when a head nucleus enters Rename. Only Active NCS can be decremented, when a tail nucleus leaves Rename. However, when Active NCS is decremented to 0, Rename is not processing an NCSF nest anymore, and Max Active NCS is reset. Both counters are also reset on a pipeline flush. We found that supporting only two nested NCSF'd µ-ops at any given time is sufficient to achieve most of the benefits. Any head nucleus entering Rename while Max Active NCS is saturated behaves as unfused and the tail nucleus is marked as not fused in the AQ through the NCS Tag. The additions to the Rename stage are depicted in 2 of Figure <ref type="figure">7</ref> and amount to 4 bits of storage. Moreover, one bit is added to each source and destination physical register identifier flowing in the pipeline to inform whether that physical register belongs to the head nucleus or to the tail nucleus, as shown in 3 of Figure <ref type="figure">7</ref>. This amounts to 700 bits in the AQ, 800 bits in the IQ and 256 bits in the LQ.</p><p>Nevertheless, non-consecutive fusion is problematic if there exists register dependencies between i) The catalyst and the tail nucleus and ii) The head and tail nucleii, either direct or indirect. To illustrate the first class of dependencies, consider the following example where µ-op 3 is fused with µ-op 1:</p><formula xml:id="formula_2">[1] ld x1, 0(x2) [2] add x2, x4, 1 [3] ld x4, 8(x2)</formula><p>Instruction 3 has a Write-after-Read (WaR) dependency with 2 through x4 as well as a Read-after-Write (RaW) dependency through x2. Both dependencies can be circumvented through register renaming.</p><p>Write-after-Read (x4): All the destination registers of the fused µ-op are renamed together, when the fused µ-op (replacing 1) enters rename. This is incorrect because it means that 2 will be able to see the version of x4 produced by 3 in the Register Alias Table (RAT) and use it as a source.</p><p>To prevent 2 from observing the new name of x4, Helios prevents the NCSF'd µ-op from updating the RAT for the destination register(s) of the tail nucleus. Rather, those physical registers are stored in a dedicated buffer. When the corresponding tail nucleus goes through Rename, destination renaming is performed by reading the physical register identifier(s) from the buffer to update the RAT. In this work, a 2-entry buffer (one per NCSF nesting level) is sufficient, each entry storing a physical register identifier. This buffer is tagged using the entry number of the head nucleus, which is the NCS Tag of the tail nucleus. It is shown in 4 of Figure <ref type="figure">7</ref> and amount to 34 bits of storage (additional information is needed to handle deadlocks and discussed later). This scheme enforces in-order destination register renaming, allowing the Active List (which contains the in-flight register mappings) to remain consistent with program order. The buffer is entirely invalidated and physical registers names in valid entries are put back on the Free List on a pipeline flush.</p><p>Read-after-Write (x2): When the NCSF'd µ-op renames its sources, 2 has neither been allocated a new physical register for x2 nor has it updated the RAT. Consequently, the fused µ-op will incorrectly rename the source operand of 3. When the existence of a RaW dependency is eventually determined, it may not be trivial to re-execute the NCSF'd µop as it may already have left the IQ cycles ago. To alleviate this concern, Helios prevents pending NCSF'd µ-op from issuing until the existence or absence of a RaW dependency is determined. This allows the correct source register name to be provided to the IQ entry of the NCSF'd µ-op as it becomes validated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Queue</head><p>To that extent, each IQ entry is first augmented with a NCS Ready bit, which encodes whether an NCSF'd µ-op is pending or validated. A µ-op may issue only if NCS Ready is set. All µ-ops in flight between Rename and the IQ also carry this bit. When an NCSF'd µ-op leaves Rename, its associated NCS Ready bit is reset, while all other µ-ops leave Rename with the bit set. Those additional bits are depicted in 6 of Figure <ref type="figure">7</ref>, and amount to 160 bits of IQ storage.</p><p>Second, each entry of the RAT is augmented with an Inside NCS bit. Inside NCS is set in the RAT entry when a µ-op renames its destination register and Rename is currently processing an NCSF'd µ-op (Active NCS is not 0). All Inside NCS bits are reset when Rename stops processing NCSF'd µ-ops (Active NCS is decremented to 0), or when the pipeline flushes. Those additional bits are depicted in 5 of Figure <ref type="figure">7</ref>, and amount to 32 bits of storage in the RAT. When a tail nucleus enters Rename, it still looks up its source registers in the RAT for the purpose of validating the NCSF'd µ-op. If one of the source has the Inside NCS bit set, there is a RaW dependency between the tail nucleus and the catalyst. This design may yield false positives, however, this is inconsequential in Helios since absence or presence of a RaW incurs the same validation latency.</p><p>To summarize, when an NCSF'd µ-op dispatches to the IQ, it cannot issue because the NCSF Ready bit is not set yet. Eventually, the corresponding tail nucleus leaves Rename having potentially detected a RaW dependency, hence an incorrect NCSF'd µ-op residing in the IQ.</p><p>Regardless, the tail nucleus flows to Dispatch to set the NCSF Ready bit in the corresponding NCSF'd µ-op IQ entry, and, if needed, to correct the source register name(s). To do so, it uses a Dispatch slot to write the IQ entry. The ROB and LQ/SQ do not need to be amended.</p><p>The precise IQ entry to amend is tracked by the Dispatch stage using a small fully associative buffer that has as many entries as the supported nesting NCSF depth (2 in our case), much like the Rename buffer. Entries are tagged with the AQ entry number of the head nucleus (which is the NCS Tag of the tail nucleus) and are allocated when a pending NCSF'd µop dispatches. Entries are reclaimed when the corresponding tail nucleus validates the head nucleus IQ entry, or there is a pipeline flush. The Dispatch buffer is shown in 7 of Figure <ref type="figure">7</ref> and amounts to 64 bits of storage (the buffer also stores pointers to the ROB and LQ/SQ for the purpose of dynamically unfusing a pending NCSF'd µ-op).</p><p>Note that if the two nucleii are in the same rename group, we assume that the Rename stage can take care of any RaW dependency in-place, and the NCSF'd µ-op therefore leaves Rename validated.</p><p>Deadlocks: In Section II-B, we highlighted that standard fusion does not consider consecutive load pair patterns if the tail nucleus directly depends on the head nucleus. This also applies in the context of NCSF, although given the predictive nature of Helios, there is no easy way to determine that two non-consecutive µ-ops are directly dependent in the AQ. Moreover, indirect dependencies also have to be considered. Let us consider the example below where 1 and 3 are assumed to be fused despite having different architectural base registers:</p><formula xml:id="formula_3">[1] ld x1, 0(x2) [2] add x3, x1, 1 [3] ld x4, 0(x3)</formula><p>We cannot issue the fused load pair comprised of 1 and 3 until x2 and x3 are available. However, x3 cannot be produced until x1 is produced by the load pair µ-op, which cannot happen until x3 is produced by 2. In other words, the load pair cannot issue until it executes and execution is therefore deadlocked.</p><p>To detect if the tail nucleus directly or indirectly depends on the head nucleus, we rely on a dedicated detection mechanism that lives in the Renamer: When an NCSF'd µ-op is renamed, it writes a Deadlock Tag field in the RAT entries of its destination register(s). This field is a one-hot vector with as many bits as nesting levels (2 in our case) that encodes the current value of Maximum Active NCS. The Deadlock Tag is propagated from source(s) to destination(s) as younger µ-ops are renamed. If there are multiple source registers, their Deadlock Tag fields are OR'd before being written in the RAT entries of the destination register(s). Similarly, a younger (i.e., nested) head nucleus sets its corresponding bit in the Deadlock Tag of its destination register(s) but also propagates the Deadlock Tag from its sources by OR'ing the two tags. All Deadlock Tag bits are reset when the last tail nucleus of an NCSF nest leaves Rename or when the pipeline flushes. Those additional bits are illustrated in 8 of Figure <ref type="figure">7</ref> and amount to 64 bits of storage in the RAT as well as 4 bits in the Rename Buffer.</p><p>If any of the source registers of a tail nucleus has a Deadlock Tag with the relevant bit set, then there is a dependency-based deadlock and NCSF should not have taken place. The relevant bit to check can be retrieved from the 2-entry Rename buffer used to handle WaR dependencies which is tagged with the NCS Tag of the tail nucleus (hence the 4 additional bits of storage in this buffer). Recovery is done by letting the tail nucleus flow through Dispatch and unfuse the corresponding NCSF'd µ-op by amending the relevant structures (pointer to ROB/IQ/LQ/SQ entries of pending NCSF'd µ-op are kept in the dedicated Dispatch buffer to enable this). In particular, the source and destination registers that belong to the tail nucleus will be marked as invalid in the IQ and LQ, and the tail access offset and size will be amended to 0 in the LQ. The tail nucleus further occupies a second dispatch slot to get its own entries.</p><p>Deadlocks stemming from the presence of serializing instructions (including fences) within the catalyst can be identified by adding a single bit, NCSF Serializing, in the Rename stage which is set if Max Active NCS is at least one and a serializing instruction enters Rename. When a tail nucleus enters Rename and this bit is set, it performs the same actions as when a dependency-based deadlock is identified to "unfuse" the corresponding pending NCSF'd µ-op. This bit is depicted in 9 of Figure <ref type="figure">7</ref>.</p><p>3) Instruction Commit, Exception and Interrupts: To preserve in-order semantics, both nucleii and catalyst must be ready to retire for the NCSF'd µ-op to commit. This ensures that if a misprediction or an exception (also known as fault) is detected in the catalyst, the head-nucleus has not retired yet and it can be unfused or flushed, thereby guaranteeing precise exceptions. This does not mean that the nucleii and catalyst have to commit in a single cycle, however. As a consequence, if an extended commit group has started committing, the group must finish committing before any pending interrupt is processed. In our experiments, the catalyst size is often limited (10.5 µ-ops on average), meaning that the latency increase in processing interrupts will be minor. This is achieved through the Active NCS Rename counter. Specifically, all µ-ops that went through Rename when Active NCS was not 0 are part of an "extended" commit group, which they indicate by setting the Ext ComGroup bit in their ROB entry. To avoid deadlocks, a second bit is added in each ROB entry. This bit set to 1 by the first head nucleus of an NCSF nest, and serves to delineate "extended" commit groups. The Commit stage then leverages those bits to determine if the ROB head can retire. We note that Commit may establish the boundaries of such groups off the critical path by scanning the ROB as µ-ops dispatch. Those bits are depicted in 10 of Figure <ref type="figure">7</ref> and amounts to 704 bits of storage in the ROB.</p><p>4) Memory Consistency &amp; Sequential Semantics: In Helios, NCSF is speculative, therefore, it cannot guarantee that i) There is no store µ-op in the catalyst of a store pair NCSF'd µ-op and ii) If there is such a store µ-op, it cannot guarantee that is does not overlap with the tail nucleus. As a result an NCSF'd store pair µ-op risks violating store-store ordering in models enforcing it and same-address sequentialconsistency for all memory models.</p><p>Helios prevents NCS store pair fusion when finding a store µ-op in the catalyst. This is achieved by adding a NCSF StorePair bit to Rename, which is set when any store µ-op other than the first head nucleus of the NCSF nest is renamed. Any store tail nucleus seeing this bit set will proceed to unfuse the corresponding pending NCSF'd store pair µ-op waiting in the IQ, similarly to the deadlock case. The NCSF StorePair bit is depicted in 11 of Figure <ref type="figure">7</ref>.</p><p>Conversely, NCS load pair fusion may have loads and stores in the catalyst since loads already execute speculatively out-of-order with respect to other loads while respecting load order <ref type="bibr" target="#b8">[9]</ref> and with respect to other stores while respecting sequential semantics <ref type="bibr" target="#b17">[18]</ref>.</p><p>5) Memory µ-ops with Different Base Registers: As pointed out in Section III-D, a non-negligible amount of pairs of loads access data that fit within a cacheline through a different base address register. In the context of a microarchitecture with register renaming, this can be either through a different physical base register or a different architectural base register. In the former case, the frontend can make a reasonable decision by inspecting the nucleii and the catalyst, but this requires inspecting an arbitrary number of µ-ops from the AQ, which is not desirable. Moreover, in the other case (different architectural registers), the frontend cannot generally determine that two µ-ops are candidates for pairing with static information only. Fortunately, DBR load pair fusion is captured by the predictive scheme employed by Helios.</p><p>6) Load/Store Queue: NCSF by itself does not impact the baseline LQ/SQ designs as introduced in Section II-B. However, although that information is already encoded in the base address and the bitvector, the LQ/SQ entries may also store i) an offset from the base address for the second access, such that the base address of the second access can easily be re-generated if needed (6 bits) and ii) The access size of the second access if different access sizes are considered (2 bits). Given the processor configuration we consider, this amounts to 704 bits as we assume the LQ/SQ entries can already track 64B accesses (e.g., to support wide vector extensions such as AVX512). This storage is reported by 12 of Figure <ref type="figure">7</ref>.</p><p>7) Summary: Supporting NCSF in the different stages of the pipeline requires a total of 4.77Kbits, or 0.60KB, for the processor configuration we consider (see Table <ref type="table">II</ref>). Adding the Fusion predictor yields a grand total of 76.77Kbits (9.60KB).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Repairing Microarchitectural State</head><p>In Helios, several events require the microarchitectural state to be repaired:</p><p>1) Rename: An NCSF'd µ-op has at least one incorrect source name because of a RaW between the catalyst and the tail nucleus. 2) Rename: A dependency-based deadlock is discovered. 3) Rename: A store tail nucleus enters Rename and the NCSF StorePair bit is set. 4) Rename: A tail nucleus enters Rename and the NCSF Serializing bit is set. 5) Execute: The head and tail nucleii could not fuse because they span more than a cacheline size region. 6) Execute: The memory access belonging to the tail nucleus faults. 7) Execute: A mispredicted µ-op (e.g., branch, memory dependency, fault) is discovered in the catalyst. Case 1: The pending NCSF'd µ-op is kept in the IQ, so the tail nucleus can amend it in place using a dispatch slot.</p><p>Cases 2, 3 &amp; 4: The NCSF'd µ-op is unfused in place using the same mechanism as in Case 1 and the fact that we attach one bit to each physical register identifier to inform whether the register belongs to the head or the tail nucleus (Section IV-B1). Unfusing also requires amending the NCSF'd µ-op Load Queue (resp. Store Queue) entry, which is also tracked in the dedicated Dispatch side buffer. In those cases, the tail nucleus also dispatches and therefore occupies two dispatch slots, preventing one younger µ-op to dispatch that cycle as an additional penalty.</p><p>Case 5, 6 &amp; 7: In the two first cases, the NCSF'd µ-op tail nucleus became validated and the tail nucleus has left the pipeline after fused µ-op validation. Therefore, it needs to be refetched after flushing the pipeline. In the last case, the control flow is incorrect, hence a pipeline flush is needed. The only degree of freedom is the flush point. Indeed, we can either i) Unfuse any NCSF'd µ-op whose catalyst contains the mispredicted (resp. faulting) instruction and flush from the mispredicted (resp. faulting) instruction or ii) Flush from the oldest NCSF'd µ-op whose catalyst contains the mispredicted (resp. faulting) instruction.</p><p>Since we only consider at most two nested/interleaved NCSF'd µ-op, any mispredicted (resp. faulting) instruction may need to unfuse at most two NCSF'd µ-op (only one for cases 5 &amp; 6 since the mispredicted/faulting µ-op is one of the two nested NCSF'd µ-ops). As a result, in this paper, we consider solution i) by having all µ-ops that can potentially trigger a pipeline flush keep two pointers to their associated "encompassing" NCSF'd µ-ops. The pointers are obtained at Rename by first grabbing the NCS Tag of valid entries in the Rename buffer used to handle WaR and then using this tag to retrieve a ROB entry number or sequence number from the Dispatch buffer used to handle RaW dependencies of NCSF'd µ-ops. The pointers are then stored in a FIFO queue that stores the information needed to recover from pipeline flushes (e.g., PC). Note that since most µ-ops fuse with a µ-op that is close by, the impact of flushing more than necessary (solution ii)) will remain limited. However, even solution ii) requires µ-ops that can trigger a pipeline flush to be able to determine the older NCSF'd µ-ops whose catalyst they belong to (e.g., through a pointer). As pointed out in Section IV-B3, a mispredicted (resp. faulting) µ-op will always find relevant head-nucleii in the ROB and be able to unfuse them through the pointers, which guarantees precise exceptions and correct branch misprediction recovery.</p><p>An upper bound cost for solution i) is therefore two 9-bit ROB pointers per ROB entry, <ref type="foot" target="#foot_4">4</ref> amounting to 6336 bits and increasing the total cost of supporting NCS fusion in Helios to around 83Kbits (around 10.4KB).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EVALUATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Simulation Methodology</head><p>Our simulation infrastructure employs a modified version of the RISC-V Spike Simulator <ref type="bibr" target="#b20">[21]</ref> and an in-house cyclelevel simulator modeling a seven-stage pipeline as described by González et al. <ref type="bibr" target="#b9">[10]</ref>. Spike runs in full-system mode with a Linux kernel and injects instructions into our outof-order processor model, modeled after an Intel Icelake microarchitecture. A first insight obtained from evaluating Helios is that using such a deep machine with equally wide frontend (Fetch/Decode/Rename) stages prevents the Allocation Queue from getting filled and greatly limits fusion opportunities. As a result, Helios features 8-wide Fetch and </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Out-of-order processor</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Intel Icelake Predictors L-TAGE <ref type="bibr" target="#b24">[25]</ref>, Store-set <ref type="bibr">[</ref> Decode stages to ensure that the Allocation Queue gets filled even in high IPC workloads. Our model implements a Total-Store-Order consistency model, thereby being compliant with the RISC-V TSO extension (Ztso). The high level characteristics of the system used in our simulations are displayed in Table <ref type="table">II</ref>. We evaluate Helios with the SPEC CPU 2017 <ref type="bibr" target="#b27">[28]</ref> <ref type="foot" target="#foot_5">5</ref> and MiBench <ref type="bibr" target="#b10">[11]</ref> benchmark suites. We skip the Linux kernel boot and setup for all the applications. Then, SPEC applications skip an additional 10B instructions and report results for the next 500M instructions. MiBench applications run until completion. SPEC workloads run using reference inputs while MiBench workloads use the large input set. The binaries were compiled with GCC 10.2.0 targeting the RV64G ISA with flags -O3 -static.</p><p>We consider five configurations. RISCVFusion fuses µops using the non-bold idioms of Table <ref type="table">I</ref> (i.e., no memory pairs), as suggested by Celio et al. <ref type="bibr" target="#b6">[7]</ref>. CSF-SBR fuses only consecutive memory instruction that access contiguous data through the same base register, but may be asymmetric (different access sizes). RISCVFusion++ fuses all instructions from Table <ref type="table">I</ref>. Helios implements a predictor and machinery to fuse consecutive as well as non-consecutive memory pairs. Finally, OracleFusion is an upper bound configuration that fuses all eligible memory pairs as well as non memory pairs from Table <ref type="table">I</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Results</head><p>1) Fusion pairs: Figure <ref type="figure" target="#fig_6">8</ref> shows the percentage of CSF and NCSF in Helios and OracleFusion with respect to total dynamic memory instructions. On average, the total number of fused pairs in Helios is close to the upper limit of OracleFusion. Helios delivers 6.7% CSF and 5.5% NCSF pairs. It is interesting to note that the number of CSF in Helios is slightly higher when compared to OracleFusion which fuses only 6.1%. The reason is that OracleFusion will immediately be able to fuse distant µ-ops the first time it encounters them. Conversely, Helios has a training phase during which it may fuse two consecutive µ-ops, the head nucleus of which is actually the tail nucleus of the NCSF'd µ-op identified by OracleFusion. Since CSF µ-ops are not inserted in the UCH, Helios will "favor" CSF over NCSF in this case. In our experiments, the distance between head nucleus and tail nucleus averages at 10.5 dynamic instructions (amean), suggesting that currently implemented decode width are not sufficient to perform "brute force" NCSF at Decode.</p><p>2) Helios Fusion predictor: Table <ref type="table" target="#tab_8">III</ref> summarizes the performance of the fusion predictor. Coverage includes only the pairs that need predictions: NCSF and CSF load pairs that use different base registers.</p><p>On average, the predictor is able to correctly fuse 68.2% of the eligible dynamic memory µ-ops. The relative increase in CSF compared to OracleFusion is one of the factors for missing coverage as it results in fewer available µ-ops for NCSF/CSF-DBR. A related reason is the delay caused by the training phase of the fusion predictor itself. Overall, Helios fuses 12.2% of the dynamic µ-ops, approaching the 13.6% of OracleFusion. Prediction accuracy in a deeply pipelined processor is crucial, especially since the misprediction penalty may be higher than the expected gains of a correct fusion prediction. Through tagging and confidence estimation, the Helios fusion predictor provides high accuracy: 99.7% on average. 641.leela has the lowest accuracy among all the application, at 97.7%. Note that higher accuracy may always be traded for lower coverage using better confidence estimation e.g., probabilistic counters <ref type="bibr" target="#b19">[20]</ref>. Average mispredictions per kilo instructions (MPKI) in Helios stands at 0.1416.</p><p>3) Processor Stalls &amp; IPC: Figure <ref type="figure">9</ref> shows the percentage of Rename and Dispatch structural stalls with respect to the total execution cycles for baseline, Helios and OracleFusion. Figure <ref type="figure">10</ref> reports the IPC for all configurations.</p><p>In applications that encounter many dispatch stalls in the baseline, fusion generally provides high IPC gains, for example applications (e.g., 600. perlbench 1 &amp; 2, 602.gcc, 657.xz 1, rsynth). In 657.xz 1, Dispatch spends 88% of the execution cycles waiting for an SQ entry. Thanks to Helios, 657.xz 1 gets a high IPC improvement of 70% due to 27.6% additional NCSF pairs (Figure <ref type="figure" target="#fig_6">8</ref>). Application 602.gcc (resp. rijndael, typeset) also suffer from many SQ stalls in the baseline, a significant part of which are avoided in Helios, yielding an IPC improvement of 14.8% (resp. 11.94%, 20.6%).</p><p>Nonetheless, applications that do not have significant stalls in the baseline still benefit from Helios on the IPC front due to reduced execution latency of paired loads and the potential doubling of the load/store throughput (e.g., most MiBench workloads). One interesting case that does not follow this trend is 605.mcf, who suffers few stalls in the baseline and features a reasonable number of fused memory pairs in Helios. Yet, a fusion MPKI slightly higher than average results in an overall IPC degradation of 1%.</p><p>On average Helios provide an IPC improvement of 14.2% over a baseline without fusion, and 8.2% over only fusing consecutive and contiguous memory pairs that use the same base register (CSF-SBR). Helios achieves most of the benefits of OracleFusion which stands at 16.3% improvement. Part of the gap could undoubtedly be bridged by tuning the fusion predictor or considering other algorithms <ref type="bibr" target="#b26">[27]</ref>. Other configurations discussed in the paper achieve 0.8% (RISCVFusion), 6% (CSF-SBR), and 7% (RISCVFusion++) IPC improvement, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. RELATED WORK</head><p>Coalescing memory operations. Wilson et al. suggest to leverage the access FIFO in front of the L1D in some designs to serve multiple loads in one go by reading the whole cacheline and letting entries of the FIFO match against returning data <ref type="bibr" target="#b30">[31]</ref>. Rivers et al. follow a similar -in spiritpath in the pursuit of efficient data cache multiporting, in which each bank of the data cache is augmented with a single highly ported line buffer from which multiple loads can be served each cycle <ref type="bibr" target="#b21">[22]</ref>. Similarly, Baoni et al. introduce Fat loads <ref type="bibr" target="#b5">[6]</ref> in which an initial fat load brings a full cacheline in a dedicated buffer to allow subsequent loads to that cacheline to be served from this fast buffer. Fat loads is a very effective technique to reduce pressure on the L1D and DTLB while also speeding up performance since buffers can be accessed faster than the L1D.</p><p>Other techniques to coalesce memory accesses include to always read double words (8 bytes) from the data cache, even for smaller accesses, and store the result in a load-load forwarding capable Load-Store Queue <ref type="bibr" target="#b13">[14]</ref>, keeping retired loads and stores alive longer than needed to create more loadstore and load-load forwarding opportunities <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b18">[19]</ref>, and coalescing non-consecutive stores after they commit while guaranteeing total store order <ref type="bibr" target="#b22">[23]</ref>.</p><p>These techniques are oblivious to consecutivity, dependencies, and whether the accesses use different base or offset registers. In addition, they do not require augmenting the µ-ops with additional source and/or destination register fields. Although they may coalesce more than two accesses into a single one, all coalesced µ-ops are still treated as distinct entities in the pipeline, preventing them from providing savings in the ROB/IQ/LQ/SQ. Conversely, microarchitectural fusion and especially Helios reduces occupancy in the pipeline structures while also coalescing two accesses into a single one. Nevertheless, Helios and such prior proposals are quasiorthogonal in the sense that if the Helios prediction scheme were built to fuse only accesses that fall within the same cacheline, then fused µ-ops would be able to leverage Fat Loads line buffers <ref type="bibr" target="#b5">[6]</ref>.</p><p>Fusion. Kim and Lipasti introduce macro-op scheduling for the purpose of simplifying the IQ logic <ref type="bibr" target="#b15">[16]</ref>. In this work, candidate pairs of µ-ops are scheduled as a unit to relax the wakeup &amp; select loop since the IQ has to schedule half as often if it schedules "fused" pairs of µ-ops. While macro-ops occupy a single slot in the IQ, thereby saving capacity, they do not execute faster.</p><p>Celio et al. <ref type="bibr" target="#b6">[7]</ref> make the case that fusion can be used to improve "work per µ-op" in RISC-V designs, rather than adding more powerful -yet common-instructions to the ISA, and discuss a number of potential idioms amenable to fusion. In this case, fusion is restricted to consecutive and contiguous (for memory) instructions.</p><p>The works closest to this paper are Kim and Lipasti <ref type="bibr" target="#b16">[17]</ref> and Thakker et al. <ref type="bibr" target="#b28">[29]</ref>. Both refer to non-consecutive fusion of memory instructions. The key differences with our proposal is that they remain conservative in the content of the catalyst. Kim and Lipasti only handle a catalyst made up of ALU instructions and do not handle non-consecutive store combining. Thakker et al. is more aggressive but still does not fuse in the presence of a RaW or WaR with the catalyst, contrarily to Helios. Conservative catalyst content limits the potential of non-consecutive fusion (18.89% of the NCSF'd µ-ops have a RaW or WaR in the catalyst, 82.25% have a µ-op that may cause a pipeline flush). Moreover, our approach uses a predictive scheme and can therefore scale to large Allocation Queues, whereas the two proposed mechanism appear to rely on combinatorial logic to identify candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>This work demonstrates that there exists significant potential for fusing non-consecutive memory instructions in RISC-V binaries and introduces the Helios microarchitecture to leverage this potential. Helios relies on a predictive scheme to fuse distant µ-ops and tackles numerous challenges to guarantee correct execution. Helios achieves a significant performance uplift over microarchitectures supporting various flavours of fusion, notably 14.2% over no fusion and 8.2% over consecutive and contiguous only memory fusion. While Helios was introduced in the context of the RISC-V ISA, we expect that similar potential exists in general purpose programs regardless of ISA, and Helios could therefore benefit other widely available processor families (e.g., x86, Arvm8).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 . 3 6</head><label>23</label><figDesc>Figure 2. Percentage of fused µ-ops considering all or just memory fusion idioms, relative to total dynamic µ-ops.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Normalized IPC improvement of fusion pairs presented in Figure 2 with respect to no fusion as baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Figure 5 also reports the fusion potential brought by fusing instructions that do not share the same physical base register (DBR suffix in the Figure), and amount to 1.5% of dynamic µ-ops on average, including CSF and NCSF.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 . 3 6Figure 5 .</head><label>435</label><figDesc>Figure 4. Paired consecutive memory µ-ops relative to total dynamic µ-ops.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Number of CSF and NCSF pairs in Helios and OracleFusion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 . 3 6 7 Figure 10 .</head><label>93710</label><figDesc>Figure 9. Percentage of processor stalls with respect to total execution cycles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>8 bytes from [x2] and [x2 + 8] respectively, and the tail nucleus does not depend on the head nucleus. Hence, they can be fused in a single load pair µ-op that accesses 16 bytes from [x2]. Note that in thisFigure 1. Non-consecutive fusion of the head nucleus and tail nucleus over the catalyst region.</figDesc><table><row><cell>head nucleus</cell><cell>ld x1, 0(x2) add x7, x8, x5</cell><cell></cell></row><row><cell>catalyst</cell><cell>sub x12, x7, x11</cell><cell>ldp x1, x3, 0(x2)</cell></row><row><cell>tail nucleus</cell><cell>mv x15, x8 ld x3, 8(x2)</cell><cell>NCSF'd CTF'd load-pair µ-op</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>noted that this process is out of the execution critical path and can be done post-commit. That is, a post-commit decoupling queue in which at most n committing loads (resp. store) µ-ops are inserted each cycle can be implemented. If the queue is full, µ-ops are simply dropped and will get a chance to train at a later time. The queue is drained at a rate of m µ-op each cycle, with m the number of UCH ports. In our experiments, on average 0.23 loads update and 0.28 loads search the UCH per cycle at commit (0.13 and 0.16 per cycle for stores). Experiments further suggest that implementing an 8-entry queue in front of the load UCH and allowing a single UCH search and update per cycle has no impact on the performance of Helios.</figDesc><table><row><cell>Fusion Pred.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>UCH</cell></row><row><cell>Decode</cell><cell>Alloc Queue</cell><cell>Rename</cell><cell>Dispatch</cell><cell>...</cell><cell>Execute</cell><cell>...</cell><cell>Commit</cell></row><row><cell>-Fuse consecutive µ-ops -Access fusion predictor</cell><cell>-Mark NCSF/ NCTF predicted µ-ops as fused</cell><cell>-Discover reg. dependencies. between catalyst and nucleii -Discover fusion-(e.g., deadlocks) preventing cases</cell><cell>-Repair incorrect dependencies in IQ -Unfuse incorrectly NCSF/NCTF µ-ops</cell><cell></cell><cell>-Discover address-based NCSF misp. -Update fusion predictor</cell><cell></cell><cell>-Train the fusion predictor -Track fusion-based commit groups</cell></row><row><cell></cell><cell cols="5">Figure 6. Overview of fusion-related responsibilities in Helios.</cell><cell></cell><cell></cell></row></table><note>be</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table III HELIOS</head><label>III</label><figDesc>FUSION PREDICTOR'S COVERAGE, ACCURACY AND MPKI FOR ALL THE APPLICATIONS USED IN THIS PAPER.</figDesc><table><row><cell>Benchmark</cell><cell cols="2">Coverage Accuracy</cell><cell>MPKI</cell></row><row><cell>600.perlbench s 1</cell><cell>76.56</cell><cell>99.96</cell><cell>0.0183</cell></row><row><cell>600.perlbench s 2</cell><cell>75.30</cell><cell>99.90</cell><cell>0.0592</cell></row><row><cell>600.perlbench s 3</cell><cell>71.99</cell><cell>99.97</cell><cell>0.0170</cell></row><row><cell>602.gcc s 1</cell><cell>63.32</cell><cell>99.51</cell><cell>0.1776</cell></row><row><cell>602.gcc s 2</cell><cell>62.73</cell><cell>99.52</cell><cell>0.1709</cell></row><row><cell>602.gcc s 3</cell><cell>63.43</cell><cell>99.52</cell><cell>0.1722</cell></row><row><cell>605.mcf s</cell><cell>62.24</cell><cell>98.60</cell><cell>0.8350</cell></row><row><cell>620.omnetpp s</cell><cell>67.86</cell><cell>99.40</cell><cell>0.3018</cell></row><row><cell>623.xalancbmk s</cell><cell>82.94</cell><cell>99.97</cell><cell>0.0269</cell></row><row><cell>631.deepsjeng s</cell><cell>58.82</cell><cell>98.68</cell><cell>0.4602</cell></row><row><cell>641.leela s</cell><cell>62.15</cell><cell>97.74</cell><cell>0.8172</cell></row><row><cell>648.exchange2 s</cell><cell>50.04</cell><cell>99.56</cell><cell>0.1595</cell></row><row><cell>657.xz s 1</cell><cell>99.99</cell><cell>99.99</cell><cell>0.0000</cell></row><row><cell>657.xz s 2</cell><cell>73.90</cell><cell>99.90</cell><cell>0.0207</cell></row><row><cell>adpcm</cell><cell>58.90</cell><cell>99.99</cell><cell>0.0005</cell></row><row><cell>basicmath</cell><cell>61.68</cell><cell>99.99</cell><cell>0.0013</cell></row><row><cell>bitcount</cell><cell>74.54</cell><cell>99.56</cell><cell>0.1083</cell></row><row><cell>blowfish</cell><cell>48.00</cell><cell>99.89</cell><cell>0.0254</cell></row><row><cell>crc32</cell><cell>66.49</cell><cell>99.99</cell><cell>0.0046</cell></row><row><cell>dijkstra</cell><cell>85.64</cell><cell>99.99</cell><cell>0.0058</cell></row><row><cell>fft</cell><cell>57.05</cell><cell>99.93</cell><cell>0.0210</cell></row><row><cell>gsm toast</cell><cell>65.34</cell><cell>99.51</cell><cell>0.3765</cell></row><row><cell>gsm untoast</cell><cell>67.89</cell><cell>99.99</cell><cell>0.0010</cell></row><row><cell>jpeg</cell><cell>72.74</cell><cell>99.99</cell><cell>0.0061</cell></row><row><cell>patricia</cell><cell>62.80</cell><cell>99.99</cell><cell>0.0036</cell></row><row><cell>qsort</cell><cell>66.97</cell><cell>99.77</cell><cell>0.0965</cell></row><row><cell>rijndael</cell><cell>62.22</cell><cell>99.85</cell><cell>0.0471</cell></row><row><cell>rsynth</cell><cell>64.23</cell><cell>99.99</cell><cell>0.0047</cell></row><row><cell>sha</cell><cell>69.22</cell><cell>99.99</cell><cell>0.0023</cell></row><row><cell>stringsearch</cell><cell>67.76</cell><cell>99.97</cell><cell>0.0115</cell></row><row><cell>susan</cell><cell>91.36</cell><cell>99.99</cell><cell>0.0010</cell></row><row><cell>typeset</cell><cell>69.26</cell><cell>99.17</cell><cell>0.5758</cell></row><row><cell>Average</cell><cell>68.23</cell><cell>99.68</cell><cell>0.1416</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">This is also the case for "architectural" fusion when the compiler transforms two regular Armv8 ldr instructions into one Armv8 ldp instruction.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_1">0 0 . p e r l b e n c h _ 1 6 0 0 . p e r l b e n c h _ 2 6 0 0 . p e r l b e n c h _ 3</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2">In this paper, RISC-V memory instructions always translate to a single µ-op, hence the PC to µ-op equivalence for memory µ-ops.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3">In this paper, "nested" includes interleaved pairs, e.g., µ-ops ld 0 ld 1 ld 2 ld 3 where ld 0 fuses with ld 2 and ld 1 fuses with ld 3 .</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4">A dedicated queue smaller than the ROB may be implemented to track this information for potential flushers only.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_5">Due to a known Spike limitation at the time of the study (https://github.com/riscv-collab/riscv-gcc/issues/175), we were unable to run application</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="625" xml:id="foot_6">.x264 s in full system mode.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ACKNOWLEDGMENT This work was supported by the European Research Council under the European Union's Horizon 2020 research and innovation programme (grant agreement No 819134), the Ramón y Cajal Research Contract (RYC2018-025200-I), and the Vicerrectorado de Investigación e Internacionalización of the University of Murcia under the Talento 2021 programme.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Software Optimization Guide for AMD EPYC™ 7003 Processors</title>
		<author>
			<persName><forename type="first">Advanced</forename><surname>Micro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devices</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2022-04">Apr.-2022</date>
			<biblScope unit="volume">56665</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">ARM Architecture Reference Manual ARMv8-A</title>
		<author>
			<persName><forename type="first">Risc</forename><surname>Advanced</surname></persName>
		</author>
		<author>
			<persName><surname>Machines</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022-04">Apr.-2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Arm ® Cortex™-A77 Core Software Optimization Guide</title>
		<author>
			<persName><forename type="first">Risc</forename><surname>Advanced</surname></persName>
		</author>
		<author>
			<persName><surname>Machines</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022-04">Apr.-2022</date>
		</imprint>
	</monogr>
	<note>Issue 3,&quot; p. 68 Section 4.13</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Arm ® Neoverse™-N2 Core Software Optimization Guide</title>
		<author>
			<persName><forename type="first">Risc</forename><surname>Advanced</surname></persName>
		</author>
		<author>
			<persName><surname>Machines</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022-04">Apr.-2022</date>
		</imprint>
	</monogr>
	<note>88 Section 4.13</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Filter caching for free: The untapped potential of the store buffer</title>
		<author>
			<persName><forename type="first">R</forename><surname>Alves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Black-Schaffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kaxiras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">46th Int&apos;l Symp. on Computer Architecture (ISCA)</title>
				<imprint>
			<date type="published" when="2019-06">Jun. 2019</date>
			<biblScope unit="page" from="436" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fat loads: Exploiting locality amongst contemporaneous load operations to optimize cache accesses</title>
		<author>
			<persName><forename type="first">V</forename><surname>Baoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Sohi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">54th Int&apos;l Symp. on Microarchitecture (MICRO)</title>
				<imprint>
			<date type="published" when="2021-10">Oct. 2021</date>
			<biblScope unit="page" from="366" to="379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">The renewed case for the reduced instruction set computer: Avoiding isa bloat with macro-op fusion for risc-v</title>
		<author>
			<persName><forename type="first">C</forename><surname>Celio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dabbelt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Asanović</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.02318</idno>
		<imprint>
			<date type="published" when="2016-07">Jul. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Memory dependence prediction using store sets</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">Z</forename><surname>Chrysos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Emer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">25th Int&apos;l Symp. on Computer Architecture (ISCA)</title>
				<imprint>
			<date type="published" when="1998-06">Jun. 1998</date>
			<biblScope unit="page" from="142" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Two techniques to enhance the performance of memory consistency models</title>
		<author>
			<persName><forename type="first">K</forename><surname>Gharachorloo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hennessy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">20th Int&apos;l Conf. on Parallel Processing (ICPP)</title>
				<imprint>
			<date type="published" when="1991-08">Aug. 1991</date>
			<biblScope unit="page" from="355" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Processor Microarchitecture: An Implementation Perspective, ser. Synthesis Lectures on Computer Architecture</title>
		<author>
			<persName><forename type="first">A</forename><surname>González</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Latorre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Magklis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Morgan &amp; Claypool Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mibench: A free, commercially representative embedded benchmark suite</title>
		<author>
			<persName><forename type="first">M</forename><surname>Guthaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ringenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mudge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th Int&apos;l Workshop on Workload Characterization (WWC)</title>
				<imprint>
			<date type="published" when="2001-12">Dec. 2001</date>
			<biblScope unit="page" from="3" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Intel ® 64 and IA-32 Architectures Optimization Reference Manual</title>
		<author>
			<persName><surname>Intel</surname></persName>
		</author>
		<idno>248966-045</idno>
		<imprint>
			<date type="published" when="2022-04">Apr.-2022</date>
			<biblScope unit="page" from="3" to="12" />
		</imprint>
	</monogr>
	<note>Section 3.4.2.2</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Intel ® 64 and IA-32 Architectures Software Developer&apos;s Manual, Pub 325383-076us</title>
		<author>
			<persName><surname>Intel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022-04">Apr.-2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Reducing cache traffic and energy with macro data load</title>
		<author>
			<persName><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 Int&apos;l Symp. on Low Power Electronics and Design (ISLPED)</title>
				<meeting>the 2006 Int&apos;l Symp. on Low Power Electronics and Design (ISLPED)</meeting>
		<imprint>
			<date type="published" when="2006-10">Oct. 2006</date>
			<biblScope unit="page" from="147" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The alpha 21264 microprocessor architecture</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Kessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Mclellan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Webb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings International Conference on Computer Design. VLSI in Computers and Processors (Cat. No. 98CB36273)</title>
				<meeting>International Conference on Computer Design. VLSI in Computers and Processors (Cat. No. 98CB36273)</meeting>
		<imprint>
			<date type="published" when="1998-10">Oct. 1998</date>
			<biblScope unit="page" from="90" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Macro-op scheduling: Relaxing scheduling loop constraints</title>
		<author>
			<persName><forename type="first">I</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lipasti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">36th Int&apos;l Symp. on Microarchitecture (MICRO)</title>
				<imprint>
			<date type="published" when="2003-12">Dec. 2003</date>
			<biblScope unit="page" from="277" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Implementing optimizations at decode time</title>
		<author>
			<persName><forename type="first">I</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Lipasti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">29th Int&apos;l Symp. on Computer Architecture (ISCA)</title>
				<imprint>
			<date type="published" when="2002-05">May 2002</date>
			<biblScope unit="page" from="221" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Streamlining inter-operation memory communication via data dependence prediction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Moshovos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Sohi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">30th Int&apos;l Symp. on Microarchitecture (MICRO)</title>
				<imprint>
			<date type="published" when="1997-12">Dec. 1997</date>
			<biblScope unit="page" from="235" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Reducing data cache energy consumption via cached load/store queue</title>
		<author>
			<persName><forename type="first">D</forename><surname>Nicolaescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Veidenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nicolau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Int&apos;l Symp. on Low Power Electronics and Design (ISLPED)</title>
				<meeting>the 2003 Int&apos;l Symp. on Low Power Electronics and Design (ISLPED)</meeting>
		<imprint>
			<date type="published" when="2003-08">Aug. 2003</date>
			<biblScope unit="page" from="252" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Probabilistic counter updates for predictor hysteresis and stratification</title>
		<author>
			<persName><forename type="first">N</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zilles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th Int&apos;l Symp. on High-Performance Computer Architecture (HPCA)</title>
				<imprint>
			<date type="published" when="2006-02">Feb. 2006</date>
			<biblScope unit="page" from="110" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">;</forename><surname>Risc-V Software</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Risc-V Isa</forename><surname>Spike</surname></persName>
		</author>
		<author>
			<persName><surname>Simulator</surname></persName>
		</author>
		<ptr target="https://github.com/riscv-software-src/riscv-isa-sim" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On high-bandwidth data cache design for multi-issue processors</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Rivers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Tyson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Austin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">30th Int&apos;l Symp. on Microarchitecture (MICRO)</title>
				<imprint>
			<date type="published" when="1997-12">Dec. 1997</date>
			<biblScope unit="page" from="46" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Non-speculative store coalescing in total store order</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kaxiras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">45th Int&apos;l Symp. on Computer Architecture (ISCA)</title>
				<imprint>
			<date type="published" when="2018-06">Jun. 2018</date>
			<biblScope unit="page" from="221" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Journal of Instruction-Level Parallelism (JILP) Special Issue: The Second Championship Branch Prediction Competition (CBP-2)</title>
		<author>
			<persName><forename type="first">A</forename><surname>Seznec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007-12">Dec. 2007</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
	<note>A 256 kbits L-TAGE branch predictor</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The L-TAGE branch predictor</title>
		<author>
			<persName><forename type="first">A</forename><surname>Seznec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Instruction-Level Parallelism</title>
		<imprint>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2007-05">May 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Design tradeoffs for the alpha ev8 conditional branch predictor</title>
		<author>
			<persName><forename type="first">A</forename><surname>Seznec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Felix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sazeides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">29th Int&apos;l Symp. on Computer Architecture (ISCA)</title>
				<imprint>
			<date type="published" when="2002-05">May 2002</date>
			<biblScope unit="page" from="295" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A case for (partially) tagged geometric history length branch prediction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Seznec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Michaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Instruction-Level Parallelism</title>
		<imprint>
			<biblScope unit="page">23</biblScope>
			<date type="published" when="2006-02">Feb. 2006</date>
		</imprint>
	</monogr>
	<note>JILP)</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Standard Performance Evaluation Corporation</title>
		<ptr target="http://www.spec.org/cpu2017" />
	</analytic>
	<monogr>
		<title level="m">SPEC CPU2017</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Combining load or store instructions</title>
		<author>
			<persName><forename type="first">H</forename><surname>Thakker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Speier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Jaget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Dieffenderfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Morrow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Tekmen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Stempel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Garg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020-02">20 200 004 550A1. Feb., 2020</date>
			<pubPlace>U.S. Patent</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Design of the RISC-V instruction set architecture</title>
		<author>
			<persName><forename type="first">A</forename><surname>Waterman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-01">Jan. 2016</date>
		</imprint>
		<respStmt>
			<orgName>University of California, Berkeley</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Increasing cache port efficiency for dynamic superscalar microprocessors</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Olukotun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rosenblum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">23rd Int&apos;l Symp. on Computer Architecture (ISCA)</title>
				<imprint>
			<date type="published" when="1996-05">May 1996</date>
			<biblScope unit="page" from="147" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Two-level adaptive training branch prediction</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">24th Int&apos;l Symp. on Microarchitecture (MICRO)</title>
				<imprint>
			<date type="published" when="1991-11">Nov. 1991</date>
			<biblScope unit="page" from="51" to="61" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
