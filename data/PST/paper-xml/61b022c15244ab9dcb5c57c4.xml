<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">OOD-GNN: Out-of-Distribution Generalized Graph Neural Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-12-07">7 Dec 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Haoyang</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
							<email>wwzhu@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">OOD-GNN: Out-of-Distribution Generalized Graph Neural Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-12-07">7 Dec 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2112.03806v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph neural networks (GNNs) have achieved impressive performance when testing and training graph data come from identical distribution. However, existing GNNs lack out-of-distribution generalization abilities so that their performance substantially degrades when there exist distribution shifts between testing and training graph data. To solve this problem, in this work, we propose an out-of-distribution generalized graph neural network (OOD-GNN) for achieving satisfactory performance on unseen testing graphs that have different distributions with training graphs. Our proposed OOD-GNN employs a novel nonlinear graph representation decorrelation method utilizing random Fourier features, which encourages the model to eliminate the statistical dependence between relevant and irrelevant graph representations through iteratively optimizing the sample graph weights and graph encoder. We further design a global weight estimator to learn weights for training graphs such that variables in graph representations are forced to be independent. The learned weights help the graph encoder to get rid of spurious correlations and, in turn, concentrate more on the true connection between learned discriminative graph representations and their ground-truth labels. We conduct extensive experiments to validate the out-of-distribution generalization abilities on two synthetic and 12 real-world datasets with distribution shifts. The results demonstrate that our proposed OOD-GNN significantly outperforms state-of-the-art baselines.</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graph structured data is ubiquitous in the real world, e.g., biology networks <ref type="bibr" target="#b0">[1]</ref>, social networks <ref type="bibr" target="#b1">[2]</ref>, molecular graphs <ref type="bibr" target="#b2">[3]</ref>, knowledge graphs <ref type="bibr" target="#b3">[4]</ref>, etc. Recently, deep learning models on graphs, especially graph neural networks (GNNs) <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref>, have increasingly emerged as prominent approaches for representation learning of graphs <ref type="bibr" target="#b7">[8]</ref>. Significant methodological advances have been made in the field of GNNs, which have achieved promising performance in a wide variety of applications <ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref>.</p><p>Despite their enormous success, the existing GNN approaches for graph representation learning generally assume that the testing and training graph data are independently sampled from the identical distribution, i.e., the I.I.D. assumption. In many real-world scenarios, however, it is difficult to guarantee this assumption to be valid. In particular, the testing distribution may suffer unobserved or uncontrolled shifts compared with the training distribution. For example, in the field of drug discovery, the prediction of biochemical properties of molecules is commonly trained on limited available experimental data, but the model needs to be tested on an extraordinarily diverse and combinatorially large universe of candidate molecules <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>. The model performance of existing methods can be substantially degraded under distribution shifts due to the lack of out-of-distribution (OOD) generalization ability in realistic data splits <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b2">3]</ref>. Therefore, it is of paramount importance to learn GNNs capable of out-of-distribution generalization and achieve relatively stable performances (c) OGB Molecule Dataset <ref type="bibr" target="#b26">[27]</ref>. For validating OOD generalization, this dataset is split based on the scaffolds (i.e., two-dimensional structural frameworks) of molecules. The testing set consists of structurally distinct molecules with scaffolds that are not in the training set.</p><p>Figure <ref type="figure">1</ref>: Examples of out-of-distribution testing graphs under complex distribution shifts. Figure <ref type="figure">1a</ref> denotes the models are trained on small graphs but tested on larger graphs. Figure <ref type="figure">1b</ref> denotes the models trained with clean node features but tested with noisy features. Figure <ref type="figure">1c</ref> represents a more realistic and challenging case, i.e., distribution shifts exist on both graph structures and node features.</p><p>under distribution shifts, especially for some high-stake applications, e.g., medical diagnosis <ref type="bibr" target="#b15">[16]</ref>, criminal justice <ref type="bibr" target="#b16">[17]</ref>, financial analysis <ref type="bibr" target="#b17">[18]</ref>, and molecular prediction <ref type="bibr" target="#b2">[3]</ref>, etc.</p><p>Some pioneering works <ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref> focus on the size generalization problem by testing on larger graphs than the training graphs. Besides size generalization, the capability of out-of-distribution generalization for GNNs is not explored until recently <ref type="bibr" target="#b21">[22]</ref>. In out-of-distribution scenarios, when there exist complex heterogeneous distribution shifts, the performance of current GNN models can degrade substantially, which is mainly induced by the spurious correlations. The spurious correlations intrinsically come from the subtle correlations between irrelevant representations and relevant representations <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>. For example, in the field of drug discovery (see Figure <ref type="figure">1c</ref>), the GNN models trained on molecules with one group of scaffolds (two-dimensional structural frameworks of molecules) may learn the spurious correlations between the scaffolds and labels (i.e., whether some drug can inhibit HIV replication) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b14">15]</ref>. When tested on molecules with different scaffolds (out-of-distribution testing molecules), the existing GNN models may make incorrect predictions based on the spurious correlations.</p><p>In this paper, we propose to learn decorrelated graph representations through sample reweighting <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref> to eliminate the dependence between irrelevant and relevant representations, which is one of the major causes of degrading model performance under distribution shifts. However, learning decorrelated graph representations to improve out-of-distribution generalization for GNNs is fundamentally different from traditional methods and thus remains largely unexplored and challenging. Specifically, it poses the following challenges.</p><p>• GNNs fuse heterogeneous information from node features and graph structures such that the complex and unobserved non-linear dependencies among representations are much more difficult to be measured and eliminated than the linear cases for decorrelation of non-graph data.</p><p>• Although sample reweighting is effective on small datasets, for real-world large-scale graphs, it is inefficient or even infeasible to consistently learn a global weight for each graph in the dataset due to the high computational complexity and excessive storage consumption.</p><p>To tackle these challenges, we propose a novel out-of-distribution generalized graph neural network (OOD-GNN) capable of handling graph distribution shifts in complex and heterogeneous situations.</p><p>In particular, we first propose to eliminate the statistical dependence between relevant and irrelevant graph representations of the graph encoder by a novel nonlinear graph representation decorrelation method utilizing random Fourier features <ref type="bibr" target="#b27">[28]</ref>, which scales linearly with the sample size and can get rid of unexpected spurious correlations. Next, to reduce computational complexity, we propose a novel scalable global-local weight estimator to learn the sample weight for each graph. The local weights for a mini-batch of graphs and global weights for the entire graphs are optimized jointly to effectively maintain the consistency of weights over the whole graph dataset. Finally, the parameters of the graph encoder and sample weights for graph representation decorrelation are optimized iteratively to learn discriminant graph representations for predictions.</p><p>We conduct extensive experiments on both synthetic graph datasets and well-known real-world graph benchmarks. The experimental results demonstrate that the representations learned from OOD-GNN can achieve substantial performance gains on the graph prediction tasks, including graph classification and regression, under distribution shifts.</p><p>The contributions of this paper are summarized as follows:</p><p>• We propose a novel out-of-distribution generalized graph neural network (OOD-GNN) capable of learning out-of-distribution (OOD) generalized graph representation under complex distribution shifts. • We propose a nonlinear graph representation decorrelation method based on random Fourier features and sample reweighting. The decorrelated graph representations can substantially improve the out-of-distribution generalization ability in various OOD graph prediction benchmarks. • We present a scalable global-local weight estimator to learn graph weights for the whole dataset consistently and efficiently. Extensive empirical results show that OOD-GNN greatly outperforms baselines on various graph prediction benchmarks under distribution shifts.</p><p>We review related works in Section 2. In Section 3, we describe the problem formulation and the details of our proposed OOD-GNN. Section 4 presents the experimental results including quantitative comparisons on both synthetic and real-world datasets, ablation studies, complexity analysis, hyperparameter sensitivity, etc. Finally, we conclude our work in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Graph Neural Network. GNNs <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref> have been attracting considerable attention in recent years because of their notable success in representing graph-structure data. They generally utilize a messagepassing paradigm, which combines node features and graph topology to update node embeddings. To obtain the representation of the entire graph, graph pooling <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30</ref>] is adopted to summarize node embeddings. Many GNNs and their variants <ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref> have been proposed, achieving state-of-the-art performance on various graph tasks, including node classification <ref type="bibr" target="#b4">[5]</ref>, link prediction <ref type="bibr" target="#b34">[35]</ref>, and graph classification <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b35">36]</ref>. Despite their successes, the performance of GNNs drops substantially when there are distribution shifts between training and testing graphs <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b14">15]</ref>. The existing works largely ignore the out-of-distribution generalization ability of GNNs, which is crucial to realistic applications deployed in the wild <ref type="bibr" target="#b26">[27]</ref>.</p><p>Size generalization of GNNs. The main goal of size generalization is to make GNNs work well on testing graphs whose size distribution is different from that of training graphs <ref type="bibr">[37, 19-21, 38, 39]</ref>. In these works, GNNs are trained on relatively small graphs and then generalize to larger graphs with the help of attention mechanisms <ref type="bibr" target="#b18">[19]</ref>, self/semi-supervised learning <ref type="bibr" target="#b19">[20]</ref>, causal modeling <ref type="bibr" target="#b20">[21]</ref>, etc. However, these methods only test on graphs of different sizes and ignore more realistic and challenging settings where the distribution shifts emerge in the graph topologies and node features.</p><p>The expressiveness of GNNs. The Weisfeiler-Lehman graph isomorphism test is most commonly used to measure the expressiveness power of GNNs <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b31">32]</ref>. Assuming appropriate optimization, a more expressive GNN can achieve smaller error on the training data <ref type="bibr" target="#b39">[40]</ref>. Some works <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42]</ref> also study the generalization capability of GNNs over the training distribution. These works are orthogonal to out-of-distribution generalization, including unseen graph topological structures and features studied in this paper. The findings in <ref type="bibr" target="#b21">[22]</ref> show that encoding task-specific non-linearities in the GNN architecture or features can improve the out-of-distribution generalization. However, it is largely unknown in practice that how to enhance the generalization ability of GNNs when there are distribution shifts between training and testing graphs.</p><p>Representation decorrelation. The spurious correlation between the irrelevant (non-critical) representations and labels is recognized as one major cause of model degradation under distribution shifts <ref type="bibr" target="#b42">[43]</ref><ref type="bibr" target="#b43">[44]</ref><ref type="bibr" target="#b44">[45]</ref><ref type="bibr" target="#b45">[46]</ref>. Some pioneering works adopt regularizers to penalize high correlation explicitly <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b42">43]</ref>. However, these methods could introduce a substantial computational overhead, yield marginal improvements, or require extra supervision to control the strength of the penalty. There are also some works learning decorrelated representations with sample reweighting <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b48">[49]</ref><ref type="bibr" target="#b49">[50]</ref><ref type="bibr" target="#b50">[51]</ref>, which is shown effective in improving the generalization ability theoretically <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b52">53]</ref> and empirically <ref type="bibr" target="#b48">[49]</ref>. However, most of these methods are proposed under linear settings. In contrast, GNNs fuse heterogeneous information from node features and graph topological structures so that there exist complex and unobserved non-linear dependencies among representations. The linear sample reweighting methods can not be applied to eliminate non-linear dependencies for the decorrelation of graph data. We also observe a significant performance drop in the experiments if only linear dependencies between representations are eliminated.</p><p>3 Method</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Notations and Problem Formulation</head><p>Let G tr = {G n } N tr n=1 and G te = {G n } N te n=1 be the training and testing graph dataset, which are under distribution shifts, i.e., P(G tr ) = P(G te ). G te is unobserved in the training stage. A graph encoder Φ : G → Z is a mapping from the input graph space G to a d-dimensional representation space Z. In this work, we consider Φ as GNNs. R : Z → Y is a classifier, mapping the representation space Z to the label space Y. G, Z, Y denote sets of random variables in G, Z, Y, respectively. Denote graph representations for G tr as Z ⊂ R N tr ×d . Z n * denotes the representation of the n-th graph and Z * i is the random variable corresponding to the i-th dimension of Z. Graph weights are W = {w n } N tr n=1 , where w n is the weight for the n-th graph G n in G tr and we constrain N tr n=1 w n = N tr . By jointly optimizing the graph encoder Φ, classifier R, and graph weights W, we aim to eliminate the statistical dependence of all dimensions in representation Z such that the predictor R • Φ : G → Y can achieve satisfactory generalization performance when testing on out-of-distribution graphs P(G te ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Statistical Independence with Graph Reweighting</head><p>The correlation between relevant and irrelevant parts in representations is recognized as the main performance obstacle when P(G tr ) = P(G te ), i.e., OOD testing data <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b25">26]</ref>. The relevant parts in representations denote the truly discriminant information to predict ground-truth labels, which are invariant under distribution shifts, e.g., the predictive functional blocks of molecules. On the other hand, the irrelevant parts include non-informative features that could change across different domains, e.g., scaffold structure in predicting molecule functions. GNNs fuse available information from node features and graph topologies into a unified low-dimensional representation for each graph. So it is difficult or even infeasible to distinguish which dimensionality in the representation denotes relevant and irrelevant parts without extra supervision, which is unavailable and expensive to collect. Therefore, we propose to encourage the graph encoder to eliminate the statistical dependence of all dimensions in the graph representation. Formally, we expect</p><formula xml:id="formula_0">Z * i ⊥ ⊥ Z * j , ∀i, j ∈ [1, d], i = j.<label>(1)</label></formula><p>For measuring the independence between continuous random variables Z * i and Z * j in d-dimensional graph representation space Z, it is inapplicable to resort to histogram-based measures unless d is small enough. So we introduce Hilbert-Schmidt Independence Criterion (HSIC) <ref type="bibr" target="#b54">[55]</ref>. Specifically, consider a measurable, positive definite kernel k Z * i on the domain of random variable Z * i . Denote the corresponding Reproducing Kernel Hilbert Spaces (RKHS) by H Z * i . HSIC is defined as</p><formula xml:id="formula_1">HSIC(Z * i , Z * j ) := C Z * i,Z * j<label>2</label></formula><p>HS , where C Z * i,Z * j is the cross-covariance operator in the RKHS of k Z * i and k Z * j . The independence can be determined as follows <ref type="bibr" target="#b55">[56]</ref>.</p><formula xml:id="formula_2">Proposition 1. Assume E[k Z * i (Z * i , Z * i )] &lt; ∞ and E[k Z * j (Z * j , Z * j )] &lt; ∞, and k Z * i k Z * j is a characteristic kernel, then HSIC(Z * i , Z * j ) = 0 ⇔ Z * i ⊥ ⊥ Z * j .<label>(2)</label></formula><p>Although a finite-sample estimate of HSIC has been used in practice for statistical testing <ref type="bibr" target="#b54">[55]</ref>, it is infeasible to be utilized for training the graph encoder Φ on large-scale datasets (e.g., the OGBG-MOLHIV dataset in our experiments contains 41,127 graphs). The bottleneck lies in that the computational cost of HSIC grows as the batch size of training data increases. We therefore consider the squared Frobenius norm C Z * i,Z * j 2 F , an analogue corresponding to the HSIC in Euclidean space<ref type="foot" target="#foot_0">1</ref>  <ref type="bibr" target="#b56">[57]</ref>, where C Z * i,Z * j is the partial cross-covariance matrix defined as:</p><formula xml:id="formula_3">C Z * i,Z * j = 1 N tr −1 N tr n=1 f (Z ni ) − 1 N tr N tr m=1 f (Z mi ) • g(Z nj ) − 1 N tr N tr m=1 g(Z mj ) ,<label>(3)</label></formula><p>where Z ni and Z nj denote the value of random variables Z * i and Z * j given the input graph G n .</p><formula xml:id="formula_4">f (Z * i ) := (f 1 (Z * i ), f 2 (Z * i ), . . . , f Q (Z * i )), g(Z * j ) := (g 1 (Z * j ), g 2 (Z * j ), . . . , g Q (Z * j )),<label>(4)</label></formula><p>with</p><formula xml:id="formula_5">f q (Z * i ), g q (Z * j ) ∈ H RFF , ∀q ∈ [1, Q]. H RFF = {h : x → √ 2cos(wx+φ)|w ∼ N (0, 1)</formula><p>, φ ∼ Uniform(0, 2π)} denotes the random Fourier features function space, from which we select Q functions. In a nutshell, random Fourier feature (RFF) is an effective technique to approximate kernel-based independence test <ref type="bibr" target="#b57">[58]</ref>. Note that as Q grows, the accuracy of independence judgement increases. And Q = 5 is solid enough to measure the independence of random variables in practice <ref type="bibr" target="#b57">[58]</ref>.</p><p>Using the independence criterion above, we elaborate on graph reweighting which encourages the independence of the variables in graph representation. Define the graph weights W = {w n } N tr n=1 where w n ∈ R is the learnable weight for the n-th graph G n in the training set. The graph weights can be directly utilized into Eq. ( <ref type="formula" target="#formula_3">3</ref>), so the partial cross-covariance matrix can be calculated as:</p><formula xml:id="formula_6">C W Z * i,Z * j = 1 N tr −1 N tr n=1 w n f (Z ni ) − 1 N tr N tr m=1 w m f (Z mi ) • w n g(Z nj ) − 1 N tr N tr m=1 w m g(Z mj ) .<label>(5)</label></formula><p>The learnable graph weight W participates in the optimization process to eliminate the dependence between representations to the greatest possible extent by minimizing the squared Frobenius norm of the partial cross-covariance matrix C W Z * i,Z * j 2 F in Eq. ( <ref type="formula" target="#formula_6">5</ref>). For the optimization, we iteratively optimize the graph weights W, graph encoder Φ, and classifier R as follows:</p><formula xml:id="formula_7">Φ * , R * = argmin Φ,R N tr n=1 w n (R • Φ (G n ) , Y n ) ,<label>(6)</label></formula><formula xml:id="formula_8">W * = argmin W 1≤i&lt;j≤d C W Z * i,Z * j 2 F ,<label>(7)</label></formula><p>where denotes the cross-entropy loss for graph classification tasks or mean squared error loss for graph regression tasks. The optimization of graph weights W in Eq. ( <ref type="formula" target="#formula_8">7</ref>) encourages the graph encoder to generate the graph representations Z = Φ(G), where each dimension keeps independent with others and thus eliminates the spurious correlations. The optimization of graph encoder Φ and classifier R in Eq. ( <ref type="formula" target="#formula_7">6</ref>) based on the weighted graph datasets will lead to good performance on the specific prediction tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Global-Local Graph Weight Estimator</head><p>Note that directly optimizing Eqs. (6)(7) requires all N tr graph weights W and graph representations Z to calculate accurately C W Z * i,Z * j . Therefore, we need to load the entire dataset simultaneously for optimization, which is infeasible on large-scale datasets due to the high computational cost and excessive storage consumption. A straightforward alternative is to learn only graph representations and corresponding weights over a mini-batch of data. However, the consistency of the weights cannot be maintained since different mini-batches do not share information. Therefore, the dependence between different graph representation dimensions is hard to eliminate over the whole training dataset.</p><p>To tackle this problem, we propose a novel scalable global-local weight estimator to achieve the balance of optimization efficiency and weight consistency. In essence, we adopt global weights to keep the consistency of the learnable weights over the whole dataset, and the local weights encourage the independence of different dimensions of the graph representations over a mini-batch. Next, we elaborate on the detailed designs.</p><p>Global weights. We maintain K groups of global representations</p><formula xml:id="formula_9">Z (g) = [Z (g1) , • • • , Z (g K ) ] and the corresponding global weights W (g) = [W (g1) , • • • , W (g K ) ]</formula><p>, where the size of each group equals to the mini-batch, i.e., Z (g k ) ∈ R |B|×d and W </p><formula xml:id="formula_10">Z (l) = {Z (l) n * } |B| n=1 , Z<label>(l)</label></formula><p>n * = Φ(G n ) and uniformly initialize the local graph weights, i.e., W (l) = (1, 1, . . . , 1). Then, the local graph representations Z (l) and weights W (l) are concatenated with the K groups of global graph representations Z (g) for optimization. We denote</p><formula xml:id="formula_11">Z = Z (g1) , • • • , Z (g K ) Z (l) ∈ R (K+1)|B|×d , W = W (g1) , • • • , W (g K ) W (l) ∈ R (K+1)|B| ,<label>(8)</label></formula><p>where [• •] is concatenation. Then, we calculate the weighted partial cross-covariance matrix in Eq. ( <ref type="formula" target="#formula_6">5</ref>) using Z, W and optimize the objective function. Using our proposed estimator, the computational cost for each mini-batch is O((K+1)|B|), as opposed to O(N tr ) in directly optimizing Eqs. (6) <ref type="bibr" target="#b6">(7)</ref>.</p><p>Weights Update. At the end of each training iteration, we adopt a momentum update to dynamically update the global representations Z (g) and weights W (g) by the optimized local Z (l) and W (l) :</p><formula xml:id="formula_12">Z (g k ) ← γ k Z (g k ) + (1 − γ k )Z (l) , W (g k ) ← γ k W (g k ) + (1 − γ k )W (l) .<label>(9)</label></formula><p>Here γ k ∈ [0, 1) is a momentum coefficient for each group of global representations Z (g k ) and W (g k ) weights. The global Z (g k ) and W (g k ) with a large γ k serve as a long-term memory for global information over the whole training dataset, while those with a small γ k serve as a short-term memory.</p><p>Finally the global weights can be progressively updated and ensure the consistency of the whole graph dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training Procedure</head><p>The training procedure of our proposed OOD-GNN is shown in Algorithm 1.</p><p>At the training stage, we iteratively optimize the graph weights W, graph encoder Φ, and classifier R. Specifically, as shown in Algorithm 1, we first perform forward propagation for each sampled minibatch B to obtain the local graph representations</p><formula xml:id="formula_13">Z (l) = {Z (l) n * } |B| n=1 , Z<label>(l)</label></formula><p>n * = Φ(G n ) (line 3 in Algorithm 1) and uniformly initialize the local graph weights W (l) = (1, 1, . . . , 1) (line 4). To maintain consistency of the weights and improve efficiency, we concatenate the global representations and weights with local representations and weights to obtain Z and W (line 5). After that, we calculate the partial cross-covariance matrix C W Z * i, Z * j and optimize the graph weights by minimizing the following objective function (line 7): Calculate</p><formula xml:id="formula_14">W (l) * = argmin W (l) 1≤i&lt;j≤d C W Z * i, Z * j 2 F ,<label>(10)</label></formula><formula xml:id="formula_15">Z (l) = {Z (l) n * } |B| n=1 , Z<label>(l)</label></formula><formula xml:id="formula_16">n * = Φ(G n ) 4:</formula><p>Initialize W (l) = (1, 1, . . . , 1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Concatenate global and local representations/weights as Eq. ( <ref type="formula" target="#formula_11">8</ref>)</p><p>6:</p><p>for e ← 1 to Epoch_Reweight do 7:</p><p>Optimize the graph weights by minimizing Eq. ( <ref type="formula" target="#formula_8">7</ref>)</p><p>8:</p><p>end for 9:</p><p>Back propagate with weighted prediction loss as Eq. ( <ref type="formula" target="#formula_7">6</ref>)</p><p>10:</p><p>Update global representations and weights as Eq. ( <ref type="formula" target="#formula_12">9</ref>)</p><p>11:</p><p>end for 12: end for Next, we optimize the graph encoder Φ and classifier R by performing back propagation with weighted prediction loss (line 9):</p><formula xml:id="formula_17">Φ * , R * = argmin Φ,R |B| n=1 w n (R • Φ (G n ) , Y n ) ,<label>(11)</label></formula><p>where</p><formula xml:id="formula_18">w n = W (l) * n</formula><p>is the optimized weight for the n-th graph in the minibatch B. At the end of each iteration, the global representations and weights are updated by the optimized local graph representations and local graph weights (line 10).</p><p>At the testing stage, we directly adopt the optimized graph encoder Φ * and classifier R * to learn graph representations and conduct predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we empirically evaluate the effectiveness of the proposed OOD-GNN on both synthetic and real-world datasets and conduct ablation studies. More experimental results (including hyper-parameter sensitivity, training dynamic, weight distribution, time complexity, etc.) are also present and analyzed in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Baselines</head><p>We compare our OOD-GNN with several representative state-of-the-art methods:</p><p>• GCN <ref type="bibr" target="#b4">[5]</ref>: It is one of the most famous GNNs, following a recursive neighborhood aggregation (or message passing) scheme. • GIN <ref type="bibr" target="#b6">[7]</ref>: It is shown to be one of the most expressive GNNs in representation learning of graphs.</p><p>• GCN-virtual and GIN-virtual <ref type="bibr" target="#b14">[15]</ref>: We also consider the variants of GCN and GIN augmented with virtual node, i.e., adding a node that is connected to all the nodes in the original graphs. • FactorGCN <ref type="bibr" target="#b58">[59]</ref>: It decomposes the input graph into several interpretable factor graphs for graph-level disentangled representations, which is a state-of-the-art disentangled GNN model for graph classification. • PNA <ref type="bibr" target="#b59">[60]</ref>: It takes multiple neighborhood aggregation schemes into account and generalizes several GNN models with different neighborhood aggregation schemes. • TopKPool <ref type="bibr" target="#b60">[61]</ref>: It propagates only part of the input and this part is not uniformly sampled from the input. It can thus select some local parts of the input graph and ignore the rest to summarize the graph representation. • SAGPool <ref type="bibr" target="#b28">[29]</ref>: It is a graph pooling method based on self-attention mechanism, which can be used to calculate attention scores and retain important nodes for graph-level representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Datasets</head><p>To cover more realistic and challenging cases of graph distribution shifts, we compare our method and baselines on both synthetic and real-world datasets:</p><p>• Synthetic Datasets.</p><p>We use two synthetic datasets to evaluate the effectiveness of our proposed method, and examples of these datasets are shown in Figure <ref type="figure">1a and 1b</ref>.</p><p>(1) TRIANGLES. Counting the number of triangles in a graph is a common task that can be solved analytically but is challenging for GNNs. We first generate 4,000 random graphs, and train on graphs containing 4 to 25 nodes, and test on graphs with 4 to 100 nodes. The node features are set as one-hot degrees. The dataset is split into 3,000/500/500 graphs used as training/validation/testing sets. The task is to predict the number of triangles in each graph. The number of classes is 10 (i.e., each graph has 1 to 10 triangles). Based on this setting, there exist distribution shifts with regard to graph sizes between training and testing data.</p><p>(2) MNIST-75SP. Each graph in MNIST-75SP is converted from an image in MNIST <ref type="bibr" target="#b61">[62]</ref> using super-pixels <ref type="bibr" target="#b62">[63]</ref>. We randomly sample 7,000 images of MNIST and extract no more than 75 superpixels for each image to generate the graph. The node features are set as the super-pixel coordinates and intensity. The dataset is split into 6,000/500/500 graphs used as training/validation/testing sets. The task is to classify each graph into the corresponding handwritten digit labeled from 0 to 9. To simulate distribution shifts with respect to graph features, we follow <ref type="bibr" target="#b18">[19]</ref> and generate two testing graph datasets. For the first testing set, Test(noise), we add Gaussian noise, drawn from N (0, 0.4), to node features. For the second testing set, Test(color), we colorize images by adding two more channels and add independent Gaussian noise, drawn from N (0, 0.4), to each channel. The graph structures (adjacency matrices) are not changed for testing graphs. • Real-world Datasets.</p><p>(1) Molecule and social datasets. We consider three commonly used graph classification benchmarks: COLLAB <ref type="bibr" target="#b63">[64]</ref>, PROTEINS <ref type="bibr" target="#b64">[65]</ref>, and D&amp;D <ref type="bibr" target="#b65">[66]</ref>. Following <ref type="bibr" target="#b18">[19]</ref>, these datasets are split based on the size of each graph. D&amp;D 200 and D&amp;D 300 denote the two datasets whose maximum graph size in the training set is 200 and 300, respectively. All the methods are trained on smaller graphs and tested on unseen larger graphs. Specifically, COLLAB is derived from 3 public collaboration datasets, i.e., High Energy Physics, Condensed Matter Physics, and Astro Physics. We train on graphs with 32 to 35 nodes and test on graphs with 32 to 492 nodes. PROTEINS is a protein dataset. We train on graphs with 4 to 25 nodes and test on graphs with 6 to 620 nodes. D&amp;D is also a dataset that consists of proteins. We consider two types of splitting methods, (2) Open Graph Benchmark (OGB) <ref type="bibr" target="#b14">[15]</ref>. We consider 9 graph property prediction datasets from a benchmark of distribution shifts OGBG-MOL * in Open Graph Benchmark (OGB), i.e., TOX21, BACE, BBBP, CLINTOX, SIDER, TOXCAST, HIV, ESOL, FREESOLV. The task is to predict the target molecular properties as accurately as possible. We adopt the default scaffold splitting procedure, namely splitting the graphs based on their two-dimensional structural frameworks. Note that this scaffold splitting strategy aims to separate structurally different molecules into different subsets, which provides a more realistic and challenging scenario of out-of-distribution generalization. Figure <ref type="figure">1c</ref> shows some examples of the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Implementation Details</head><p>We implement our method in PyTorch. The number of epochs (i.e., Epoch in Algorithm 1) is set to 100. The batch size is chosen from {64, 128, 256}. The learning rate is chosen from {0.0001, 0.001}. The number of epochs of learning graph weights (i.e., Epoch_Reweight in Algorithm 1) is set to 20. The dimensionality of the representations and hidden layers d is chosen from {128, 300} for Open Graph Benchmark, and {64, 256} for other datasets. We use GIN <ref type="bibr" target="#b6">[7]</ref> as the graph encoder Φ : G → Z since it is shown to be one of the most expressive GNNs, and the number of layers is chosen from <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6]</ref>. We set Q = 1 to sample random Fourier features. The 2 -norm is adopted on the weights to prevent degenerated solutions. The number of groups of global representations and weights K = 1 with the momentum coefficient γ = 0.9 in the updating step. The classifier R : Z → Y is realized by a two-layer MLP. These hyper-parameters are tuned on the validation set. We report the mean values with standard deviations of 10 repeated experiments.</p><p>We conduct the experiments with the following hardware and software configurations:</p><p>• Operating System: Ubuntu 18.04.1 LTS • CPU: Intel(R) Xeon(R) CPU E5-2699 v4@2.20GHz • GPU: NVIDIA GeForce GTX TITAN X with 12GB of Memory • Software: Python 3.6.5; NumPy 1.18.0; PyTorch 1.7.0; PyTorch Geometric 1.6.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results on Synthetic Graphs</head><p>The results on TRIANGLES and MNIST-75SP are reported in Table <ref type="table" target="#tab_1">2</ref>. On TRIANGLES, there exist distribution shifts on the graph sizes. OOD-GNN consistently achieves the best testing performance compared with other baselines on the out-of-distribution testing graphs, demonstrating the OOD generalization capability of our method. The accuracy of a strong baseline PNA on training graphs is impressive but drops significantly on the OOD testing graphs. FactorGCN, as a disentangled graph representation learning method, decomposes the input graph into several independent factor graphs so that it may change the semantic implication of representations into these implicit factors and affect the performance. In contrast, OOD-GNN learns graph weights so that the semantic of the graph representations will not be affected, leading to better generalization ability.</p><p>On MNIST-75SP, there exist distribution shifts on the graph features, i.e., graphs in the testing datasets have larger noises. OOD-GNN achieves the best performance consistently compared with other methods. For this dataset, each graph consists of super-pixel nodes and edges that are formed based on the spatial distance between super-pixel centers. Therefore, the graph topological structures are relatively more discriminative than node features in making predictions. Traditional GNNs fuse heterogeneous information from both graph topological structures and features into unified graph representations, so these baselines may learn the spurious correlations, leading to poor generalization performance. As the complex non-linear dependencies between graph structures and features are eliminated, our method is able to learn the true connections between relevant representations (i.e., informative graph topological structures) and labels, and conduct inference according to them only, thus generalize better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results on Real-world Graphs</head><p>On real-world molecule and social datasets (i.e., COLLAB, PROTEINS, and D&amp;D), the training and testing graphs are split by graph sizes, i.e., our method and baselines are trained on small graphs and tested on larger graphs. The results are presented in Table <ref type="table" target="#tab_2">3</ref>. OOD-GNN consistently yields the best testing performance on all the datasets. In particular, OOD-GNN improves over the strongest baselines by 2.2%, 6.0%, and 1.7% in PROTEINS 25 , D&amp;D 200 , D&amp;D 300 respectively. Our model achieves the best out-of-distribution generalization performance under size distribution shifts by encouraging independence between relevant and irrelevant representations. The results of baselines degrade due to the spurious correlations between irrelevant representations and labels. For example, each graph in the COLLAB dataset corresponds to an ego-network of different researchers from one </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>OGBG-MOLBACE</head><p>OOD-GNN no RFF GIN (c) OGBG-MOLBACE Figure <ref type="figure">2</ref>: Ablation study results of our method. The blue curves with circle markers show that as dimensionality of random Fourier features increases, the generalization performance of OOD-GNN improves. The purple markers show that if we remove random Fourier features and only eliminate linear correlation, the performance drops significantly. The orange markers represent the results of GIN, the graph encoder baseline in our method. field, and the label denotes the corresponding research field. The truly predictive representations are from the graph topological structures. If the GNN models learn spurious correlations between graph sizes and labels, they will fail to make correct predictions on larger OOD testing graphs.</p><p>The graph classification results on nine Open Graph Benchmark (OGB) datasets are shown in Table <ref type="table" target="#tab_3">4</ref>. The datasets are split based on the scaffold, i.e., the two-dimensional structural framework. So the distribution shifts between training and testing graphs exist on the graph topological structure and features, leading to a more challenging scenario. None of the baselines is consistently competitive across all datasets, as opposed to our proposed method. Notice that adding virtual nodes to GCN and GIN is not a promising improvement for generalization since it can provide performance gains on some datasets but fail on the others. FactorGCN shows poor results on these datasets, possibly because it enforces the decomposition of the input graphs into several independent factor graphs for disentanglement, which is hard to achieve without sufficient supervision. PNA is proposed to address the size generalization problem but still fails under the more complex distribution shifts. TopKPool selects some local parts of the input graph and ignores the others. The strongest baseline on molecule and social datasets, i.e., SAGPool, pools the nodes with the self-attention mechanism. However, the accurate selection for TopKPool and calculation of attention scores for SAGPool are easily affected by the spurious correlations on OOD test graphs and therefore also fail to generalize. In contrast, OOD-GNN shows a strong capability of out-of-distribution generalization when the input graphs have complicated structures, especially for the large-scale real-world graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Studies</head><p>We perform ablation studies over a number of key components of our method to analyze their functionalities more deeply. Specifically, we compare OOD-GNN with the following two variants: (1) Variant 1: it sets the dimensionality of random Fourier features to different values. (2) Variant 2: it removes all the random Fourier features. For simplicity, we only report the results on one synthetic dataset (i.e., TRIANGLES) and two real-world datasets (i.e., D&amp;D 300 and OGBG-MOLBACE), while the results on other datasets show similar patterns.</p><p>Variant 1 exploits the effect of different dimensions of random Fourier features. Note that our method adopts random Fourier features (see Eq. ( <ref type="formula" target="#formula_4">4</ref>)), which sample from Gaussian to learn the graph weights and encourage the independence of representations. It is shown in <ref type="bibr" target="#b57">[58]</ref> that if sampling more random Fourier features (i.e., when Q in Eq. ( <ref type="formula" target="#formula_4">4</ref>) increases), the learned graph representations will be more independent. However, there exists a trade-off between independence and computational efficiency since the more random Fourier features are sampled, the higher the computational cost becomes. When the computational resources are extremely limited, it is also feasible to randomly select part of the dimensions in graph representations to calculate the dependence. In Figure <ref type="figure">2</ref>, the x-axis represents the dimensionality of random Fourier features compared to graph representations, e.g., "2x" indicates Q = 2 in Eq. (4), while "0.2x" means we randomly select 20% dimensions of graph representations. We observe from Figure <ref type="figure">2</ref> that as the dimensionality of random Fourier features increases, the performance on OOD testing graphs grows consistently, which demonstrates that eliminating the statistical dependence between different dimensions of the graph representations will encourage the independence between relevant and irrelevant representations and lead to better out-of-distribution generalization ability.   Variant 2 removes all the random Fourier features and the optimization in Eqs. (6)(7) will degenerate to linear cases, i.e., only eliminating linear correlation rather than encouraging independence between different dimensions of graph representations. In Figure <ref type="figure">2</ref>, this variant is termed as "no RFF". We can observe a clear performance drop for this variant, demonstrating that the complex non-linear dependencies are common in the graph representations. By eliminating non-linear dependence between representations, the GNNs will be encouraged to learn true connections between the input graphs and the corresponding labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Training Dynamic</head><p>We can observe the convergence of our proposed method empirically, although Eqs. (6)(7) are iteratively optimized. In Figure <ref type="figure" target="#fig_5">3</ref> (a)(b)(c), we show the weighted prediction loss in the training process on TRIANGLES, D&amp;D 300 , and OGBG-MOLBACE, respectively. The loss converges in no more than 100 epochs to about 0.67, 0.30, and 0.25 on the three datasets, respectively. The results on the other datasets show similar patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Weights Distribution</head><p>In Figure <ref type="figure" target="#fig_6">4</ref> (a)(b)(c), to further investigate the effectiveness of the graph reweighting, we show the distribution of the learned graph weights on TRIANGLES, D&amp;D 300 , and OGBG-MOLBACE when the training is finished. The results show that our proposed method learns non-trivial weights, and the weights distribution is slightly different across different datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Time Complexity</head><p>Our method is not only effective but efficient to learn out-of-distribution generalized graph representation under complex distribution shifts         </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8">Number of Parameters</head><p>The parameters of our method consist of two parts, i.e., the graph encoder and graph weights. The former is determined by the graph encoder GNN architecture, which is GIN in our setting. The latter is determined by the number of graphs. Taking the OGBG-MOLBACE dataset for example, the number of parameters of our method is about 0.9M if we set the number of message-passing layers as 5 and the dimensionality of the representations as 300. Notice that our method has comparable or fewer parameters than the baselines. For the OGBG-MOLBACE dataset with the same hyper-parameter settings, GIN and PNA (two baselines in the experiments) have 0.9M and 6.0M parameters, respectively. Nevertheless, our method achieves impressive out-of-distribution generalization performance against the baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.9">Hyper-parameter Sensitivity</head><p>We investigate the sensitivity of hyper-parameters of our method, including the number of messagepassing layers in the graph encoder, the dimensionality of the representations d, the size of global weights, and the momentum coefficient γ in updating global weights. For simplicity, we only report the results on TRIANGLES (see Figure <ref type="figure" target="#fig_9">5</ref>), D&amp;D 300 (see Figure <ref type="figure" target="#fig_11">6</ref>), and OGBG-MOLBACE (see Figure <ref type="figure" target="#fig_15">7</ref>), while the results on other datasets show similar patterns. From Figures <ref type="figure" target="#fig_15">5-7</ref>, we observe that the performance relies on an appropriate choice of the number of message-passing layers of the graph encoder. Since the task of counting triangles is relatively simple, the graph encoder with two message-passing layers is good enough on TRIANGLES, while five layers are needed to achieve the best performance on OGBG-MOLBACE. When the number of layers of graph encoder is small, the model has limited capacity and may not be able to fuse enough information from neighbors. On the other hand, a very large number of layers could lead to the over-smoothing problem <ref type="bibr" target="#b66">[67]</ref>. Besides, the optimal dimensionality of the representations d for TRIANGLES is relatively smaller than that for D&amp;D 300 and OGBG-MOLBACE.</p><p>In addition, as the size of global weights increases, the performance is improved. The global representations and weights can help to learn consistent graph sample weights on the whole dataset and therefore improve the generalization ability of the model. Finally, we find that the momentum coefficient γ also has a slight influence on the performance. A large γ will make the update of global representations and weights slower, and a small one will accelerate the update, corresponding to emphasizing long-term and short-term memory, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we propose a novel out-of-distribution generalized graph neural network (OOD-GNN) to solve the problem of generalization of GNNs under complex and heterogeneous distribution shifts. We present a nonlinear graph representation decorrelation method by utilizing random Fourier features and sample reweighting, so that the learned representations of OOD-GNN are encouraged to eliminate the statistical dependence between the representations. We further design a scalable global-local weight estimator, which can learn graph weights for the whole dataset consistently and efficiently. Extensive experiments on both synthetic and real-world datasets demonstrate the superiority of our method against state-of-the-art baselines for out-of-distribution generalization.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(g k ) ∈ R |B| . They serve as the memory of the encoded graph representations and the corresponding weights from historical mini-batches during the training stage. Since these global representations and weights are shared across different mini-matches, they maintain a global summarization of the whole training dataset. The size of global weights only depends on the mini-batch size, which is a hyper-parameter and independent of the training dataset size. Local weights. For each mini-batch B of the input graphs {G n } |B| n=1 , we first calculate their graph representations</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 1 1 : 2 :</head><label>112</label><figDesc>The training procedure of OOD-GNN. Input: A graph dataset G = {G n } N n=1 Output: Learned graph encoder Φ * and classifier R * for e ← 1 to Epoch do for sampled minibatch B = {G n }</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The weighted prediction loss in the training process on three datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The distribution of the learned graph weights after training on three datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>. The time complexity of our method is O(|E| d + |V | d 2 + K|B|d 2 ), where |V |, |E| denotes the total number of nodes and edges in the graphs, d is the dimensionality of the representation, K is the number of groups of global weights, and |B| is the batch size. Specifically, the time complexity of the graph encoder GIN is O(|E| d + |V | d 2 ) and the optimization of graph weights in Eq. (7) has O(K|B|d 2 ) complexity. As a comparison, the time complexity of GIN, our backbone GNN, is O(|E| d + |V | d 2 ), i.e., our time complexity is on par since d, K, and |B| are small constants that are unrelated to the dataset size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The analyses of different hyper-parameters on TRIANGLES dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The analyses of different hyper-parameters on D&amp;D 300 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: The analyses of different hyper-parameters on OGBG-MOLBACE dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The statistics of the datasets. #Graphs is the number of graphs in the dataset. Average #Nodes/#Edges are the average number of nodes and edges in a graph of the dataset, respectively. #Tasks denotes the dimensionality of output required for prediction. Task type includes binary classification, multi-classification, and regression. The various split methods for training/validation/testing dataset cover complex and realistic distribution shifts.</figDesc><table><row><cell>Category</cell><cell>Name</cell><cell>#Graphs</cell><cell>Average #Nodes</cell><cell>Average #Edges</cell><cell>#Tasks</cell><cell>Task Type</cell><cell>Split Method</cell><cell>Metric</cell></row><row><cell>Synthetic</cell><cell>TRIANGLES MNIST-75SP</cell><cell>4,000 7,000</cell><cell>15.6 66.8</cell><cell>48.9 600.2</cell><cell>1 1</cell><cell>Regression Multi-class.</cell><cell>Size Feature</cell><cell>Accuracy Accuracy</cell></row><row><cell>Molecule and social datasets</cell><cell>COLLAB PROTEINS D&amp;D</cell><cell>5,000 1,113 1,178</cell><cell>74.5 39.1 284.3</cell><cell>2457.8 72.8 715.7</cell><cell cols="2">1 1 Binary class. Multi-class. 1 Binary class.</cell><cell>Size Size Size</cell><cell>Accuracy Accuracy Accuracy</cell></row><row><cell></cell><cell>TOX21</cell><cell>7,831</cell><cell>18.6</cell><cell>19.3</cell><cell cols="4">12 Binary class. Scaffold ROC-AUC</cell></row><row><cell></cell><cell>BACE</cell><cell>1,513</cell><cell>34.1</cell><cell>36.9</cell><cell cols="4">1 Binary class. Scaffold ROC-AUC</cell></row><row><cell></cell><cell>BBBP</cell><cell>2,039</cell><cell>24.1</cell><cell>26.0</cell><cell cols="4">1 Binary class. Scaffold ROC-AUC</cell></row><row><cell></cell><cell>CLINTOX</cell><cell>1,477</cell><cell>26.2</cell><cell>27.9</cell><cell cols="4">2 Binary class. Scaffold ROC-AUC</cell></row><row><cell></cell><cell>SIDER</cell><cell>1,427</cell><cell>33.6</cell><cell>35.4</cell><cell cols="4">27 Binary class. Scaffold ROC-AUC</cell></row><row><cell>Open Graph Benchmark</cell><cell>TOXCAST</cell><cell>8,576</cell><cell>18.8</cell><cell>19.3</cell><cell cols="4">12 Binary class. Scaffold ROC-AUC</cell></row><row><cell>OGBG-MOL*</cell><cell>HIV</cell><cell>41,127</cell><cell>25.5</cell><cell>27.5</cell><cell cols="4">1 Binary class. Scaffold ROC-AUC</cell></row><row><cell></cell><cell>ESOL</cell><cell>1,128</cell><cell>13.3</cell><cell>13.7</cell><cell>1</cell><cell>Regression</cell><cell>Scaffold</cell><cell>RMSE</cell></row><row><cell></cell><cell>FREESOLV</cell><cell>642</cell><cell>8.7</cell><cell>8.4</cell><cell>1</cell><cell>Regression</cell><cell>Scaffold</cell><cell>RMSE</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Graph classification accuracy (%) on training and testing sets of two synthetic datasets. Test(large) denotes larger graph sizes in testing set and Test(noise)/Test(color) represent adding Gaussian noises/color noises respectively. In each column, the boldfaced score denotes the best result and the underlined score represents the second-best result. ± denotes standard deviation. termed D&amp;D 200 and D&amp;D 300 . For D&amp;D 200 , we train on graphs with 30 to 200 nodes and test on graphs with 201 to 5, 748 nodes. For D&amp;D 300 , we train on 500 graphs with 30 to 300 nodes and test on other graphs with 30 to 5, 748 nodes.</figDesc><table><row><cell></cell><cell cols="2">TRIANGLES</cell><cell></cell><cell>MNIST-75SP</cell><cell></cell></row><row><cell></cell><cell>Train</cell><cell>Test(large)</cell><cell>Train</cell><cell cols="2">Test(noise) Test(color)</cell></row><row><cell>GCN</cell><cell>28.3±0.6</cell><cell>21.3±1.9</cell><cell>51.7±1.0</cell><cell>26.5±1.4</cell><cell>27.0±1.3</cell></row><row><cell cols="2">GCN-virtual 32.4±0.6</cell><cell>17.0±1.8</cell><cell>55.1±2.3</cell><cell>26.0±1.5</cell><cell>26.1±1.8</cell></row><row><cell>GIN</cell><cell>34.7±0.7</cell><cell>22.2±1.9</cell><cell>67.6±0.8</cell><cell>27.9±2.5</cell><cell>34.3±4.4</cell></row><row><cell>GIN-virtual</cell><cell>34.2±0.6</cell><cell>17.6±1.7</cell><cell>66.7±0.9</cell><cell>25.7±2.9</cell><cell>33.4±1.2</cell></row><row><cell>FactorGCN</cell><cell>10.6±1.6</cell><cell>4.2±0.9</cell><cell>46.7±1.2</cell><cell>19.7±1.4</cell><cell>24.8±1.3</cell></row><row><cell>PNA</cell><cell>43.7±3.6</cell><cell>16.8±2.4</cell><cell>83.0±0.9</cell><cell>22.8±7.3</cell><cell>29.2±6.3</cell></row><row><cell>TopKPool</cell><cell>28.3±0.3</cell><cell>22.0±0.2</cell><cell>61.0±3.7</cell><cell>17.0±1.0</cell><cell>16.9±1.5</cell></row><row><cell>SAGPool</cell><cell>26.7±1.0</cell><cell>23.7±0.7</cell><cell>60.2±1.3</cell><cell>19.6±3.4</cell><cell>20.1±3.7</cell></row><row><cell cols="2">OOD-GNN 29.9±0.7</cell><cell>25.1±0.8</cell><cell>63.2±1.1</cell><cell>31.5±0.9</cell><cell>38.5±1.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Graph classification accuracy (%) on the testing set of OOD-GNN and baselines. Our OOD-GNN outperforms the baselines significantly on all graph classification benchmarks, indicating its superiority against graph size distribution shifts. The best result and the second-best result for each dataset are in bold and underlined, respectively.</figDesc><table><row><cell></cell><cell cols="4">COLLAB35 PROTEINS25 D&amp;D200 D&amp;D300</cell></row><row><cell># Train/Test graphs</cell><cell>500/4500</cell><cell>500/613</cell><cell>462/716</cell><cell>500/678</cell></row><row><cell>#Nodes Train</cell><cell>32-35</cell><cell>4-25</cell><cell>30-200</cell><cell>30-300</cell></row><row><cell>#Nodes Test</cell><cell>32-492</cell><cell>6-620</cell><cell>201-5748</cell><cell>30-5748</cell></row><row><cell>GCN</cell><cell>65.9±3.4</cell><cell>75.1±2.2</cell><cell cols="2">29.2±8.2 71.9±3.6</cell></row><row><cell>GCN-virtual</cell><cell>61.5±1.6</cell><cell>70.4±3.7</cell><cell cols="2">41.6±8.0 71.6±4.4</cell></row><row><cell>GIN</cell><cell>55.5±4.9</cell><cell>74.0±2.7</cell><cell cols="2">43.0±8.3 67.8±4.3</cell></row><row><cell>GIN-virtual</cell><cell>54.8±2.7</cell><cell>66.0±7.5</cell><cell cols="2">46.7±4.5 72.1±4.3</cell></row><row><cell>FactorGCN</cell><cell>51.0±1.3</cell><cell>63.5±4.8</cell><cell cols="2">42.3±3.1 55.9±1.6</cell></row><row><cell>PNA</cell><cell>59.6±5.5</cell><cell>71.4±3.4</cell><cell cols="2">47.3±6.8 70.1±2.1</cell></row><row><cell>TopKPool</cell><cell>52.8±1.0</cell><cell>64.9±3.0</cell><cell cols="2">34.6±5.6 69.3±3.6</cell></row><row><cell>SAGPool</cell><cell>67.0±1.7</cell><cell>76.2±0.7</cell><cell cols="2">54.3±5.0 78.4±1.1</cell></row><row><cell>OOD-GNN</cell><cell>67.2±1.8</cell><cell>78.4±0.9</cell><cell cols="2">60.3±4.5 80.1±1.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Results on nine Open Graph Benchmark (OGB) datasets. We report the ROC-AUC (%) for classification tasks and RMSE for regression tasks with the standard deviation on the test set of all methods. None of the baseline methods is consistently competitive across all datasets, while our proposed method shows impressive performance. (↑) means that higher values indicate better results, and (↓) represents the opposite.</figDesc><table><row><cell></cell><cell>TOX21</cell><cell>BACE</cell><cell>BBBP</cell><cell>CLINTOX</cell><cell>SIDER</cell><cell>TOXCAST</cell><cell>HIV</cell><cell>ESOL</cell><cell>FREESOLV</cell></row><row><cell>Metric</cell><cell></cell><cell></cell><cell></cell><cell>ROC-AUC (↑)</cell><cell></cell><cell></cell><cell></cell><cell cols="2">RMSE (↓)</cell></row><row><cell>GCN</cell><cell cols="3">75.3±0.7 79.2±1.4 68.9±1.5</cell><cell>91.3±1.7</cell><cell>59.6±1.8</cell><cell>63.5±0.4</cell><cell>76.1±1.0</cell><cell>1.11±0.03</cell><cell>2.64±0.24</cell></row><row><cell cols="4">GCN-virtual 77.5±0.9 68.9±7.0 67.8±2.4</cell><cell>88.6±2.1</cell><cell>59.8±1.5</cell><cell>66.7±0.5</cell><cell>76.0±1.2</cell><cell>1.02±0.10</cell><cell>2.19±0.12</cell></row><row><cell>GIN</cell><cell cols="3">74.9±0.5 73.0±4.0 68.2±1.5</cell><cell>88.1±2.5</cell><cell>57.6±1.4</cell><cell>63.4±0.7</cell><cell>75.6±1.4</cell><cell>1.17±0.06</cell><cell>2.76±0.35</cell></row><row><cell>GIN-virtual</cell><cell cols="3">77.6±0.6 73.5±5.2 69.7±1.9</cell><cell>84.1±3.8</cell><cell>57.6±1.6</cell><cell>66.1±0.5</cell><cell>77.1±1.5</cell><cell>1.00±0.07</cell><cell>2.15±0.30</cell></row><row><cell>FactorGCN</cell><cell cols="3">57.8±2.1 70.0±0.6 54.1±1.1</cell><cell>64.2±2.1</cell><cell>53.3±1.7</cell><cell>51.2±0.8</cell><cell>57.1±1.5</cell><cell>3.39±0.15</cell><cell>5.69±0.32</cell></row><row><cell>PNA</cell><cell cols="3">71.5±0.5 77.4±2.1 66.2±1.2</cell><cell>81.2±2.0</cell><cell>59.6±1.1</cell><cell>60.6±0.2</cell><cell>79.1±1.3</cell><cell>0.94±0.02</cell><cell>2.92±0.16</cell></row><row><cell>TopKPool</cell><cell cols="3">75.6±0.9 76.9±2.4 68.6±1.1</cell><cell>86.9±1.1</cell><cell>60.6±1.5</cell><cell>64.7±0.1</cell><cell>76.7±1.1</cell><cell>1.17±0.03</cell><cell>2.08±0.10</cell></row><row><cell>SAGPool</cell><cell cols="3">74.7±3.1 76.6±1.0 69.3±2.1</cell><cell>88.7±1.0</cell><cell>61.3±1.3</cell><cell>64.8±0.2</cell><cell>77.7±1.3</cell><cell>1.22±0.05</cell><cell>2.28±0.12</cell></row><row><cell cols="4">OOD-GNN 78.4±0.8 81.3±1.2 70.1±1.0</cell><cell>91.4±1.3</cell><cell>64.0±1.3</cell><cell>68.7±0.3</cell><cell>79.5±0.9</cell><cell>0.88±0.05</cell><cell>1.81±0.14</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">In a finite-dimensional Euclidean space, the Hilbert-Schmidt norm • HS is identical to the Frobenius norm.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Network biology: understanding the cell&apos;s functional organization</title>
		<author>
			<persName><forename type="first">Albert-Laszlo</forename><surname>Barabasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoltan</forename><forename type="middle">N</forename><surname>Oltvai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature reviews genetics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="101" to="113" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">David</forename><surname>Easley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
		<title level="m">crowds, and markets</title>
				<imprint>
			<publisher>Cambridge university press Cambridge</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Moleculenet: a benchmark for molecular machine learning</title>
		<author>
			<persName><forename type="first">Zhenqin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><forename type="middle">N</forename><surname>Feinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caleb</forename><surname>Geniesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Aneesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Leswing</surname></persName>
		</author>
		<author>
			<persName><surname>Pande</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemical science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="513" to="530" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding: A survey of approaches and applications</title>
		<author>
			<persName><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhendong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2724" to="2743" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Graph representation learning</title>
		<author>
			<persName><surname>William L Hamilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Synthesis Lectures on Artifical Intelligence and Machine Learning</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Learning graph-level representation for drug discovery</title>
		<author>
			<persName><forename type="first">Junying</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofei</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.03741</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A graph neural network framework for social recommendations</title>
		<author>
			<persName><forename type="first">Wenqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoyong</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Intention-aware sequential recommendation with structured intent transition</title>
		<author>
			<persName><forename type="first">Haoyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Modeling polypharmacy side effects with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Monica</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="457" to="466" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Zinc 15-ligand discovery for everyone</title>
		<author>
			<persName><forename type="first">Teague</forename><surname>Sterling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">J</forename><surname>Irwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2324" to="2337" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The art and practice of structurebased drug design: a molecular modeling perspective</title>
		<author>
			<persName><forename type="first">Regine</forename><forename type="middle">S</forename><surname>Bohacek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Mcmartin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">C</forename><surname>Guida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medicinal research reviews</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="50" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Graph neural network-based diagnosis prediction</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Buyue</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianli</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Big Data</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="379" to="390" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Risk prediction of theft crimes in urban communities: An integrated model of lstm and st-gcn</title>
		<author>
			<persName><forename type="first">Xinge</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofeng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huanggang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiansong</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="217222" to="217230" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Using external knowledge for financial event prediction based on graph neural networks</title>
		<author>
			<persName><forename type="first">Yiying</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongyu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Libo</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Information and Knowledge Management</title>
				<meeting>the 28th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2161" to="2164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Understanding attention and generalization in graph neural networks</title>
		<author>
			<persName><forename type="first">Boris</forename><surname>Knyazev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><forename type="middle">R</forename><surname>Amer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="4202" to="4212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On size generalization in graph neural networks</title>
		<author>
			<persName><forename type="first">Gilad</forename><surname>Yehudai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><surname>Meirom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gal</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haggai</forename><surname>Maron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Size-invariant graph representations for graph classification extrapolations</title>
		<author>
			<persName><forename type="first">Beatrice</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangze</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">How neural networks extrapolate: From feedforward to graph neural networks</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mozhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingling</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.02893</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">Invariant risk minimization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An empirical study on robustness to spurious correlations using pre-trained language models</title>
		<author>
			<persName><forename type="first">Lifu</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Garima</forename><surname>Lalwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spandana Gella, and He He</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="621" to="633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning to reweight examples for robust deep learning</title>
		<author>
			<persName><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyuan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4334" to="4343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Stable prediction across unknown environments</title>
		<author>
			<persName><forename type="first">Kun</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susan</forename><surname>Athey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoxuan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1617" to="1626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Wilds: A benchmark of in-the-wild distribution shifts</title>
		<author>
			<persName><forename type="first">Pang</forename><surname>Wei Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiori</forename><surname>Sagawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sang</forename><surname>Michael Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marvin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshay</forename><surname>Balsubramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">Lanas</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irena</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5637" to="5664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Random features for large-scale kernel machines</title>
		<author>
			<persName><forename type="first">Ali</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Self-attention graph pooling</title>
		<author>
			<persName><forename type="first">Junhyun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inyeop</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3734" to="3743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Hierarchical multi-view graph pooling with structure learning</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengwei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dai</forename><surname>Huifen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Can</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
				<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Weisfeiler and leman go neural: Higher-order graph neural networks</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Ritzert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Eric Lenssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName><surname>Grohe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4602" to="4609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Self-propagation graph neural network for recommendation</title>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junfeng</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Higher-order attribute-enhancing heterogeneous graph neural networks</title>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingtong</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hekai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifang</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European semantic web conference</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Higher-order interaction goes neural: A substructure assembling graph attention network for graph classification</title>
		<author>
			<persName><forename type="first">Jianliang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoting</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingming</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Measuring abstract reasoning in neural networks</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICML</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Analysing mathematical reasoning abilities of neural models</title>
		<author>
			<persName><forename type="first">David</forename><surname>Saxton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Neural execution of graph algorithms</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matilde</forename><surname>Padovano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">What graph neural networks cannot learn: depth vs width</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Loukas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Generalization and representational limits of graph neural networks</title>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Stability and generalization of graph convolutional neural networks</title>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi-Li</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1539" to="1548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Reducing overfitting in deep networks by decorrelating representations</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Regularizing deep neural networks with an ensemble-based decorrelation method</title>
		<author>
			<persName><forename type="first">Shuqin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuexian</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yazhou</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2177" to="2183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">Devansh</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.00164</idno>
		<title level="m">Predicting with high correlation features</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">The influence of correlations between noncritical features and reinforcement on stimulus generalization</title>
		<author>
			<persName><forename type="first">Christina</forename><forename type="middle">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">C</forename><surname>Vladescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caio</forename><forename type="middle">F</forename><surname>Kenneth F Reeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samantha</forename><forename type="middle">L</forename><surname>Miguel</surname></persName>
		</author>
		<author>
			<persName><surname>Breeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Behavior Analysis</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="346" to="366" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">How correlations influence lasso prediction</title>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Hebiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Lederer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1846" to="1854" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Regularizing cnns with locally constrained decorrelations</title>
		<author>
			<persName><forename type="first">Jordi</forename><surname>Pau Rodríguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josep</forename><forename type="middle">M</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Gonfaus</surname></persName>
		</author>
		<author>
			<persName><surname>Roca</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01967</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Stable prediction with model misspecification and agnostic distribution shift</title>
		<author>
			<persName><forename type="first">Kun</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoxuan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susan</forename><surname>Athey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="4485" to="4492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deep stable learning for out-of-distribution generalization</title>
		<author>
			<persName><forename type="first">Xingxuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renzhe</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheyan</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5372" to="5382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Removing the feature correlation effect of multiplicative noise</title>
		<author>
			<persName><forename type="first">Zijun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yining</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongpeng</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.07023</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Stable learning via sample reweighting</title>
		<author>
			<persName><forename type="first">Zheyan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Kunag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="5692" to="5699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Why stable learning works? a theory of covariate shift generalization</title>
		<author>
			<persName><forename type="first">Renzhe</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheyan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingxuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.02355</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Diva: Domain invariant variational autoencoders</title>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Ilse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jakub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Louizos</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Imaging with Deep Learning</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="322" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Measuring statistical dependence with hilbert-schmidt norms</title>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on algorithmic learning theory</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="63" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Kernel measures of conditional dependence</title>
		<author>
			<persName><forename type="first">Kenji</forename><surname>Fukumizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="489" to="496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Learning de-biased representations with biased representations</title>
		<author>
			<persName><forename type="first">Hyojin</forename><surname>Bahng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaegul</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seong Joon</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="528" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Approximate kernel-based conditional independence tests for fast non-parametric causal discovery</title>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">V</forename><surname>Strobl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shyam</forename><surname>Visweswaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Causal Inference</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Factorizable graph convolutional networks</title>
		<author>
			<persName><forename type="first">Yiding</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zunlei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingli</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinchao</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Principal neighbourhood aggregation for graph nets</title>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Cavalleri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominique</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Graph u-nets</title>
		<author>
			<persName><forename type="first">Hongyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">international conference on machine learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2083" to="2092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Slic superpixels compared to state-of-the-art superpixel methods</title>
		<author>
			<persName><forename type="first">Radhakrishna</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Appu</forename><surname>Shaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelien</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sabine</forename><surname>Süsstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2274" to="2282" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">A new space for comparing graphs</title>
		<author>
			<persName><forename type="first">Anshumali</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM 2014)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="62" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Protein function prediction via graph kernels</title>
		<author>
			<persName><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soon</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName><surname>Schönauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><forename type="middle">J</forename><surname>Svn Vishwanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hans-Peter</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">suppl_1</biblScope>
			<biblScope unit="page" from="47" to="56" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Distinguishing enzyme structures from non-enzymes without alignments</title>
		<author>
			<persName><forename type="first">D</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">J</forename><surname>Dobson</surname></persName>
		</author>
		<author>
			<persName><surname>Doig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of molecular biology</title>
		<imprint>
			<biblScope unit="volume">330</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="771" to="783" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Lasagne: A multi-layer graph convolutional network framework via node-aware deep architecture</title>
		<author>
			<persName><forename type="first">Xupeng</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingxia</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
