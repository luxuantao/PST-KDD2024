<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">. ABSTRACT A fair amount of work has been done in recent years on reducing power consumption in caches by using a small instruction buffer placed between the execution pipe and a larger main cache [1,2,6]. These techniques, however, often degrade the overall system performance. In this paper, we propose using a small instruction buffer, also called a loop cache, to save power. A loop cache has no address tag store. It consists of a direct-mapped data array and a loop cache controller. The loop cache controller knows precisely whether the next instruction request will hit in the loop cache, well ahead of time. As a result, there is no performance degradation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<title level="a" type="main">. ABSTRACT A fair amount of work has been done in recent years on reducing power consumption in caches by using a small instruction buffer placed between the execution pipe and a larger main cache [1,2,6]. These techniques, however, often degrade the overall system performance. In this paper, we propose using a small instruction buffer, also called a loop cache, to save power. A loop cache has no address tag store. It consists of a direct-mapped data array and a loop cache controller. The loop cache controller knows precisely whether the next instruction request will hit in the loop cache, well ahead of time. As a result, there is no performance degradation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Low cost</term>
					<term>low power</term>
					<term>embedded systems</term>
					<term>small program loops</term>
					<term>instruction buffering</term>
				</keywords>
			</textClass>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">INTRODUCTION</head><p>Many embedded applications are characterized by spending a large fraction of execution time on small program loops. A fair amount of work has been done in recent years on reducing power consumption in caches by using a small instruction buffer placed between the execution pipe and a larger main cache <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b5">6]</ref>. Using this approach, however, may incur cycle penalties due to instruction buffer misses (the requested instructions are not found in the buffer). Another approach is to operate the main cache in the same cycle only if there is a buffer miss, possibly at the expense of a longer cycle time. In portable embedded systems, these performance degrading techniques could have an adverse affect on the energy consumption at the system level <ref type="bibr" target="#b2">[3]</ref>.</p><p>In this paper, we propose using a small instruction buffer, referred to here as a loop cache, to reduce the instruction fetch energy when executing tight program loops. This is achieved without degrading the performance, as measured by increased number of execution cycles or longer cycle time. In this technique, the instruction fetch datapath consists of two levels of instruction caching: an on-chip main cache (unified or split), and a small loop cache. Instructions are supplied to the execution core either from the loop cache or from the main cache. The loop cache controller knows precisely whether the next instruction request will hit in the loop cache, well ahead of time. The loop cache has no address tag store. Its cache array can be implemented as a direct-mapped array. Furthermore, there is no valid bit associated with each loop cache entry.</p><p>Our proposed technique is based on the definition, detection and utilization of a special class of branch instructions, called the short backward branch instruction (sbb). When a sbb is detected in an instruction stream and is found to be taken, the loop cache controller assumes that we are starting to execute the second iteration of a program loop, and tries to fill up the loop cache. Starting from the third iteration, the controller directs all the instruction requests to the loop cache and shuts of the main cache completely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Short Backward Branch Instructions</head><p>A short backward branch instruction (sbb) is any PC-relative branch instruction that fit the following instruction format. It can be conditional or unconditional.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1. sbb Instruction Format</head><p>The upper portion of the branch displacement field of a sbb are all ones (indicating a negative branch displacement). The lower portion of the branch displacement field, ld, is w bit wide. By definition, a sbb has a maximum backward branch distance given by 2 w instructions. The size of the loop cache is also given by 2 w instructions. This definition ensures that if a sbb is decoded and recognized as such by the decoding hardware, the loop size in question is guaranteed to be no larger than the loop cache size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Triggering sbb</head><p>When a sbb is detected in an instruction stream and found to be taken, the hardware assumes that the program is executing a loop and initiates all the appropriate control actions to utilize the loop cache. The sbb that triggers this transition is called the triggering sbb. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Loop Cache Organization</head><p>Figure <ref type="figure">2</ref> shows the organization of a 2 w -entry loop cache and how it is being accessed with an instruction address A[31:0]. A loop cache does not have an address tag store. The loop cache array can be implemented as a directmapped array. It is indexed using the index(A) field of the instruction address. The index(A) field is w-bit wide. The array can store 2 w instructions. By definition of a sbb, the maximum program loop size that can be recognized and captured by the loop cache is 2 w instructions. Thus accessing the loop cache during program loop execution is guaranteed to be unique and non-conflicting. That is: (i) an instruction in the program loop will always be mapped to a unique location in the loop cache array; and (ii) there could never be more than one instruction from a given program loop to compete for a particular cache location. When the program loop being captured is smaller than the loop cache size, only part of the loop cache array is utilized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2. Loop Cache Organization</head><p>Unlike many other loop caching schemes, our loop caching scheme does not require the program loop to be aligned to any particular address boundary. The software can place a loop at any arbitrary starting address.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Determining Loop Cache Hit/Miss Early</head><p>In order to determine in advance, whether the next instruction fetch will hit in the loop cache, the controller needs the following information on a cycle-to-cycle basis: (a) is the next instruction fetch a sequential fetch or is there a change of control flow (cof)? (b) if there is a cof, is it caused by the triggering sbb? (c) is the loop cache completely warmed up with the program loop so we could access the loop cache instead of the main cache?</p><p>Information pertaining to (a) can be easily obtained from the instruction decode unit as well as the fetch and branch unit in the pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Monitoring sbb Execution</head><p>To obtain information pertaining to (b), a counter based scheme similar to those proposed in <ref type="bibr" target="#b2">[3]</ref> could be used. In this scheme, when a sbb is encountered and taken, its lower displacement field, ld, is loaded into a w-bit increment counter called Count_Register (see Figure <ref type="figure" target="#fig_0">3</ref>). The hardware then infers the size of the program loop from this branch displacement field. It does so by incrementing this negative displacement by one, each time an instruction is executed sequentially. When the counter becomes zero, the hardware knows that we are executing the triggering sbb. If the triggering sbb is taken again, the increment counter is re-initialized with the ld field from the sbb, and the process described above repeats itself. Using this scheme, the controller knows whether a cof is caused by the triggering sbb, by examining the value in the Count_Register. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Loop Cache Controller</head><p>The state transition diagram for the loop cache controller is show in Figure <ref type="figure" target="#fig_1">4</ref>. The controller ensures that the loop cache is warmed up before it is being utilized. It begins with an IDLE state. When a sbb is decoded, its ld field is loaded into the Count_Register. If the sbb is taken, the controller enters a FILL state. The sbb becomes the triggering sbb. When in the FILL state, the controller fills the loop cache with the instructions being fetched from the main cache. As the negative value in the Count_Register increases and becomes zero, the controller knows that the instruction currently being executed is the triggering sbb. If the triggering sbb is not taken, the controller returns to the IDLE state. Otherwise, it enters an ACTIVE state. While in the FILL state, if there is a cof that is not caused by the triggering sbb, the controller also returns to the IDLE state. When in the ACTIVE state, the controller directs all the instruction requests to the loop cache. When in the ACTIVE state, the controller will returns to the IDLE state if one of the following two events occurs: (i) the triggering sbb is not taken (the loop sequentially exits through the sbb); or (ii) there is a cof that is not caused by the triggering sbb.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experimental Results</head><p>Instruction level simulations were performed using the Powerstone benchmarks (shown in Table <ref type="table" target="#tab_1">1</ref>) to quantify our (the sbb becomes the triggering sbb) FILL caching technique. These benchmarks were compiled to the M•CORE TM ISA <ref type="bibr" target="#b3">[4]</ref> using the Diab 4.2.2 compiler. There is no performance degradation associated with this technique. We define the main cache access rates as the number of instruction references made to the main cache using a loop cache, divided by the number of instruction references made to the main cache without a loop cache.</p><p>Figure <ref type="figure" target="#fig_3">5</ref> shows the average main cache access rates for the entire benchmark suite, as a function of w. The access rate decreases most dramatically from w=2 to w=5. This is because the program loops found in these benchmarks were dominated by loops with size of 32 instructions or less. With a 32-entry (w=5) loop cache, the main cache access rate is reduced by about 37.9% </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Summary</head><p>Low system cost and low energy consumption are two important factors to consider in designing many embedded systems. In this paper, we proposed a low-cost instruction caching scheme to reduce the instruction fetch energy when executing small tight loops. Our proposed technique is unique in the following ways <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5]</ref>:</p><p>• This technique is based on the definition, detection and utilization of a special class of branch instructions, called the short backward branch instruction (sbb). By definition of a sbb instruction, the size of the program loop that the hardware is trying to capture is guaranteed to be no larger than the loop cache size. Furthermore, the hardware infers from the sbb instruction, how many instructions are actually contained in the loop. • Our caching scheme does not have an address tag store.</p><p>The loop cache array can be implemented as a direct mapped array. It does not have a valid bit associated with each loop cache entry. The low area cost associated with the loop cache allows it to be tightly integrated into the microprocessor core. Tight integration not only further reduces the instruction fetch energy, it could also reduce or even eliminate the adverse impact on memory access time due to the presence of the loop cache. • Unlike many other loop cache implementations, our scheme does not require the program loops to be aligned to any particular address boundary. The software is allowed to place a loop at any arbitrary starting address. • Lastly, and most importantly, there is no cycle count penalty nor cycle time degradation associated with this technique. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Counter based scheme to monitor sbb executions</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Loop Cache Controller</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>taken (triggering sbb not taken) || (cof that is not caused by the triggering sbb) ACTIVE ((no cof) || cof that is caused by the triggering sbb (triggering sbb not taken) || (cof that is not caused by the triggering sbb) no cof (cof that is caused by the triggering sbb))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Main Cache Access Rates</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Energy Reduction Using Loop Caches For Embedded Applications with Small Tight Loops</head><label></label><figDesc></figDesc><table><row><cell>Lea Hwang Lee, Bill Moyer, John Arends</cell></row><row><cell>M • CORE Technology Center, Motorola, Inc.</cell></row><row><cell>P.O. Box 6000, MD TX77/F51</cell></row><row><cell>Austin, TX 78762-6000</cell></row><row><cell>{leahwang,billm,arends}@lakewood.sps.mot.com</cell></row></table><note>opcode 1 1 . . . . 1 1 X X . . . X X upper displacement (ud) lower displacement (ld) w bit branch displacementInstruction Fetch</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 : Powerstone Benchmarks</head><label>1</label><figDesc></figDesc><table><row><cell>Bench-</cell><cell>Dynamic</cell><cell>Description</cell></row><row><cell>mark</cell><cell>Inst. Count</cell><cell></cell></row><row><cell>auto</cell><cell>17374</cell><cell>Automobile control application</cell></row><row><cell>blit</cell><cell>72416</cell><cell>Graphics application</cell></row><row><cell>compress</cell><cell>322101</cell><cell>A Unix utility</cell></row><row><cell>des</cell><cell>510814</cell><cell>Data Encryption Standard</cell></row><row><cell>engine</cell><cell>955012</cell><cell>Engine control application</cell></row><row><cell>fir_int</cell><cell>629166</cell><cell>Integer FIR filter</cell></row><row><cell>g3fax</cell><cell>1412648</cell><cell>Group three fax decode</cell></row><row><cell>g721</cell><cell>231706</cell><cell>Adaptive differential PCM for voice compres-</cell></row><row><cell></cell><cell></cell><cell>sion</cell></row><row><cell>jpeg</cell><cell>1342076</cell><cell>JPEG 24-bit image decompression standard</cell></row><row><cell>map3d</cell><cell>1228596</cell><cell>3D interpolating function for automobile con-</cell></row><row><cell></cell><cell></cell><cell>trol applications</cell></row><row><cell>pocsag</cell><cell>131159</cell><cell>POCSAG communication protocol for paging</cell></row><row><cell></cell><cell></cell><cell>applications</cell></row><row><cell>servo</cell><cell>41132</cell><cell>Hard disc drive servo control</cell></row><row><cell>summin</cell><cell>1330505</cell><cell>Handwriting recognition</cell></row><row><cell>ucbqsort</cell><cell>674165</cell><cell>U.C.B. Quick Sort</cell></row><row><cell>v42bis</cell><cell>1488430</cell><cell>Modem encoding/decoding</cell></row></table><note>M•CORE is a trademark of Motorola Inc.</note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A New Scheme for I-Cache energy reduction in High-Performance Processors</title>
		<author>
			<persName><forename type="first">N</forename><surname>Bellas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Hajj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Polychronopoulos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998-06-28">June 28th 1998</date>
			<biblScope unit="volume">98</biblScope>
			<pubPlace>Barcelona, Spain</pubPlace>
		</imprint>
	</monogr>
	<note>Power Driven Microarchitecture Workshop, held in conjunction with ISCA</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The Filter Cache: An Energy Efficient Memory Structure</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Mangione-Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l. Symp. on Microarchitecture</title>
				<meeting>Int&apos;l. Symp. on Microarchitecture</meeting>
		<imprint>
			<publisher>December</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="184" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Low-Cost Branch Folding for Embedded Applications with Small Tight Loops</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Moyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Arends</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int&apos;l Workshop on Compiler and Architecture Support for Embedded Computing Systems</title>
		<imprint>
			<date type="published" when="1998-12">December 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">M•core Reference</forename><surname>Manual</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>Motorola Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Data Processing System Having a Cache and Method Therefor</title>
		<author>
			<persName><forename type="first">B</forename><surname>Moyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Arends</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">US Patent</title>
		<imprint>
			<biblScope unit="volume">893</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="1999-04-06">6th April, 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cache Design Trade-offs for Power and Performance Optimization: A Case Study</title>
		<author>
			<persName><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Despain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l. Symp. on Low Power Design</title>
				<meeting>Int&apos;l. Symp. on Low Power Design</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="63" to="68" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
