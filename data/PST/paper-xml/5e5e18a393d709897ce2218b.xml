<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LEARNING ROBUST REPRESENTATIONS VIA MULTI-VIEW INFORMATION BOTTLENECK</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Marco</forename><surname>Federici</surname></persName>
							<email>m.federici@uva.nl</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Anjan</forename><surname>Dutta</surname></persName>
							<email>a.dutta@exeter.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Exeter</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Patrick</forename><surname>Forré</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nate</forename><surname>Kushmann</surname></persName>
							<email>nkushman@microsoft.com</email>
							<affiliation key="aff3">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
							<email>zeynep.akata@uni-tuebingen.de</email>
							<affiliation key="aff4">
								<orgName type="institution">University of Tuebingen</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">LEARNING ROBUST REPRESENTATIONS VIA MULTI-VIEW INFORMATION BOTTLENECK</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The information bottleneck principle provides an information-theoretic method for representation learning, by training an encoder to retain all information which is relevant for predicting the label while minimizing the amount of other, excess information in the representation. The original formulation, however, requires labeled data to identify the superfluous information. In this work, we extend this ability to the multi-view unsupervised setting, where two views of the same underlying entity are provided but the label is unknown. This enables us to identify superfluous information as that not shared by both views. A theoretical analysis leads to the definition of a new multi-view model that produces state-of-the-art results on the Sketchy dataset and label-limited versions of the MIR-Flickr dataset. We also extend our theory to the single-view setting by taking advantage of standard data augmentation techniques, empirically showing better generalization capabilities when compared to common unsupervised approaches for representation learning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The goal of deep representation learning <ref type="bibr" target="#b26">(LeCun et al., 2015)</ref> is to transform a raw observational input, x, into a, typically lower-dimensional, representation, z, that contains the information relevant for a given task or set of tasks. Significant progress has been made in deep learning via supervised representation learning, where the labels, y, for the downstream task are known while p(y|x) is learned directly <ref type="bibr" target="#b39">(Sutskever et al., 2012;</ref><ref type="bibr" target="#b19">Hinton et al., 2012)</ref>. Due to the cost of acquiring large labeled datasets, a recently renewed focus on unsupervised representation learning seeks to generate representations, z, that are useful for a wide variety of different tasks where little to no labeled data is available <ref type="bibr" target="#b10">(Devlin et al., 2018;</ref><ref type="bibr" target="#b33">Radford et al., 2019)</ref>.</p><p>Our work is based on the information bottleneck principle <ref type="bibr">(Tishby et al., 2000)</ref> where a representation becomes less affected by nuisances by discarding all information from the input that is not useful for a given task, resulting in increased robustness. In the supervised setting, one can directly apply the information bottleneck principle by minimizing the mutual information between the data x and its representation z, I(x; z), while simultaneously maximizing the mutual information between z and the label y <ref type="bibr" target="#b1">(Alemi et al., 2017)</ref>. In the unsupervised setting, discarding only superfluous information is more challenging, as without labels the model cannot directly identify which information is relevant. Recent literature <ref type="bibr">(Devon Hjelm et al., 2019;</ref><ref type="bibr" target="#b44">van den Oord et al., 2018)</ref> has focused on the InfoMax objective maximizing I(x, z) instead of minimizing it, to guarantee that all the predictive information is retained by the representation, but doing nothing to discard the irrelevant information.</p><p>In this paper, we extend the information bottleneck method to the unsupervised multi-view setting.</p><p>To do this, we rely on a basic assumption of the multi-view literature -that each view provides the same task-relevant information <ref type="bibr" target="#b49">(Zhao et al., 2017)</ref>. Hence, one can improve generalization by discarding all the information not shared by both views from the representation. We do this by maximizing the mutual information between the representations of the two views (Multi-View InfoMax objective) while at the same time eliminating the information not shared between them, since it is guaranteed to be superfluous. The resulting representations are more robust for the given task as they have eliminated view specific nuisances.</p><p>Our contributions are three-fold: (1) We extend the information bottleneck principle to the unsupervised multi-view setting and provide a rigorous theoretical analysis of its application. (2) We define a new model 1 that empirically leads to state-of-the-art results in the low-label setting on two standard multi-view datasets, Sketchy and MIR-Flickr. (3) By exploiting data augmentation techniques, we empirically show that the representations learned by our model in single-view settings are more robust than existing unsupervised representation learning methods, connecting our theory to the choice of augmentation strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES AND FRAMEWORK</head><p>The challenge of representation learning can be formulated as finding a distribution p(z|x) that maps data observations x ∈ X into a representation z ∈ Z, capturing some desired characteristics. Whenever the end goal involves predicting a label y, we consider only z that are discriminative enough to identify y. This requirement can be quantified by considering the amount of label information that remains accessible after encoding the data, and is known as sufficiency of z for y <ref type="bibr" target="#b0">(Achille &amp; Soatto, 2018)</ref>: Definition 1. Sufficiency: A representation z of x is sufficient for y if and only if I(x; y|z) = 0.</p><p>Any model that has access to a sufficient representation z must be able to predict y at least as accurately as if it has access to the original data x instead. In fact, z is sufficient for y if and only if the amount of information regarding the task is unchanged by the encoding procedure (see Proposition B.1 in the Appendix): I(x; y|z) = 0 ⇐⇒ I(x; y) = I(y; z).</p><p>(1)</p><p>Among sufficient representations, the ones that result in better generalization for unlabeled data instances are particularly appealing. When x has higher information content than y, some of the information in x must be irrelevant for the prediction task. This can be better understood by subdividing I(x; z) into two components by using the chain rule of mutual information (see Appendix A):</p><formula xml:id="formula_0">I(x; z) = I(x; z|y) superfluous information + I(y; z) predictive information</formula><p>.</p><p>(2)</p><p>Conditional mutual information I(x; z|y) represents the information in z that is not predictive of y, i.e. superfluous information. While I(y; z) determines how much label information is accessible from the representation. Note that this last term is independent of the representation as long as z is sufficient for y (see Equation <ref type="formula">1</ref>). As a consequence, a sufficient representation contains minimal data information whenever I(x; z|y) is minimized.</p><p>Minimizing the amount of superfluous information can be done directly only in supervised settings.</p><p>In fact, reducing I(x; z) without violating the sufficiency constraint necessarily requires making some additional assumptions on the predictive task (see Theorem B.1 in the Appendix). In the next section we describe the basis of our technique, a strategy to safely reduce the information content of a representation even when the label y is not observed, by exploiting redundant information in the form of an additional view on the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">SUFFICIENCY AND ROBUSTNESS IN THE MULTI-VIEW SETTING</head><p>In this section we extend our analysis of sufficiency and minimality to the multi-view setting.</p><p>Intuitively, we can guarantee that z is sufficient for predicting y even without knowing y by ensuring that z maintains all information which is shared by v 1 and v 2 . This intuition relies on a basic assumption of the multi-view environment -that the two views provide the same predictive information. To formalize this we define redundancy. Definition 2. Redundancy: v 1 is redundant with respect to v 2 for y if and only if</p><formula xml:id="formula_1">I(y; v 1 |v 2 ) = 0</formula><p>Intuitively, a view v 1 is redundant for a task whenever it is irrelevant for the prediction of y if v 2 is already observed. Whenever v 1 and v 2 are mutually redundant (v 1 is redundant with respect to v 2 for y, and vice-versa), we can show the following: Corollary 1. Let v 1 and v 2 be two mutually redundant views for a target y and let z 1 be a representation of v 1 . If z 1 is sufficient for v 2 (I(v 1 ; v 2 |z 1 ) = 0) then z 1 is as predictive for y as the joint observation of the two views (I(v 1 v 2 ; y) = I(y; z 1 )).</p><p>In other words, whenever it is possible to assume mutual redundancy, any representation which contains all the information shared by both views (the redundant information) is as predictive as their joint observation.</p><p>By factorizing the mutual information between v 1 and z 1 analogously to Equation <ref type="formula">2</ref>, we can identify two components:</p><formula xml:id="formula_2">I(v 1 ; z 1 ) = I(v 1 ; z 1 |v 2 ) superfluous information + I(v 2 ; z 1 )</formula><p>predictive information for v2</p><p>.</p><p>Since I(v 2 ; z 1 ) has to be maximal if we want the representation to be sufficient for the label, we conclude that I(v 1 ; z 1 ) can be reduced by minimizing I(v 1 ; z 1 |v 2 ). This term intuitively represents the information z 1 contains which is unique to v 1 and is not predictable by observing v 2 . Since we assumed mutual redundancy between the two views, this information must be irrelevant for the predictive task and, therefore, it can be safely discarded. The proofs and formal assertions for the above statements and Corollary 1 can be found in Appendix B.</p><p>The less the two views have in common, the more I(v 1 ; z 1 ) can be reduced without violating sufficiency for the label, and consequently, the more robust the resulting representation. At the extreme, v 1 and v 2 share only label information, in which case we can show that z 1 is minimal for y and our method is identical to the supervised information bottleneck method without needing to access the labels. Conversely, if v 1 and v 2 are identical, then our method degenerates to the InfoMax principle since no information can be safely discarded (see Appendix E).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">THE MULTI-VIEW INFORMATION BOTTLENECK LOSS FUNCTION</head><p>Given v 1 and v 2 that satisfy the mutual redundancy condition for a label y, we would like to define an objective function for the representation z 1 of v 1 that discards as much information as possible without losing any label information. In Section 3.1 we showed that we can obtain sufficiency for y by ensuring that the representation z 1 of v 1 is sufficient for v 2 , and that decreasing I(z 1 ; v 1 |v 2 ) will increase the robustness of the representation by discarding irrelevant information. So we can combine these two requirements using a relaxed Lagrangian objective to obtain the minimal sufficient representation z 1 for v 2 :</p><formula xml:id="formula_3">L 1 (θ; λ 1 ) = I θ (z 1 ; v 1 |v 2 ) − λ 1 I θ (v 2 ; z 1 ),<label>(3)</label></formula><p>where θ denotes the dependency on the parameters of the encoder p θ (z 1 |v 1 ), and λ 1 represents the Lagrangian multiplier introduced by the constrained optimization. Symmetrically, we define a loss L 2 to optimize the parameters ψ of a conditional distribution p ψ (z 2 |v 2 ) that defines a minimal sufficient representation z 2 of the second view v 2 for v 1 :</p><formula xml:id="formula_4">L 2 (ψ; λ 2 ) = I ψ (z 2 ; v 2 |v 1 ) − λ 2 I ψ (v 1 ; z 2 ),<label>(4)</label></formula><p>By defining z 1 and z 2 on the same domain Z and re-parametrizing the Lagrangian multipliers, the average of the two loss functions L 1 and L 2 can be upper bounded as follows:</p><p>Multi-View Single-View</p><formula xml:id="formula_5">v1 v2 p θ (z1|v1) p ψ (z2|v2) ∼ ∼ z1 z2 DSKL Îξ (z1; z2) x v1 v2 p θ (z1|v1) p θ (z2|v2) ∼ ∼ z1 z2 DSKL Îξ (z1; z2) t1 t2 1 Algorithm 1: L MIB (θ, ψ; β, B) if Multi-View then {(v (i) 1 , v (i) 2 )} B i=1 ∼ p(v1, v2); else {x (i) } B i=1 ∼ p(x); {(t (i) 1 , t (i) 2 )} B i=1 ∼ p 2 (t); for i ← 1 to B do v (i) 1 , v (i) 2 ← t (i) 1 (x (i) ), t (i) 2 (x (i) ); end for end if for i ← 1 to B do z (i) 1 , z (i) 2 ∼ p θ (z1|v (i) 1 ), p ψ (z2|v (i) 2 ); L (i) m ← DSKL(p θ (z1|v (i) 1 )||p ψ (z2|v (i) 2 )); end for return β B B i=1 L (i) m − Îξ ({(z (i) 1 , z (i) 2 )} B i=1 )</formula><p>Figure <ref type="figure">1</ref>: Visualization our Multi-View Information Bottleneck model for both multi-view and single-view settings, where Îξ (z 1 ; z 2 ) refers to the sample-based parametric mutual information estimation. Whenever p(v 1 ) and p(v 2 ) have the same distribution, the two encoders can share their parameters.</p><formula xml:id="formula_6">L M IB (θ, ψ; β) = −I θψ (z 1 ; z 2 ) + β D SKL (p θ (z 1 |v 1 )||p ψ (z 2 |v 2 )),<label>(5)</label></formula><p>where D SKL represents the symmetrized KL divergence obtained by averaging the expected value of D KL (p θ (z 1 |v 1 )||p ψ (z 2 |v 2 )) and D KL (p ψ (z 2 |v 2 )||p θ (z 1 |v 1 )) for joint observations of the two views, while the coefficient β defines the trade-off between sufficiency and robustness of the representation, which is a hyper-parameter in this work. The resulting Multi-View Infomation Bottleneck (MIB) model (Equation <ref type="formula" target="#formula_6">5</ref>) is visualized in Figure <ref type="figure">1</ref>, while the batch-based computation of the loss function is summarized in Algorithm 1.</p><p>The symmetrized KL divergence D SKL (p θ (z 1 |v 1 )||p ψ (z 2 |v 2 )) can be computed directly whenever p θ (z 1 |v 1 ) and p ψ (z 2 |v 2 ) have a known density, while the mutual information between the two representations I θψ (z 1 ; z 2 ) can be maximized by using any sample-based differentiable mutual information lower bound. We tried the Jensen-Shannon I JS <ref type="bibr">(Devon Hjelm et al., 2019;</ref><ref type="bibr" target="#b31">Poole et al., 2019)</ref> and the InfoNCE I NCE <ref type="bibr" target="#b44">(van den Oord et al., 2018)</ref> estimators. These both require introducing an auxiliary parameteric model C ξ (z 1 , z 2 ) which is jointly optimized during the training procedure using re-parametrized samples from p θ (z 1 |v 1 ) and p ψ (z 2 |v 2 ). The full derivation for the MIB loss function can be found in Appendix F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">SELF-SUPERVISION AND INVARIANCE</head><p>Our method can also be applied when multiple views are not available by taking advantage of standard data augmentation techniques. This allows learning invariances directly from the augmented data, rather than requiring them to be built into the model architecture.</p><p>By picking a class T of data augmentation functions t : X → W that do not affect label information, it is possible to artificially build views that satisfy mutual redundancy for y. Let t 1 and t 2 be two random variables over T, then v 1 := t 1 (x) and v 2 := t 2 (x) must be mutually redundant for y.</p><p>Since data augmentation functions in T do not affect label information (I(v 1 ; y) = I(v 2 ; y) = I(x; y)), a representation z 1 of v 1 that is sufficient for v 2 must contain same amount of predictive information as x. Formal proofs for this statement can be found in Appendix B.4.</p><p>Whenever the two transformations for the same observation are independent (I(t 1 ; t 2 |x) = 0), they introduce uncorrelated variations in the two views, which will be discarded when creating a representation using our training objective. As an example, if T represents a set of small translations, the two resulting views will differ by a small shift. Since this information is not shared, any z 1 which is optimal according to the MIB objective must discard fine-grained details regarding the position.</p><p>To enable parameter sharing between the encoders, we generate the two views v 1 and v 2 by independently sampling two functions from the same function class T with uniform probability. As a result, t 1 and t 2 will have the same distribution, and so the two generated views will also have the same marginals (p(v 1 ) = p(v 2 )). For this reason, the two conditional distributions p θ (z 1 |v 1 ) and p ψ (z 2 |v 2 ) can share their parameters and only one encoder is necessary. Full (or partial) parameter sharing can be also applied in the multi-view settings whenever the two views have the same (or similar) marginal distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RELATED WORK</head><p>The relationship between our method and past work on representation learning is best described using the Information Plane <ref type="bibr">(Tishby et al., 2000)</ref>. In this setting, each representation z of x for a predictive task y can be characterised by the amount of information regarding the raw observation I(x; z) and the corresponding measure of accessible predictive information I(y; z) (x and y axis respectively on Figure <ref type="figure" target="#fig_0">2</ref>). Ideally, a good representation would be maximally informative about the label while retaining a minimal amount of information from the observations (top left corner of the parallelogram). Further details on the Information Plane and the bounds visualized in Figure <ref type="figure" target="#fig_0">2</ref>  Thanks to recent progress in mutual information estimation <ref type="bibr" target="#b30">(Nguyen et al., 2008;</ref><ref type="bibr" target="#b21">Ishmael Belghazi et al., 2018;</ref><ref type="bibr" target="#b31">Poole et al., 2019)</ref>, the InfoMax principle <ref type="bibr">(Linsker, 1988)</ref> has gained attention for unsupervised representation learning <ref type="bibr">(Devon Hjelm et al., 2019;</ref><ref type="bibr" target="#b44">van den Oord et al., 2018)</ref>. Since the InfoMax objective involves maximizing I(x; z), the resulting representation aims to preserve all the information regarding the raw observations (top right corner in Figure <ref type="figure" target="#fig_0">2</ref>).</p><p>Concurrent work has applied the InfoMax principle in the Multi-View setting <ref type="bibr" target="#b22">(Ji et al., 2019;</ref><ref type="bibr" target="#b16">Hénaff et al., 2019;</ref><ref type="bibr" target="#b40">Tian et al., 2019;</ref><ref type="bibr" target="#b4">Bachman et al., 2019)</ref>, aiming to maximize mutual information between the representation z of a first data-view x and a second one v 2 . The target representation for the Multi-View InfoMax (MV-InfoMax) models should contain at least the amount of information in x that is predictive for v 2 , targeting the region I(z; x) ≥ I(x; v 2 ) on the Information Plane (purple dotted line in Figure <ref type="figure" target="#fig_0">2</ref>). Since the MV-InfoMax has no incentive to discard any information regarding x from z, a representation that is optimal according to the InfoMax principle is also optimal for any MV-InfoMax model. Our model with β = 0 (Equation <ref type="formula" target="#formula_6">5</ref>) belongs to this family of objectives since the incentive to remove superfluous information is removed. Despite their success, Tschannen et al. ( <ref type="formula">2019</ref>) has shown that the effectiveness of the InfoMax models is due to inductive biases introduced by the architecture and estimators rather than the training objective itself, since the InfoMax and MV-InfoMax objectives can be trivially maximized by using invertible encoders.</p><p>On the other hand, Variational Autoencoders (VAEs) (Kingma &amp; Welling, 2014) define a training objective that balances compression and reconstruction error <ref type="bibr" target="#b2">(Alemi et al., 2018)</ref> through an hyper-parameter β. Whenever β is close to 0, the VAE objective aims for a lossless representation, approaching the same region of the Information Plane as the one targeted by InfoMax <ref type="bibr" target="#b5">(Barber &amp; Agakov, 2003)</ref>. When β approaches large values, the representation becomes more compressed, showing increased generalization and disentanglement <ref type="bibr" target="#b18">(Higgins et al., 2017;</ref><ref type="bibr" target="#b7">Burgess et al., 2018)</ref>, and, as β approaches infinity, I(z; x) goes to zero. During this transition from low to high β, however, there are no guarantees that VAEs will retain label information (Theorem B.1 in the Appendix). The path between the two regimes depends on how well the label information aligns with the induc-tive bias introduced by encoder <ref type="bibr" target="#b23">(Jimenez Rezende &amp; Mohamed, 2015;</ref><ref type="bibr" target="#b25">Kingma et al., 2016</ref><ref type="bibr">), prior (Tomczak &amp; Welling, 2018)</ref> and decoder architectures <ref type="bibr" target="#b15">(Gulrajani et al., 2017;</ref><ref type="bibr" target="#b8">Chen et al., 2017)</ref>.</p><p>The idea of discarding irrelevant information was introduced in <ref type="bibr">Tishby et al. (2000)</ref> and identified as one of the possible reasons behind the generalization capabilities of deep neural networks by <ref type="bibr" target="#b41">Tishby &amp; Zaslavsky (2015)</ref> and <ref type="bibr" target="#b0">Achille &amp; Soatto (2018)</ref>. Representations based on the information bottleneck principle explicitly minimize the amount of superfluous information in the representation while retaining all the label information from the data (top-left corner of the Information Plane in Figure <ref type="figure" target="#fig_0">2</ref>). This direction of research has been explored for both single-view <ref type="bibr" target="#b2">(Alemi et al., 2018)</ref> and multi-view settings <ref type="bibr" target="#b45">(Wang et al., 2019)</ref>, even if explicit label supervision is required to train the representation z.</p><p>In contrast to all of the above, our work is the first to explicitly identify and discard superfluous information from the representation in the unsupervised multi-view setting. This is because unsupervised models based on the β-VAE objective remove information indiscriminately without identifying which part is relevant for teh predictive task, and the InfoMax and Multi-View InfoMax methods do not explicitly try to remove superfluous information at all. The MIB objective, on the other hand, results in the representation with the least superfluous information, i.e. the most robust among the representations that are optimal according to Multi-View InfoMax, without requiring any additional label supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In this section we demonstrate the effectiveness of our model against state-of-the-art baselines in both the multi-view and single-view setting. In the single-view setting, we also estimate the coordinates on the Information Plane for each of the baseline methods as well as our method to validate the theory in Section 3.</p><p>The results reported in the following sections are obtained using the Jensen-Shannon I JS <ref type="bibr">(Devon Hjelm et al., 2019;</ref><ref type="bibr" target="#b31">Poole et al., 2019)</ref> estimator, which resulted in better performance for MIB and the other InfoMax-based models (Table <ref type="table" target="#tab_2">2</ref> in the supplementary material). In order to facilitate the comparison between the effect of the different loss functions, the same estimator is used across the different models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">MULTI-VIEW TASKS</head><p>We compare MIB on the sketch-based image retrieval <ref type="bibr" target="#b34">(Sangkloy et al., 2016)</ref> and Flickr multiclass image classification <ref type="bibr" target="#b20">(Huiskes &amp; Lew, 2008)</ref> tasks with domain specific and prior multi-view learning methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">SKETCH-BASED IMAGE RETRIEVAL</head><p>Dataset. The Sketchy dataset <ref type="bibr" target="#b34">(Sangkloy et al., 2016)</ref> consists of 12,500 images and 75,471 handdrawn sketches of objects from 125 classes. As in <ref type="bibr" target="#b27">Liu et al. (2017)</ref>, we also include another 60,502 images from the ImageNet <ref type="bibr" target="#b9">(Deng et al., 2009)</ref> from the same classes, which results in total 73,002 natural object images. As per the experimental protocol of <ref type="bibr" target="#b48">Zhang et al. (2018)</ref>, a total of 6,250 sketches (50 sketches per category) are randomly selected and removed from the training set for testing purpose, which leaves 69,221 sketches for training the model.</p><p>Experimental Setup. The sketch-based image retrieval task is a ranking of 73,002 natural images according to the unseen test (query) sketch. Retrieval is done for our model by generating representations for the query sketch as well as all natural images, and ranking the image by the euclidean distance of their representation from the sketch representation. The baselines use various domain specific ranking methodologies. Model performance is computed based on the class of the ranked pictures corresponding to the query sketch. The training set consists of pairs of image v 1 and sketch v 2 randomly selected from the same class, to ensure that both views contain the equivalent label information (mutual redundancy).</p><p>Following recent work <ref type="bibr" target="#b48">(Zhang et al., 2018;</ref><ref type="bibr" target="#b12">Dutta &amp; Akata, 2019)</ref>, we use features extracted from images and sketches by a VGG <ref type="bibr" target="#b35">(Simonyan &amp; Zisserman, 2014)</ref> architecture trained for classification</p><formula xml:id="formula_7">v1 ∈ R 4096 v2 ∈ R 4096 y ∈ [125]</formula><p>"cat" "apple" Method mAP@all Prec@200 SaN <ref type="bibr" target="#b47">(Yu et al., 2017)</ref> 0.208 0.292 GN Triplet <ref type="bibr" target="#b34">(Sangkloy et al., 2016)</ref> 0.529 0.716 Siamese CNN <ref type="bibr" target="#b32">(Qi et al., 2016)</ref> 0.481 0.612 Siamese-AlexNet <ref type="bibr" target="#b27">(Liu et al., 2017)</ref>   Results. Table <ref type="table">5</ref>.1.1 shows that the our model achieves strong performance for both mean average precision (mAP@all) and precision at 200 (Prec@200), suggesting that the representation is able to capture the common class information between the paired pictures and sketches. The effectiveness of MIB on the retrieval task can be mostly attributed to the regularization introduced with the symmetrized KL divergence between the two encoded views. In addition to discarding view-private information, this term actively aligns the representations of v 1 and v 2 , making the MIB model especially suitable for retrieval tasks</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">MIR-FLICKR</head><p>Dataset. The MIR-Flickr dataset <ref type="bibr" target="#b20">(Huiskes &amp; Lew, 2008)</ref> consists of 1M images annotated with 800K distinct user tags. Each image is represented by a vector of 3,857 hand-crafted image features (v 1 ), while the 2,000 most frequent tags are used to produce a 2000-dimensional multi-hot encoding (v 2 ) for each picture. The dataset is divided into labeled and unlabeled sets that respectively contain 975K and 25K images, where the labeled set also contains 38 distinct topic classes together with the user tags. Training images with less than two tags are removed, which reduces the total number of training samples to 749,647 pairs <ref type="bibr" target="#b36">(Sohn et al., 2014;</ref><ref type="bibr" target="#b46">Wang et al., 2016)</ref>. The labeled set contains 5 different splits of train, validation and test sets of size 10K/5K/10K respectively.</p><p>Experimental Setup. Following standard procedure in the literature <ref type="bibr" target="#b37">(Srivastava &amp; Salakhutdinov, 2014;</ref><ref type="bibr" target="#b46">Wang et al., 2016)</ref>, we train our model on the unlabeled pairs of images and tags. Then a multi-label logistic classifier is trained from the representation of 10K labeled train images to the corresponding macro-categories. The quality of the representation is assessed based on the performance of the trained logistic classifier on the labeled test set. Each encoder consists of a multi-layer perceptron of 4 hidden layers with ReLU activations learning two 1024-dimensional representations z 1 and z 2 for images v 1 and tags v 2 respectively. Examples of the two views, labels, and further details on the training procedure are in Appendix G.</p><p>Results. Our MIB model is compared with other popular multi-view learning models in Figure <ref type="figure" target="#fig_2">3</ref> for β = 0 (Multi-View InfoMax), β = 1 and β = 10 −3 (best on validation set). Although the tuned MIB performs similarly to Multi-View InfoMax with a large number of labels, it outperforms it when fewer labels are available. Furthermore, by choosing a larger β the accuracy of our model drastically increases in scarce label regimes, while slightly reducing the accuracy when all the labels are observed (see right side of Figure <ref type="figure" target="#fig_2">3</ref>). This effect is likely due to a violation of the mutual Published as a conference paper at ICLR 2020  redundancy constraint (see Figure <ref type="figure" target="#fig_4">6</ref> in the supplementary material) which can be compensated with smaller values of β for less aggressive compression.</p><p>A possible reason for the effectiveness of MIB against some of the other baselines may be its ability to use mutual information estimators that do not require reconstruction. Both Multi-View VAE (MVAE) and Deep Variational CCA (VCCA) rely on a reconstruction term to capture cross-modal information, which can introduce bias that decreases performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">SELF-SUPERVISED SINGLE-VIEW TASK</head><p>In this section, we compare the performance of different unsupervised learning models by measuring their data efficiency and empirically estimating the coordinates of their representation on the Information Plane. Since accurate estimation of mutual information is extremely expensive <ref type="bibr" target="#b28">(McAllester &amp; Stratos, 2018)</ref>, we focus on relatively small experiments that aim to uncover the difference between popular approaches for representation learning.</p><p>Dataset. The dataset is generated from MNIST by creating the two views, v 1 and v 2 , via the application of data augmentation consisting of small affine transformations and independent pixel corruption to each image. These are kept small enough to ensure that label information is not effected. Each pair of views is generated from the same underlying image, so no label information is used in this process (details in Appendix G).</p><p>Experimental Setup. To evaluate, we train the encoders using the unlabeled multi-view dataset just described, and then fix the representation model. A logistic regression model is trained using the resulting representations along with a subset of labels for the training set, and we report the accuracy of this model on a disjoint test set as is standard for the unsupervised representation learning literature <ref type="bibr" target="#b43">(Tschannen et al., 2019;</ref><ref type="bibr" target="#b40">Tian et al., 2019;</ref><ref type="bibr" target="#b44">van den Oord et al., 2018)</ref>. We estimate I(x; z) and I(y; z) using mutual information estimation networks trained from scratch on the final representations using batches of joint samples {(x (i) , y (i) , z (i) )} B i=1 ∼ p(x, y)p θ (z|x). All models are trained using the same encoder architecture consisting of 2 layers of 1024 hidden units with ReLU activations, resulting in 64-dimensional representations. The same data augmentation procedure was also applied for single-view architectures and models were trained for 1 million iterations with batch size B = 64.</p><p>Results. Figure <ref type="figure" target="#fig_3">4</ref> summarizes the results. The empirical measurements of mutual information reported on the Information Plane are consistent with the theoretical analysis reported in Section 4: models that retain less information about the data while maintaining the maximal amount of predictive information, result in better classification performance at low-label regimes, confirming the hypothesis that discarding irrelevant information yields robustness and more data-efficient represen- tations. Notably, the MIB model with β = 1 retains almost exclusively label information, hardly decreasing the classification performance when only one label is used for each data point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS AND FUTURE WORK</head><p>In this work, we introduce Multi-View Information Bottleneck, a novel method for taking advantage of multiple data-views to produce robust representations for downstream tasks. In our experiments, we compared MIB empirically against other approaches in the literature on three such tasks: sketchbased image retrieval, multi-view and unsupervised representation learning. The strong performance obtained in the different areas show that Multi-View Information Bottleneck can be practically applied to various tasks for which the paired observations are either readily available or artificially produced. Furthermore, the positive results on the MIR-Flickr dataset show that our model can work well in practice even when mutual redundancy holds only approximately.</p><p>There are multiple extensions that we would like to explore in future work. One interesting direction would be considering more than two views. In Appendix D we discuss why the mutual redundancy condition cannot be trivially extended to more than two views, but we still believe such an extension is possible. Secondly, we believe that exploring the role played by different choices of data augmentation could bridge the gap between the Information Bottleneck principle and the literature on invariant neural networks <ref type="bibr" target="#b6">(Bloem-Reddy &amp; Whye Teh, 2019)</ref>, which are able to exploit known symmetries and structure of the data to remove superfluous information.</p><p>Corollary B.1.1. Let z be a representation of x that discards observational information. There is always a label y for which a z is not predictive, while the original observations are. Hypothesis:</p><formula xml:id="formula_8">(H 1 ) x is discrete (H 2 ) z discards information regarding x: I(z ; x) &lt; H(x)</formula><p>Thesis:</p><p>(T 1 ) ∃y.I(y; x) &gt; I(y; z ) = 0</p><p>Proof. By construction using Theorem B.1.</p><p>1. Set z = x:</p><formula xml:id="formula_9">(C 1 ) I(x; z) (P5) = H(x) − H(x|z) (H1) = H(x) 2. I(z ; x) &lt; H(x) (C1) =⇒ I(z ; x) &lt; I(x; z)</formula><p>Since the hypothesis are met, we conclude that there exist y such that I(y; x) &gt; I(y; z ) = 0</p><formula xml:id="formula_10">B.3 MULTI-VIEW B.3.1 MULTI-VIEW REDUNDANCY AND SUFFICIENCY Proposition B.2. Let v 1 , v 2</formula><p>, y be random variables with joint distribution p(v 1 , v 2 , y). Let z 1 be a representation of v 1 , then:</p><formula xml:id="formula_11">I(v 1 ; y|z 1 ) ≤ I(v 1 ; v 2 |z 1 ) + I(v 1 ; y|v 2 )</formula><p>Hypothesis:</p><p>(H 1 ) z 1 is a representation of v 1 : I(y; z 1 |v 2 v 1 ) = 0</p><p>Thesis:</p><formula xml:id="formula_12">(T 1 ) I(v 1 ; y|z 1 ) ≤ I(v 1 ; v 2 |z 1 ) + I(v 1 ; y|v 2 ) Proof. Since z 1 is a representation of v 1 : (C 1 ) I(y; z 1 |v 2 v 1 ) = 0</formula><p>Therefore:</p><formula xml:id="formula_13">I(v 1 ; y|z 1 ) (P3) = I(v 1 ; y|z 1 v 2 ) + I(v 1 ; v 2 ; y|z 1 ) (P3) = I(v 1 ; y|v 2 ) − I(v 1 ; y; z 1 |v 2 ) + I(v 1 ; v 2 ; y|z 1 ) (P3) = I(v 1 ; y|v 2 ) − I(y; z 1 |v 2 ) + I(y; z 1 |v 2 v 1 ) + I(v 1 ; v 2 ; y|z 1 ) (P1) ≤ I(v 1 ; y|v 2 ) + I(y; z 1 |v 2 v 1 ) + I(v 1 ; v 2 ; y|z 1 ) (H1) = I(v 1 ; y|v 2 ) + I(v 1 ; v 2 ; y|z 1 ) (P3) = I(v 1 ; y|v 2 ) + I(v 1 ; v 2 |z 1 ) − I(v 1 ; v 2 |z 1 y) (P1) ≤ I(v 1 ; y|v 2 ) + I(v 1 ; v 2 |z 1 )</formula><p>• v 1 and v 2 are mutually redundant for y</p><p>• v 2 and v 3 are mutually redundant for y</p><p>Then, we show that v 1 is not necessarily mutually redundant with respect to v 3 for y.</p><p>Let v 1 , v 2 and v 3 be fair and independent binary random variables. Defining y as the exclusive or operator applied to v 1 and v 3 ( y := v 1 XOR v 3 ), we have that I(v 1 ; y) = I(v 3 ; y) = 0. In this settings, v 1 and v 2 are mutually redundant for y:</p><formula xml:id="formula_14">I(v 1 ; y|v 2 ) = H(v 1 |v 2 ) − H(v 1 |v 2 y) = H(v 1 ) − H(v 1 ) = 0 I(v 2 ; y|v 1 ) = H(v 2 |v 1 ) − H(v 2 |v 1 y) = H(v 2 ) − H(v 2 ) = 0</formula><p>Analogously, v 2 and v 3 are also mutually redundant for y as the three random variables are not predictive for each other. Nevertheless, v 1 and v 3 are not mutually redundant for y:</p><formula xml:id="formula_15">I(v 1 ; y|v 3 ) = H(v 1 |v 3 ) − H(v 1 |v 3 y) 0 = H(v 1 ) = 1 I(v 3 ; y|v 1 ) = H(v 3 |v 1 ) − H(v 3 |v 1 y) 0 = H(v 3 ) = 1 Where H(v 1 |v 3 y) = H(v 3 |v 1 y) = 0 follows from v 1 = v 3 XOR y and v 3 = v 1 XOR y, while H(v 1 ) = H(v 3 ) = 1 holds by construction.</formula><p>This counter-intuitive higher order interaction between multiple views makes our theory non-trivial to generalize to more than two views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E EQUIVALENCES OF DIFFERENT OBJECTIVES</head><p>Different objectives in literature can be seen as a special case of the Multi-View Information Bottleneck principle. In this section we show that the supervised version of Information Bottleneck is equivalent to the corresponding Multi-View version whenever the two redundant views have only label information in common. A second subsection show equivalence between InfoMax and Multi-View Information Bottleneck whenever the two views are identical.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 MULTI-VIEW INFORMATION BOTTLENECK AND SUPERVISED INFORMATION BOTTLENECK</head><p>Whenever the two mutually redundant views v 1 and v 2 have only label information in common (or when one of the two views is the label itself) the Multi-View Information Bottleneck objective is equivalent to the respective supervised version. This can be shown by proving that I(v 1 ; z 1 |v 2 ) = I(v 1 ; z 1 |y), i.e. a representation z 1 of v 1 that is sufficient and minimal for v 2 is also sufficient and minimal for y. Proposition E.1. Let v 1 and v 2 be mutually redundant views for a label y that share only label information. Then a sufficient representation z 1 of v 1 for v 2 that is minimal for v 2 is also a minimal representation for y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hypothesis:</head><p>(H 1 ) v 1 and v 2 are mutually redundant for y: I(v 1 ; y|v 2 ) + I(v 2 ; y|v 1 ) = 0 (H 2 ) v 1 and v 2 share only label information:</p><formula xml:id="formula_16">I(v 1 ; v 2 ) = I(v 1 ; y) (H 3 ) z 1 is sufficient for v 2 : I(v 1 ; v 2 |z 1 ) = 0</formula><p>Thesis:</p><formula xml:id="formula_17">(T 1 ) I(v 1 ; z 1 |v 2 ) = I(v 1 ; z 1 |y)</formula><p>Proof.</p><p>1. Consider I(v 1 ; z):</p><formula xml:id="formula_18">I(v 1 ; z 1 ) (P3) = I(v 1 ; z 1 |v 2 ) + I(v 1 ; v 2 ; z 1 ) (P3) = I(v 1 ; z 1 |v 2 ) + I(v 1 ; v 2 ) − I(v 1 ; v 2 |z 1 ) (H3) = I(v 1 ; z 1 |v 2 ) + I(v 1 ; v 2 ) (H1) = I(v 1 ; z 1 |v 2 ) + I(v 1 ; y)</formula><p>2. Using Corollary 1, from (H 2 ) and (H 3 ) follows I(v 1 ; y|z 1 ) = 0 3. I(v 1 ; z) can be alternatively expressed as:</p><formula xml:id="formula_19">I(v 1 ; z 1 ) (P3) = I(v 1 ; z 1 |y) + I(v 1 ; y; z 1 ) (P3) = I(v 1 ; z 1 |y) + I(v 1 ; y) − I(v 1 ; y|z 1 ) (Cor1) = I(v 1 ; z 1 |y) + I(v 1 ; y)</formula><p>Equating 1 and 3, we conclude</p><formula xml:id="formula_20">I(v 1 ; z 1 |v 2 ) = I(v 1 ; z 1 |y), therefore z 1 which minimizes I(v 1 ; z 1 |v 2 ) is also minimizing I(v 1 ; z 1 |y). When I(v 1 ; z 1 |y) is minimal, I(y; z 1 ) is also minimal (see equation 2).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 MULTI-VIEW INFORMATION BOTTLENECK AND INFOMAX</head><p>Whenever v 1 = v 2 , a representation z 1 of v 1 that is sufficient for v 2 must contain all the original information regarding v 1 . Furthermore since I(v 1 ; z 1 |v 2 ) = 0 for every representation, no superfluous information can be identified and removed. As a consequence, a minimal sufficient representation z 1 of v 1 for v 2 is any representation for which mutual information is maximal, hence InfoMax.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F LOSS COMPUTATION</head><p>Starting from Equation 3, we consider the average of the losses L 1 (θ; λ 1 ) and L 2 (ψ; λ 2 ) that aim to create the minimal sufficient representations z 1 and z 2 respectively:</p><formula xml:id="formula_21">(P2) = I θψ (z 1 ; z 2 v 2 ) − I θψ (z 1 ; z 2 |v 2 ) = * I θψ (z 1 ; z 2 v 2 ) = I θψ (z 1 ; z 2 ) + I θψ (z 1 ; v 2 |z 2 ) ≥ I θψ (z 1 ; z 2 )</formula><p>Where * follows from z 2 representation of v 2 . The bound reported in this equation is tight whenever z 2 is sufficient for z 1 (I θψ (z 1 ; v 2 |z 2 ) = 0). This happens whenever z 2 contains all the information regarding z 1 (and therefore v 1 ). Once again, the same bound can symmetrically be used to show I θ (z 2 ; v 1 ) ≥ I θψ (z 1 ; z 2 ). Therefore, the loss function in Equation 6 can be upper-bounded with:</p><formula xml:id="formula_22">L 1+2 2 (θ, ψ; λ 1 , λ 2 ) ≤ D SKL (p θ (z 1 |v 1 )||p ψ (z 2 |v 2 )) − λ 1 + λ 2 2 I θψ (z 1 ; z 2 )<label>(7)</label></formula><p>Where:</p><formula xml:id="formula_23">D SKL (p θ (z 1 |v 1 )||p ψ (z 2 |v 2 )) := 1 2 D KL (p θ (z 1 |v 1 )||p ψ (z 2 |v 2 )) + 1 2 D KL (p ψ (z 2 |v 2 )||p θ (z 1 |v 1 ))</formula><p>Lastly, multiplying both terms with β := 2 λ1+λ2 and re-parametrizing the objective, we obtain:</p><formula xml:id="formula_24">L M IB (θ, ψ; β) = −I θψ (z 1 ; z 2 ) + β D SKL (p θ (z 1 |v 1 )||p ψ (z 2 |v 2 ))<label>(8)</label></formula><p>G EXPERIMENTAL PROCEDURE AND DETAILS</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.1 MODELING</head><p>The two stochastic encoders p θ (z 1 |v 1 ) and p ψ (z 2 |v 2 ) are modeled by Normal distributions parametrized with neural networks (µ θ , σ 2 θ ) and (µ ψ , σ 2 ψ ) respectively:</p><formula xml:id="formula_25">p θ (z 1 |v 1 ) := N z 1 |µ θ (v 1 ), σ 2 θ (v 1 ) p ψ (z 2 |v 2 ) := N z 2 |µ ψ (v 2 ), σ 2 ψ (v 2 )</formula><p>Since the density of the two encoders can be evaluated, the symmetrized KL-divergence in equation 4 can be directly computed. On the other hand, I θψ (z 1 ; z 2 ) requires the use of a mutual information estimator.</p><p>To facilitate the optimization, the hyper-parameter β is slowly increased during training, starting from a small value ≈ 10 −4 to its final value with an exponential schedule. This is because the mutual information estimator is trained together with the other architectures and, since it starts from a random initialization, it requires an initial warm-up. Starting with bigger β results in the encoder collapsing into a fixed representation. The update policy for the hyper-parameter during training has not shown strong influence on the representation, as long as the mutual information estimator network has reached full capacity.</p><p>All the experiments have been performed using the Adam optimizer with a learning rate of 10 −4 for both encoders and the estimation network. Higher learning rate can result in instabilities in the training procedure. The results reported in the main text relied on the Jensen-Shannon mutual information estimator <ref type="bibr">(Devon Hjelm et al., 2019)</ref> since the InfoNCE counterpart <ref type="bibr" target="#b44">(van den Oord et al., 2018)</ref> generally resulted in worse performance that could be explained by the effect of the factorization of the critic network <ref type="bibr" target="#b31">(Poole et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2 SKETCHY EXPERIMENTS</head><p>• Input: The two views for the sketch-based classification task consist of 4096 dimensional sketch and image features extracted from two distinct VGG-16 network models which were pre-trained on images and sketches from the TU-Berlin dataset <ref type="bibr" target="#b13">Eitz et al. (2012)</ref> for endto-end classification. The feature extractors are frozen during the training procedure of for the two representations. Each training iteration used batches of size B = 128.</p><p>• Encoder and Critic architectures: Both sketch and image encoders consist of multi-layer perceptrons of 2 hidden ReLU units of size 2,048 and 1,024 respectively with an output of size 2x64 that parametrizes mean and variance for the two Gaussian posteriors. The critic architecture also consists of a multi layer perceptron of 2 hidden ReLU units of size 512.</p><p>• β update policy: The initial value of β is set to 10 −4 . Starting from the 10,000 th training iteration, the value of β is exponentially increased up to 1.0 during the following 250,000 training iterations. The value of β is then kept fixed to one until the end of the training procedure (500,000 iterations).</p><p>• Evaluation: All natural images are used as both training sets and retrieval galleries. The 64 dimensional real outputs of sketch and image representation are compared using Euclidean distance. For having a fair comparison other methods that rely on binary hashing <ref type="bibr" target="#b27">(Liu et al., 2017;</ref><ref type="bibr" target="#b48">Zhang et al., 2018)</ref>, we used Hamming distance on a binarized representation (obtained by applying iterative quantization <ref type="bibr" target="#b14">Gong et al. (2013)</ref> on our real valued representation). We report the mean average precision (mAP@all) and precision at toprank 200 (Prec@200) <ref type="bibr" target="#b38">Su et al. (2015)</ref> on both the real and binary representation to evaluate our method and compare it with prior works. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.3 MIR-FLICKR EXPERIMENTS</head><formula xml:id="formula_26">v 1 ∈ R 3857 v 2 ∈ {0, 1} 2000 y ∈ {0, 1}<label>38</label></formula><p>"watermelon", "hilarious", "chihuahua", "dog" "animals", "dog", "food" "colors", "cores", "centro", "comercial", "building" "clouds", "sky", "structures"</p><p>• Input: Whitening is applied to the handcrafted image features. Batches of size B = 128 are used for each update step.</p><p>• Encoders and architectures: The two encoders consists of a multi layer perceptron of 4 hidden ReLU units of size 1,024, which exactly resemble the architecture used in <ref type="bibr" target="#b46">Wang et al. (2016)</ref>. Both representations z 1 and z 2 have a size of 1,024, therefore the two architecture output a total of 2x1,024 parameters that define mean and variance of the respective factorized Gaussian posterior. Similarly to the Sketchy experiments, the critic is consists of a multi-layer perceptron of 2 hidden ReLU units of size 512.</p><p>• β update policy: The initial value of β is set to 10 −8 . Starting from 150000 th iteration, β is set to exponentially increase up to 1.0 (and 10 −3 ) during the following 150,000 iterations.</p><p>• Evaluation: Once the models are trained on the unlabeled set, the representation of the 25,000 labeled images is computed. The resulting vectors are used for training and evaluating a multi-label logistic regression classifier on the respective splits. The optimal parameters (such as β) for our model are chosen based on the performance on the validation set. In Table <ref type="table">3</ref>, we report the aggregated mean of the 5 test splits as the final value mean average precision value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.4 MNIST EXPERIMENTS</head><p>• Input: The two views v 1 and v 2 for the MNIST dataset are generated by applying small translation ([0-10]%), rotation <ref type="bibr">([-15,15]</ref> degrees), scale ([90,110]%), shear <ref type="bibr">([-15,15]</ref> degrees) and pixel corruption (20%). Batches of size B = 64 samples are used during training.</p><p>• Encoders, Decoders and Critic architectures: All the encoders used for the MNIST experiments consist of neural networks with two hidden layers of 1,024 units and ReLU activations, producing a 2x64-dimensional parameter vector that is used to parameterize mean and variance for the Gaussian posteriors. The decoders used for the VAE experiments also consist of the networks of the same size. Similarly, the critic architecture used for mutual information estimation consists of two hidden layers of 1,204 units each and ReLU activations.</p><p>• β update policy: The initial value of β is set to 10 −3 , which is increased with an exponential schedule starting from the 50,000 th until 1the 50,000 th iteration. The value of β is then kept constant until the 1,000,000 th iteration. The same annealing policy is used to trained the different β-VAEs reported in this work.</p><p>• Evaluation: The trained representation are evaluated following the well-known protocol described in Tschannen et al. ( <ref type="formula">2019</ref> The Jensen-Shannon mutual information lower bound is maximized during training, while the numerical estimation are computed using an energy-based bound <ref type="bibr" target="#b31">(Poole et al., 2019;</ref><ref type="bibr">Devon Hjelm et al., 2019)</ref>. The final values for I(x; z) and I(y; z) are computed by averaging the mutual information estimation on the whole dataset. In order to reduce the variance of the estimator, the lowest and highest 5% are removed before averaging. This practical detail makes the estimation more consistent and less susceptible to numerical instabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.4.1 RESULTS AND VISUALIZATION</head><p>In this section we include additional quantitative results and visualizations which refer to the singleview MNIST experiments reported in section 5.2.   <ref type="figure" target="#fig_3">4</ref>. Both the results obtained using the Jensen-Shannon I JSD <ref type="bibr">(Devon Hjelm et al., 2019;</ref><ref type="bibr" target="#b31">Poole et al., 2019)</ref> and the InfoNCE I NCE <ref type="bibr" target="#b44">(van den Oord et al., 2018)</ref> estimators are reported.</p><p>Figure <ref type="figure" target="#fig_6">7</ref> reports the linear projection of the embedding obtained using the MIB model. The latent space appears to roughly consists of ten clusters which corresponds to the different digits. This observation is consistent with the empirical measurement of input and label information I(x; z) ≈ I(z; y) ≈ log 10, and the performance of the linear classifier in scarce label regimes. As the cluster are distinct and concentrated around the respective centroids, 10 labeled examples are sufficient to align the centroid coordinates with the digit labels. to the top-left. When the amount of corruption approaches 100%, the mutual redundancy assumption is clearly violated, and the performances of MIB deteriorate. In the initial part of the transitions between the two regimes (which corresponds to extremely low probability of corruption) the MIB models drops some label information that is quickly re-gained when pixel corruption becomes more frequent. We hypothesize that this behavior is due to a problem with the optimization procedure, since the corruption are extremely unlikely, the Monte-Carlo estimation for the symmetrized Kullback-Leibler divergence is more biased. Using more examples of views produced from the same data-point within the same batch could mitigate this issue.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Information Plane determined by I(x; z) (x-axis) and I(y; z) (y-axis). Different objectives are compared based on their target.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>) and comparison between MIB and other popular models in literature on the sketch-based image retrieval task (on the right). * denotes models that use a 64-bits binary representation. The results for MIB corresponds to β = 1.on the TU-Berlin dataset<ref type="bibr" target="#b13">(Eitz et al., 2012)</ref>. The resulting flattened 4096-dimensional feature vectors are fed to our image and sketch encoders to produce a 64-dimensional representation. Both encoders consist of neural networks with hidden layers of 2048 and 1024 units respectively. Size of the representation and regularization strength β are tuned on a validation sub-split. We evaluate MIB on five different train/test splits and report mean and standard deviation in Table5.1.1. Further details on our training procedure and architecture are in Appendix G.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Left: mean average precision (mAP) of the classifier trained on different multi-view representations for the MIR-Flickr task. Right: comparing the performance for different values of β and percentages of given labeled examples (from 1% up to 100%). Each model uses encoders of comparable size, producing a 1024 dimensional representation. † results from Wang et al. (2016).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure4: Comparing the representations obtained with different objectives on MNIST dataset. The empirical estimation of the coordinates on the Information Plane (in nats on the left) is followed by the respective classification accuracy for different number of randomly sampled labels (from 1 example per label up to 6000 examples per label). Representations that discard more observational information tend to perform better in scarce label regimes. The measurements used to produce the two graphs are reported in Appedix G.4.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Examples of pictures v 1 , tags v 2 and category labels y for the MIR-Flickr dataset (Srivastava &amp; Salakhutdinov, 2014). As visualized is the second row, the tags are not always predictive of the label. For this reason, the mutual redundancy assumption holds only approximately.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>);<ref type="bibr" target="#b40">Tian et al. (2019)</ref>;<ref type="bibr" target="#b4">Bachman et al. (2019)</ref>;<ref type="bibr" target="#b44">van den Oord et al. (2018)</ref>. Each logistic regression is trained 5 different balanced splits of the training set for different percentages of training examples, ranging from 1 example per label to the whole training set. The accuracy reported in this work has been computed on the disjoint test set. Mean and standard deviation are computed according to the 5 different subsets used for training the logistic regression. Mean and variance for the mutual information estimation reported on the Information Plane (Figure4) are computed by training two estimation networks from scratch on the final representation of the non-augmented train set. The two estimation architectures consist of 2 hidden layers of 2048 and 1024 units each, and have been trained with batches of size B = 256 for a total of approximately 25,000 iterations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Linear projection of the embedding obtained by applying the MIB encoder to the MNIST test set. The 64 dimensional representation is projected onto the two principal components. Different colors are used to represent the 10 digit classes.</figDesc><graphic url="image-12.png" coords="25,195.86,81.86,158.40,158.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Visualization of the coordinates on the Information Plane (plot on the left) and prediction accuracy (center and right) for the MV-InfoMax and MIB objectives with different amount of training labels and corruption percentage used for data-augmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Visualization of the coordinates on the Information Plane (plot on the left) and prediction accuracy (center and right) for the β-VAE, Multi-View InfoMax and Multi-View Information Bottleneck objectives with different amount of training labels and different values of the respective hyperparameter β.</figDesc><graphic url="image-14.png" coords="26,254.96,182.65,247.14,109.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Examples of the two views and class label from the Sketchy dataset (on the left</figDesc><table><row><cell>Triplet-AlexNet (Liu et al., 2017) DSH  *  (Liu et al., 2017) GDH  *  (Zhang et al., 2018)</cell><cell>0.518 0.573 0.711 0.810</cell><cell>0.690 0.761 0.866 -</cell></row><row><cell>MV-InfoMax 2 MIB MIB  *  (64-bits)</cell><cell cols="2">0.008 0.856±0.005 0.851± 0.004 0.834±0.003 0.008 0.848±0.005</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>reports the quantitative results used for to produce the visualizations reported in Figure4, including the comparison between the performance resulting from different mutual information estimators. As the Jensen-Shannon estimator generally resulted in better performance for the InfoMax, MV-InfoMax and MIB models, all the experiments reported on the main text make use of this estimator. Note that the InfoMax model with the I JS estimator is equivalent to the global model reported in Devon<ref type="bibr" target="#b11">Hjelm et al. (2019)</ref>, while MV-InfoMax with the I NCE estimator results in a similar architecture to the one introduced in<ref type="bibr" target="#b40">Tian et al. (2019)</ref>.</figDesc><table><row><cell>Model VAE (beta=0) VAE (beta=4) VAE (beta=8) InfoMax (INCE) InfoMax (IJS) MV-InfoMax (INCE) MV-InfoMax (IJS) MIB (β = 1, INCE) MIB (β = 1, IJS)</cell><cell>I(x; z) [nats] I(z; y) [nats] 12.5 ± 0.7 2.3 ± 0.2 7.5 ± 1.0 2.0 ± 0.2 3.0 ± 0.5 1.0 ± 0.1 12.8 ± 0.5 2.3 ± 0.2 13.7 ± 0.7 2.2 ± 0.2 12.2 ± 0.7 2.3 ± 0.2 11.1 ± 1.0 2.3 ± 0.2 4.6 ± 0.7 2.1 ± 0.2 2.4 ± 0.2 2.1 ± 0.2</cell><cell>Test Accuracy [%] 50 Ex 3750 Ex 43.8 ± 1.6 65.6 ± 3.3 10 Ex 89.0 ± 0.4 55.9 ± 2.6 81.4 ± 4.0 94.2 ± 0.3 43.8 ± 2.8 61.1 ± 4.8 81.9 ± 1.1 25.4 ± 1.9 39.6 ± 3.3 69.2 ± 0.7 35.0 ± 2.8 52.5 ± 2.8 74.4 ± 1.1 50.2 ± 3.6 75.8 ± 3.8 94.6 ± 0.4 54.0 ± 6.1 78.3 ± 4.4 94.1 ± 0.3 81.8 ± 5.0 92.7 ± 0.9 97.19 ± 0.08 97.75 ± 0.05 60000 Ex 91.3 ± 0.1 96.0 ± 0.2 87.2 ± 0.6 74.6 ± 0.6 78.2 ± 1.2 96.5 ± 0.1 95.90 ± 0.08 97.1 ± 0.2 97.2 ± 0.2 97.70 ± 0.06 97.82 ± 0.01</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparison of the amount of input information I(x; z), label information I(z; y), and accuracy of a linear classifier trained with different amount of labeled Examples (Ex) for the models reported in Figure</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0">MULTI-VIEW INFORMATION BOTTLENECKAs a motivating example, consider v 1 and v 2 to be two images of the same object from different view-points and let y be its label. Assuming that the object is clearly distinguishable from both v 1 and let v 2 , any representation z containing all information accessible from both views would also contain the necessary label information. Furthermore, if z captures only the details that are visible from both pictures, it would eliminate the view-specific details and reduce the sensitivity of the representation to view-changes. The theory to support this intuition is described in the following where v 1 and v 2 are jointly observed and referred to as data-views.1 Code available at https://github.com/mfederici/Multi-View-Information-Bottleneck</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">These results are included only for completeness, as the Multi-View InfoMax objective does not produce consistent representations for the two views so there is no straight-forward way to use it for ranking.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank Andy Keller, Karen Ullrich, Maximillian Ilse and the anonymous reviewers for their feedback and insightful comments. This work has received funding from the ERC under the Horizon 2020 program (grant agreement No. 853489). The Titan Xp and Titan V used for this research were donated by the NVIDIA Corporation.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A PROPERTIES OF MUTUAL INFORMATION AND ENTROPY</head><p>In this section we enumerate some of the properties of mutual information that are used to prove the theorems reported in this work. For any random variables w, x, y and z: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B THEOREMS AND PROOFS</head><p>In the following section we prove the statements reported in the main text of the paper. Whenever a random variable z is defined to be a representation of another random variable x, we state that z is conditionally independent from any other variable in the system once x is observed. This does not imply that z must be a deterministic function of x, but that the source of stochasticity for z is independent of the other random variables. As a result whenever z is a representation of x:</p><p>for any variable (or groups of variables) a and b in the system. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 NO FREE GENERALIZATION</head><p>Theorem B.1. Let x, z and y be random variables with joint distribution p(x, y, z). Let z be a representation of x that satisfies I(x; z) &gt; I(x; z ), then it is always possible to find a label y for which z is not predictive for y while z is.</p><p>Hypothesis:</p><p>(H 1 ) z is a representation of x: I(y; z |x) = 0 (H 2 ) I(x; z) &gt; I(x; z )</p><p>Thesis:</p><p>(T 1 ) I(x; z ) &lt; I(x; z) =⇒ ∃y.I(y; z) &gt; I(y; z ) = 0</p><p>Proof. By construction.</p><p>1. We first factorize x as a function of two independent random variables (Proposition 2.1 <ref type="bibr" target="#b0">Achille &amp; Soatto (2018)</ref>) by picking y such that:</p><p>for some deterministic function f . Note that such y always exists.</p><p>2. Since x is a function of y and z :</p><p>(C 4 ) I(x; z|yz ) = 0</p><p>Considering I(y; z):</p><p>&gt; 0</p><p>Since I(y; z ) = 0 by construction and I(y; z) &gt; 0, the y built in 1. satisfies the conditions reported in the thesis.</p><p>Proposition B.3. Let v 1 be a redundant view with respect to v 2 for y. Any representation z 1 of v 1 that is sufficient for v 2 is also sufficient for y.</p><p>Hypothesis:</p><p>Thesis:</p><p>Proof. Using the results from Theorem B.2:</p><p>and y be random variables with distribution p(v 1 , v 2 , y). Let z be a representation of v 1 , then</p><p>Hypothesis:</p><p>Thesis:</p><p>Proof.</p><p>Corollary B.2.1. Let v 1 and v 2 be mutually redundant views for y. Let z 1 be a representation of v 1 that is sufficient for v 2 . Then: I(y; z 1 ) = I(v 1 v 2 ; y)</p><p>Hypothesis:</p><p>(H 2 ) v 1 and v 2 are mutually redundant for y: I(y; v 1 |v 2 ) + I(y; v 2 |v 1 ) = 0 (H 3 ) z 1 is sufficient for v 2 : I(v 2 ; v 1 |z) = 0</p><p>Thesis:</p><p>(T 1 ) I(y; z 1 ) = I(v 1 v 2 ; y)</p><p>Proof. Using Theorem B.2</p><p>Since I(y; z 1 ) ≤ I(y; v 1 v 2 ) is a consequence of the data processing inequality, we conclude that I(y; z 1 ) = I(y; v 1 v 2 )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 SUFFICIENCY AND AUGMENTATION</head><p>Let x and y be random variables with domain X and Y respectively. Let T be a class of functions t : X → W and let t 1 and t 2 be a random variables over T that depends only on x. For the theorems and corollaries discussed in this section, we are going to consider the independence assumption that can be derived from the graphical model G reported in Figure <ref type="figure">5</ref>. Proposition B.4. Whenever I(t 1 (x); y) = I(t 2 (x); y) = I(x; y) the two views t 1 (x) and t 2 (x) must be mutually redundant for y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hypothesis:</head><p>(H 1 ) Independence relations determined by G Thesis:</p><p>(T 1 ) I(t 1 (x); y) = I(t 2 (x); y) = I(x; y) =⇒ I(t 1 (x); y|t 2 (x)) + I(t 2 (x); y|t 1 (x)) = 0 Proof.</p><p>1. Considering G we have:</p><p>2. Since t 2 (x) is uniquely determined by x and t 2 :</p><p>Therefore I(y; x) = I(y; t 2 (x)) =⇒ I(t 1 (x); y|t 2 (x)) = 0</p><p>The proof for I(y; x) = I(y; t 1 (x)) =⇒ I(t 2 (x); y|t 1 (x)) = 0 is symmetric, therefore we conclude I(t 1 (x); y) = I(t 2 (x); y) = I(x; y) =⇒ I(t 1 (x); y|t 2 (x)) + I(t 2 (x); y|t 1 (x)) = 0 Theorem B.3. Let I(t 1 (x); y) = I(t 2 (x); y) = I(x; y). Let z 1 be a representation of t 1 (x) . If z 1 is sufficient for t 2 (x) then I(x; y) = I(y; z 1 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hypothesis:</head><p>(H 1 ) Independence relations determined by G (H 2 ) I(t 1 (x); y) = I(t 2 (x); y) = I(x; y) Thesis:</p><p>(T 1 ) I(t 1 (x); t 2 (x)|z 1 ) = 0 =⇒ I(x; y) = I(y; z 1 ) Proof. Since t 1 (x) is redundant for t 2 (x) (Proposition B.4) any representation z 1 of t 1 (x) that is sufficient for t 2 (x) must also be sufficient for y (Theorem B.2). Using Proposition B.1 we have I(y; z 1 ) = I(y; t 1 (x)). Since I(y; t 1 (x)) = I(y; x) by hypothesis, we conclude I(x; y) = I(y; z 1 )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C INFORMATION PLANE</head><p>Every representation z of x must satisfy the following constraints:</p><p>• 0 ≤ I(y; z) ≤ I(x; y): The amount of label information ranges from 0 to the total predictive information accessible from the raw observations I(x; y).</p><p>• I(y; z) ≤ I(x; z) ≤ I(y; z) + H(x|y): The representation must contain more information about the observations than about the label. When x is discrete, the amount of discarded label information I(x; y) − I(y; z) must be smaller than the amount of discarded observational information H(x) − I(x; z), which implies I(x; z) ≤ I(y; z) + H(x|y).</p><p>Proof. Since z is a representation of x:</p><p>(C 1 ) I(y; z|x) = 0</p><p>Considering the four bounds separately:</p><p>1. I(y; z) ≥ 0: Follows from P 1 2. I(x; z) ≥ I(y; z): Follows from: Note that the discreetness of x is required only to prove bound 4. For continuous x bounds 1, 2 and 3 still hold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D NON-TRANSITIVITY OF MUTUAL REDUNDANCY</head><p>The mutual redundancy condition between two views v 1 and v 2 for a label y can not be trivially extended to an arbitrary number of views, as the relation is not transitive because of some higher order interaction between the different views and the label. This can be shown with a simple example.</p><p>Given three views v 1 , v 2 and v 3 and a task y such that:</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Emergence of Invariance and Disentanglement in Deep Representations</title>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Achille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Deep Variational Information Bottleneck</title>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">V</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fixing a Broken ELBO</title>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">V</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rif</forename><forename type="middle">A</forename><surname>Saurous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep canonical correlation analysis</title>
		<author>
			<persName><forename type="first">Galen</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raman</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Bilmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Buchwalter</surname></persName>
		</author>
		<title level="m">Learning Representations by Maximizing Mutual Information Across Views. arXiv</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The im algorithm: A variational approach to information maximization</title>
		<author>
			<persName><forename type="first">David</forename><surname>Barber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Agakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
		<title level="m">Probabilistic symmetry and invariant neural networks. arXiv</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">P</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arka</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loic</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Watters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Lerchner</surname></persName>
		</author>
		<title level="m">Understanding disentangling in β-VAE. arXiv</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Ilya Sutskever, and Pieter Abbeel. Variational Lossy Autoencoder</title>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><surname>Bert</surname></persName>
		</author>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semantically tied paired cycle consistency for zero-shot sketchbased image retrieval</title>
		<author>
			<persName><forename type="first">Anjan</forename><surname>Dutta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">How do humans sketch objects?</title>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Eitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Alexa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>ACM TOG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Iterative quantization: A procrustean approach to learning binary codes for large-scale image retrieval</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">PixelVAE: A Latent Variable Model for Natural Images</title>
		<author>
			<persName><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kundan</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrien</forename><forename type="middle">Ali</forename><surname>Taiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Visin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Hénaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Ali Eslami</surname></persName>
		</author>
		<author>
			<persName><surname>Van Den Oord</surname></persName>
		</author>
		<title level="m">Data-Efficient Image Recognition with Contrastive Predictive Coding. arXiv</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multilingual Distributed Representations without Word Alignment</title>
		<author>
			<persName><forename type="first">Karl</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hermann</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">beta-vae: Learning basic visual concepts with a constrained variational framework</title>
		<author>
			<persName><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loic</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arka</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Lerchner</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Deep neural networks for acoustic modeling in speech recognition</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdel-Rahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Kingsbury</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>SPM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The mir flickr retrieval evaluation</title>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">J</forename><surname>Huiskes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Lew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMIR</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="39" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">MINE: Mutual Information Neural Estimation</title>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Ishmael Belghazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aristide</forename><surname>Baratin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sai</forename><surname>Rajeswar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Invariant Information Clustering for Unsupervised Image Classification and Segmentation</title>
		<author>
			<persName><forename type="first">Xu</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">João</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Variational Inference with Normalizing Flows</title>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Jimenez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rezende</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Auto-Encoding Variational Bayes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Improved variational inference with inverse autoregressive flow</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Durk P Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafal</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Self-organization in a perceptual network</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<editor>R. Linsker</editor>
		<imprint>
			<date type="published" when="1988">2015. 1988</date>
		</imprint>
	</monogr>
	<note>Nature</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep Sketch Hashing: Fast Free-hand Sketch-Based Image Retrieval</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fumin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianglong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">David</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Stratos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Formal Limitations on the Measurement of Mutual Information. arXiv</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multimodal deep learning</title>
		<author>
			<persName><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juhan</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Estimating divergence functionals and the likelihood ratio by convex risk minimization</title>
		<author>
			<persName><forename type="first">Xuanlong</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">On Variational Bounds of Mutual Information</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName><surname>Tucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sketch-based image retrieval via siamese convolutional neural network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>OpenAI Blog</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">The sketchy database: learning to retrieve badly drawn bunnies</title>
		<author>
			<persName><forename type="first">Patsorn</forename><surname>Sangkloy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Burnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cusuh</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>ACM TOG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">Very Deep Convolutional Networks for Large-Scale Image Recognition. arXiv</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Improved multimodal deep learning with variation of information</title>
		<author>
			<persName><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenling</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Multimodal learning with deep boltzmann machines</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">A relationship between the average precision and the area under the roc curve</title>
		<author>
			<persName><forename type="first">Wanhua</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mu</forename><surname>Zhu</surname></persName>
		</author>
		<editor>ICTIR</editor>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><surname>Krizhevsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Contrastive Multiview Coding. arXiv</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep Learning and the Information Bottleneck Principle</title>
		<author>
			<persName><forename type="first">Naftali</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noga</forename><surname>Zaslavsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ITW</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<author>
			<persName><forename type="first">Naftali</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><forename type="middle">C</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Bialek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The information bottleneck method. arXiv, 2000. Jakub M. Tomczak and Max Welling. VAE with a VampPrior</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">On Mutual Information Maximization for Representation Learning</title>
		<author>
			<persName><forename type="first">Josip</forename><surname>Michael Tschannen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">K</forename><surname>Djolonga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Rubenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><surname>Lucic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<title level="m">Representation Learning with Contrastive Predictive Coding. arXiv</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep multi-view information bottleneck</title>
		<author>
			<persName><forename type="first">Qi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Boudreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qixing</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pang-Ning</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayu</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIAM</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Deep Variational Canonical Correlation Analysis</title>
		<author>
			<persName><forename type="first">Weiran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
		<idno>arXiv</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Sketch-a-Net that Beats Humans</title>
		<author>
			<persName><forename type="first">Qian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Hospedales</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Generative domain-migration hashing for sketch-to-image retrieval</title>
		<author>
			<persName><forename type="first">Jingyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fumin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengyang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Tao Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Multi-view learning overview: Recent progress and new challenges</title>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xijiong</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiliang</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
