<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BRISK: Binary Robust Invariant Scalable Keypoints</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Stefan</forename><surname>Leutenegger</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Autonomous Systems Lab</orgName>
								<orgName type="institution">ETH Zürich</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Margarita</forename><surname>Chli</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Autonomous Systems Lab</orgName>
								<orgName type="institution">ETH Zürich</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Roland</forename><forename type="middle">Y</forename><surname>Siegwart</surname></persName>
							<email>roland.siegwart@mavt.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Autonomous Systems Lab</orgName>
								<orgName type="institution">ETH Zürich</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">BRISK: Binary Robust Invariant Scalable Keypoints</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">006E57EF0C5DFF7EE72D623DAD0D56FC</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T17:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Effective and efficient generation of keypoints from an image is a well-studied problem in the literature and forms the basis of numerous Computer Vision applications. Established leaders in the field are the SIFT and SURF algorithms which exhibit great performance under a variety of image transformations, with SURF in particular considered as the most computationally efficient amongst the highperformance methods to date.</p><p>In this paper we propose BRISK 1 , a novel method for keypoint detection, description and matching. A comprehensive evaluation on benchmark datasets reveals BRISK's adaptive, high quality performance as in state-of-the-art algorithms, albeit at a dramatically lower computational cost (an order of magnitude faster than SURF in cases). The key to speed lies in the application of a novel scale-space FAST-based detector in combination with the assembly of a bit-string descriptor from intensity comparisons retrieved by dedicated sampling of each keypoint neighborhood.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Decomposing an image into local regions of interest or 'features' is a widely applied technique in Computer Vision used to alleviate complexity while exploiting local appearance properties. Image representation, object recognition and matching, 3D scene reconstruction and motion tracking all rely on the presence of stable, representative features in the image, driving research and yielding a plethora of approaches to this problem.</p><p>The ideal keypoint detector finds salient image regions such that they are repeatably detected despite change of viewpoint; more generally it is robust to all possible image transformations. Similarly, the ideal keypoint descriptor captures the most important and distinctive information content enclosed in the detected salient regions, such that the same structure can be recognized if encountered. More-over, on top of fulfilling these properties to achieve the desired quality of keypoints, the speed of detection and description needs also to be optimized to fit within the timeconstraints of the task at hand.</p><p>In principle, state-of-the-art algorithms target applications with either strict requirements in precision or speed of computation. Lowe's SIFT approach <ref type="bibr" target="#b8">[9]</ref> is widely accepted as one of highest quality options currently available, promising distinctiveness and invariance to a variety of common image transformations -however, the at the expense of computational cost. On the other end of the spectrum, a combination of the FAST <ref type="bibr" target="#b13">[14]</ref> keypoint detector and the BRIEF <ref type="bibr" target="#b3">[4]</ref> approach to description offers a much more suitable alternative for real-time applications. However, despite the clear advantage in speed, the latter approach suffers in terms of reliability and robustness as it has minimal tolerance to image distortions and transformations, in particular to in-plane rotation and scale change. As a result, real-time applications like SLAM <ref type="bibr" target="#b5">[6]</ref> need to employ probabilistic methods <ref type="bibr" target="#b4">[5]</ref> for data association to discover matching consensus.</p><p>The inherent difficulty in extracting suitable features from an image lies in balancing two competing goals: highquality description and low computational requirements. This is where this work aims to set a new milestone with the BRISK methodology. Perhaps the most relevant work tackling this problem is SURF <ref type="bibr" target="#b1">[2]</ref> which has been demonstrated to achieve robustness and speed, only, as evident in our results, BRISK achieves comparable quality of matching at much less computation time. In a nutshell, this paper proposes a novel method for generating keypoints from an image, structured as follows:</p><p>• Scale-space keypoint detection: Points of interest are identified across both the image and scale dimensions using a saliency criterion. In order to boost efficiency of computation, keypoints are detected in octave layers of the image pyramid as well as in layers in-between. The location and the scale of each keypoint are obtained in the continuous domain via quadratic function fitting.</p><p>• Keypoint description: A sampling pattern consisting of points lying on appropriately scaled concentric circles is applied at the neighborhood of each keypoint to retrieve gray values: processing local intensity gradients, the feature characteristic direction is determined. Finally, the oriented BRISK sampling pattern is used to obtain pairwise brightness comparison results which are assembled into the binary BRISK descriptor.</p><p>Once generated, the BRISK keypoints can be matched very efficiently thanks to the binary nature of the descriptor. With a strong focus on efficiency of computation, BRISK also exploits the speed savings offered in the SSE instruction set widely supported on today's architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Identifying local interest points to be used for image matching can be traced a long way back in the literature, with Harris and Stephens <ref type="bibr" target="#b6">[7]</ref> proposing one of the earliest and probably most well-known corner detectors. The seminal work of Mikolajzyk et al. <ref type="bibr" target="#b12">[13]</ref> presented a comprehensive evaluation of the most competent detection methods at the time, which revealed no single all-purpose detector but rather the complementary properties of the different approaches depending on the context of the application. The more recent FAST criterion <ref type="bibr" target="#b13">[14]</ref> for keypoint detection has become increasingly popular in state-of-the-art methods with hard real-time constraints, with AGAST <ref type="bibr" target="#b9">[10]</ref> extending this work for improved performance.</p><p>Amongst the best quality features currently in the literature is the SIFT <ref type="bibr" target="#b8">[9]</ref>. The high descriptive power and robustness to illumination and viewpoint changes has rated the SIFT descriptor at the top of the rankings list in the survey in <ref type="bibr" target="#b10">[11]</ref>. However, the high dimensionality of this descriptor makes SIFT prohibitively slow. PCA-SIFT <ref type="bibr" target="#b7">[8]</ref> reduced the descriptor from 128 to 36 dimensions, compromising however its distinctiveness and increasing the time for descriptor formation which almost annihilates the increased speed of matching. The GLOH descriptor <ref type="bibr" target="#b11">[12]</ref> is also worth noting here, as it belongs to the family of SIFT-like methods and has been shown to be more distinctive but also more expensive to compute than SIFT.</p><p>The growing demand for high-quality, high-speed features has led to more research towards algorithms able to process richer data at higher rates. Notable is the work of Agrawal et al. <ref type="bibr" target="#b0">[1]</ref> who apply a center-symmetric local binary pattern as an alternative to SIFT's orientation histograms approach. The most recent BRIEF <ref type="bibr" target="#b3">[4]</ref> is designed for super-fast description and matching and consists of a binary string containing the results of simple image intensity comparisons at random pre-determined pixel locations. Despite the simplicity and efficiency of this approach, the method is very sensitive to image rotation and scale changes restricting its application to general tasks.</p><p>Probably the most appealing features at the moment are the SURF <ref type="bibr" target="#b1">[2]</ref>, which have been demonstrated to be significantly faster than SIFT. SURF detection uses the determinant of the Hessian matrix (blob detector), while the description is done by summing Haar wavelet responses at the region of interest. While demonstrating impressive timings with respect to the state-of-the-art, SURF are, in terms of speed, still orders of magnitude away from the fastest, yet limited quality features currently available.</p><p>In this paper, we present a novel methodology dubbed 'BRISK' for high-quality, fast keypoint detection, description and matching. As suggested by the name, the method is rotation as well as scale invariant to a significant extent, achieving performance comparable to the state-of-the-art while dramatically reducing computational cost. Following a description of the approach, we present experimental results performed on the benchmark datasets and using the standardized evaluation method of <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>. Namely, we present evaluation of BRISK with respect to SURF and SIFT which are widely accepted as a standard of comparison under common image transformations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">BRISK: The Method</head><p>In this section, we describe the key stages in BRISK, namely feature detection, descriptor composition and keypoint matching to the level of detail that the motivated reader can understand and reproduce. It is important to note that the modularity of the method allows the use of the BRISK detector in combination with any other keypoint descriptor and vice versa, optimizing for the desired performance and the task at hand.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Scale-Space Keypoint Detection</head><p>With the focus on efficiency of computation, our detection methodology is inspired by the work of Mair et al. <ref type="bibr" target="#b9">[10]</ref> for detecting regions of interest in the image. Their AGAST is essentially an extension for accelerated performance of the now popular FAST, proven to be a very efficient basis for feature extraction. With the aim of achieving invariance to scale which is crucial for high-quality keypoints, we go a step further by searching for maxima not only in the image plane, but also in scale-space using the FAST score s as a measure for saliency. Despite discretizing the scale axis at coarser intervals than in alternative high-performance detectors (e.g. the Fast-Hessian <ref type="bibr" target="#b1">[2]</ref>), the BRISK detector estimates the true scale of each keypoint in the continuous scale-space.</p><p>In the BRISK framework, the scale-space pyramid layers consist of n octaves c i and n intra-octaves d i , for i = {0, 1, . . . , n -1} and typically n = 4. The octaves are formed by progressively half-sampling the original image (corresponding to c 0 ). Each intra-octave d i is located in-between layers c i and c i+1 (as illustrated in Figure <ref type="figure">1</ref>). The first intra-octave d 0 is obtained by downsampling the original image c 0 by a factor of 1.5, while the rest of the intra-octave layers are derived by successive halfsampling. Therefore, if t denotes scale then t(c i ) = 2 i and t</p><formula xml:id="formula_0">(d i ) = 2 i • 1.5.</formula><p>It is important to note here that both FAST and AGAST provide different alternatives of mask shapes for keypoint detection. In BRISK, we mostly use the 9-16 mask, which essentially requires at least 9 consecutive pixels in the 16pixel circle to either be sufficiently brighter or darker than the central pixel for the FAST criterion to be fulfilled.</p><p>Initially, the FAST 9-16 detector is applied on each octave and intra-octave separately using the same threshold T to identify potential regions of interest. Next, the points belonging to these regions are subjected to a non-maxima suppression in scale-space: firstly, the point in question needs to fulfill the maximum condition with respect to its 8 neighboring FAST scores s in the same layer. The score s is defined as the maximum threshold still considering an image point a corner. Secondly, the scores in the layer above and below will need to be lower as well. We check inside equally sized square patches: the side-length is chosen to be 2 pixels in the layer with the suspected maximum. Since the neighboring layers (and therefore its FAST scores) are represented with a different discretization, some interpolation is applied at the boundaries of the patch. Figure <ref type="figure">1</ref> depicts an example of this sampling and the maxima search.</p><p>The detection of maxima across the scale axis at octave c 0 is a special case: in order to obtain the FAST scores for a virtual intra-octave d -1 below c 0 , we apply the FAST 5-8 mask on c 0 . However, the scores in patch of d -1 are in this case not required to be lower than the score of the examined point in octave c 0 .</p><p>Considering image saliency as a continuous quantity not only across the image but also along the scale dimension, we perform a sub-pixel and continuous scale refinement for each detected maximum. In order to limit complexity of the refinement process, we first fit a 2D quadratic function in the least-squares sense to each of the three scores-patches (as obtained in the layer of the keypoint, the one above, and the one below) resulting in three sub-pixel refined saliency maxima. In order to avoid resampling, we consider a 3 by 3 score patch on each layer. Next, these refined scores are used to fit a 1D parabola along the scale axis yielding the final score estimate and scale estimate at its maximum. As a final step, we re-interpolate the image coordinates between the patches in the layers next to the determined scale. An example of the BRISK detection in two images of the Boat sequence (defined in Section 4) is shown up-close in Figure <ref type="figure">2</ref>. the Boat sequence exhibiting small zoom and in-plane rotation. The size of the circles denote the scale of the detected keypoints while the radials denote their orientation. For clarity, the detection threshold is set here to a stricter value than in the typical setup, yielding slightly lower repeatability.</p><formula xml:id="formula_1">octave c i FAST score s log ( ) : scale 2 t t i i+1 i-1 interpolated position intra-octave d i-1 octave c i+1 octave c i-1 intra-octave d i</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Keypoint Description</head><p>Given a set of keypoints (consisting of sub-pixel refined image locations and associated floating-point scale values), the BRISK descriptor is composed as a binary string by concatenating the results of simple brightness comparison tests. This idea has been demonstrated in <ref type="bibr" target="#b3">[4]</ref> to be very efficient, however here we employ it in a far more qualitative manner. In BRISK, we identify the characteristic direction of each keypoint to allow for orientation-normalized descriptors and hence achieve rotation invariance which is key to general robustness. Also, we carefully select the brightness comparisons with the focus on maximizing descriptiveness. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Sampling Pattern and Rotation Estimation</head><p>The key concept of the BRISK descriptor makes use of a pattern used for sampling the neighborhood of the keypoint. The pattern, illustrated in Figure <ref type="figure" target="#fig_1">3</ref>, defines N locations equally spaced on circles concentric with the keypoint. While this pattern resembles the DAISY descriptor <ref type="bibr" target="#b14">[15]</ref>, it is important to note that its use in BRISK is entirely different, as DAISY was built specifically for dense matching, deliberately capturing more information and thus resulting to demanding speed and storage requirements.</p><p>In order to avoid aliasing effects when sampling the image intensity of a point p i in the pattern, we apply Gaussian smoothing with standard deviation σ i proportional to the distance between the points on the respective circle. Positioning and scaling the pattern accordingly for a particular keypoint k in the image, let us consider one of the N • (N -1)/2 sampling-point pairs (p i , p j ). The smoothed intensity values at these points which are I(p i , σ i ) and I(p j , σ j ) respectively, are used to estimate the local gradient g(p i , p j ) by</p><formula xml:id="formula_2">g(p i , p j ) = (p j -p i ) • I(p j , σ j ) -I(p i , σ i ) p j -p i 2 . (<label>1</label></formula><formula xml:id="formula_3">)</formula><p>Considering the set A of all sampling-point pairs:</p><formula xml:id="formula_4">A = (p i , p j ) ∈ R 2 × R 2 | i &lt; N ∧ j &lt; i ∧ i, j ∈ N (2)</formula><p>we define a subset of short-distance pairings S and another subset of L long-distance pairings L:</p><formula xml:id="formula_5">S = {(p i , p j ) ∈ A | p j -p i &lt; δ max } ⊆ A L = {(p i , p j ) ∈ A | p j -p i &gt; δ min } ⊆ A.<label>(3)</label></formula><p>The threshold distances are set to δ max = 9.75t and δ min = 13.67t (t is the scale of k). Iterating through the point pairs in L, we estimate the overall characteristic pattern direction of the keypoint k to be:</p><formula xml:id="formula_6">g = g x g y = 1 L • (pi,pj )∈L g(p i , p j ).<label>(4)</label></formula><p>The long-distance pairs are used for this computation, based on the assumption that local gradients annihilate each other and are thus not necessary in the global gradient determination -this was also confirmed by experimenting with variation of the distance threshold δ min .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Building the Descriptor</head><p>For the formation of the rotation-and scale-normalized descriptor, BRISK applies the sampling pattern rotated by α = arctan2 (g y , g x ) around the keypoint k. The bit-vector descriptor d k is assembled by performing all the shortdistance intensity comparisons of point pairs (p α i , p α j ) ∈ S (i.e. in the rotated pattern), such that each bit b corresponds to:</p><formula xml:id="formula_7">b = 1, I(p α j , σ j ) &gt; I(p α i , σ i ) 0, otherwise ∀(p α i , p α j ) ∈ S<label>(5)</label></formula><p>While the BRIEF descriptor is also assembled via brightness comparisons, BRISK has some fundamental differences apart from the obvious pre-scaling and pre-rotation of the sampling pattern. Firstly, BRISK uses a deterministic sampling pattern resulting in a uniform sampling-point density at a given radius around the keypoint. Consequently, the tailored Gaussian smoothing will not accidentally distort the information content of a brightness comparison by blurring two close sampling-points in a comparison. Furthermore, BRISK uses dramatically fewer sampling-points than pairwise comparisons (i.e. a single point participates in more comparisons), limiting the complexity of lookingup intensity values. Finally, the comparisons here are restricted spatially such that the brightness variations are only required to be locally consistent. With the sampling pattern and the distance thresholds as shown above, we obtain a bit-string of length 512. The bit-string of BRIEF64 also contains 512 bits, thus the matching for a descriptor pair will be performed equally fast by definition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Descriptor Matching</head><p>Matching two BRISK descriptors is a simple computation of their Hamming distance as done in BRIEF <ref type="bibr" target="#b3">[4]</ref>: the number of bits different in the two descriptors is a measure of their dissimilarity. Notice that the respective operations reduce to a bitwise XOR followed by a bit count, which can both be computed very efficiently on today's architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Notes on Implementation</head><p>Here, we give a very brief overview of some implementation issues which contribute significantly to the overall computational performance and the reproducibility of the method. All the BRISK functionality builds on the common 2D feature interface of OpenCV 2.2 allowing easy integration and interchangeability with existing features (SIFT, SURF, BRIEF, etc.).</p><p>The detection process uses the AGAST implementation <ref type="bibr" target="#b9">[10]</ref> for computing saliency scores. The non-maxima suppression benefits from early termination capability limiting the saliency scores calculation to a minimum. Building the image pyramid makes use of some SSE2 and SSSE3 commands, both concerning the halfsampling as well as the downsampling by a factor of 1.5.</p><p>In order to efficiently retrieve gray values with the sampling pattern, we generate a look-up table of discrete rotated and scaled BRISK pattern versions (consisting of the sampling-point locations and the properties of the Gaussian smoothing kernel as well as the indexing of long and short distance pairings) consuming around 40MB of RAM -which is still acceptable for applications constrained to low computational power.</p><p>We furthermore use the integral image along with a simplified Gaussian kernel version inspired by <ref type="bibr" target="#b1">[2]</ref>: the kernel is scalable when changing σ without any increase in computational complexity. In our final implementation we use as an approximation a simple square box mean filter with floating point boundaries and side length ρ = 2.6 • σ.</p><p>Thus we do not need time-consuming Gaussian smoothing of the whole image with many different kernels, but we instead retrieve single values using an arbitrary parameter σ.</p><p>We also integrated an improved SSE Hamming distance calculator achieving matching at 6 times the speed of the current OpenCV implementation as used for example with BRIEF in OpenCV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Our proposed method has been extensively tested following the now established evaluation method and datasets in the field first proposed by Mikolajczyk and Schmid <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>. For the sake of consistency with results presented in other works, we also used their MATLAB evaluation scripts which are available online. Each of the datasets contains a sequence of six images exhibiting an increasing amount of transformation. All comparisons here are performed against the first image in each dataset. Figure <ref type="figure" target="#fig_3">4</ref> shows one image for each dataset analyzed.</p><p>The transformations cover view-point change (Graffiti and Wall), zoom and rotation (Boat), blur (Bikes and Trees), brightness changes (Leuven) as well as JPEG compression  (Ubc). Since the viewpoint change scenes are planar, the image pairs in all sequences are provided with a ground truth homography used to determine the corresponding keypoints. In the rest of the section we present quantitative results concerning the detector and descriptor performance of BRISK compared to SIFT (OpenCV2.2 implementation) as well as SURF (original implementation). Our evaluation uses similarity matching which considers any pair of keypoints with descriptor distance below a certain threshold a match -in contrast to e.g. nearest neighbor matching, where a database is searched for the match with the lowest descriptor distance. Finally, we also demonstrate BRISK's big advantage in computational speed by listing comparative timings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">BRISK Detector Repeatability</head><p>The detector repeatability score as defined in <ref type="bibr" target="#b12">[13]</ref> is calculated as the ratio between the corresponding keypoints and the minimum total number of keypoints visible in both images. The correspondences are identified by looking at the overlap area of the keypoint region in one image (i.e. the extracted circle) and the projection of the keypoint region from the other image (i.e. ellipse-like): if the region of intersection is larger than 50% of the union of the two regions, it is considered a correspondence. Note that this method is largely dependent on the assignment of the keypoint circle radius, i.e. the constant factor between scale and radius. We choose this such that the average radii obtained with the BRISK detector approximately match the average radii obtained with the SURF and SIFT detectors.</p><p>The assessment of repeatability scores (a selection of results is shown in Figure <ref type="figure" target="#fig_4">5</ref>) is performed using constant BRISK detection thresholds across one sequence. For the sake of a fair comparison with the SURF detector, we adapt the respective Hessian threshold such that it outputs approximately the same number of correspondences in the similarity based matching setup.</p><p>As illustrated in Figure <ref type="figure" target="#fig_4">5</ref>, the BRISK detector exhibits equivalent repeatability as the SURF detector as long as the image transformations applied are not too large. Given the clear advantage in computational cost of the BRISK over the SURF detector however, the proposed method constitutes a strong competitor, even if the performance at larger transformations appears to be slightly inferior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation and Comparison of the Overall BRISK Algorithm</head><p>Since our work aims at providing an overall fast as well as robust detection, description and matching, we evaluate the joint performance of all these stages in BRISK and compare it to SIFT and SURF. Figure <ref type="figure">6</ref> shows the precisionrecall curves using threshold-based similarity matching for a selection of image pairs of different datasets. Again, for this assessment we adapt the detection thresholds such that they output an approximately equal number of correspondences in the spirit of fairness. Note that the evaluation results here are different from the ones in <ref type="bibr" target="#b2">[3]</ref>, where all descriptors are extracted on the same regions (obtained with the Fast-Hessian detector).</p><p>As illustrated in Figure <ref type="figure">6</ref>, BRISK performs competitively with SIFT and SURF in all datasets and even outperforms the other two in some cases. The reduced performance of BRISK in the Trees dataset is attributed to the detector performance: while SURF detects 2606 and 2624 regions in the images, respectively, BRISK only detects 2004 regions in image 4 compared to 5949 found in image 1 to achieve the approximately same number of correspondences. The same holds for the other blur dataset, Bikes: saliency as assessed with FAST is inherently more sensi- tive to blur than blob-like detectors. We therefore also show the evaluation of the BRISK descriptors extracted from the SURF regions for the Trees dataset, demonstrating again that the descriptor performance is comparable to SURF. Evidently, SIFT performs significantly worse in the Trees, Boat, and Ubc datasets, which can be explained with the limited detector repeatability in these cases. On the other hand, SIFT and BRISK handle the important case of pure in-plane rotation very well and better than SURF.</p><p>In order to complete the experimental section, we want to make the link to BRIEF. Figure <ref type="figure">7</ref> shows a comparison of the unrotated, single-scale BRISK version (SU-BRISK) to 64 byte BRIEF features on the same (single scale) AGAST keypoints. Also included are the rotation invariant, singlescale S-BRISK, as well as the standard BRISK. The experiment is conducted with two image pairs: on the one hand, we used the first two images in the Wall dataset proving that SU-BRISK and BRIEF64 are exhibiting a very similar performance in the absence of scale change and in-plane rotation. Notice that this is really the situation BRIEF was designed for. On the other hand, we applied the different versions to the first two images of the Boat sequence: this experiment demonstrates some advantage of the SU-BRISK over BRIEF in terms of robustness against small rotation (10 • ) and scale changes (10%). Furthermore, the well known and intuitive price for both rotation and scale invariance is easily observable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Timings</head><p>Timings have been recorded on a laptop with a quadcore i7 2.67 GHz processor (only using one core, however) running Ubuntu 10.04 (32-bit), using the implementation and setup as detailed above. Table <ref type="table" target="#tab_1">1</ref> presents the results concerning detection on the first image of the Graffiti sequence, while Table <ref type="table" target="#tab_2">2</ref> shows the matching times. The values are averaged over 100 runs. Note that all matchers do a brute-force descriptor distance computation without any early termination optimizations.</p><p>The timings show a clear advantage of BRISK. Its detection and descriptor computation is typically an order of magnitude faster than the one of SURF, which are considered to be the fastest rotation and scale invariant features currently available. It is also important to highlight that BRISK is easily scalable for faster execution by reducing the number of sampling-points in the pattern at some expense of matching quality -which might be affordable in a particular application. Moreover, scale and/or rotation invariance can be omitted trivially, increasing the speed as well as the matching quality in applications where they are not needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">An Example</head><p>Complementary to the extensive evaluation presented above, we also provide a real-world example demonstrating matching using BRISK. Figure <ref type="figure" target="#fig_7">8</ref> shows an image pair exhibiting various transformations. A similarity match with a threshold of 90 was performed (out of 512 comparisons) resulting in robust matches without significant outliers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We have presented a novel method named BRISK, which tackles the classic Computer Vision problem of detecting, describing and matching image keypoints for cases without sufficient a priori knowledge on the scene and camera poses. In contrast to well-established algorithms with proven high performance, such as SIFT and SURF, the method at hand offers a dramatically faster alternative at comparable matching performance -a statement which we base on an extensive evaluation using an established framework. BRISK relies on an easily configurable circular sampling pattern from which it computes brightness comparisons to form a binary descriptor string. The unique properties of BRISK can be useful for a wide spectrum of applications, in particular for tasks with hard real-time constraints or limited computation power: BRISK finally offers the quality of high-end features in such time-demanding applications. Amongst avenues for further research into BRISK, we aim to explore alternatives to the scale-space maxima search of saliency scores to yield higher repeatability whilst maintaining speed. Furthermore, we aim at analyzing both theoretically and experimentally the BRISK pattern and the configuration of comparisons, such that the information content and/or robustness of the descriptor is maximized.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .Figure 2 .</head><label>12</label><figDesc>Figure1. Scale-space interest point detection: a keypoint (i.e. saliency maximum) is identified at octave c i by analyzing the 8 neighboring saliency scores in c i as well as in the corresponding scores-patches in the immediately-neighboring layers above and below. In all three layers of interest, the local saliency maximum is sub-pixel refined before a 1D parabola is fitted along the scale-axis to determine the true scale of the keypoint. The location of the keypoint is then also re-interpolated between the patch maxima closest to the determined scale.</figDesc><graphic coords="3,308.30,411.78,105.23,84.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. The BRISK sampling pattern with N = 60 points: the small blue circles denote the sampling locations; the bigger, red dashed circles are drawn at a radius σ corresponding to the standard deviation of the Gaussian kernel used to smooth the intensity values at the sampling points. The pattern shown applies to a scale of t = 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Datasets used for evaluation: viewpoint change (Graffiti and Wall), zoom and rotation (Boat), JPEG compression (Ubc), brightness change (Leuven), and blur (Bikes and Trees).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Repeatability scores for 50% overlap error of the BRISK and the SURF detector. The resulting similarity correspondences (approximately matched between the detectors) are given as numbers above the bars.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>4 Figure 6 .</head><label>46</label><figDesc>Figure 6. Evaluation results showing precision-recall curves (of all detection, extraction and matching stages jointly) for BRISK, SURF and SIFT. Results are shown for viewpoint changes (a and b), pure in-plane rotation (c), zoom and rotation (d), blur (e and f), brightness changes (g) and JPEG compression (h). The number of similarity correspondences are indicated in the figures per algorithm. The red dotted line in (f) shows the performance of BRISK descriptors extracted from SURF regions, yielding 2274 correspondences. Overall, BRISK exhibits competitive performance in all cases and even outperforms SIFT and SURF in some cases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>2 Figure 7 .</head><label>27</label><figDesc>Figure 7. Comparison of different BRISK versions to 64 byte BRIEF.BRIEF, as well as both SU-BRISK (single-scale, unrotated) and S-BRISK (single-scale) are extracted from AGAST keypoints detected in the original image. Notice that the BRISK pattern was scaled such that it matches the BRIEF patch size. The standard version of BRISK had to be extracted from our scale-invariant corner detection with adapted threshold to match the number of correspondences: they are 850 in the Wall pair and 1530 in the Boat pair.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. BRISK matching example: a detection threshold of 70 is used and a matching Hamming distance threshold of 90. The resulting matches are connected by the green lines showing no clear false positives. The authors provide a reference implementation of BRISK downloadable from http://www.asl.ethz.ch/people/lestefan/personal/BRISK .</figDesc><graphic coords="8,48.63,105.63,481.75,173.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Detection and extraction timings for the first image in the Graffiti sequence (size: 800 × 640 pixels).</figDesc><table><row><cell></cell><cell>SIFT</cell><cell>SURF</cell><cell>BRISK</cell></row><row><cell>Detection threshold</cell><cell>4.4</cell><cell>45700</cell><cell>67</cell></row><row><cell>Number of points</cell><cell>1851</cell><cell>1557</cell><cell>1051</cell></row><row><cell>Detection time [ms]</cell><cell>1611</cell><cell>107.9</cell><cell>17.20</cell></row><row><cell>Description time [ms]</cell><cell>9784</cell><cell>559.1</cell><cell>22.08</cell></row><row><cell>Total time [ms]</cell><cell>11395</cell><cell>667.0</cell><cell>39.28</cell></row><row><cell>Time per point (ms)</cell><cell cols="3">6.156 0.4284 0.03737</cell></row><row><cell></cell><cell cols="3">SIFT SURF BRISK</cell></row><row><cell>Points in first image</cell><cell>1851</cell><cell>1557</cell><cell>1051</cell></row><row><cell>Points in second image</cell><cell>2347</cell><cell>1888</cell><cell>1385</cell></row><row><cell>Total time [ms]</cell><cell cols="2">291.6 194.6</cell><cell>29.92</cell></row><row><cell cols="3">Time per comparison [ns] 67.12 66.20</cell><cell>20.55</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Matching timings for the Graffiti image 1 and 3 setup.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The reference implementation of BRISK can be downloaded from http://www.asl.ethz.ch/people/lestefan/personal/ BRISK</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgements</head><p>This research was supported by the Autonomous Systems Lab, ETH Zurich and the EC's 7th Framework Programme (FP7/2001-2013) under grant agreement no. 231855 (sFly). We are grateful to Simon Lynen and Davide Scaramuzza for their valuable inputs, as well as to many other colleagues at ETH Zurich for very helpful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">CenSurE: Center surround extremas for realtime feature detection and matching</title>
		<author>
			<persName><forename type="first">M</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Blas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">SURF: Speeded up robust features</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Image Understanding (CVIU)</title>
		<imprint>
			<date type="published" when="2005">2008. 1, 2, 5</date>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page" from="346" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">SURF: Speeded up robust features</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">BRIEF: Binary Robust Independent Elementary Features</title>
		<author>
			<persName><forename type="first">M</forename><surname>Calonder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Strecha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2004">2010. 1, 2, 3, 4</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Active Matching</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">MonoSLAM: Real-time single camera SLAM</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Molton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Stasse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1052" to="1067" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A combined corner and edge detector</title>
		<author>
			<persName><forename type="first">C</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stephens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 4th Alvey Vision Conference</title>
		<meeting>4th Alvey Vision Conference</meeting>
		<imprint>
			<date type="published" when="1988">1988</date>
			<biblScope unit="page" from="147" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">PCA-SIFT: A More Distinctive Representation for Local Image Descriptors</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adaptive and generic corner detection based on the accelerated segment test</title>
		<author>
			<persName><forename type="first">E</forename><surname>Mair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Burschka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Suppa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hirzinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2005">2010. 2, 5</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A performance evaluation of local descriptors</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A performance evaluation of local descriptors</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A comparison of affine region detectors</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schaffalitzky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kadir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Machine learning for highspeed corner detection</title>
		<author>
			<persName><forename type="first">E</forename><surname>Rosten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Drummond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Daisy: an Efficient Dense Descriptor Applied to Wide Baseline Stereo</title>
		<author>
			<persName><forename type="first">E</forename><surname>Tola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
