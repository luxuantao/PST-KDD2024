<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ON THE VARIANCE OF THE ADAPTIVE LEARNING RATE AND BEYOND</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Haoming</forename><surname>Jiang</surname></persName>
							<email>jianghm@gatech.edu</email>
						</author>
						<author>
							<persName><forename type="first">Georgia</forename><surname>Tech</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
							<email>wzchen@microsoft.com</email>
						</author>
						<author>
							<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
							<email>jfgao@microsoft.com</email>
						</author>
						<author>
							<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
							<email>hanj@illinois</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois</orgName>
								<address>
									<settlement>Urbana-Champaign</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Dynamics</orgName>
								<address>
									<postCode>365 AI</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">University of Illinois</orgName>
								<address>
									<settlement>Urbana-Champaign</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ON THE VARIANCE OF THE ADAPTIVE LEARNING RATE AND BEYOND</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The learning rate warmup heuristic achieves remarkable success in stabilizing training, accelerating convergence and improving generalization for adaptive stochastic optimization algorithms like RMSprop and Adam. Pursuing the theory behind warmup, we identify a problem of the adaptive learning rate -its variance is problematically large in the early stage, and presume warmup works as a variance reduction technique. We provide both empirical and theoretical evidence to verify our hypothesis. We further propose Rectified Adam (RAdam), a novel variant of Adam, by introducing a term to rectify the variance of the adaptive learning rate. Experimental results on image classification, language modeling, and neural machine translation verify our intuition and demonstrate the efficacy and robustness of RAdam. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adam-eps</head><p>Adam-2k Adam-vanilla RAdam Adam-warmup Fast and stable optimization algorithms are what generations of researchers have been pursuing <ref type="bibr" target="#b12">(Gauss, 1823;</ref><ref type="bibr" target="#b4">Cauchy, 1847)</ref>. Remarkably, stochastic gradient-based optimization, such as stochastic gradient descent (SGD), has witnessed tremendous success in many fields of science and engineering despite its simplicity. Recently, many efforts have been made to accelerate optimization by applying adaptive learning rate.</p><p>In particular, Adagrad <ref type="bibr" target="#b11">(Duchi et al., 2010)</ref> and its variants, e.g., RMSprop <ref type="bibr" target="#b16">(Hinton et al., 2012)</ref>, Adam <ref type="bibr" target="#b18">(Kingma &amp; Ba, 2014)</ref>, Adadelta <ref type="bibr" target="#b31">(Zeiler, 2012)</ref> and Nadam <ref type="bibr" target="#b10">(Dozat, 2016)</ref>, stand out due to their fast convergence, and have been considered as the optimizer of choice in many applications.</p><p>However, it has been observed that these optimization methods may converge to bad/suspicious local optima, and have to resort to a warmup heuristic -using a small learning rate in the first few epochs of training to mitigate such problem <ref type="bibr" target="#b28">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b25">Popel &amp; Bojar, 2018)</ref>. For example, when training typical Transformers based neural machine translation models on the De-En IWSLT'14 dataset, removing the warmup stage increases the training loss from 3 to around 10, as shown in Figure <ref type="figure" target="#fig_0">1</ref>. Similar phenomena are observed in other scenarios like BERT (a bidirectional transformer language model) pre-training <ref type="bibr" target="#b9">(Devlin et al., 2019)</ref>.</p><p>Duo to the lack of the theoretical underpinnings, there is neither guarantee that warmup would bring consistent improvements for various machine learning settings nor guidance on how we should conduct warmup. Thus, researchers typically use different settings in different applications and have to take a trial-and-error approach, which can be tedious and time-consuming.</p><p>In this paper, we conduct both empirical and theoretical analysis of the convergence issue to identify its origin. We show that its root cause is: the adaptive learning rate has undesirably large variance in the early stage of model training, due to the limited amount of training samples being used. Thus, to reduce such variance, it is better to use smaller learning rates in the first few epochs of training, which justifies the warmup heuristic.</p><p>Inspired by our analysis results, we propose a new variant of Adam, called Rectified Adam (RAdam), which explicitly rectifies the variance of the adaptive learning rate based on derivations. We conduct extensive experiments on language modeling, image classification, and neural machine translation. RAdam brings consistent improvement over the vanilla Adam, which verifies the variance issue generally exists on various tasks across different network architectures.</p><p>In summary, our main contributions are two-fold:</p><p>• We identify the variance issue of the adaptive learning rate and present a theoretical justification for the warmup heuristic. We show that the convergence issue is due to the undesirably large variance of the adaptive learning rate in the early stage of model training. • We propose a new variant of Adam (i.e., RAdam), which not only explicitly rectifies the variance and is theoretically sound, but also compares favorably with the heuristic warmup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES AND MOTIVATIONS</head><p>Generic adaptive methods. Algorithm 1 is a generic framework (all operations are element-wise).</p><p>It describes various popular stochastic gradient descent algorithms <ref type="bibr" target="#b26">(Reddi et al., 2018)</ref>. Specifically, different optimization algorithms can be specified by different choices of φ(.) and ψ(.), where φ(.) specifies how the momentum at time step t is calculated, and ψ(.) how the adaptive learning rate at t is calculated. For example, in the Adam algorithm, we have:</p><formula xml:id="formula_0">φ(g 1 , • • • , g t ) = (1 − β 1 ) t i=1 β t−i 1 g t 1 − β t 1 and ψ(g 1 , • • • , g t ) = 1 − β t 2 (1 − β 2 ) t i=1 β t−i 2 g 2 i .<label>(1)</label></formula><p>For numerical stability, the function ψ(.) in Equation 1 is usually calculated as ψ(g</p><formula xml:id="formula_1">1 , • • • , g t ) = √ 1−β t 2 + √ (1−β2) t i=1 β t−i 2 g 2 i</formula><p>, where is a relatively small / negligible value (e.g., 1 × 10 −8 ).</p><p>Algorithm 1: Generic adaptive optimization method setup. All operations are element-wise.</p><p>Input: {α t } T t=1 : step size, {φ t , ψ t } T t=1 : function to calculate momentum and adaptive rate, θ 0 : initial parameter, f (θ): stochastic objective function. Output: θ T : resulting parameters while t = 1 to T do</p><formula xml:id="formula_2">g t ← ∆ θ f t (θ t−1 ) (Calculate gradients w.r.t. stochastic objective at timestep t) m t ← φ t (g 1 , • • • , g t ) (Calculate momentum) l t ← ψ t (g 1 , • • • , g t ) (Calculate adaptive learning rate) θ t ← θ t−1 − α t m t l t (Update parameters) return θ T</formula><p>Learning rate warmup. Instead of setting the learning rate α t as a constant or in a decreasing order, a learning rate warmup strategy sets α t as smaller values in the first few steps, thus not satisfying ∀t α t+1 ≤ α t . For example, linear warmup sets α t = t α 0 when t &lt; T w . Warmup has been demonstrated to be beneficial in many deep learning applications. For example, in the NMT experiments in Figure <ref type="figure" target="#fig_0">1</ref>, the training loss convergences around 10 when warmup is not applied (Adam-vanilla), and it surprisingly decreases to below 3 after applying warmup (Adam-warmup).</p><p>To further analyze this phenomenon, we visualize the histogram of the absolute value of gradients on a log scale in Figure <ref type="figure">2</ref>. We observe that, without applying warmup, the gradient distribution is distorted to have a mass center in relatively small values within 10 updates. Such gradient distortion means that the vanilla Adam is trapped in bad/suspicious local optima after the first few -.1' -.1/ -.<ref type="foot" target="#foot_0">2</ref> &lt; -./0 -.1' -.1/ -.2 &lt; -./0 -.1' -.1/ -.2 &lt; -./0 -.1' -.1/ -.2 -.<ref type="foot" target="#foot_1">3</ref>   updates. Warmup essentially reduces the impact of these problematic updates to avoid the convergence problem. In the following sections, we focus our analysis on learning rate warmup for the Adam algorithm, while it can be applied to other algorithms that use similar adaptive learning rate (ψ(.)) designs, e.g., RMSprop <ref type="bibr" target="#b16">(Hinton et al., 2012)</ref> and Nadam <ref type="bibr" target="#b10">(Dozat, 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">VARIANCE OF THE ADAPTIVE LEARNING RATE</head><p>In this section, we first introduce empirical evidence, then analyze the variance of the adaptive learning rate to support our hypothesis -Due to the lack of samples in the early stage, the adaptive learning rate has an undesirably large variance, which leads to suspicious/bad local optima.</p><p>To convey our intuition, we begin with a special case. When t = 1, we have ψ(g 1 ) = 1/g 2 1 . We view {g 1 , • • • , g t } as i.i.d. Gaussian random variables following N (0, σ 2 ) 2 . Therefore, 1/g 2 1 is subject to the scaled inverse chi-squared distribution, Scale-inv-X 2 (1, 1/σ 2 ), and Var[ 1/g 2 1 ] is divergent. It means that the adaptive ratio can be undesirably large in the first stage of learning. Meanwhile, setting a small learning rate at the early stage can reduce the variance (Var[αx] = α 2 Var[x]), thus alleviating this problem. Therefore, we suggest it is the unbounded variance of the adaptive learning rate in the early stage that causes the problematic updates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">WARMUP AS VARIANCE REDUCTION</head><p>In this section, we design a set of controlled experiments to verify our hypothesis. Particularly, we design two variants of Adam that reducing the variance of the adaptive learning rate: Adam-2k and Adam-eps. We compare them to vanilla Adam with and without warmup on the IWSLT'14 German to English translation dataset <ref type="bibr" target="#b5">(Cettolo et al., 2014)</ref>.</p><p>In order to reduce the variance of the adaptive learning rate (ψ(.)), Adam-2k only updates ψ(.) in the first two thousand iterations, while the momentum (φ(.)) and parameters (θ) are fixed 3 ; other than this, it follows the original Adam algorithm. To make comparison with other methods, its iterations are indexed from -1999 instead of 1. In Figure <ref type="figure" target="#fig_0">1</ref>, we observe that, after getting these additional two thousand samples for estimating the adaptive learning rate, Adam-2k avoids the convergence problem of the vanilla-Adam. Also, comparing Figure <ref type="figure">2</ref> and Figure <ref type="figure">3</ref>, getting large enough samples prevents the gradient distribution from being distorted. These observations verify our hypothesis that the lack of sufficient data samples in the early stage is the root cause of the convergence issue.</p><p>Another straightforward way to reduce the variance is to increase the value of in ψ</p><formula xml:id="formula_3">(g 1 , • • • , g t ) = √ 1−β t 2 + √ (1−β2) t i=1 β t−i 2 g 2 i .</formula><p>Actually, if we assume ψ(.) is subject to the uniform distribution, its variance equals to 1 12 2 . Therefore, we design Adam-eps, which uses a non-negligibly large = 10 −4 , while = 10 −8 for vanilla Adam. Its performance is summarized in Figure <ref type="figure" target="#fig_0">1</ref>. We observe that it does not suffer from the serious convergence problem of vanilla-Adam. This further demonstrates that the convergence problem can be alleviated by reducing the variance of the adaptive learning rate, and also explains why tuning is important in practice <ref type="bibr" target="#b21">(Liu et al., 2019)</ref>. Besides, similar to Adam-2k, it prevents the gradient distribution from being distorted (as shown in Figure <ref type="figure">3</ref>). However, as in Figure <ref type="figure" target="#fig_0">1</ref>, it produces a much worse performance comparing to Adam-2k and Adam-warmup. We conjecture that this is because large induces a large bias into the adaptive learning rate and slows down the optimization process. Thus, we need a more principled and rigorous way to control the variance of the adaptive learning rate. In the next subsection, we will present a theoretical analysis of the variance of the adaptive learning rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">ANALYSIS OF ADAPTIVE LEARNING RATE VARIANCE</head><p>As mentioned before, Adam uses the exponential moving average to calculate the adaptive learning rate. For gradients {g 1 , • • • , g t }, their exponential moving average has a larger variance than their simple average. Also, in the early stage (t is small), the difference of the exponential weights of</p><formula xml:id="formula_4">{g 1 , • • • , g t } is relatively small (up to 1 − β t−1</formula><p>2 ). Therefore, for ease of analysis, we approximate the distribution of the exponential moving average as the distribution of the simple average <ref type="bibr" target="#b23">(Nau, 2014)</ref>, i.e., p(ψ(.)) = p(</p><formula xml:id="formula_5">1−β t 2 (1−β2) t i=1 β t−i 2 g 2 i ) ≈ p( t t i=1 g 2 i ). Since g i ∼ N (0, σ 2 ), we have t t i=1 g 2 i ∼ Scale-inv-X 2 (t, 1 σ 2 )</formula><p>. Therefore, we assume</p><formula xml:id="formula_6">1−β t 2 (1−β2) t i=1 β t−i 2 g 2</formula><p>i also subjects to a scaled inverse chi-square distribution with ρ degrees of freedom (further analysis on this approximation is conducted in Section 5.3). Based on this assumption, we can calculate Var[ψ 2 (.)] and the PDF of ψ 2 (.). Now, we proceed to the analysis of its square root variance, i.e., Var[ψ(.)], and show how the variance changes with ρ (which corresponds to number of used training samples). <ref type="bibr">)</ref>] monotonically decreases as ρ increases.</p><formula xml:id="formula_7">Theorem 1. If ψ 2 (.) ∼ Scale-inv-X 2 (ρ, 1 σ 2 ), Var[ψ(.</formula><p>Proof. For ∀ ρ &gt; 4, we have:</p><formula xml:id="formula_8">Var[ψ(.)] = E[ψ 2 (.)] − E[ψ(.)] 2 = τ 2 ( ρ ρ − 2 − ρ 2 2ρ−5 π B( ρ − 1 2 , ρ − 1 2 ) 2 ),<label>(2)</label></formula><p>where B(.) is the beta function. By analyzing the derivative of Var[ψ(.)], we know it monotonically decreases as ρ increases. The detailed derivation is elaborated in the Appendix A.</p><p>Theorem 1 gives a qualitative analysis of the variance of the adaptive learning rate. It shows that, due to the lack of used training samples in the early stage, Var[ψ(.)] is larger than the late stage (Figure <ref type="figure">8</ref>). To rigorously constraint the variance, we perform a quantified analysis on Var[ψ(.)] by estimating the degree of freedoms ρ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RECTIFIED ADAPTIVE LEARNING RATE</head><p>In the previous section, Equation <ref type="formula" target="#formula_8">2</ref>gives the analytic form of Var[ψ(.)], where ρ is the degree of freedoms. Here, we first give an estimation of ρ based on t to conduct a quantified analysis for</p><formula xml:id="formula_9">Var[ψ(g 1 , • • • , g t )]</formula><p>, then we describe the design of the learning rate rectification, and compare it to the heuristic warmup strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">ESTIMATION OF ρ</head><p>The exponential moving average (EMA) can be interpreted as an approximation to the simple moving average (SMA) in real application <ref type="bibr" target="#b23">(Nau, 2014)</ref>, i.e.,</p><formula xml:id="formula_10">p (1 − β 2 ) t i=1 β t−i 2 g 2 i 1 − β t 2 ≈ p f (t,β2) i=1 g 2 t+1−i f (t, β 2 ) . (<label>3</label></formula><formula xml:id="formula_11">)</formula><p>Algorithm 2: Rectified Adam. All operations are element-wise.</p><p>Input: {α t } T t=1 : step size, {β 1 , β 2 }: decay rate to calculate moving average and moving 2nd moment, θ 0 : initial parameter, f t (θ): stochastic objective function. Output: θ t : resulting parameters m 0 , v 0 ← 0, 0 (Initialize moving 1st and 2nd moment) where f (t, β 2 ) is the length of the SMA which allows the SMA to have the same "center of mass" with the EMA. In other words, f (t, β 2 ) satisfies:</p><formula xml:id="formula_12">ρ ∞ ← 2/(1 − β 2 ) − 1 (Compute the maximum length of the approximated SMA) while t = {1, • • • , T } do g t ← ∆ θ f t (θ t−1 ) (Calculate gradients w.r.t. stochastic objective at timestep t) v t ← 1/β 2 v t−1 + (1 − β 2 )g 2 t (Update exponential moving 2nd moment) m t ← β 1 m t−1 + (1 − β 1 )g t (Update exponential moving 1st moment) m t ← m t /(1 − β t 1 ) (Compute bias-corrected moving average) ρ t ← ρ ∞ − 2tβ t 2 /(1 − β t 2 )(</formula><formula xml:id="formula_13">(1 − β 2 ) t i=1 β t−i 2 • i 1 − β t 2 = f (t,β2) i=1 (t + 1 − i) f (t, β 2 ) .<label>(4)</label></formula><p>By solving Equation 4, we have:</p><formula xml:id="formula_14">f (t, β 2 ) = 2 1−β2 − 1 − 2tβ t 2 1−β t 2 .</formula><p>In the previous section, we assume:</p><formula xml:id="formula_15">1−β t 2 (1−β2) t i=1 β t−i 2 g 2 i ∼ Scale-inv-X 2 (ρ, 1 σ 2 ).</formula><p>Here, since g i ∼ N (0, σ 2 ), we have</p><formula xml:id="formula_16">f (t,β 2 ) i=1 g 2 t+1−i f (t,β2)</formula><p>∼ Scale-inv-X 2 (f (t, β 2 ), 1 σ 2 ). Thus, Equation 3 views Scale-inv-X 2 (f (t, β 2 ), 1 σ 2 ) as an approximation to Scale-inv-X 2 (ρ, 1 σ 2 ). Therefore, we treat f (t, β 2 ) as an estimation of ρ. For ease of notation, we mark f (t, β 2 ) as ρ t . Also, we refer 2 1−β2 − 1 as ρ ∞ (maximum length of the approximated SMA), due to the inequality f (t,</p><formula xml:id="formula_17">β 2 ) ≤ lim t→∞ f (t, β 2 ) = 2 1−β2 − 1.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">VARIANCE ESTIMATION AND RECTIFICATION</head><p>Based on previous estimations, we have</p><formula xml:id="formula_18">Var[ψ(.)] = τ 2 ( ρt ρt−2 − ρt 2 2ρ t −5 π B( ρt−1 2 , ρt−1 2 ) 2</formula><p>). The value of this function in the early stage is significantly larger than the late stage (as analyzed later, it decays roughly at the speed of O( 1 ρt )). For example, the variance at ρ t = 5 is over 100 times larger than the variance at ρ t = 500. Additionally, based on Theorem 1, we know min ρt Var[ψ(.)] = Var[ψ(.)]| ρt=ρ∞ and mark this minimal value as C var . In order to ensure that the adaptive learning rate (ψ(.)) has consistent variance, we rectify the variance at the t-th timestamp as below,</p><formula xml:id="formula_19">Var[r t ψ(g 1 , • • • , g t )] = C var where r t = C var /Var[ψ(g 1 , • • • , g t )]</formula><p>. Although we have the analytic form of Var[ψ(.)] (i.e., Equation <ref type="formula" target="#formula_8">2</ref>), it is not numerically stable. Therefore, we use the first-order approximation to calculate the rectification term. Specifically, by approximating ψ 2 (.) to the first order <ref type="bibr" target="#b29">(Wolter, 2007)</ref>,</p><formula xml:id="formula_20">ψ 2 (.) ≈ E[ψ 2 (.)] + 1 2 E[ψ 2 (.)] (ψ 2 (.) − E[ψ 2 (.)]) and Var[ψ(.)] ≈ Var[ψ 2 (.)] 4 E[ψ 2 (.)] .</formula><p>Since ψ 2 (.) ∼ Scale-inv-X 2 (ρ t , 1 σ 2 ), we have:</p><formula xml:id="formula_21">Var[ψ(.)] ≈ ρ t /[2(ρ t − 2)(ρ t − 4)σ 2 ].</formula><p>(5) In Section 5.3, we conduct simulation experiments to examine Equation 5 and find that it is a reliable approximation. Based on Equation <ref type="formula">5</ref>, we know that Var[ ψ(.)] decreases approximately at the    With this approximation, we can calculate the rectification term as:</p><formula xml:id="formula_22">r t = (ρ t − 4)(ρ t − 2)ρ ∞ (ρ ∞ − 4)(ρ ∞ − 2)ρ t .</formula><p>Applying our rectification term to Adam, we come up with a new variant of Adam, Rectified Adam (RAdam), as summarized in Algorithm 2. Specifically, when the length of the approximated SMA is less or equal than 4, the variance of the adaptive learning rate is intractable and the adaptive learning rate is inactivated. Otherwise, we calculate the variance rectification term and update parameters with the adaptive learning rate. It is worth mentioning that, if β 2 ≤ 0.6, we have ρ ∞ ≤ 4 and RAdam is degenerated to SGD with momentum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">IN COMPARISON WITH WARMUP AND OTHER STABILIZATION TECHNIQUES</head><p>Different from the analysis in this paper, warmup is originally proposed to handle training with very large batches for SGD <ref type="bibr" target="#b14">(Goyal et al., 2017;</ref><ref type="bibr" target="#b13">Gotmare et al., 2019;</ref><ref type="bibr" target="#b3">Bernstein et al., 2018;</ref><ref type="bibr" target="#b30">Xiao et al., 2017)</ref>. We notice that r t has a similar form to the heuristic linear warmup, which can be viewed as setting the rectification term as min(t,Tw)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tw</head><p>. It verifies our intuition that warmup works as a variance reduction technique. RAdam deactivates the adaptive learning rate when its variance is divergent, thus avoiding undesired instability in the first few updates. Besides, our method does not require an additional hyperparameter (i.e., T w ) and can automatically adapt to different moving average rules.</p><p>Here, we identify and address an underlying issue of adaptive optimization methods independent of (neural) model architectures. Thus, the proposed rectification term is orthogonal to other training stabilization techniques such as gradient clipping <ref type="bibr" target="#b2">(Bengio et al., 2013)</ref>, smoothing the adaptive learning rate (i.e., increasing , applying geometric mean filter <ref type="bibr" target="#b7">(Chen &amp; Gu, 2018)</ref>, or adding range constraints (Luo et al., 2019)), initialization <ref type="bibr" target="#b1">(Balduzzi et al., 2017;</ref><ref type="bibr" target="#b32">Zhang et al., 2019)</ref> and normalization <ref type="bibr" target="#b0">(Ba et al., 2016;</ref><ref type="bibr" target="#b17">Ioffe &amp; Szegedy, 2015)</ref>. Indeed, these techniques can be combined with the proposed variance rectification method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>We evaluate RAdam on several benchmarks: One Billion Word for language modeling; Cifar10 and ImageNet for image classification; IWSLT'14 De-En/EN-DE and WMT'16 EN-De for neural machine translation. Following <ref type="bibr" target="#b22">Loshchilov &amp; Hutter (2018)</ref>, we decouple weight decays in the vanilla Adam, Adam with warmup and RAdam in our experiments. Details are in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">COMPARING TO VANILLA ADAM</head><p>As analyzed before, the adaptive learning rate has undesirably large variance in the early stage of training and leads to suspicious/bad local optima on NMT. One question we are interested in Different learning rates lead to similar performance.</p><p>Sensitive to the choice of the learning rate.</p><p>X-axis is the epoch #. is: whether such an issue widely exits in other similar tasks and applications. Thus, we conduct a set of experiments with two classical tasks of NLP and CV, i.e., language modeling and image classification. RAdam not only results in consistent improvements over the vanilla Adam, but also demonstrates its robustness to the change of learning rates. It verifies that the variance issue exists in various machine learning applications, and has a big impact on the model behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance Comparison.</head><p>The performances on language modeling (i.e., One Billion Word <ref type="bibr" target="#b6">(Chelba et al., 2013)</ref>) and image classification (i.e., CIFAR10 <ref type="bibr" target="#b19">(Krizhevsky et al., 2009)</ref> and ImageNet <ref type="bibr" target="#b8">(Deng et al., 2009)</ref>) are presented in Figure <ref type="figure" target="#fig_5">4</ref>, 5. The results show that RAdam outperforms Adam in all three datasets. As shown in Figure <ref type="figure" target="#fig_5">4</ref>, although the rectification term makes RAdam slower than the vanilla Adam in the first few epochs, it allows RAdam to converge faster after that. In other words, by reducing the variance of the adaptive learning rate in the early stage, it gets both faster convergence and better performance, which verifies the impact of the variance issue. We also observe that RAdam obtains consistent improvements over Adam on image classification. It is worth noting that, on both ImageNet and CIFAR10, although RAdam fails to outperform SGD in terms of test accuracy, it results in a better training performance (e.g., the training accuracy of SGD, Adam, and RAdam on ImageNet are 69.57, 69.12 and 70.30 respectively).</p><p>Robustness to Learning Rate Change. Besides performance improvements, RAdam also improves the robustness of model training. We use different initial learning rates, conduct experiments with ResNet-20 on the CIFAR10 datasets, and summarize their performance in Figure <ref type="figure">6</ref>. For learning rates within a broad range (i.e., {0.1, 0.03, 0.01, 0.003}), RAdam achieves consistent model performances (their test accuracy curves highly overlap with each other), while Adam and SGD are shown to be more sensitive to the learning rate. The observation can be interpreted that by rectifying the variance of the adaptive learning rate, RAdam improves the robustness of model training and can adapt to different learning rates of a broader range.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">COMPARING TO HEURISTIC WARMUP</head><p>To examine the effectiveness of RAdam, we first conduct comparisons on neural machine translation, on which the state-of-the-art employs Adam with the linear warmup. Specifically, we conduct experiments on three datasets, i.e., IWSLT'14 De-En, IWSLT'14 En-De, and WMT'16 En-De. Due to the limited size of the IWSLT'14 dataset, we conduct experiments using 5 different random seeds and report their mean and standard derivation. As discussed before, the vanilla Adam algorithm leads to suspicious/bad local optima (i.e., converges to a training perplexity around 500), and needs a learning rate warmup stage to stabilize the training.</p><p>We summarize the performance obtained with the heuristic warmup and our proposed rectification term in Table <ref type="table" target="#tab_2">2</ref> and visualize the training curve of IWSLT De-En in Figure <ref type="figure" target="#fig_0">1</ref>. With a consistent adaptive learning rate variance, our proposed method achieves similar performance to that of previous state-of-the-art warmup heuristics. It verifies our intuition that the problematic updates of Adam are indeed caused by the undesirably large variance in the early stage.</p><p>Moreover, we applied Adam with warmup on the CIFAR10 dataset. Its best accuracy on the test set is 91.29, which is similar to <ref type="bibr">RAdam (91.38)</ref>. However, we found that RAdam requires less hyperparameter tuning. Specifically, we visualize their learning curves in Figure <ref type="figure">7</ref>. For some warmup steps, Adam with warmup is relatively more sensitive to the choice of the learning rate. RAdam, at the same time, is not only more robust, but also can automatically control the warmup behavior (i.e., without requiring the length of warmup). For example, when setting the learning rate as 0.1, Adam with 100 steps of warmup fails to get satisfying performance and only results in an accuracy of 90.13; RAdam successfully gets an accuracy of 91.06, with the original setting of the moving average calculation (i.e., β 1 = 0.9, β 2 = 0.999). We conjecture the reason is due to the fact that RAdam, which is based on a rigorous variance analysis, explicitly avoids the extreme situation where the variance is divergent, and rectifies the variance to be consistent in other situations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">SIMULATED VERIFICATION</head><p>In Sections 3 and 4, we approximate Var[ t/ t i=1 g 2 i ] to the first order, and assume ψ 2 (.) =</p><formula xml:id="formula_23">1−β t 2 (1−β2) t i=1 β t−i 2 g 2 i</formula><p>subjects to a scaled inverse chi-square distribution (this assumption covers the approximation from EMA to SMA). Here, we examine these two approximations using simulations.</p><p>First Order Approximation of Var[ t/ t i=1 g 2 i ]. To compare Equations 5 and 2, we assume τ = 1 and plot their values and difference for ν = {5, • • • , 500} in Figure <ref type="figure">8</ref>. The curve of the analytic form and the first-order approximation highly overlap, and their difference is much smaller than their value. This result verifies that our first-order approximation is very accurate.</p><p>Scaled Inverse Chi-Square Distribution Assumption. In this paper, we assume g i accords to a Normal distribution with a zero mean. We also assume ψ 2 (.) accords to the scaled inverse chi-square distribution to derive the variance of Var[ψ(.)], based on the similarity between the exponential moving average and simple moving average. Here, we empirically verify this assumption. Specifically, since g i in the optimization problem may not be zero-mean, we assume its expectation is µ and sample g i from N (µ, 1). Then, based on these samples, we calculate the variance of the original adaptive learning rate and the proposed rectified adaptive learning rate, i.e., Var[ 1 vt ] and Var[ rt vt ] respectively. We set β 2 to 0.999, the number of sampled trajectories to 5000, the number of iterations to 6000, and summarize the simulation results in Figure <ref type="figure">9</ref>. Across all six settings with different µ, the adaptive learning rate has a larger variance in the first stage and the rectified adaptive learning rate has relative consistent variance. This verifies the reliability of our assumption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we explore the underlying principle of the effectiveness of the warmup heuristic used for adaptive optimization algorithms. Specifically, we identify that, due to the limited amount of samples in the early stage of model training, the adaptive learning rate has an undesirably large variance and can cause the model to converge to suspicious/bad local optima. We provide both empirical and theoretical evidence to support our hypothesis, and further propose a new variant where (i) is from Legendre duplication formula.</p><p>So we only need to show</p><formula xml:id="formula_24">(t − 2) Γ( t−1 2 ) 2 Γ(t/2) 2 ≤ 2 (9)</formula><p>Using Gautschi's inequality ( Γ(x+1) Γ(x+s) &lt; (x + 1) 1−s ), we have</p><formula xml:id="formula_25">(t − 2) Γ( t−1 2 ) 2 Γ(t/2) 2 ≤ (t − 2)( t − 1 2 ) −1 = 2(t − 2) t − 1 &lt; 2 (10) B IMPLEMENTATION DETAILS B.1 LANGUAGE MODELING</formula><p>Our implementation is based on the previous work <ref type="bibr" target="#b20">(Liu et al., 2018)</ref>. Specifically, we use two-layer LSTMs with 2048 hidden states with adaptive softmax to conduct experiments on the one billion words dataset. Word embedding (random initialized) of 300 dimensions is used as the input and the adaptive softmax is incorporated with a default setting (cut-offs are set to <ref type="bibr">[4000,</ref><ref type="bibr">40000,</ref><ref type="bibr">200000]</ref>).</p><p>Additionally, as pre-processing, we replace all tokens occurring equal or less than 3 times with as UNK, which shrinks the dictionary from 7.9M to 6.4M. Dropout is applied to each layer with a ratio of 0.1, gradients are clipped at 5.0. We use the default hyper-parameters to update moving averages, i.e.β 1 = 0.9 and β 2 = 0.999. The learning rate is set to start from 0.001, and decayed at the start of 10th epochs. LSTMs are unrolled for 20 steps without resetting the LSTM states and the batch size is set to 128. All models are trained on one NVIDIA Tesla V100 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 IMAGEINE CLASSIFICATION</head><p>We use the default ResNet architectures <ref type="bibr" target="#b15">(He et al., 2016)</ref> in a public pytorch re-implementation<ref type="foot" target="#foot_2">4</ref> . Specifically, we use 20-layer ResNet (9 Basic Blocks) for CIFAR-10 and 18-layer ResNet (8 Basic Blocks) for ImageNet. Batch size is 128 for CIFAR-10 and 256 for ImageNet. The model is trained for 186 epoches and the learning rate decays at the 81-th and the 122-th epoches by 0.1 on CIFAR-10, while the model is trained for 90 epoches and the learning rate decays at the 31-th and the 61-th epoch by 0.1 on ImageNet. For Adam and RAdam, we set β 1 = 0.9, β 2 = 0.999. For SGD, we set the momentum factor as 0.9. The weight decay rate is 10 −4 . Random cropping and random horizontal flipping are applied to training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 NEURAL MACHINE TRANSLATION</head><p>Our experiments are based on the default Transformers <ref type="bibr" target="#b28">(Vaswani et al., 2017)</ref> implementation from the fairseq package <ref type="bibr" target="#b24">(Ott et al., 2019)</ref>. Specifically, we use word embedding with 512 dimensions and 6-layer encoder / decoder with 4 head and 1024 hidden dimensions on the IWSLT14' dataset; use word embedding with 512 dimension and 6-layer encoder / decoder with 8 heads and 2048 hidden dimensions. Label smoothed cross entropy is used as the objective function with an uncertainty = 0.1 <ref type="bibr" target="#b27">(Szegedy et al., 2016)</ref>. We use linear learning rate decay starting from 3e −4 , and the checkpoints of the last 20 epoches are averaged before evaluation. As to the wamrup strategy, we use a linear warmup for Adam in the first 4000 updates, and set β 2 to satisfy ν = 4000 (β 2 = 0.9995). In the IWSLT'14 dataset, we conduct training on one NVIDIA Tesla V100 GPU, set maximum batch size as 4000, apply dropout with a ratio 0.3, using weight decay of 0.0001 and clip the gradient norm at 25. In the WMT'16 dataset, we conduct training on four NVIDIA Quadro R8000 GPUs and set maximum batch size as 8196.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C DOWNGRADING TO SGDM</head><p>As a byproduct determined by math derivations, we degenerated RAdam to SGD with momentum in the first several updates. Although this stage only contains several gradient updates, these up-dates could be quite damaging (e.g., in our Figure <ref type="figure">2</ref>, the gradient distribution is distorted within 10 gradient updates). Intuitively, updates with divergent adaptive learning rate variance could be more damaging than the ones with converged variance, as divergent variance implies more instability. As a case study, we performed experiments on the CIFAR10 dataset. Five-run average results are summarized in Table <ref type="table" target="#tab_3">3</ref>. The optimizer fails to get an equally reliably model when changing the first 4 updates to Adam, yet the influence of switching is less deleterious when we change 5-8 updates instead. This result verifies our intuition and is in agreement with our theory the first few updates could be more damaging than later updates. By saying that, we still want to emphasize that this part (downgrading to SGDM) is only a minor part of our algorithm design whereas our main focus is on the mechanism of warmup and the derivation of the rectification term. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Training loss v.s. # of iterations of Transformers on the De-En IWSLT'14 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: The absolute gradient histogram of the Transformers on the De-En IWSLT' 14 dataset during the training (stacked along the y-axis). X-axis is absolute value in the log scale and the height is the frequency. Without warmup, the gradient distribution is distorted in the first 10 steps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Language modeling (LSTMs) on the One Billion Word.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Training of ResNet-18 on the ImageNet and ResNet-20 on the CIFAR10 dataset. speed of O( 1 ρt ).With this approximation, we can calculate the rectification term as:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Figure 6: Performance of RAdam, Adam and SGD with different learning rates on CIFAR10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Compute the length of the approximated SMA) if the variance is tractable, i.e., ρ t &gt; 4 then</figDesc><table><row><cell>2 )/v t (Compute adaptive learning rate) l t ← (1 − β t (ρt−4)(ρt−2)ρ∞ (ρ∞−4)(ρ∞−2)ρt (Compute the variance rectification term) r t ← θ</cell></row></table><note>t ← θ t−1 − α t r t m t l t (Update parameters with adaptive momentum) else θ t ← θ t−1 − α t m t (Update parameters with un-adapted momentum) return θ T</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Image Classification</figDesc><table><row><cell></cell><cell>Method Acc.</cell></row><row><cell>CIFAR10</cell><cell>SGD Adam RAdam 91.38 91.51 90.54</cell></row><row><cell>ImageNet</cell><cell>SGD Adam RAdam 67.62 69.86 66.54</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>BLEU score on Neural Machine Translation.</figDesc><table><row><cell>Method</cell><cell cols="3">IWSLT'14 DE-EN IWSLT'14 EN-DE WMT'16 EN-DE</cell></row><row><cell>Adam with warmup RAdam</cell><cell>34.66 ± 0.014 34.76 ± 0.003</cell><cell>28.56 ± 0.067 28.48 ± 0.054</cell><cell>27.03 27.27</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Performance on CIFAR10 (lr = 0.1).</figDesc><table><row><cell>1-4 steps</cell><cell>5-8 steps</cell><cell>8+ steps</cell><cell>test</cell><cell>train</cell><cell>train</cell></row><row><cell></cell><cell></cell><cell></cell><cell>acc</cell><cell>loss</cell><cell>error</cell></row><row><cell>RAdam</cell><cell>RAdam</cell><cell>RAdam</cell><cell>91.08</cell><cell>0.021</cell><cell>0.74</cell></row><row><cell cols="2">Adam (w. divergent var.) RAdam</cell><cell>RAdam</cell><cell>89.98</cell><cell>0.060</cell><cell>2.12</cell></row><row><cell>SGD</cell><cell cols="2">Adam (w. convergent var.) RAdam</cell><cell>90.29</cell><cell>0.038</cell><cell>1.23</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">The mean zero normal assumption is valid at the beginning of the training, since weights are sampled from normal distributions with mean zero<ref type="bibr" target="#b1">(Balduzzi et al., 2017)</ref>, further analysis is conducted in Section 5.3.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">Different from<ref type="bibr" target="#b13">Gotmare et al. (2019)</ref>, all parameters and first moments are frozen in the first 2000 iterations.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2">https://github.com/bearpaw/pytorch-classification</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGE</head><p>We thank Aeyuan Allen-Zhu for valuable discussions and comments, Microsoft Research Technology Engineering team for setting up GPU machines. Research was sponsored in part by DARPA No. W911NF-17-C-0099 and FA8750-19-2-1004, National Science Foundation IIS 16-18481, IIS  17-04532, and IIS-17-41317, and DTRA HDTRA11810026.   </p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="formula">2</ref><p>, Equation 5 and their difference (absolute difference). The x-axis is ρ and the y-axis is the variance (log scale). of Adam, whose adaptive learning rate is rectified so as to have a consistent variance. Empirical results demonstrate the effectiveness of our proposed method. In future work, we plan to replace the rectification strategy by sharing the second moment estimation across similar parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A PROOF OF THEOREM 1</head><p>For ease of notation, we refer ψ 2 (.) as x and 1 σ 2 as τ 2 . Thus, x ∼ Scale-inv-X 2 (ρ, τ 2 ) and:</p><p>where Γ(.) is the gamma function. Therefore, we have:</p><p>Based on Equation 6 and 7, for ∀ ρ &gt; 4, we have:</p><p>where B(.) is the beta function. To prove the monotonic property of Var[ψ(.)], we need to show:</p><p>Proof. The target inequality can be re-wrote as ∂ ∂t</p><p>where (i) is derived from Legendre duplication formula. Simplify the above inequality, we get:</p><p>We only need to show</p><p>where the first inequality is from ln(x) − 1/(2x) &gt; Ψ(x) &gt; ln(x + 0.5) − 1/x.</p><p>Therefore, we only need to show</p><p>which is equivalent to</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lei Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Ryan Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The shattered gradients problem: If resnets are the answer, then what is the question?</title>
		<author>
			<persName><forename type="first">David</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Frean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lennox</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wan-Duo</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Mcwilliams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Advances in optimizing recurrent networks</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Boulanger-Lewandowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="8624" to="8628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">signsgd: Compressed optimisation for non-convex problems</title>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamyar</forename><surname>Azizzadenesheli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Méthode générale pour la résolution des systemes déquations simultanées</title>
		<author>
			<persName><forename type="first">Augustin</forename><surname>Cauchy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comp. Rend. Sci. Paris</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="536" to="538" />
			<date type="published" when="1847">1847. 1847</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Report on the 11th iwslt evaluation campaign, iwslt</title>
		<author>
			<persName><forename type="first">Mauro</forename><surname>Cettolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Stüker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Spoken Language Translation</title>
				<meeting>the International Workshop on Spoken Language Translation</meeting>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">One billion word benchmark for measuring progress in statistical language modeling</title>
		<author>
			<persName><forename type="first">Ciprian</forename><surname>Chelba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Brants</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><surname>Robinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Closing the generalization gap of adaptive gradient methods in training deep neural networks</title>
		<author>
			<persName><forename type="first">Jinghui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanquan</forename><surname>Gu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.06763</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Incorporating nesterov momentum into adam</title>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Dozat</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT</title>
				<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Carl-Friedrich</forename><surname>Gauss</surname></persName>
		</author>
		<title level="m">Theoria combinationis observationum erroribus minimis obnoxiae. Commentationes Societatis Regiae Scientiarum Gottingensis Recentiores</title>
				<imprint>
			<date type="published" when="1823">1823</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A closer look at deep learning heuristics: Learning rate restarts, warmup and distillation</title>
		<author>
			<persName><forename type="first">Akhilesh</forename><surname>Gotmare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Andrew Tulloch</title>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
	</analytic>
	<monogr>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Neural networks for machine learning lecture 6a overview of mini-batch gradient descent</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>Cited on</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Efficient contextualized representation: Language model pruning for sequence labeling</title>
		<author>
			<persName><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>EMNLP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adaptive gradient methods with dynamic bound of learning rate</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter ; Liangchen Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanhao</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<editor>
			<persName><surname>Iclr</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2018">2018. 2019</date>
		</imprint>
	</monogr>
	<note>Fixing weight decay regularization in adam</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Forecasting with moving averages</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Nau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">fairseq: A fast, extensible toolkit for sequence modeling</title>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Training tips for the transformer model</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Popel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Prague Bulletin of Mathematical Linguistics</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="43" to="70" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">On the convergence of adam and beyond</title>
		<author>
			<persName><forename type="first">Satyen</forename><surname>Sashank J Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjiv</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Taylor series methods. In Introduction to variance estimation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kirk</surname></persName>
		</author>
		<author>
			<persName><surname>Wolter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dscovr: Randomized primal-dual block coordinate algorithms for asynchronous distributed optimization</title>
		<author>
			<persName><forename type="first">Lin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qihang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><surname>Matthew D Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<title level="m">Adadelta: an adaptive learning rate method</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fixup initialization: Residual learning without normalization</title>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
