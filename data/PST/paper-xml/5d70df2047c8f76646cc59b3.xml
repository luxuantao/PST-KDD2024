<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Double-Branch Multi-Attention Mechanism Network for Hyperspectral Image Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-06-01">1 June 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wenping</forename><surname>Ma</surname></persName>
							<email>wpma@mail.xidian.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">International Research Center for Intelligent Perception and Computation</orgName>
								<orgName type="department" key="dep2">School of Artificial Intelligence</orgName>
								<orgName type="laboratory" key="lab1">Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education</orgName>
								<orgName type="laboratory" key="lab2">Joint International Research Laboratory of Intelligent Perception and Computation</orgName>
								<orgName type="institution">Xidian University</orgName>
								<address>
									<postCode>710071</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qifan</forename><surname>Yang</surname></persName>
							<email>qfyang_1@stu.xidian.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">International Research Center for Intelligent Perception and Computation</orgName>
								<orgName type="department" key="dep2">School of Artificial Intelligence</orgName>
								<orgName type="laboratory" key="lab1">Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education</orgName>
								<orgName type="laboratory" key="lab2">Joint International Research Laboratory of Intelligent Perception and Computation</orgName>
								<orgName type="institution">Xidian University</orgName>
								<address>
									<postCode>710071</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yue</forename><surname>Wu</surname></persName>
							<idno type="ORCID">0000-0002-3459-5079</idno>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Xidian University</orgName>
								<address>
									<postCode>710071</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Zhao</surname></persName>
							<email>weizhao_90@stu.xidian.edu.cn</email>
							<idno type="ORCID">0000-0002-3459-5079</idno>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">International Research Center for Intelligent Perception and Computation</orgName>
								<orgName type="department" key="dep2">School of Artificial Intelligence</orgName>
								<orgName type="laboratory" key="lab1">Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education</orgName>
								<orgName type="laboratory" key="lab2">Joint International Research Laboratory of Intelligent Perception and Computation</orgName>
								<orgName type="institution">Xidian University</orgName>
								<address>
									<postCode>710071</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiangrong</forename><surname>Zhang</surname></persName>
							<email>xrzhang@mail.xidian.edu.cn</email>
							<idno type="ORCID">0000-0003-0379-2042</idno>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">International Research Center for Intelligent Perception and Computation</orgName>
								<orgName type="department" key="dep2">School of Artificial Intelligence</orgName>
								<orgName type="laboratory" key="lab1">Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education</orgName>
								<orgName type="laboratory" key="lab2">Joint International Research Laboratory of Intelligent Perception and Computation</orgName>
								<orgName type="institution">Xidian University</orgName>
								<address>
									<postCode>710071</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Double-Branch Multi-Attention Mechanism Network for Hyperspectral Image Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-06-01">1 June 2019</date>
						</imprint>
					</monogr>
					<idno type="MD5">BC73D1D27316566C24CF9B4A7BE02D6E</idno>
					<idno type="DOI">10.3390/rs11111307</idno>
					<note type="submission">Received: 8 May 2019; Accepted: 27 May 2019;</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>hyperspectral image classification</term>
					<term>spectral-spatial feature fusion</term>
					<term>channel-wise attention</term>
					<term>spatial-wise attention</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, Hyperspectral Image (HSI) classification has gradually been getting attention from more and more researchers. HSI has abundant spectral and spatial information; thus, how to fuse these two types of information is still a problem worth studying. In this paper, to extract spectral and spatial feature, we propose a Double-Branch Multi-Attention mechanism network (DBMA) for HSI classification. This network has two branches to extract spectral and spatial feature respectively which can reduce the interference between the two types of feature. Furthermore, with respect to the different characteristics of these two branches, two types of attention mechanism are applied in the two branches respectively, which ensures to extract more discriminative spectral and spatial feature. The extracted features are then fused for classification. A lot of experiment results on three hyperspectral datasets shows that the proposed method performs better than the state-of-the-art method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recently, remote sensing image has been studied in more and more areas, including image registration <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref>, change detection <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, object detection <ref type="bibr" target="#b5">[6]</ref> and so on. As is known to all, Hyperspectral Imaging (HSI) is a special type of remote sensing image which has abundant spectral and spatial information <ref type="bibr" target="#b6">[7]</ref>, and has been studied in many fields, including forest vegetation cover monitoring <ref type="bibr" target="#b7">[8]</ref>, classification of land-use <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>, change area detection <ref type="bibr" target="#b10">[11]</ref>, anomaly detection <ref type="bibr" target="#b11">[12]</ref> and environmental protection <ref type="bibr" target="#b12">[13]</ref>.</p><p>In HSI, supervised classification is the most studied task. However, the high-dimensional nature of the spectral channel can bring with it the 'curse of dimensionality', which makes conventional techniques inefficient. How to extract the most discriminative feature from the high dimensionality of the spectral channel is the key in HSI classification. Therefore, traditional HSI classification methods usually contain two steps, e.g., feature engineering and classifier classification. There are two mainstreams in feature engineering, one is feature selection and the other is feature extraction. Feature selection aims to pick up several spectral channel to reduce dimensionality and feature extraction refers to using some nonlinear mapping function to transform the original spectral domain to a lower dimensional space. After feature engineering, the selected feature or extracted feature will be fed to general-purpose classifiers for classification.</p><p>In the early stage, researchers focused on spectral-based methods and without considering the spatial information. However, HSI has local consistency, so some researchers took spatial information into consideration and had performed better. Gabor feature <ref type="bibr" target="#b13">[14]</ref> and differential morphological profile (DMP) <ref type="bibr" target="#b14">[15]</ref> feature are two types of low-level feature which could represent the shape information of the HSI and could also lead to satisfactory classification results. In <ref type="bibr" target="#b15">[16]</ref>, Paheding et al. used multiscale spatial texture features for HSI classification. However, The HSI usually contains various types and levels features, so it is impossible to describe all types of objects by setting empirical parameters. One method may perform well on a dataset while performs worse on another dataset.</p><p>Deep Learning (DL) has shown extremely powerful ability to extract hierarchical and nonlinear features, which are very useful for classification. So far, many works based on DL have been done in the community of HSI classification. For example, Chen et al. <ref type="bibr" target="#b16">[17]</ref> used stacked autoencoder (SAE) to extract spectral and spatial features and use logistic regression to get classification result. Similarly, they used a Restricted Boltzmann Machine (RBM) and deep belief network (DBN) in <ref type="bibr" target="#b17">[18]</ref> for classification. Tao et al. <ref type="bibr" target="#b18">[19]</ref> used two sparse stacked auto-encoder to learn the spatial and spectral features of the HSI separately, then he stacked the spatial and spectral features and fed them into a liner SVM for classification. Ma et al. <ref type="bibr" target="#b19">[20]</ref> used a spatial updated deep autoencoder to extract both spatial and spectral information with a single deep network, and utilized an improved collaborative representation in feature space for classification. Zhang et al. <ref type="bibr" target="#b20">[21]</ref> utilized a recursive autoencoder to learn spatial and spectral information and adopted a weighting scheme to fuse the spatial information. In <ref type="bibr" target="#b21">[22]</ref>, Paheding et al. proposed a Progressively Expanded Neural Network (PEN Net), which is a novel neural network.</p><p>The input of the aforementioned methods is one dimensional, and they utilized the spatial feature but destroyed the initial spatial structure. With the emergence of the convolutional neural network (CNN), some new methods have also been introduced. CNN can extract the spatial information without destroying the original spatial structure. For example, Hu et al. <ref type="bibr" target="#b22">[23]</ref> employed deep CNN for HSI classification. Chen et al. <ref type="bibr" target="#b23">[24]</ref> proposed a novel 3D-CNN model combined with regularization to extract spectral-spatial features for classification. The obtained results reveal that 3D-CNN perform better than 1D-CNN and 2D-CNN. Mercedes E. Paoletti et al. <ref type="bibr" target="#b24">[25]</ref> proposed the deep pyramidal residual network to extract multi-scale spatial feature for classification. Recently, some new training methods also have emerged in the literature, including active learning <ref type="bibr" target="#b25">[26]</ref>, self-pace learning <ref type="bibr" target="#b26">[27]</ref>, semi-supervised learning <ref type="bibr" target="#b27">[28]</ref> and generative adversarial network (GAN) <ref type="bibr" target="#b28">[29]</ref>. Furthermore, some superpixels based methods also play an important role in HSI classification <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref>. In <ref type="bibr" target="#b31">[32]</ref>, Jiang et al. studied the influence of label noise on the HSI classification problem and proposed a random label propagation algorithm (RLPA) which is used to cleanse the label noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Motivation</head><p>Inspired by the residual network <ref type="bibr" target="#b32">[33]</ref>, Zhong et al. <ref type="bibr" target="#b33">[34]</ref> proposed a Spectral-Spatial Residual Network (SSRN) which contains spectral residual block and spatial residual block to extract spectral features and spatial features sequentially. SSRN has achieved the state-of-the-art performance in HSI classification problem. Based on SSRN and DenseNet <ref type="bibr" target="#b34">[35]</ref>, Wang et al. <ref type="bibr" target="#b35">[36]</ref> proposed a fast densely connected spectral-spatial convolution network (FDSSC) for HSI classification and has achieved better performance while reducing the training time.</p><p>Although SSRN and FDSSC have achieved the highest classification accuracy, there are still some problems need to be solved. The biggest problem is that the two frameworks firstly extracts spectral features then extracts spatial features. In the procedure of extracting spatial features, the extracted spectral features may be destroyed because the spectral features and spatial features are in different domain.</p><p>More recently, Fang et al. <ref type="bibr" target="#b36">[37]</ref> proposed a network using 3-D CNN with spectral-wise attention mechanism (MSDN-SA) which applied spectral-wise attention mechanism in a densely connected 3D convolution network. However, it only considers the spectral-wise attention while not considering the spatial-wise attention.</p><p>Recently, an intuitive and effective attention module named Convolutional Block Attention Module (CBAM) was proposed in <ref type="bibr" target="#b37">[38]</ref>, which sequentially applies channel attention mechanism and spatial attention mechanism in the network to adaptively refine the feature map, which results in improvements in classification performance.</p><p>Inspired by the CBAM and to solve the problem of SSRN and FDSSC, we propose the double-branch multi-attention mechanism network for HSI classification. The framework consists of two parallel branches, i.e., spectral branch and spatial branch. To extract more discriminative features, in the spectral branch and spatial branch we apply channel-wise attention and spatial-wise attention separately. After the two branches extract corresponding features, we fuse them by a concatenation operation to get the spectral-spatial feature. Finally, the softmax classifier are added to get the last classification result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Contribution</head><p>To be summarized, our main contributions can be listed as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>We propose a densely connected 3DCNN-based Double-Branch Multi-Attention mechanism network (DBMA). This network has two branches to extract spectral and spatial features separately which can reduce the interference between the two types of features. The extracted spectral and spatial features are fused for classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>We apply both the channel-wise attention and spatial-wise attention in the HSI classification problem. The channel-wise attention is aiming to emphasize informative spectral features while suppress less useful spectral features, while the spatial attention is aimed at focusing on the most informative ares in the input patches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>Compared with other recently proposed methods, the proposed network achieves the best classification accuracy. Furthermore, the training time and test time of our proposed network are also less than the two compared deep-learning algorithm, which indicates the superiority of our method.</p><p>The rest of this paper is organized as follows: Section 2 illustrates the related work. Section 3 presents a detailed description of the proposed classification method. The experiment results and analysis are provided in Section 4. Finally, Section 5 concludes the whole paper and briefly introduce our future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we will briefly introduce some basic knowledge and related work, including cube-based HSI classification framework, residual connection and densely connection, FDSSC and attention mechnasim.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Cube-Based HSI Classification Framework</head><p>Traditional pixel-based classification architecture only uses spectral information for classification while cube-based architecture uses both spectral and spatial information. Given an HSI dataset with size of X ∈ R w×h×d , There are total w × h pixels in the image, however, only N pixels has corresponding labels. Firstly, we random split the pixels with their labels into three sets, i.e., training set, validation set and test set. Then, we extract the 3D cube as the input of the network. Different from a pixel-based architecture which directly uses the pixel as input to train network for classification, cube-based framework uses 3D structure of HSI for classification. The reason using cube-based framework is that the spatial information is also important for classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Residual Connection and Densely Connection</head><p>Residual connection was first proposed in <ref type="bibr" target="#b32">[33]</ref>. In principle, a residual connection adds a skip connection in the basic of tradition CNN model. As is shown in Figure <ref type="figure" target="#fig_0">1a</ref>, H is the abbreviation of hidden block and represents several convolutional layers with activation layers and BatchNorm layers.</p><p>ResNet allows input information to be passed directly to subsequent layers. The skip connection can be seen as an identity mapping. In ResNet, the output of the l-th block can be computed as:</p><formula xml:id="formula_0">x l = H l (x l-1 ) + x l-1<label>(1)</label></formula><p>Through the residual connection, the original function F(x) can be transformed to H(x) + x. In addition the H(x) is easy to learn than F(x). Therefore, ResNet can achieve better result than traditional CNN models. Furthermore, ResNet wouldn't bring extra parameters but can speed up the training process. Based on residual connection, Gao et al. <ref type="bibr" target="#b34">[35]</ref> proposed the concept of densely connection and DenseNet. In DenseNet, any hidden block has path to any previous block and back block. Differing from the residual connection, which combines features through summation, dense connectivity combines features by concatenating them. In DenseNet, all previous feature maps of lblocks can be used to compute the output of the l-th block:</p><formula xml:id="formula_1">0 x 1 x 1 l x  l x 0 H 1 H 2 H 3 H 1 l x  l x 2 l x  2 l H  1 l H  (a) Residual connection 0 x 1 x 1 l x  l x 0 H 1 H 2 H 3 H 1 l x  l x 2 l x  2 l H  1 l H  (b) Densely connection</formula><formula xml:id="formula_2">x l = H l [x 0 , x 1 , ..., x l-1 ]<label>(2)</label></formula><p>where x 0 , ..., x l-1 is the feature maps of the previous blocks. H l (•) consists of batch normalization (BN), activation layers and convolution layers. In DenseNet, as is shown in Figure <ref type="figure" target="#fig_0">1b</ref>, each block has been linked to each previous block and back block. Note that if each function H l produces k feature maps, the (l + 1)th layer will have k 0 + k × (l -1) input features, where k 0 is the number of channels in the input layer, while the output will still be k feature maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Fast Dense Spectral-Spatial Convolution Network (FDSSC)</head><p>Based on residual connection, Zhong et al. <ref type="bibr" target="#b33">[34]</ref> proposed a Spectral-Spatial Residual Network (SSRN) which contains spectral residual block and spatial residual block to extract spectral features and spatial features sequentially. Inspired by SSRN and DenseNet, Wang et al. <ref type="bibr" target="#b35">[36]</ref> proposed the FDDSC network for HSI classification which achieved better performance while reduced the training time. In this part, we will introduce FDSSC in detail.</p><p>As illustrated in <ref type="bibr" target="#b35">[36]</ref>, the structure of FDSSC is shown in Figure <ref type="figure" target="#fig_1">2</ref>. FDSSC consists of a dense spectral block, a reducing dimension block and a dense spatial block. The input patch of FDSSC is set to 9 × 9 × L. The dense spectral block aims to extract spectral feature using densely connected 3D convolution and the kernel size is set to 1 × 1 × 7. The 1 × 1 × d (d &gt; 1) convolution operation does not extract any spatial features because the kernel size of spatial dimension is set to 1. Therefore, a kernel size of 1 × 1 × 7 extracts the spectral features and retains the spatial features. Through the dense spectral block, we get spectral feature with size of (9 × 9 × b, 60). 60 refers to the number of feature maps. The reducing dimension block aims to reduce the dimension of feature maps and the number of parameters to be trained. In reducing dimension block, the padding method of 3D convolution is set to 'valid' to decrease the size of feature maps. After learning the spectral features, we get 60 feature maps with size of 9 × 9 × b. Then, the 3D convolution layer with kernel size of 1 × 1 × b is used to get 200 feature maps with size of 9 × 9 × 1. After that, the feature maps are reshaped to get 1 feature map with size of 9 × 9 × 200. To further reduce the dimension of feature maps' size, the convolution layer with kernel size of 3 × 3 × 200 transformed the feature maps to get feature maps with size of (7 × 7 × 1, 24).</p><p>Then, the dense spatial block is used to extract spatial features. The kernel size in the dense spatial block is set to 3 × 3 × 1. A kernel with size of a × a × 1 (a &gt; 1) learns the spatial features while not learning any spectral features.</p><p>After the dense spatial block, we get feature with size of (7 × 7 × 1, 60). Then, the global average pooling layer is employed to get a feature vector with length of 60. The global average pooling layer can be seen as a special case of pooling layer which can aggregate information and reduce parameters. The feature vector is feed to softmax classifier for classification result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Attention Mechanism</head><p>Inspired by the human perception process <ref type="bibr" target="#b38">[39]</ref>, the attention mechanism has been applied in the image categorization <ref type="bibr" target="#b39">[40]</ref>, and were later shown to yield significant improvements for Visual Question Answering (VQA) and captioning <ref type="bibr" target="#b40">[41]</ref><ref type="bibr" target="#b41">[42]</ref><ref type="bibr" target="#b42">[43]</ref>. As is known to all, the importance of every spectral channel and the area of the input patch is different while extracting features. In addition, the attention mechanism can focus on the most informative part and decrease other region's weight, which is believed to be similar to the human eye's attention mechanism. In CBAM <ref type="bibr" target="#b37">[38]</ref>, the network has two attention module, i.e., channel attention module and spatial attention module which focus on informative channel and informative area respectively. Later, we will introduce the two modules in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1.">Channel-Wise Attention Module</head><p>The channel-wise attention module mainly refines the feature maps' weight in the channel-wise. Each channel of the feature map can be seen as a feature detector, and channel attention focuses on the meaningful channel and decrease the meaningless channel's value to a certain degree.</p><p>As is shown in Figure <ref type="figure" target="#fig_2">3</ref>, a MaxPooling layer and an AvgPooling layer are used to aggregate spatial information, the two pooling operations can be seen as two different spatial descriptors: F c avg and F c max , which denote average-pooled features and max-pooled features respectively. Note that the output features are a one-dimensional vector and the length of the vector is the same as the number of the input features. Then the two types of features are feed forwarded to a shared network to produce the channel attention map. The shared network is composed of a 3-layer perceptron (MLP) with one hidden layer. The hidden layer has C/L units, which is used to reduce the training numbers and generate more nonlinear mapping, where L is the reduction ratio and C is the channel numbers. Then the output feature vectors are merged using element-wise summation. Through the sigmoid function, the channel attention map is obtained. The channel attention map is a vector of which the length is the same as the number of input feature maps and the value is in range of (0,1). The bigger the value is, the more important the corresponding channel is. Then the channel attention map is multiplied with the input feature to get the channel-refined feature. The procedure of generating mapping function can be computed as:</p><formula xml:id="formula_3">M c (F) = σ(MLP(AvgPool(F)) + MLP(MaxPool(F))) = σ(W 1 (W 0 (F c avg )) + W 1 (W 0 (F c max ))) (3)</formula><p>where σ is the sigmoid function, </p><formula xml:id="formula_4">W 0 ∈ C/L × C and W 1 ∈ C × C/L.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2.">Spatial-Wise Attention Module</head><p>In contrast to the channel-wise attention, the spatial-wise attention focuses on the informative region of the spatial dimension. As is shown in Figure <ref type="figure" target="#fig_4">4</ref>, similar to the channel-wise attention module, two types of pooling operations are used to generate different feature descriptors: F s avg ∈ R 1×H×W and F s max ∈ R 1×H×W . In contrast with the channel-wise attention module, the pooling operation in the spatial-wise attention module is along the channel axis. Then, the output feature descriptors are fused by concatenation operation. Then a convolution layer is applied to the concatenated feature. After the convolution layer, we can get the spatial attention map. Then, the input feature is multiplied with the spatial attention map to get spatial-refined feature maps which focus on the most informative region. To be summarized, the spatial attention map is computed as:</p><formula xml:id="formula_5">M s (F) = σ( f N×N ([AvgPool(F); MaxPool(F)])) = σ( f N×N ([F s avg ; F s max ]))<label>(4)</label></formula><p>where σ denotes the activation function and we choose the sigmoid function here, f N×N represents a convolution operation with the filter size of N × N.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>FDSSC has achieved a very high performance in HSI classification, however, it firstly extracts spectral feature then extracts spatial feature. It means that the firstly extracted spectral features may be influenced in the process of extracting the spatial features because the two types of features are in different domain. In contrast to FDSSC, in our framework, the spectral feature and spatial feature are extracted in two parallel branches and fused for classification.</p><p>Figure <ref type="figure" target="#fig_5">5</ref> illustrates the whole framework of our method. Firstly, given a hyperspectral image with H × W × L size, we extract the 7 × 7 neighborhoods of the center pixel together with its corresponding category label as samples. In contrast to FDSSC using 9 × 9 neighborhoods as input, we use a smaller input size which can reduce the training time. Then, we divide the samples into 3 sets, i.e., training set X train , validation set X val and testing set X test . The training set is used for training model for many epochs, validation set is used for evaluating the classification accuracy and to pick up the network with the highest classification accuracy. Finally, the testing set is used for testing the trained model and the effectiveness of the proposed method. As can be seen in Figure <ref type="figure" target="#fig_5">5</ref>, our network has two branches, i.e., Spectral Branch with Channel Attention and Spatial Branch with Spatial Attention. As can be seen in Figure <ref type="figure" target="#fig_6">6</ref>, for convenience, the top branch is called Spectral Branch while the bottom one is called Spatial Branch. Next, we will introduce the two branches.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Spectral Branch with Channel Attention</head><p>We take Indian Pines dataset for example and the input patch size is set to 7 × 7 × 200. Spectral Branch consists of a dense spectral block and a channel attention block. First of all, 3D convolutional with kernel size of 1 × 1 × 7 is used. In the first convolutional operation, we use 'valid' padding method and the stride is set to (1,1,2), which is used to reduce the number of spectral channels to a certain degree. After the first convolutional layer, feature maps' with shape of (7 × 7 × 97, 24) are obtained. Then, the dense spectral block which consists of 3 convolutional layers with batch normalization layers is used to extract spectral feature. In the dense spectral block, as the existence of concatenation, we set the stride to (1,1,1) to maintain the feature maps' size. After dense spectral block, spectral feature with size of (7 × 7 × 1, 60) is obtained. However, the importance of the 60 channels is different. To focus on which is important and obtain more discriminative spectral feature, channel attention block as illustrated in Section 2.4.1 is applied. After channel attention block, the important channel will be highlighted while the less important channel will be suppressed. Finally the Global Average Pooling layer is employed to get the spectral feature with size of 1 × 60. Details of the layers of the Spectral Branch are described in Table <ref type="table" target="#tab_0">1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Spatial Branch with Spatial Attention</head><p>Spatial Branch consists of a dense spatial block and a spatial attention block. First of all, 3D convolutional with kernel size of 1 × 1 × 200 is used to reduce the number of spectral channels. After the first convolution layer, feature maps with shape of (7 × 7 × 1, 24) will be obtained. The number of spectral channel decreases from 200 to 1, which will reduce the number of training parameters and prevent overfitting. Then the dense spatial block consists of 3 convolutional layers together with batch normalization layers is used to extract spatial feature. After dense spatial block, spatial feature with size of (7 × 7 × 1, 60) is obtained. The dense spatial block aims to extract spatial feature, however, the importance of different position of the input patch is different. To focus on 'where' is an informative part and get more discriminative spatial feature, the spatial attention block in Section 2.4.2 is used. After Spatial attention block, the features of areas where is more important will be highlighted while the features of areas where is less important will be suppressed. Then the Global Average Pooling layer is employed to get the spatial feature with size of 1 × 60. Details of the layers of the Spatial Branch are described in Table <ref type="table" target="#tab_1">2</ref>. </p><formula xml:id="formula_6">× 7 × 200) Conv (1 × 1 × 200) (7 × 7 × 1, 24) BN-Relu-Conv (3 × 3 × 1) (7 × 7 × 1, 24) Concatenate - (7 × 7 × 1, 24) BN-Relu-Conv (3 × 3 × 1) (7 × 7 × 1, 24) Concatenate - (7 × 7 × 1, 24) BN-Relu-Conv (3 × 3 × 1) (7 × 7 × 1, 24) Concatenate - (7 × 7 × 1, 24) BN-Relu-Conv (3 × 3 × 1) (7 × 7 × 1, 60) AveragePooling/maxpooling - (7 × 7 × 1, 1) Concatenate - (7 × 7 × 1, 2) Conv-sigmoid (3 × 3 × 1) (7 × 7 × 1, 1) Multiply - (7 × 7 × 1, 60) GlobalAveragePooling - (1 × 60)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Spectral-Spatial Fusion for Classification</head><p>Through Spectral Branch and Spatial Branch, the spectral feature and spatial feature are obtained. Afterwards, the two features are fused through concatenation for classification. As the two features are not in the same domain, the concatenation operation is used instead of add operation. Through the fully connected layer and soft-max activation, final classification result is obtained.</p><p>Network implementation details for other datasets are carried out in a similar manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets Description</head><p>In the experiments, three widely used HSI datasets are used to test the proposed method, i.e., the Indian Pines (IP) dataset, the Pavia University (UP) dataset and Salinas Valley (SV) dataset. Three metrics, i.e., overall accuracy (OA), average accuracy (AA), and Kappa coefficient (K) are used to quantitatively evaluate the classification performance. OA refers to the ratio of the number of correct classifications to the total number of pixels to be classified. AA refers to the average accuracy of all classes. Kappa coefficients are used for consistency testing and can also be used to measure classification accuracy. The higher of the 3 index's value, the better the classification effect is.</p><p>Indian Pines (IP): The Indian Pines dataset, was firstly gathered by Airborne Visible/Infrared Imaging Spectrometer (AVIRIS) from Northwest Indiana. The image has 16 classes and 145 × 145 pixels with a resolution of 20 m/pixel. 20 bands was discarded and the remaining 200 bands are adopted for analysis. The wavelength of spectral is in range of 0.4 um to 2.5 um.</p><p>Pavia University (UP): Pavia University dataset, was firstly gathered by the reflective optics imaging spectrometer (ROSIS-3) from the University of Pavia, Italy. The image has 9 classes and 610 × 340 pixels with a spatial resolution of 1.3 m/pixel. 12 noisy bands are removed and the left 103 bands are used for analysis. The wavelength of spectral is in range of 0.43 um to 0.86 um.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Salinas Valley (SV):</head><p>This dataset was gatherd by the AVIRIS sensor from Salinas Valley, CA, USA. The image has 16 classes and 512 × 217 pixels with a resolution of 3.7 m/pixel. For classification, 20 bands are removed and 204 bands are preserved. The wavelength is in range of 0.4 um to 2.5 um.</p><p>Tables 3-5 list the categories and pixel counts for each dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental Setting</head><p>To demonstrate the effectiveness of the proposed method, our method is compared with several widely used methods and the state-of-the-art methods, including (1) spectral-based classifier, i.e., the SVM with RBF kernel <ref type="bibr" target="#b43">[44]</ref>; (2) spectral-spatial classifier Gabor-SVM <ref type="bibr" target="#b44">[45]</ref> and DMP-SVM <ref type="bibr" target="#b45">[46]</ref>;</p><p>(3) deeplearning-based classifier 3DCNN <ref type="bibr" target="#b23">[24]</ref>, SSRN <ref type="bibr" target="#b33">[34]</ref> and the recently proposed method fast dense Spectral-Spatial Network (FDSSC) <ref type="bibr" target="#b35">[36]</ref>. Next, we will introduce these methods separately.</p><p>SVM: For SVM, we simply feed all bands of the HSI to SVM with an radial basis function kernel.</p><p>Gabor-SVM: For Gabor-SVM, we extract gabor feature of the HSI and feed the gabor feature into SVM with an RBF kernel. We use PCA to extract first 10 PCs of the original image. 4 orientations and 3 scales are selected to construct the Gabor filters. For each PC, the length of the gabor feature vector is 12. So the gabor feature vector length is 120.</p><p>DMP-SVM: For DMP-SVM, we extract the differential morphological profiles features and feed the feature into the SVM with radial basis function. To extract the DMP feature, we use the first 5 PCs, and the sizes of the structure elements are set to 2, 4, 6, 8 and 10 so the DMP feature vector length is 50.</p><p>It has to be noted that the best parameter setting of SVM, Gabor-SVM, DMP-SVM are obtained by cross validation to ensure the best classification result.</p><p>3DCNN: For 3DCNN, we use 27 × 27 × 200, 27 × 27 × 103, 27 × 27 × 204 neighbors of each pixel as the input data, respectively. We design the network follow the instruction in <ref type="bibr" target="#b23">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SSRN:</head><p>The architecture of the SSRN is set out in <ref type="bibr" target="#b33">[34]</ref>. We use 7 × 7 × L neighbors of each pixel as the input data, where L denotes the spectral channel number of the dataset. We set two spectral residual blocks and two spatial residual blocks according to <ref type="bibr" target="#b33">[34]</ref>.</p><p>FDSSC: The architecture of the FDSSC is set out in <ref type="bibr" target="#b35">[36]</ref>. The input patch size is set to 9 × 9 × L and we set one dens spectral dense block and one spatial dense block in the architecture.</p><p>besides the training method, the number of samples used for training also plays an important role. The more data used in training stage usually leads to a higher test accuracy, but the corresponding training time and computation complexity will increase dramatically. Therefore, for IP dataset, we choose 5% training samples and 5% validation samples. In addition, for UP dataset and SV dataset, since their samples are enough for every class, we only choose 1% training samples and 1% validation samples to save the training time.</p><p>For 3DCNN, SSRN, FDSSC and our method, the batch size is set to 32 and the Adam optimizer is adopted. The learning rate is set to 0.01 and we train each model for 200 epochs. While training the model, the model with the highest classification performance in validation samples is restored for testing. The early stopping strategy is also adopted, i.e., if the accuracy in validation set does not improve for 20 epochs, we terminate the training stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Classification Maps and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1.">Classification Maps and Result of IP Dataset</head><p>The results of IP dataset are reported in Table <ref type="table" target="#tab_5">6</ref> and the highest class-specific accuracies are in bold. Figure <ref type="figure" target="#fig_7">7</ref> shows the classification maps of different methods.</p><p>From Table <ref type="table" target="#tab_5">6</ref>, we can see that our method achieves the best performance, with 98.19% OA, 96.31% AA and 0.9794 Kappa. For SVM, it achieves the worst performance with only 74.73% OA. Compared with the original SVM, the Gabor-SVM and DMP-SVM lead to a better performance because they also consider the spatial information for classification. However, the Gabor feature performs better than the DMP feature in terms of 3 indexes. For the four deep learning method, i.e., 3DCNN, SSRN, FDSSC and our method, 3DCNN is better than DMP-SVM with nearly 9% improvement in OA but worse than Gabor-SVM. SSRN and FDSSC is better than 3DCNN with nearly 4% improvement in OA. The reason of the FDSSC's success in HSI classification can be concluded as the following: first, it extracts spectral feature and spatial feature separately. Second, the dense connection can deepen the structure. The two advantage ensures FDSSC can extract more discriminative features. However, our method, improves the OA 2.49% compared with FDSSC and the other two indexs are also higher than FDSSC. Although our method achieves worse result than FDSSC in some classes, the OA, AA and kappa coefficient are the highest among these methods.</p><p>From the classification maps shown in Figure <ref type="figure" target="#fig_7">7</ref>, 'salt-and-pepper' noise is the worst for SVM due to the lack of incorporation of spatial information in the classification while the classification map of Gabor-SVM and DMP-SVM show more spatial continuity because they have consider the spatial information. Among these methods, our method shows least 'salt-and-pepper' noise which corresponds to the result of Table <ref type="table" target="#tab_5">6</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.">Classification Maps and Result of UP Dataset</head><p>The results of the Pavia University dataset are reported in Table <ref type="table" target="#tab_6">7</ref> and the highest class-specific accuracies are in bold. The classification maps of different methods are shown in Figure <ref type="figure" target="#fig_8">8</ref>.</p><p>From Table <ref type="table" target="#tab_6">7</ref> we can see that our method achieves the best performance in terms of 3 index. For accuracy of every class, although our method has not achieved the best performance in every class, but for class 7, which have only 13 training samples, our method performs well, while other methods performed poor in this class. For class 8, other methods' accuracy are all lower than 85%, which is a very low accuracy, but our method can achieve accuracy of 95%.</p><p>Although Gabor-SVM and DMP-SVM show little improvement in the aspect of OA, but the classification maps of them show more spatial continuity than SVM. For deep-learning-based models, 3DCNN improves OA about 4.5% compared with Gabor-SVM while FDSSC improves OA about 5% compared with 3DCNN which is very large improvement. However, our method achieves the highest performance in the three index among these methods.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3.">Classification Maps and Results of SV Dataset</head><p>The results of the SV dataset are listed in Table <ref type="table" target="#tab_7">8</ref> and the highest class-specific accuracies are in bold. The classification maps of different methods are shown in Figure <ref type="figure" target="#fig_9">9</ref>.</p><p>From Table <ref type="table" target="#tab_7">8</ref> we can see that SVM, Gabor-SVM and DMP-SVM perform poorly in terms of OA, which are all below 91%. The classification maps of them also show large areas of mislabeled. This phenomenon has been avoided in 3DCNN, SSRN, FDSSC and our method. Furthermore, our method performs the best in terms of 3 indexes compared with other methods. In addition, the classification map of our method shows less mislabeled areas than other methods. For class 15, the accuracy of other method are all low than 93%, but our method can achieve the accuracy of 98.28%, which is the highest among these methods.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Investigation on Running Time</head><p>Tables 9-11 list the training and test time of the seven methods on the IP, UP and SV datasets, respectively. From Tables 9-11, we can find that SVM-based methods usually spend less time than deep-learning-based methods. Furthermore, Gabor-SVM and DMP-SVM spend less time than SVM because the length of Gabor-feature and DMP feature is shorter than the original feature. It has to be noted that, for Gabor-SVM and DMP-SVM, the training stage does not include the process of extracting the Gabor and DMP feature. For deep-learning-based methods, 3DCNN spends the most time due to the large input size and the large number of parameters to be trained. The training time and test time of SSRN and FDSSC is less than 3DCNN and the accuracy of them is much higher than 3DCNN, which proves the superiority of SSRN and FDSSC. FDSSC spends less time in training stage while more time in test stage compared with SSRN because the dense connected structure helps FDSSC to come to convergence more quickly, while FDSSC usually have more parameters which slows down the test speed. For our method, it spends less training time while gets much higher classification accuracy than FDSSC. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Investigation on the Number of Training</head><p>In Section 4.2, we have illustrated the effectiveness of our method, especially in the case of having a small number of training samples. In this part, we would further investigate the performance with different number of training samples.</p><p>Figure <ref type="figure" target="#fig_11">10</ref> shows the experiment results. For IP dataset, the number of training samples per class is varied from 5% to 10% with an interval of 1%. For UP dataset and SV dataset, the number of training samples per class is varied from 0.2% to 1.4% with an interval of 0.3%.</p><p>As expected, with the training samples' number increasing, the accuracy increases. We can see that no matter in what case, our method still performs better than other methods. From Figure <ref type="figure" target="#fig_11">10a</ref>, we can see that SVM has the worst performance among the 7 methods and the OA is not higher than 80% in all cases. The Gabor-SVM outperforms DMP-SVM in all cases. With the number of training samples increasing, the 3DCNN gradually outperforms Gabor-SVM. The accuracy of FDSSC is slenderly higher than SSRN. Among these 7 methods, our method is always better than FDSSC in term of OA, especially in the circumstance of having very few training samples, which indicates the superiority of our method.</p><p>As is shown in Figure <ref type="figure" target="#fig_11">10b</ref>, interestingly, Gabor-SVM performs worse than DMP-SVM and when the training samples are very few (i.e. 0.2%-0.5%), SVM performs better than DMP-SVM, Gabor-SVM and 3DCNN, which indicates that when the training samples is very few, the Gabor feature, DMP feature give little improvement for classification, 3DCNN is also not suitable in the case of having very few training samples, while SVM seems very suitable for classification in this case. In contrast with the aforementioned methods, FDSSC, SSRN and our method still perform well in all cases which indicates the stability of the 3 methods. Apparently, our method performs better than FDSSC and SSRN in all cases.</p><p>As is shown in Figure <ref type="figure" target="#fig_11">10c</ref>, the same as UP dataset, SVM performs well in SV dataset, always better than DMP-SVM. For Gabor-SVM, when the training samples is very few, it performs worse than SVM, but with the training samples increasing, it outperforms SVM. Also, Gabor feature seems be more suitable for SV dataset than DMP feature. Among these methods, FDSSC, SSRN and our method still have good performance, which is much better than 3DCNN. Besides, our method achieves the highest accuracy in all cases.</p><p>Thus, our method is suitable in the circumstance when the number of training samples is limited.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Effectiveness of Channel Attention Mechanism and Spatial Attention Mechanism</head><p>To validate the effectiveness of channel-wise attention mechanism and spatial-wise attention mechanism, we do three another experiments, i.e., without spectral attention and spatial attention (denoted as proposed1), only with spatial attention (denoted as proposed2) and only with spectral attention (denoted as proposed3). From Figure <ref type="figure" target="#fig_12">11</ref> we can see that without attention mechanism, the accuracy of three datasets will decrease in three dataset, which proves the effectiveness of attention mechanism. Furthermore, the spectral attention mechanism plays a more important role in HSI classification than spatial attention mechanism. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, a Double-Branch Multi-Attention mechanism network was proposed for HSI classification. It has two branches to extract spectral feature and spatial feature respectively, using densely connected 3D convolution layer with kernels of different sizes. Furthermore, according to the different purposes and characteristics of the two branches, the channel attention and spatial attention are applied in the two branches respectively to extract more discriminative feature. Our work is on the basic of FDSSC and CBAM. FDSSC is the state-of-the-art architecture in HSI classification, and CBAM is a novel and efficient attention network in image classification. Although it seems like a minor improvement, a lot of experiment results shows that our proposed method outperforms other state-of-the-art methods, especially in the case of having very few training samples. Furthermore, the training time is also reduced compared with the other two deep-learning methods because the attention blocks speed up the convergence of the network.</p><p>However, due to the attention block, the parameters of the network increase, which results in more time cost while testing stage. On the one hand, 3DCNN uses kernels of 3 dimensions and results in more parameters to train. To reduce the impact, we first reduce the spectral channels to 1 using 3D kernel with size of 1 × 1 × L (L represents the number of spectral channel), and set the kernel size of spectral domain to 1 in the dense spectral block. In our future work, we will try to use 2DCNN directly to extract spatial information. On the other hand, Recurrent Neural Network (RNN) seems more suitable for dealing with sequence data than CNN because it considers the order and relationship of the data. Obviously, HSI data can be regarded as sequence data and the relationship between different bands is useful for classification. In our future work, we will try to use RNN to extract spectral information.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Comparison of Residual connection and Dense connection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Structure of the Fast Dense Spectral-Spatial Convolution Network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Structure of channel-wise attention.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Structure of spatial-wise attention.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. The training procedure of our method.</figDesc><graphic coords="7,93.81,447.49,63.61,64.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Structure of DBMA network. The top branch is called Spectral Branch consisting of dense spectral block and channel attention block, which is used to extract spectral feature. The bottom branch is called Spatial Branch consisting of dense spatial block and spatial attention block, which is used to extract spatial feature.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Classification maps of the IP dataset with 5% training samples. The first image (a) represents ground-truth (GT) and images from (b)-(h) are the classification maps using different methods.</figDesc><graphic coords="13,117.77,489.95,72.00,71.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Classification maps of the UP dataset using 1% training samples. The first image (a) represents ground-truth (GT) and images from (b)-(h) are the classification maps using different methods.</figDesc><graphic coords="14,124.97,459.24,57.60,103.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Classification maps of The SV dataset. The first image (a) represents ground-truth (GT) and images from (b)-(h) are the classification maps using different methods.</figDesc><graphic coords="15,309.59,289.10,72.00,170.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Results on IP dataset with different ratios of training samples Results on UP dataset with different ratios of training samples Results on SV dataset with different ratios of training samples</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. The OA results of SVM, Gabor-SVM, DMP-SVM, 3DCNN, SSRN, FDSSC and proposed method with different number of training samples on the (a) IP dataset, (b) UP dataset, and (c) SV dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. Effect of different attention mechanism on different datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Network structure of Spectral Branch.</figDesc><table><row><cell>Layer Name</cell><cell>Kernel Size</cell><cell>Output Size</cell></row><row><cell>Input</cell><cell>-</cell><cell>(7 × 7 × 200)</cell></row><row><cell>Conv</cell><cell>(1 × 1 × 7)</cell><cell>(7 × 7 × 97, 24)</cell></row><row><cell>BN-Relu-Conv</cell><cell>(1 × 1 × 7)</cell><cell>(7 × 7 × 97, 24)</cell></row><row><cell>Concatenate</cell><cell>-</cell><cell>(7 × 7 × 97, 24)</cell></row><row><cell>BN-Relu-Conv</cell><cell>(1 × 1 × 7)</cell><cell>(7 × 7 × 97, 24)</cell></row><row><cell>Concatenate</cell><cell>-</cell><cell>(7 × 7 × 97, 24)</cell></row><row><cell>BN-Relu-Conv</cell><cell>(1 × 1 × 7)</cell><cell>(7 × 7 × 97, 24)</cell></row><row><cell>Concatenate</cell><cell>-</cell><cell>(7 × 7 × 97, 24)</cell></row><row><cell>BN-Relu-Conv</cell><cell>(1 × 1 × 97)</cell><cell>(7 × 7 × 1, 60)</cell></row><row><cell>AveragePooling/maxpooling</cell><cell>(7 × 7 × 1)</cell><cell>(1 × 1 × 1, 60)</cell></row><row><cell>Add</cell><cell>-</cell><cell>(1 × 1 × 1, 60)</cell></row><row><cell>FC</cell><cell>30</cell><cell>30</cell></row><row><cell>FC-sigmoid</cell><cell>60</cell><cell>60</cell></row><row><cell>Multiply</cell><cell>-</cell><cell>(7 × 7 × 1, 60)</cell></row><row><cell>GlobalAveragePooling</cell><cell>-</cell><cell>(1 × 60)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Network structure of Spatial Branch.</figDesc><table><row><cell>Layer Name</cell><cell>Kernel Size</cell><cell>Output Size</cell></row><row><cell>Input</cell><cell>-</cell><cell>(7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>The number of training, validation, and test samples in IP dataset.</figDesc><table><row><cell>Order</cell><cell>Class</cell><cell cols="4">Total Number Train Val Test</cell></row><row><cell>1</cell><cell>Alfalfa</cell><cell>46</cell><cell>2</cell><cell>2</cell><cell>42</cell></row><row><cell>2</cell><cell>Corn-notill</cell><cell>1428</cell><cell>71</cell><cell>71</cell><cell>1286</cell></row><row><cell>3</cell><cell>Corn-mintill</cell><cell>830</cell><cell>41</cell><cell>41</cell><cell>748</cell></row><row><cell>4</cell><cell>Corn</cell><cell>237</cell><cell>11</cell><cell>11</cell><cell>215</cell></row><row><cell>5</cell><cell>Grass-pasture</cell><cell>483</cell><cell>24</cell><cell>24</cell><cell>435</cell></row><row><cell>6</cell><cell>Grass-trees</cell><cell>730</cell><cell>36</cell><cell>36</cell><cell>658</cell></row><row><cell>7</cell><cell>Grass-pasture-mowed</cell><cell>28</cell><cell>1</cell><cell>1</cell><cell>26</cell></row><row><cell>8</cell><cell>Hay-windrowed</cell><cell>478</cell><cell>23</cell><cell>23</cell><cell>432</cell></row><row><cell>9</cell><cell>Oats</cell><cell>20</cell><cell>1</cell><cell>1</cell><cell>18</cell></row><row><cell>10</cell><cell>Soybean-notill</cell><cell>972</cell><cell>48</cell><cell>48</cell><cell>876</cell></row><row><cell>11</cell><cell>Soybean-mintill</cell><cell>2455</cell><cell>122</cell><cell cols="2">122 2211</cell></row><row><cell>12</cell><cell>Soybean-clean</cell><cell>593</cell><cell>29</cell><cell>29</cell><cell>535</cell></row><row><cell>13</cell><cell>Wheat</cell><cell>205</cell><cell>10</cell><cell>10</cell><cell>185</cell></row><row><cell>14</cell><cell>Woods</cell><cell>1265</cell><cell>63</cell><cell>63</cell><cell>1139</cell></row><row><cell>15</cell><cell>Buildings-Grass-Trees-Drives</cell><cell>386</cell><cell>19</cell><cell>19</cell><cell>348</cell></row><row><cell>16</cell><cell>Stone-Steel-Towers</cell><cell>93</cell><cell>4</cell><cell>4</cell><cell>85</cell></row><row><cell></cell><cell>Total</cell><cell>10,249</cell><cell>505</cell><cell cols="2">505 9239</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>The number of training, validation, and test samples in UP dataset.</figDesc><table><row><cell>Order</cell><cell>Class</cell><cell cols="3">Total Number Train Val</cell><cell>Test</cell></row><row><cell>1</cell><cell>Asphalt</cell><cell>6631</cell><cell>66</cell><cell>66</cell><cell>6499</cell></row><row><cell>2</cell><cell>Meadows</cell><cell>18,649</cell><cell>186</cell><cell cols="2">186 18,277</cell></row><row><cell>3</cell><cell>Gravel</cell><cell>2099</cell><cell>20</cell><cell>20</cell><cell>2059</cell></row><row><cell>4</cell><cell>Trees</cell><cell>3064</cell><cell>30</cell><cell>30</cell><cell>3004</cell></row><row><cell>5</cell><cell>Painted metal sheets</cell><cell>1345</cell><cell>13</cell><cell>13</cell><cell>1319</cell></row><row><cell>6</cell><cell>Bare Soil</cell><cell>5029</cell><cell>50</cell><cell>50</cell><cell>4929</cell></row><row><cell>7</cell><cell>Bitumen</cell><cell>1330</cell><cell>13</cell><cell>13</cell><cell>1304</cell></row><row><cell>8</cell><cell>Self-Blocking Bricks</cell><cell>3682</cell><cell>36</cell><cell>36</cell><cell>3610</cell></row><row><cell>9</cell><cell>Shadows</cell><cell>947</cell><cell>9</cell><cell>9</cell><cell>929</cell></row><row><cell></cell><cell>Total</cell><cell>42,776</cell><cell>423</cell><cell cols="2">423 41,930</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>The number of training, validation, and test samples in SV dataset.</figDesc><table><row><cell>Order</cell><cell>Class</cell><cell cols="3">Number of Samples Train Val</cell><cell>Test</cell></row><row><cell>1</cell><cell>Brocoli-green-weeds-1</cell><cell>2009</cell><cell>20</cell><cell>20</cell><cell>1969</cell></row><row><cell>2</cell><cell>Brocoli-green-weeds-2</cell><cell>3726</cell><cell>37</cell><cell>37</cell><cell>3652</cell></row><row><cell>3</cell><cell>Fallow</cell><cell>1976</cell><cell>19</cell><cell>19</cell><cell>1938</cell></row><row><cell>4</cell><cell>Fallow-rough-plow</cell><cell>1394</cell><cell>13</cell><cell>13</cell><cell>1368</cell></row><row><cell>5</cell><cell>Fallow-smooth</cell><cell>2678</cell><cell>26</cell><cell>26</cell><cell>2626</cell></row><row><cell>6</cell><cell>Stubble</cell><cell>3959</cell><cell>39</cell><cell>39</cell><cell>3881</cell></row><row><cell>7</cell><cell>Celery</cell><cell>3579</cell><cell>35</cell><cell>35</cell><cell>3509</cell></row><row><cell>8</cell><cell>Grapes-untrained</cell><cell>11,271</cell><cell>112</cell><cell cols="2">112 11,047</cell></row><row><cell>9</cell><cell>Soil-vinyard-develop</cell><cell>6203</cell><cell>62</cell><cell>62</cell><cell>6079</cell></row><row><cell>10</cell><cell>Corn-senesced-green-weeds</cell><cell>3278</cell><cell>32</cell><cell>32</cell><cell>3214</cell></row><row><cell>11</cell><cell>Lettuce-romaine-4wk</cell><cell>1068</cell><cell>10</cell><cell>10</cell><cell>1048</cell></row><row><cell>12</cell><cell>Lettuce-romaine-5wk</cell><cell>1927</cell><cell>19</cell><cell>19</cell><cell>1889</cell></row><row><cell>13</cell><cell>Lettuce-romaine-6wk</cell><cell>916</cell><cell>9</cell><cell>9</cell><cell>898</cell></row><row><cell>14</cell><cell>Lettuce-romaine-7wk</cell><cell>1070</cell><cell>10</cell><cell>10</cell><cell>1050</cell></row><row><cell>15</cell><cell>Vinyard-untrained</cell><cell>7268</cell><cell>72</cell><cell>72</cell><cell>7124</cell></row><row><cell>16</cell><cell>Vinyard-vertical-trellis</cell><cell>1807</cell><cell>18</cell><cell>18</cell><cell>1771</cell></row><row><cell></cell><cell>Total</cell><cell>54,129</cell><cell>533</cell><cell cols="2">533 53,063</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Class-specific results for the IP dataset using 5% training samples.</figDesc><table><row><cell>Class Color</cell><cell>SVM</cell><cell cols="6">Gabor-SVM DMP-SVM 3DCNN SSRN FDSSC Proposed</cell></row><row><cell>1</cell><cell>27.78</cell><cell>100.0</cell><cell>0.00</cell><cell>93.94</cell><cell>86.49</cell><cell>87.88</cell><cell>100.0</cell></row><row><cell>2</cell><cell>66.78</cell><cell>91.29</cell><cell>70.89</cell><cell>87.32</cell><cell>96.35</cell><cell>97.72</cell><cell>97.10</cell></row><row><cell>3</cell><cell>72.45</cell><cell>86.86</cell><cell>88.35</cell><cell>95.45</cell><cell>96.60</cell><cell>93.32</cell><cell>99.03</cell></row><row><cell>4</cell><cell>45.10</cell><cell>90.09</cell><cell>100.0</cell><cell>95.72</cell><cell>97.18</cell><cell>94.93</cell><cell>92.20</cell></row><row><cell>5</cell><cell>82.94</cell><cell>93.66</cell><cell>98.94</cell><cell>88.76</cell><cell>99.26</cell><cell>99.51</cell><cell>99.26</cell></row><row><cell>6</cell><cell>84.11</cell><cell>98.29</cell><cell>92.21</cell><cell>93.21</cell><cell>97.44</cell><cell>98.93</cell><cell>98.20</cell></row><row><cell>7</cell><cell>100.0</cell><cell>0.00</cell><cell>0.00</cell><cell>100.0</cell><cell>88.89</cell><cell>100.0</cell><cell>81.25</cell></row><row><cell>8</cell><cell>87.63</cell><cell>98.15</cell><cell>98.53</cell><cell>99.54</cell><cell>97.48</cell><cell>95.70</cell><cell>100.0</cell></row><row><cell>9</cell><cell>72.73</cell><cell>0.00</cell><cell>0.00</cell><cell>90.0</cell><cell>100.0</cell><cell>100.0</cell><cell>85.71</cell></row><row><cell>10</cell><cell>73.64</cell><cell>91.82</cell><cell>91.08</cell><cell>88.72</cell><cell>93.20</cell><cell>92.84</cell><cell>98.00</cell></row><row><cell>11</cell><cell>68.35</cell><cell>90.21</cell><cell>68.12</cell><cell>94.61</cell><cell>94.93</cell><cell>96.55</cell><cell>98.46</cell></row><row><cell>12</cell><cell>66.29</cell><cell>85.08</cell><cell>89.63</cell><cell>76.39</cell><cell>84.95</cell><cell>82.86</cell><cell>98.15</cell></row><row><cell>13</cell><cell>88.04</cell><cell>100.0</cell><cell>100.0</cell><cell>95.05</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>14</cell><cell>92.50</cell><cell>97.83</cell><cell>96.81</cell><cell>94.83</cell><cell>99.56</cell><cell>99.21</cell><cell>99.74</cell></row><row><cell>15</cell><cell>66.16</cell><cell>98.19</cell><cell>96.47</cell><cell>83.51</cell><cell>94.04</cell><cell>95.04</cell><cell>96.12</cell></row><row><cell>16</cell><cell>98.61</cell><cell>88.24</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>98.82</cell><cell>97.67</cell></row><row><cell>OA</cell><cell>74.73</cell><cell>92.36</cell><cell>82.28</cell><cell>91.40</cell><cell>95.59</cell><cell>95.70</cell><cell>98.19</cell></row><row><cell>AA</cell><cell>74.57</cell><cell>81.86</cell><cell>74.44</cell><cell>92.32</cell><cell>95.39</cell><cell>95.83</cell><cell>96.31</cell></row><row><cell>kappa</cell><cell>0.7096</cell><cell>0.9124</cell><cell>0.7940</cell><cell>0.9019</cell><cell>0.9497</cell><cell>0.9510</cell><cell>0.9794</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>Class-specific results for the UP dataset using 1% training samples.</figDesc><table><row><cell>Class Color</cell><cell>SVM</cell><cell cols="6">Gabor-SVM DMP-SVM 3DCNN SSRN FDSSC Proposed</cell></row><row><cell>1</cell><cell>93.72</cell><cell>77.61</cell><cell>89.74</cell><cell>88.17</cell><cell>99.67</cell><cell>99.48</cell><cell>99.37</cell></row><row><cell>2</cell><cell>93.36</cell><cell>92.93</cell><cell>89.95</cell><cell>97.08</cell><cell>98.61</cell><cell>98.79</cell><cell>99.73</cell></row><row><cell>3</cell><cell>65.61</cell><cell>87.13</cell><cell>81.20</cell><cell>75.29</cell><cell>79.16</cell><cell>99.64</cell><cell>99.16</cell></row><row><cell>4</cell><cell>87.48</cell><cell>77.60</cell><cell>97.45</cell><cell>97.88</cell><cell>100.0</cell><cell>100.0</cell><cell>98.21</cell></row><row><cell>5</cell><cell>98.89</cell><cell>88.11</cell><cell>99.92</cell><cell>100.0</cell><cell>100.0</cell><cell>99.92</cell><cell>100.0</cell></row><row><cell>6</cell><cell>83.71</cell><cell>96.12</cell><cell>85.19</cell><cell>93.97</cell><cell>95.83</cell><cell>98.66</cell><cell>97.45</cell></row><row><cell>7</cell><cell>62.96</cell><cell>89.27</cell><cell>67.73</cell><cell>75.35</cell><cell>92.57</cell><cell>94.36</cell><cell>100.0</cell></row><row><cell>8</cell><cell>74.55</cell><cell>83.94</cell><cell>81.76</cell><cell>79.88</cell><cell>88.20</cell><cell>84.67</cell><cell>95.12</cell></row><row><cell>9</cell><cell>100.0</cell><cell>56.41</cell><cell>96.20</cell><cell>97.27</cell><cell>99.57</cell><cell>100.0</cell><cell>99.36</cell></row><row><cell>OA</cell><cell>87.68</cell><cell>87.73</cell><cell>88.61</cell><cell>92.29</cell><cell>96.40</cell><cell>97.48</cell><cell>98.88</cell></row><row><cell>AA</cell><cell>84.48</cell><cell>83.24</cell><cell>87.68</cell><cell>89.43</cell><cell>94.85</cell><cell>97.28</cell><cell>98.71</cell></row><row><cell>kappa</cell><cell>0.8369</cell><cell>0.8363</cell><cell>0.8470</cell><cell>0.8974</cell><cell>0.9522</cell><cell>0.9666</cell><cell>0.9850</cell></row><row><cell>(a) GT</cell><cell></cell><cell>(b) SVM (87.68%)</cell><cell cols="5">(c) Gabor-SVM (87.73%) (d) DMP-SVM (88.61%)</cell></row><row><cell cols="2">(e) 3DCNN (92.29%)</cell><cell>(f) SSRN (96.40%)</cell><cell></cell><cell cols="2">(g) FDSSC (97.48%)</cell><cell cols="2">(h) Proposed (98.88%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 .</head><label>8</label><figDesc>Class-specific results for the SV dataset using 1% training samples.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Class Color SVM Gabor-SVM DMP-SVM 3DCNN SSRN FDSSC Proposed</head><label></label><figDesc></figDesc><table><row><cell>1</cell><cell>99.57</cell><cell>92.68</cell><cell>98.75</cell><cell>99.76</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>2</cell><cell>97.89</cell><cell>88.80</cell><cell>91.08</cell><cell>92.19</cell><cell>99.97</cell><cell>99.21</cell><cell>99.59</cell></row><row><cell>3</cell><cell>91.30</cell><cell>93.65</cell><cell>79.83</cell><cell>97.10</cell><cell>99.85</cell><cell>97.58</cell><cell>97.14</cell></row><row><cell>4</cell><cell>97.40</cell><cell>79.85</cell><cell>97.84</cell><cell>97.79</cell><cell>98.41</cell><cell>96.88</cell><cell>96.33</cell></row><row><cell>5</cell><cell>97.58</cell><cell>73.44</cell><cell>96.47</cell><cell>95.84</cell><cell>99.58</cell><cell>99.38</cell><cell>99.88</cell></row><row><cell>6</cell><cell>100.0</cell><cell>91.64</cell><cell>93.13</cell><cell>96.15</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>7</cell><cell>99.69</cell><cell>92.66</cell><cell>93.90</cell><cell>99.11</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>8</cell><cell>75.07</cell><cell>91.16</cell><cell>74.96</cell><cell>89.91</cell><cell>93.55</cell><cell>94.13</cell><cell>93.69</cell></row><row><cell>9</cell><cell>97.19</cell><cell>90.68</cell><cell>88.56</cell><cell>99.67</cell><cell>99.10</cell><cell>99.84</cell><cell>99.59</cell></row><row><cell>10</cell><cell>93.27</cell><cell>93.84</cell><cell>91.56</cell><cell>98.67</cell><cell>99.12</cell><cell>98.26</cell><cell>99.56</cell></row><row><cell>11</cell><cell>95.47</cell><cell>93.57</cell><cell>99.02</cell><cell>85.80</cell><cell>94.49</cell><cell>95.58</cell><cell>100.0</cell></row><row><cell>12</cell><cell>93.41</cell><cell>95.52</cell><cell>98.62</cell><cell>98.28</cell><cell>92.60</cell><cell>98.64</cell><cell>99.89</cell></row><row><cell>13</cell><cell>97.58</cell><cell>94.40</cell><cell>97.19</cell><cell>98.44</cell><cell>100.0</cell><cell>100.0</cell><cell>97.71</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 .</head><label>8</label><figDesc>Cont.    </figDesc><table><row><cell>Class Color</cell><cell>SVM</cell><cell cols="6">Gabor-SVM DMP-SVM 3DCNN SSRN FDSSC Proposed</cell></row><row><cell>14</cell><cell>92.76</cell><cell>92.18</cell><cell>95.69</cell><cell>97.45</cell><cell>95.36</cell><cell>97.30</cell><cell>100.0</cell></row><row><cell>15</cell><cell>66.22</cell><cell>92.02</cell><cell>67.72</cell><cell>86.73</cell><cell>90.87</cell><cell>90.06</cell><cell>98.28</cell></row><row><cell>16</cell><cell>98.25</cell><cell>93.14</cell><cell>52.32</cell><cell>97.52</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell>OA</cell><cell>88.16</cell><cell>90.54</cell><cell>83.92</cell><cell>94.19</cell><cell>96.72</cell><cell>96.86</cell><cell>98.04</cell></row><row><cell>AA</cell><cell>93.29</cell><cell>90.58</cell><cell>88.54</cell><cell>95.65</cell><cell>97.68</cell><cell>97.92</cell><cell>98.85</cell></row><row><cell>kappa</cell><cell>0.8680</cell><cell>0.8944</cell><cell>0.8204</cell><cell>0.9353</cell><cell>0.9635</cell><cell>0.9650</cell><cell>0.9782</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 .</head><label>9</label><figDesc>Running time of SVM, Gabor-SVM, DMP-SVM, 3DCNN, SSRN, FDSSC, and our method on the IP dataset.</figDesc><table><row><cell>Dataset</cell><cell>Method</cell><cell cols="2">Training Times (s) Test Times (s)</cell></row><row><cell></cell><cell>SVM</cell><cell>5.9</cell><cell>1.20</cell></row><row><cell></cell><cell>Gabor-SVM</cell><cell>5.0</cell><cell>0.74</cell></row><row><cell></cell><cell>DMP-SVM</cell><cell>4.1</cell><cell>0.44</cell></row><row><cell>Indian Pines</cell><cell>3DCNN</cell><cell>381.0</cell><cell>25.96</cell></row><row><cell></cell><cell>SSRN</cell><cell>361.4</cell><cell>8.15</cell></row><row><cell></cell><cell>FDSSC</cell><cell>329.5</cell><cell>10.25</cell></row><row><cell></cell><cell>proposed</cell><cell>314.2</cell><cell>10.85</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 .</head><label>10</label><figDesc>Running time of SVM, Gabor-SVM, DMP-SVM, 3DCNN, SSRN, FDSSC, and our method on the UP dataset.</figDesc><table><row><cell>Dataset</cell><cell>Method</cell><cell cols="2">Training Times (s) Test Times (s)</cell></row><row><cell></cell><cell>SVM</cell><cell>4.2</cell><cell>2.06</cell></row><row><cell></cell><cell>Gabor-SVM</cell><cell>4.8</cell><cell>2.50</cell></row><row><cell></cell><cell>DMP-SVM</cell><cell>3.7</cell><cell>1.35</cell></row><row><cell>Pavia University</cell><cell>3DCNN</cell><cell>375.6</cell><cell>33.48</cell></row><row><cell></cell><cell>SSRN</cell><cell>352.5</cell><cell>26.54</cell></row><row><cell></cell><cell>FDSSC</cell><cell>341.2</cell><cell>30.47</cell></row><row><cell></cell><cell>proposed</cell><cell>317.4</cell><cell>31.82</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 11 .</head><label>11</label><figDesc>Running time of SVM, Gabor-SVM, DMP-SVM, 3DCNN, SSRN, FDSSC, and our method on the SV dataset.</figDesc><table><row><cell>Dataset</cell><cell>Method</cell><cell cols="2">Training Times(s) Test Times(s)</cell></row><row><cell></cell><cell>SVM</cell><cell>6.3</cell><cell>5.65</cell></row><row><cell></cell><cell>Gabor-SVM</cell><cell>5.5</cell><cell>4.47</cell></row><row><cell></cell><cell>DMP-SVM</cell><cell>4.3</cell><cell>2.43</cell></row><row><cell>Salinas</cell><cell>3DCNN</cell><cell>342.5</cell><cell>45.25</cell></row><row><cell></cell><cell>SSRN</cell><cell>330.2</cell><cell>38.92</cell></row><row><cell></cell><cell>FDSSC</cell><cell>325.8</cell><cell>41.13</cell></row><row><cell></cell><cell>proposed</cell><cell>312.5</cell><cell>42.51</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Remote Sens. 2019, 11, 1307; doi:10.3390/rs11111307 www.mdpi.com/journal/remotesensing</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>3 1 u u 3 3 2,1 u u</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Funding:</head><p>The research was jointly supported by the National Natural Science Foundations of China (Nos. 61702392, 61772400), and the Fundamental Research Funds for the Central Universities (Nos. JB190307, JB181704).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflicts of Interest:</head><p>The authors declare no conflict of interest.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A novel point-matching algorithm based on fast sample consensus for image registration</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M;</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L;</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
		<idno type="DOI">10.1109/LGRS.2014.2325970</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="43" to="47" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">PSOSAC: particle swarm optimization sample consensus algorithm for remote sensing image registration</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W;</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M;</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/LGRS.2017.2783879</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="242" to="246" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A Novel Two-Step Registration Method for Remote Sensing Images Based on Deep and Local Features</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y;</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L;</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H;</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2019.2893310</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosc. Remote Sens</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Change Detection in Remote Sensing Images Based on Image Mapping and a Deep Capsule Network</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y;</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H;</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X;</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs11060626</idno>
		<imprint/>
	</monogr>
	<note>Remote Sens. 2019, 11, 626. [CrossRef</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Change Detection Based on Multi-Grained Cascade Forest and Multi-Scale Fusion for SAR Images</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y;</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y;</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T;</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L;</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hou</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs11020142</idno>
		<imprint/>
	</monogr>
	<note>Remote Sens. 2019, 11, 142. [CrossRef</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A Novel Multi-Model Decision Fusion Network for Object Detection in Remote Sensing Images</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y;</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W;</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X;</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs11070737</idno>
	</analytic>
	<monogr>
		<title level="j">Remote Sens</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><surname>Multiscale</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs11060695</idno>
	</analytic>
	<monogr>
		<title level="m">Deep Middle-level Feature Fusion Network for Hyperspectral Classification</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Improved Capability in Stone Pine Forest Mapping and Management in Lebanon Using Hyperspectral CHRIS-Proba Data Relative to Landsat ETM+</title>
		<author>
			<persName><forename type="first">M</forename><surname>Awad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Jomaa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Arab</surname></persName>
		</author>
		<idno type="DOI">10.14358/PERS.80.8.725</idno>
	</analytic>
	<monogr>
		<title level="j">Photogramm. Eng. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="725" to="731" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Hyperspectral imagery classification using sparse representations of convolutional neural network features</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs8020099</idno>
		<imprint/>
	</monogr>
	<note>Remote Sens. 2016, 8, 99. [CrossRef</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A sparse and low-rank near-isometric linear embedding method for feature extraction in hyperspectral imagery classification</title>
		<author>
			<persName><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2017.2686842</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosc. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="4032" to="4046" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A Novel Change Detection Method for Multitemporal Hyperspectral Images Based on Binary Hyperspectral Change Vectors</title>
		<author>
			<persName><forename type="first">D</forename><surname>Marinelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bovolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bruzzone</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2019.2894339</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosc. Remote Sens</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Global and local real-time anomaly detectors for hyperspectral remote sensing imagery</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs70403966</idno>
	</analytic>
	<monogr>
		<title level="j">Remote Sens</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="3966" to="3985" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sea water chlorophyll-a estimation using hyperspectral images and supervised artificial neural network</title>
		<author>
			<persName><forename type="first">M</forename><surname>Awad</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ecoinf.2014.07.004</idno>
	</analytic>
	<monogr>
		<title level="j">Ecol. Inform</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="60" to="68" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Gabor-filtering-based nearest regularized subspace for hyperspectral image classification</title>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Du</surname></persName>
		</author>
		<idno type="DOI">10.1109/JSTARS.2013.2295313</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1012" to="1022" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Classification and feature extraction for remote sensing images from urban areas based on morphological transformations</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Benediktsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pesaresi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Amason</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2003.814625</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geoscie. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1940" to="1949" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Classification of hyperspectral image using multiscale spatial texture features</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sidike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Asari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 8th Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS)</title>
		<meeting>the 2016 8th Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS)<address><addrLine>Los Angeles, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-08">August 2016</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep learning-based classification of hyperspectral data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<idno type="DOI">10.1109/JSTARS.2014.2329330</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2094" to="2107" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Spectral-spatial classification of hyperspectral data based on deep belief network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<idno type="DOI">10.1109/JSTARS.2015.2388577</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="2381" to="2392" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised spectral-spatial feature learning with stacked sparse autoencoder for hyperspectral imagery classification</title>
		<author>
			<persName><forename type="first">C</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2438" to="2442" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Spectral-spatial classification of hyperspectral image based on deep auto-encoder</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Geng</surname></persName>
		</author>
		<idno type="DOI">10.1109/JSTARS.2016.2517204</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="4073" to="4085" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Recursive Autoencoders-Based Unsupervised Feature Learning for Hyperspectral Image Classification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Huyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.1109/LGRS.2017.2737823</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1928" to="1932" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Progressively Expanded Neural Network (PEN Net) for hyperspectral image classification: A new neural network paradigm for remote sensing image analysis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sidike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">K</forename><surname>Asari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sagan</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.isprsjprs.2018.09.007</idno>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">146</biblScope>
			<biblScope unit="page" from="161" to="181" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks for hyperspectral image classification</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1155/2015/258619</idno>
	</analytic>
	<monogr>
		<title level="j">J. Sens</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep feature extraction and classification of hyperspectral images based on convolutional neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ghamisi</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2016.2584107</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="6232" to="6251" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep pyramidal residual networks for spectral-spatial hyperspectral image classification</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Paoletti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Haut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fernandez-Beltran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pla</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2018.2860125</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="740" to="754" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Active learning with convolutional neural networks for hyperspectral image classification using a new bayesian approach</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Haut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Paoletti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Plaza</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2018.2838665</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="6440" to="6441" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Self-paced learning-based probability subspace projection for hyperspectral image classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNNLS.2018.2841009</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="630" to="635" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A novel semi-supervised hyperspectral image classification approach based on spatial neighborhood information and classifier combination</title>
		<author>
			<persName><forename type="first">K</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Du</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.isprsjprs.2015.03.006</idno>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="page" from="19" to="29" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unsupervised Feature Extraction in Hyperspectral Images Based on Wasserstein Generative Adversarial Network</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2018.2876123</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Superpixel-based 3D deep neural networks for hyperspectral image classification</title>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Pun</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2017.09.007</idno>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="600" to="616" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">SuperPCA: A superpixelwise PCA approach for unsupervised feature extraction of hyperspectral imagery</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2018.2828029</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="4581" to="4593" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Hyperspectral image classification in the presence of noisy labels</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2018.2861992</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="851" to="865" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-07-01">26 June-1 July 2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Spectral-Spatial Residual Network for Hyperspectral Image Classification: A 3-D Deep Learning Framework</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chapman</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2017.2755542</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="847" to="858" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07">July 2017</date>
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">A Fast Dense Spectral-Spatial Convolution Network Framework for Hyperspectral Images Classification</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs10071068</idno>
		<imprint>
			<date type="published" when="1068">2018. 1068</date>
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
	<note>Remote Sens</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Hyperspectral Images Classification Based on Dense Convolutional Networks with Spectral-Wise Attention Mechanism</title>
		<author>
			<persName><forename type="first">B</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C W</forename><surname>Chan</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs11020159</idno>
	</analytic>
	<monogr>
		<title level="j">Remote Sens</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">159</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Convolutional block attention module</title>
		<author>
			<persName><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>So Kweon</surname></persName>
		</author>
		<author>
			<persName><surname>Cbam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)<address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-09">September 2018</date>
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Recurrent models of visual attention</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="2204" to="2212" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07">July 2015</date>
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Visual7w: Grounded question answering in images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-07-01">26 June-1 July 2016</date>
			<biblScope unit="page" from="4995" to="5004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Stacked attention networks for image question answering</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-07-01">26 June-1 July 2016</date>
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Dual attention networks for multimodal reasoning and matching</title>
		<author>
			<persName><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07">July 2017</date>
			<biblScope unit="page" from="299" to="307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Classification of hyperspectral remote sensing images with support vector machines</title>
		<author>
			<persName><forename type="first">F</forename><surname>Melgani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bruzzone</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2004.831865</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="1778" to="1790" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Hyperspectral region classification using a three-dimensional Gabor filterbank</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Healey</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2010.2046494</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="3457" to="3464" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Spectral and spatial classification of hyperspectral data using SVMs and morphological profiles</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fauvel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Benediktsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chanussot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Sveinsson</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2008.922034</idno>
		<ptr target="http://creativecommons.org/licenses/by/4.0/)" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="3804" to="3814" />
			<date type="published" when="2008">2008</date>
			<pubPlace>Basel, Switzerland</pubPlace>
		</imprint>
	</monogr>
	<note>Licensee MDPI. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
