<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IterefinE: Iterative KG Refinement Embeddings using Symbolic Knowledge</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-06-03">3 Jun 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Siddhant</forename><surname>Arora</surname></persName>
							<email>siddhantarora1806@gmail.com</email>
							<affiliation key="aff0">
								<address>
									<addrLine>1. See [Wang et</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Maya</forename><surname>Ramanath</surname></persName>
							<email>ramanath@cse.iitd.ac.in</email>
							<affiliation key="aff0">
								<address>
									<addrLine>1. See [Wang et</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Deepak</forename><surname>Sharma</surname></persName>
							<email>dsharma080@gmail.com</email>
							<affiliation key="aff0">
								<address>
									<addrLine>1. See [Wang et</addrLine>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">IterefinE: Iterative KG Refinement Embeddings using Symbolic Knowledge</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-06-03">3 Jun 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2006.04509v1[cs.AI]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Knowledge Graphs (KGs) extracted from text sources are often noisy and lead to poor performance in downstream application tasks such as KG-based question answering. While much of the recent activity is focused on addressing the sparsity of KGs by using embeddings for inferring new facts, the issue of cleaning up of noise in KGs through KG refinement task is not as actively studied. Most successful techniques for KG refinement make use of inference rules and reasoning over ontologies. Barring a few exceptions, embeddings do not make use of ontological information, and their performance in KG refinement task is not well understood. In this paper, we present a KG refinement framework called IterefinE which iteratively combines the two techniques -one which uses ontological information and inferences rules, viz.,PSL-KGI, and the KG embeddings such as ComplEx and ConvE which do not. As a result, IterefinE is able to exploit not only the ontological information to improve the quality of predictions, but also the power of KG embeddings which (implicitly) perform longer chains of reasoning. The IterefinE framework, operates in a co-training mode and results in explicit type-supervised embeddings of the refined KG from PSL-KGI which we call as TypeE-X. Our experiments over a range of KG benchmarks show that the embeddings that we produce are able to reject noisy facts from KG and at the same time infer higher quality new facts resulting in upto 9% improvement of overall weighted F1 score.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Knowledge graphs (KGs) represent facts as a set of directed edges or triples s,r,o where r is the relation between entities s and o. A critical issue in large-scale KGs is the presence of noise from the automatic extraction methods used to populate them. For instance, NELL <ref type="bibr" target="#b1">[Carlson et al., 2010]</ref> is known to contain various kinds of errors including: different names for the same entity (e.g., australia and austalia), incorrect relationships -both due to wrong relation label as well as incorrect linkage altogether-between entities (e.g., matt flynn, athleteplayssport, baseball is false since Matt Flynn is an NFL player), incompatible entity types, and many more <ref type="bibr" target="#b18">[Pujara et al., 2013]</ref>. It has also been observed that such noise can significantly degrade the performance of KG embeddings <ref type="bibr" target="#b19">[Pujara et al., 2017]</ref>.</p><p>The KG refinement task aims to reduce the noise in KG by not only predicting additional links (relations) and types for entities (i.e., performing KG completion), but also eliminating incorrect facts. Methods for noise reduction in KG include the use of association rule mining over the noisy KG to induce rules which can help in eliminating incorrect facts <ref type="bibr" target="#b11">[Ma et al., 2014]</ref>; reconciling diverse evidence from multiple extractors <ref type="bibr" target="#b4">[Dong et al., 2014]</ref>; the use of ontology reasoners <ref type="bibr" target="#b14">[Nakashole et al., 2011</ref>] and many more. A detailed survey of approaches for KG refinement is available in <ref type="bibr" target="#b17">[Paulheim, 2017]</ref>. On the other hand, neural and tensor-based embeddings have seen significant success in entity type and new fact predictions <ref type="bibr" target="#b16">[Nickel et al., 2012</ref><ref type="bibr" target="#b23">, Trouillon et al., 2016</ref><ref type="bibr" target="#b3">, Dettmers et al., 2018]</ref>. It is worth noting that embeddings, with a few recent exceptions <ref type="bibr" target="#b6">[Guo et al., 2016</ref><ref type="bibr" target="#b12">, Minervini et al., 2017</ref><ref type="bibr">, 2018</ref><ref type="bibr" target="#b5">, Fatemi et al., 2019]</ref>, do not make use of rich taxonomic/ontological rules when available. Methods such as Probabilistic Soft Logic (PSL) and Markov Logic Network (MLN) have been adapted for the KG refinement problem, and can address both the completion as well as noise removal stages of the KG completion problem. They can also make use of ontological rules effectively, and specifically, the PSL-KGI implementation uses rules defined on schema-level features <ref type="bibr" target="#b18">[Pujara et al., 2013]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Contributions</head><p>In this paper we investigate the combined use of ontologies and embeddings in the KG refinement task. Ontologies are among the best methods to eliminate noisy facts in KGs, while embeddings provide a means of implicitly reasoning over longer chains of facts. Specifically, we use Probabilistic Soft Logic (PSL) that can incorporate inference rules and ontologies, along with state-of-the-art KG embedding methods,viz., ConvE <ref type="bibr" target="#b3">[Dettmers et al., 2018]</ref> and ComplEx <ref type="bibr" target="#b23">[Trouillon et al., 2016]</ref>, which do not make use of any ontological rules.</p><p>The resulting framework called IterefinE is based on the observation that the mispredictions by the embeddings based methods are often due to the lack of type compatibility between the entities due to their typeagnostic nature <ref type="bibr" target="#b25">[Xie et al., 2016</ref><ref type="bibr" target="#b7">, Jain et al., 2018]</ref>. Since PSL-KGI is able to predict entity types by making use of ontological information along with many candidate facts derived using its inference rules, IterefinE transfer these predictions from PSL-KGI to the embeddings. This results in embeddings with explicit type supervision, which we call as TypeE-ComplEx and TypeE-ConvE. Further, we feed the predictions back from TypeE-ComplEx (correspondingly, TypeE-ConvE) over the training set to the PSL-KGI, resulting in additional evidence for inference. This feedback cycle can be repeated for multiple iterations, although we have observed over various benchmark datasets that the performance stabilizes within 2 to 3 iterations. Our key findings reported in this paper are as follows:</p><p>(i) Explicit type supervision improves the weighted F1-score of embeddings by up to 9% over those which do not have type supervision.</p><p>(ii) Explicit type supervised models also outperform the implict type supervised models <ref type="bibr" target="#b7">[Jain et al., 2018]</ref>.</p><p>The margin of improvement is large when the ontological information is sufficiently rich to begin with.</p><p>(iii) Rich ontological information is a critical ingredient for the performance of TypeE-ConvE and TypeE-ComplEx, particularly when we consider their ability to remove the noisy triples. We observed that on datasets like YAGO3-10 and FB15K-237, we improved F1 scores on noisy triples by 30% to 100%.</p><p>We note that, although we have experimented with ConvE and ComplEx, it easy to instantiate IterefinE to work with other embeddings, which we plan to explore in our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we describe how KG refinement is accomplished by methods based on inference rules and embeddings-based methods. There are other research directions for (partially) solving the KG refinement problem such as rule induction <ref type="bibr" target="#b11">[Ma et al., 2014]</ref>, classification with diverse extractors <ref type="bibr" target="#b4">[Dong et al., 2014]</ref>, crowdsourcing, etc., (see <ref type="bibr" target="#b17">[Paulheim, 2017]</ref> for an overview). While these works have their own strengths and weaknesses, our focus in this paper is on the use of ontological rules (exemplified by PSL-KGI) and embeddings (we use ComplEx, ConvE and <ref type="bibr" target="#b7">[Jain et al., 2018]</ref>). Rule induction methods are orthogonal to our work, and may augment or replace the set of rules we use. Further, evidence from diverse extractors as in the case of <ref type="bibr" target="#b4">[Dong et al., 2014]</ref> can be incorporated into the PSL-KGI framework in a straightforward manner (see details about confidence values of triples in the Background section).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">KG Refinement with Ontological Rules</head><p>Methods based on Markov-Logic Networks or Probabilistic Soft Logic (PSL), model the KG refinement task as a constrained optimization problem that scores facts in the KG with the help of various symbolic (logical)</p><p>rules. An important input to these formulations are the probabilistic sources of information such as the confidence scores obtained during extraction <ref type="bibr" target="#b18">[Pujara et al., 2013</ref><ref type="bibr" target="#b8">, Jiang et al., 2012]</ref> from multiple sources. Of these methods, PSL-KGI <ref type="bibr" target="#b18">[Pujara et al., 2013</ref><ref type="bibr" target="#b19">[Pujara et al., , 2017] ]</ref> is shown not only to perform better with KG noise and sparsity, but also to be quite scalable. It uses the following sources of information in addition to the noisy input KG: confidence scores of extractions, a small seed set of manually labeled correct facts and type labels and ontology information and inference rules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Refinement task with KG embeddings</head><p>KG embedding methods define a scoring function f to score the plausibility of a triple 1 and learn embeddings in such a way as to maximise the plausibility of the triples that are already present in the KG <ref type="bibr" target="#b15">[Nickel et al., 2011</ref><ref type="bibr" target="#b21">, Socher et al., 2013</ref><ref type="bibr" target="#b23">, Trouillon et al., 2016]</ref>.</p><p>An important step in learning is the generation of negative samples since the existing triples are all labeled positive. The negative samples are typically generated by corrupting one or more components of the triple. With this dataset containing both positive and negative samples, training can be done for the refinement task with a negative log-likelihood loss function as follows <ref type="bibr" target="#b23">[Trouillon et al., 2016]</ref>.</p><formula xml:id="formula_0">L(G) = (s,r,o,y)∈G y log f (s, r, o) + (1 − y) log (1 − f (s, r, o))<label>(1)</label></formula><p>where (s, r, o) is the relation triple, f is the scoring function, and y denotes whether the triple is given positive label or negative. Similar to the setting for PSL-KGI, embedding-based methods can also be used to predict type labels of entities (the typeOf relation). We work with ComplEx <ref type="bibr" target="#b23">[Trouillon et al., 2016]</ref> and ConvE <ref type="bibr" target="#b3">[Dettmers et al., 2018]</ref> embeddings which have shown state of the art performance in many KG prediction tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Type and Taxonomy Enhanced Embeddings</head><p>There are some recent efforts to incorporate type hierarchy information in KG embeddings -e.g., TKRL <ref type="bibr" target="#b25">[Xie et al., 2016]</ref> and TransC <ref type="bibr" target="#b10">[Lv et al., 2018]</ref>. Recently, SimplE + <ref type="bibr" target="#b5">[Fatemi et al., 2019]</ref> includes taxonomic information -i.e., subtype and subproperty information-and also shows that state-of-the-art embeddings like ComplEx <ref type="bibr" target="#b23">[Trouillon et al., 2016]</ref>, <ref type="bibr">SimplE [Kazemi and Poole, 2018]</ref>, ConvE <ref type="bibr" target="#b3">[Dettmers et al., 2018]</ref> cannot enforce subsumption. Taking a different approach <ref type="bibr" target="#b7">[Jain et al., 2018]</ref> propose extending standard KG embeddings without explicit type supervision by representing entities as a two-part vector with one part encoding only the type information while the other one is a traditional vector embedding of the entity (and corresponding change to the relation embeddings as well). Specifically it uses the following scoring function :</p><formula xml:id="formula_1">f (s, r, o) = σ(s t • r h ) * Y(s, r, o) * σ(o t • r t ),<label>(2)</label></formula><p>where s t and o t denote the embedding vectors for implicit type label of entities, and r h and r t denote the implicit type embeddings for domain and range of relation r. Y is the scoring function used by the underlying embeddings-based method -we experiment with ComplEx and ConvE. These embeddings enforce type compatibilities during KG link prediction task, and they showed nearly 5-8 point improvements in MRR and type F1 scores. In our work, we build on this idea further by adding another layer of explicitly supervised type vector to entity/relation embeddings.</p><p>Note, however, that our focus in this paper is not on embeddings that enforce ontological constraints, but on improving the KG refinement by combining the strengths of KG embeddings with methods like PSL-KGI and MLNs which can work with arbitrary (first-order) constraints.</p><p>Recently, there has been some work in modeling structural as well as uncertainty information of relations in the embedding space. <ref type="bibr" target="#b2">[Chen et al., 2019]</ref> uses Probabilistic Soft Logic to come up with plausibility scores for each fact which they train to match with the uncertainty score of seen relation triplets as well as minimize the plausibility score for relation triplets. However, they do not focus on the KG refinement task and they also do not investigate how existing Knowledge Graph Embedding methods can be used in conjunction with this approach to effectively embed Uncertain graphs. There has also been some research in using rule-based reasoning and KG embeddings together in an iterative manner in <ref type="bibr" target="#b26">[Zhang et al., 2019]</ref>.</p><p>They achieve improvements in the performance of link prediction tasks for sparse entities which cannot be effectively modelled by standard embedding methods. However, at each iteration, they are adding more rules to their database, which makes their approach less scalable to us since we are continuously removing noise from Knowledge Graph, thus making the size of resultant Knowledge Graph stable. Also, the feedback in their work was rules learned from embedding with a robust pruning strategy. In contrast, we passed feedback as relation triples along with their predicted score as additional context for the PSL-KGI model to generate high quality predictions. Finally, we test this feedback in Knowledge Graph refinement manner where we couple the task of removing noise as well as inferring new rules together in a coupled manner with both the tasks benefiting from each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Background</head><p>We use the PSL-KGI implementation generously provided by the authors 2 . The inputs to PSL-KGI are:</p><p>(i) the triples extracted from multiple input sources and confidence values for these triples, (ii) ontology information, such as sub-class (SUB) and sub-property (RSUB) information; the domain and range of relations (DOM, RNG); "same" entities (SAMEENT), entities and relations that are mutually exclusive (MUT and RMUT); and inverse relations (INV). We reproduce the list of information used in <ref type="bibr" target="#b18">[Pujara et al., 2013]</ref> in tabular form in Table <ref type="table" target="#tab_0">1</ref>.</p><p>(iii) Inference rules -specifically, there are 7 general constraints that were first introduced in the earlier work on Markov Logic Networks (MLN) based work <ref type="bibr" target="#b8">[Jiang et al., 2012]</ref>. These rules are listed in Appendix A in Table <ref type="table" target="#tab_1">2</ref>. Based on these PSL-KGI defines a PSL program that combines the ontological rules and constraints with atoms in the KG.The ontological information and inference rules are summarized in Table <ref type="table" target="#tab_1">1 and Table 2</ref> respectively. The solution to the PSL program essentially provides most likely interpretation of the KG, defining a probability distribution over the KG. By appropriately selecting the threshold on the probability value, it is possible to reject noisy facts. It is also important to note that PSL-KGI also generates a number of candidate facts that are not originally in the KG by soft-inference over the ontology and inference rules. While the extraction confidence for a triple may be high, it is possible for PSL-KGI to output a low score for that triple because of the inference rules. This enables it to determine correct type labels and in expanding the seed set iteratively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ontological Information Description</head><p>2. https://github.com/linqs/psl-examples/tree/master/knowledge-graph-identification</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Class</head><p>Ontological Rule</p><p>Uncertain Extractions  <ref type="bibr" target="#b0">[Blum and Mitchell, 1998]</ref>, to combine the strengths of PSL-KGI and KG embeddings. The mechanism consists of two stages, as shown in Figure <ref type="figure" target="#fig_1">1</ref>. In the first stage, PSL-KGI is used to generate high-quality type predictions, and in the second stage, an enhanced KG embeddings method, which we term as TypeE-X (where X is an embeddings method such as ComplEx), takes as input, the type predictions and relation triples labeled true in the training set. At the end of the second stage, the embeddings generated are expected to be of higher quality. The feedback to PSL-KGI is completed by passing the predictions from the KG refinement of TypeE-X back to PSL-KGI which takes them, along with the original extraction scores, as additional context for predicting relation triples. Note that this process can be repeated iteratively, allowing the propagation of potentially more context at each iteration 3 .</p><formula xml:id="formula_2">w CR−T : CAN DREL T (E 1 , E 2 , R) ⇒ REL(E 1 , E 2 , R) w CL−T : CAN DLBL T (E, L) ⇒ LBL(E, L) Entity Resolution SAM EEN T (E 1 , E 2 ) ∧ LBL(E 1 , L) ⇒ LBL(E 2 , L) SAM EEN T (E 1 , E 2 ) ∧ REL(E 1 , E, R) ⇒ REL(E 2 , E, R) SAM EEN T (E 1 , E 2 ) ∧ REL(E, E 1 , R) ⇒ REL(E, E 2 , R) INV IN V (R, S) ∧ REL(E 1 , E 2 , R) ⇒ REL(E 2 , E 1 , S) Selectional Preference DOM (R, L) ∧ REL(E 1 , E 2 , R) ⇒ LBL(E 1 , L) RN G(R, L) ∧ REL(E 1 , E 2 , R) ⇒ LBL(E 2 , L) Subsumption SU B(L, P ) ∧ LBL(E, L) ⇒ LBL(E, P ) RSU B(R, S) ∧ REL(E 1 , E 2 , R) ⇒ REL(E 1 , E 2 , S) Mutual Exclusion M U T (L 1 , L 2 ) ∧ LBL(E, L 1 ) ⇒ ¬LBL(E, L 2 ) RM U T (R, S) ∧ REL(E1, E2, R) ⇒ ¬REL(E1, E2, S)</formula><p>Our observations show that passing all newly predicted triples by TypeE-X back to PSL-KGI as feedback would make our approach nonscalable for multiple iterations. Therefore, we only add some of the top most positive and most negative relations so that the size of the KG remains stable without sacrificing accuracy.</p><p>In order to ensure that an optimal number of positive and negative triples are fed back to PSL-KGI, we calculate separate thresholds for each. First, the classifier threshold t 1 determines which triples are predicted as positive and which are negative. This threshold is determined by optimizing over a validation set. Second, we divide the set of triples, using t 1 , into positive triples, denoted by P 1 , and negative triples, denoted by N 1 . Now, we choose two new thresholds t 2 and t 3 :</p><formula xml:id="formula_3">t 2 = t 1 + Φ 1 * mean(P 1 ) t 3 = t 1 − Φ 2 * mean(N 1 )<label>(3)</label></formula><p>where mean(X) is the mean score of triples in set X, Φ 1 and Φ 2 are parameters that can be tuned. Then we add all relations with predicted probability greater than t 2 , along with their inferred probabilities, as a form of positive feedback for our PSL-KGI model of the next iteration. Similarly, the negative feedback would consist of all relations with predicted probabilities less than t 3 . We discuss the impact of these thresholds on the size of the KG and the prediction accuracy in Section 6.4.  To incorporate the type inferences for entities generated by PSL-KGI in KG embeddings (the second stage), we modify the typed model <ref type="bibr" target="#b7">[Jain et al., 2018]</ref> as follows:</p><p>Instead of just using the implicit type embeddings, we concatenate them with embeddings of explicit types transferred from PSL-KGI. Note that the implicit type embeddings are learned for each entity or relation, whereas the explicit type embeddings are the same for all entities with the same type label. The scoring function for extended typed model, TypeE-X, with an underlying embedding model X is</p><formula xml:id="formula_4">f (s, r, o) = σ ((s t s l ) • (r h r dom )) * Y (s, r, o) * σ ((o t o l ) • (r t r range )) ,<label>(4)</label></formula><p>where s l denotes the explicit type label assigned to entity s, r dom and r range provide the explicit type labels for domain and range of a relation respectively. The type compatibility is enforced by concatenating, denoted , the two vectors and taking their dot product. In case an explicit type label for an entity is unknown, we use the UNK embedding as per the convention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Preparing Datasets for Evaluating the KG Refinement Task</head><p>Before we present the details of the datasets used in our study, we first present the methodology followed to prepare them for use in the KG refinement task. As discussed earlier, apart from NELL, none of the KG benchmarks contain noise labels, making them unsuitable for evaluating the KG refinement task. We prepare them as follows:</p><p>• We sample a random 25% of all facts (including the typeOf relations) and corrupt them by randomly changing their subject, relation label or object. Note that this was the same model followed in an earlier study <ref type="bibr" target="#b19">[Pujara et al., 2017]</ref>.</p><p>• We further refine the noise model by ensuring that half of the corrupted facts have entities that are type compatible to the relation of the fact. This makes it harder for detecting corrupted facts simply by using type compatibility checks.</p><p>To capture realistic KG refinement settings, we further add extraction scores generated by sampling them from two different normal distributions: N (0.7, 0.2) for facts in the original KG and N (0.3, 0.2) for added noisy facts <ref type="bibr" target="#b18">[Pujara et al., 2013]</ref>. The SAMEENT facts between entities are generated by calculating the average of the two Jaccard similarity score over sets of relationships with these pair of entities as head and tail entity respectively -the average score acts as the confidence score of the fact. Finally, for all datasets, the test and validation sets are created by randomly partitioning the KG. Note that for all datasets the test set also includes the facts that were part of the original benchmark test collection.   The NELL subset taken from its 165 th iteration <ref type="bibr" target="#b1">[Carlson et al., 2010]</ref>) has been used for the KG refinement task <ref type="bibr" target="#b18">[Pujara et al., 2013</ref><ref type="bibr" target="#b8">, Jiang et al., 2012]</ref>. It comes with a rich ontology from the NELL system, and contains multiple sources of information i.e., a single fact is present with multiple extraction scores. Since the original dataset does not have validation set, we split the test set into 2 equal halves preserving the same class balance, and use them as our validation and test split.</p><p>YAGO3-10: YAGO3-10 [ <ref type="bibr" target="#b3">Dettmers et al., 2018]</ref> is a subset of the YAGO3 <ref type="bibr" target="#b22">[Suchanek et al., 2007]</ref> knowledge graph. It is often used for evaluating the KG completion task. We have augmented it with ontological facts and entity types derived from YAGO3. Since YAGO3 has a large number of types, we contract the type hierarchy to make it comparable to other datasets. We linked YAGO facts directly with the YAGO taxonomy by skipping the rdf:type entities at leaves of taxonomy (from YAGO simple types) and the first level of YAGO taxonomy. Then all facts upto length 3 in the hierarchy of taxonomy were included.</p><p>FB15K-237: FB15K-237 <ref type="bibr" target="#b3">[Dettmers et al., 2018]</ref>, another popular benchmark does not have ontological and type label information. Therefore, we use the type labels for entities from <ref type="bibr" target="#b25">[Xie et al., 2016]</ref> which also provides the domain and range information for relations. The subclass information is populated by reconstructing the type hierarchy from type label facts. Mutually exclusive labels, relations and inverse relations are automatically created by mining the KG -e.g. we can find inverse relations by checking if all reverse edges exists in the KG for a relation.</p><p>WN18RR: WN18RR, similar to FB15K-237, does not contain ontological and type information. We used the synset information obtained from <ref type="bibr">[Villmow, 2018]</ref>, to assign type labels for entities. For example, for synset hello.n.01, the type is considered as noun(n). Using an older ontology 4 we derived the rest of ontological information for the dataset.</p><p>Table <ref type="table" target="#tab_3">3</ref> summarizes the size of different KG datasets we use in our evaluation. Table <ref type="table" target="#tab_4">4</ref> shows the amount of ontological information for each dataset. NELL and FB15K-237 have reasonably rich ontological information compared to YAGO3-10 and WN18RR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experimental Evaluation</head><p>We evaluate the performance of TypeE-X models in the KG refinement task, and compare them with Com-plEx <ref type="bibr" target="#b23">[Trouillon et al., 2016]</ref> and ConvE <ref type="bibr" target="#b3">[Dettmers et al., 2018]</ref>, two state-of-the-art KG embeddings methods, 4. https://www.w3.org/2006/03/wn/wn20/ and PSL-KGI. We also use ComplEx and ConvE as base embedding models for our TypeE-X method to get TypeE-ComplEx and TypeE-ConvE respectively. We use a single hyper-parameter threshold as the cutoff for classifying a test triple based on the prediction score <ref type="bibr" target="#b18">[Pujara et al., 2013]</ref>. Our experiments were run on Intel(R) Xeon(R) x86-64 machine with 64 CPUs using 1 NVIDIA GTX 1080 Ti GPU. We observe the average running time with TypeE-ComplEx to be between 25-100 minutes and with TypeE-ConvE to be between 120-420 minutes per iteration. The increased time observed for TypeE-ConvE experiments is because of the fact that ConvE takes longer time to train than ComplEx 5 . The hyper-parameter is tuned on the validation set and used unchanged for the test set. We use φ 1 = 0.5 and φ 2 = 0.75 in Equation 3 as these hyperparameters were found to work across a variety of datasets.</p><p>The structure of experimental analysis we conducted are as follows:</p><p>• In Section 6.2, we report on the quality of embeddings generated by our TypeE-X methods compared with ComplEx, ConvE and PSL-KGI. In addition, we also compare our explicitly supervised TypeE-X methods with the implicitly supervised embeddings proposed by <ref type="bibr" target="#b7">[Jain et al., 2018]</ref>.</p><p>• In Section 6.3, we analyse how our accuracy changes as we increase the number of feedback iterations.</p><p>• In Section 6.4, we discuss the effect of the threshold parameters t 2 and t 3 on the size of the KG and the prediction accuracy.</p><p>• In Section 6.5, we present an ablation study and analyse the impact of the various ontological rules on the accuracy.</p><p>Evaluation Metric: Our main evaluation metric is the weighted F1 (wF1) measure. The reason for this is that in the KG refinement task, there is an imbalance in the two classes -noisy facts and correct facts 6 . Weighted F1 is defined as the individual class F1 score weighted by the number of instances per class in the test set.</p><formula xml:id="formula_5">wF 1 = w 1 * F 1(l 1 ) + w 0 * F 1(l 0 ) (5)</formula><p>where w k is the fraction of samples with label k (k ∈ {0, 1} in our setting), F 1(l k ) is the F 1 score computed only for class k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Baselines</head><p>In addition to the baselines ComplEx, ConvE and PSL-KGI, we compare our method with two other ensemble methods, described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ConvE + ComplEx:</head><p>In the first stage, instead of using PSL-KGI for predictions, we use ConvE. These predictions (along with the original KG) are used as input to the second stage which used ComplEx.</p><p>Note that this baseline combines to similar methods.</p><p>α -model: This baseline is a simple score combination of two different methods (in contrast to the two stages with iterations of our method). We use the setting introduced in R-GCN <ref type="bibr" target="#b20">(Schlichtkrull et al. [2018]</ref>) to combine scores of KG embeddings and PSL-KGI methods using the equation given below:</p><formula xml:id="formula_6">f (h, r, t) α−model = α * f (h, r, t) P SL−KGI + (1 − α) * f (h, r, t) model (6)</formula><p>Here the hyperparameter α is chosen based on the validation set. The optimal alpha value obtained are reported in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Accuracy of TypeE-X</head><p>Our main results are shown in Table <ref type="table" target="#tab_6">6</ref>. We include separate F1 measures for the two classes as well as the weighted F1 measure. This helps us analyse how well each method performs in identifying the correct (+ve) and noisy facts (-ve). From the table, we observe that our proposed combined methods TypeE-X consistently outperform the KG embeddings methods as well as the baseline PSL-KGI. Note that PSL-KGI is a formidable baseline over NELL since it contains a rich ontology.</p><p>For the positive class (correct facts), our method performs slightly better than the second best competitor, while for the negative class (noisy facts), both our methods show substantial improvements for YAGO3-10 and FB15K-237 datasets, while performing on par with PSL-KGI for NELL. The only dataset on which our methods fail to beat the PSL-KGI baseline is WN18RR and this is because of its very limited ontology (please refer to Table <ref type="table" target="#tab_4">4</ref>). Further, for all datasets our TypeE-X methods have the best wF 1 numbers. We have therefore validated our initial hypothesis that ontological information of high quality is tremendously helpful in improving the quality of embeddings.</p><p>Comparison with Baseline Ensemble Models. From Table <ref type="table" target="#tab_6">6</ref>, we see that TypeE-X models perform much better than ConvE +ComplEx. We hypothesize that this is because PSL-KGI and embeddings methods are complementary in nature. That is, PSL-KGI is better at removing noisy facts, while embeddings methods are better at inferring new facts. In contrast, when we combine ComplEx and ConvE, the resultant model cannot incorporate rich ontological information and, hence, cannot effectively remove noise from the KG. This intuition is confirmed by looking at low -ve F1 of these methods when compared to TypeE-X models in Table <ref type="table" target="#tab_6">6</ref> 7 .</p><p>We also observe that α-models perform better than the corresponding individual methods, but not better than our TypeE-X methods. This observation shows that our methodology of combining the two approaches in a pipeline fashion is more powerful than a simple weighted combination of these methods. The reason is that, in our method, each of the individual methods benefits from the strength of the other method since the results of one are used as input for the other. As a result, both these methods gain from each other's 5. Additional scalability experiments are reported in Appendix A.1 6. Noisy facts are much lower in number compared to correct facts. 7. For more observations regarding noise removal, please refer to Appendix A.2 performance. Further, we list the computed alpha values that showed the best validation performance in Table <ref type="table" target="#tab_5">5</ref>. We observe that the alpha values are mostly tilted towards the better-performing model.</p><p>Comparison with unsupervised type inference. In Table <ref type="table" target="#tab_7">7</ref> we compare the performance of TypeE-ComplEx which has explicit type supervision with the unsupervised type-compatible embeddings-based method proposed by Jain et al. <ref type="bibr" target="#b7">[Jain et al., 2018]</ref>. As these results indicate, while explicitly ensuring type compatibility helps to improve performance, adding type inferences from PSL-KGI to TypeE-ComplEx significantly improves the relation scores, improving weighted F1 up to 18% (over NELL).</p><p>Dataset <ref type="bibr" target="#b7">[Jain et al., 2018]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Analysis of feedback iterations</head><p>We have already shown in Table <ref type="table" target="#tab_6">6</ref> that our TypeE-X methods output higher quality predictions compared to the other baselines. In this section, we analyse the conditions under which multiple iterations can improve the quality of predictions.</p><p>Figure <ref type="figure" target="#fig_2">2</ref> shows how the wF 1 values of our TypeE-X methods change over six feedback iterations. Recall from Figure <ref type="figure" target="#fig_1">1</ref> that each iteration involves adding high quality tuples from PSL-KGI inferences to TypeE-X and feeding back high quality tuples from TypeE-X predictions back to PSL-KGI.</p><p>The main observation we make in Figure <ref type="figure" target="#fig_2">2</ref> is that the accuracy of predictions on datasets with a rich and good quality ontology (NELL, FB15K-237 and YAGO3-10) do not do not vary much. In fact, for NELL and YAGO3-10 the accuracy actually increases in multiple iterations (best accuracy for NELL is in the 6 th iteration, and for YAGO3-10 it is in the 3 rd ), while for FB15K-237, there is only a small decrease over the first and last iterations.</p><p>In contrast, for the WN18RR dataset, the accuracy degrades quite rapidly after the first iteration. The reason is that this dataset does not have even a moderate number of ontological rules that are of high quality 8 . This results in lower quality inference from PSL-KGI which feeds into the TypeE-X method. This results in lower quality predictions from TypeE-X, which is then fed back into PSL-KGI. Thus a cascading effect of low quality predictions from each method results in a rapid drop in prediction quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Impact of hyper-parameters t 1 and t 2</head><p>The threshold parameters t 1 and t 2 determine how many positive and negative triples are fed back to PSL-KGI from TypeE-X. The number of such feedback triples has an impact on both, the size of the KG as well as the accuracy of predictions (because PSL-KGI now performs inference using the new triples that have been fed back). Figure <ref type="figure" target="#fig_3">3</ref> shows, for FB15K-237, two heatmaps which quantify the impact of t 1 and t 2 . In the left heatmap, the impact of adding the top-k percent of positive and negative tuples on the size of the KG is shown 9 and in the right heatmap, the impact on the accuracy is shown. We observe that by adding very few 8. Recall from Section 5 that the ontology rules were obtained from <ref type="bibr">[Villmow, 2018]</ref> and an older ontology. 9. The size is normalized: positive and negative tuples, with slightly more positive tuples than negative tuples as feedback is sufficient to obtain the best accuracy, while ensuring that the KG size does not explode. We performed an ablation study to determine what kind of ontology rules were most useful in increasing prediction accuracy. The results for two datasets, NELL and FB15K-237 are shown in Figure <ref type="figure">8</ref>. From the table, we observe that the it is the Subclass, Domain and Range rules that are the most important. Clearly these rules are most useful in correctly predicting types, which in turn are crucial for the accuracy of the TypeE-X methods. Further, as these results show, none of the individual ontological components alone show performance comparable to using all the components (and thus all the rules) in the PSL-KGI phase of IterefinE. Although positive class performance over FB15K-237 remains unchanged when using any one ontological component, the performance over negative classes deteriorates significantly over using all the components. Thus, we argue that our proposal of using as much ontological information available in a KG is consistently superior for the KG refinement task. </p><formula xml:id="formula_7">(newsize−originalsize) (originalsize)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion and Future work</head><p>We have looked at the KG refinement task and methods for the same, from probabilistic rule based methods like PSL-KGI <ref type="bibr" target="#b18">[Pujara et al., 2013]</ref> to KG embedding methods like type-ComplEx <ref type="bibr" target="#b7">[Jain et al., 2018]</ref>. We showed their performance on existing datasets in the literature, and how the extent pf ontology plays a crucial role in performance of PSL-KGI.</p><p>To overcome their individual limitations, we present a simple mechanism to combine the PSL-KGI and typed models, by providing feedback inputs to each of the two methods. Such a method provides the individual model components with additional context, generated from each other, which leads to an increase in performance of both components on almost all datasets used for the KG refinement task evaluation. We further closely inspect into what controls the magnitude of the improvement and what are effect of further iterations of this feedback mechanism.</p><p>For future work, we will look in ways to training the whole pipeline end to end thus increasing the flexibility and efficieny of our approach. Further we will also look into the concept of data augmentation for KG, so that more context is available related to a triple for prediction.</p><p>from TypeE-X models seems to be very helpful to PSL-KGI model thus motivating our approach. Finally, our model seems to be stable in its performance on noise removal with increasing iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 IterefinE Algorithm</head><p>Algorithm 1 IterefinE 1: procedure TYPEE-X(KG, inst)</p><p>2:</p><p>Algorithm on input KG with ontological instances 3:</p><p>for iter in [1, M AX IT ER] do 4:</p><p>Predictions are for existing &amp; inferred triplets and type labels. Rel P red denotes relation prediction and T ype pred denote type predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>(Rel P red, T ype pred) ← PSL-KGI (KG, inst)</p><formula xml:id="formula_8">6:</formula><p>for thresh in [0,1] do 7:</p><p>Use the threshold to classify triplets as noisy or valid 8:</p><p>(Rel Labels, T ype Labels) ← (Rel P red &gt; thresh, T ype pred &gt; thresh) Use the threshold that maximises performance on validation set Predict type for each entity by using type label prediction from PSL-KGI and using subclass constraints to contract type hierarachy Follow Equation 3 to send some of the triplets along with their predicted score as feedback for PSL-KGI method</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>3. For an algorithmic listing of IterefinE, please refer toAppendix A.3   </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: IterefinE: Combining PSL-KGI and embedding model X resulting in TypeE-X model. SCORING FUNCTION FOR TYPEE-X</figDesc><graphic url="image-1.png" coords="6,196.76,98.28,225.55,106.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Graph showing the weighted F1 score (y-axis) obtained at the given number of feedback iterations (x-axis).</figDesc><graphic url="image-2.png" coords="11,93.07,90.86,211.68,158.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: (Left) Variation of size with percentage of top positive and negative triples for TypeE-ComplEx after the first feedback iteration. (Right) Variation of wF1. Both heatmaps are for FB15K-237</figDesc><graphic url="image-5.png" coords="11,304.76,319.12,172.80,163.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Performance ← Test(V alid Rel P red, V alid T ype pred)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>← GENERATE TYPE (T ype pred, SU B inst) 18: Use the cleaner Knowledge Graph and high quality type predictions to train KG embedding method 19: M ODEL ← TypeE-X (KG clean, T ypes) 20: Obtained in fashion similar to t best 21: t 1 ← Best validation performance for M ODEL 22:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Ontological Information used in PSL-KGI Implementation</figDesc><table><row><cell>Domain (DOM)</cell><cell>Domain Of relation</cell></row><row><cell>Range (RNG)</cell><cell>Range of relation</cell></row><row><cell cols="2">Same Entity (SAMEENT) Helps perform Entity Resolution by specifying equivalence class of entities</cell></row><row><cell>MUT</cell><cell>Specifies that 2 entities are mutually exclusive in their type labels</cell></row><row><cell>Subclass (SUB)</cell><cell>Subsumption of labels</cell></row><row><cell>INV</cell><cell>Inversely related relations</cell></row><row><cell>RMUT</cell><cell>Mutually exclusive relations</cell></row><row><cell>SUBPROP (RSUB)</cell><cell>Subsumption of relations</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Ontological Inference Rules used by PSL-KGI 4. Combining PSL-KGI with KG embeddings</figDesc><table /><note>OVERVIEWWe now present a simple mechanism, partially based on the concept of co-training</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Number of entities, relation types and observed triples in datasets</figDesc><table><row><cell>Dataset</cell><cell cols="8">DOM RNG SUB RSUB MUT RMUT INV SAMEENT</cell></row><row><cell>NELL</cell><cell>418</cell><cell>418</cell><cell>288</cell><cell>461</cell><cell>17K</cell><cell>48K</cell><cell>418</cell><cell>8K</cell></row><row><cell>FB15K-237</cell><cell>237</cell><cell>237</cell><cell>44K</cell><cell>0</cell><cell>147K</cell><cell>53K</cell><cell>44</cell><cell>20K</cell></row><row><cell>YAGO3-10</cell><cell>37</cell><cell>37</cell><cell>828</cell><cell>2</cell><cell>30</cell><cell>870</cell><cell>8</cell><cell>20K</cell></row><row><cell>WN18RR</cell><cell>11</cell><cell>11</cell><cell>13</cell><cell>0</cell><cell>0</cell><cell>66</cell><cell>0</cell><cell>20K</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>: # of instances for each ontological component required by PSL-KGI</cell></row><row><cell>5.1 Datasets</cell></row><row><cell>NELL:</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>table 5 and model could be either ComplEx or ConvE. Optimal α values obtained based on performance on validation set</figDesc><table><row><cell></cell><cell cols="2">Method</cell><cell cols="7">NELL YAGO3-10 FB15K-237 WN18RR</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">α -ComplEx</cell><cell></cell><cell>1.0</cell><cell>0.4</cell><cell></cell><cell>0.7</cell><cell cols="2">0.3</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">α -ConvE</cell><cell></cell><cell>1.0</cell><cell>0.4</cell><cell></cell><cell>0.6</cell><cell cols="2">0.9</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell></cell><cell>NELL</cell><cell></cell><cell></cell><cell>YAGO3-10</cell><cell></cell><cell></cell><cell>FB15K-237</cell><cell></cell><cell></cell><cell>WN18RR</cell><cell></cell></row><row><cell></cell><cell cols="12">+ve F1 -ve F1 wF1 +ve F1 -ve F1 wF1 +ve F1 -ve F1 wF1 +ve F1 -ve F1 wF1</cell></row><row><cell>ComplEx</cell><cell>0.82</cell><cell>0.58</cell><cell>0.73</cell><cell>0.94</cell><cell>0.43</cell><cell>0.88</cell><cell>0.96</cell><cell>0.4</cell><cell>0.92</cell><cell>0.93</cell><cell>0.26</cell><cell>0.86</cell></row><row><cell>ConvE</cell><cell>0.74</cell><cell>0.55</cell><cell>0.67</cell><cell>0.94</cell><cell>0.37</cell><cell>0.87</cell><cell>0.95</cell><cell>0.37</cell><cell>0.90</cell><cell>0.93</cell><cell>0.07</cell><cell>0.84</cell></row><row><cell>PSL-KGI</cell><cell>0.85</cell><cell>0.68</cell><cell>0.79</cell><cell>0.91</cell><cell>0.39</cell><cell>0.85</cell><cell>0.92</cell><cell>0.39</cell><cell>0.88</cell><cell>0.91</cell><cell>0.37</cell><cell>0.85</cell></row><row><cell>ConvE +ComplEx</cell><cell>0.82</cell><cell>0.58</cell><cell>0.73</cell><cell>0.95</cell><cell>0.43</cell><cell>0.89</cell><cell>0.96</cell><cell>0.39</cell><cell>0.92</cell><cell>0.93</cell><cell>0.15</cell><cell>0.85</cell></row><row><cell>α -ComplEx</cell><cell>0.85</cell><cell>0.68</cell><cell>0.79</cell><cell>0.94</cell><cell>0.50</cell><cell>0.89</cell><cell>0.96</cell><cell>0.58</cell><cell>0.93</cell><cell>0.94</cell><cell>0.24</cell><cell>0.87</cell></row><row><cell>α -ConvE</cell><cell>0.85</cell><cell>0.68</cell><cell>0.79</cell><cell>0.94</cell><cell>0.41</cell><cell>0.88</cell><cell>0.95</cell><cell>0.47</cell><cell>0.92</cell><cell>0.92</cell><cell>0.34</cell><cell>0.85</cell></row><row><cell>TypeE-ComplEx</cell><cell>0.86</cell><cell>0.68</cell><cell>0.79</cell><cell>0.95</cell><cell>0.56</cell><cell>0.91</cell><cell>0.98</cell><cell>0.82</cell><cell>0.97</cell><cell>0.93</cell><cell>0.24</cell><cell>0.85</cell></row><row><cell>TypeE-ConvE</cell><cell>0.86</cell><cell>0.67</cell><cell>0.79</cell><cell>0.95</cell><cell>0.47</cell><cell>0.89</cell><cell>0.98</cell><cell>0.77</cell><cell>0.96</cell><cell>0.94</cell><cell>0.31</cell><cell>0.87</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Overall performance of all models in KG refinement task using the best wF1 measure obtained in first 6 iterations. +ve F1 indicate the F1 score for correct facts and -ve F1 indicate F1 score for noisy facts.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Weighted F1 scores on relation triples in the test set by<ref type="bibr" target="#b7">[Jain et al., 2018]</ref> and TypeE-ComplEx.Anecdotes. Looking at the example predictions by both TypeE-ComplEx and ComplEx on YAGO3-10, we observed that TypeE-ComplEx is able to correctly identify simple noisy facts like Leinster Rugby, hasGender, Republic of Ireland , where there is a clear type incompatibility, which ComplEx is unable to identify. Further TypeE-ComplEx is able to identify noisy facts such as Richard Appleby, playsFor, Sporting Kansas City , with type compatible entities, by finding the reasoning context that connect Richard Appleby with football teams of UK and not US.</figDesc><table><row><cell></cell><cell></cell><cell>TypeE-ComplEx</cell></row><row><cell>NELL</cell><cell>0.60</cell><cell>0.71</cell></row><row><cell>YAGO3-10</cell><cell>0.88</cell><cell>0.92</cell></row><row><cell>FB15K-237</cell><cell>0.93</cell><cell>0.97</cell></row><row><cell>WN18RR</cell><cell>0.85</cell><cell>0.85</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Ablation study for performance without ontology subclass in KG refinement task for TypeE-ComplEx models. We have shown results for FB15K-237 at end of second epoch and NELL at end of third epoch.</figDesc><table><row><cell>Method</cell><cell></cell><cell>NELL</cell><cell></cell><cell></cell><cell>FB15K-237</cell><cell></cell></row><row><cell></cell><cell>+ve F1</cell><cell>-ve F1</cell><cell>wf1</cell><cell>+ve F1</cell><cell>-ve F1</cell><cell>wF1</cell></row><row><cell>All rules</cell><cell>0.86</cell><cell>0.68</cell><cell>0.79</cell><cell>0.98</cell><cell>0.80</cell><cell>0.97</cell></row><row><cell>No rules</cell><cell>0.82</cell><cell>0.58</cell><cell>0.73</cell><cell>0.96</cell><cell>0.4</cell><cell>0.92</cell></row><row><cell>w/o DOM</cell><cell cols="6">0.85 (-0.01) 0.65 (-0.03) 0.78 (-0.01) 0.98 (0.00) 0.76 (-0.04) 0.96 (-0.01)</cell></row><row><cell>w/o SAMEENT</cell><cell cols="3">0.85 (-0.01) 0.67 (-0.01) 0.79 (0.00)</cell><cell>0.98 (0.00)</cell><cell>0.80 (0.00)</cell><cell>0.97 (0.00)</cell></row><row><cell>w/o MUT</cell><cell cols="2">0.85 (-0.01) 0.68 (0.00)</cell><cell>0.79 (0.00)</cell><cell>0.98 (0.00)</cell><cell>0.80 (0.00)</cell><cell>0.97 (0.00)</cell></row><row><cell>w/o RNG</cell><cell cols="6">0.82 (-0.04) 0.65 (-0.03) 0.76 (-0.03) 0.97 (-0.01) 0.72 (-0.08) 0.95 (-0.02)</cell></row><row><cell>w/o SUB</cell><cell cols="4">0.84 (-0.02) 0.63 (-0.05) 0.77 (-0.02) 0.98 (0.00)</cell><cell>0.81 (0.01)</cell><cell>0.97 (0.00)</cell></row><row><cell>w/o RMUT</cell><cell cols="3">0.86 (0.00) 0.67 (-0.01) 0.79 (0.00)</cell><cell>0.98 (0.00)</cell><cell>0.80 (0.00)</cell><cell>0.97 (0.00)</cell></row><row><cell>w/o INV</cell><cell cols="4">0.85 (-0.01) 0.66 (-0.02) 0.78 (-0.01) 0.98 (0.00)</cell><cell>0.81 (0.01)</cell><cell>0.97 (0.00)</cell></row><row><cell>w/o RSUB</cell><cell cols="3">0.86 (0.00) 0.67 (-0.01) 0.79 (0.00)</cell><cell>0.98 (0.00)</cell><cell>0.80 (0.00)</cell><cell>0.97 (0.00)</cell></row><row><cell cols="5">ONLY DOM+RNG 0.84 (-0.02) 0.65 (-0.03) 0.77 (-0.02) 0.98 (0.00)</cell><cell>0.80 (0.00)</cell><cell>0.97 (0.00)</cell></row><row><cell>ONLY DOM</cell><cell cols="6">0.84 (-0.02) 0.64 (-0.04) 0.77 (-0.02) 0.98 (0.00) 0.73 (-0.07) 0.96 (-0.01)</cell></row><row><cell>ONLY RNG</cell><cell cols="6">0.83 (-0.03) 0.63 (-0.05) 0.76 (-0.03) 0.98 (0.00) 0.76 (-0.04) 0.96 (-0.01)</cell></row><row><cell cols="7">ONLY SAMEENT 0.83 (-0.03) 0.63 (-0.05) 0.76 (-0.03) 0.98 (0.00) 0.73 (-0.07) 0.96 (-0.01)</cell></row><row><cell>ONLY MUT</cell><cell cols="6">0.83 (-0.03) 0.63 (-0.05) 0.76 (-0.03) 0.98 (0.00) 0.73 (-0.07) 0.96 (-0.01)</cell></row><row><cell>ONLY SUB</cell><cell cols="6">0.82 (-0.04) 0.60 (-0.08) 0.74 (-0.05) 0.98 (0.00) 0.74 (-0.06) 0.96 (-0.01)</cell></row><row><cell>ONLY RMUT</cell><cell cols="6">0.83 (-0.03) 0.62 (-0.06) 0.76 (-0.03) 0.98 (0.00) 0.76 (-0.04) 0.96 (-0.01)</cell></row><row><cell>ONLY INV</cell><cell cols="6">0.84 (-0.02) 0.63 (-0.05) 0.76 (-0.03) 0.98 (0.00) 0.73 (-0.07) 0.96 (-0.01)</cell></row><row><cell>ONLY RSUB</cell><cell cols="3">0.83 (-0.03) 0.62 (-0.06) 0.76 (-0.03)</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Predictions having less than t best removed 15: KG clean ← FILTER(KG,t best )</figDesc><table><row><cell>13:</cell><cell>t best ← max thresh (Valid Performance)</cell></row><row><cell>14:</cell><cell></cell></row><row><cell>16:</cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Scalability issues</head><p>Scalability is an essential issue in the feedback experiments. We initially passed the predictions of TypeE-X embedding on the entire knowledge graph as feedback to the PSL-KGI model. We observed that this approach does not make the model scalable, and the memory required by PSL-KGI increased exponentially. With our current approach using thresholds t 2 and t 3 in order to feed back only the high confidence triples, we are able to prevent the size of the KG from exploding. Figure <ref type="figure">4</ref> shows the size of the KG for the various datasets over different iterations. We observe two key behaviors, as seen in Figure <ref type="figure">4</ref>. We see that for FB15K-237, the size almost uniformly increases with increasing iterations, whereas, for YAGO3-10, the size of KG becomes stable after a few iterations. This phenomenon is again related to how useful the initial ontology is to help the PSL-KGI model to filter noise added at every iteration. Since for Yago and NELL, ontology comes with the datasets, they are of high quality, and we observe much more stability in Knowledge Graph sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Noise removal</head><p>We investigate our performance on cleaning up noisy KGs. The observed behavior can be seen in figure <ref type="figure">5</ref>. As expected, type compatible noise is harder to remove than type noncompatible noise. Moreover, for both datasets, the performance on type compatible noise seems to get better for the first few iterations. This can be attributed to the fact that feedback in terms of type predictions and high-quality relation triples from PSL-KGI are getting better with the increasing number of iterations. This improvement shows that the feedback sent </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Combining labeled and unlabeled data with co-training</title>
		<author>
			<persName><forename type="first">Avrim</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eleventh annual conference on Computational learning theory</title>
				<meeting>the eleventh annual conference on Computational learning theory</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="92" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Toward an architecture for never-ending language learning</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Betteridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Kisiel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Burr</forename><surname>Settles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">M</forename><surname>Estevam R Hruschka</surname></persName>
		</author>
		<author>
			<persName><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Fourth AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Embedding uncertain knowledge graphs</title>
		<author>
			<persName><forename type="first">Xuelu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijia</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlo</forename><surname>Zaniolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 conference on Association for the Advancement of Artificial Intelligence (AAAI)</title>
				<meeting>the 2019 conference on Association for the Advancement of Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Convolutional 2d knowledge graph embeddings</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Knowledge vault: A web-scale approach to probabilistic knowledge fusion</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geremy</forename><surname>Heitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wilko</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ni</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Strohmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaohua</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="601" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improved Knowledge Graph Embedding Using Background Taxonomic Information</title>
		<author>
			<persName><forename type="first">Bahare</forename><surname>Fatemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siamak</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI</title>
				<meeting>of AAAI</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Jointly embedding knowledge graphs and logical rules</title>
		<author>
			<persName><forename type="first">Shu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="192" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Type-sensitive knowledge base inference without explicit type supervision</title>
		<author>
			<persName><forename type="first">Prachi</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pankaj</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumen</forename><surname>Chakrabarti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="75" to="80" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning to refine an automatically extracted knowledge base using markov logic</title>
		<author>
			<persName><forename type="first">Shangpu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Lowd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dejing</forename><surname>Dou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE 12th International Conference on Data Mining</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="912" to="917" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Simple embedding for link prediction in knowledge graphs</title>
		<author>
			<persName><forename type="first">Mehran</forename><surname>Seyed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31</title>
				<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4284" to="4295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Differentiating concepts and instances for knowledge graph embedding</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1971" to="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning disjointness axioms with association rule mining and its application to inconsistency detection of linked data</title>
		<author>
			<persName><forename type="first">Yanfang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianxing</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guilin</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Chinese Semantic Web and Web Science Conference</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="29" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Regularizing knowledge graph embeddings via equivalence and inversion axioms</title>
		<author>
			<persName><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Costabello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emir</forename><surname>Muñoz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vít</forename><surname>Novácek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre-Yves</forename><surname>Vandenbussche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases -European Conference, ECML PKDD</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Towards neural theorem proving at scale</title>
		<author>
			<persName><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matko</forename><surname>Bosnjak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<idno>CoRR, abs/1807.08204</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Scalable knowledge harvesting with high precision and high recall</title>
		<author>
			<persName><forename type="first">Ndapandula</forename><surname>Nakashole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Theobald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Forth International Conference on Web Search and Web Data Mining, WSDM 2011</title>
				<meeting>the Forth International Conference on Web Search and Web Data Mining, WSDM 2011<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">February 9-12, 2011. 2011</date>
			<biblScope unit="page" from="227" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A three-way model for collective learning on multi-relational data</title>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hans-Peter</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="809" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Factorizing YAGO: scalable machine learning for linked data</title>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hans-Peter</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st World Wide Web Conference</title>
				<meeting>the 21st World Wide Web Conference<address><addrLine>Lyon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-04-16">2012. 2012. April 16-20, 2012. 2012</date>
			<biblScope unit="page" from="271" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Knowledge graph refinement: A survey of approaches and evaluation methods</title>
		<author>
			<persName><forename type="first">Heiko</forename><surname>Paulheim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Semantic web</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="489" to="508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Knowledge graph identification</title>
		<author>
			<persName><forename type="first">Jay</forename><surname>Pujara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Semantic Web Conference</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="542" to="557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sparsity and noise: Where knowledge graph embeddings fall short</title>
		<author>
			<persName><forename type="first">Jay</forename><surname>Pujara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eriq</forename><surname>Augustine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1751" to="1756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Sejr</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rianne</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-93417-4_38</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-93417-438" />
	</analytic>
	<monogr>
		<title level="m">The Semantic Web -15th International Conference, ESWC 2018</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Aldo</forename><surname>Gangemi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Maria-Esther</forename><surname>Vidal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Pascal</forename><surname>Hitzler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Raphaël</forename><surname>Troncy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Laura</forename><surname>Hollink</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Anna</forename><surname>Tordai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mehwish</forename><surname>Alam</surname></persName>
		</editor>
		<meeting><address><addrLine>Heraklion, Crete, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">June 3-7, 2018. 2018</date>
			<biblScope unit="volume">10843</biblScope>
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Reasoning with neural tensor networks for knowledge base completion</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="926" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Yago: a core of semantic knowledge</title>
		<author>
			<persName><forename type="first">Gjergji</forename><surname>Fabian M Suchanek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerhard</forename><surname>Kasneci</surname></persName>
		</author>
		<author>
			<persName><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th international conference on World Wide Web</title>
				<meeting>the 16th international conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="697" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Complex embeddings for simple link prediction</title>
		<author>
			<persName><forename type="first">Théo</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Éric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
		<ptr target="https://github.com/villmow/datasetsknowledgeembedding" />
	</analytic>
	<monogr>
		<title level="m">Johannes Villmow. Transforming wn18 / wn18rr back to text</title>
				<imprint>
			<date type="published" when="2016">2016. 2018</date>
			<biblScope unit="page" from="2071" to="2080" />
		</imprint>
	</monogr>
	<note>International Conference on Machine Learning</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding: A survey of approaches and applications</title>
		<author>
			<persName><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhendong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2724" to="2743" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Representation learning of knowledge graphs with hierarchical types</title>
		<author>
			<persName><forename type="first">Ruobing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2965" to="2971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Iteratively learning embeddings and rules for knowledge graph reasoning</title>
		<author>
			<persName><forename type="first">Wen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bibek</forename><surname>Paudel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaoyan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abraham</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 conference on World Wide Web (WWW)</title>
				<meeting>the 2019 conference on World Wide Web (WWW)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
