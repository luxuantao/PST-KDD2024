<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Yuxuan Wang received his B.E. degree in network engineering from Nanjing University of Posts and Telecommunications, Nanjing, China, in 2009. He is currently pursuing his Ph.D. degree at The Ohio State University. He is interested in machine learning, optimization, speech separation, and computational neuroscience</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Pardo</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
							<email>dwang@cse.ohio-state.edu</email>
						</author>
						<author>
							<persName><forename type="first">Kun</forename><surname>Han</surname></persName>
							<email>hank@cse.ohio-state.edu</email>
						</author>
						<author>
							<persName><forename type="first">D</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Ohio State University</orgName>
								<address>
									<postCode>43210</postCode>
									<settlement>Columbus</settlement>
									<region>OH</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Engineering and the Center for Cognitive Science</orgName>
								<orgName type="institution">The Ohio State University</orgName>
								<address>
									<settlement>Columbus</settlement>
									<region>OH</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Yuxuan Wang received his B.E. degree in network engineering from Nanjing University of Posts and Telecommunications, Nanjing, China, in 2009. He is currently pursuing his Ph.D. degree at The Ohio State University. He is interested in machine learning, optimization, speech separation, and computational neuroscience</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5D57902F7B531365E5117DB0683366DF</idno>
					<idno type="DOI">10.1109/TASL.2012.2221459</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Exploring Monaural Features for Classification-Based Speech Segregation</head><p>Yuxuan Wang, Kun Han, and DeLiang Wang, Fellow, IEEE Abstract-Monaural speech segregation has been a very challenging problem for decades. By casting speech segregation as a binary classification problem, recent advances have been made in computational auditory scene analysis on segregation of both voiced and unvoiced speech. So far, pitch and amplitude modulation spectrogram have been used as two main kinds of time-frequency (T-F) unit level features in classification. In this paper, we expand T-F unit features to include gammatone frequency cepstral coefficients (GFCC), mel-frequency cepstral coefficients, relative spectral transform (RASTA) and perceptual linear prediction (PLP). Comprehensive comparisons are performed in order to identify effective features for classification-based speech segregation. Our experiments in matched and unmatched test conditions show that these newly included features significantly improve speech segregation performance. Specifically, GFCC and RASTA-PLP are the best single features in matched-noise and unmatched-noise test conditions, respectively. We also find that pitch-based features are crucial for good generalization to unseen environments. To further explore complementarity in terms of discriminative power, we propose to use a group Lasso approach to select complementary features in a principled way. The final combined feature set yields promising results in both matched and unmatched test conditions. Index Terms-Binary classification, computational auditory scene analysis (CASA), feature combination, group Lasso, monaural speech segregation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>S PEECH segregation, also known as the cocktail party problem, refers to the problem of segregating target speech from its background interference. Monaural speech segregation, which is the task of speech segregation from monaural recordings, is important for many real-world applications including robust speech and speaker recognition, audio information retrieval and hearing aids design (see e.g., <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b6">[7]</ref>). However, despite decades of effort, monaural speech segregation still remains one of the hardest problems in signal and speech processing. In this paper, we are concerned with monaural speech segregation from nonspeech interference; in other words, we do not address multitalker separation.</p><p>Numerous algorithms have been developed to attack the monaural speech segregation problem. For example, spectral subtraction <ref type="bibr" target="#b3">[4]</ref> and Weiner filtering <ref type="bibr" target="#b5">[6]</ref> are two representative techniques. However, assumptions regarding background interference are needed to make them work reasonably well. Another line of research relies on source models, e.g., training models for different speakers. Algorithms such as <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref> can work well if the statistical properties of the observations correspond well to training conditions. Generalization to different sources usually needs model adaptation, which is a non-trivial issue.</p><p>Computational auditory scene analysis (CASA), which is inspired by Bregman's account of auditory scene analysis (ASA) <ref type="bibr" target="#b1">[2]</ref>, has shown considerable promise in the last decade. The estimation of the ideal binary mask (IBM) is suggested as a primary goal of CASA <ref type="bibr" target="#b34">[35]</ref>. The IBM is a time-frequency (T-F) binary mask, constructed from premixed target and interference. A mask value 1 for a T-F unit indicates that the signal-to-noise ratio (SNR) within the unit exceeds a threshold (target-dominant), and 0 otherwise (interference-dominant). In this work, we use a 0 dB threshold in all the experiments. A series of recent experiments <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b36">[37]</ref> shows that IBM processing of sound mixtures yields large speech intelligibility gains.</p><p>The estimation of the IBM may be viewed as binary classification of T-F units. Recent studies have applied this formulation and achieved good speech segregation results in both anechoic and reverberant environments <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b38">[39]</ref>. In <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b19">[20]</ref>, the pitch-based features are used in training a classifier to separate target and interference dominant units. However, the pitch-based features cannot deal with unvoiced speech that lacks harmonic structure. Seltzer et al. <ref type="bibr" target="#b28">[29]</ref> and Weiss et al. <ref type="bibr" target="#b38">[39]</ref> use comb filter and spectrogram statistics as features. In <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, amplitude modulation spectrogram (AMS) is used, which makes unvoiced speech segregation possible as AMS is a characteristic of both voiced and unvoiced speech. Unfortunately, the generalization ability of AMS is not good <ref type="bibr" target="#b10">[11]</ref>.</p><p>For classification, the use of an appropriate classifier is obviously important. Our previous study <ref type="bibr" target="#b10">[11]</ref> focuses on classifier comparisons, and suggests that support vector machines (SVMs) work better than Gaussian mixture models (GMMs). However, this study only uses two existing features. Equally important for classification is the choice of appropriate features, which are less studied. It should be noted that we are concerned with T-F unit level features, i.e., spectral/cepstral features extracted from each T-F unit. Feature extraction is possible be-cause a T-F unit is a signal of a certain length. To our knowledge, aside from the features used in <ref type="bibr" target="#b28">[29]</ref>, only pitch and AMS have been used as T-F unit level features. On the other hand, in the speech and speaker recognition community, many acoustic features have been explored, such as gammatone frequency cepstral coefficients (GFCC), mel-frequency cepstral coefficients (MFCC), relative spectral transform (RASTA) and perceptual linear prediction (PLP), each having its own advantages. However, they have not been studied as T-F unit level features for classification-based speech segregation.</p><p>The objective of this paper is to conduct a comprehensive feature study for classification-based speech segregation. That said, we fix SVM as the classifier and explore the use of existing speech and speaker features under the same classification framework. Our contributions are as follows:</p><p>• We propose to extract conventional speech/speaker features within each T-F unit to significantly enlarge the feature repository for unit classification. • We propose a principled method to identify a complementary feature set. It is shown in speech recognition that complementarity exists between basic acoustic features <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b41">[42]</ref>. To investigate complementary features in terms of discriminative power, we address the corresponding group variable selection problem using a group least absolute shrinkage and selection operator (Lasso) <ref type="bibr" target="#b40">[41]</ref>. • We systematically compare the segregation performance of the newly included features and combinations in various acoustic environments. This paper is organized as follows. We present an overview of the system along with the methodology of extracting features at the T-F unit level in Section II. Section III describes a group Lasso approach to combining different features. Unit labeling results are reported in Section IV. We conclude this paper in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. SYSTEM OVERVIEW AND FEATURE EXTRACTION</head><p>We describe the architecture of our segregation system as follows. A sound mixture with the 16 kHz sampling frequency is first fed into a 64-channel gammatone filterbank, with center frequencies equally spaced from 50 Hz to 8000 Hz on the equivalent rectangular bandwidth rate scale. Gammatone filters model human auditory filters (critical bands) <ref type="bibr" target="#b25">[26]</ref>, and 64 channels provide an adequate frequency representation (see e.g., <ref type="bibr" target="#b36">[37]</ref>). The output in each channel is then divided into 20-ms frames with 10-ms overlapping between consecutive frames. This procedure produces a time-frequency representation of the sound mixture, called a cochleagram <ref type="bibr" target="#b35">[36]</ref>. Our computational goal is to estimate the ideal binary mask for the mixture. Since the energy distribution of speech signals in different channels can be very different, we train a Gaussian-kernel SVM <ref type="bibr" target="#b10">[11]</ref> for each subband channel separately, and ground truth labels are provided by the IBM. We use 5-fold cross validation to determine the hyperparameters. Feature extraction is performed at the T-F unit level in the way described below. After obtaining a binary mask, i.e., estimated IBM, from trained SVM classifiers, the target speech is segregated from the sound mixture in a resynthesis step <ref type="bibr" target="#b35">[36]</ref>. Note that we do not perform auditory segmentation, which is usually done for better segregation <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b19">[20]</ref>, as we want to directly compare the unit labeling performance of each feature type. Auditory segmentation refers to a stage of processing that breaks the auditory scene into contiguous T-F regions each of which contains acoustic energy mainly from a single sound source. Acoustic features are usually derived at the frame level. But since a binary decision needs to be made for each T-F unit, we need to find an appropriate representation for each T-F unit (recall that each T-F unit contains a slice of a subband signal). This can be done in a straightforward way as follows. To get acoustic features for the T-F unit in channel and at frame , we take the filtered output in channel . Treating as the input, conventional frame-level acoustic feature extraction is carried out and the feature vector at frame is taken as the feature representation for . The unit level features derived this way obviously contain redudancy, as the subband signals are limited to the bandwidth of the corresponding gammatone filters. Nevertheless, such redundancy does no harm to classification in our experiments. We also proposed a method to reduce the dimensionality for unit level features, which derives different acoustic features based on bandlimited spectral features. Interested readers are referred to our technical report <ref type="bibr" target="#b37">[38]</ref>. Fig. <ref type="figure" target="#fig_0">1</ref> illustrates how to derive a 12th order RASTA-PLP feature vector (including zeroth cepstral coefficient) for the T-F unit in channel 20 and at frame 50.</p><p>In the following, we describe the features used in our experiments. These features have been successfully used in many speech processing tasks. We use the RASTAMAT toolbox <ref type="bibr" target="#b7">[8]</ref> for extracting MFCC, PLP, and RASTA-PLP features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Amplitude Modulation Spectrogram</head><p>AMS features have been applied to speech segregation problems recently <ref type="bibr" target="#b22">[23]</ref>. To extract AMS features, we extract the envelope of the mixture signal by full-wave rectification and decimate it by a factor of 4. The decimated envelope is Hanning windowed and zero-padded for a 256-point FFT. The resulted FFT magnitudes are integrated by 15 triangular windows uniformly spaced from 15.6 to 400 Hz, producing a 15-D AMS feature vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Perceptual Linear Prediction</head><p>PLP <ref type="bibr" target="#b11">[12]</ref> is a popular representation in speech recognition, and it is designed to find smooth spectra consisting of resonant peaks. To derive PLPs, we first warp the power spectrum to a 20-channel Bark scale using trapezoidal filters. Then, equal loudness preemphasis is applied, followed by applying an intensity loudness law. Finally, cepstral coefficients from linear predictions form the PLP features. Following common practice in speech recognition, we use a 12th order linear prediction model, yielding 13-D (including zeroth cepstral coefficient) PLP features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Relative Spectral Transform-PLP</head><p>RASTA filtering <ref type="bibr" target="#b12">[13]</ref> is often coupled with PLP for robust speech recognition. In our experiments, we use a log-RASTA filtering approach. After the power spectrum is warped to the Bark scale, we -compress the resulted auditory spectrum, filter it by the RASTA filter (single pole at 0.94), and expand it again by an exponential function. Subsequently, PLP analysis is taken on this filtered spectrum. In essence, RASTA filtering serves as a modulation-frequency bandpass filter, which emphasizes the modulation frequency range most relevant to speech while discarding lower or higher modulation frequencies. Same as PLP, we use 13-D RASTA-PLP in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Gammatone Frequency Cepstral Coefficient</head><p>To get GFCC features <ref type="bibr" target="#b30">[31]</ref>, a signal is decomposed by a 64-channel gammatone filterbank first. Then, we decimate a filter response to an effective sampling rate of 100 Hz, resulting in a 10-ms frame shift. The magnitudes of the decimated filter outputs are then loudness-compressed by a cubic root operation. Finally, discrete cosine transform (DCT) is applied to the compressed signal to yield GFCC. As suggested in <ref type="bibr" target="#b29">[30]</ref>, we use 31-D GFCC in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Mel-Frequency Cepstral Coefficient</head><p>We follow the standard procedure to get MFCC. The signal is first preemphasized, followed by a 512-point short-time Fourier transform with a 20-ms Hamming window to get its power spectrogram. The power spectra are then warped to the mel scale followed by a operation and DCT. Note that we warp the magnitudes to a 64-channel mel scale, for fair comparisons with GFCCs in which a 64-channel gammatone filterbank is used for subband analysis. We use 31-D MFCC in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Pitch-Based Features</head><p>Pitch is a primary cue for ASA. In our experiments, we use a set of pitch-based features originally proposed in <ref type="bibr" target="#b13">[14]</ref>, and its effectiveness has been confirmed in both anechoic and reverberant environments with additive noise <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b19">[20]</ref>. Although we are only concerned with nonspeech interference in this paper, it should be noted that pitch can also be effective for segregating target speech from competing speech. To get pitch-based features for the T-F unit , we first calculate the normalized autocorrelation function at each time lag , denoted by :</p><p>(</p><p>where is the frame shift and is the sampling period. The summation is over a 20-ms frame. If the signal in is voiced and dominated by the target speech, it should have a period close to the pitch period at frame . That is, given the pitch period of the target speech at frame , measures how well the signal in is consistent with the target speech.</p><p>The second and third features involve the average instantaneous frequency derived from the zero-crossing rate of . If the signal in belongs to target speech, the product of and gives a harmonic number. Hence, we set the second feature to be the nearest integer of and the third feature to be the difference between the actual value of the product and its nearest integer. These two features have complementary information to the first feature <ref type="bibr" target="#b16">[17]</ref>. The next three features are the same as the first three except that they are extracted from the envelopes of filter responses. The envelopes are calculated by using a low-pass FIR filter with passband and a Kaiser window of 18.25 ms. The resulting 6-D feature vector is: <ref type="bibr" target="#b1">(2)</ref> where denotes the round operation, and subscript indicates envelope. It should be noted that pitch exists only in voiced speech. In this study, classifiers are trained on ground truth pitch extracted from clean speech by PRAAT <ref type="bibr" target="#b2">[3]</ref>, but tested on pitch estimated by a recently proposed multipitch tracker <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. FEATURE COMBINATION: A GROUP LASSO APPROACH</head><p>Different acoustic features characterize different properties of the speech signal. As observed in speech recognition, feature combination may lead to significant performance improvement <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b41">[42]</ref>. Here, feature combination is usually done in three ways. The simplest method is to directly try different combinations. The exponential number of possibilities renders this method unrealistic when the number of features is large. The second way is to perform unsupervised feature transformation such as kernel-PCA <ref type="bibr" target="#b31">[32]</ref> on the concatenated feature vector. The third way is to apply supervised feature transformation such as linear discriminant analysis (LDA) <ref type="bibr" target="#b8">[9]</ref> to the concatenated feature vector. However, an issue with feature transformation relates to complementarity; i.e., it is unclear which feature types are complementary after transformation. Here, by complementarity, we mean that each feature type provides complementary information to boost classification and thus their combination (concatenation in paper) should outperform an individual type.</p><p>Therefore, our goal is to find a principled way to select a set of complementary features, and such complementarity should be related to the discrimination of target-dominance and interference-dominance. This problem can be cast as a group variable selection problem, which is to find important groups of explanatory factors for prediction in the regression framework.</p><p>Group Lasso <ref type="bibr" target="#b40">[41]</ref>, a generalization of the widely used Lasso operator <ref type="bibr" target="#b33">[34]</ref>, is designed to tackle this problem by incorporating a mixed-norm regularization over regression coefficients. Since our labels are binary, we use the logistic regression extension of group Lasso <ref type="bibr" target="#b24">[25]</ref>, which can be efficiently solved by block coordinate gradient descent. The estimator is <ref type="bibr" target="#b2">(3)</ref> where is the th training sample, is the ground truth label scaled to , and is the intercept. refers to the norm. consists of predefined non-overlapping groups and is the index set of the th group. The first term in the minimization is a standard log loss that concerns discrimination. The second term is an mixed-norm regularization, which imposes an regularization between groups and an regularization within each group. It is well known that the norm induces sparsity, therefore the regularization results in group sparsity hence group level feature selection. Regularization parameter controls the level of sparsity of the resulting model. In practice, we usually calculate first, above which is very close to zero. We then use with as in (3) for the ease of choosing appropriate parameter values.</p><p>To do feature combination, all the features are concatenated together to form a long feature vector, and each feature type is defined as a group; e.g., AMS (all 15 feature elements) is defined as the first group, PLP as the second, and so on. Then, for a fixed (hence ), we solve (3) to get . Since group sparsity is induced, shall be zeros (or small numbers) for some groups , meaning that these groups (feature types) contribute little to discrimination in the presence of the other groups. Groups shall be selected if the magnitudes of their regression coefficients are greater than zero. Since (3) is solved at each channel separately, different types of features may get selected for different channels. A subband SVM classifier is then trained on the selected features and a cross-validation accuracy is obtained. To select a "global" set of complementary features, we average the cross-validation accuracies and corresponding regression coefficients across frequency channels. Features having significant average responses or peaks are considered to be complementary for the particular choice of . This is done for varying from 0 to 1 with the step size of 0.05. To achieve a good trade-off between discrimination power and model complexity which is the number of groups selected, we empirically determine the final combination by leveraging the averaged cross-validation accuracies with the corresponding model complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EVALUATION RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Setup</head><p>We use the IEEE corpus <ref type="bibr" target="#b17">[18]</ref> for most of our evaluations. All utterances are downsampled to 16 kHz. For training, we mix 50 utterances recorded by a female talker with three types of noise at 0 dB. The three noises are: N1-bird chirps, N2-crow noise, and N3-cocktail party noise <ref type="bibr" target="#b13">[14]</ref>. We choose 20 new utterances from the IEEE corpus for testing. The test utterances are different from those in training. Unless stated otherwise, test utterances from the same female talker are used, i.e., a speaker-dependent setting. This enables us to directly compare with <ref type="bibr" target="#b22">[23]</ref> where the same speaker is used in training and testing. Relaxing speaker dependency is examined in Section IV-I. Two test conditions are employed. In the matched-noise condition, we mix the test utterances with different cuts from the trained noises (i.e., N1-N3) in order to test the performance on unseen utterances. In the unmatched-noise condition, the test utterances are mixed with three unseen noises: N4-crowd noise at a playground, N5-electric fan noise, and N6-traffic noise. The test mixtures are all mixed at 0 dB except in Section IV-H. There are approximately 800 seconds of mixtures for training in most of the experiments. The experiments in Section IV-G use longer training data as the number of training utterances is increased. For testing, there are approximately 650 seconds of mixtures for the IEEE test set and 700 seconds for the TIMIT test set (see Section IV-I). The number of T-F units to be classified is about for the IEEE test set and for the TIMIT test set. The dimensionality of each feature is described in Section II. As mentioned before, for the pitch-based features, ground truth pitch and estimated pitch are used in training and testing, respectively. We use PITCH to denote the 6-D pitch-based features.</p><p>To put the performance of our classification-based segregation in perspective, we include results from a recent CASA system, the tandem algorithm <ref type="bibr" target="#b16">[17]</ref>, which jointly performs voiced speech segregation and pitch estimation in an iterative fashion. The tandem algorithm is initialized by the same estimated pitch from <ref type="bibr" target="#b20">[21]</ref>. We use ideal sequential grouping for the tandem algorithm, because the algorithm does not deal with the issue of sequential grouping, i.e., it does not have a way to group pitch contours (and their associated masks) of the same speaker across time to form a segregated sentence. So these results represent the ceiling performance of the tandem algorithm.</p><p>Aside from the tandem algorithm which tries to estimate the IBM explicitly, we focus on comparisons between different features under the same framework. Comparisons with fundamentally different techniques are not included in this study which is about feature exploration for classification-based speech separation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation Criteria</head><p>Since the task is classification, it is straightforward to measure the performance using classification accuracy. However, simply using accuracy as the evaluation criterion may not be appropriate, as miss and false-alarm errors are treated equally. Speech intelligibility studies <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref> have shown that falsealarm (FA) errors are far more detrimental to human speech intelligibility than miss errors. Kim et al. have thus proposed the HIT-FA rate as an evaluation criterion, and shown that this rate is well correlated to intelligibility <ref type="bibr" target="#b23">[24]</ref>. The HIT rate is the percent of correctly classified target dominant T-F units in the IBM. The FA rate is the percent of wrongly classified interference-dominant T-F units in the IBM. Therefore, we use HIT-FA as our main evaluation criterion. Another criterion is the IBM-modulated SNR of the segregated speech. When computing SNRs, the target speech resynthesized from the IBM is used as the ground truth signal <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b16">[17]</ref>, as the IBM represents the ground truth of classification. This IBM-modulated SNR complements the above classification-based criteria by taking into account the underlying signal energy of each T-F unit.</p><p>We should note that other evaluation criteria have been developed in the speech separation community, including SNR and source to distortion ratio (SDR). Unlike the IBM which is directly motivated by the auditory masking phenomenon, SNR and SDR do not take into consideration perceptual effects. Also, it is well known that SNR may not correlate to speech intelligibility and the relationship between SDR and speech intelligibility is still unknown. Because of its correlation with speech intelligibility, we prefer the HIT-FA rate over SNR and SDR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Single Features</head><p>In terms of HIT-FA, we document unit labeling performance at three levels: voiced speech intervals (pitched frames), unvoiced speech intervals (unpitched frames), and overall. Voiced/unvoiced speech intervals are determined by ground truth pitch. Both classification accuracy and SNR are evaluated at the overall level. Table <ref type="table" target="#tab_0">I</ref> gives the results in the matched-noise test condition. In this condition, all features are able to maintain a low FA rate. The performance differences mainly stem from the HIT rate. Clearly, AMS does not perform well compared with the other features as it fails to label a lot of target-dominant units. In contrast, GFCC manages to achieve high HIT rates, with 79% overall HIT-FA, which is significantly better than other single features. The classification accuracy and SNR using GFCC are also significantly higher than those obtained by the other features (except MFCC in terms of SNR). Unvoiced speech is important to speech intelligibility, and its segregation is a difficult task due to the lack of harmonicity and weak energy <ref type="bibr" target="#b15">[16]</ref>. Again, AMS performs the worst whereas GFCC does a very good job at segregating unvoiced speech. The good performance of GFCC is probably due to its effectiveness as a speaker identification feature <ref type="bibr" target="#b30">[31]</ref>. An encouraging observation in the matched-noise condition is that some general acoustic features such as GFCC and MFCC significantly outperform PITCH even in voiced intervals. This remains true even when ground truth pitch is used in (2), which achieves 72% HIT-FA in voiced intervals. Similarly, the tandem algorithm, which includes auditory segmentation, is not competitive. For systematic comparison, we have produced the receiver operating characteristic (ROC) curves for overall classification obtained by using single features, and interested readers are referred to our technical report <ref type="bibr" target="#b37">[38]</ref>.</p><p>Unlike the matched-noise condition, the unseen broadband noises are more demanding for generalization. The segregation results in the unmatched-noise condition are listed in Table <ref type="table" target="#tab_0">II</ref>. We can see that the classification accuracy and both HIT rate and FA rate are affected, and the main degradation comes from substantially increased FA rates. Contrary to the other features, PITCH is the least affected feature type with only 5% reduction in HIT-FA. Using ground truth pitch it is able to achieve 68% HIT-FA in voiced intervals. As the pitch-based features reflect intrinsic properties of speech, we do not expect that the change of interference will dramatically change pitch characteristics in target-dominant T-F units. Similarly, the tandem algorithm obtains a fairly low FA rate and achieves the best HIT-FA result in voiced intervals in this condition. Among others, it is interesting to see that RASTA-PLP becomes the best performing feature type in terms of all three criteria. As shown in <ref type="bibr" target="#b12">[13]</ref>, RASTA-PLP effectively acts as a modulation-frequency filter, which retains slow modulations corresponding to speech.</p><p>We have used Student's -tests at a 5% significance level to examine if an improvement is statistically significant. We use the symbol " " to denote that a result is significantly better than the previously studied AMS feature. As can be seen in Tables <ref type="table" target="#tab_0">I</ref> and<ref type="table" target="#tab_0">II</ref>, almost all the improvements are statistically significant. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Combining With Pitch-Based Features</head><p>Considering the excellent performance of some features in the matched-noise condition and the robustness of the pitchbased features in the unmatched-noise condition, it seems sensible to combine the single features with the pitch-based features. If the pitch tracker dose not detect pitch in a frame, we simply set pitch-based features to all zeros in the combination. Fig. <ref type="figure" target="#fig_1">2(a)</ref> shows the overall HIT-FA results for pairwise combinations in the matched-noise condition. Due to pitch estimation errors, the combination does not improve the performance in this test condition. However, it can be seen that the combination using the ideal (ground-truth) pitch significantly improves the performance for all the features. Results for the unmatchednoise condition are listed in Fig. <ref type="figure" target="#fig_1">2(b)</ref>. Even with estimated pitch, the performance of all the features is significantly boosted by the combination, demonstrating the role of the pitch-based features in generalization to unseen noises. As before, RASTA-PLP leads the overall performance in this combination. We note here that all the improvements are statistically significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Adding Delta Features</head><p>Difference features, also known as delta features, are found to be useful in speech processing as they capture variations. We now investigate the effects of including delta features. A positive effect of adding delta features with AMS has been shown in <ref type="bibr" target="#b22">[23]</ref>. Fig. <ref type="figure" target="#fig_2">3</ref> shows the overall HIT-FA results by adding firstorder delta features (denoted by ) along time in matched and unmatched-noise conditions. We can clearly see improvements in both test conditions. Two observations are in order. First, adding deltas is helpful for unvoiced speech segregation (not shown). Second, all features benefit from adding deltas in the unmatched-noise condition, indicating their effect in improving generalization. We note here that all the improvements are statistically significant.</p><p>We have also experimented with adding additional deltas along frequency channel as suggested in <ref type="bibr" target="#b22">[23]</ref>. This also yields some improvements yet at the expense of added dimensionality. As a trade-off, in the next few experiments, we add deltas along frequency only for PITCH which has a low dimensionality, producing a 18-D feature representation denoted by .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Feature Combination</head><p>In this subsection, we evaluate feature combination as described in Section III. Since we want the selected features to be general, the mixtures from both IEEE female and male talkers are used to form the training data for the group Lasso. As outlined in Section III, we concatenate AMS, PLP, RASTA-PLP, MFCC, GFCC, PITCH and their deltas together and define each feature type as a group. Group Lasso feature selection is then performed   on the normalized concatenated feature vector. We empirically found that offers a good trade-off between model complexity and cross-validation accuracy. We plot the averages of the magnitudes of regression coefficients across channels in Fig. <ref type="figure" target="#fig_3">4</ref>. It is clear that AMS, RASTA-PLP, MFCC and PITCH are associated with larger regression coefficients, while the coefficients of PLP are zero in almost all channels. GFCC's contribution to model fitting is relatively weak (i.e., its regression coefficients are relatively small), making it almost redundant given AMS, RASTA-PLP, MFCC and PITCH. We set the final combined feature set to AMS RASTA PLP MFCC , resulting in a 90-D feature vector. We do not include deltas for AMS and MFCC because we found that they improve performance only slightly at the expense of nearly doubling the dimensionality. Since we have already validated the effectiveness of PITCH, we will also present comparisons with AMS RASTA PLP MFCC, which comes from the feature selection and is referred as the complementary feature set in the rest of the paper.</p><p>The segregation results of feature combination in the matched and unmatched-noise conditions are shown in Tables <ref type="table" target="#tab_1">III</ref> and<ref type="table" target="#tab_2">IV</ref>.</p><p>To show that the feature combination is not redundant, we also include results from AMS RASTA PLP, AMS MFCC, and RASTA PLP MFCC. As a comparison, we also present results using LDA for feature combination. LDA is applied to the same concatenated feature vector on which group Lasso is applied. We use the symbol " " to denote that a result is significantly better than all the other features. We can see that the complementary feature set AMS RASTA PLP MFCC performs the best (equaling , see Fig. <ref type="figure" target="#fig_2">3</ref>(a)) in the matched test condition, and is significantly better than all the other single features in the unmatched test condition (see Table <ref type="table" target="#tab_0">II</ref>). The final combined feature set generalizes well to unseen noises as shown in Table <ref type="table" target="#tab_2">IV</ref>. For reference, the final combined feature set using ground truth pitch achieves 84% and 76% HIT-FA rates in the two test conditions, respectively. LDA does not achieve comparable results in either test condition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Training Corpus Size</head><p>As mentioned in Section IV-A, our training set is created from 50 clean utterances. In the following, we examine the dependence on the number of training utterances. We retrain SVM classifiers using 20, 100, and 200 utterances mixed with the same noises N1-N3 for representative features. The overall HIT-FA results are given in Fig. <ref type="figure" target="#fig_4">5</ref>(a) and (b) for matched and unmatched-noise conditions.</p><p>In the matched-noise condition, more utterances for training enable each feature type to improve the unit labeling performance. Specifically, we obtain about 5% improvements by increasing the number of training utterances from 20 to 200, except for RASTA-PLP, which seems to saturate when 200 utterances are used. In the unmatched-noise condition, no significant performance gain is achieved beyond 50 for GFCC and the complementary feature set. However, for RASTA-PLP, a 5% gain is achieved by using 100 utterances compared to 20, and the performance seems to keep increasing with more training utterances. It is worth noting that the performance of the complementary feature set using only 20 training utterances surpasses the other features using more training utterances. In summary, there is a clear benefit of training on more utterances for the matched-noise condition, which is consistent with the results in <ref type="bibr" target="#b21">[22]</ref>; yet the performance dependence on the number of training utterances in the unmatched-noise condition is significant only for certain feature types. In future research, it would be interesting to study the performance profile using even more utterances for RASTA-PLP and the complementary feature set (which contains RASTA-PLP), especially in the unmatchednoise condition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Evaluation in Different SNR Conditions</head><p>From a practical point of view, it is interesting to know how well a model trained on a single SNR condition generalizes to different SNR conditions. To examine this question, we use the subband SVMs already trained on 0 dB mixtures described in Section IV-A to segregate the same test mixtures at 5 dB, 5 dB, and 10 dB. Tables V and VI give the overall HIT-FA and SNR results for matched and unmatched-noise conditions. All features are impacted by the input SNR mismatch. The reason for the performance degradation seems twofold. First, a change of SNR leads to a change of power spectrum distribution at the T-F unit level, leading to a deviation from training. Second, a change of SNR also leads to a change of the IBM, which becomes denser (sparser) as SNR increases (decreases). Such a change in the prior probability of unit labels presents an issue to discriminative classifiers such as SVM. This is a clear trend in the 10 dB case, in which we observe that the HIT rate decreases significantly. Relatively speaking, MFCC and RASTA-PLP hold up well, especially at the lower SNR level. Again, the inclusion of the pitch-based features clearly helps each feature type to stabilize the labeling performance. The final combined feature set significantly outperforms the other features in each SNR condition. When ground truth pitch is used, it achieves 86%, 81%, and 72% HIT-FA in the matched-noise condition, and 75%, 75%, and 68% in the unmatched-noise condition, at 5, 5 and 10 dB SNR respectively. These results are comparable to the matched-SNR scenarios. In terms of reconstruction SNR, the combined feature set consistently and significantly improves for each input SNR condition.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Generalization to Different Speakers</head><p>Previous experiments are mainly based on the IEEE female talker. We now show that the key conclusions hold for the IEEE male talker as well. The training and testing settings are the same as before, except that data from a male talker are used. Table <ref type="table" target="#tab_5">VII</ref> shows the segregation results from representative features. As in the female case, GFCC is good as a single feature, PITCH is effective for generalization, and combined features are better than single features.</p><p>To further test generalization to different speakers, we create a new test set for each gender by mixing 20 utterances from the TIMIT corpus <ref type="bibr" target="#b9">[10]</ref> with N1-N6 at 0 dB. The new test utterances are chosen from 10 different TIMIT speakers of the same gender, each providing 2 utterances. We use the models previously trained on the IEEE corpus for each gender on the new test set without change. The results of representative features for unseen female and male talkers are shown in Tables <ref type="table" target="#tab_6">VIII</ref> and<ref type="table" target="#tab_7">IX</ref>, respectively. The classification performance is expected to degrade when tested on unseen speakers, as is evident from the   performance of single features. Adding PITCH clearly helps. The feature combinations are more robust than single features, and the final combined feature set performs reasonably well compared to the matched-speaker case for both genders.</p><p>Our preliminary results on cross-gender generalization show that all the above features perform worse, presumably due to significant deviations of spectro-temporal distributions between the two genders. Two methods can be used to deal with the cross-gender issue. First, one can first identify the gender of the target speech and then use gender-dependent classifiers. Gender identification can be achieved with high accuracy <ref type="bibr" target="#b39">[40]</ref>. Second, one can train classifiers by including the multiple speakers of both genders into the training set. We show the results of using the second method by training a classifier on the IEEE female and male talkers and test on mixtures from both. Fig. <ref type="figure" target="#fig_5">6</ref> shows the overall HIT-FA results, and the performance of the multi-speaker classifier is nearly as good as that of using corresponding speaker-dependent classifiers. These results indicate that the selected features perform well across different speakers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. DISCUSSION</head><p>Since different subbands in a gammatone filterbank are not independent, it is reasonable to use frame-level features directly in training subband classifiers (see <ref type="bibr" target="#b38">[39]</ref>), rather than using T-F unit level features as done in this paper. We have tried such training using conventional frame-level features. We have opted for using T-F unit level features mainly because our experiments show that, although frame-level features produce comparable performance in matched-noise conditions, the performance is significantly worse than unit-level features in unmatched test conditions. Frame-level features, such as GFCC, may be more susceptible to local distortions in a few subbands than unit-level features, as suggested in robust automatic speech recognition (ASR) <ref type="bibr" target="#b32">[33]</ref>. Also, features such as pitch-based ones are defined at the T-F unit level, which may create issues for feature combination if other features are derived at the frame level. Nevertheless, it is an interesting question if one can extract unit-level features directly from frame-level ones; if so, feature extraction could be significantly sped up. It may be easy for some features such as energy, but it is unclear how this could be done for cepstral features.</p><p>Formulating monaural speech segregation as binary classification has been shown as an effective approach in both speech segregation and robust ASR domains. Nevertheless, only pitch and AMS have been employed as primary T-F unit level features so far. In this paper, we have significantly expanded the unit level feature repository to include features commonly used in speech and speaker processing. For both voiced and unvoiced speech segregation, these newly included features have achieved significant improvements in terms of SNR as well as HIT-FA, a criterion that is well correlated with human speech intelligibility. In terms of single features, GFCC shows excellent performance in the matched-noise test condition, and RASTA-PLP in the unmatched conditions.</p><p>The complementarity among these features is systematically exploited by using a group Lasso approach, which selects a compact set of important feature types contributing to target and interference discrimination. The complementary feature set AMS RASTA PLP MFCC has shown stable performance in various test conditions and outperforms each of its components significantly.</p><p>Generalization is a critical issue for classification-based speech segregation. We have examined the generalization performance of each feature type in several unmatched conditions. These results point to the robustness of the pitch-based features, which are parameterized by estimated pitch. Pitch-based features have also been shown to generalize well to reverberant conditions in classification-based segregation <ref type="bibr" target="#b19">[20]</ref>. Nevertheless, the pitch-based features need to be combined with general acoustic features in order to segregate unvoiced speech and improve voiced speech segregation. The final combined feature set achieves promising segregation results in various test conditions. We plan to address reverberant speech segregation in future work using this combined feature set.</p><p>In addition to pitch, our results suggest that RASTA filtering also plays an important role in good generalization. RASTA filtering effectively captures low modulation frequencies corresponding to speech. The inclusion of this speech property sig-nificantly reduces FA rates, which degrade significantly in unmatched conditions. It would be interesting to explore new features that characterize both pitch and low modulation frequencies in future research.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Illustration of deriving RASTA-PLP features for the T-F unit in channel 20 and at frame 50 .</figDesc><graphic coords="2,310.02,64.14,234.00,160.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Overall HIT-FA performance for pairwise combination of single features and pitch-based features in (a) the matched-noise condition, and (b) the unmatched-noise condition. (a) Matched-noise condition. (b) Unmatched-noise condition.</figDesc><graphic coords="6,58.98,65.10,208.98,351.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Effects of delta features on overall HIT-FA performance in (a) the matched-noise condition, and (b) the unmatched-noise condition. (a) Matched-noise condition. (b) Unmatched-noise condition.</figDesc><graphic coords="6,325.98,64.14,204.12,342.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Averages of the magnitudes of regression coefficients across channels, where R-PLP stands for RASTA-PLP.</figDesc><graphic coords="7,42.00,377.16,246.14,84.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Overall HIT-FA rates of representative features as a function of the number of training utterances. "COMP" stands for the complementary feature set AMS RASTA PLP MFCC (a) Matched-noise condition. (b) Unmatchednoise condition.</figDesc><graphic coords="8,37.98,64.14,252.17,105.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Overall HIT-FA comparisons between speaker-dependent and multispeaker classifiers on the IEEE corpus.</figDesc><graphic coords="9,63.00,333.12,205.25,163.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I SEGREGATION</head><label>I</label><figDesc>PERFORMANCE FOR SINGLE FEATURES IN THE MATCHED-NOISE CONDITION. BOLDFACE INDICATES BEST RESULT. " " INDICATES THE RESULT IS SIGNIFICANTLY BETTER THAN AMS AT A 5% SIGNIFICANCE LEVEL TABLE II SEGREGATION PERFORMANCE FOR SINGLE FEATURES IN THE UNMATCHED-NOISE CONDITION</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE III SEGREGATION</head><label>III</label><figDesc>PERFORMANCE FOR FEATURE COMBINATION IN THE MATCHED-NOISE CONDITION. " " INDICATES THAT THE RESULT IS SIGNIFICANTLY BETTER THAN ALL THE OTHER FEATURES AT A 5% SIGNIFICANCE LEVEL</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE IV SEGREGATION</head><label>IV</label><figDesc>PERFORMANCE FOR FEATURE COMBINATION IN THE UNMATCHED-NOISE CONDITION</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE V SEGREGATION</head><label>V</label><figDesc>PERFORMANCE IN THE MATCHED-NOISE CONDITION WHEN TESTED ON DIFFERENT SNR CONDITIONS</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE VI SEGREGATION</head><label>VI</label><figDesc>PERFORMANCE IN THE UNMATCHED-NOISE CONDITION WHEN TESTED ON DIFFERENT SNR CONDITIONS</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VII SEGREGATION</head><label>VII</label><figDesc>PERFORMANCE ON THE IEEE MALE TALKER</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VIII SEGREGATION</head><label>VIII</label><figDesc>PERFORMANCE WHEN TESTED ON TIMIT FEMALE SPEAKERS</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE IX SEGREGATION</head><label>IX</label><figDesc>PERFORMANCE WHEN TESTED ON TIMIT MALE SPEAKERS</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank Z. Jin for providing his pitch tracking code.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the Air Force Office of Scientific Research (AFOSR) under Grant FA9550-08-1-0155 and in part by an STTR grant from the AFOSR. The associate editor coordinating the review of this manuscript and approving it for publication was Prof. Bryan</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Articulation and Intelligibility</title>
		<author>
			<persName><forename type="first">J</forename><surname>Allen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Morgan &amp; Claypool</publisher>
			<pubPlace>San Rafael, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Auditory Scene Analysis: The Perceptual Organization of Sound</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Bregman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Praat: Doing Phonetics by Computer</title>
		<author>
			<persName><forename type="first">P</forename><surname>Boersma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weenink</surname></persName>
		</author>
		<ptr target="http://www.fon.hum.uva.nl/praat" />
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note>Version 4.3.14</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Suppression of acoustic noise in speech using spectral subtraction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Boll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech, Lang. Process</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="113" to="120" />
			<date type="published" when="1979-04">Apr. 1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Isolating the energetic component of speech-on-speech masking with ideal time-frequency segregation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Brungart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="page" from="4007" to="4018" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">New insights into the noise reduction Wiener filter</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Benesty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Doclo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech, Lang. Process</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1218" to="1234" />
			<date type="published" when="2006-08">Aug. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Dillon</surname></persName>
		</author>
		<title level="m">Hearing Aids</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Thieme</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">PLP and RASTA (and MFCC, and Inversion) in Matlab</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ellis</surname></persName>
		</author>
		<ptr target="http://www.ee.columbia.edu/dpwe/re-sources/matlab/rastamat/" />
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Combining spectral representations for large-vocabulary continuous speech recognition</title>
		<author>
			<persName><forename type="first">G</forename><surname>Garau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Renals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech, Lang. Process</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="508" to="518" />
			<date type="published" when="2008-03">Mar. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">DARPA TIMIT Acoustic-Phonetic Continuous Speech Corpus</title>
		<author>
			<persName><forename type="first">J</forename><surname>Garofolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIST</title>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An SVM based classification approach to speech separation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="5212" to="5215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Perceptual linear predictive (PLP) analysis of speech</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hermansky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1738" to="1752" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">RASTA processing of speech</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hermansky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Morgan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech, Lang. Process</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="578" to="589" />
			<date type="published" when="1994-10">Oct. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Monaural speech organization and segregation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biophysics Program</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<pubPlace>Columbus, OH</pubPlace>
		</imprint>
		<respStmt>
			<orgName>The Ohio State Univ.</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Monaural speech segregation based on pitch tracking and amplitude modulation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1135" to="1150" />
			<date type="published" when="2004-09">Sep. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Segregation of unvoiced speech from nonspeech interference</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="page" from="1306" to="1319" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A tandem algorithm for pitch estimation and voiced speech segregation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech, Lang. Process</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2067" to="2079" />
			<date type="published" when="2010-11">Nov. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">IEEE recommended practice for speech quality measurements</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio Electroacoust</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="225" to="246" />
			<date type="published" when="1969-09">Sep. 1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A maximum likelihood approach to single-channel source separation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn Res</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1365" to="1392" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A supervised learning approach to monaural segregation of reverberant speech</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech, Lang. Process</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="625" to="638" />
			<date type="published" when="2009-05">May 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">HMM-based multipitch tracking for noisy and reverberant speech</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech, Lang. Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1091" to="1102" />
			<date type="published" when="2011-07">Jul. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Improving speech intelligibility in noise using environment-optimized algorithms</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Loizou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech, Lang. Process</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2080" to="2090" />
			<date type="published" when="2010-11">Nov. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An algorithm that improves speech intelligibility in noise for normal-hearing listeners</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Loizou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="page" from="1486" to="1494" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Factors influencing intelligibility of ideal binary-masked speech: Implications for noise reduction</title>
		<author>
			<persName><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Loizou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1673" to="1682" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The group Lasso for logistic regression</title>
		<author>
			<persName><forename type="first">L</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V D</forename><surname>Geer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bühlmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc. Series B</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="71" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An efficient auditory filterbank based on the gammatone function</title>
		<author>
			<persName><forename type="first">R</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Nimmo-Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Holdsworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">APU Report</title>
		<imprint>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">One microphone source separation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Roweis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="page" from="793" to="799" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Single-channel speech separation using sparse non-negative matrix factorization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Olsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICSLP</title>
		<meeting>ICSLP</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A Bayesian classifier for spectrographic mask estimation for missing feature speech recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Seltzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Stern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Commun</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="379" to="393" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">An auditory-based feature for robust speech recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="4625" to="4628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Robust speaker identification using auditory features and computational auditory scene analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1589" to="1592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Robust feature extraction using kernel PCA</title>
		<author>
			<persName><forename type="first">T</forename><surname>Takiguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ariki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="509" to="512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sub-band based recognition of noisy speech</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tibrewala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hermansky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="1255" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Regression shrinkage and selection via the lasso</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc. Series B</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="267" to="288" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">On ideal binary mask as the computational goal of auditory scene analysis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech Separation by Humans and Machines</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Divenyi</surname></persName>
		</editor>
		<meeting><address><addrLine>Norwell, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Kluwer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="181" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m">Computational Auditory Scene Analysis: Principles, Algorithms and Applications</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Brown</surname></persName>
		</editor>
		<meeting><address><addrLine>Hoboken, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley-IEEE Press</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Speech intelligibility in background noise with ideal binary time-frequency masking</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Kjems</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pedersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Boldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lunner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="page" from="2336" to="2347" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Exploring monaural features for classification-based speech segregation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dept. of CSE</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
		<respStmt>
			<orgName>Ohio State Univ</orgName>
		</respStmt>
	</monogr>
	<note>Tech. Rep. TR37</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Estimating single-channel source separation masks: Relevance vector machine classifiers vs. pitch-based masking</title>
		<author>
			<persName><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ellis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Workshop Statist</title>
		<meeting>Workshop Statist</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note>Percept. Audition</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Gender recognition from speech. Part I: Coarse analysis</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Childers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1828" to="1840" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Model selection and estimation in regression with grouped variables</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc. Series B</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="67" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Using multiple acoustic feature sets for speech recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zolnay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kocharov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Commun</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="514" to="525" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
