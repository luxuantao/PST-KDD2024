<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Image Captioning: Transforming Objects into Words</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Simao</forename><surname>Herdade</surname></persName>
							<email>sherdade@verizonmedia.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Yahoo Research San Francisco</orgName>
								<address>
									<postCode>94103</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Armin</forename><surname>Kappeler</surname></persName>
							<email>akappeler@apple.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Yahoo Research San Francisco</orgName>
								<address>
									<postCode>94103</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kofi</forename><surname>Boakye</surname></persName>
							<email>kaboakye@verizonmedia.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Yahoo Research San Francisco</orgName>
								<address>
									<postCode>94103</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Joao</forename><surname>Soares</surname></persName>
							<email>jvbsoares@verizonmedia.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Yahoo Research San Francisco</orgName>
								<address>
									<postCode>94103</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Image Captioning: Transforming Objects into Words</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">693AFF0A1056A2BEB600197B23E72AA7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Image captioning models typically follow an encoder-decoder architecture which uses abstract image feature vectors as input to the encoder. One of the most successful algorithms uses feature vectors extracted from the region proposals obtained from an object detector. In this work we introduce the Object Relation Transformer, that builds upon this approach by explicitly incorporating information about the spatial relationship between input detected objects through geometric attention. Quantitative and qualitative results demonstrate the importance of such geometric attention for image captioning, leading to improvements on all common captioning metrics on the MS-COCO dataset. Code is available at https:// github.com/yahoo/object_relation_transformer.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Image captioning-the task of providing a natural language description of the content within an image-lies at the intersection of computer vision and natural language processing. As both of these research areas are highly active and have experienced many recent advances, progress in image captioning has naturally followed suit. On the computer vision side, improved convolutional neural network and object detection architectures have contributed to improved image captioning systems. On the natural language processing side, more sophisticated sequential models, such as attention-based recurrent neural networks, have similarly resulted in more accurate caption generation.</p><p>Inspired by neural machine translation, most conventional image captioning systems utilize an encoder-decoder framework, in which an input image is encoded into an intermediate representation of the information contained within the image, and subsequently decoded into a descriptive text sequence. This encoding can consist of a single feature vector output of a CNN (as in <ref type="bibr" target="#b24">[25]</ref>), or multiple visual features obtained from different regions within the image. In the latter case, the regions can be uniformly sampled (e.g., <ref type="bibr" target="#b25">[26]</ref>), or guided by an object detector (e.g., <ref type="bibr" target="#b1">[2]</ref>) which has been shown to yield improved performance.</p><p>While these detection based encoders represent the state-of-the art, at present they do not utilize information about the spatial relationships between the detected objects, such as relative position and size. This information can often be critical to understanding the content within an image, however, and is used by humans when reasoning about the physical world. Relative position, for example, can aid in distinguishing "a girl riding a horse" from "a girl standing beside a horse". Similarly, relative size can help differentiate between "a woman playing the guitar" and "a woman playing the ukelele". Incorporating spatial relationships has been shown to improve the performance of object detection itself, as demonstrated in <ref type="bibr" target="#b8">[9]</ref>. Furthermore, in machine translation encoders, positional relationships are often encoded, in particular in the case of the Transformer <ref type="bibr" target="#b22">[23]</ref>, an attention-based encoder architecture. The use of relative positions and sizes of detected objects, then, should be of benefit to image captioning visual encoders as well, as evidenced in Figure <ref type="figure" target="#fig_0">1</ref>.  In this work, we propose and demonstrate the use of object spatial relationship modeling for image captioning, specifically within the Transformer encoder-decoder architecture. This is achieved by incorporating the object relation module of <ref type="bibr" target="#b8">[9]</ref> within the Transformer encoder. The contributions of this paper are as follows:</p><p>• We introduce the Object Relation Transformer, an encoder-decoder architecture designed specifically for image captioning, that incorporates information about the spatial relationships between input detected objects through geometric attention.</p><p>• We quantitatively demonstrate the usefulness of geometric attention through both baseline comparison and an ablation study on the MS-COCO dataset.</p><p>• Lastly, we qualitatively show that geometric attention can result in improved captions that demonstrate enhanced spatial awareness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Many early neural models for image captioning <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b24">25]</ref> encoded visual information using a single feature vector representing the image as a whole, and hence did not utilize information about objects and their spatial relationships. Karpathy and Fei-Fei in <ref type="bibr" target="#b10">[11]</ref>, as a notable exception to this global representation approach, extracted features from multiple image regions based on an R-CNN object detector <ref type="bibr" target="#b6">[7]</ref> and generated separate captions for the regions. As a separate caption was generated for each region, however, the spatial relationship between the detected objects was not modeled. This is also true of their follow-on dense captioning work <ref type="bibr" target="#b9">[10]</ref>, which presented an end-to-end approach for obtaining captions relating to different regions within an image. Fang et al. in <ref type="bibr" target="#b5">[6]</ref> generated image descriptions by first detecting words associated with different regions within the image. The spatial association was made by applying a fully convolutional neural network to the image and generating spatial response maps for the target words. Here again, the authors did not explicitly model any relationships between the spatial regions.</p><p>A family of attention based approaches <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b27">28]</ref> to image captioning have also been proposed that seek to ground the words in the predicted caption to regions in the image. As the visual attention is often derived from higher convolutional layers of a CNN, the spatial localization is limited and often not semantically meaningful. Most similar to our work, Anderson et al. in <ref type="bibr" target="#b1">[2]</ref> addressed this limitation of typical attention models by combining a "bottom-up" attention model with a "top-down" LSTM. The bottom-up attention acts on mean-pooled convolutional features obtained from the proposed regions of interest of a Faster R-CNN object detector <ref type="bibr" target="#b19">[20]</ref>. The top-down LSTM is a two-layer LSTM in which the first layer acts as a visual attention model that attends to the relevant detections for the current token and the second layers is a language LSTM that generates the next token. The authors demonstrated state-of-the-art performance for both visual question answering and image captioning using this approach, indicating the benefits of combining features derived from object detection with visual attention. Again, spatial information-which we propose in this work via geometric attention-was not utilized. Geometric attention was first introduced by Hu et al. for object detection in <ref type="bibr" target="#b8">[9]</ref>. There, the authors used bounding box coordinates and sizes to infer the importance of the relationship of pairs of objects, the assumption being that if two bounding boxes are closer and more similar in size to each other, then their relationship is stronger.</p><p>The most successful subsequent work followed the above paradigm of obtaining image features with an object detector, and generating captions through an attention LSTM. As a way of adding global context, Yao et al. in <ref type="bibr" target="#b28">[29]</ref> introduced two Graph Convolutional Networks: a semantic relationship graph, and a spatial relationship graph that classifies the relationship between two boxes into 11 classes, such as "inside", "cover", or "overlap". In contrast, our approach directly utilizes the size ratio and difference of the bounding box coordinates, implicitly encoding and generalizing the aforementioned relationships. Yang et al. in <ref type="bibr" target="#b26">[27]</ref> similarly leveraged graph structures, extracting object image features into an image scene graph. In addition, they used a semantic scene graph (i.e., a graph of objects, their relationships, and their attributes) autoencoder on caption text to embed a language inductive bias in a dictionary that is shared with the image scene graph. While this model may learn typical spatial relationships found in text, it is inherently unable to capture the visual geometry specific to a given image. The use of self-critical reinforcement learning for sentence generation <ref type="bibr" target="#b20">[21]</ref> has also proven to be important for state-of-the-art captioning approaches, such as those above. Liu et al. in <ref type="bibr" target="#b14">[15]</ref> proposed an alternative reinforcement learning approach over a visual policy that, in effect, acts as an attention mechanism to combine features from the image regions provided by an object detector. The visual policy, however, does not utilize spatial information about these image regions.</p><p>Recent developments in NLP, namely the Transformer architecture <ref type="bibr" target="#b22">[23]</ref> have led to significant performance improvements for various tasks such as translation <ref type="bibr" target="#b22">[23]</ref>, text generation <ref type="bibr" target="#b3">[4]</ref>, and language understanding <ref type="bibr" target="#b18">[19]</ref>. In <ref type="bibr" target="#b21">[22]</ref>, the Transformer was applied to the task of image captioning. The authors explored extracting a single global image feature from the image as well as uniformly sampling features by dividing the image into 8x8 partitions. In the latter case, the feature vectors were fed in a sequence to the Transformer encoder. In this paper we propose to improve upon this uniform sampling by adopting the bottom-up approach of <ref type="bibr" target="#b1">[2]</ref>. The Transformer architecture is particularly well suited as a bottom-up visual encoder for captioning since it does not have a notion of order for its inputs, unlike an RNN. It can, however, successfully model sequential data with the use of positional encoding, which we apply to the decoded tokens in the caption text. Rather than encode an order to objects, our Object Relation Transformer seeks to encode how two objects are spatially related to each other and weight them accordingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Approach</head><p>Figure <ref type="figure" target="#fig_1">2</ref> shows an overview of the proposed image captioning algorithm. We first use an object detector to extract appearance and geometry features from all the detected objects in the image, as described in Section 3.1. Thereafter, we use the Object Relation Transformer to generate the caption text. Section 3.2 describes how we use the Transformer architecture <ref type="bibr" target="#b22">[23]</ref> in general for image captioning. Section 3.3 explains our novel addition of box relational encoding to the encoder layer of the Transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Object Detection</head><p>Following <ref type="bibr" target="#b1">[2]</ref>, we use Faster R-CNN <ref type="bibr" target="#b19">[20]</ref> with ResNet-101 <ref type="bibr" target="#b7">[8]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Standard Transformer Model</head><p>The Transformer <ref type="bibr" target="#b22">[23]</ref> model consists of an encoder and a decoder, both of which are composed of a stack of layers (in our case 6). For image captioning, our architecture uses the feature vectors from the object detector as inputs and generates a sequence of words (i.e., the image caption) as outputs.</p><p>Every image feature vector is first processed through an input embedding layer, which consists of a fully-connected layer to reduce the dimension from 2048 to d model = 512 followed by a ReLU and a dropout layer. The embedded feature vectors are then used as input tokens to the first encoder layer of the Transformer model. We denote x n as the n-th token of a set of N tokens. For encoder layers 2 to 6, we use the output tokens of the previous encoder layer as the input to the current layer.</p><p>Each encoder layer consists of a multi-head self-attention layer followed by a small feed-forward neural network. The self-attention layer itself consists of 8 identical heads. Each attention head first calculates the queries Q, keys K and values V for the N tokens as follows</p><formula xml:id="formula_0">Q = XW Q , K = XW K , V = XW V ,<label>(1)</label></formula><p>where X contains all the input vectors x 1 ...x N stacked into a matrix and W Q , W K , and W V are learned projection matrices.</p><p>The attention weights for the appearance features are then computed according to</p><formula xml:id="formula_1">Ω A = QK T √ d k (2)</formula><p>where Ω A is an N × N attention weight matrix, whose elements ω mn A are the attention weights between the m-th and n-th token. Following the implementation of <ref type="bibr" target="#b22">[23]</ref>, we choose a constant scaling factor of d k = 64, which is the dimension of the key, query, and value vectors. The output of the head is then calculated as</p><formula xml:id="formula_2">head(X) = self-attention(Q, K, V ) = softmax(Ω A )V<label>(3)</label></formula><p>Equations 1 to 3 are calculated for every head independently. The output of all 8 heads are then concatenated to one output vector and multiplied with a learned projection matrix W O , i.e.,</p><formula xml:id="formula_3">MultiHead(Q, K, V ) = Concat(head 1 , . . . , head h )W O<label>(4)</label></formula><p>The next component of the encoder layer is the point-wise feed-forward network (FFN), which is applied to each output of the attention layer</p><formula xml:id="formula_4">FFN(x) = max(0, xW 1 + b 1 )W 2 + b 2<label>(5)</label></formula><p>where W 1 ,b 1 and W 2 ,b 2 are the weights and biases of two fully connected layers. In addition, skip-connections and layer-norm are applied to the outputs of the self-attention and the feed-forward layer.</p><p>The decoder then uses the generated tokens from the last encoder layer as input to generate the caption text. Since the dimensions of the output tokens of the Transformer encoder are identical to the tokens used in the original Transformer implementation, we make no modifications on the decoder side. We refer the reader to the original publication <ref type="bibr" target="#b22">[23]</ref> for a detailed explanation of the decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Object Relation Transformer</head><p>In our proposed model, we incorporate relative geometry by modifying the attention weight matrix Ω A in Equation <ref type="formula">2</ref>. We multiply the appearance based attention weights ω mn A of two objects m and n, by a learned function of their relative position and size. We use the same function that was first introduced in <ref type="bibr" target="#b8">[9]</ref> to improve the classification and non-maximum suppression stages of a Faster R-CNN object detector.</p><p>First we calculate a displacement vector λ(m, n) for bounding boxes m and n from their geometry features (x m , y m , w m , h m ) and (x n , y n , w n , h n ) (center coordinates, widths, and heights) as</p><formula xml:id="formula_5">λ(m, n) = log |x m -x n | w m , log |y m -y n | h m , log w n w m , log h n h m ,<label>(6)</label></formula><p>The geometric attention weights are then calculated as</p><formula xml:id="formula_6">ω mn G = ReLU (Emb(λ)W G )<label>(7)</label></formula><p>where Emb(•) calculates a high-dimensional embedding following the functions P E pos described in <ref type="bibr" target="#b22">[23]</ref>, where sinusoid functions are computed for each value of λ(m, n). In addition, we multiply the embedding with the learned vector W G to project down to a scalar and apply the ReLU non-linearity.</p><p>The geometric attention weights ω mn G are then incorporated into the attention mechanism according to</p><formula xml:id="formula_7">ω mn = ω mn G exp(ω mn A ) N l=1 ω ml G exp(ω ml A )<label>(8)</label></formula><p>where ω mn A are the appearance based attention weights from Equation 2 and ω mn are the new combined attention weights.</p><p>The output of the head can be calculated as</p><formula xml:id="formula_8">head(X) = self-attention(Q, K, V ) = ΩV (<label>9</label></formula><formula xml:id="formula_9">)</formula><p>where Ω is the N × N matrix whose elements are given by ω mn .</p><p>The Bounding Box Relational Encoding diagram in Figure <ref type="figure" target="#fig_1">2</ref> shows the multi-head self-attention layer of the Object Relation Transformer. Equations 6 to 9 are represented with the Relation boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Implementation Details</head><p>Our algorithm was developed in PyTorch using the image captioning implementation in <ref type="bibr" target="#b15">[16]</ref> as our basis. We ran our experiments on NVIDIA Tesla V100 GPUs. Our best performing model was pre-trained for 30 epochs with a softmax cross-entropy loss using the ADAM optimizer with learning rate defined as in the original Transformer paper, with 20000 warmup steps, and a batch size of 10. We trained for an additional 30 epochs using self-critical reinforcement learning <ref type="bibr" target="#b20">[21]</ref> optimizing for CIDEr-D score, and did early-stopping for best performance on the validation set (which contains 5000 images). On a single GPU the training with cross-entropy loss and the self-critical training take about 1 day and 3.5 days, respectively. The models compared in sections 5.3-5.6 are evaluated after training for 30 epochs with standard cross-entropy loss, using ADAM optimization with the above learning rate schedule, and with batch size 15. The evaluation in those sections for the best performing models was obtained setting beam size to 2, in consistency with other research on image captioning optimization <ref type="bibr" target="#b20">[21]</ref> (appendix A).</p><p>Only in Table <ref type="table" target="#tab_1">1</ref>, for a fair comparison with other models in the literature, we present our result with the same beam size of 5 that other works have used to communicate their performance.</p><p>5 Experimental Evaluation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Dataset and Metrics</head><p>We trained and evaluated our algorithm on the Microsoft COCO (MS-COCO) 2014 Captions dataset <ref type="bibr" target="#b13">[14]</ref>. We report results on the Karpathy validation and test splits <ref type="bibr" target="#b10">[11]</ref>, which are commonly used in other image captioning publications. The dataset contains 113K training images with 5 human annotated captions for each image. The Karpathy test and validation sets contain 5K images each. We evaluate our models using the CIDEr-D <ref type="bibr" target="#b23">[24]</ref>, SPICE <ref type="bibr" target="#b0">[1]</ref>, BLEU <ref type="bibr" target="#b17">[18]</ref>, METEOR <ref type="bibr" target="#b2">[3]</ref>, and ROUGE-L <ref type="bibr" target="#b12">[13]</ref> metrics. While it has been shown experimentally that BLEU and ROUGE have lower correlation with human judgments than the other metrics <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b23">24]</ref>, the common practice in the image captioning literature is to report all the aforementioned metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Comparative Analysis</head><p>We compare our proposed algorithm against the best results from a single model<ref type="foot" target="#foot_0">1</ref> of the self-critical sequence training (Att2all) <ref type="bibr" target="#b20">[21]</ref> the Bottom-up Top-down (Up-Down) <ref type="bibr" target="#b1">[2]</ref> baseline, and the three best to date image captioning models <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b26">27]</ref>. Table <ref type="table" target="#tab_1">1</ref> shows the metrics for the test split as reported by the authors. Following the implementation of <ref type="bibr" target="#b1">[2]</ref>, we fine-tune our model using the self-critical training optimized for CIDEr-D score <ref type="bibr" target="#b20">[21]</ref> and apply beam search with beam size 5, achieving a 6.8% relative improvement over the Up-Down baseline, as well as the state-of-the-art for the captioning specific metrics CIDEr-D, SPICE, as well as METEOR, and BLEU-4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Positional Encoding</head><p>Our proposed geometric attention can be seen as a replacement for the positional encoding of the original Transformer network. While objects do not have an inherent notion of order, there do exist some simpler analogues to positional encoding, such as ordering by object size, or left-to-right or top-to-bottom based on bounding box coordinates. We provide a comparison between our geometric attention and these object orderings in Table <ref type="table" target="#tab_2">2</ref>. For box size, we simply calculate the area of each bounding box and order from largest to smallest. For left-to-right we order bounding boxes according to the x-coordinate of their centroids. Analogous ordering is performed for top-to-bottom using the centroid y-coordinate. Based on the CIDEr-D scores shown, adding such an artificial ordering to the detected objects decreases the performance. We observed similar decreases in performance across all other metrics (SPICE, BLEU, METEOR and ROUGE-L). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Ablation Study</head><p>Table <ref type="table" target="#tab_3">3</ref> shows the results for our ablation study. We show the Bottom-Up and Top-Down algorithm <ref type="bibr" target="#b1">[2]</ref> as our baseline algorithm. The second row replaces the LSTM with a Transformer network. The third row includes the proposed geometric attention. The last row includes beam search with beam size 2. The contribution of the Object Relation Transformer is small for METEOR, but significant for CIDEr-D and the BLEU metrics. Overall we can see the most improvements on the CIDEr-D and BLEU-4 scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Geometric Improvement</head><p>In order to demonstrate the advantages of the geometric attention layer, we performed a more detailed comparison of the Object Relation Transformer against the Standard Transformer. For each of the considered metrics, we performed a two-tailed t-test with paired samples in order to determine whether the difference caused by adding the geometric attention was statistically significant. The metrics were first computed for each individual image in the test set for each of the two Transformer models, so that we are able to run the paired tests. In addition to the standard evaluation metrics, we also report metrics obtained from SPICE by splitting up the tuples of the scene graphs according to different semantic subcategories. For each subcategory, we are able to compute precision, recall, and F-scores. The measures we report are the F-scores computed by taking only the tuples in each subcategory. More specifically, we report SPICE scores for: Object, Relation, Attribute, Color, Count, and Size <ref type="bibr" target="#b0">[1]</ref>. Note that for a given image, not all SPICE subcategory scores might be available. For example, if the reference captions for a given image have no mention of color, then the SPICE Color score is not defined and therefore we omit that image from that particular analysis. In spite of this, each subcategory analyzed had at least 1000 samples. For this experiment, we did not use self-critical training for either Transformer and they were both run with a beam size of 2.</p><p>The metrics computed over the 5000 images of the test set are shown in Tables <ref type="table" target="#tab_4">4</ref> and<ref type="table" target="#tab_5">5</ref>. We first note that for all of the metrics, the Object Relation Transformer presents higher scores than the Standard Transformer. The score difference was statistically significant (using a significance level α = 0.05) for CIDEr-D, BLEU-1, ROUGE-L (Table <ref type="table" target="#tab_4">4</ref>), Relation, and Count (Table <ref type="table" target="#tab_5">5</ref>). The significant improvements in CIDEr-D and Relation are in line with our expectation that adding the geometric attention layer would help the model in determining the correct relationships between objects. In addition, it is interesting to see a significant improvement in the Count subcategory of SPICE, from 11.30 to 17.51. Though image captioning methods in general show a large deficit in Count scores when compared with humans <ref type="bibr" target="#b0">[1]</ref>, we are able to show a significant improvement by adding explicit positional information. Some examples illustrating these improvements are presented in Section 5.6.   <ref type="table" target="#tab_6">6</ref> and<ref type="table" target="#tab_7">7</ref>. The images in Table <ref type="table" target="#tab_6">6</ref> illustrate an improvement in determining when a relationship between objects should be expressed, as well as in determining what that relationship should be. An example of correctly determining that a relationship should exist is shown in the third image of Table <ref type="table" target="#tab_6">6</ref>, where the two chairs are actually related to the umbrella by being underneath it. In addition, an example where the Object Relation Transformer correctly infers the type of relationship between objects is shown in the first image of Table <ref type="table" target="#tab_6">6</ref>, where the man in fact is not on the motorcycle, but is working on it. The examples in Table <ref type="table" target="#tab_7">7</ref> specifically illustrate the Object Relation Transformer's marked ability to better count objects.   In order to better understand the failure modes of our model, we manually reviewed a set of generated captions. We used our best performing model-the Object Relation Transformer trained with selfcritical reinforcement learning-with a beam size of 5 to generate captions for 100 randomly sampled images from the MS-COCO's test set. For each generated caption, we described the errors and then grouped them into distinct failure modes. An error was counted each time a term was wrong, extraneous, or missing. All errors were then tallied up, with each image being able to contribute with multiple errors. There were a total of 62 observed errors, which were grouped into 4 categories: 58% of the errors pertained to objects or things, 21% to relations, 16% to attributes, and 5% to syntax. Note that while these failure modes are very similar to the semantic subcategories from SPICE, we were not explicitly aiming to adhere to those. In addition, one general pattern that stood out were the errors in identifying rare or unusual objects. Some examples of unusual objects that were not correctly identified include: parking meter, clothing mannequin, umbrella hat, tractor, and masking tape. This issue is also noticeable, even if to a lesser degree, in rare relations and attributes. Another interesting observation was that the generated captions tend to be less descriptive and less discursive than the ground truth captions. The above results and observations can be used to help prioritize future efforts in image captioning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have presented the Object Relation Transformer, a modification of the conventional Transformer, specifically adapted to the task of image captioning. The proposed Transformer encodes 2D position and size relationships between detected objects in images, building upon the bottom-up and topdown image captioning approach. Our results on the MS-COCO dataset demonstrate that the Transformer does indeed benefit from incorporating spatial relationship information, most evidently when comparing the relevant sub-metrics of the SPICE captioning metric. We have also presented qualitative examples of how incorporating this information can yield captioning results demonstrating better spatial awareness.</p><p>At present, our model only takes into account geometric information in the encoder phase. As a next step, we intend to incorporate geometric attention in our decoder cross-attention layers between objects and words. We aim to do this by explicitly associating decoded words with object bounding boxes. This should lead to additional performance gains as well as improved interpretability of the model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: A visualization of self-attention in our proposed Object Relation Transformer. The transparency of the detected object and its bounding box is proportional to the attention weight with respect to the chair outlined in red. Our model strongly correlates this chair with the companion chair to the left, the beach beneath them, and the umbrella above them, relationships displayed in the generated caption.</figDesc><graphic coords="2,207.00,72.00,198.00,164.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overview of Object Relation Transformer architecture. The Bounding Box Relational Encoding diagram describes the changes made to the Transformer architecture</figDesc><graphic coords="2,108.00,317.44,396.00,186.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>To illustrate the advantages of the Object Relation Transformer relative to the Standard Transformer, we present example images with the corresponding captions generated by each model. The captions presented were generated using the following setup: both the Object Relation Transformer and the Standard Transformer were trained without self-critical training and both were run with a beam size of 2 on the 5000 images of the test set. We chose examples for which there were was a marked improvement in the score of the Object Relation Transformer relative to the Standard Transformer. This was done for the Relation and Count subcategories of SPICE scores. The example images and captions are presented in Tables</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Standard: a man</head><label></label><figDesc>on a motorcycle on the road a couple of bears standing on top of a rock two chairs and an umbrella on a beach a laptop computer sitting on top of a wooden desk Ours: a man is working on a motorcycle in a parking lot two brown bears standing next to each other on a rock two beach chairs under an umbrella on the beach a desk with a laptop and a keyboard</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>as the base CNN for object detection and feature extraction. Using intermediate feature maps from the ResNet-101 as inputs, a Region Proposal Network (RPN) generates bounding boxes for object proposals. Using non-maximum</figDesc><table /><note><p>suppression, overlapping bounding boxes with an intersection-over-union (IoU) exceeding a threshold of 0.7 are discarded. A region-of-interest (RoI) pooling layer is then used to convert all remaining bounding boxes to the same spatial size (e.g. 14 × 14 × 2048). Additional CNN layers are applied to predict class labels and bounding box refinements for each box proposal. We further discard all bounding boxes where the class prediction probability is below a threshold of 0.2. Finally, we apply mean-pooling over the spatial dimension to generate a 2048-dimensional feature vector for each object bounding box. These feature vectors are then used as inputs to the Transformer model.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparative analysis to existing state-of-the-art approaches. The model denoted as Ours refers to the Object Relation Transformer fine-tuned using self-critical training and generating captions using beam search with beam size 5.</figDesc><table><row><cell>Algorithm</cell><cell cols="6">CIDEr-D SPICE BLEU-1 BLEU-4 METEOR ROUGE-L</cell></row><row><cell>Att2all [21]</cell><cell>114</cell><cell>-</cell><cell>-</cell><cell>34.2</cell><cell>26.7</cell><cell>55.7</cell></row><row><cell>Up-Down [2]</cell><cell>120.1</cell><cell>21.4</cell><cell>79.8</cell><cell>36.3</cell><cell>27.7</cell><cell>56.9</cell></row><row><cell>Visual-policy[15]</cell><cell>126.3</cell><cell>21.6</cell><cell>-</cell><cell>38.6</cell><cell>28.3</cell><cell>58.5</cell></row><row><cell>GCN-LSTM [29] 1</cell><cell>127.6</cell><cell>22.0</cell><cell>80.5</cell><cell>38.2</cell><cell>28.5</cell><cell>58.3</cell></row><row><cell>SGAE [27]</cell><cell>127.8</cell><cell>22.1</cell><cell>80.8</cell><cell>38.4</cell><cell>28.4</cell><cell>58.6</cell></row><row><cell>Ours</cell><cell>128.3</cell><cell>22.6</cell><cell>80.5</cell><cell>38.6</cell><cell>28.7</cell><cell>58.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Positional Encoding Comparison (models trained with softmax cross-entropy for 30 epochs)</figDesc><table><row><cell>Positional Encoding</cell><cell>CIDEr-D</cell></row><row><cell>no encoding</cell><cell>111.0</cell></row><row><cell>positional encoding (ordered by box size)</cell><cell>108.7</cell></row><row><cell>positional encoding (ordered left-to-right)</cell><cell>110.2</cell></row><row><cell>positional encoding (ordered top-to-bottom)</cell><cell>109.1</cell></row><row><cell>geometric attention</cell><cell>112.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Ablation Study. All metrics are reported for the validation and the test split, after training with softmax cross-entropy for 30 epochs. The Transformer (Transf) and the Object Relational Transformer (ObjRel Transf) is described in detail in Section 3</figDesc><table><row><cell>Algorithm</cell><cell></cell><cell cols="6">CIDEr-D SPICE BLEU-1 BLEU-4 METEOR ROUGE-L</cell></row><row><cell>Up-Down + LSTM</cell><cell>val test</cell><cell>105.6 106.6</cell><cell>19.7 19.9</cell><cell>75.5 75.6</cell><cell>32.9 32.9</cell><cell>26.5 26.5</cell><cell>55.6 55.4</cell></row><row><cell>Up-Down + Transf</cell><cell>val test</cell><cell>110.5 111.0</cell><cell>20.8 20.9</cell><cell>75.2 75.0</cell><cell>33.3 32.8</cell><cell>27.6 27.5</cell><cell>55.8 55.6</cell></row><row><cell>Up-Down + ObjRel Transf</cell><cell>val test</cell><cell>113.2 112.6</cell><cell>21.0 20.8</cell><cell>76.1 75.6</cell><cell>34.4 33.5</cell><cell>27.7 27.6</cell><cell>56.4 56.0</cell></row><row><cell cols="2">Up-Down + ObjRel Transf val</cell><cell>114.7</cell><cell>21.1</cell><cell>76.5</cell><cell>35.5</cell><cell>27.9</cell><cell>56.6</cell></row><row><cell>+ Beamsize 2</cell><cell>test</cell><cell>115.4</cell><cell>21.2</cell><cell>76.6</cell><cell>35.5</cell><cell>28.0</cell><cell>56.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparison of different captioning metrics for the Standard Transformer and our proposed Object Relation Transformer (denoted Ours below), trained with softmax cross-entropy for 30 epochs. The table shows that the Object Relation Transformer has significantly higher CIDEr-D, BLEU-1 and ROUGE-L scores. The p-values come from two-tailed t-tests using paired samples. Values marked in bold were considered significant at α = 0.05.</figDesc><table><row><cell>Algorithm</cell><cell cols="6">CIDEr-D SPICE BLEU-1 BLEU-4 METEOR ROUGE-L</cell></row><row><cell>Standard Transformer</cell><cell>113.21</cell><cell>21.04</cell><cell>75.60</cell><cell>34.58</cell><cell>27.79</cell><cell>56.02</cell></row><row><cell>Ours</cell><cell>115.37</cell><cell>21.24</cell><cell>76.63</cell><cell>35.49</cell><cell>27.98</cell><cell>56.58</cell></row><row><cell>p-value</cell><cell>0.01</cell><cell>0.15</cell><cell>&lt;0.001</cell><cell>0.051</cell><cell>0.24</cell><cell>0.01</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Breakdown of SPICE metrics for the Standard Transformer and our proposed Object Relation Transformer (denoted Ours below), trained with softmax cross-entropy for 30 epochs. The table shows that the Object Relation Transformer has significantly higher Relation and Count scores. The p-values come from two-tailed t-tests using paired samples. Values marked in bold were considered significant at α = 0.05.</figDesc><table><row><cell>Algorithm</cell><cell>All</cell><cell cols="4">SPICE Object Relation Attribute Color Count Size</cell></row><row><cell cols="3">Standard Transformer 21.04 37.83</cell><cell>5.88</cell><cell>11.31</cell><cell>14.88 11.30 5.82</cell></row><row><cell>Ours</cell><cell cols="2">21.24 37.92</cell><cell>6.31</cell><cell>11.37</cell><cell>15.49 17.51 6.38</cell></row><row><cell>p-value</cell><cell>0.15</cell><cell>0.64</cell><cell>0.01</cell><cell>0.81</cell><cell>0.35 &lt;0.001 0.34</cell></row><row><cell>5.6 Qualitative Analysis</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Example images and captions for which the SPICE Relation metric for Object Relation Transformer shows an improvement over the metric for the Standard Transformer.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Example images and captions for which the SPICE Count metric for the Object Relation Transformer shows an improvement over the metric for the Standard Transformer.</figDesc><table><row><cell>Standard: a large bird</cell><cell>a little girl sitting on top</cell><cell>a group of young men</cell><cell>three children are sitting</cell></row><row><cell>is standing in a cage</cell><cell>of a giraffe</cell><cell>riding skateboards down</cell><cell>on a bunk bed</cell></row><row><cell></cell><cell></cell><cell>a sidewalk</cell><cell></cell></row><row><cell>Ours: two large birds</cell><cell>a giraffe with two kids</cell><cell>two young men riding</cell><cell>two young children are</cell></row><row><cell>standing in a fenced in</cell><cell>sitting on it</cell><cell>skateboards down a side-</cell><cell>sitting on the beds</cell></row><row><cell>area</cell><cell></cell><cell>walk</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Some publications include results obtained from an ensemble of models. Specifically, the ensemble of two distinct graph convolution networks in GCN-LSTM<ref type="bibr" target="#b28">[29]</ref> achieves a superior CIDEr-D score to our stand-alone model.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank Ruotian Luo for making his image captioning code available on GitHub <ref type="bibr" target="#b15">[16]</ref>.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">SPICE: Semantic propositional image caption evaluation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="382" to="398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Meteor universal: Language specific translation evaluation for any target language</title>
		<author>
			<persName><forename type="first">M</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ninth workshop on statistical machine translation</title>
		<meeting>the ninth workshop on statistical machine translation</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="376" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">From captions to visual concepts and back</title>
		<author>
			<persName><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1473" to="1482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Relation networks for object detection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3588" to="3597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Densecap: Fully convolutional localization networks for dense captioning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4565" to="4574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3128" to="3137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multimodal neural language models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="595" to="603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Text Summarization Branches Out</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Context-aware visual policy network for sequence-level image captioning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM International Conference on Multimedia, MM &apos;18</title>
		<meeting>the 26th ACM International Conference on Multimedia, MM &apos;18</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1416" to="1424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<ptr target="https://github.com/ruotianluo/ImageCaptioning.pytorch" />
		<title level="m">An image captioning codebase in PyTorch</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6632</idno>
		<title level="m">Deep captioning with multimodal recurrent neural networks (m-RNN)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">BLEU: a method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting on association for computational linguistics</title>
		<meeting>the 40th annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI Blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Self-critical sequence training for image captioning</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Marcheret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Goel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="7008" to="7024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-07">July 2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2556" to="2565" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">CIDEr: Consensus-based image description evaluation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4566" to="4575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Auto-encoding scene graphs for image captioning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="10685" to="10694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Review networks for caption generation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2361" to="2369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Exploring visual relationship for image captioning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="684" to="699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Image captioning with semantic attention</title>
		<author>
			<persName><forename type="first">Q</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4651" to="4659" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
