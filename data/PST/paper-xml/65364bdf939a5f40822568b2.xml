<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Robust User Behavioral Sequence Representation via Multi-scale Stochastic Distribution Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chilin</forename><surname>Fu</surname></persName>
							<email>chilin.fcl@antgroup.com</email>
						</author>
						<author>
							<persName><forename type="first">Jun</forename><surname>Zhou</surname></persName>
							<email>jun.zhoujun@antfin.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Weichang Wu</orgName>
								<orgName type="laboratory">Ant Group Hangzhou</orgName>
								<address>
									<settlement>Zhejiang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">Ant Group Hangzhou</orgName>
								<address>
									<settlement>Zhejiang</settlement>
									<country>China Xiaolu Zhang</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Jun Hu</orgName>
								<orgName type="laboratory">Ant Group Hangzhou</orgName>
								<address>
									<settlement>Zhejiang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Jing Wang</orgName>
								<orgName type="laboratory">Ant Group Hangzhou</orgName>
								<address>
									<settlement>Zhejiang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="laboratory">Ant Group Hangzhou</orgName>
								<address>
									<settlement>Zhejiang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="laboratory">Ant Group Hangzhou</orgName>
								<address>
									<settlement>Zhejiang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Robust User Behavioral Sequence Representation via Multi-scale Stochastic Distribution Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3583780.3614714</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>machine learning</term>
					<term>sequential data mining</term>
					<term>representation learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>User behavior representation learned by self-supervised pre-training tasks is widely used in various domains and applications. Conventional methods usually follow the methodology in Natural Language Processing (NLP) to set the pre-training tasks. They either randomly mask some of the behaviors in the sequence and predict the masked ones or predict the next ? behaviors. These methods fit for text sequence, in which the tokens are sequentially arranged subject to linguistic criterion. However, the user behavior sequences can be stochastic with noise and randomness. The same paradigm is intractable for learning a robust user behavioral representation.</p><p>Though the next user behavior can be stochastic, the behavior distribution over a period of time is much more stable and less noisy. Based on this, we propose a Multi-scale Stochastic Distribution Prediction (MSDP) algorithm for learning robust user behavioral sequence representation. Instead of using predictions on concrete behavior as pre-training tasks, we take the prediction on user's behaviors distribution over a period of time as the selfsupervision signal. Moreover, inspired by the recent success of the multi-task prompt training method on Large Language Models (LLM), we propose using the window size of the predicted time period as a prompt, enabling the model to learn user behavior representations that can be applied to prediction tasks across various future time periods. We generate different window size prompts through stochastic sampling. It effectively improves the generalization capability of the learned sequence representation. Extensive experiments demonstrate that our approach can learn robust user behavior representation successfully, which significantly outperforms state-of-the-art (SOTA) baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>? Computing methodologies ? Machine learning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>User modeling from abundant unlabeled user behavior data is one of the most important issues in machine learning applications, which greatly improves the generalization ability of user representation and empowers user modeling in many downstream tasks. Existing works of literature about user modeling on unlabeled user behaviors usually follow the methodology in NLP <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">12]</ref> to construct the pretraining task in a self-supervised manner. They either randomly mask some of the behaviors in the sequence and predict the masked ones <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b13">14]</ref>, or predict the next ? behaviors in an auto-regressive manner <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b17">18]</ref>. However, the above methods have seldom considered the fact that the user behavior sequence is quite different from the text sequence. The text sequence follows the linguistic criterion in human language with few noisy and stochastic signals, but the user behaviors are the opposite. For example, many verbs continuously occurring in the sentence violate the grammar and can be regarded as noise. But it is common that a user repeatedly clicks and purchases the same product online in a short period. The text sequence also contains little randomness e.g. adjectives are followed by nouns and an adverb is followed by a verb in regular grammar. While user behaviors, e.g. user purchase behaviors, can be random. For a user who is buying e.g. a bunch of stationeries, the order of payment for each stationery can be arbitrary, such as paying for the pen first then the paper, or vice versa should be the same semantics. Since interfered by the noise and randomness in user behavioral sequence, the difficulty of masked behaviors prediction and next ? behaviors prediction tasks significantly increase, which affects the robustness of the learned user representation.</p><p>To resolve the above problem, we propose a new paradigm named Multi-scale Stochastic Distribution Prediction (MSDP) for learning robust representation. Considering the noise and sequential randomness in user behaviors, predicting the specific user behavior is intractable. The overall behavior distribution is more predictable than the several masked or next behaviors and is less sensitive to noise and randomness. Therefore, we use the prediction of users' behavior distribution over a period of time as the pre-training task instead of predicting several masked or next ? behaviors. Moreover, we propose a multi-scale stochastic sampling method for improving the generalization ability of the user representation. Specifically, different downstream tasks may care about users' interests or habits over different time periods. such as the task to predict whether the user will buy a cup of coffee the next day and the task to predict whether the user will repay their credit card in the next month. Therefore, we predict users' behavior distribution over multiple different periods (e.g. 5 days, 10 days, or 30 days in the future) with a stochastic strategy in the training stage to satisfy the potential various downstream tasks. Compared to conventional pre-training tasks, the MSDP shows more robustness to the noise and randomness and achieves significant improvements in different downstream tasks.</p><p>The contributions of this paper are listed as follows: i) We propose to predict user Behavior Distribution over a time period to learn behavioral sequence representation. Different from previous paradigms of predicting specific masked behaviors or next ? behaviors, it is a new paradigm of setting self-supervised learning tasks to achieve robust representation from user behavior sequences.</p><p>ii) We develop a novel Multi-scale Stochastic Distribution Prediction (MSDP) algorithm for predicting user behaviors distribution. The algorithm uses a Multi-scale Stochastic Prompt Training algorithm. It effectively improves the generalization ability of user representation and satisfies various downstream tasks.</p><p>iii) We introduce a Contrastive Regularization term into the algorithm. It is a constrastive loss between unmasked sequence and masked one, to reinforce the representation on history behavioral sequence and avoid over-fitting on future behavior prediction.</p><p>Experiments on a business dataset for risk management and a public dataset for e-commerce user interest discovery, shows that the user behavioral sequence representation vector learned by the proposed algorithm can significantly improve the performance on downstream tasks, compared with classical methods and state-ofthe-art ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Behavioral Sequence Representation Learning</head><p>Most of the previous and state-of-the-art literatures on behavioral sequence representation learning follows the same methodology in conventional NLP for setting pre-training tasks, either predicting the masked tokens in the sequence e.g. <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b12">13]</ref> which we call Masked Behavior Prediction (MBP), or predicting one of ? tokens in future <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b17">18]</ref> which we call Next Behavior Prediction (NBP).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">MBP and NBP.</head><p>The MBP model <ref type="bibr" target="#b9">[10]</ref> mask one of the items ? in user behavioral sequence ? and predict whether the masked item ? is from user sequence ?. The NBP model <ref type="bibr" target="#b14">[15]</ref> maximize the relatedness between the representation of ? and future next ? items {? 1 , . . . , ? ? }. The overall representation on behavioral sequence ? is learned by maximizing the sum of the likelihood of MBP and NBP. Most recent works maskes some improvements on the design of the masked or next behavior prediction task. Generally they introduce extra auxiliary tasks along with the MBP or NBP tasks for learning behavioral sequence representation. For example, <ref type="bibr" target="#b13">[14]</ref> proposes to complement the masked items with other randomly selected items, and introduces an extra discrimination model besides the MBP model to discriminate whether the behavioral sequence is a complemented one. It enables the model to take the relatedness between user behaviors into consideration. In <ref type="bibr" target="#b17">[18]</ref>, it rearranges the items of the user interaction history with a certain probability, and use the model to predict whether the user interaction history has been rearranged to learn the sequence-level information of the user's entire interaction history. Along with the MBP task, it use the extra rearrange sequence prediction task to learn the sequencelevel information of the user's entire interaction history, and use NBP task for fine-tuning.</p><p>We consider the most similar literature to our work under the methodology of NBP to be <ref type="bibr" target="#b7">[8]</ref>. In order to avoid the noise in shortterm behavior signals, it propose to predict the long-term user behavior over a certain period of time. For reducing model complexity, instead of predicting all behaviors in a certain time window, it predicts several user behaviors sampled from the next ? days.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Other</head><p>Methodologies. There exists recent work using new methodologies for learning behavioral sequence representation besides MBP and NBP. <ref type="bibr" target="#b3">[4]</ref> employs contrastive learning for modeling sequence-level user representation, and constructing the selfsupervision signals by transforming the original user behaviors by data augmentations. Two Siamese weight-sharing networks as in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b15">16]</ref> are implemented to learn user-oriented representations. The sequence representation is learned by maximizing the similarity of learned embedding of the same user by these two Siamese networks.</p><p>The most similar literature to our work that is not in the conventional MBP or NBP methodology is <ref type="bibr" target="#b6">[7]</ref>. Instead of predicting specific items in future, based on a distribution consistency hypothesis that future behavior distribution should be consistent to the previous distribution, it learns the sequence representation by maximizing the Kullback-Leibler divergence <ref type="bibr" target="#b10">[11]</ref> between the item occurrence distribution of a previous range of time and that of future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multi-task Prompt Training</head><p>Multi-task prompt training is an emerging approach in machine learning that aims to improve the performance of multiple related tasks through the use of task-specific prompts. Unlike traditional multi-task learning methods that rely on shared representations, multi-task prompt training leverages the flexibility of task-specific prompts to train models simultaneously on multiple tasks.</p><p>One notable application of multi-task prompt training is seen in the Text-to-Text Transfer Transformer (T5) model introduced in <ref type="bibr" target="#b8">[9]</ref>. T5 adopts a unified Text-to-Text framework where multiple tasks are trained using task-specific prompts. By employing this approach, T5 achieves remarkable performance across various natural language processing tasks. The model's ability to learn shared Inspired by these works, in this paper, we adopt a prompt training approach, where the time window size of future prediction is used as the prompt for model training. By randomly sampling prediction time windows of different scales during training, our model is prompted to learn general user behavior representations that can be applied to downstream prediction tasks spanning multiple time periods, thus improving the model's effectiveness in capturing temporal dependencies.</p><formula xml:id="formula_0">x t-1 x t-2 x t x t-3 x 1 x 2 ? 0 T W 1 W 2 W 3 x t+5 x t+4 x t+6 x t+3 x t+1 x t+2 x t+7 T+W 1 T+W2 T+W3</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROPOSED METHOD 3.1 Problem definition</head><p>Given a behavioral sequence ? of length ? over a time period of (0,? ] from user ?, it is defined as</p><formula xml:id="formula_1">? ? = {? 1 , ? 2 , . . . , ? ? -1 , ? ? },<label>(1)</label></formula><p>where ? ? is the ?-th sequential behavior in the sequence, and ? ? X where X ? N ? is the user behavior set containing all kinds of user behaviors defined by ? discrete values. We aim at learning a behavioral sequence representation vector v ? from ? ? , by extracting the information contained in ? ? in a self-supervised way via setting pre-training tasks. As shown in Figure .1, the pre-training tasks include distribution prediction tasks as Multi-scale Stochastic Distribution Prediction (MSDP) for learning a robust sequence representation, and contrastive tasks for model regularization that maximize the cosine similarity of the embedding vectors from masked sequence ?? and unmasked sequence ? ? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multi-scale Stochastic Distribution Prediction</head><p>The proposed model consists of Specifically, suppose we observe the user behaviors in time window (? ,? +? ], the ground-truth label of the behavior distribution is given by ? ? ? {0, 1} ? , where ? is the size of the candidate user behavior set, and ? is the window size prompt we set for the model. The pre-training task is to predict the behavior distribution, with the likelihood function defined as</p><formula xml:id="formula_2">L ? ?? ? = - 1 ? ?? ? ? ? S ? ?? ?=1 ? ? ? log ?? ? ,<label>(2)</label></formula><p>where ? ? and ?? are the ground truth and predicted labels for the ?-th behavior prediction, ?? is the output generated by the behavioral sequence representation learning model ? (? ? ). The feature representation vector v ? is from the middle layer embedding of ? (? ? ). Training the model over one single size of time window will constrain the generalization capability, leading to the consequence that the learned representation only fits for downstream prediction tasks on future one typical window size. In contrast, training models on every window size will lead to the explosion of the number of tasks.</p><p>To tackle this problem, for each sample we draw from the behavioral sequence dataset S, we randomly sample ? window size ? from a uniform distribution in the continuous time domain as ? ? Uniform[? ??? ,? ??? ], where ? ??? and ? ??? are the minimum and maximum values of time window size ? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Constrastive Regularization.</head><p>In addition to the proposed selfsupervision signals from the behavior distribution in time window (? ,? + ? ] constructed from the view of behavior distribution prediction for learning the behavioral sequence representation in (0,? ], we also introduce a behavior reconstruction task trying to reconstruct the masked behavioral sequence in (0,? ] as a regularization term, in order to avoid over-fit on future behaviors while ignoring the behavioral sequence itself.</p><p>Referring to the idea of SimSiam in contrastive learning <ref type="bibr" target="#b2">[3]</ref>, we propose to maximize the cosine similarity of the representation v? of the randomly masked behavioral sequence ?? , and v ? of the unmasked behavioral sequence ? ? , defined by:</p><formula xml:id="formula_3">D ( v? , v ? ) = v? ? v? ? 2 ? v ? ?v ? ? 2 ,<label>(3)</label></formula><p>where ? ? ? 2 is the ? 2 -norm, v? ? ? (? ( ?? )) is the hidden layer representation of randomly masked behavioral sequence and v ? ? ? (? (? ? )) is the behavioral sequence representation of the unmasked sequence ? ? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Objective Function.</head><p>Supposing that we totally sample ? time windows from the uniform distribution ,the overall objective function is given by the sum of the simulation likelihood in Eq.2 with the regularization term in Eq.3 as</p><formula xml:id="formula_4">L = 1 ? ? ?? ?=1 L ? ? ?? ? + ?D (? ( ?? ) , ? (? ? )) ,<label>(4)</label></formula><p>where ? is a non-negative coefficient to control the strength of contrastive regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Utilization in downstream tasks.</head><p>As shown in Figure. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>To verify the effectiveness of the proposed method, we apply the learned behavioral sequence representation vector to multiple downstream prediction missions. We compare the model performance with 3 classical and 1 stat-of-the-art baselines on 2 datasets, using different feature representation vectors from the proposed method and baselines for downstream prediction missions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment Setup</head><p>For a fair comparison, we use transformer encoder <ref type="bibr" target="#b11">[12]</ref> with the same hyperparameters as the feature extractor on all compared methods. For the downstream tasks, we use the user representation vector output of the pre-trained model as input and a DNN with two fully connected layers (512-256) as the model structure.</p><p>4.1.1 Dataset. The experimental datasets include an Overdue risk (O-risk) management dataset in which the downstream tasks are to predict the overdue risk of Alipay 12 . users in future ? days, and an e-commerce behavior dataset Tmall<ref type="foot" target="#foot_2">3</ref> in <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19]</ref> to predict the user's interested item category also in future ? days. O-risk dataset contains users' all the finance activities on Alipay including consumption, loan and repayment, etc. We choose top-200 high frequency activities to construct the behavior sequence, and predict the future distribution over the 200 activities. The learned representation is used in 5 downstream prediction tasks for predicting the overdue risk of a user in future 5, 15, 30, 60, 90-days respectively.</p><p>Tmall dataset contains users' purchase records on Tmall<ref type="foot" target="#foot_3">4</ref> including 90 categories of products. We predict users' purchase distribution over the 90 categories to learn a representation. The downstream tasks are to predict the most interesting item category (we consider the item with the most times of clicks as the user's favored item) in future 5, 15, 30, 60, 90-days.</p><p>4.1.2 Baselines. Based on the 3 key components in our approach i.e. Distribution Prediction, Multi-scale Prompt Training, and Contrastive Regularization, we conduct a series of comparative experiments to testify the effectiveness of our method. Specifically, we set up 4 categories of baselines:</p><p>conventional MBP&amp;NBP baselines. To demonstrate the role of the proposed Distribution Prediction task in the model, we compare the MSDP model with state-of-the-art MBP and NBP methods, which use different prediction tasks. Including: i) BERT4Rec <ref type="bibr" target="#b17">[18]</ref>. A standard implementation of MBP, i.e. masked behavior prediction task.</p><p>ii) PTUM <ref type="bibr" target="#b14">[15]</ref>. A method consists both MBP and NBP, containing masked behavior prediction and next ? behaviors prediction tasks.</p><p>other methodologies. Apart from the traditional MBP and NBP methods, we also compared our proposed MSDP model with two other SOTA methods of different methodologies. One is an unsupervised user behavior representation method based on contrastive learning, and the other is a behavior distribution prediction method that is most similar to ours. Including: i) UserBERT <ref type="bibr" target="#b13">[14]</ref> is a state-of-the-art method for user modeling from unlabeled data using two contrastive pre-training tasks.</p><p>ii) SUMN <ref type="bibr" target="#b6">[7]</ref> learns the user representation by a behavioral consistency loss, which is similar to the distribution prediction task but it only focuses on a certain period of the future.</p><p>iii) static-DP. By fixing the window size prompt value in MSDP model, the model is reduced to learning user behavior representations through predicting a static time window like SUMN. The difference is that static-DP predicts whether behavior will occur during the specified time window, instead of predicting the frequency of occurrence.</p><p>Multi-task baselines. To demonstrate the effectiveness of our Multi-scale Prompt Training method, we compare the MSDP model with different multi-task training baseline under the same task number setting. Including: i) Multi-task SUMN. This baseline extends the SUMN model with a shared bottom multi-task structure. Each branch predicts the behavior distribution for a certain period of time. The shared layer embedding is learned as the behavior representation.</p><p>ii) Multi-task Distribution Prediction (MTDP). Similarly, we also extend static-DP through a multi-task form.</p><p>Ablation on Contrastive Regularization (CR). To investigate the effectiveness of CR in the model, we conduct experiments by removing the CR term as a baseline in the MSDP model, also with the other two baseline models static-DP and MTDP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Evaluation.</head><p>For the favorite category prediction task on the Tmall dataset and the overdue risk prediction task on the real-world dataset, we take ACC(accuracy) and KS(Kolmogorov-Smirnov) as the evaluation metric, respectively. Since our purpose is to verify the effectiveness of the pre-training task of MSDP, the difference between the above-compared methods is only their pre-training tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results and Discussion</head><p>As shown in Table .1, we compare MSDP with single-task and multitask methods. The time scopes we predict include 5-days, 15-days, 30-days, 60-days and 90-days. In specific time scopes, we predict users' favorite product types on Tmall dataset and the risk of overdue on O-risk dataset. On both datasets, the performance of the MSDP model is better than baselines, whether compared to the single-task baseline or the multi-task baseline. In Table .2, we compare the performance of the multi-task model and the MSDP model on the same downstream 5-days prediction task under different task number settings. Table .3 shows the results of our ablation study on the Contrastive Regularization term. On different DP models, the models with CR consistently outperformed those without CR.</p><p>Specifically, we have the following interesting findings. proposed DP vs. SUMN We compare our method with the most similar related work SUMN. SUMN predicts the user behaviors specific occurrence number of times as the user behavior distribution in the form of regression task, while our DP task predicts if the user behavior occurs in a future period of time, in the form of classification task. As shown in Table .1, both static-DP and MTDP, outperform the corresponding single-task SUMN and multi-task SUMN algorithm. The regression pre-training task in SUMN is not as suitable as the classification task in our model, for user behavior sequences with high randomness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Effects of Multi-scale Stochastic Prompt Training.</head><p>single-task vs. multi-task vs. multi-scale As shown in Table.1, on two short-term 5-days and 15-days prediction tasks, the behavioral representation of the multi-task MTDP model is slightly inferior to that of the single-task static-DP model. While for longterm prediction tasks e.g. 30, 60 and 90-days, the MTDP model performs better. In contrast, SUMN shows the opposite trend: the multi-task SUMN performs better on downstream prediction tasks of 30-days or less, while the single-task SUMN model performs better on 60-days and 90-days prediction. Overall, the behavioral sequence representation learned by the multi-task model shows more generality than single-task pre-trained representations, resulting in better performance on most downstream tasks.</p><p>The MSDP model trained using multi-scale prompt training outperforms both single-task and multi-task models, indicating that the Multi-scale Stochastic Prompt Training approach can help the model explore time dependency relationships at multiple time scales and learn better user behavior sequence representations, compared to conventional multi-task training.  .1, we present the results of the multi-task models under 5 pre-training tasks. To further explore the impact of different numbers of pre-training tasks on conventional multi-task models, and further demonstrate the generalization ability of MSDP model, we gradually increase the number of pre-training tasks for the multi-task model in Table <ref type="table" target="#tab_4">2</ref> to obtain a more generalized multi-task sequence representation. We test the performance on the same downstream task of 5-days prediction.</p><p>As shown in Table .2, as the number of pre-training tasks increases, the performance of multi-task SUMN model improves, but it is still inferior to that of the MTDP and MSDP models. The behavioral representation of the MTDP model shows a trend of first increasing and then decreasing, with the best performance achieved with two tasks i.e. 5-days and 15-days. Compared to MSDP, the 2-tasks pre-training MTDP is slightly better on the Tmall dataset than MSDP, while slightly worse on the O-risk dataset.</p><p>Based on the results of Table .1 and Table .2, the advantage of MSDP model over MTDP model lies in its generalization ability. Unlike MTDP, MSDP does not require pre-training task selection or task number setting based on downstream tasks, and its learned representation can still achieve remarkable performance on different downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Effects of Contrastive Regularization.</head><p>with vs. without Contrastive Regularization (CR) We investigate the impact of the CR term on our proposed DP models, including static-DP, MTDP, and MSDP. As shown in Table .3, models with CR term perform better than those without CR term. The CR term can improve the accuracy of the DP models by 1 point and the KS by 1 -2 points. By using the CR term in the model to incorporate a contrastive loss between unmasked and masked behavioral sequences, the over-fitting on future behavior prediction task during model training can be relieved, thus improving the performance of the learned representation vector on downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Industrial application results.</head><p>The MSDP model is deployed in the risk control system of Ant Group since October 2022, being used in 4 institutional credit risk control businesses and 6 personal credit risk control businesses. By incorporating the user behavioral representation learned by MSDP into downstream task-specific risk models, the Kolmogorov-Smirnov (KS) of downstream models are improved by 2 -5 point, with a relative lift of 5% -12%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we focus on the problem of learning user behavioral sequence representation. Traditional methods tackle this problem following the conventional methodology in NLP, by predicting the specific masked item or future item. We specify that this pretraining methodology doesn't fit user behavioral sequences containing noise and randomness. We propose the Multi-scale Stochastic Distribution Prediction (MSDP) algorithm for learning representation from noisy and stochastic user behavioral sequences. Two major innovations of this algorithm are: i) We propose to predict user behaviors distribution over a time period instead of predicting specific behaviors or items; ii) We develop a multi-scale stochastic sampling method to sample time windows of different time scales for model training. Experiments are conducted on 2 real-world datasets, including a business one and a public one, to show the effectiveness of the proposed algorithm.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>3. 2 . 2</head><label>22</label><figDesc>Multi-scale Stochastic Prompt Training. The setting for the size of the time windows ? can be tricky in practical model design.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>4. 2 . 1</head><label>21</label><figDesc>Effects of Distribution Prediction. Distribution Prediction (DP) vs. MBP and NBP DP models including static-DP, MTDP and MSDP significantly outperform the BERT4Rec model based on MBP, and the PTUM model simultaneously adopt MBP and NBP. They also achieve superior results than state-of-the-art work UserBERT. It clarifies that NBP &amp; MBP methods are not suitable for user behavior sequences with randomness, while DP methods help to learn a robust and stable behavioral sequence representation than conventional methodology.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Contrastive Regularization term based on contrastive learning. This term is added to prevent model over-fitting on future predictions and neglecting the fitting of behavior distributions.3.2.1 Distribution Prediction.Given the behavioral sequence ? ? over a period of time (0,? ], we use the user behaviors over the next period of time window (? ,? +? ] as the self-supervision signal. As aforementioned, because there is noise and sequential randomness in the supervision signals, instead of predicting the specific user behavior in (? ,? + ? ], we predict the behavior distribution over (? ,? + ? ].</figDesc><table /><note><p>3 components: i) Definition of the pre-training task: Different from traditional NBP or MBP methods, we take the behavior Distribution Prediction for a future period of time as the pre-training task. ii) Pre-training method: Different from traditional multi-task pre-training methods, we use Multi-scale Stochastic Prompt Training to train the model with different tasks i.e. different sizes of prediction time windows as prompts. iii) A</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>When testing on downstream tasks {? 1 , ? 2 , . . . , ? ? } in future time (? + ? , +?), the representation vector is generated for behavioral sequence in (? ,? + ? ].</figDesc><table><row><cell>Training</cell><cell></cell><cell cols="2">Sequential Model</cell><cell></cell><cell></cell><cell cols="2">Distribution Predictor</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>? !</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>x 1</cell><cell>x 2</cell><cell>?</cell><cell>x t-2</cell><cell>x t-1</cell><cell>x t</cell><cell>x t+1</cell><cell>x t+2</cell><cell>?</cell><cell>x t+n</cell></row><row><cell>0</cell><cell></cell><cell></cell><cell></cell><cell>W</cell><cell>T</cell><cell></cell><cell></cell><cell></cell><cell>T+W</cell><cell>P1</cell><cell>P2</cell><cell>PN</cell></row><row><cell></cell><cell></cell><cell cols="2">Testing</cell><cell></cell><cell></cell><cell>? !</cell><cell></cell><cell></cell><cell>v !</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Sequential Model</cell><cell></cell><cell></cell></row><row><cell cols="11">Figure 2: Train and test settings. When training model, behav-</cell></row><row><cell cols="11">ioral sequence representation is learned from (0,? ] to predict</cell></row><row><cell cols="8">the behavior distribution in (? ,? +? ].</cell><cell></cell><cell></cell></row></table><note><p>2, in the training stage, based on the observed behavioral sequence in (0,? +? ], we use the user behaviors distribution in [? ,? + ? ] as the self-supervision signals, and pre-train the sequential model to learn a behavioral sequence representation for the user behaviors in (0,? ]. In the testing stage, for the downstream prediction tasks {? 1 , ? 2 , . . . , ? ? } in future time (? +? , +?), we use the pre-trained sequential model to generate a sequence embedding v ? for the behavioral sequence in (? ,? + ? ]. Then v ? is utilized as a constant input for the downstream prediction missions.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Evaluation results on Tmall and O-risk on downstream missions under # of pre-training task 5.</figDesc><table><row><cell cols="2">Dataset</cell><cell></cell><cell></cell><cell>Tmall (ACC)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>O-risk (KS)</cell><cell></cell><cell></cell></row><row><cell cols="2">Time Scope</cell><cell cols="10">5-days 15-days 30-days 60-days 90-days 5-days 15-days 30-days 60-days 90-days</cell></row><row><cell></cell><cell>BERT4Rec [18]</cell><cell>62.3</cell><cell>61.1</cell><cell>53.6</cell><cell>34.2</cell><cell>22.1</cell><cell>42.3</cell><cell>41.7</cell><cell>48.7</cell><cell>31.8</cell><cell>30.2</cell></row><row><cell></cell><cell>PTUM [15]</cell><cell>65.1</cell><cell>65.4</cell><cell>52.1</cell><cell>36.6</cell><cell>28.5</cell><cell>43.7</cell><cell>43.3</cell><cell>51.5</cell><cell>33.5</cell><cell>34.8</cell></row><row><cell>Single-task</cell><cell>UserBERT [14]</cell><cell>66.2</cell><cell>67.8</cell><cell>54.1</cell><cell>40.3</cell><cell>36.7</cell><cell>45.1</cell><cell>44.9</cell><cell>52.1</cell><cell>36.9</cell><cell>37.2</cell></row><row><cell></cell><cell>SUMN [7]</cell><cell>61.7</cell><cell>64.2</cell><cell>55.3</cell><cell>50.6</cell><cell>41.2</cell><cell>44.2</cell><cell>46.9</cell><cell>51.0</cell><cell>35.8</cell><cell>39.7</cell></row><row><cell></cell><cell>static-DP</cell><cell>67.5</cell><cell>69.1</cell><cell>56.8</cell><cell>47.5</cell><cell>38.4</cell><cell>46.3</cell><cell>48.6</cell><cell>53.6</cell><cell>37.6</cell><cell>41.3</cell></row><row><cell>Multi-task</cell><cell>SUMN [7] MTDP</cell><cell>64.2 66.5</cell><cell>66.1 68.7</cell><cell>55.9 56.2</cell><cell>50.5 50.3</cell><cell>40.8 42.6</cell><cell>45.3 46.1</cell><cell>48.2 47.5</cell><cell>53.2 55.1</cell><cell>36.6 38.1</cell><cell>39.1 40.8</cell></row><row><cell cols="2">Multi-scale MSDP</cell><cell>68.0</cell><cell>69.4</cell><cell>57.1</cell><cell>51.2</cell><cell>43.7</cell><cell>46.9</cell><cell>49.2</cell><cell>55.3</cell><cell>38.5</cell><cell>41.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Evaluation results on Tmall and O-risk under different # of pre-training tasks. The learned behavioral sequence representation is evaluated on the same down-stream 5-days prediction task.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell></cell><cell>Tmall (ACC)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>O-risk (KS)</cell><cell></cell><cell></cell></row><row><cell># of tasks</cell><cell cols="10">1-tasks 2-tasks 3-tasks 4-tasks 5-tasks 1-tasks 2-tasks 3-tasks 4-tasks 5-tasks</cell></row><row><cell>Multi-task SUMN[7]</cell><cell>61.7</cell><cell>62.3</cell><cell>62.5</cell><cell>63.2</cell><cell>64.2</cell><cell>44.2</cell><cell>44.8</cell><cell>45.5</cell><cell>45.1</cell><cell>45.3</cell></row><row><cell>MTDP</cell><cell>67.5</cell><cell>68.5</cell><cell>67.4</cell><cell>66.8</cell><cell>66.5</cell><cell>46.3</cell><cell>46.7</cell><cell>46.1</cell><cell>46.4</cell><cell>46.1</cell></row><row><cell>MSDP</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>68.0</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>46.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Ablation study on Contrastive Regularization (CR) term, with vs. without CR.</figDesc><table><row><cell>Datasets</cell><cell>Methods</cell><cell cols="3">static-DP MTDP MSDP</cell></row><row><cell>Tmall (ACC)</cell><cell>without-CR with-CR</cell><cell>66.1 67.5</cell><cell>65.7 66.5</cell><cell>67.3 68.0</cell></row><row><cell>O-risk (KS)</cell><cell>without-CR with-CR</cell><cell>44.8 46.3</cell><cell>45.3 46.1</cell><cell>45.4 46.9</cell></row><row><cell cols="3">influence of task number In Table</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://www.alipay.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>The data set is desensitized and encrypted, which is only used for academic research. It does not represent any real business situation</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://tianchi.aliyun.com/dataset/140281</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>https://www.tmall.com/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Adiwardana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">R</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Hall</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.09977</idno>
		<title level="m">Noah Fiedel, Romal Thoppilan, ..., and Illia Polosukhin. 2020. Towards a human-like opendomain chatbot</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning representations by maximizing mutual information across views</title>
		<author>
			<persName><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Buchwalter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Exploring simple siamese representation learning</title>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="15750" to="15758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning transferable user representations with sequential behaviors via contrastive pretraining</title>
		<author>
			<persName><forename type="first">Mingyue</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fajie</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Conference on Data Mining (ICDM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="51" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unified language model pre-training for natural language understanding and generation</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">.</forename><forename type="middle">.</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="13042" to="13053" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Exploiting behavioral consistence for universal user representation</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinghui</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiquan</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoxiao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="4063" to="4071" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Pinner-Former: Sequence Modeling for User Representation at Pinterest</title>
		<author>
			<persName><forename type="first">Nikil</forename><surname>Pancha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Rosenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="3702" to="3712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">.</forename><forename type="middle">.</forename><surname>Mateusz Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Kyuyong</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanock</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyung-Min</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minkyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Young-Jin</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jisu</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seungjae</forename><surname>Jung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.00573</idno>
		<title level="m">One4all user representation for recommender systems in e-commerce</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">R?nyi divergence and Kullback-Leibler divergence</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Van Erven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Harremos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="3797" to="3820" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Attention is All you Need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno>ArXiv abs/1706.03762</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">UserBERT: Pretraining User Model with Contrastive Self-supervision</title>
		<author>
			<persName><forename type="first">Chuhan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangzhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongfeng</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2087" to="2092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">PTUM: Pre-training User Model from Unlabeled User Behaviors via Self-supervision</title>
		<author>
			<persName><forename type="first">Chuhan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangzhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxun</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongfeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>In Findings of the Association for Computational Linguistics: EMNLP 2020. 1939-1944</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised embedding learning via invariant and spreading instance feature</title>
		<author>
			<persName><forename type="first">Mang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6210" to="6219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Large Scale Purchase Prediction with Historical User Actions on B2C Online Retail Platform</title>
		<author>
			<persName><forename type="first">Yuyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.6515</idno>
		<ptr target="http://arxiv.org/abs/1408.6515" />
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">RESETBERT4Rec: A pre-training model integrating time and user historical behavior for sequential recommendation</title>
		<author>
			<persName><forename type="first">Qihang</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th international ACM SIGIR conference on research and development in information retrieval</title>
		<meeting>the 45th international ACM SIGIR conference on research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1812" to="1816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Stock constrained recommendation in tmall</title>
		<author>
			<persName><forename type="first">Wenliang</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaowei</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2287" to="2296" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
